{"paper": {"title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "authors": ["Shuohang Wang", "Jing Jiang"], "authorids": ["shwang.2014@phdis.smu.edu.sg", "jingjiang@smu.edu.sg"], "summary": "Using Match-LSTM and Answer Pointer to select a variable length answer from a paragraph", "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our tasks. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features. Besides, our boundary model also achieves the best performance on the MSMARCO dataset (Nguyen et al. 2016).", "keywords": ["Natural language processing", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper provides two approaches to question answering: pointing to spans, and use of match-LSTM. The models are evaluated on SQuAD and MSMARCO. The reviewers we satisfied that, with the provision of additional comparisons and ablation studies submitted during discussion, the paper was acceptable to the conference, albeit marginally so."}, "review": {"r1Uouwldg": {"type": "rebuttal", "replyto": "SyA6vNePe", "comment": "Sorry for such a late PDF update! \n\nWe just received the official evaluation on the MSMARCO (Microsoft MAchine Reading COmprehension Dataset) test data set:\n\nbleu_1: 0.4071608785489091\nbleu_2: 0.3388083705901471\nbleu_3: 0.3064746094507665\nbleu_4: 0.2872954087295379\nrouge_l: 0.373298227899\n\nAnd that is consistent with the performance on the development data set. The leaderboard (http://www.msmarco.org/) of this task is coming soon! \n\nWe also added the performance of the ensemble model on SQuAD dataset.\n\nThank you!\n", "title": "Official evaluation on MSMARCO"}, "rJYcoOeDl": {"type": "rebuttal", "replyto": "SyA6vNePe", "comment": "We thank you for your valuable comments again!\n\nQ: 2. It is surprising that the long time of revision (1 month) was not sufficient to run the best ensemble model, even on CPU.\nA: I'm so sorry about it. I didn't have much experience in training ensemble model and made a mistake during the training. As we have GPU now, we have submitted better tuned models with F1 score 77% for a single model and 80% for its ensemble model on the development data set. We will update it after getting the feedback.\n\nQ: 3. I encourage the authors to update the pdf with the official evaluation on MSMARCO, it is surprising that it takes so long (more than a week).\nA: I'm sorry about it. We haven't received the official evaluation yet.We will update it as soon as we get it.\n\nQ:4. The additional experiments with the MSMARCO dataset is interesting and encouraging. There some questions though:\n- \"We select the span that is most similar the answer for training and only predict the span in the passages\": this sentence is not clear to me. Also please clarify how this done at test time. How is the selection made?\nA: We select the span that has the largest F1 score with the golden answer for training. During the test time, we only select one span in the passages as the answer.\n\nQ: - Are there no other prior work results on this task/dataset?\nA: We haven't found other paper exploring this data set besides the dataset paper. The original paper provides several baselines on a subset of the data set as follows:\n\n(1)Best Passage         / Best ROUGE-L of any passage / 0.351\n(2)Passage Ranking      / A DSSM-alike passage ranking model / 0.177\n(3)Sequence to Sequence / Vanilla seq2seq model predicting answers from questions / 0.089\n(4)Memory Network       / Seq2seq model with MemNN for passages / 0.119\n\nAs this subset is not released, we didn't add these performance into our pdf version. What we can see is that the baselines are much worse to the Golden(Best) Passage performance. But our boundary model can outperform the Golden(Best) Passage performance.\n\n\nQ:- You show the human selected passage, but what be the upper bound for selecting a passage, according to the evaluation metrics?\nA:  The upper bound hasn't been provided by the authors yet. The corresponding baselines and the performance of our models will be released through their official website (http://www.msmarco.org/) soon.\n\nQ: - The computation for bleu 1 bleu 2 bleu 3 bleu 4 seems wrong as blue 4 should be much lower than blue 3,2,1 etc, by the definition of the measure. How is it computed?\nA: As the answer could just be a span in the passages, I think the exact match could make blue4 score much higher. We will further explore it. Thank you!\n", "title": "Re: Weak accept from reviewer3"}, "HkQuw3S8l": {"type": "rebuttal", "replyto": "SyTuyCMEx", "comment": "We thank you for your valuable comments!\n\nQ: It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.\nA: I'm sorry about it. Our model still runs on CPUs and that takes time to train a single model. We haven\u2019t successfully built an ensemble model on it yet. We will further explore it and update it.\n\nQ: It would be interested if this approach generalizes to other datasets.\nA: We further explored the dataset \"MS MARCO: A Human Generated MAchine Reading COmprehension Dataset\", where the question is a real submission to the Bing search. Our experiment is shown in Table 2. Our boundary model can outperform the \"Golden Passage\" baseline, which uses the human selected passage from the 10 candidate passages as the answer. We only predict one span in these passages. Our performance on the hidden test data is still in submission and we will update the paper when we get feedback.\n\nQ: The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction.\nA: We added the analysis of the difference between our work and VQA problem in the related work. Thank you for the suggestion!\n\nQ: I am wondering how much this task can be seen as a \u201cguided extractive summarization\u201d, i.e. where the question guides the summarization process. \nA: It's an quite interesting problem! We will further explore it! Thank you for the suggestion! \n\nQ: Page 6, last paragraph: missing \u201c.\u201d: \u201c\u2026 searching This\u2026\u201d\nA: We have fixed the typo in the updated version. Thank you!", "title": "Re: Interesting combination of existing approaches with encouraging results"}, "r1UWPnSUx": {"type": "rebuttal", "replyto": "SkmLjBzNx", "comment": "We thank you for your valuable comments!\n\nQ: The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.\nA: We have added these baselines in Table 1 with the model name \"LSTM with Ans-Ptr\". We use the final state of the question LSTM to replace the representation computed by the attention mechanism and the performance drop heavily without attention.\n\nQ: It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.\nA: The sequence model has a problem of early stop prediction, while the boundary model can somehow overcome this. We did another experiment on the \"MS MARCO: A Human Generated MAchine Reading Comprehension Dataset\" , where the question is a real submission to the Bing search engine. As the average length of the answer in this dataset is 16 words which is much longer than the 3 words in average for the SQuAD,  our boundary model can significantly outperform the sequence model. The average length of the answers generated by the sequence model is 7 words, while the boundary model is 21 words. We further show comparison between the predictions of the boundary and the sequence models in the appendix B.\n\nQ: I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\nA: We show the predictions of different models on different types of questions in Appendix B. Our model can't solve the questions that need several sentences reasoning.\n\nQ: Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across \ndimension of Q. Why not learn different activations for each dimension? \nA: We think that it is somehow introducing the word position information of the question when computing the attention weights, if each dimension learns \na diffenrt {W^p}. We think the semantic meaning of the word is more important than the position information for the attention mechanism. We will explore it in the future. Thank you for the suggestion!\n\nQ: I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.\nA: I'm sorry about it. Our model still runs on CPUs and that takes time to train a single model. We haven\u2019t successfully built an ensemble model on it yet. We will further explore it and update it.\n\nQ: Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?\nA: Our model was first posted in August 2017 through arXiv, two months earlier than the DCR model and other similar work was posted. Our model has some similarities with DCR, such as the attention parts inspired by match-LSTM. While they maximize the correct span from all the candidate spans, we predict the start and the end points of the span. We've added this in the revision.", "title": "Re: More analyses / ablation studies / insights needed regarding the functioning of the proposed model"}, "H1a0rhHLg": {"type": "rebuttal", "replyto": "rkDverW4g", "comment": "We thank you for your valuable comments!\n\nAs our models on the SQuAD dataset was released through arXiv in August 2016, around two months earlier than the other studies on the SQuAD dataset, we believe we were the first to bring the following two useful insights into this task:\n(1) Using attention mechanism to make each word of the passage to have a certain degree of compatibility with the question is important for the SQuAD dataset.\n(2) Only predicting the start and the end points of the answer span is better than predicting the answer span word by word.\n\nThe other studies on the SQuAD dataset (Bidirectional Attention Flow for Machine Comprehension (https://openreview.net/forum?id=HJ0UKP9ge) and Dynamic Co-attention Networks For Question Answering (https://openreview.net/forum?id=rJeKjwvclx)) further explored these two insights and achieved the state-of-art performance.\n\nIn the latest version of the paper, we further added experiments on the dataset \u201cMS MARCO: A Human Generated Machine Reading Comprehension Dataset\u201d. We believe we are the first to explore this dataset where the question is a real submission to the Bing search engine. As the average length of the answer in this dataset is 16 words,  which is much longer than the average (3 words) for the SQuAD dataset, we further showed that our boundary model could significantly outperform the sequence model. The sequence model will always stop the prediction early, while the boundary model can somehow overcome this. Our boundary model can outperform the \"Golden Passage\" baseline, which uses the human selected passage from the 10 candidate passages as the answer. We only predict one span in these passages. We've updated the paper by adding this experiment and the analysis, and several prediction cases in the appendix.\n\nWe will further explore deep match-LSTM. Thank you for the suggestion!", "title": "Re: reviewer2"}, "r1tqnyT7e": {"type": "rebuttal", "replyto": "B1-q5Pqxl", "comment": "Dear reviewers,\n\nThank you for your valuable comments again! We have made the corresponding revisions and updated a new pdf version. We briefly list the changes here:\n\nAnonReviewer1:\n1.We clarify the dimension of row vector $\\alpha_i$ to be $1\\times Q$.\n2.We add the visualization of the $\\alpha$ values for the question requiring world knowledge in Figure 2 and add the corresponding analysis at the end of the section \"Experiments\".\n\nAnonReviewer2:\nWe revise the description of the state-of-the-art results in the last paragraph in \"Introduction\".\nWe clarify the dimension of $G$ to be $l\\time Q$, the row vector $\\alpha_i$ to be $1\\times Q$, the column vector $w$ to be $l\\times 1$ for equation (2). So is the equation (8).\nWe clarify the statement of footnote 3 about the output gates in the pre-processing layer.\nWe clarify the description of global search on the spans in both the boundary model description part and the Table 2.\n\nAnonReviewer3:\nWe clarify the integration of match-LSTM and pointer network in the last two paragraphs of the \"Introduction\".\nWe directly cite the works of the baselines in Table 2.\n\nThanks,\nShuohang", "title": "An updated pdf version"}, "HyC5dZxmg": {"type": "rebuttal", "replyto": "Bytflkgme", "comment": "Dear reviewer,\n\nWe thank you for your valuable comments and will make the revisions as your comments soon!\n\nWe will clarify the integration of match-LSTM in pointer network in the introduction!\n\nYes, the reported numbers are identical and we will directly cite the corresponding works of the baselines in Table 2.\n\nThanks,\nShuohang", "title": "Re: Clarify contribution"}, "Hko9QZlXl": {"type": "rebuttal", "replyto": "rkKHgByQg", "comment": "Dear reviewer,\n\nThank you for your valuable comments!\n\n1. In equation 3, the weighted sum of the question hidden states is a vector computed by the multiplication between $H^q$ and $\\alpha_i$, where $H^q$ is a matrix with the dimension of \"l*Q\" and $\\alpha_i$ is a vector with the dimension of \"Q\". We will clarify the dimension of $\\alpha$ in the revision.\n\n2. We think match-LSTM also has the ability to find the world knowledge alignment as long as similar knowledge appeared in the training data set. We just visualized the $\\alpha$ values for this instance, and we find that \"European\", \"Parliament\", \"Council\", \"European\" and \"Union\" have larger probabilities aligned to the word \"governing\" in the question. Besides, we also find that \"and\" has larger probability aligned to the word \"two\" in the question. We relist this raw instance in the development data set here:\n\nQuestion: Which two governing bodies have legislative veto power?\nSen: ... the European Parliament and the Council of the European Union have powers of amendment and veto during the legislative process ...\n\nWe will show this visualization in the revision and add the analyse of the world knowledge issue soon.\n\nThanks,\nShuohang", "title": "Re: Clarification Questions"}, "Bytflkgme": {"type": "review", "replyto": "B1-q5Pqxl", "review": "Hi,\n\nit would we great if you could clarify in words how your work integrates match-LSTM in pointr network, to clarify your contribution in that respect.\n[I think the reader would benefit from this if you clarify that in the introduction]\n\nA minor comment:\nTable 2 would be clearer if you directly cite the corresponding works there if the numbers you report are identical to the ones the authors report in their work.\n\nThanks.The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.\n\nStrength:\n-\tThe suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results)\n-\tThe paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.\n\n\nWeaknesses:\n1.\tIt is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text.\n2.\tExperimental evaluation\n2.1.\tIt is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.\n2.2.\tIt would be interested if this approach generalizes to other datasets.\n\n\nOther (minor/discussion points)\n-\tThe task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction.\n-\tI am wondering how much this task can be seen as a \u201cguided extractive summarization\u201d, i.e. where the question guides the summarization process.\n-\tPage 6, last paragraph: missing \u201c.\u201d: \u201c\u2026 searching This\u2026\u201d\n\n\n\nSummary:\nWhile the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.\n", "title": "Clarify contribution", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyTuyCMEx": {"type": "review", "replyto": "B1-q5Pqxl", "review": "Hi,\n\nit would we great if you could clarify in words how your work integrates match-LSTM in pointr network, to clarify your contribution in that respect.\n[I think the reader would benefit from this if you clarify that in the introduction]\n\nA minor comment:\nTable 2 would be clearer if you directly cite the corresponding works there if the numbers you report are identical to the ones the authors report in their work.\n\nThanks.The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.\n\nStrength:\n-\tThe suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results)\n-\tThe paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.\n\n\nWeaknesses:\n1.\tIt is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text.\n2.\tExperimental evaluation\n2.1.\tIt is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.\n2.2.\tIt would be interested if this approach generalizes to other datasets.\n\n\nOther (minor/discussion points)\n-\tThe task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction.\n-\tI am wondering how much this task can be seen as a \u201cguided extractive summarization\u201d, i.e. where the question guides the summarization process.\n-\tPage 6, last paragraph: missing \u201c.\u201d: \u201c\u2026 searching This\u2026\u201d\n\n\n\nSummary:\nWhile the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.\n", "title": "Clarify contribution", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkKHgByQg": {"type": "review", "replyto": "B1-q5Pqxl", "review": "Dear Authors,\n\nCould you please clarify the following --\n\n1. In equation 3, weighted question (which is a matrix, right?) is being combined with the current token of the passage (which is a vector, right?). So, could you please clarify how is a vector being combined with matrix? What exact operation is being used?\n\n2. How is the match-LSTM expected to work when the questions are not paraphrases of sentences from the original text? For instance, for the questions which require world knowledge to resolve the correspondences with the sentences from the text (see type-2 in table 3 in the SQuAD paper (https://arxiv.org/pdf/1606.05250.pdf)).\n\nThanks,Summary:\nThe paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.\n\nStrengths:\n1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features.\n2. Significant performance boost over the baseline presented in the SQuAD paper.\n3. Some insightful analyses of the results such as performance is better when answers are short, \"why\" questions are difficult to answer.\n\nWeaknesses/Questions/Suggestions:\n1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.\n2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.\n3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? \n5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.\n6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?\n\nReview Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.", "title": "Clarification Questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkmLjBzNx": {"type": "review", "replyto": "B1-q5Pqxl", "review": "Dear Authors,\n\nCould you please clarify the following --\n\n1. In equation 3, weighted question (which is a matrix, right?) is being combined with the current token of the passage (which is a vector, right?). So, could you please clarify how is a vector being combined with matrix? What exact operation is being used?\n\n2. How is the match-LSTM expected to work when the questions are not paraphrases of sentences from the original text? For instance, for the questions which require world knowledge to resolve the correspondences with the sentences from the text (see type-2 in table 3 in the SQuAD paper (https://arxiv.org/pdf/1606.05250.pdf)).\n\nThanks,Summary:\nThe paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.\n\nStrengths:\n1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features.\n2. Significant performance boost over the baseline presented in the SQuAD paper.\n3. Some insightful analyses of the results such as performance is better when answers are short, \"why\" questions are difficult to answer.\n\nWeaknesses/Questions/Suggestions:\n1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.\n2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.\n3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? \n5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.\n6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?\n\nReview Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.", "title": "Clarification Questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1npqk17g": {"type": "rebuttal", "replyto": "rkT3SnaMx", "comment": "We thank you for your valuable comments! \n\nFor the state-of-the-art results, by the time of submission, the related paper wasn't released, so we didn't go into further discussion. The results were collected from the data set Leaderboard (https://rajpurkar.github.io/SQuAD-explorer/) which keeps updating the best performance on the hidden test data set. We will cite this link instead in the revision.\n\nFor the equation (2),  $H^q$ is a matrix, so $G_i$ represents a matrix with the dimension of l*Q here. $wG_i$ and $b \\bigotimes e_Q$ would be the vectors with the dimension of Q. We think it's clearer to show the softmax function with a vector as input in equation (2). So is the equation (8). We will clarify the dimension of G and F in the revision.\n\nFor the footnote 2, usually LSTM will get each hidden state through the equation \"h=o*tanh(c)\" where \"o\"is the output gate and \"c\" is the memory cell. We find the output gate in the pre-processing LSTM doesn't influence the performance, so we simplify it as \"h=tanh(c)\" to save some training time. We will add this in the revision.\n\nAbout the global search: After getting the probabilities of the boundary points, we need to select a_s and a_e which make P(a_s) * P(a_e|a_s) largest as the boundary points. Actually, the selection of a_s will not influence the value of P(a_e|a_s), as the hidden representation of a_s is computed through $H^r \\beta$ in equation (9). Without other restrictions, we can simply select a_s and a_e which make P(a_s) and P(a_e) largest respectively with the complexity of O(p), where \"p\" is the length of the paragraph sequence.  While the boundary model could point to a too long span covering several sentences, so we manually limit the length of the predicted span. Under this restriction, we need to search for the best a_s and a_e with the complexity of O(s*p), where \"s\" is the longest span we limit for the prediction. We will add more explanation about it in the revision.", "title": "Re: few doubts"}, "rkT3SnaMx": {"type": "review", "replyto": "B1-q5Pqxl", "review": "Hi, \nIn the introduction you  mentioned some state-of-the-art results (Salesforce Resarch?) that are not discussed in the result section.\nI have some trouble to understand equation (2) is it possible that you missed the index j in both \\alpha and G?\nI am probably missing something because in this way the shapes do not sum up for me.\nSame thing at equation (8)\nCould you elaborate more on the statement of footnote 2?\nThe description of global search on the spans is not very clear, could you add something to it?\nSUMMARY.\nThis paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.\nThe proposed model combines two well-know neural network architectures match-lstm and pointer nets.\nFirst the passage and the questions are encoded with a unidirectional LSTM.\nThen the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.\nFor each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.\nThe same process is done in the opposite direction with a backward lstm.\nThe final representation is a concatenation of the two lstms.\nAs a decoded a pointer network is used.\nThe authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.\n\nThe proposed model is tested on the Stanford Question Answering Dataset.\nAn ensemble of the proposed model achieves performance close to state-of-the-art models.\n\n\n----------\n\nOVERALL JUDGMENT\n\nI think the model is interesting mainly because of the use of pointer networks as a decoder.\nOne thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.\nThe analysis of the model is interesting and insightful.\nThe sharing of the code is good.", "title": "few doubts", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkDverW4g": {"type": "review", "replyto": "B1-q5Pqxl", "review": "Hi, \nIn the introduction you  mentioned some state-of-the-art results (Salesforce Resarch?) that are not discussed in the result section.\nI have some trouble to understand equation (2) is it possible that you missed the index j in both \\alpha and G?\nI am probably missing something because in this way the shapes do not sum up for me.\nSame thing at equation (8)\nCould you elaborate more on the statement of footnote 2?\nThe description of global search on the spans is not very clear, could you add something to it?\nSUMMARY.\nThis paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.\nThe proposed model combines two well-know neural network architectures match-lstm and pointer nets.\nFirst the passage and the questions are encoded with a unidirectional LSTM.\nThen the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.\nFor each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.\nThe same process is done in the opposite direction with a backward lstm.\nThe final representation is a concatenation of the two lstms.\nAs a decoded a pointer network is used.\nThe authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.\n\nThe proposed model is tested on the Stanford Question Answering Dataset.\nAn ensemble of the proposed model achieves performance close to state-of-the-art models.\n\n\n----------\n\nOVERALL JUDGMENT\n\nI think the model is interesting mainly because of the use of pointer networks as a decoder.\nOne thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.\nThe analysis of the model is interesting and insightful.\nThe sharing of the code is good.", "title": "few doubts", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}