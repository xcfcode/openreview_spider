{"paper": {"title": "Training deep neural-networks using a noise adaptation layer", "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"], "summary": "Training neural network with noisy labels", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "keywords": ["Deep learning", "Optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "Reviewers agreed that the problem was important and the method was interesting and novel. The main (shared) concerns were preliminary nature of the experiments and questions around scalability to more classes. \n \n During the discussion phase, the authors provided additional CIFAR-100 results and introduced a new approximate but scalable method for performing inference. I engaged the reviewers in discussion, who were originally borderline, to see what they thought about the changes. R2 championed the paper, stating that the additional experiments and response re: scalability were an improvement. On the balance, I think the paper is a poster accept."}, "review": {"HkmwWl08x": {"type": "rebuttal", "replyto": "ryRA5jeVl", "comment": "In the revised version we proposed a scaleble strategy and showed that it does not cause performance degradation.", "title": "case of many classes"}, "rJD6wuvrl": {"type": "rebuttal", "replyto": "H1eu-hlVl", "comment": "1. We added a discussion about the scalability of the method and proposed a scalable variant with the same perfromance.\n\n2. The consecutive softmax layers are exact implementation of a probabilistic modeling that describes the relation between the corrrect and noisy labels. We compare our approach to methods that are based on  a compound objective with two losses and show better results.\n\n3. We added experiments on CNN network with several layers. \n\n4  We added experiment on CIFAR-100  and showed the our method is still better than previous approaches", "title": "reply to review"}, "HkPOYHeBl": {"type": "rebuttal", "replyto": "H12GRgcxg", "comment": "We thank the reviewers for the comments and suggestions\n\n1) We uploaded a revised version that contains experiments on CIFAR-100 dataset. We results on the CIFAR-100 are consistent with results on other data set and show that the performance of our method is better than previous methods.   \n\n2) In the uploaded revised version we also addressed the scalability issue of the method in case of many classes. In our approach we initialized the second soft-max layer using the confusion matrix of the baseline system. The confusion matrix is a good estimation of the label noise.  Assume the rows of the matrix correspond to the true labels and the matrix columns correspond to the noisy labels. The k largest elements in the i-th row are the most occurring noisy class values when the true class value is i.    We can thus connect  i-th elements in the first softmax layer only to its k most probable noisy class  candidates.  (Note that if we connect the i-label in the first softmax only to the i-label in the second softmax layer,  we obtain the standard baseline model.) Taking only k connections to the second softamx layer solves the scalability problem. In the revised version we show experiments on CIFAR-100  that show that by using this scalable approach there is no performance degradation.\n", "title": "Reply to reviews"}, "Sy52_tV7g": {"type": "rebuttal", "replyto": "ByZM18kXg", "comment": "If there are many different classes there is indeed a scalability issue.  We can handle it using standard approaches that address soft-max with many labels such as hierarchical soft-max, NCE and importance sampling.  ", "title": "case of many classes"}, "BJnHvFN7l": {"type": "rebuttal", "replyto": "HJcIz8ZXe", "comment": "First in our paper we provide an alternative to the EM approach. Unless the noise level is very high or there is not enough data, the EM algorithm will converge to a good estimation of the noise distribution. ", "title": "EM behavior "}, "HJcIz8ZXe": {"type": "review", "replyto": "H12GRgcxg", "review": "In your experiments, how well can your EM algorithm recover the \"converted\neach label with probability p to a different label\", from the experiments section?This paper looks at how to train if there are significant label noise present.\nThis is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label.\n\nThe second directly integrates out the true label and simply optimizes the p(z|x).\n\nPros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated.\nCons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.\n\n- comments:\nEquation 11 should be expensive, what happens if you are training on imagenet with 1000 classes?\nIt would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. \n\nOverall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.\n", "title": "Recovering the latent p", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BksCTxHEe": {"type": "review", "replyto": "H12GRgcxg", "review": "In your experiments, how well can your EM algorithm recover the \"converted\neach label with probability p to a different label\", from the experiments section?This paper looks at how to train if there are significant label noise present.\nThis is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label.\n\nThe second directly integrates out the true label and simply optimizes the p(z|x).\n\nPros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated.\nCons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.\n\n- comments:\nEquation 11 should be expensive, what happens if you are training on imagenet with 1000 classes?\nIt would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. \n\nOverall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.\n", "title": "Recovering the latent p", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByZM18kXg": {"type": "review", "replyto": "H12GRgcxg", "review": "Suppose there are N classes to predict, the proposed methods requires (N+1) sets of softmax with the size of N. For speech tasks, there are normally around 10k output classes or even more. Discussions on the scalability of the method and whether they still work well in those cases would very helpful.   The paper addressed the erroneous label problem for supervised training. The problem is well formulated and the presented solution is novel. \n\nThe experimental justification is limited. The effectiveness of the proposed method is hard to gauge, especially how to scale the proposed method to large number of classification targets and whether it is still effective.\n\nFor example, it would be interesting to see whether the proposed method is better than training with only less but high quality data. \n\nFrom Figure 2, it seems with more data, the proposed method tends to behave very well when the noise fraction is below a threshold and dramatically degrades once passing that threshold. Analysis and justification of this behavior whether it is just by chance or an expected one of the method would be very useful. \n\n ", "title": "Scalability to large number of classes", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BkL101z4e": {"type": "review", "replyto": "H12GRgcxg", "review": "Suppose there are N classes to predict, the proposed methods requires (N+1) sets of softmax with the size of N. For speech tasks, there are normally around 10k output classes or even more. Discussions on the scalability of the method and whether they still work well in those cases would very helpful.   The paper addressed the erroneous label problem for supervised training. The problem is well formulated and the presented solution is novel. \n\nThe experimental justification is limited. The effectiveness of the proposed method is hard to gauge, especially how to scale the proposed method to large number of classification targets and whether it is still effective.\n\nFor example, it would be interesting to see whether the proposed method is better than training with only less but high quality data. \n\nFrom Figure 2, it seems with more data, the proposed method tends to behave very well when the noise fraction is below a threshold and dramatically degrades once passing that threshold. Analysis and justification of this behavior whether it is just by chance or an expected one of the method would be very useful. \n\n ", "title": "Scalability to large number of classes", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}