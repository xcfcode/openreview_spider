{"paper": {"title": "Meta-learning with differentiable closed-form solvers", "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "summary": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"]}, "meta": {"decision": "Accept (Poster)", "comment": "The reviewers disagree strongly on this paper. Reviewer 2 was the most positive, believing it to be an interesting contribution with strong results. Reviewer 3 however, was underwhelmed by the results. Reviewer 1 does not believe that the contribution is sufficiently novel, seeing it as too close to existing multi-task learning approaches.\n\nAfter considering all of the discussion so far, I have to agree with reviewer 2 on their assessment. Much of the meta learning literature involves changing the base learner *for a fixed architecture* and seeing how it affects performance. There is a temptation to chase performance by changing the architecture, adding new regularizers, etc., and while this is important for practical reasons, it does not help to shed light on the underlying fundamentals. This is best done by considering carefully controlled and well understood experimental settings. Even still, the performance is quite good relative to popular base learners.\n\nRegarding novelty, I agree it is a simple change to the base learner, using a technique that has been tried before in other settings (linear regression as opposed to classification), however its use in a meta learning setup is novel in my opinion, and the new experimental comparison regression on top of pre-trained CNN features helps to demonstrate the utility of its use in the meta-learning settings.\n\nWhile the novelty can certainly be debated, I want to highlight two reasons why I am opting to accept this paper: 1) simple and effective ideas are often some of the most impactful. 2) sometimes taking ideas from one area (e.g., multi-task learning) and demonstrating that they can be effective in other settings (e.g., meta-learning) can itself be a valuable contribution. I believe that the meta-learning community would benefit from reading this paper.\n"}, "review": {"r1eEpO66S4": {"type": "rebuttal", "replyto": "HyxnZh0ct7", "comment": "The ICLR'19 camera-ready version of the paper has been uploaded.\nCode available at https://github.com/bertinetto/r2d2", "title": "Camera-ready."}, "ByeHwtpR4E": {"type": "rebuttal", "replyto": "BkxDPnDZMV", "comment": "We agree on the importance of making the research in ML (or any field) accessible and reproducible - we are glad that initiatives such as the reproducibility challenge exist.\n\nWe are also glad that the authors were able to reproduce our main findings, despite not having had access to our implementation and using a different framework (we used PyTorch and we are working on finalizing the code release).\n\nIn response to specific details of the report:\n- We agree that the details of stride and padding amounts are missing, and we will update the paper accordingly. This should also resolve the difference in feature dimension between our paper and the replication.\n- We believe that our sentence \u201cTraining is stopped when the error on the meta-validation set does not decrease meaningfully for 20,000 episodes\u201d has been misinterpreted, as the authors say: \u201cwe opted to meta-train for a fixed 20k iterations\u201d.\nWhat we meant is that we performed early-stopping if error does not decrease for a period of 20k episodes, not that we train for 20k episodes in total.\nClearly, in this way the total number of training epoch varies, but we observed that generally it stops around 60k-80k episodes. We will make this point more clear in the camera ready. This also means that our results were obtained with longer training than in the replication.\n- Re the sentence \u201cdifferent neural architectures should be taken into consideration when comparing results\u201d and direct comparison with MAML in general.\nThis comment refers to the fact that we did not report results on a 32-channels embedding in our experiments, which is instead what MAML uses.\nHowever, we believe that our experiments already show that performance is not simply the result of a trivial increase in capacity.\nTo demonstrate that, we reported both a) results of our method on a 64-channels embedding and b) results of three representative baselines (protonets, MAML and GNN) with our embeddings (the * in our tables).", "title": "Thank you for your contribution"}, "BkgTUolegV": {"type": "rebuttal", "replyto": "SJeA_NNyxE", "comment": "1) We wrote: \u201c\u201c[multi-task learning] is different to our work, and in general to all of the previous literature on meta-learning applied to few-shot classification (e.g. Finn et al. 2017, Ravi & Larochelle 2017, Vinyals et al. 2016, etc). Notably, these methods and ours take into account adaptation *already during the training process*, which requires back-propagating errors through the very fine-tuning process.\u201d\u201d\n\n2) R1 answered with: \u201c\u201cMerely because some other paper also had small novelty and got accepted in the past I can not see why this paper should also get accepted\u201d\u201d\n\n3) We then observed that *R1 did not refute any of our point of rebuttal* (long answer in this thread) and seems to be dismissive of the above papers, which are widely accepted by the community.\n\n> \u201c\u201c However, using a multi-task technique in meta-learning setting cannot be treated as a novel or original contribution.\u201d\u201d\nAgain, it is not what we do - we amply addressed this point both on OpenReview (last two answers to the reviewer) and in the paper.\n\nWe would like to repeat that if this were true, the baseline experiment we described (applying ridge regression in the manner that the reviewer refers to as standard) would not have been possible, since our method and the baseline would then be the same (which they are not -- both in methodology and results).", "title": "We were responding to the claim that these papers had small novelty and thus our own as well, which we disagree with"}, "r1l23D05y4": {"type": "rebuttal", "replyto": "SyxWU3iYkE", "comment": "The reviewer has not refuted any of the points we made above. Namely:\n\n- That meta-learning approaches (like ours) back-propagate errors through the fine-tuning process, a major departure from standard multi-task/transfer learning.\n- That *not doing so* incurs a large performance penalty, as demonstrated by our experiments.\n\nWe invite the reviewer to address these points, rather than just reiterate a subjective judgment over the value of meta-learning. While we respect this opinion, our paper cannot be rejected based solely on the reviewer\u2019s opinion that meta-learning papers are not novel in general (compared to multi-task learning).\n", "title": "3 papers with hundreds of citations (Finn et al., Ravi & Larochelle, Vinyals et al.) cannot be dismissed as \u201csome other paper also had small novelty\u201d"}, "HyeglRQ_JE": {"type": "rebuttal", "replyto": "BJg6awLwk4", "comment": "Thank you for pointing us to this interesting paper! We agree that methods with limited inductive bias such as protonets are attractive, and there is indeed a good case for their performance scaling better with computation and data.\nWe are looking forward to try out the proposed testbed. One possible advantage of using our  R2-D2 with the deeper architectures of their setup is that we can concatenate activations from multiple layers together without increasing the computational burden of the base-learner thanks to the Woodbury identity.", "title": "Thanks"}, "rygq4xgwkN": {"type": "rebuttal", "replyto": "r1e-G4YL1N", "comment": "We thank the anonymous commenter for pointing out a GitHub repo with improvements. We note that neither data augmentation nor the optimizer schedule are mentioned at all in the associated published paper.\n\nAdditionally, the mentioned improvements are not specific to prototypical networks (or to any method for that matter), and can also be applied to ours.  As such, we fail to see how this says anything about the merits of our proposal.\nIn our experiments, we compare against prototypical networks using the same setup of the original paper (Adam optimizer, halving LR every 20 epochs; no data augmentation).\nIn this fair comparison, we outperform it.\n\nWe would gain no knowledge by showing that \u201cproto-nets with data augmentation and optimizer improvements\u201d (as suggested) beats \u201cR2D2 with no data augmentation\u201d, or that \u201cMAML with a ResNet base\u201d beats \u201cR2D2 with 4 layers\u201d. These are apples-to-oranges comparisons which make any scientific conclusion very hard to draw.\n\nInstead, a proper comparison is to take the innovation of each paper -- the prototype layer in proto-nets, and the ridge regression layer in R2D2 -- and compare them, with everything else fixed. This includes data augmentation, as well as network model and initialization.\n\nCarefully controlled comparisons are a core part of the scientific method, and ignoring them will lead to unsubstantiated conclusions.\n", "title": "These improvements can be used in any few-shot learning methods. We outperform prototypical networks in an apples-to-apples comparison."}, "r1gGLd-507": {"type": "rebuttal", "replyto": "HyxnZh0ct7", "comment": "We would like to thank both reviewers and anonymous commenters for their feedback and participation.\nIn light of the discussion, the Appendix of the paper has been updated:\n\n* Section B offers a runtime analysis which reveals that R2-D2 is several times faster than MAML and almost as fast as a simple (fixed) metric learning method such as prototypical networks, while still allowing per-episode adaptation.\n* Section A reports the accuracy of the 1-vs-all variant of LR-D2 (as suggested by AnonReviewer2), which is comparable with the one of R2-D2.\n* Finally, Section C extends the discussion sparked here on OpenReview about a) the nature of our contribution b) the disambiguation with the multi-task learning paradigm .", "title": "To all: Appendix now includes runtime analysis, 1-vs-all experiment and extended discussion"}, "BkljQHC-T7": {"type": "rebuttal", "replyto": "r1xPm1Kah7", "comment": "We thank the reviewer for the comments and questions.\n\n> \u201cWhy one can simply treat \\hat{Y} as a scaled and shifted version of X\u2019W?\u201d\nIn the case of logistic regression, the scaling and shifting is not needed, and we have \\hat{Y}=X\u2019W. This is because logistic regression is a classification algorithm, and directly outputs class scores. These scores are fed to the (cross-entropy) loss L.\n\nHowever, ridge regression is a regression algorithm, and its regression targets are one-hot encoded labels, which is only an approximation of the discrete problem (classification). This means that an extra calibration step is needed (eq. 6), to allow the network to tune the regressed outputs into classification scores for the cross-entropy loss L.\n\n> \u201cThe empirical performance of the proposed approach is not very promising and it does not outperform the comparison methods, e.g., SNAIL\u201d\nOur method actually outperforms SNAIL on an apples-to-apples comparison, with the same number of layers. We would like to draw the reviewer\u2019s attention to the last paragraph of the \u201cMulti-class classification\u201d subsection (page 8).\n\nThe result mentioned by the reviewer uses a ResNet, while we use a 4-layer CNN to remain comparable to prior work. SNAIL with a 4-layer CNN ([11] Appendix B) performs much worse than our method (7.4% to 10.0% accuracy improvement).\n\nEven disregarding the great difference in architecture capacity, our proposal's performance coincides with SNAIL on miniImageNet 5way-5shot and it is comparable on 3 out of 4 Omniglot setups. We would have liked to establish a comparison also on CIFAR, but unfortunately the official code for SNAIL hasn\u2019t been released.\n\nBorrowing the words of AnonReviewer2: \u201cNotably, the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced.\u201d\n\nWe hope that this addresses the two concerns raised by the reviewer. We will be happy to answer any other question about the paper.\n", "title": "Our proposal demonstrates results competitive with SNAIL despite using a much simpler architecture (SNAIL uses ResNet, we just use 4 conv layers)."}, "H1e3St5u67": {"type": "rebuttal", "replyto": "SJlghKO937", "comment": "We thank the reviewer for the comment.\nHowever, we believe that the low score originates from a misunderstanding of our proposal.\nBelow, we try to bring some clarity by disambiguating between what the reviewer refers to and our method.\nIf our interpretation of what the reviewer refers to as \u201centirely common\u201d is incorrect, it would be great to be provided with at least one reference, so that we can continue the conversation on the same ground.\n\n> \u201cnovel contribution?\u201d , \u201ctraining multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common.\u201d\n\u201cIt is also common freeze the feature representation learned from the first set of tasks, and to simply use it for new tasks by modifying the last layer\u201d\n\nWe understand that the reviewer is hinting at the common multi-task scenario with a shared network and task-specific layers (e.g. Caruana 1993). He/she also refers to basic transfer learning approaches in which a CNN is first pre-trained on one dataset/task and then adapted to a different dataset/task by simply adapting the final layer(s) (e.g. Yosinski et al. \u201cHow transferable are features in deep neural Networks?\u201d - NIPS 2014; Chu et al. \u201cBest Practices for Fine-tuning Visual Classifiers to New Domains\u201d - ECCVw 2016).\n\nIf so, then this is significantly different to our work, and in general to all of the previous literature on meta-learning applied to few-shot classification (e.g. Finn et al. 2017, Ravi & Larochelle 2017, Vinyals et al. 2016, etc).\nNotably, these methods and ours take into account adaptation *already during the training process*, which requires back-propagating errors through the very fine-tuning process.\n\nWithin this setup, our main contribution is to propose an adaptation procedure based on closed-form regressors, which have the important characteristic of allowing different models for different episodes while still being fast because of 1) their convergence in one (R2-D2) or few (LR-D2) steps, 2) the use of the Woodbury identity, which is particularly convenient in the few-shot data regime, and 3) back-propagation through the closed-form regressor can be made efficient.\n\nTo better illustrate our point, we conducted a baseline experiment.\nFirst, we pre-trained the same 4-layers CNN architecture, but for a standard classification problem, using the same training samples as our method. We simply added a final fully-connected layer (with 64 outputs, like the number of classes in the training splits) and used the cross-entropy loss.\nThen, we used the convolutional part of this trained network as a feature extractor and fed its activation to our ridge-regression layer to produce a per-episode set of weights.\nOn miniImagenet, the drop in performance w.r.t. our proposed R2-D2 is very significant: 13.8% and 11.6% accuracy for the 1 and 5 shot problems respectively.\nResults are consistent on CIFAR, though less drastic: 11.5% and 5.9%.\n\nThis confirms that simply using a \u201cshared feature representation and task specific final layer\u201d as commented by the reviewer is not what we are doing and it is not a good strategy to obtain results competitive with the state-of-the-art in few-shot classification.\nInstead, it is necessary to enforce the generality of the underlying features during training explicitly, which we do by back-propagating through the fine-tuning procedure (the closed-form regressors).\n\nWe would like to conclude remarking that, probably, the source of confusion arises from the overlap that exists in general between the few-shot learning and the transfer/multi-task learning sub-communities.\nWe realize that the two have developed fairly separately while trying to solve very related problems, and unfortunately the similarities/differences are not acknowledged enough in few-shot classification papers, including our own. We intend to alleviate this problem in our related work section, and invite the reviewer to suggest more relevant works from this area.\n", "title": "This is a comment on a different technique than what we propose"}, "SyedD2rXaQ": {"type": "rebuttal", "replyto": "Byg2xy8MpX", "comment": "Thank you. \n\n> \u201cI understand that the main novelty here is to apply fine tuning on the test set (of tasks sampled for training) in meta-learning, instead of on the training data of a single supervised learning task (as we normally did in supervised learning).\u201d\n\nSorry but this is not claimed in the paper or in the answer above. Clearly, the overall training framework is not novel and it is common in the few-shot learning literature. In fact, we specifically wrote: \u201cOur training procedure (and indeed, all meta-learning methods for few-shot learning, such as MAML, SNAIL, etc) ...\u201d.\n\nThe point of our previous comment was simply to clarify why different episodes correspond to different sets of parameters.\n\n\n> \u201c\u201cchanging the model of base learners cannot be recognized as a novelty\u201d\nWe strongly disagree with the statement. This is exactly the nature of the contribution of most approaches for few-shot classification. For example, both MAML and prototypical networks use the same algorithm (SGD) in the external loop, while they vastly differ for the method used in the inner loop (SGD and nearest neighbour respectively).\n\nOur contribution is to use closed-form solvers such as ridge regression to tackle few-shot classification, which is novel in the literature and it is a non-trivial endeavor.\nAs stated by AR2: \u201c[it] strikes an interesting compromise between not performing any adaptation for each new task (as is the case in pure metric learning methods [e.g. prototypical networks]]) and performing an expensive iterative procedure, such as MAML or Meta-Learner LSTM where there is no guarantee that after taking the few steps prescribed by the respective algorithms the learner has converged.\u201d\n\nBesides offering a trade-off with respect to existing techniques, our proposal also presents a significant practical value in terms of performance, as outlined in our experimental section.\n", "title": "There is ample precedent in the few-shot learning literature for proposing new base learners as the main contribution."}, "Hkgvq5AZpm": {"type": "rebuttal", "replyto": "Syegwm5yaQ", "comment": "Thank you, this is a really nice paper. The bi-level optimization point of view is very insightful. Although their framework is very general, they seem to specialize it in the experiments using gradient descent for the inner loop, which is different from our closed-form solutions.", "title": "."}, "rkevXdCb6m": {"type": "rebuttal", "replyto": "r1ggxa85nm", "comment": "> \u201cI am confused about whether the proposed method is the same as \u2026 multiple models (e.g., logistic regression) for different tasks based on shared input features provided by a pre-trained model (e.g., CNN)\u201d\n\nThank you for participating in the discussion. This describes well only the behavior at test-time -- when facing a new task, a new regressor is learned based on pre-trained features (hence, different tasks will have different parameters). However, this leaves out a crucial detail: where does this pre-trained CNN come from?\n\nThe standard approach is to use a CNN that was pre-trained on ImageNet or another task. However, there is no guarantee that the CNN features will transfer well to unknown tasks. In the case of few-shot learning, with only 1 or 5 training samples, fine-tuning will result in extreme over-fitting.\n\nOur training procedure (and indeed, all meta-learning methods for few-shot learning, such as MAML, SNAIL, etc) train the CNN features specifically to perform well on new, unseen tasks. \u201cPerforming well on unseen tasks\u201d is formalized as achieving a low error after fine-tuning. This means that we have to back-propagate errors through the fine-tuning procedure, which can be SGD (MAML) or a ridge/logistic regression solver (ours). The end result is a CNN that is especially trained to be fine-tuned later under the same conditions; this differs substantially from standard pre-training.\n\nThere is a nice, informal introduction to this (admittedly subtle!) distinction, that was written by the authors of MAML:\nhttps://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/\n", "title": "It's the procedure to generate the pre-trained convnet"}, "B1lS7rnxT7": {"type": "rebuttal", "replyto": "SkeX8K6thm", "comment": "We thank the reviewer for the insightful comments and analysis.\n\n> \u201cOne-vs-all classifiers\u201d for LR-D2\nThis is a great suggestion, and we are not quite sure how we missed it. We will update the results for 5-way classification incorporating this method.\n\n> \u201cablation where for the LR-D2 variant SGD was used ... instead of Newton\u2019s method\u201d\nWe previously did exactly this experiment, although for the R2-D2 (ridge regression) variant. We did not include it due to space constraints. It is equivalent to MAML, which also uses SGD, but adapting only the classification layer for new tasks (instead of adapting all parameters).\n\nWe tested this variant on miniImageNet with 5 classes, with the lowest-capacity CNN (which is the most favorable model for MAML/SGD). It yields 45.4\u00b11.6% accuracy for 1-shot classification and 61.7\u00b11.0% for 5-shot classification. Comparing it to Table 1, there\u2019s a drop in performance compared to our closed form solver (3.5% and 4.4% less accuracy, respectively), and also compared to the original MAML (3.3% and 1.4% respectively).\n\nAlthough we expect the conclusions for logistic regression (LR-D2) to be similar, we will extend the experiment to this case and report the results.\n\n> \u201cNeither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example\u201d\nWe agree, and will amend the text. Their interest may lie more in their technical novelty.\n\n> Suggestions on multinomial term and sentence grammar\nThese do improve the readability of the text and will be corrected.\n", "title": "Response to AR2"}, "r1xPm1Kah7": {"type": "review", "replyto": "HyxnZh0ct7", "review": "This paper proposes a new meta-learning method based on closed-form solutions for task specific classifiers such as ridge regression and logistic regression (iterative). The idea of the paper is quite interesting, comparing to the existing metric learning based methods and optimization based methods. \n\nI have two concerns on this paper. \nFirst, the motivation and the rationale of the proposed approach is not clear. In particular, why one can simply treat \\hat{Y} as a scaled and shifted version of X\u2019W?\n\nSecond, the empirical performance of the proposed approach is not very promising and it does not outperform the comparison methods, e.g., SNAIL.  It is not clear what is the advantage. \n", "title": "results are not very promising", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJlghKO937": {"type": "review", "replyto": "HyxnZh0ct7", "review": "Summary: The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep NN), and treating each task  as having its own final layer which could be a ridge regression or a logistic regression. The paper also proposes to separate the data for each task into a training set used to optimize the last, task specific layer, and a validation set used to optimize all previous layers and hyper parameters. \n\nNovelty: This reviewer is unsure what the paper claims as a novel contribution. In particular training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common. It is also common freeze the feature representation learned from the first set of tasks, and to simply use it for new tasks by modifying the last (few) layer(s) which would according to this paper qualify as meta-learning since the new task can be learned with very few new examples. \n\n", "title": "Not clear what is novel here", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkeX8K6thm": {"type": "review", "replyto": "HyxnZh0ct7", "review": "This paper proposes a meta-learning approach for the problem of few-shot classification. Their method, based on parametrizing the learner for each task by a closed-form solver, strikes an interesting compromise between not performing any adaptation for each new task (as is the case in pure metric learning methods) and performing an expensive iterative procedure, such as MAML or Meta-Learner LSTM where there is no guarantee that after taking the few steps prescribed by the respective algorithms the learner has converged. For this reason, I find that leveraging existing solvers that admit closed-form solutions is an attractive and natural choice. \n\nSpecifically, they propose ridge regression as their closed-form solver (R2-D2 variant). This is easily incorporated into the meta-learning loop with any hyperparameters of this solver being meta-learned, along with the embedding weights as is usually done. The use of the Woodbury equation allows to rewrite the closed form solution in a way that scales with the number of examples instead of the dimensionality of the features; therefore taking advantage of the fact that we are operating in a few-shot setting. While regression may seem to be a strange choice for eventually solving a classification task, it is used as far as I understand due to the availability of this widely-known closed-form solution. They treat the one-hot encoded labels of the support set as the regression targets, and additionally calibrate the output of the network (via a transformation by a scale and bias) in order to make it appropriate for classification. Based on the loss of ridge regression on the support set of a task, a parameter matrix is learned for that task that maps from the embedding dimensionality to the number of classes. This matrix can then be used directly to multiply the embedded (via the fixed for the purposes of the episode embedding function) query points, and for each query point, the entry with the maximum value in the corresponding row of the resulting matrix will constitute the predicted class label.\n\nThey also experimented with a logistic regression variant (LR-D2) that does not admit a closed-form solution but can be solved efficiently via Newton\u2019s Method under the form of Iteratively Reweighted Least Squares. When using this variant they restrict to tackling the case of binary-classification.\n\nA question that comes to mind about the LR-D2 variant: while I understand that a single logistic regression classifier is only capable of binary classification, there seems to be a straightforward extension to the case of multiple classes, where one classifier per class is learned, leading to a total of N one-vs-all classifiers (where N is the way of the episode). I\u2019m curious how this would compare in terms of performance against the ridge regression variant which is naturally multi-class. This would allow to directly apply this variant in the common setting and would enable for example still oversampling classes at meta-training time as is done usually.\n\nI would also be curious to see an ablation where for the LR-D2 variant SGD was used as the optimizer instead of Newton\u2019s method. That variant may require more steps (similar to MAML), but I\u2019m curious in practice how this performs.\n\nA few other minor comments:\n- In the related work section, the authors write: \u201cOn the other side of the spectrum, methods that optimize standard iterative learning algorithms, [...] are accurate but slow.\u201d Note however that neither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example. So I wouldn\u2019t really present this as a trade-off between accuracy and speed.\n- I find the term multinomial classification strange. Why not use multi-class classification?\n- In page 8, there is a sentence that is not entirely grammatically correct: \u2018Interestingly, increasing the capacity of the other method it is not particularly helpful\u2019.\n\nOverall, I think this is good work. The idea is natural and attractive. The writing is clear and comprehensive. I enjoyed how the explanation of meta learning and the usual episodic framework was presented. I found the related work section thorough and accurate too. The experiments are thorough as well, with appropriate ablations to account for different numbers of parameters used between different methods being compared. This approach is evidently effective for few-shot learning, as demonstrated on the common two benchmarks as well as on a newly-introduced variant of cifar that is tailored to few-shot classification. Notably, the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced. Interestingly, other models such as MAML actually suffer when given additional capacity, potentially due to overfitting.\n", "title": "A good idea that achieves good results", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}