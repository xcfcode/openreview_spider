{"paper": {"title": "Stabilizing Adversarial Nets with Prediction Methods", "authors": ["Abhay Yadav", "Sohil Shah", "Zheng Xu", "David Jacobs", "Tom Goldstein"], "authorids": ["jaiabhay@cs.umd.edu", "sohilas@umd.edu", "xuzh@cs.umd.edu", "djacobs@umiacs.umd.edu", "tomg@cs.umd.edu"], "summary": "We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.", "abstract": "Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.", "keywords": ["adversarial networks", "optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper provides a simple technique for stabilizing GAN training, and works over a variety of GAN models.\n\nOne of the reviewers expressed concerns with the value of the theory. I think that it would be worth emphasizing that similar arguments could be made for alternating gradient descent, and simultaneous gradient descent. In this case, if possible, it would be good to highlight how the convergence of the prediction method approach differs from the alternating descent approach. Otherwise, highlight that this theory simply shows that the prediction method is not a completely crazy idea (in that it doesn't break existing theory).\n\nPractically, I think the experiments are sufficiently interesting to show that this approach has promise. I don't see the updated results for Stacked GAN for a fixed set of epochs (20 and 40 at different learning rates). Perhaps put this below Table 1."}, "review": {"HyUTUNfHz": {"type": "rebuttal", "replyto": "HJR6IerZf", "comment": "Dear Reviewer,\n\nWe have tried addressing all your concerns in our latest response, please let us know if you still have any remaining concerns ? \n", "title": "Any remaining Concerns ?"}, "SyaZlQnEG": {"type": "rebuttal", "replyto": "rJTFwew4M", "comment": "> Rather, I'm saying that other algorithms have similar theoretical guarantees\nCould the reviewer be more specific on which algorithms have similar theoretical guarantees with ours? We believe we have clearly distinguish our analysis from the references mentioned before. We would like to emphasize theoretical results (especially upper bound) only provides worst-case guarantee. The empirical performance may vary for different algorithms under same guarantees. Again, we agree GANs may not satisfy our assumptions that have been widely used in GAN optimization, but the analysis is not unnecessary.\n\n> the proof of Theorem 1 seems like it would apply to simultaneous gradient descent\nThe main purpose of theorem 1 is to show the prediction method converges in a convex-concave setting. It is correct that similar analysis can be applied to general alternating gradients and simultaneous gradients without the prediction step. However, we are unaware of previous convergence analysis for alternative gradients with prediction step except for the (non-stochastic) bilinear problems discussed in related work. \n\n> Why should I expect it to use more significantly more RAM?\nIn most current implementation frameworks, we need to store weights of generator, weights of discriminator, gradients of generator, and gradients of discriminator, and  all the four variables need to be stored in RAM (GPU memory) for simultaneous gradient descent. However, only one of the gradients (either generator or discriminator) needs to be stored in GPU anytime for alternate updates. It can make a big difference for training large networks on GPU with limited memory.\n\n>> prediction makes really difficult problems really easy\nThe reviewer is simply nitpicking the part of our response. In our earlier response we have clearly quoted, \n\n\u201cThe purpose of the experiments is not to show that we can train things that are *impossible* to train via other methods (indeed, almost anything is possible if you tune the hyperparameters and network architecture enough), but rather that prediction makes really difficult problems really easy.\u201d \n\nThis do suggest that the models considered in our work can be solved by various methods. However, each of these models requires different tricks to actually make them work (For more details please refer section 3.2). In our work, three different GAN models were considered. For each of these models, single method i.e., prediction has been shown to work equally well for the default setting and remains stable for wide range of hyper-parameters. Moreover, it is not clear whether the tricks mentioned in Improved WGAN paper works well when applied to other GAN models or loss functions.\n\n> Your measure of 'really difficult' is  behind the GAN literature in an empirical sense.\nCould the reviewer be more specific ? Pointer to any references ?\n\nRegarding Imagenet:\nThe reported variance was the outcome of the inception score code. In our latest revision, the updated  figure is now an average over five different instances.\n\nWe also thank reviewer for suggestions on improving our paper presentation.\n\n\n", "title": "Response to reviewer concerns"}, "HJR6IerZf": {"type": "review", "replyto": "Skj8Kag0Z", "review": "NOTE:\nI'm very willing to change my recommendation if I turn out to be wrong \nabout the issues I'm addressing and if certain parts of the experiments are fixed.\n\nHaving said that, I do (think I) have some serious issues: \nboth with the experimental evaluation and with the theoretical results.\nI'm pretty sure about the experimental evaluation and less sure about the theoretical results.\n\n\nTHEORETICAL CLAIMS:\n\nThese are the complaints I'm not as sure about:\n\nTheorem 1 assumes that L is convex/concave.\nThis is not generally true for GANs.\nThat's fine and it doesn't necessarily make the statement useless, but:\n\nIf we are willing to assume that L is convex/concave, \nthen there already exist other algorithms that will provably converge\nto a saddle point (I think). [1] contains an explanation of this.\nGiven that there are other algorithms with the same theoretical guarantees,\nand that those algorithms don't magically make GANs work better, \nI am much less convinced about the value of your theorem.\n\nIn [0] they show that GANs trained with simultaneous gradient descent are locally asymptotically stable, \neven when L is not convex/concave. \nThis seems like it makes your result a lot less interesting, though perhaps I'm wrong to think this?\n\nFinally, I'm not totally sure you can show that simultaneous gradient descent won't converge \nas well under the assumptions you made.\nIf you actually can't show that, then the therom *is* useless, \nbut it's also the thing I've said that I'm the least sure about.\n\n\nEXPERIMENTAL EVALUATION:\n\nRegarding the claims of being able to train with a higher learning rate:\nI would consider this a useful contribution if it were shown that (by some measure of GAN 'goodness')\na high goodness was achieved faster because a higher learning rate was used.\nYour experiments don't support this claim presently, because you evaluate all the models at the same step.\nIn fact, it seems like both evaluated Stacked GAN models get worse performance with the higher learning rate.\nThis calls into question the usefulness of training with a higher learning rate.\nThe performance is not a huge amount worse though (based on my understanding of Inception Scores),\nso if it turns out that you could get that performance\nin 1/10th the time then that wouldn't be so bad.\n\nRegarding the experiment with Stacked GANs, the scores you report are lower than what they report [2].\nTheir reported mean score for joint training is 8.59.\nAre the baseline scores you report from an independent reproduction?\nAlso, the model they have trained uses label information. \nDoes your model use label information?\nGiven that your reported improvements are small, it would be nice to know what the proposed mechanism is by \nwhich the score is improved. \nWith a score of 7.9 and a standard deviation of 0.08, presumably none of the baseline model runs\nhad 'stability issues', so it doesn't seem like 'more stable training' can be the answer.\n\nFinally, papers making claims about fixing GAN stability should support those claims by solving problems\nwith GANs that people previously had a hard time solving (due to instability).\nI don't believe this is true of CIFAR10 (especially if you're using the class information).\nSee [3] for an example of a paper that does this by generating 128x128 Imagenet samples with a single generator.\n\nI didn't pay as much attention to the non-GAN experiments because\na) I don't have as much context for evaluating them, because they are a bit non-standard.\nb) I had a lot of issues with the GAN experiments already and I don't think the paper should be accepted unless those are addressed.\n\n\n[0] https://arxiv.org/abs/1706.04156 (Gradient Descent GAN Optimization is Locally Stable)\n\n[1] https://arxiv.org/pdf/1705.07215.pdf (On Convergence and Stability of GANs)\n\n[2] https://arxiv.org/abs/1612.04357 (Stacked GAN)\n\n[3] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs)\n\nEDIT: \nAs discussed below, I have slightly raised my score. \nI would raise it more if more of my suggestions were implemented (although I'm aware that the authors don't have much (any?) time for this - and that I am partially to blame for that, since I didn't respond that quickly).\nI have also slightly raised my confidence.\nThis is because now I've had more time to think about the paper, and because the authors didn't really address a lot of my criticisms (which to me seems like evidence that some of my criticisms were correct).", "title": "Some issues with both experiments and theoretical claims.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rk7Fq_IEM": {"type": "rebuttal", "replyto": "Skj8Kag0Z", "comment": "Dear ACs and Reviewers, \n\nDo you have any questions? \nAre there any remaining concerns?\n\nBest regards, \nThe Authors", "title": "Any questions or concerns ?"}, "SkpeoDLEG": {"type": "rebuttal", "replyto": "rJvkqPIVz", "comment": "Thanks for trying it out, please let me know if you need any help implementing it ?", "title": "Thanks "}, "ry-q9ZOlf": {"type": "review", "replyto": "Skj8Kag0Z", "review": "This paper proposes a simple modification to the standard alternating stochastic gradient method for GAN training, which stabilizes training, by adding a prediction step. \n\nThis is a clever and useful idea, and the paper is very well written. The proposed method is very clearly motivated, both intuitively and mathematically, and the authors also provide theoretical guarantees on its convergence behavior. I particularly liked the analogy with the damped harmonic oscillator.  \n\nThe experiments are well designed and provide clear evidence in favor of the usefulness of the proposed technique. I believe that the method proposed in this paper will have a significant impact in the area of GAN training.\n\nI have only one minor question: in the prediction step, why not use a step size, say \n$\\bar{u}_k+1 = u_{k+1} + \\gamma_k (u_{k+1} \u2212 u_k)$, such that the \"amount of predition\" may be adjusted?\n", "title": "A simple modification to alternating stochastic gradient for GAN training, which stabilizes training, essentially for free. Clever and useful idea, solid and insightful analysis, good presentation. ", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJgHrDugM": {"type": "review", "replyto": "Skj8Kag0Z", "review": "This work proposes a framework for stabilizing adversarial nets using a prediction step. The prediction step is motivated by primal-dual algorithms in convex optimization where the term having both variables is bi-linear.  \n\nThe authors prove a convergence result when the function is convex in one variable and concave in the other. This problem is more general than the previous one in convex optimization.  Then this prediction step is applied in many recent applications in training adversarial nets and compared with state-of-the-art solvers. The better performance of this simple step is shown in most of the numerical experiments. \n\nThough this work applies one step from the convex optimization to solve a more complicated problem and obtain improved performance,  there is more work to be done. Whether there is a better generalization of this prediction step? There are also other variants of primal-dual algorithms in convex optimization; can other modification including the accelerated variants be applied?", "title": "Good work", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyyOtPi7G": {"type": "rebuttal", "replyto": "Bk1l-xfmf", "comment": "As per the suggestion, we experimented with Imagenet dataset using AC-GAN [1] model with and without prediction. Please find the result in the supplementary material. Note that unlike the model used in spectral regularization [2] article, AC-GAN model do not use conditional BN, resnet blocks, hinge loss etc. Thus compared to [2], the reported inception score is low. We stick to AC-GAN as it is the only publicly available model which works best on Imagenet dataset.\n\n[1] https://arxiv.org/abs/1610.09585 (AC-GAN)\n[2] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs)", "title": "Results on Imagenet"}, "rkou2HjXf": {"type": "rebuttal", "replyto": "Skj8Kag0Z", "comment": "As per the suggestion from Reviewer4, we experimented with Imagenet dataset using AC-GAN [1] model with and without prediction. Please find the result in the supplementary material. Note that unlike the model used in spectral regularization [2] article, AC-GAN model do not use conditional BN, resnet blocks, hinge loss etc. Thus compared to [2], the reported inception score is low. We stick to AC-GAN as it is the only publicly available model which works best on Imagenet dataset.\n\n[1] https://arxiv.org/abs/1610.09585 (AC-GAN)\n[2] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs)", "title": "Results on Imagenet"}, "Bk1l-xfmf": {"type": "rebuttal", "replyto": "HJR6IerZf", "comment": "The purpose of the experiments is not to show that we can train things that are *impossible* to train via other methods (indeed, almost anything is possible if you tune the hyperparameters and network architecture enough), but rather that prediction makes really difficult problems really easy.  Compared to simple alternating gradient methods, prediction methods are more stable than other methods, work with a much wider range of hyperparameters than classical schemes, and don\u2019t suffer from the collapse phenomenon that make it difficult to use other methods. \n   \n Below, we address reviewer\u2019s comments that seem to pertain to specific dataset and architecture:\n \nRegarding DCGAN experiments (Without using label information):\nFigure 4 uses the finely tuned learning rate and momentum parameters that come with the pre-packaged DCGAN code distribution. This figure shows that DCGAN collapses frequently; even with these fine tuned parameters it still requires a carefully chosen stopping time/epoch to avoid collapse.  With prediction it does not collapse at all.  The purpose of increasing the learning rate is not to show that \u201cbetter\u201d results could be had, but rather to show that prediction methods don\u2019t require finely tuned parameters.  If you have a look at the additional experiments in the appendix (page 18), we train DCGAN with a litany of different learning rate and momentum parameters.  The prediction method succeeds without any collapse events in all of these cases, while non-prediction is unstable as soon as we move away from the carefully tuned parameter choices.\n\nRegarding Stacked GAN (With using label information):  \n   We reproduced this experiment using the Stacked Gan author\u2019s publicly available code, but were not able to get the same inception scores for Stacked GAN as the original authors.  Note the release code did not come with code for computing inception scores, and we used a well-known Tensor Flow implementation that may differ from what the original author\u2019s used. \n   We ran all the scenarios for a fixed number of epochs (200 epochs, which is default in the Stacked GAN\u2019s released code) to ensure a fair comparison. Indeed, prediction method was able to achieve the best inception score of 8.83 at lesser epoch than 200. Having said that, as per the suggestion, below we also report the performance score measured at the fewer number of epochs for higher learning rates. The quantitative comparison based on the inception score for learning rates of 0.0005 (200/5 = 40 epochs) and 0.001 (200/10 = 20 epochs) are as follows-\n\nLearning Rate\t\t\t              0.0005 (epochs=40)\t\t         0.001 (epochs=20)\nStacked GAN (joint)                                 5.80 +/- 0.15                                    1.42 +/- 0.01\nStacked GAN (joint) + Prediction           8.10 +/- 0.10                                    7.79 +/- 0.07\n\nRegarding the absence of problems that are \u201chard\u201d without prediction:  In Figure 8 of the appendix, we solve a toy problem that is famously hard:  trying to recover all of the modes in a Gaussian mixture model.  The prediction method does this easily, while the method without prediction fails to capture all the modes.  We also \u201cturn the dial up\u201d on this problem by using 100 Gaussian components in Figure 9, and the non-prediction method produces highly irregular results unless a batch size of over 6000 (which is very much larger than the number of components) is used.  In contrast, the prediction method represents the distribution well for a wide range of batch sizes and learning rates.", "title": "Experimental comments"}, "HkUSllGXM": {"type": "rebuttal", "replyto": "HJR6IerZf", "comment": "We agree with the reviewer that theory in this area (and in deep learning in general) often requires assumptions that don\u2019t hold for neural networks.  Nonetheless, we think it is worth taking time to explore conditions under which algorithms are guaranteed to work, because this provides a theoretical proof-of-concept, and thinking through theoretical properties of a new algorithm makes it more than just another hack.   The purpose of our result is to do just that for our proposed algorithm.  We don\u2019t disagree that analysis exists for other algorithms, but we don\u2019t think the existence of other algorithms gets us \u201coff the hook\u201d from thinking about the theoretical implications of our approach.  \n\nThat being said, we think the reviewer is overestimating the state of the art in theory for GANs.  There is currently no theoretical result that does not make strong assumptions, and many results (including those referenced by the reviewer) are quite different from (and in many ways weaker than) our own.  The result in [1] shares certain assumptions with our own (convex-concave assumptions, bounded problem domain, and an ergodic measure of convergence).  However, the result in [1] does not prove convergence in the usual sense, but rather that the error will decay to within an o(1) constant.  In contrast, our result shows that the error decays to zero.  The result in [1] also requires simultaneous gradient descent, which is not commonly used in practice (because it requires more RAM to store [extremely large] iterates and it uses a stale iterate when updating the generator and discriminator one-at-a-time).  In contrast, our result concerns the commonly used alternating direction approach.\n   The result in [0] shows stability using a range of assumptions that are different from (but not necessarily stronger or weaker than) our own.  They require the discriminator to be a linear classifier, and make a strict concavity assumption on the loss function.  They also require an assumption (called Property I) that is analogous to the \u201cstrict saddle\u201d assumption in the saddle-point literature (see, e.g. Lee 2016, \u201cGradient Descent Converges to Minimizers\u201d), which is known not to hold for general neural nets.  Also, note that the result in [0] is only a local stability result (it only holds once the iterates get close to a saddle satisfying the assumptions), whereas our result is a global convergence result that holds for any initialization.\n\tFinally, we emphasize that both [0] and [1] are great works that make numerous important contributions to this field and address a host of issues beyond just convergence proofs.  Our purpose here is not to make any claims that our result is \u201cbetter\u201d than theirs, but rather to state what differentiates our result from the literature, and why we felt it was worth putting it in the paper.  ", "title": "Response for theoretical comments"}, "HyXpJeMQz": {"type": "rebuttal", "replyto": "HJgHrDugM", "comment": "We thank the reviewer for the thoughtful comments and suggestions for future work.   We think the idea of pursuing accelerated methods is particularly interesting.  We have actually already done some experiments with Nesterov-type acceleration (as described for saddle-point problems by Chambolle and Pock), however it seems that the benefits of acceleration vanish when we move from deterministic to stochastic updates.  We\u2019ve made similar observations for standard convex (non-saddle) problems.  That being said, we\u2019re still interested in this direction, and are keeping our eyes peeled for possible ways forward.", "title": "Regarding accelerated methods"}, "HkOZkgzQf": {"type": "rebuttal", "replyto": "ry-q9ZOlf", "comment": "Thanks for the thoughtful comments!  To answer your question:  it is indeed possible to generalize this method by adding an extra stepsize parameter for the prediction step, and this is something that we have experimented with extensively.  It can be shown that your proposed \u201cgamma\u201d parameter method is stable (under convexity assumptions) whenever gamma is between 0 and 2.  However, we have not been able to find any worthwhile advantages to choosing any gamma different from 1.  Choosing a smaller gamma weakens the stability benefits of prediction, and choosing a larger gamma seems to slow down convergence a bit.  The latter effect can be compensated for by choosing a larger learning rate, but even in this case the method doesn\u2019t run noticeably faster than with gamma=1.  For this reason, including this \u201cgamma\u201d seemed like unnecessarily added complexity, so we removed it and went with a cleaner presentation.", "title": "Regarding step size gamma"}}}