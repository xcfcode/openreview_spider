{"paper": {"title": "The Neural Noisy Channel", "authors": ["Lei Yu", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Tomas Kocisky"], "authorids": ["lei.yu@cs.ox.ac.uk", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "tkocisky@google.com"], "summary": "We formulate sequence to sequence transduction as a noisy channel decoding  problem and use recurrent neural networks to parameterise the source and channel  models.", "abstract": "We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper adapts NMT to a noisy channel formulation utilizing the recently developed SSNT framework. The paper is well-written and has solid experimental results. Howver, the paper can be improved with a bit more originality and impact. In, short the pros and cons of the paper are:\n \n Pro: \n - Clarity: All agree paper was very \"well written\" \n - Quality: Reviewers note the \"strong experimental section\". Comprehensive results. \n \n Cons: \n - Originality: There were concerns about technical novelty: (a) \"this paper does not present anything that is particular novel on top of the SSNT\" (b) not that \"conceptually different from the work of Tillmann et al\". \n - Impact: Reviewers were not completely convinced that this method could not work with simpler means. For instance by using clever reranking or utilizing the deep speech style unprincipled combination. However, this paper does produce a better approach for this problem."}, "review": {"Sk9xRdqwe": {"type": "rebuttal", "replyto": "Hymc1VSPl", "comment": "Hi Tamer,\n\nThanks for your comment. I've updated the related work section of our paper to include a discussion of the paper Alignment-Based Neural Machine Translation. The revised version of our paper should be on arXiv soon. I think the model in your proposed paper is more closely related to the direct model, i.e. the model presented in the EMNLP paper Online Segment to Segment Neural Transduction, where the conditional distribution is factored into the product of alignment and word probabilities due to the introduction of latent variable. In this paper, based on the noisy channel model, the conditional probability p(output sequence y| input sequence x) is decomposed into the channel model p(x | y) and the language model p(y) so that abundant unpaired data can be used.\n\nIn terms of the MT experiments, we haven't experimented on larger dataset yet, but that's in our TODO list. With my current implementation (without any tricks for speeding things up), it takes about 3 days to train the direct/channel model on the 184k sentence pairs with 1 GPU of Tesla K40m. I'm sure there's a big room of improvement regarding the training time, which can be achieved both from the algorithm side and the engineering side. We are working on this direction too. ", "title": "Response"}, "Hymc1VSPl": {"type": "rebuttal", "replyto": "SJ25-B5eg", "comment": "This is an interesting paper that investigates alternative neural machine translation approaches. Falling back to traditional concepts machine translation while using powerful neural models is essential to understanding the leap achieved by neural machine translation over conventional methods.\n\nIn terms of related work, there is a close relation to the paper Alignment-Based Neural Machine Translation (Alkhouli et al. 2016)*, whether in the decomposition of the model into neural alignment and  lexical/word probabilities, or using a weighted log-linear combination to combine the models which are eventually used in a standalone decoder. A notable difference is that the paper uses direct translation models (as opposed to the inverted ones proposed by the authors here), and the fact that the models are trained on word-aligned data which speeds up training. As for the alignment model, the former choses to model non-monotone source jumps whereas the current paper choses two classes (emit and shift) to model monotone alignments. In (Alkhouli et al. 2016), decoding is done using a beam search decoder that hypothesizes alignments and word translations\n\nAs for the MT experiments, did you try the model combination using larger data (i.e. >=100M tokens). How long did the current models take to train on the 184K sentence pairs you are using? It would also be interesting to see how much you gain/lose if the models were to be trained using given word alignments (e.g. obtained using GIZA++). Training then should be faster.\n\n* https://www.aclweb.org/anthology/W/W16/W16-2206.pdf", "title": "Comments on related work and MT experiments"}, "H1K_G0zIl": {"type": "rebuttal", "replyto": "Bk_9pwZrl", "comment": "Thanks for your review.\n\nWe will add the results for direct + LM + bias soon, but our preliminary experiments indicated this is a quite ineffective combination (in line with previous findings of Gulcehre et al., 2015).\n\nThe hyperparameters in Eq3 controls how much each model contributes to the score of output sequences. In the sentence compression task, increasing the weight for the channel model (\\lambda_1) and and the bias (\\lambda_4) results in higher recall and lower precision. Increasing the weights for the direct model (\\lambda_2) and language model (\\lambda_3) results in lower recall and higher precision. The bias for length is less sensitive than the others. On the heldout dataset with 1000 sentence pairs, ROUGE recall/precision increase/decrease by 1 point when \\lambda_4 is increased by 0.2. The ratio between \\lambda_1, \\lambda_2 and \\lambda_3 influences the performance significantly. The model behaves badly when the ratio, i.e. the difference between \\lambdas, is big. For example, keeping the other weights to 1, decrease \\lambda_3 from 1.0 to 0.8, the ROUGE-1 precision drops from ~33 to 30. If \\lambda_3 is set to 0.5, the ROUGE-1 precision drops even further reaching ~23. We simply use grid search to find the relatively optimal hyperparameters, with the range of [0.2, 0.5, 0.8, 1.0].\n\nThe performance is not sensitive to the beam size K1 and K2. Using a big K1 and K2 does not have much difference as using smaller ones. We did not tune these values in the experiments.\n\nIn general, our decoding algorithm shares the same search criterion as the work of Tillmann et al. (1997). There are a few subtle differences: 1) Without any pruning, as described in (Tillmann et al, 1997), the decoding algorithm has a complexity of I * J_max * |V|^2, where I is the length of the source sequence, and J_max is the maximum length of target sequence and |V| is the size of vocabulary. We reduce the complexity to I * I * J_max * |V| by introducing the auxiliary direct model to propose a constant number of candidate output sequences, followed by reranking with the noisy channel model. 2) In the work of Tillmann et al. (1997), they restrict the difference z_j - z_{j - 1} (jump size) to be no greater than 2 and use a bigram language model. By contrast, in our algorithm, we don\u2019t have this restriction.\n", "title": "Response"}, "H1-heCz8e": {"type": "rebuttal", "replyto": "Hyfg6dyNx", "comment": "Thanks for your review. We will add the results for direct + LM + bias soon.\nThe direct model in the paper refers to SSNT.", "title": "Response"}, "HJDfN3RHl": {"type": "rebuttal", "replyto": "SJ25-B5eg", "comment": "Hi authors,\n\nWe apologize for the delay. There are now 3 full reviews on this work. When you have an opportunity please enter in a rebuttal so that reviewers can discuss. ", "title": "Authors: Reviews now in, chance for rebuttal"}, "SJ7t8KmQe": {"type": "rebuttal", "replyto": "rJ6BqxXXe", "comment": "Thanks for your comment, we will edit the paper to make it clearer.\n\nIn terms of your questions --\nThe effect of length bias: smaller/no length bias tends to generate shorter output sequence. That results in higher precision and lower recall in ROUGE.\n\nThe alignment transition: Yes, the decoding algorithm runs frame-by-frame. It is consistent with the model defined in Section 2.1.\n\nThe function getCandidateOutputs: yes, it means to obtain partial candidate outputs via backtracing. We include this function in the pseudocode in order to present the algorithm clearly. In practice, these partial output sequences can be cached without backtracing.\n\n\n", "title": "Response"}, "BJd_Q_Q7e": {"type": "rebuttal", "replyto": "Hk9c-6hzg", "comment": "Thanks for your comment. In this work, we factorise p (output sequence y | input sequence x) into the product of a \"channel model\" p(x | y) and \"source model\" p(y), which is a language model in these experiments. We let the segment to segment neural transduction model (SSNT) of Yu et al. (EMNLP, 2016) parameterise p(x | y), the reverse transduction, and an LSTM language model parameterise p(y). Note that in order to serve as a channel model, the predicted token is required to condition on the incrementally constructed context, i.e. an unidirectional LSTM is used as the encoder of the channel model. In the Yu et al. work, p(y | x) was modelled directly without the two-part factorisation, and thus suffers from the explaining away problems we discuss in the introduction. A second difference is that in this work, we train SSNT and the language model separately, in many experiments using different data resources. For the channel model, the training procedure is the same as that described in Yu et al, 2016.\n\nFinally, we propose a novel decoding algorithm which is described in Section 3 in the paper. The pseudocode is provided in Appendix A. To briefly summarise the decoding algorithm, we employ the direct model to guide the beam search, and use the noisy channel model or a linear combination of the noisy channel model and the direct model to rerank the partial outputs in the beam.\n", "title": "Response to \"Difference from previous work\""}, "rJ6BqxXXe": {"type": "rebuttal", "replyto": "SJ25-B5eg", "comment": "After presenting everything as noisy channel (i.e. p(x|y)), it would be beneficial to also mention p(x|y) in the model description in section 2 and in the inference section 2.2.\n\nIn section 3, the presentation changes to using p(x|y), before more exactly introducing, how p(x|y) is modeled and trained.\nIt is not immediately clear, what is the auxiliary direct model q(y,z|x) - i.e. it is the model inferred in section 2.2. (== proposal model == decoding model).\n\nThe explaining paragraph in section 3: \"the search problem remains nontrivial [...] softmax over the input variables\",\ncould be made more clear by saying, that in the model p(x|y) we need to avoid conditioning on the full (output word) sequence y,\nwhich is possible by using only the forward LSTM on the outputs. \n\nThe sentence in section 3 is not complete: \"The top K1 partial output sequences.\"\n\nIn Tables 1/2, you should introduce the meaning of RG-1/RG-2/RG-L.\nWhat is the effect of leaving out/adding the length bias?\nbias(uni)/bias(bi) is misleading, it should be channel(bi)/direct(bi)\n\nThe alignment transition model (EMIT/SHIFT) is calculated frame-by-frame (end of section 2.1). Is also the decoding algorithm in appendix A running frame-by-frame? \n\nIn Appendix A, the function getCandidateOutputs is not clear. Does it mean a partial backtrace to obtain the full sequence y_1..j?", "title": "comments on readability"}, "Hk9c-6hzg": {"type": "review", "replyto": "SJ25-B5eg", "review": "Please can you explain how much it is different from your previous work in terms of the model and training algorithm, i.e. Yu et al 2016 Online segment to segment neural transduction, apart from the experiments?This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new. \n\nThe experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see \n\nDeep Speech 2: End-to-End Speech Recognition in English and Mandarin\n\nfrom Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are\n\n1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?\n\n2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences. \n\nThe paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public.\n", "title": "Difference from previous work", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hyfg6dyNx": {"type": "review", "replyto": "SJ25-B5eg", "review": "Please can you explain how much it is different from your previous work in terms of the model and training algorithm, i.e. Yu et al 2016 Online segment to segment neural transduction, apart from the experiments?This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new. \n\nThe experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see \n\nDeep Speech 2: End-to-End Speech Recognition in English and Mandarin\n\nfrom Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are\n\n1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?\n\n2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences. \n\nThe paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public.\n", "title": "Difference from previous work", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}