{"paper": {"title": "Rule Mining in Feature Space", "authors": ["Stefano Teso", "Andrea Passerini"], "authorids": ["teso@disi.unitn.it", "passerini@disi.unitn.it"], "summary": "We propose an algorithm to discover logical theories from relational embeddings of knowledge bases.", "abstract": "Relational embeddings have emerged as an excellent tool for inferring novel facts\nfrom partially observed knowledge bases. Recently, it was shown that some\nclasses of embeddings can also be exploited to perform a simplified form of rule\nmining. By interpreting logical conjunction as a form of composition between re-\nlation embeddings, simplified logical theories can be mined directly in the space\nof latent representations. In this paper, we present a method to mine full-fledged\nlogical theories, which are significantly more expressive, by casting the semantics\nof the logical operators to the space of the embeddings. In order to extract relevant\nrules in the space of relation compositions we borrow sparse reconstruction pro-\ncedures from the field of compressed sensing. Our empirical analysis showcases\nthe advantages of our approach.", "keywords": ["Unsupervised Learning"]}, "meta": {"decision": "Reject", "comment": "The reviewers concur that the paper is well written and the topic is interesting, but that the authors have not put sufficient effort into motivating their approach and evaluating it. The baselines seem too simple, the evaluation is incomplete. It is furthermore disappointing that the authors not only did not respond to the reviews, but did not respond to the pre-review questions. There is little in this review process that would support the paper being accepted, and therefore I concur with the reviewers' opinion and support rejection."}, "review": {"ByEXWCl7x": {"type": "review", "replyto": "H1_QSDqxl", "review": "Thank you for this interesting paper. The weighting of different paths for link prediction was also present in Lao et al. Random Walk Inference and Learning in A Large Scale Knowledge Base (EMNLP 11); the current paper proposes to learn from paths directly in embedding space, which may be much faster. I still have a few questions:\n\n1) The experimental section shows the relative performance of different rule miners, but there does not seem to be any reference with respect to the original algorithm (here, nonnegative RESCAL). How can we use these rules together with the original algorithm to perform link prediction? What performance gains can be expected? \n\n2) While the method to learn the paths seems applicable to many embedding-based algorithms for relational learning, I wonder what properties of the underlying algorithm are necessary for this principle to effectively work well. For instance, the authors use nonnegative RESCAL; to what extent is the non-negativity a critical feature for the approach to work?The paper presents a nice idea of directly finding rules such as brother(father) => uncle in knowledge bases, by directly searching in embedding space. The idea is to interpret the successive application of relationships as the multiplication of the relation-dependent matrices in non-negative RESCAL. \n\nThe experimental section provides an evaluation of the rules that are found by the algorithm. Nonetheless, the work seems only at its first stages for now, and many questions are left open:\n\n1) while the approach to find rules seems very general, the reason why it should work is unclear. What properties of the embedding space or of the initial algorithm are required for this approach to find meaningful rules? Can we apply the same principles to other algorithms than non-negative RESCAL?\n\n2) there is no real evaluation in terms of link prediction. How can we use these rules in conjunction with the original algorithm to improve link prediction? What performance gains can be expected? Can these rules find links that would not be found be the original algorithm in the first place?\n\n3) scaling: for now the number of parameters of the rule miner is (#relationships)^(max. path length + 1). How does this method scale on standard benchmarks such as FB15k where there is more than a 1000 of relationships?\n", "title": "questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1I4zQrEl": {"type": "review", "replyto": "H1_QSDqxl", "review": "Thank you for this interesting paper. The weighting of different paths for link prediction was also present in Lao et al. Random Walk Inference and Learning in A Large Scale Knowledge Base (EMNLP 11); the current paper proposes to learn from paths directly in embedding space, which may be much faster. I still have a few questions:\n\n1) The experimental section shows the relative performance of different rule miners, but there does not seem to be any reference with respect to the original algorithm (here, nonnegative RESCAL). How can we use these rules together with the original algorithm to perform link prediction? What performance gains can be expected? \n\n2) While the method to learn the paths seems applicable to many embedding-based algorithms for relational learning, I wonder what properties of the underlying algorithm are necessary for this principle to effectively work well. For instance, the authors use nonnegative RESCAL; to what extent is the non-negativity a critical feature for the approach to work?The paper presents a nice idea of directly finding rules such as brother(father) => uncle in knowledge bases, by directly searching in embedding space. The idea is to interpret the successive application of relationships as the multiplication of the relation-dependent matrices in non-negative RESCAL. \n\nThe experimental section provides an evaluation of the rules that are found by the algorithm. Nonetheless, the work seems only at its first stages for now, and many questions are left open:\n\n1) while the approach to find rules seems very general, the reason why it should work is unclear. What properties of the embedding space or of the initial algorithm are required for this approach to find meaningful rules? Can we apply the same principles to other algorithms than non-negative RESCAL?\n\n2) there is no real evaluation in terms of link prediction. How can we use these rules in conjunction with the original algorithm to improve link prediction? What performance gains can be expected? Can these rules find links that would not be found be the original algorithm in the first place?\n\n3) scaling: for now the number of parameters of the rule miner is (#relationships)^(max. path length + 1). How does this method scale on standard benchmarks such as FB15k where there is more than a 1000 of relationships?\n", "title": "questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkCkG0yme": {"type": "review", "replyto": "H1_QSDqxl", "review": "- Nations, Kinship and UMLS are standard relational learning benchmarks. But most previous works use area under the precision recall curve as metric. How does the proposed approach compare to previous work with this metric?\n- All datasets used in the experiments are rather small. How would FRM scale with larger datasets (high number of entities and/or of relation types and/or of triples)?This paper proposes a process to mine rules from vector space representations learned from KBs (using nonnegative RESCAL).\n\nThe paper is nicely written. \nBut its motivations are unclear: what is the underlying motivation to mine rules from embedding spaces?\n- If it is for better performance on link prediction then the paper does not show this. The experiments do not compare FRM against the performance of the original vector space model.\n- If it is for a better interpretability and debugging of the representations learned by vector space models, then there should have more elements on this in the paper.\n\nOther remarks:\n- The fact that the performance of the methods in Figure 1 and 2 are not compared to any baseline is problematic.\n- The scalability of the rule miner is a big drawback that should be addressed.\n- Figure 3 does not do a good job at convincing that rule based systems should be used for prediction or interpretation. The learned rules are bad for both cases.\n\n", "title": "Questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1Wo8pSEx": {"type": "review", "replyto": "H1_QSDqxl", "review": "- Nations, Kinship and UMLS are standard relational learning benchmarks. But most previous works use area under the precision recall curve as metric. How does the proposed approach compare to previous work with this metric?\n- All datasets used in the experiments are rather small. How would FRM scale with larger datasets (high number of entities and/or of relation types and/or of triples)?This paper proposes a process to mine rules from vector space representations learned from KBs (using nonnegative RESCAL).\n\nThe paper is nicely written. \nBut its motivations are unclear: what is the underlying motivation to mine rules from embedding spaces?\n- If it is for better performance on link prediction then the paper does not show this. The experiments do not compare FRM against the performance of the original vector space model.\n- If it is for a better interpretability and debugging of the representations learned by vector space models, then there should have more elements on this in the paper.\n\nOther remarks:\n- The fact that the performance of the methods in Figure 1 and 2 are not compared to any baseline is problematic.\n- The scalability of the rule miner is a big drawback that should be addressed.\n- Figure 3 does not do a good job at convincing that rule based systems should be used for prediction or interpretation. The learned rules are bad for both cases.\n\n", "title": "Questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}