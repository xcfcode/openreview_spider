{"paper": {"title": "Discriminator Rejection Sampling", "authors": ["Samaneh Azadi", "Catherine Olsson", "Trevor Darrell", "Ian Goodfellow", "Augustus Odena"], "authorids": ["sazadi@berkeley.edu", "catherio@google.com", "trevor@eecs.berkeley.edu", "goodfellow@google.com", "augustusodena@google.com"], "summary": "We use a GAN discriminator to perform an approximate rejection sampling scheme on the output of the GAN generator.", "abstract": "We propose a rejection sampling scheme using the discriminator of a GAN to\napproximately correct errors in the GAN generator distribution. We show that\nunder quite strict assumptions, this will allow us to recover the data distribution\nexactly. We then examine where those strict assumptions break down and design a\npractical algorithm\u2014called Discriminator Rejection Sampling (DRS)\u2014that can be\nused on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of\nGaussians and on the state of the art SAGAN model. On ImageNet, we train an\nimproved baseline that increases the best published Inception Score from 52.52 to\n62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use\nDRS to further improve on this baseline, improving the Inception Score to 76.08\nand the FID to 13.75.", "keywords": ["GANs", "rejection sampling"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a discriminator dependent rejection sampling scheme for improving the quality of samples from a trained GAN. The paper is clearly written, presents an interesting idea and the authors extended and improved the experimental analyses as suggested by the reviewers."}, "review": {"BkeHrKU1kE": {"type": "rebuttal", "replyto": "SklRqc0yTQ", "comment": "Please see this comment: https://openreview.net/forum?id=S1GkToR5tm&noteId=SyxH1nd7R7 or the updated PDF for experimental results on (what we think is) the simpler rejection scheme you mention. \n\nPlease also let us know if there's anything else you think we can do to improve the paper quality.", "title": "Re: simpler rejection scheme"}, "r1lSsOUk1E": {"type": "rebuttal", "replyto": "r1e5iSqf6X", "comment": "Thanks very much for the review, please see this comment: https://openreview.net/forum?id=S1GkToR5tm&noteId=SyxH1nd7R7 for some ablation experiments and comparisons with heuristic rejection schemes. \n\nLet us know if there's anything else you think we can do to improve the work.", "title": "Re: comparisons w/ heuristic rejection schemes"}, "BkgY088kJV": {"type": "rebuttal", "replyto": "Ske86fcY0Q", "comment": "Thanks for bringing [1] to our attention; we hadn't seen it.\nWe'll first summarize our understanding of the algorithm from [1] (which we'll call IR for 'Importance Resampling')\nand then we'll discuss differences.\n\nIR somehow computes importance weights for a set of samples using the Discriminator/Critic from a trained GAN.\nA single sample is drawn as follows:\nN samples from the trained GAN are prepared and importance weights are computed.\nA single one of the samples is then 'accepted' using a categorical distribution over N categories parameterized by the importance weights.\n\nThe differences (between IR and DRS and between our scientific evaluation and theirs) are:\n\n1. [1] don't continue to train D to approximate D^*.\nWe theoretically motivate the importance of this, and we also show (in the new experiments we ran for the rebuttal) that this is important empirically.\nThis difference may explain the small improvement given by IR (see below).\n\n2. [1] sample one image at a time given a set of N candidates instead of the probabilistic sampling as in DRS.\nThat is, their acceptance ratio is controlled by N.\nI don't think that this procedure will recover p_data given finite N?\nIt's hard to say for sure without knowing more detail about how they are getting the importance weights.\n\n3. We add the 'gamma trick', which you already noted is crucial to making the algorithm work in practice.\nImagine that the weights of n-1 samples are tiny (e.g. e-10) and the weight of one sample is close to 1.\nNormalizing all of the samples by \\sum{w_i} does not make any difference in the weights and thus, this importance re-sampling would not do much.\nThe 'gamma trick' changes the acceptance probabilities such that they cover the whole range of 0 to 1 scores.\nThis effect was also illustrated in Figure 2-A.\nThis results in a more efficient sampling scheme when acceptance probabilities for most of the samples are very small,\nwhich happened in our ImageNet experiment (purple histogram of Figure 2-A).\n\n4. [1] don't really provide evidence that IR yields quantitative improvement.\nIn the supplementary material, they show a single run on which the Inception score is changed from 7.28 to 7.42, an improvement of less than 2%.\nOur work shows that DRS yields improvements of (61.44 / 52.34 ~ 17%) and (76.08 / 62.36 ~ 22%) respectively on the baseline and improved\nversions of SAGAN[2] we used for experiments.\nApart from [3] (a concurrent submission to ICLR), these results are the best achieved in the literature.\nWe think it's reasonably to expect that DRS could improve the results from [3] as well.\n\n5. [1] seem to compare IR to a weak baseline in the experiment from the Supplementary Material.\nThis experiment is (presumably) conducted on the unsupervised CIFAR-10 task.\n7.42 is not only far from the state of the art at the time [1] was written (this is important because it gives evidence about whether IR can be 'stacked'\nwith other improvements), but it's less than the reported performance of the main method from [1], which is given as 7.47 +/- 0.10.\nThis is strange, because it suggests that the baseline for this experiment was not trained as well as the model in the main text (its performance of 7.28 is nearly\n2 standard deviations worse).\nFootnote 1 in the main text says 'We used a less well-trained model and picked our samples based on the importance weights to highlight the difference.',\nbut it's unclear if this was also intentionally done in the supplementary material.\n\n6. [1] don't compute the FID of the accepted samples, so there is no way to know if diversity has been sacrificed for sample quality.\nWe compute the FID and show that it has improved after DRS.\n\n7. [1] don't provide any theoretical analysis of IR.\n\n8. [1] don't include any illustrative toy experiments that suggest why resampling might work.\nWe propose and give support (using the mixture of gaussians experiment) for the hypothesis that it's easier for the\ndiscriminator to tell that certain regions of X are 'bad' than it is for the Generator to avoid spitting out samples in that region.\n\n\nPS:\nWe don't mean to be overly negative about [1].\nWe understand that IR was not the primary contribution of that work.\nWe just wish to emphasize the scope of the difference between the fraction of that work focusing on IR and our work.\n\nPPS:\nWe saw this message after the deadline to modify the PDF.\nWe will of course add this discussion to the final copy of the PDF when the time comes.\n\n[1] Chi-square generative adversarial network. In ICML, 2018.\n[2] Self-Attention GAN\n[3] Large Scale GAN Training for High Fidelity Natural Image Synthesis\n", "title": "Thanks - here's our take on [1]"}, "BJgZuI6m0m": {"type": "rebuttal", "replyto": "SyxH1nd7R7", "comment": "We have also added plots corresponding to the above values", "title": "The paper has been updated to reference these results"}, "SyxH1nd7R7": {"type": "rebuttal", "replyto": "S1GkToR5tm", "comment": "Reviewers 1 and 2 both mentioned that they would like to see comparisons to certain baselines. \nWe have now performed such comparisons.\nWe are working on adding them to the PDF, but I will discuss the results here in the meantime.\n\nWe evaluated 4 different rejection sampling schemes on the mixture-of-Gaussians dataset:\n\n(1) Always reject samples falling below a hard threshold and DO NOT train the Discriminator to 'convergence'.\n\n(2) Always reject samples falling below a hard threshold and train the Discriminator to convergence.\n\n(3) Use probabilistic sampling as in eq 8 and DO NOT train the Discriminator to convergence.\n\n(4) Our original DRS algorithm, in which we use probabilistic sampling and train the Discriminator to convergence.\n\nIn (1) and (2), we were careful to set the hard threshold so that the actual acceptance rate was the same as in (3) and (4).\n\nBroadly speaking:\n4 performs best\n3 performs OK but yields less 'good samples' than 4.\n2 yields the same number of 'good samples' as 3, but completely fails to sample from 5 of the 25 modes.\n1 actually yields the most 'good samples' for the modes it hits, but it only hits 4 modes!\n\nThese results show that\na) continuing to train D so that it can approximate D^* (which we have already motivated theoretically) is helpful in practice. \nb) performing sampling as in eq 8 (which we also motivated theoretically) is helpful in practice. \n\nBelow we provide, for each method, the number of samples within 1, 2, 3 and 4 std deviations and the number of modes hit.\nFor reference, we also compute these statistics for the ground truth distribution and the unfiltered samples from the GAN.\n\nWe would have liked to perform the same analysis on SAGAN, but we currently don't have access to resources that would\nallow us to do this before the response deadline.\n\nDRS ABLATION STUDY\nGROUND TRUTH\nCentroid coverage: 25\nwithin 1 std: 0.3934\nwithin 2 std: 0.8661\nwithin 3 std: 0.9891\nwithin 4 std: 0.9999\nVANILLA GAN\nCentroid coverage: 25\nwithin 1 std: 0.273\nwithin 2 std: 0.5308\nwithin 3 std: 0.6615\nwithin 4 std: 0.7561\n(1) THRESHOLD NO FT\nCentroid coverage: 4\nwithin 1 std: 0.3849\nwithin 2 std: 0.9255\nwithin 3 std: 0.9944\nwithin 4 std: 0.9982\nTHRESHOLD\n(2) Centroid coverage: 20\nwithin 1 std: 0.3478\nwithin 2 std: 0.7023\nwithin 3 std: 0.8359\nwithin 4 std: 0.8928\n(3) DRS NO FT\nCentroid coverage: 25\nwithin 1 std: 0.314962934062\nwithin 2 std: 0.601736246586\nwithin 3 std: 0.73585641826\nwithin 4 std: 0.811841591885\n(4) DRS\nCentroid coverage: 25\nwithin 1 std: 0.35277582572\nwithin 2 std: 0.657589599438\nwithin 3 std: 0.817463106114\nwithin 4 std: 0.897487702038\n", "title": "We have run the requested comparisons"}, "r1e5iSqf6X": {"type": "review", "replyto": "S1GkToR5tm", "review": "This paper proposes a rejection sampling algorithm for sampling from the GAN generator. Authors establish a very clear connection between the optimal GAN discriminator and the rejection sampling acceptance probability. Then they explain very clearly that in practice the connection is not exact, and propose a practical algorithm. \n\nExperimental results suggest that the proposed algorithm helps the increase the accuracy of the generator, measured in terms of inception score and Frechet inception distance. \n\nIt would be interesting though to see if the proposed algorithm buys anything over a trivial rejection scheme such as looking at the discriminator values and rejecting the samples if they fall below a certain threshold. This being said, I do understand that the proposed practical acceptance ratio in equation (8) is 'close' to the theoretically justified acceptance ratio. Since in practice the learnt discriminator is not exactly the ideal discriminator D*(x), I think it is super okay to add a constant and optimize it on a validation set. (Equation (7) is off anyways since in practice the things (e.g. the discriminator) are not ideal). But again, I do think it would make the paper much stronger to compare equation (8) with some other heuristic based rejection schemes.\n\n ", "title": "Good paper!", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1gYHA1-a7": {"type": "rebuttal", "replyto": "S1GkToR5tm", "comment": "We have written individual replies to Reviews 2 and 3 (these are the only reviews at present). \n\nWe have also update the PDF to include a new figure (fig 6) on the effect of gamma. \n\nWe are working on making more updates to the draft for purposes of clarity.", "title": "FYI"}, "SkeXZRk-Tm": {"type": "rebuttal", "replyto": "rJgNPOjOnX", "comment": "We thank the reviewer for his/her time and feedback. We appreciate the kind words relating to the clarity and comprehensiveness of our submission, and hope to address any remaining concerns the reviewer has here.\n\nOTHER APPLICATIONS:\n (a) Suppose we\u2019re designing molecules for drug discovery purposes using a generative model. \nAt some point, we will have to physically test the molecules that we have designed, which could be costly.\n If the discriminator can throw out some obviously unrealistic molecule designs, this will save us money and time.\n(b) For text generation applications, a nonsensical generated sentence in a dialog system could be rejected by the discriminator, reducing the frequency of embarrassing mistakes. \n(c) In RL applications, if we are predicting future states with a generative model, we could use this technique to throw out silly predictions, reducing the risk of taking a silly action predicated on those predictions. \n(d) More generally, you could use DRS on models that are not GANs.\n\nADDRESSING D* ISSUE\n\nYou\u2019re right about this - we will change the wording. We don\u2019t do anything to *fix* the problem that we can\u2019t actually compute D*, we just show that you don\u2019t need to precisely recover D* to get good results. The first paragraph on page 5 speculates on why this might be so, and figures 4 and 5 provide evidence for this speculation.\n\nREGARDING GAMMA:\n\nWe agree that gamma is an important hyperparameter, because it modulates the acceptance rate. \nWe have already made the figure you propose and have updated the PDF to include it. It is now figure 6. \nPlease let us know if there are other experiments that you think would\nimprove the quality of the work.\n", "title": "Thanks for the review!"}, "SylUYa1bpQ": {"type": "rebuttal", "replyto": "SklRqc0yTQ", "comment": "Thanks very much for the review. \nWe think that there have been two misunderstandings here, one about the Gaussian Mixture experiment and one about the purpose of the quantity F_hat(x).\nThese are our fault; we should have made the paper more clear and we are modifying the draft to do so.\nIn the meantime, we will address both issues here. We use > for quotes. \n\nGAUSSIAN MIXTURE EXPERIMENT:\n> - GAN setting: 10K examples are generated and reported in figure 3?\nThis much is true.\n\n\n> - DRS setting: 10K examples are generated, and submitted to algorithm in figure 1. For each batch, a line search sets gamma so that 95% of the examples are accepted. Thus only 9.5K are reported in figure 3.\nThis part is not true.\nYou probably got confused by the line 'We generate 10,000 samples from the generator with and without DRS.' which we agree is unclear. \n\nFirst, we generate as many samples as needed to yield 10K acceptances, so both plots have 10k dots on them.\n\nSecond, there is no line search.\nEach example is given an acceptance probability p that is generated from substituting F_hat from equation 8 for F in equation 6.\nThen, a pseudo-random number in [0,1] is compared with p to determine acceptance. \nThus, for any given batch, the number of examples accepted is non-deterministic.\nWe think that this point also relates to the misunderstanding regarding the purpose of F_hat.\n\nThird, gamma is subtracted from F.\nSo setting gamma equal to the 95th %-ile value of F means that an example where F(x) is at the 95th %-ile will have a 50% chance of being accepted, because\n1 / (1 + e^(-F_hat(x))) = 1 / (1 + e^0) = 1 / 2 in this case. \nThe result is that around 23% of samples drawn from the generator made it into the final DRS plot, which means we had to draw a little less than 50k samples from the generator. \n\n\n> If this is my understanding, then the comparison in Figure 3 in unfair, as DRS is allowed to pick and choose.\nWe're unsure what you mean here.\nIt's true in some sense that DRS is allowed to pick and choose, but from our perspective this is part of the definition of rejection sampling?\nThe generator can't figure out how to stop yielding bad samples, but the discriminator can tell which samples are bad, so we can\nthrow those out and get a distribution closer to the ground truth distribution at the cost of having to generate extra samples from the generator.\n\n\nPURPOSE OF F_HAT:\n> Let's jump to equation (8): compared to a simple use of the discriminator for rejection, it adds the term under the log\nWe don't think this is correct - the log already exists and we just add the gamma and epsilon terms.\nThe discussion after eq 5 shows that the acceptance probability p(x) is exp(D_tilde^*(x) - D_tilde^*(x^*)).\nThe tildes are important, because they mean that we are operating not on the sigmoid output of D but on the logit that is passed to the sigmoid output.\nThen we ask what F(x) would have to be s.t. 1 / (1 + e^(-F(x))) = p(x).\nThis results in equation 7, *which already has the log term*.\nThe only difference between F_hat and F is that we introduce the epsilon for numerical stability and the gamma to modulate the acceptance probability.\n\n> First order Taylor expansion of...\nWhat you say here is true, but we are not thresholding. \nWe think this is the root of the misunderstanding.\nWe don't consider the hard thresholding algorithm here because it might deterministically reject certain samples for which D^* is low,\nwhich means that we would never be able to actually draw samples from p_d, even in the idealized setting of section 3.1\n\nPlease let us know if this response answers all of your questions. \nWe are happy to expand.\n", "title": "Thanks for the review!"}, "SklRqc0yTQ": {"type": "review", "replyto": "S1GkToR5tm", "review": "his paper assumes that, in a GAN, the generator is not perfect and some information is left in the discriminator, so that it can be used to 'reject' some of the 'fake' examples produced by the generator.\n\nThe introduction, problem statement and justification for rejection sampling are excellent, with a level of clarity that makes it understandable by non expert readers, and a wittiness that makes the paper fun to read. I assume this work is novel: the reviewer is more an expert in rejection than in GANs, and is aware how few publications rely on rejection.\n\nHowever, the authors fail to compare their algorithm to a much simpler rejection scheme, and a revised version should discuss this issue.\nLet's jump to equation (8): compared to a simple use of the dicriminator for rejection, it adds the term under the log.\nThe basic rejection equation would read F(x) = D*(x) - gamma and one would adjust the threshold gamma to obtain the desired operating point. I am wondering why no comparison is provided with basic rejection? \n\nLet me try to understand the Gaussian mixture experiment, as the description is ambiguous:\n- GAN setting: 10K examples are generated and reported in figure 3?\n- DRS setting: 10K examples are generated, and submitted to algorithm in figure 1. For each batch, a line search sets gamma so that 95% of the examples are accepted. Thus only 9.5K are reported in figure 3.\n- What about basic rejection using F(x) = D*(x) - gamma: how does it compare to DRS at the same 95% accept?\n\nIf this is my understanding, then the comparison in Figure 3 in unfair, as DRS is allowed to pick and choose.\nFor completeness, basic rejection should also be added.\n\nGoing back to Eq.(8), one realizes that the difference between DRS rejection and basic rejection may be negligible.\nFirst order Taylor expansion of log(1-x) that would apply to the case where the rejection probability is small yields:\nF(x) = (D*(x) - D*_M) + exp(D*(x) - D*_M) \n\nx+ exp(x) is monotonous, so thresholding over it is the same as thresholding over x: back to basic rejection!", "title": "Very well written paper, with excellent results, but experiments may be unfair, and a much simpler rejection scheme may work equally well.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJgNPOjOnX": {"type": "review", "replyto": "S1GkToR5tm", "review": "This paper proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling (DRS), to help filter \u2018good\u2019 samples from GANs\u2019 generator. More specifically, after training GANs\u2019 generator and discriminator are fixed; GANs\u2019 discriminator is further exploited to design a rejection sampler, which is used to reject the \u2018bad\u2019 samples generated from the fixed generator; accordingly, the accepted generated samples have good quality (better IS and FID results). Experiments of SAGAN model on GMM toys and ImageNet dataset show that DRS helps further increases the IS and reduces the FID.\n\nThe paper is easy to follow, and the experimental results are convincing. However, I am curious about the follow questions.\n\n(1)\tBesides helping generate better samples, could you list several other applications where the proposed technique is useful? \n\n(2)\tIn the last paragraph of Page 4, I don\u2019t think the presented Discriminator Rejection Sampling \u201caddresses\u201d the issues in Sec 3.2, especially the first paragraph of Page 5.\n\n(3)\tThe hyperparameter gamma in Eq. (8) is of vital importance for the proposed DRS. Actually, it is believed the key to determining whether DRS works or not. Detailed analysis/experiments about hyperparameter gamma are considered missing. \n", "title": "A post-processing method to filter \u2018good\u2019 generated samples for GANs ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}