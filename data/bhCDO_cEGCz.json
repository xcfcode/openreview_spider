{"paper": {"title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning", "authors": ["Zhenfang Chen", "Jiayuan Mao", "Jiajun Wu", "Kwan-Yee Kenneth Wong", "Joshua B. Tenenbaum", "Chuang Gan"], "authorids": ["~Zhenfang_Chen1", "~Jiayuan_Mao1", "~Jiajun_Wu1", "~Kwan-Yee_Kenneth_Wong2", "~Joshua_B._Tenenbaum1", "~Chuang_Gan1"], "summary": "We propose a neural-symbolic framework to learn physical concepts of objects and events via causal reasoning on videos.", "abstract": "We study the problem of dynamic visual reasoning on raw videos. This is a challenging problem; currently, state-of-the-art models often require dense supervision on physical object properties and events from simulation, which are impractical to obtain in real life. In this paper, we present the Dynamic Concept Learner (DCL), a unified framework that grounds physical objects and events from video and language. DCL first adopts a trajectory extractor to track each object over time and to represent it as a latent, object-centric feature vector. Building upon this object-centric representation, DCL learns to approximate the dynamic interaction among objects using graph networks. DCL further incorporates a semantic parser to parse question into semantic programs and, finally, a program executor to run the program to answer the question, levering the learned dynamics model. After training, DCL can detect and associate objects across the frames, ground visual properties and physical events, understand the causal relationship between events, make future and counterfactual predictions, and leverage these extracted presentations for answering queries. DCL achieves state-of-the-art performance on CLEVRER, a challenging causal video reasoning dataset, even without using ground-truth attributes and collision labels from simulations for training. We further test DCL on a newly proposed video-retrieval and event localization dataset derived from CLEVRER, showing its strong generalization capacity.", "keywords": ["Concept Learning", "Neuro-Symbolic Learning", "Video Reasoning", "Visual Reasoning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper received 4 reviews with mixed initial ratings: 6,7,7,5. The main concerns of R1, who gave an unfavorable score, included lack of clarity (the manuscript is hard to follow) and limited empirical evaluation (the method is tested on a single synthetic dataset, CLEVRER). The latter point is echoed in other reviews as well. In response to that, the authors submitted a new revision and provided detailed responses to each of the reviews separately, which seemed to have addressed these concerns. R1 upgraded the rating and recommended acceptance.\nAs a result, the final recommendation is to accept this submission for presentation at ICLR as a poster."}, "review": {"p6m6x7IUmVW": {"type": "review", "replyto": "bhCDO_cEGCz", "review": "Summary:\n\nThe paper presents a neural-symbolic approach for video question answering. In particular, the authors propose the Dynamic Concept Learner (DCL), a network architecture to localize object trajectories over time and further build a graph network to model the physical interaction between objects and events. Finally, a standard symbolic executor is used to reason across visual and language cues for question answering. The proposed method is evaluated on CLEVRER dataset in which it achieves new state of the art performance.\n\nPros:\n\n(1) The main contribution of this paper is that it is able to ground object and event concepts to model the dynamic scene of the interactions between objects/events without the use of extra supervision. This is well motivated as we do not always have access to explicit labels such as object attributes, masks, and collision events in real-world applications.\n\n(2) The idea of dynamics prediction to support predictive and counterfactual questions is new. This makes the proposed framework as a whole interesting IMHO.\n\n(3) The proposed method sets new state of the art performance on CLEVRER dataset in all question types. On video grounding and video-text retrieval tasks, DCL seems to achieve interesting results.\n\nCons:\n\nDespite the pros above, I have some comments on the paper:\n\n(1) I found the paper is quite hard to follow. Figure 2 and its description represent the process of DCL (object trajectory detector -> dynamic predictor -> feature extractor -> symbolic executor). However, the multi-step training paradigm in Sec. 1, 3.1, and 3.2 seems to follow a different structure. In addition, some details are missing in the paper. For example, in the \u2018Grounding Object and Event Concepts\u2019 subsection, where did you get the vector embedding for the concept \"moving\" (s_moving) from?\n\n(2) The motivation in the subsection \u2018Trajectory refinement\u2019 is exactly as what explained in DeepSORT [1] \u2013 a common tracking algorithm. In addition, I believe the use of Kalman Filters in DeepSORT makes it better in handling occlusion than the Eq. 2. I suggest the author discuss the object tracking literature for better judgments. \n\n(3) One of the main weaknesses of this paper is they only evaluate their method on one synthetic dataset which is CLEVRER. It seems to me that the proposed method is particularly designed to solve this dataset. While I find the results interesting, I think physical interactions in those synthetic data are too simple and do not reflect much what happens in real-world data. Not to mention the limit in terms of linguistic variants of those synthetic datasets compared to human natural language. I would appreciate if the author can evaluate the proposed model on other datasets such as CATER [2] for temporal reasoning and Video QA datasets (TGIF-QA [3], TVQA [4], etc).\n\n(4) Similar to my concerns in (3), it would be more convincing to see if the DCL works on video grounding datasets such as ActivityNet-Entities or YouCook2, MSR-VTT for video-text retrieval to support their claims on the generalization of the method. \n\nI am willing to raise the rating if the author could properly address my concerns.\n\nReferences:\n- [1] Wojke, Nicolai, Alex Bewley, and Dietrich Paulus. \"Simple online and realtime tracking with a deep association metric.\" 2017 IEEE international conference on image processing (ICIP). IEEE, 2017.\n- [2] Girdhar, Rohit, and Deva Ramanan. \"Cater: A diagnostic dataset for compositional actions and temporal reasoning.\" arXiv preprint arXiv:1910.04744 (2019).\n- [3] Jang, Yunseok, et al. \"Tgif-qa: Toward spatio-temporal reasoning in visual question answering.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n- [4] Lei, J., Yu, L., Bansal, M., & Berg, T. L. (2018). Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696.\n", "title": "I find the method is interesting but the paper could be improved on its current form", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "xvt9B7jQdzU": {"type": "rebuttal", "replyto": "qPHZB0LrulM", "comment": "Dear Reviewer 4,\n\nThanks again for your constructive comments.  We have made substantial changes in the revision according to your review. In particular, we have the following changes according to your comments. 1). We have added new experiments on a real-world physical dataset [1] to further show DCL\u2019s effectiveness and generalization capacity in (Sec. 4.5, Table 5 and Table 6). 2). We have provided qualitative visual examples and failure cases for concept learning in a new website (https://dynamicconceptlearner.github.io) to show its interpretability and effectiveness. 3. We have revised the paper carefully, including providing more training and inference details (Sec. 3.2 and appendix H), analysis of the differences between action videos and physical videos (Sec. 2), and grammar checking.\n\nAs the discussion period is about to end, please don\u2019t hesitate to let us know if there are any additional clarifications that we can offer, as we would love to convince you of the merits of the paper. Thanks!\n\n[1] Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. ICML, 2016.", "title": "[Revision Updated] look forward to your feedback!"}, "d3zXdnRRIs": {"type": "rebuttal", "replyto": "lTqMTgNROIR", "comment": "Dear Reviewer 3,\n\nThanks again for your detailed and positive comments.\nWe have made substantial changes in the revision according to your review.\nIn particular, we have the following changes according to your comments,\n\n- We have added new experiments on a real-world physical dataset [1] to further show DCL\u2019s effectiveness and generalization capacity in (Sec. 4.5, Table 5 and Table 6).\n- We have revised the paper carefully, including providing more training and inference details (Sec. 3.2 and appendix H), analysis of the differences between action videos and physical videos (Sec. 2) and model limitations (Sec. 5).\n- We have provided qualitative visual examples and failure cases for concept learning in a new website (https://dynamicconceptlearner.github.io) to show its interpretability and effectiveness.\n- We will release our code, data and models as soon as possible.\n\nAs the discussion period is about to end, please don\u2019t hesitate to let us know if there are any additional clarifications that we can offer, as we would love to convince you of the merits of the paper. Thanks!\n\n[1] Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. ICML, 2016.", "title": "[Revision Updated] look forward to your feedback!"}, "Lzi88VdwSss": {"type": "rebuttal", "replyto": "VtiRgTG8etn", "comment": "Dear Reviewer 2,\n\nThanks again for your detailed and constructive comments.\nWe have made substantial changes in the revision according to your review.\nIn particular, we have the following changes according to your comments,\n\n- We have added new experiments on a real-world physical dataset [1] to further show DCL\u2019s effectiveness and generalization capacity in (Sec. 4.5, Table 5 and Table 6).\n- We have implemented some more recent baseline methods [2,3] to CLEVRER-QA (Sec. 4.2 and Table 1) and CLEVRER-Retrieval (Sec. 4.4 and Table 4) and analyze their performance.\n- We have revised the paper carefully, including providing explicit training loss functions (Sec. 3.2 and appendix H), analysis of the differences between action videos and physical videos (Sec. 2) and model limitations (Sec. 5).\n- We have provided qualitative visual examples and failure cases for concept learning in a new website (https://dynamicconceptlearner.github.io) to show its interpretability and effectiveness.\n\nAs the discussion period is about to end, please don\u2019t hesitate to let us know if there are any additional clarifications that we can offer, as we would love to convince you of the merits of the paper. Thanks!\n\n[1] Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. ICML, 2016.", "title": "[revision updated] look forward to your feedback"}, "FUT5bZ637xI": {"type": "rebuttal", "replyto": "LfQ0igMrur", "comment": "Dear Reviewer 1,\n\nThanks again for your constructive review, which has helped us improved the quality and clarity of the paper. In addition to our response above, in the revision, we have added new experiments on a real-world physical dataset and also included comparisons with additional baselines.  The visualization of our model's results could be found on the website (https://dynamicconceptlearner.github.io). \n\nAs the discussion period is about to end, please don\u2019t hesitate to let us know if there are any additional clarifications that we can offer, as we would love to convince you of the merits of the paper. We appreciate your suggestions. Thanks!\n\n", "title": "Look foward to your response!"}, "QJhvnFKcJCs": {"type": "rebuttal", "replyto": "p6m6x7IUmVW", "comment": "Thanks again for your constructive comments. We have made substantial changes in the revision according to your review. Specifically, we have the following changes according to your comments,\n\n- We have added new experiments on a real-world physical dataset [1] to further show DCL\u2019s effectiveness and generalization capacity in (Sec. 4.5, Table 5 and Table 6).\n- We have added a new tracker using Kalman Filtering and analyzed its performance in Appendix F and Table 9.\n- We have provided qualitative visual examples and failure cases for concept learning in a new website (https://dynamicconceptlearner.github.io) to show its interpretability and effectiveness.\n- We have revised the paper carefully, including providing more training and inference details (Sec. 3.2 and appendix H), analysis of the differences between action videos and physical videos (Sec. 2), model limitations (Sec. 5), content adjustment (Sec. 4.5), and grammar checking.\n\nAs the discussion period is about to end, please don\u2019t hesitate to let us know if there are any additional clarifications that we can offer, as we would love to convince you of the merits of the paper. Thanks!\n\n[1] Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. ICML, 2016.", "title": "Follow up: Revision Updated."}, "PkDA9DjSETy": {"type": "rebuttal", "replyto": "bhCDO_cEGCz", "comment": "We would like to thank the reviewers for their thoughtful feedback. We are glad to see that reviewers generally appreciated our paper's contributions and provide constructive and detailed comments for a better revision.\n\nWe would like to emphasize again that our primary goal in this paper is to design a unified framework for learning physical object and event concepts (e.g., collision, falling, stability) from videos. These tasks are of great importance in practical applications such as industrial robot manipulation, which requires AI systems with human-like physical common sense. \n\nWe have revised our manuscript to include the following changes:\n\n- We have added new experiments on a real-world physical dataset [1] to further show DCL\u2019s effectiveness and generalization capacity in Sec. 4.5. (R1, R2, R3, R4).\n- We have implemented some more recent baseline methods [2,3] to CLEVRER-QA (Sec. 4.2 and Table 1)  and CLEVRER-Retrieval (Sec. 4.4 and Table 4)  and analyze their performance. (R2).\n- We have added a new tracker using Kalman Filtering and analyzed its performance in Appendix F and Table 9. (R1)\n- We have revised the paper carefully, including providing more training and inference details (Sec. 3.2 and appendix H), analysis of the differences between action videos and physical videos (Sec. 2), model limitations (Sec. 5), content adjustment (Sec. 4.5), and grammar checking. (R1, R2, R3, R4)\n- We have provided qualitative visual examples and failure cases for concept learning in a new website (https://dynamicconceptlearner.github.io) to show its interpretability and effectiveness. (R1, R2, R3, R4)  \n\nPlease don\u2019t hesitate to let us know of any additional comments on the manuscript or the changes.\n\n\n[1] Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. ICML, 2016.\n\n[2] Thao Minh Le and others. Hierarchical Conditional Relation Networks for Video Question Answering. CVPR, 2020.\n\n[3] Shizhe Chen and others. Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning. CVPR, 2020.", "title": "General Response: Revision Updated"}, "f28reHUFXpF": {"type": "rebuttal", "replyto": "qPHZB0LrulM", "comment": "Thank you very much for your constructive comments and suggestions.\n\n> About the iterative process for trajectory refinement and model learning.\n\nWe adopt a multi-step training paradigm to refine the object trajectories and model learning.\n- Step 1: we simply associate frame object proposals based on object locations  (see Eq. (1)) and get coarse object trajectories.\n- Step 2: we learn static concepts based on the coarse object trajectories and descriptive and explanatory question-answer pairs.\n- Step 3: we use the learned static concepts to refine the object trajectories (see Eq. (2) and Eq. (3)).\n- Step 4: we optimize the full model based on the refined trajectories and all the question-answer pairs.\n\nTheoretically, Step 2 -> Step 3 can be optimized in an iterative fashion. Practically, we only use one-round iteration since we find it sufficient enough to learn the static concepts and the trajectory quality stops increasing.\n\n> About the RGB patches and their combination with the locations in dynamic predictions.\n\nTo generate the predictive scenes, we first predict the RGB patches and the location of each object with a Propagation Net.\nWe then paste RGB patches back into the predictive locations of the background image for future and counterfactual predictions.\n\nAn RGB patch $R^{3 \\times N_p \\times N_p}$ is a cropped patch centering at the centroid of the target object, where $N\\_p$ is the cropped size. We concatenate the RGB patches with the location coordinates by expanding the location coordinates to each pixel of the RGB patch and get a feature of shape $R^{7 \\times N_p \\times N_p}$ for the Propagation Net. Such a concatenation is also adopted in the previous work NS-DR[1]. We will provide more details on the implementation of Propagation in the revised version.\n\n[1] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. CLEVRER: Collision events for video representation and reasoning. ICLR, 2020.\n\n> About pre-trained object recognizer and attribute classier for pseudo labeling.\n\nOur DCL model doesn't require any object recognizer or attribute classier during training.\nDCL learns to ground and quantize the object attributes by watching videos and answering questions.\nDCL-Oracle shows DCL's performance by adding additional object and event labels.\nWe estimate the performance of concept quantization in Table 2 by comparing the predicted concepts with the concept labels provided by the official annotation.\n\n> About the generalization experiments.\n\nIn the generalization experiments, the models are trained and tested using the same domain dataset, i.e., the videos in the original CLEVRER-QA dataset and new expressions for grounding and retrieval.\nWe claim that DCL has a strong generalization capacity since the learned physical object and event concepts can be applied to new applications, including video retrieval and event localization.\nAs stated in the common concerns, we will add new experiments on real videos and a new concept to further show DCL's effectiveness and generalization in the revised version.\nThe data samples using in our model and baselines are the same. In practice, we find that our model enjoys better data efficiency than MAC.\n\n> About \u201cto show the learned concepts visually as one of the strengths of is explainability\u201d.\n\nWe will provide qualitative visual examples and failure cases for concept learning in a new website to show its interpretability and effectiveness.\n\n", "title": "Response to AnonReviewer4"}, "RB1R0Z69ryu": {"type": "rebuttal", "replyto": "bhCDO_cEGCz", "comment": "We thank all reviewers for their constructive comments.  In addition to the specific response below, we summarize our goals, address some common concerns, and describe the changes to be included in the revision.\n\n### Our Goal \n\nWe propose a unified framework to learn physical object and event concepts through dynamic visual reasoning on videos. With these learned concepts, our model can be applied to various problems, including VQA (Sec. 4.2), video grounding (Sec. 4.4), and video retrieval (Sec. 4.4). \n\n### Common Concerns\n\n> Experiments on real videos and new concepts.\n\nReviewers have raised concerns about DCL\u2019s performance on real videos. We agree that adding new experiments on real-world videos can further prove DCL\u2019s effectiveness and generalization. We are planning to add experiments on a real-world physical video dataset [1] to learn physical dynamic concepts on real videos.\n\n> Differences between physical event understanding, and understanding activities or other video understanding problems\n\nWe would like to clarify that the primary goal of our work is to design a unified framework for learning physical object and event concepts (e.g., collision, falling, stability) from videos. These tasks are of great importance in practical applications such as industrial robot manipulation, which requires AI systems with human-like physical common sense. Datasets like ActivityNet, YouCook2, and MSVD-QA focus on other problems of video understanding, such as classifying human actions (e.g., picking up a glass, opening a door), and understanding activities that one or more people engage in over extended periods of time (e.g., running, cooking, cleaning). We agree that understanding actions and activities in the video are important, fundamental research problems in computer vision, but these are not the same research questions we aim to study in this work.  We do believe that the methods we develop should be useful for more general video understanding, but that remains future work.  We also think further work will be needed to build models of intentional action for human-activity event understanding, to complement the physics models we build here for physical event understanding.\n\n### Planned Changes\n\n- We will add new experiments on a real-world physical dataset [1] to further show DCL\u2019s effectiveness and generalization capacity.\n- We will implement some more recent baseline methods [2,3] to CLEVRER-QA and CLEVRER-Retrieval and analyze their performance. \n- We will add a new tracker using Kalman Filtering and analyze its performance.\n- We will revise the paper carefully, including providing more training and inference details, analysis of the differences between - action videos and physical videos, sample complexity, model limitations, content adjustment, and grammar checking.\n- We will provide qualitative visual examples and failure cases for concept learning in a new website to show its interpretability and effectiveness.\n\n[1] Adam Lerer, Sam Gross, and Rob Fergus.  Learning physical intuition of block towers by example.  ICML, 2016.\n\n[2] Thao Minh Le and others. Hierarchical Conditional Relation Networks for Video Question Answering. CVPR, 2020.\n\n[3] Shizhe Chen and others. Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning. CVPR, 2020.\n\nPlease don\u2019t hesitate to let us know of any additional comments on the paper or on the planned changes.\n", "title": "[Pre-revision] General Response to all Reviewers "}, "pRl751VJEIn": {"type": "rebuttal", "replyto": "bhCDO_cEGCz", "comment": "Thank you for your detailed feedback. We have updated the individual response to each of your reviews under your thread. We will update the revision soon. Please let us know if you have further questions.", "title": "Pre-revision Individual Response Updated "}, "5c8K9ptPw-R": {"type": "rebuttal", "replyto": "lTqMTgNROIR", "comment": "Thanks for your sincere and constructive comments.\n\n> About the limitation of the current DCL model, performance on counterfactual questions, and common failure cases.\n\nTo achieve high QA accuracy in CLEVRER, a model mainly requires two kinds of abilities: 1). physical object and event concept learning, and 2). physical dynamic prediction.\nWhile our DCL can effectively ground object and event concepts (see Table 2), it still requires a stronger ability for long-term video dynamic prediction to achieve higher accuracy on counterfactual questions.\nNote that long-term dynamic video prediction [1,2] has always been a difficult research topic.\n\nWe will show some typical failure cases for counterfactual questions in the revised version.\n\n[1]. Li, Yunzhu, et al. \"Propagation networks for model-based control under\npartial observation.\" ICRA, 2019.\n\n[2]. Qi, Haozhi, et al. \"Learning Long-term Visual Dynamics with Region Proposal Interaction Networks.\" arXiv preprint, 2020.\n\n> About the small performance difference between DCL and DCL-Oracle.\n\nCompared with DCL, DCL-Oracle uses additional supervision for physical object and event concepts during training.\nDCL can effectively ground physical object and event concepts through only question answering and achieve high concept recognition accuracy (see Table 2). Thus, the bottleneck for the QA accuracy is not the DCL's ability to ground physical concepts but to learn physical dynamics.\nSince both DCL and DCL-Oracle use PropNet[1] as the dynamic prediction model, the performance gap is small.\n\n[1]. Li, Yunzhu, et al. \"Propagation networks for model-based control under\npartial observation.\" ICRA, 2019.\n\n> About more experiments on real-world videos\n\nWe will conduct new experiments on real videos[1] to show DCL\u2019s effectiveness and generalization.\n\n[1]. Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. ICML, 2016.\n\n> About code and data.\n\nWe will make our code, models and the new datasets available as soon as possible.\n\n> About typos and reliance on the supplemental material.\n\nThanks for the advice on writing. We will fix the grammar typos and make use of the additional page during the rebuttal for a better revised paper.", "title": "Response to AnonReviewer3"}, "cXH7K2ItmuZ": {"type": "rebuttal", "replyto": "VtiRgTG8etn", "comment": "Thanks for your detailed and constructive comments.\n\n> About experiments on real-world datasets.\n\nWe agree that evaluating DCL on more datasets would further demonstrate its effectiveness across different visual scenes.\nWe will conduct experiments on a real physical dataset[1] to show the model's effectiveness and generalization.\n\n[1]. Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. ICML, 2016.\n\n> About more recent baselines on CLEVRER-QA.\n\nWe will apply a new recent VQA baseline HCRN to the CLEVRER and analyze its performance.\nWe will cite and discuss the new baseline HCRN[1] in the revised version.\n\n[1]. Le, Thao Minh, et al. \"Hierarchical Conditional Relation Networks for Video Question Answering.\" CVPR, 2020.\n\n> About the explicit loss functions.\n\nWe will provide explicit loss functions for each module in the revised version.\n\n> About the requirement of annotation for program parser training.\n\nYes, our DCL requires 1,000 annotated programs for training the program parser.\nIt should also be noted that some previous research like [1] has been learning to parse questions into symbolic programs by only question answering.\nThey achieved this goal with REINFORCE[2] and curriculum learning.\nWe regard learning the program parser by question answering for CLEVRER as an interesting future direction.\n\n[1]. Mao, Jiayuan, et al. \"The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision.\" ICLR, 2019.\n\n[2]. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. MLJ, 1992.\n\n\n> About the new baseline for video-text retrieval.\n\nWe will implement HGR[1] for CLEVRER-Retrieval, cite and analyze its performance in the revised version.\n\n[1]. Chen, Shizhe, et al. \"Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning.\" CVPR, 2020.", "title": "Response to AnonReviewer2"}, "LfQ0igMrur": {"type": "rebuttal", "replyto": "p6m6x7IUmVW", "comment": "Thanks for your interest in the method and your detailed and constructive comments.\n\n\n> The details of the concept embedding.\n\nWe store a vector embedding for each concept in the symbolic executor. \nDuring program execution, we quantize concepts by matching the concept embeddings like \"moving\" with object features.\nDCL optimizes the concept embeddings by question answering. \n\n>Paper organization and the details of DCL.\n\nThe captions of Fig. 2 shows the inference process for a counterfactual question (object trajectory detector -> dynamic predictor -> feature extractor -> symbolic executor).  \n\nDuring training, we follow a multi-step training strategy,\n- Step 1: we learn the static concept embeddings (color, shape, and material) in the symbolic executor by descriptive and explanatory question-answer pairs. It requires no dynamic prediction (coarse trajectory detector in Eq. (1) -> feature extractor -> symbolic executor).\n- Step 2: we then use the learned concept embeddings to quantize static concepts for each object and refine the object trajectories (see Eq. (2)-(3)).\n- Step 3: we train a dynamic predictor based on the refined trajectories.\n- Step 4: we train the full model with all the question-answer pairs(object trajectory detector -> dynamic predictor ->feature extractor -> symbolic executor).\n\nWe will adjust the caption in Fig. 2 and provide more details in Sec. 3.1 and Sec. 3.2 accordingly in the revised version.\n\n>The motivation of Trajectory refinement.\n\nThanks for pointing out the object tracking literature (DeepSORT).\nWe will discuss and cite this paper in the revised version.\n\nWe would like to clarify that the contribution of \"trajectory refinement\" in this paper is not the definition of the scoring function in Eq. (2) or the linear sum assignment algorithm for optimization. Our main contribution is to build a framework that can ground static concepts from these coarse object trajectories using questions and answers pairs. These learned concepts can then help refine the object trajectories. We verify the effectiveness of the learned static concepts for better object trajectories in Table 8. It can constantly boost the performance of different optimization solvers.\n\nWe will add Kalman Filters into the tracker and analyze its performance in the revised version.\n\n> About the generalization to other real datasets.\n\nWe agree that evaluating our model in real-world videos is very important.  We will include new experimental results of DCL on a real-world block-tower video dataset[1]. We believe these results could further verify the effectiveness and generalization of DCL to learn the new dynamic concept in real videos.\n\n> About experiments on other video action datasets.\n\nPlease first refer to common concerns to see the importance of learning physical concepts and their differences from human-centric action datasets. \n\n- In Section 4, we have clearly indicated why  CATER is unsuitable for evaluating learning object and event concepts from language.   CATER  is indeed a good dataset to study dynamic and video reasoning, but unfortunately, they do not provide questions and answer pairs. \n\n- We would like to reiterate that our work aims to learn physical objects and event concepts in videos. This is a different research problem from understanding actions and activities. Compared with those action videos, the datasets and the tasks we used in this paper have richer compositional and causal structures.  These are also the main reason why models working well on these action datasets do not achieve good performances on CLEVRER. \n\n[1]. Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. ICML, 2016.\n\nPlease let us know if you have additional questions to discuss.\n\n\n", "title": "Response to AnonReviewer1"}, "VtiRgTG8etn": {"type": "review", "replyto": "bhCDO_cEGCz", "review": "> Summary:\n\nThe paper studies the problem of dynamic visual reasoning on the recently proposed synthetic video QA dataset CLEVRER for understanding visual properties, physical events, the causal relationship between events, and making future and counterfactual predictions. The paper proposes a joint framework called Dynamic Concept Learner which contains five modules (object trajectory detector, video feature extractor, dynamic predictor, language program parser, and neural symbolic executor) and follows a multi-step training paradigm to train the model purely from the question-answer pairs in CLEVRER train split.  The object trajectory detector module, the concept embedding/quantization, and the dynamic predictor module play a very important role in DCL. These modules enable the dynamic & causal reasoning abilities for DCL. The neural program parsing & execution part of DCL is similar to that in Neuro-Symbolic Concept Learner (NS-CL) (Mao et al., 2019) and thus DCL can be regarded as the dynamic extended version of  NS-CL.\n\n> Strengths:\n\n*  The paper is well-written and easy to follow.  A lot of details about the DCL model, neural programs, and dataset are provided in the appendix.\n\n* The experiment on CLEVRER shows the effectiveness of DCL over previous methods.  Additional experiments on new CLEVRER Grounding & Retrieval applications demonstrate the generalization ability of DCL trained with the original VQA task on other CLEVRER related tasks.\n\n* DCL is trained directly from QA pairs without additional labels for object attributes and events but still outperforms the previous methods, which is a big strength.\n\n* During inference, DCL can acquire attribute/event concept perception, object trajectory prediction at the same time. Once the answer is generated, the whole reasoning path for the answer is also ready, which exhibits the good model interpretability of DCL.\n\n* From *VirTex: Learning Visual Representations from Textual Annotations (Desai et al., 2020)* and *LXMERT: Learning Cross-Modality Encoder Representations from Transformers (Tan et al., 2020)*, we can see that actually textual annotations such captions and question-answer pairs contain high-quality information and are helpful for learning visual or concept representations. I think this paper is another good example of discovering such high-quality information without using additional fine-grained labels.\n\n> Some concerns and suggestions:\n\n- My major concern is that the evaluation is based on a synthetic toy video dataset and it is simplistic compared to real-world videos. Although the author demonstrates that DCL has transferability between different vision-language tasks on CLEVRER, it is still skeptical whether DCL can be generalized to real-world video QA datasets with more complex visual scenes such as TGIF-QA, MSVD-QA, or MSRVTT-QA since the dynamic attributes and events in CLEVRER is only limited to (moving, stationary) and (in, out, collision). If possible, evaluating on more datasets would further demonstrate the effectiveness of DCL across different visual scenes.\n\n* Besides NS-DR baseline which was proposed together with CLEVRER dataset, Memory (Fan et al., 2019) is the most recent visual-language reasoning approach for videos. It is evaluated on TGIF-QA, MSVD-QA, and  MSRVTT-QA in the original paper. I think the author should find some newer approaches evaluated on these datasets to make comparisons on CLEVRER. For instance, [Hierarchical Conditional Relation Networks for Video Question Answering (CVPR 20\u2019)](https://openaccess.thecvf.com/content_CVPR_2020/papers/Le_Hierarchical_Conditional_Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.pdf) can be a candidate baseline.\n\n* Although the author introduces all the five modules and describes the training steps, it is still not clear for readers to understand how to train the five modules jointly without providing an explicit loss function or training objective. I think the author should specify such a training objective explicitly.\n\n- The paper states that DCL does not require additional fine-grained labels for attributes and events. However, DCL requires additional 1000 programs to train the program parser for all question types. Can we call these 1000 programs additional annotations? For a new visual scene that is different from the current 3D synthetic domain, to make DCL suitable for the new visual domain,  we need to define new sets of programs and concepts, which may require additional annotation labors.\n\n- To make the experiments more convincing on Retrieval and Grounding tasks, the author could further compare with some recent video-text retrieval/grounding approaches. To name a few: 1) [Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Fine-Grained_Video-Text_Retrieval_With_Hierarchical_Graph_Reasoning_CVPR_2020_paper.pdf) 2) [TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval](https://arxiv.org/pdf/2001.09099.pdf) 3) [Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos](https://papers.nips.cc/paper/8344-semantic-conditioned-dynamic-modulation-for-temporal-sentence-grounding-in-videos.pdf)\n", "title": "Good paper on dynamic visual reasoning but with concerns in generalization to real-world videos", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "qPHZB0LrulM": {"type": "review", "replyto": "bhCDO_cEGCz", "review": "Pros\n1.\tThis paper extends the semantic modular network on images to the video setting.\n2.\tBy making use of the weak supervision from the question answer, it can learn different kinds of concepts including physical objects, attributes and events. \n3.\tThe paper conduct experiments on different evaluation settings including video causal reasoning, video grounding and generalization experiments.\nCons\n1.\tThe paper lacks some details on the training procedure.\n2.\tIt\u2019s better to show the learned concepts visually as one of the strengthens of is explain ability.\n3.\tHow\u2019s the model\u2019s sample complexity\n\nComments\n1.\tIs the trajectory refinement and model learning an iterative process?\n2.\tWhat does the RGB patches mean and how do you combine the RGB patches into the predicted locations in the dynamic predictions?\n3.\tIs this paper using some pre-trained object recognizer and attribute classifier which acts as some kind of pseudo labeling?\n4.\tIn the generalization experiments, does the model trained and test using the same domain dataset?\n", "title": "In this paper, the authors study the problem of dynamic visual reasoning on raw videos. Compared with the previous work which require dense supervision of the video objects and events. Instead they proposed DCL which grounds objects and events without ground truth labels.  The proposed method achieves state of the art performance on CLEVRER and other tasks including video-retrieval and event localization.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "lTqMTgNROIR": {"type": "review", "replyto": "bhCDO_cEGCz", "review": "=Summary\nThe paper proposes a new framework, Dynamic Concept Learner (DCL), which learns by watching videos and reading questions/answers.  It is inspired by prior work which combines symbolic representations with video dynamics modeling, but unlike prior work, here the authors do not use any additional supervision except for question/answer pairs. DCL is quiet complex, involving multiple components, including program parser (transforms the question/answer into executable symbolic programs), object detection&tracking, object-centric feature extractor (with static and dynamic components), object/event concept embeddings (share the same latent space as the visual features), trajectory refinement (based on static cues) and dynamic predictor (for future and counterfactual scenes), and, finally, symbolic executor (to predict whether the answer is correct), which runs on the latent features. DCL achieves state-of-the-art results on the CLEVRER dataset. The learned representations are also shown to be useful for other tasks, such as grounding and retrieval.\n\n=Strengths\n\nThe paper is overall well written and the approach is novel to the best of my knowledge.\n\nDCL achieves state-of-the-art results on the CLEVRER dataset, w/o accessing visual labels from simulation during training.\nThe biggest improvement is notably achieved on the predictive and counterfactual questions.\n\nThe authors also show that DCL learns to ground object/event concepts w/o explicit labels.\n\nFinally, the authors construct CLEVRER-Grounding and CLEVRER-Retrieval datasets to test the generalization ability of DCL. They show that it significantly improves over a baseline on both tasks. \n\n=Weaknesses/High-level comments\n\nCounterfactual questions remain particularly hard to answer, do the authors have some intuition as to what may be missing in their model? What are some common failure modes?\n\nTable 1: it is somewhat surprising that the difference between the DCL and DCL-Oracle is so small, and DCL even improves over DCL-Oracle in one case. Why is that?\n\nWhat do the authors believe is the main limitation of their proposed approach? Currently it operates on synthetic images as well as synthetic language, what will be necessary to move to more realistic domains?\n\nNothing was stated about making the code available.\n\n=Minor comments (P# - page number)\n- P1 that what the concepts => what the concepts\n- P3, Sec 3: DYNAMICS CONCEPT LEARNER => DYNAMIC CONCEPT LEARNER\n- P4: an program parser => a program parser\n- The paper heavily relies on the supplemental material.", "title": "An effective approach to learning dynamic concepts without expensive supervision", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}