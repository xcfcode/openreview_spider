{"paper": {"title": "How to train your MAML", "authors": ["Antreas Antoniou", "Harrison Edwards", "Amos Storkey"], "authorids": ["a.antoniou@sms.ed.ac.uk", "h.l.edwards@sms.ac.uk", "a.storkey@sms.ed.ac.uk"], "summary": "MAML is great, but it has many problems, we solve many of those problems and as a result we learn most hyper parameters end to end, speed-up training and inference and set a new SOTA in few-shot learning", "abstract": "The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem.Model Agnostic Meta Learning or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call MAML++.", "keywords": ["meta-learning", "deep-learning", "few-shot learning", "supervised learning", "neural-networks", "stochastic optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes several improvements for the MAML algorithm that improve its stability and performance.\nStrengths: The improvements are useful for future researchers building upon the MAML algorithm. The results demonstrate a significant improvement over MAML. The authors revised the paper to address concerns about overstatements \nWeaknesses: The paper does not present a major conceptual advance. It would also be very helpful to present a more careful ablation study of the six individual techniques.\nOverall, the significance of the results outweights the weaknesses. However, the authors are strongly encouraged to perform and include a more detailed ablation study in the final paper. I recommend accept."}, "review": {"Byg5xz2rCX": {"type": "rebuttal", "replyto": "Hkg2spUSRQ", "comment": "Thanks for your prompt response. I think your point about the _automation_ of things is correct. I will amend the paper to be more precise in that claim as per your request. Regarding the automation of additional parts of the system, I am currently working on that, but it felt like it exceeded the scope of this paper hence breaking it into smaller easier to digest papers, that tackle one thing at a time. In my experience, papers that try to do too many things at once are often incredibly hard to write, and even harder to read. \n\nI will modify the particular claim shortly. Thanks for your time.", "title": "2nd Response to Reviewer 1"}, "H1eHVoEaaX": {"type": "rebuttal", "replyto": "rJg-0D-927", "comment": "Thank you for taking the time to review our paper. Before I start delving into the technical aspects of this response. To address your concerns, I will use an enumeration that matches the indexes of your concerns.\nThe paper is indeed targeted towards a particular class of algorithms. That class being end-to-end differentiable gradient-based meta-learning. MAML and Meta-learner LSTM [1] are two instances of that particular class of algorithms. Our proposed techniques can be applied to any algorithm of that class, given that they utilize inner-loop optimization processes as part of their learning. So, even though this work is indeed targeted towards a particular class of models, that class is general enough and applicable to enough domains that we felt that an investigation of the type presented in this paper was necessary. In fact, the work in this paper was the result of the first author\u2019s attempts to build systems that learn various other components (i.e. instead of just learning a highly adaptable parameter initialization, he was attempting to learn loss functions/update functions and dynamic generation of parameter initializations given a task among others). What he realized, however, was that MAML was really hard to actually work with, being very inflexible to architecture configuration, causing gradient degradation problems, instability in training and requiring lots of manual inner loop learning rate tuning.  In attempting to fix those problems, so he could build on top more complicated systems, this paper came to be. \nIn MAML, the resulting inference model is effectively an unrolled 5 layer network over N steps. If that N=5, then the resulting model has a depth of effectively 25 layers. In standard deep networks, gradient degradation can be greatly reduced or altogether removed via the usage of skip-connections. Since in MAML we can\u2019t really apply skip-connections from a subsequent model to a previous one (because that would further complicate the gradients), we decided that the best way to inject clean/stable gradients to all iterations of the network would be to use 2 losses for each step-wise network. One loss, providing an implicit gradient, coming from subsequent iterations of the network (i.e. the original MAML loss), and another per-step loss, providing an explicit gradient, coming directly from evaluating the model on the target set. This way, every network iteration receives stable gradients which keep the network stable during the early epoch training. Eventually, the importance of earlier steps becomes 0, which means that the original MAML loss is used instead. However, since the network has already learned a stable parameterization, the stability remains throughout training (we empirically confirmed this).\nWe conducted an ablation study on 20-way 1-shot Omniglot, as shown in table 2. We did want to conduct even more exhaustive ablation studies across all Omniglot and Mini-Imagenet tasks, however, due to computing constraints we had to restrict ourselves. Using the \u201chardest\u201d Omniglot 20-way 1-shot task as the ablation study\u2019s subject seemed like a sensible thing to do since it was cheaper computationally, but \u201chard\u201d enough for the results to generalize well in other tasks.\nIndeed, annealing various components is not as novel as some of the other proposals in the paper. However, since this paper was essentially an engineer\u2019s handbook on how to train MAML-like models, we felt that people should be aware of the effect those techniques have on the system\u2019s performance.\nIndeed, there is other literature on meta-learning learning rates. Our approach\u2019s novelty lies in learning \u201cper-step\u201d and \u201cper-layer\u201d learning rates. By being able to learn per step learning rates, we allow the network to choose to decrease or increase it\u2019s learning rates at each step, to minimize overfitting. Another interesting phenomenon, that we will address in a future blog post, is the fact that across all networks, we noticed that particular layers choose to \u201cun-learn\u201d (flipping the direction of the learning rate) at particular steps. We theorize that the network might be attempting to remove some existing knowledge to replace it with new knowledge, or using forgetting as a way to steer gradients for more efficient learning.\n\nRegarding the minor concerns, yes, we will fix the referencing inconsistencies and the batch size indexing problem.\n\nOnce again, I want to thank you for taking the time to review our work.\n\n1. Ravi, S. and Larochelle, H. (2016). Optimization as a model for few-shot learning.\n", "title": "Response to Reviewer 3"}, "H1ewhME6TQ": {"type": "rebuttal", "replyto": "rJxOlkMPh7", "comment": "Thanks for taking the time to review our paper. Further thanks for your very detailed, useful and constructive comments. We will now address your concerns below in the same order they were made:\n\nWe claim that we reduce the hyperparameter choices needed because once our methodologies are applied exactly as proposed, the resulting system will achieve very high generalization and fast convergence without any additional tuning. We have attempted to initialize the learning rates from a random uniform distribution (ranging from 0.1 to 0.01) in addition to initializing manually. Both methods, interestingly, converge to very similar learning rates. Thus, random initialization suffices for that aspect, which does reduce the need for explicitly choosing a learning rate.\nRegarding the gradient directions. The alpha also includes a sign. So, in other words, the alpha also learns the direction of the learning rate, hence our claim. In fact, an interesting finding is that, in specific steps and layers, the network chooses to \u201cunlearn\u201d or flip the sign of the learning rate. Further investigation is required to understand this behavior, but a current working hypothesis is that the network is trying to \u201cforget\u201d particular parts of its weights, which somehow produces more efficient learning, in subsequent steps. We will further expand on this in a future blog post. \n\nAll of your suggestions and typo-locations are spot-on and we will take care to address all of those in the final version of the paper. Again, we really thank you for providing such a detailed and constructive review.\n\n", "title": "Response to Reviewer 1"}, "BklkMRGpam": {"type": "rebuttal", "replyto": "Skg14bX92X", "comment": "Thank you for your review. \n\nRegarding the conceptual and technical novelty concerns.\n\nTo clarify, our main contribution comes in the form of carrying an investigation on how MAML can be stabilized and how the model can be modified such that it can consistently achieve faster convergence and strong generalization results without any hyperparameter tuning required. Then, once the investigation is completed and key problem-areas isolated, we use our investigation insights to improve the system. In fact, the whole reason for doing this was because we attempted to built new research ideas on top of MAML only to find out just how sensitive and unstable the system was. Therefore, we decided that finding the issues and fixing them would enable researchers working on gradient-based end-to-end meta-learning, such as MAML or Meta Learner LSTM [1] to concentrate on the new approach they want to build rather than trying to overcome instability issues of the base methodology. Furthermore, the industry would also benefit from this, as they would have an easier time training MAML based models. \n\nMost of the proposed approaches are novel and non-obvious (i.e. LSLR, BNWB+BNRS, and multi-step loss optimization). Overcoming gradient degradation issues by utilizing multi-step target-loss optimization which is annealed over time, is in our knowledge, done for the first time in this work. Furthermore, we provide novel contributions in the form of learning things \u201cstep-by-step\u201d.\n\nFor example, we propose that learning per-layer, per-step learning rates would benefit the system, more so than just learning per-layer learning rates and sharing them. The reason is that the model would be free to choose to decrease its learning rate or otherwise change it from step to step to reduce overfitting. This technique is both novel and non-obvious. Furthermore, LSLR is not something that is possible in standard deep learning, as learning the learning rates would require an additional level of abstraction (thus entering the meta-learning arena). \n\nAnother contribution with significant novelty comes in the form of proposing a step-by-step batch norm variant, designed for meta-learning systems that require inner loop optimization. Learning batch norm parameters for every step, as well as collecting per-step running statistics speeds up the system and allows batch normalization to truly work in this setting, whereas the previous variant of batch norm used, constrained things further, instead of achieving the improved convergence and generalization that batch norm can achieve in standard deep learning training setups. \n\nThe rest of the contributions, such as annealing the derivative order and using cosine scheduling for Adam are less novel, but nonetheless important to investigate. We show from our experiments that those approaches can improve the system, something which was previously unconfirmed. \n\nThe comparative performance (between MAML and MAML++) both in convergence speed and final generalization is significant and produces state of the art results. Furthermore, that performance is achieved far more consistently and with more stability across architectures. We hold the belief that the community would really benefit from this work, hence why we submitted it.\n\n1. Ravi, S. and Larochelle, H. (2016). Optimization as a model for few-shot learning.\n", "title": "Response to Reviewer 2"}, "Skg14bX92X": {"type": "review", "replyto": "HJGven05Y7", "review": "[Summary]\nThis work presents several enhancements to the established Model-Agnostic Meta-Learning (MAML) framework. Specifically, the paper starts by analyzing the issues in the original implementations of MAML, including instability during training, costly second order derivatives evaluation, missing/shared batch normalization statistics accumulation/bias, and learning rate setting, which causes unstable or slow convergence, and weak generalization. The paper then proposes solutions corresponding to each of these issues, and reports improved performance on benchmark datasets.          \n\nPros\nGood technical enhancements that fix some issues of a popular meta-learning framework\nCons\nLittle conceptual and technical novelty \n\n[Originality]\nThe major problem I found in this work is the lack of conceptual and technical novelty. The paper basically picks up some issues of the well-established MAML framework, and applies some common practices or off-the-shelf technical treatments to fix these drawbacks and improve the training stability, convergence, or generalization, etc. E.g., it seems to me that the most effective enhancement comes from the use of adoption of learning rate setting (LSLR), or variant version of batch normalization (BNWB+BNRS) in Table 1, which have been the standard tricks to improve performance in the deep learning literature. Overall, the conceptual originality is little.         \n\n[Quality]\nThe paper does get most things well executed from the technical point of view. There does not seem any major errors to me. The results reported are also reasonable within the meta-learning context, despite lack of originality.  \n\n[Clarity]\nThe paper is generally well written and I did not have much difficulty to follow. \n\n[Significance]\nThe significance of this work is marginal, given the lack of originality. The technical enhancements presented in the paper, however, may be of interest to people working in this area. \n", "title": "A paper with marginal novelty over an established framework.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJg-0D-927": {"type": "review", "replyto": "HJGven05Y7", "review": "In the work, the authors improve a simple yet effective meta-learning algorithm called Model Agnostic meta-learning (MAML) from various aspects including training instability, batch normalization etc. The authors firstly point out the issues in MAML training and tackle each of the issue with a practical alternative approach respectfully. The few-shot classification results show convincing evidence.\n\nSome major concerns:\n1. The paper is too specific about improving one algorithm, the scope of the research is quite narrow and I'm afraid that some of the observations and proposed solutions might not generalize into other algorithms;\n2. Section 4, \"Gradient Instability \u2192 Multi-Step Loss Optimization.\" I don't see clearly why the multi-step loss would lead to stable gradients. It causes much more gradient paths than the original version. I do see the point of weighting the losses from different step;\n3. The authors should have conducted careful ablation study of each of the issues and solutions. The six ways of proposed improvements may make the the performance boost hard to understand. It would help to see which way of the proposed improvement contribute more than others;\n4. Many of the proposed improvements are essentially utilizing annealing mechanisms to stabilize the training, including 1) anneals the weighting of the losses from different step; 2) anneal the second derivative  to the first derivative;\n5. For the last two improvements about the learning rate, there are dozens of literature on meta-learning learning rate and the proposed approach does not seem to be novel;  \n \nMinors\n1. The reference style is inconsistent across the paper, sometimes it feels quite messy. For example, \"Batch Stochastic Gradient Descent Krizhevsky et al. (2012)\" \"Another notable advancement was the gradient-conditional meta-learner LSTM Ravi & Larochelle (2016)\";\n2. Equation (2) (3) the index b should start from 1, size of B should be 1 to B;\n", "title": "In-depth discussions and improvements on MAML", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJxOlkMPh7": {"type": "review", "replyto": "HJGven05Y7", "review": "Paper summary - This paper provides a bag of sensible tricks for making MAML more stable, faster to learn, and better in final performance.\nQuality - The quality of the work is strong: the results demonstrate that tweaks to MAML produce significant improvements in performance. However, I have some concern that certain portions of the text overclaim (see concerns section below).\nClarity - The paper is reasonably clear, with some exceptions (see concerns section).\nOriginality - The techniques described in the paper range from only mildly novel (e.g. MSL, DA), to very obvious (e.g. CA). Additionally, the paper's contributions amount to tweaks to a previously existing algorithm. \nSignificance - The quality of the results make this a significant contribution in my view.\nPros - Good results on a problem/algorithm of great current interest.\nCons - Only presents (in some cases obvious) tweaks to a previous algorithm; clarity and overclaiming issues in the writeup.\n\nConcerns (please address in author response)\n- The paper says  \"we \u2026 propose multiple ways to automate most of the hyperparameter searching required\". I'm not sure that this is true. The only technique that arguably removes a hyperparameter is LSLR. Even in this case, you still have to initialize the inner loop learning rates, so I'm not convinced that even this reduces hyperparameters. Perhaps I've missed something, please clarify.\n- Section 4's paragraph on LSLR seems to say that you have a single alpha for each layer of the network. If this is right, then saying your method has a \"per layer gradient direction\" is very confusing. Each layer's alpha modulates the magnitude of that layer's update vector, but not its direction. The per-layer alphas together modify the direction of the global update vector. Perhaps I've misunderstood; equations describing exactly what LSLR does would be helpful. In any case, this should be clarified in the text.\n\nSuggestions (less essential than the concerns above)\n- The write-up is redundant and carries unnecessary content. The paper would be better shorter (8 pages is not a minimum :)\nSection 1 covers a lot of background on the basics of meta-learning background that could be skipped. Other papers you cite (e.g. the MAML paper cover this). \n    - Section 2 goes into more detail about e.g. matching nets than is necessary. \n    - Section 2 explains MAML, which is then covered in much more detail in Section 3; better to leave out the Section 2 MAML paragraph. \n    - Sections 3 and 4 are very redundant. Combine them for a shorter (i.e., better!) paper.\n- The paper says, \"Furthermore, for each learning rate learned, there will be N instances of that learning rate, one for each step to be taken. By doing this, the parameters are free to learn to decrease the learning rates at each step which may help alleviate overfitting.\" Does this happen empirically? Space could be freed up (see above) to have a figure showing whether or not this happens.\n- The paper says, \"we propose MAML++, an improved meta-learning framework\" -- it's a little too far to call this a new framework. it's still MAML, with improvements.\n\nTypos\n- \"4) increase the system\u2019s computational overheads\" -> overhead\n- \"composed by\" -> composed of\n- \"Santurkar et al. (2018).\", \"Krizhevsky et al. (2012),\",  \"Finn et al. (2017) \" -> misplaced citation parens\n- \"a method that reduce\" -> reduces\n- \"An evaluation ran consisted\" -> evaluation consisted\n- The Loshchlikov and Hutter citation in the bibliography isn't right. It should be \"Sgdr: Stochastic gradient descent with restarts.\" (2016) instead of \"Fixing weight decay regularization in adam\" (2017).\n", "title": "Improving MAML", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJgieSXZ5X": {"type": "rebuttal", "replyto": "rJlF4Lf-97", "comment": "Meta-SGD learns alphas of dimensionality equal to the network parameters. Instead with LSLR we propose learning one alpha for each layer of the network. A component qualifies as a layer if it has learnable weights or biases in it. In addition, instead of just learning a learning rate and direction (alpha) for each layer to be used across all inner loop steps, we instead propose to learn different alphas for each inner loop step. This allows the network to choose to decay its alphas or otherwise change them, to maximize generalization performance (in some cases we noticed the network choosing to unlearn for some inner loop steps by using a negative learning rate and learn in others). So, to summarise, we learn one learning rate and direction for each layer for any given inner loop step. The network we used had 4 CNN layers along with a final softmax. That's a total of 5 layers, but since we learn learning rates for weights and biases separately, this means that the model learns a total of 10 learning rates and directions for any given step. For example, in the case where the model takes 5 inner loop steps, we have a total of 5 x 10 = 50 learning rates and directions, which is represented by 50 learnable parameters in the system. ", "title": "Method of Learning Gradient Directions Clarification"}, "HkgRQ8EAtQ": {"type": "rebuttal", "replyto": "rke46mAatQ", "comment": "Thanks for your comment. Firstly, I'll reiterate that the main point of the paper is to improve MAML as a model itself. Furthermore, we did a very thorough literature review but missed out on the papers you have stated. The work in our paper had already taken full shape in May thus meaning that works 1 and 3 (that came later) escaped our radar. The second paper you mentioned, \"Neural Attentive Meta Learner\" was not included in many of the latest few-shot learning papers that came out in June 2018, thus making it harder for us to be aware of it. We did try to cover everything in the literature prior to starting our work, however as is often the case, one or two papers might escape ones review. Especially in this field, where papers keep coming out on a daily basis on arxiv. We shall add the approaches you mentioned in our result tables when editing is allowed again. Thank you for informing us of some literature we were previously unaware of.", "title": "Re: Related Works with better results"}}}