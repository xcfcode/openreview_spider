{"paper": {"title": "Human perception in computer vision", "authors": ["Ron Dekel"], "authorids": ["ron.dekel@weizmann.ac.il"], "summary": "Correlates for several properties of human perception emerge in convolutional neural networks following image categorization learning.", "abstract": "Computer vision has made remarkable progress in recent years. Deep neural network (DNN) models optimized to identify objects in images exhibit unprecedented task-trained accuracy and, remarkably, some generalization ability: new visual problems can now be solved more easily based on previous learning. Biological vision (learned in life and through evolution) is also accurate and general-purpose. Is it possible that these different learning regimes converge to similar problem-dependent optimal computations? We therefore asked whether the human system-level computation of visual perception has DNN correlates and considered several anecdotal test cases. We found that perceptual sensitivity to image changes has DNN mid-computation correlates, while sensitivity to segmentation, crowding and shape has DNN end-computation correlates. Our results quantify the applicability of using DNN computation to estimate perceptual loss, and are consistent with the fascinating theoretical view that properties of human perception are a consequence of architecture-independent visual learning.", "keywords": ["Computer vision", "Transfer Learning"]}, "meta": {"decision": "Reject", "comment": "I think the reviewers evaluated this paper very carefully and were well balanced. The reviewers all agree that the presented comparison between human vision and DNNs is interesting. At the same time, none of the reviewers would strongly defend the paper. As it stands, this work seems a little too premature for publication as the analysis does not go too much beyond what we already know. We encourage the authors to deepen the investigation and resubmit."}, "review": {"S1x3sRdLx": {"type": "rebuttal", "replyto": "BJbD_Pqlg", "comment": "Jan 16-17, 2017:\n- Edited the hypothesis about \"Overshoot\" and \"Undershoot\" inconsistency with perception (result 1).\n- Added the prediction quality of perceptual threshold as a function of layer for model ResNet-152 .\n\nJan 15, 2017:\n-\tAdded results for three baseline models: two linear filter banks (Gabor decomposition and steerable pyramid) and VGG-19 with scrambled weights.\n-\tAdded results for CaffeNet model at several snapshots during training.\n-\tAdded human data for contrast sensitivity (figure 3 results).\n-\tAdded configurations for context experiments (figure 2 results). Now there are 90 configurations per CNN architecture for Segmentation, Crowding, and Shape.\n-\tCosmetics and minor corrections.\n\n", "title": "Edits"}, "Skli5zYIl": {"type": "rebuttal", "replyto": "ByL97qNEg", "comment": "Thank you for the review which raises two important points.\n\nAbout adversarial examples. Using these techniques, a small imperceptible pixel change causes a very large change in the reported class label of an image. This should not be taken as an argument against the claims of this work for two reasons.\nFirst, the approach used here considers the average change over all units in the entire network, so techniques developed to cause a large change in a single neuron (the output class label neuron) may simply not work. I may be wrong here, but it seems much more difficult to simultaneously cause a large change in millions of roughly independent neurons than it is to change one (the independence implies that a change which specifically increases one neuron will on average cause zero change in others). \nSecond, the adversarial examples are more a backdoor of the architecture than of the learned representation. That is, because the net is composed of linear components, it is susceptible to some variations in image space (Goodfellow, Shlens, Szegedy, 2014). These variations can be easily found when intentionally searched for, but are extremely rare. Even if the results reported here do not work for some extremely rare cases, it is not at odds with prediction working very well for regular images, and is not at odds with the learned representation converging towards human perception.\n\nAbout investigating the properties of the transformation \u2013 this was not attempted.\n\n \n", "title": "Response to Reviewer1"}, "H1x0okYLx": {"type": "rebuttal", "replyto": "H19W6GPVl", "comment": "Thank you for the detailed and thoughtful review, which helped improve the quality of the work.\n\nAbout your suggestion to widen or close the gap: this was not attempted, but figure 1 now depicts mis-predicted examples. (The psychophysical data for result 1 was collected by Alam et al. (2014).)\n\nAbout telling DNN models apart: the approach described in this work cannot tell which DNN model is most suited to predict perception, since the predictions of the different models are very similar. Although ResNet-152 may seem to be an exception, this is mostly an artifact of averaging across layers, a heuristic which permits a parameter-free prediction, but is less appropriate in this very deep model for some cases.\n\nThe similarity depicted in result 2 is qualitative (Easy vs. Hard), but is considered for a large number of configurations to try and show that it is robust. Table 6 now reports a quantitative comparison of order-of-difficulty for Shape, showing somewhat low values of measured similarity, not much different from baseline (the main qualitative Easy vs. Hard comparison is more robust).\n\nFollowing your suggestion, result 3 was compared with human perceptual data. Unfortunately, when correctly quantified, the contrast constancy which was reported to develop along the net is false: the representation in the final \u201cprob\u201d layer is not more similar to perception than in the early \u201cconv1_1\u201d layer. (Apparently, what develops across the net is the log-linear contrast response.)\n\n", "title": "Response to Reviewer2"}, "B1dfvkKLe": {"type": "rebuttal", "replyto": "BkcY-CZNl", "comment": "Thank you for the review, which raised a crucial issue that indeed was not adequately addressed.\n\nFollowing your suggestion, analyses were added for several baseline models: Gabor decomposition, steerable pyramid, the same architecture at different time points during training, and a scrambling of model parameters (though not all baselines for all results). While these baselines show good predictive strength in some cases, they fail in others. In a more general perspective, it is not difficult to craft simple models that exhibit the properties depicted in results 2 and 3. For example in result 2, Segmentation is just a second-order texture, Crowding is just clutter, and Shape is only partially similar. Still, what may be considered surprising and unexpected is that all these unrelated properties were picked up during object recognition learning in what appears to be a very robust way (across most architectures and tested variants). More formally, to predict the outcome of a new psychophysical experiment, DNN predictions may be worthwhile to consider.\n\nNote that result 3 is now weakened: the contrast constancy previously reported is not as strong as it seemed when quantified correctly.", "title": "Answer to Reviewer3"}, "S18fC-9Ql": {"type": "rebuttal", "replyto": "rkIeN3U7e", "comment": "We thank the reviewer for the pre-review questions, which will help improve the work.\n - Figure 1c shows accuracy of prediction as a function of layer for the different scales (100%, 66%, 50%). That is, rows 1-3 in Table 2 correspond to the three line colors of Figure 1c. The manuscript will be edited to emphasize this point. Based on receptive field sizes in model VGG-19, a factor 2 change in input scale should roughly cause a shift of one layer (i.e. conv2_1 -> conv1_1, conv3_1 -> conv_2_1, etc), unlike the small changes observed (the different lines nearly overlap).\n - There were 90 different types of patterns (varying scale, target position, and noise magnitude), each tested with three models (VGG-19, CaffeNet, GoogLeNet). In addition, model ResNet-152 was tested for 10 patterns. So 90+90+90+10=280. The manuscript will be edited to emphasize this point.\n - The manuscript will also be edited to include data on which configurations do not follow the order-of-difficulty from human Psychophysics. A main factor appears to be that two of the shapes are systematically against the order found in human Psychophysics.\n - We acknowledge that without a low-level image processing baseline it is difficult to judge the significance of the results in Figure 2. The manuscript will be edited to include a multiscale linear filter bank baseline. Also, regarding the results in Figure 1, note that the perceptual model depicted in Table 1 (by Alam et al. 2014) uses a multiscale log-Gabor filter bank. The manuscript will be edited to explain this point.", "title": "Early response"}, "rkIeN3U7e": {"type": "review", "replyto": "BJbD_Pqlg", "review": "- Did you do the threshold prediction experiment for differently scaled versions of the input image (as suggested by Table 2, row 2 and 3)? \nIf so, was there a systematical change in the best correlated L1 layer when changing the input size of the image by rescaling? I.e. is there an effect solely based on the relative receptive field size of the units?\n\n- For the experiments shown in Figure 2 there are 280 configurations for the segmentation and crowding task. In section 7 there are only 90 configurations described for each task, what are the extra configurations?\n\n- For the same experiments, could the authors comment on what configurations typically did not follow the task-difficulty ordering known from human psychophysics.\n\nEdit:\nGenerally, to judge the significance of the findings it would be nice to compare to a good baseline model. Such a model might be a multi-scale linear filter bank (compared to the multiscale non-linear filter bank that is the CNN) that is powerful in terms of low-level image processing but lacks high-level information processing abilities. An example of such a model would be the steerable pyramid by Simoncelli et al. (http://www.cns.nyu.edu/~eero/steerpyr/).The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:\n1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.\n2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.\n3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. \n\nWhile these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.\n\nIn that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network\u2019s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.\n\nIn conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.\n\nUPDATE:\n\nThank you very much for your extensive revision and inclusion of several of the suggested baselines. \nThe results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. \n\nOne further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.\n\nAll in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.\n\nI raise the my rating for this paper to 7.", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkcY-CZNl": {"type": "review", "replyto": "BJbD_Pqlg", "review": "- Did you do the threshold prediction experiment for differently scaled versions of the input image (as suggested by Table 2, row 2 and 3)? \nIf so, was there a systematical change in the best correlated L1 layer when changing the input size of the image by rescaling? I.e. is there an effect solely based on the relative receptive field size of the units?\n\n- For the experiments shown in Figure 2 there are 280 configurations for the segmentation and crowding task. In section 7 there are only 90 configurations described for each task, what are the extra configurations?\n\n- For the same experiments, could the authors comment on what configurations typically did not follow the task-difficulty ordering known from human psychophysics.\n\nEdit:\nGenerally, to judge the significance of the findings it would be nice to compare to a good baseline model. Such a model might be a multi-scale linear filter bank (compared to the multiscale non-linear filter bank that is the CNN) that is powerful in terms of low-level image processing but lacks high-level information processing abilities. An example of such a model would be the steerable pyramid by Simoncelli et al. (http://www.cns.nyu.edu/~eero/steerpyr/).The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:\n1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.\n2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.\n3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. \n\nWhile these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.\n\nIn that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network\u2019s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.\n\nIn conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.\n\nUPDATE:\n\nThank you very much for your extensive revision and inclusion of several of the suggested baselines. \nThe results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. \n\nOne further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.\n\nAll in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.\n\nI raise the my rating for this paper to 7.", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}