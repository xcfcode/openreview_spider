{"paper": {"title": "LIT: Block-wise Intermediate Representation Training for Model Compression", "authors": ["Animesh Koratana*", "Daniel Kang*", "Peter Bailis", "Matei Zaharia"], "authorids": ["koratana@stanford.edu", "ddkang@stanford.edu", "pbailis@cs.stanford.edu", "matei@cs.stanford.edu"], "summary": "", "abstract": "Knowledge distillation (KD) is a popular method for reducing the computational over-\nhead of deep network inference, in which the output of a teacher model is used to train\na smaller, faster student model. Hint training (i.e., FitNets) extends KD by regressing a\nstudent model\u2019s intermediate representation to a teacher model\u2019s intermediate representa-\ntion. In this work, we introduce bLock-wise Intermediate representation Training (LIT),\na novel model compression technique that extends the use of intermediate represen-\ntations in deep network compression, outperforming KD and hint training. LIT has two\nkey ideas: 1) LIT trains a student of the same width (but shallower depth) as the teacher\nby directly comparing the intermediate representations, and 2) LIT uses the intermediate\nrepresentation from the previous block in the teacher model as an input to the current stu-\ndent block during training, avoiding unstable intermediate representations in the student\nnetwork. We show that LIT provides substantial reductions in network depth without\nloss in accuracy \u2014 for example, LIT can compress a ResNeXt-110 to a ResNeXt-20\n(5.5\u00d7) on CIFAR10 and a VDCNN-29 to a VDCNN-9 (3.2\u00d7) on Amazon Reviews\nwithout loss in accuracy, outperforming KD and hint training in network size at a given\naccuracy. We also show that applying LIT to identical student/teacher architectures\nincreases the accuracy of the student model above the teacher model, outperforming the\nrecently-proposed Born Again Networks procedure on ResNet, ResNeXt, and VDCNN.\nFinally, we show that LIT can effectively compress GAN generators.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The authors propose a method for distilling a student network from a teacher network and while additionally constraining the intermediate representations from the student to match those of the teacher, where the student has the same width, but less depth than the teacher. The main novelty of the work is to use the intermediate representation from the teacher as an input to the student network, and the experimental comparison of the approach against previous work. \n\n The reviewers noted that the method is simple to implement, and the paper is clearly written and easy to follow. The reviewers raised some concerns, most notably that the authors were using validation accuracy to measure performance, and were thus potentially overfitting to the test data, and regarding the novelty of the work. Some of the criticisms were subsequently amended in the revised version where results were reported on a test set (the conclusions are as before).  Overall, the scores for this paper were close to the threshold for acceptance, and while it was a tough decision, the AC ultimately felt that the overall novelty of the work was slightly below the acceptance bar."}, "review": {"ryerNNfWam": {"type": "review", "replyto": "BkeUasA5YQ", "review": "This paper proposes a new approach to compress neural networks by training the student's intermediate representation to match the teacher's.\n\nThe paper is easy to follow. The idea is simple. The motivation and contribution are clear. The experiments are comprehensive.\n\nOne advantage of the proposed approach that the authors did not mention is that LIT without KD can be optimized in parallel, though I'm not sure how useful this is.\n\nOne major weakness of the paper is how the hyperparameters, such as the number of layers, the alpha, beta, tau, and so on, are tuned. It is not clear from the paper that there is a separate development set for tuning these values. If the hyperparameters are tuned on the test set, then it is not surprising LIT works better.\n\nHere are some minor questions:\n\np.5\n\nLIT outperforms KD and hint training on all settings.\n--> what are the training errors (cross entropy) for LIT, KD and hint training? what about the KD objectives (on the training set) of the model trained with LIT and the one trained with KD? this might tell us why LIT is better than the two.\n\nLIT outperforms the recently proposed Born Again procedure ...\n--> what are the training errors (cross entropy) before and after the born again procedure? this might help us understand why LIT is better.\n\nKD degrades the accuracy of student models when the teacher model is the same architecture\n--> again, the training errors (cross entropy) might be able to help us understand what is going on.\n\np.7\n\nAs shown in Table 3, none of the three variants are as effective as LIT or KD.\n--> is this claim statistically significant? some of the differences are very small.\n\nWe additionally pruned ResNets trained from scratch.\n--> what pruning method is being used?\n\nAs shown in Figure 6., LIT models are pareto optimal in accuracy vs model size.\n--> this is a very strong claim. it's better to say we fail to prune the network with the approach, but we don't know whether there exists another approach that can reduce the network size while maintaining accuracy.\n\nAs shown, L2 and L1 do not significantly differ, but smoothed L1 degrades accuracy.\n--> is this claim statistically significant?", "title": "cute idea but need more analysis", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1e_k8xc3Q": {"type": "review", "replyto": "BkeUasA5YQ", "review": "This paper introduces LIT, a network compression framework, which uses multiple intermediate representations from a teacher network to guide the training of a student network. Experiments are designed such that student networks are shallower than teacher networks, while maintaining their width. The method is validated on CIFAR-10 and 100 as well as on Amazon Reviews.\n\nThe paper is clearly written and easy to follow. The main novelty of the paper is essentially using the teacher intermediate representations as input to the student network to stabilize the training, and applying the strategy to recent networks and tasks.\n\nThe authors claim that they are only concerned with knowledge transfer between layers of the same width, that is teacher and student network been designed (by model construction) to have the same number of downsampling operations, while maintaining the same number of stages (referred to as sections in the paper). However, resnet-based architectures have been shown to perform iterative refinement of their features between downsampling operations (see e.g. https://arxiv.org/pdf/1612.07771.pdf and https://arxiv.org/pdf/1710.04773.pdf ). Moreover, these models were also shown to be good regularizers, since they can reduce their model capacity as needed (see https://arxiv.org/pdf/1804.11332.pdf).  Therefore, having experiments skipping stages would be interesting, and may allow to further compress the networks (by skipping layers or stages which do not incorporate much transformation). Following https://arxiv.org/pdf/1804.11332.pdf, for the sake of completeness, it might also be interesting to compare LIT results to the ones obtained by just removing layers in the teacher network which have small weight norms.\n\nIn method, the last sentence before \"knowledge distillation loss\" suggests the training of student networks might not be done end-to-end. Could the authors clarify this?\nIt seems there might be a typo in the KD loss of \"knowledge distillation loss\", equation (2). Shouldn't the second term of the equation be a function of p^T and q^T (with temperature)?\n\nI would suggest changing \"sections\" to stages, as previously introduced in https://arxiv.org/pdf/1612.07771.pdf .\n\nAs for the experiments, it would be more interesting to see this kind of analysis on ImageNet (pretained resnet models are readily available).\nFigure 3, why not add hint training as well?\nFigure 4, what's the dataset used here?\n\nIn Section 4.2, it seems that the choice of the IR layer in the analysis could have a significant impact. How was the layer chosen for the ablation study experiments?\n\nThere are a few overstatements in the paper:\n- page 5, paragraph 2: FitNets proposes a general framework to transfer knowledge from a teacher network to a student network through intermediate layers. Thus, the framework itself does not require the student networks to be deeper and thinner than the teacher network.\n- page 6, \"LIT can compress GANs\": authors claim to overcome limitations of KD when it comes to applying knowledge transfer to pixel-wise architecture that do not output distributions. It seems that changing the loss and using a l2 loss instead is a rather minor change, especially since performing knowledge transfer by means of l2 (although at intermediate layers) has already been explored in FitNets.\n\nPlease add references for inception and FID scores.\nPlease fix references format in page 10.", "title": "paper well presented, experimental validation could be further improved", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byxs1hHsnm": {"type": "review", "replyto": "BkeUasA5YQ", "review": "This paper proposes to compress the model by depth. It uses hint training and knowledge distillation techniques to compress a \"deep\" network block-wisely. It shows a better compression ratio than knowledge distillation or hint training while achieving comparable accuracy performance.\n\nPros: \n1. This paper considers block-wise compression. For each block, it uses the output of the teacher's last layer as input during training, which improves the learnability of the student models. \n2. The experiments include a large range of tasks, e.g., image classification, sentiment analysis and GAN. \n\nCons:\n1. Validation accuracy is used as the performance metric, which might be over-tuned. How is the performance on testing datasets?\n2. The writing and organization of the paper need some improvement, especially the experiments section.\n3. The compression ratio (3-5) is not very impressive compared with other compression techniques with pruning and quantization techniques, such as Han et al. 2015, Hubara et al. 2016.\n\nIn summary, I think this is an interesting approach to compress deep learning models. But I think the comparisons should be done in terms of testing accuracy. Otherwise, it is hard to judge the performance of this approach. \n\n=== after rebuttal ===\nThanks for the authors' response. Some of my concerns have been clarified. I increased my rating from 5 to 6. \n", "title": "A novel approach for compressing deep learning models", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkgK05M9A7": {"type": "rebuttal", "replyto": "B1g01-Lw07", "comment": "We have updated our manuscript with test accuracy results for CIFAR100 (Figure 2c, 2d). As shown, the results have not significantly changed, and LIT outperforms all baselines.\n\nBest,\nLIT team", "title": "CIFAR100 test accuracy results"}, "H1ldq-LwRm": {"type": "rebuttal", "replyto": "SJeoktdZp7", "comment": "We have conducted a statistical test for significance for the differences throughout the paper (except table 3, see below). Our conclusions are now supported with p-values in the updated manuscript.\n\nAll the differences that we have measured in table 3 are significant. We are in the process of running multiple trials of KD and will update the manuscript when it has finished.", "title": "Statistical significance "}, "B1g01-Lw07": {"type": "rebuttal", "replyto": "BkeUasA5YQ", "comment": "Dear reviewers,\n\nWe have updated our manuscript with test accuracy for CIFAR10 (Figure 2a, 2b). We are in the process of running the other experiments for test accuracy and they will be complete by the camera-ready due date. As shown, the results do not significantly differ when we hyperparameter tune on a validation set and test on a separate test set.\n\nBest,\nLIT team", "title": "Further results with test accuracy"}, "SJeoktdZp7": {"type": "rebuttal", "replyto": "ryerNNfWam", "comment": "Thank you for the thoughtful review. We have responded to your comments inline. We have improved the manuscript based on your feedback. Several experiments are in progress and we will update the manuscript upon completion.\n\n1. Hyperparameter tuning\n\nThe hyperparameters of alpha and tau are directly taken from KD. The only hyperparameter LIT introduces is beta. We are in the process of updating our results when using separate validation set for hyperparameter selection and test set (see the response to reviewer number 2). Our initial results show that LIT outperforms KD and training from scratch by the same margins.\n\n\n2. Further analysis of training errors\n\nThank you for the suggestion. We are in the process of conducting this analysis and will respond once we have completed this analysis.\n\n\n3. Differences in table 3 are small\n\nWe are in the process of running the training procedure multiple times and will perform a statistical test upon completion. We will update the manuscript once the analysis has completed. However, the trend of LIT outperforming KD is consistent across architectures (ResNet, ResNeXt, VDCNN), datasets (CIFAR10, CIFAR100, Amazon Reviews), and tasks (image classification, sentiment analysis). Additionally, a 0.5% increase in accuracy corresponds to nearly doubling the depth of the network and corresponds to a 7% reduction in error.\n\n\n4. Pruning method for LIT.\n\nWe used standard pruning proposed by Han et al. 2015 (https://arxiv.org/abs/1506.02626), in which small weights are iteratively removed and the network is retrained at each step. We have updated the manuscript to reflect this.\n\n\n5. LIT vs pruning\n\nThank you for the comment. We have updated the manuscript to avoid overclaiming. Additionally, pruning typically requires new hardware for improved inference throughput, whereas LIT does not.\n\n\n6. Statistical significance of different loss functions.\n\nWe are in the process of running the training procedure multiple times and will perform a statistical test upon completion. Once we have the results, we will update the manuscript.\n", "title": "Thank you for your review; initial response"}, "rylNtOuWTm": {"type": "rebuttal", "replyto": "Byxs1hHsnm", "comment": "Thank you for the thoughtful review. We have responded to your comments inline. We have improved the manuscript based on your feedback. Our experiments using a test dataset are in progress and we will update the manuscript upon completion.\n\n1. Validation accuracy is used as the performance metric, which might be over-tuned. How is the performance on testing datasets?\n\nThank you for your thoughtful question. We agree with your point that validation accuracy may be over-tuned. We have started to run experiments with a separate test set, which will take some time due to our limited computational resources. We have initial results for ResNet on CIFAR10, which also show that LIT outperforms training from scratch, and KD. The results are essentially the same as the results currently in the manuscript. For ResNet-110 -> ResNet-20 we found that:\n- LIT achieves 93.19%,\n- KD achieves 92.68%,\n- Training from scratch achieves 91.68%\nOnce we have completed the rest of the results, we will update the manuscript with test accuracy.\n\nWe note that the majority of compression papers (including Han et al. 2015 and Hubera et al. 2016, Li et al. 2017 mentioned below, Furlanello et al. 2018, etc.) and the original ResNet and ResNeXt papers use validation accuracy as their primary metric. Additionally, Li et al. 2017 and Conneau et al. 2017 (the original VDCNN paper) refer to validation accuracy as \u201ctest accuracy.\u201d To ensure LIT can be compared against other methods, we will also report validation accuracy, as using a separate test set requires using a different set of data.\n\n\n2. The writing and organization of the paper need some improvement, especially the experiments section.\n\nWe have improved the presentation of the experiments section by removing some redundancy, pointing to the appendix for further experimental details, and adding details for which datasets were used. Are there other points we should address?\n\n\n3. The compression ratio (3-5) is not very impressive compared with other compression techniques with pruning and quantization techniques, such as Han et al. 2015, Hubara et al. 2016.\n\nBoth Han et al. 2015 and Hubara et al. 2016 test on older networks (e.g., VGG) where the majority of the weights are in the fully connected layers. Compressing the FC layer can achieve up to ~10x compression, while compressing the convolutional layers achieves around ~1.14x compression. As the majority of weights for these older networks are in the FC layer, this achieves high compression rates for these networks.\n\nCompressing modern networks is significantly harder. For example, Li et al. 2017 (https://arxiv.org/pdf/1608.08710.pdf) only achieves ~1.6x compression on ResNet (which achieves significantly higher accuracy than VGG). We believe our results should be compared against other methods for compressing _modern_ networks. We have made this point more clear in the paper.\n\nAdditionally, in this work, we focus on compression techniques that can improve inference throughput on existing hardware. Pruning and quantization generally require special hardware (e.g., Han et al. 2016\u2019s EIE https://arxiv.org/abs/1602.01528) for inference improvements.", "title": "Thank you for the review; initial response"}, "Hyg9X__b67": {"type": "rebuttal", "replyto": "S1e_k8xc3Q", "comment": "Thank you for the thoughtful review. We have responded to your comments inline. We have improved the manuscript based on your feedback.\n\n1. Compare LIT to removing small weight norm parts of networks.\n\nLi et al. 2017 (https://arxiv.org/pdf/1608.08710.pdf) removes small norm filters from networks, including ResNets. They achieve ~1.6x compression for the same ResNets we use in this paper, which significantly underperforms LIT. We have added this reference to the paper.\n\n\n2. Clarifying the training procedure.\n\nLIT trains with the combined loss for some number of epochs. Then, LIT trains with just the KD loss after that. We have clarified this in the manuscript.\n\n\n3. Typo in Eq. 2.\n\nWe have fixed the typo.\n\n\n4. Changing sections to stages. \n\nThank you for pointing out the standard terminology. We have updated sections to stages in the manuscript.\n\n\n5. ImageNet models\n\nUnfortunately training ImageNet models and hyperparameter tuning alpha and beta are computationally expensive. We are currently running these experiments, but they may not complete by the revision close period.\n\n\n6. Hint training in Figure 3.\n\nWe were unable to complete hint training experiments in time for the submission, but they have completed. We have added hint training to Figure 3. Briefly, hint training outperforms KD, but underperforms LIT.\n\n\n7. Dataset in Figure 4.\n\nFor Figure 4, we used CIFAR10. We have updated the caption to reflect this.\n\n\n8. Choice of IR.\n\nWe used the IR after the second stage of the ResNet. We have updated the manuscript to reflect this.\n\n\n9. Overstatements in the paper.\n\nWe have fixed these statements in the paper.\n\nWe realized the issue for GANs in the paper and conducted the L2 experiment (i.e., KD with a different loss). As we show in the updated paper (Table 2), LIT outperform this procedure.\n\n\n10. References and formatting.\n\nWe have added references to the Inception and FID scores. We have additionally fixed the formatting on the last page of citations.", "title": "Thank you for your review"}}}