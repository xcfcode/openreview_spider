{"paper": {"title": "Shift Aggregate Extract Networks", "authors": ["Francesco Orsini", "Daniele Baracchi", "Paolo Frasconi"], "authorids": ["francesco.orsini@kuleuven.be", "daniele.baracchi@unifi.it", "paolo.frasconi@unifi.it"], "summary": "Shift Aggregate Extract Networks for learning on social network data", "abstract": "The Shift Aggregate Extract Network SAEN is an architecture for learning representations on social network data.\nSAEN decomposes input graphs into hierarchies made of multiple strata of objects.\nVector representations of each object are learnt by applying 'shift', 'aggregate' and 'extract' operations on the vector representations of its parts.\nWe propose an algorithm for domain compression which takes advantage of symmetries in hierarchical decompositions to reduce the memory usage and obtain significant speedups.\nOur method is empirically evaluated on real world social network datasets, outperforming the current state of the art.", "keywords": ["Supervised Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The authors present a novel architecture, called Shift Aggregate Extract Network (SAEN), for learning representations on social network data. SAEN decomposes input graphs into hierarchies made of multiple strata of objects. The proposed approach gives very promising experimental results on several real-world social network datasets. \n \n The idea is novel and interesting. However, the exposition of the framework and the approach could be significantly improved. The authors clearly made an effort to revise the paper and improve the clarity. Yet, the paper would still certainly benefit from a major revision, to clarify the exposition and spell out all the details of the framework. \n \n A extensive use of the space in the supplement could potentially help overcoming the space limitation in the main part of the paper which can make exposition of new frameworks challenging in general. This would allow an extensive explanation of the proposed framework and related concepts. \n \n A major revision of the paper will generate a stronger submission, which we invite the authors to submit to the workshop track."}, "review": {"rJtzCrbHl": {"type": "rebuttal", "replyto": "SJP14kfEx", "comment": "Thank you for the remarks.\nWe rewrote the text in order to make it more clear (see summary of changes A1).\n\n> Specifically, I wasn't confident that I understood what the labels pi represented.\nWe improved the text to make it clear (see change A3).\n\n> the 'shift' operation is simply a summation of the representations of the member objects, and that the 'aggregate' operation simply concatenates the representations from multiple relations.\n\nThe shift step involves the use of the Kronecker product and does not average nor aggregate.\nThe aggregation phase sums over the shifted representations.\n\n> In the 'shift' step, it seems more appropriate to average over the object's member's representations h_j, rather than sum over them.\nW.r.t. the 'aggregate' step the method easily be extended to handle reweighed vector representations and make averages.\nPreliminary experiments (not in the paper) with different aggregation operators (e.g. max, mean) did not show significant advantages.\n\n> The compression technique presented in Section 2.3 requires that multiple objects at a level have the same representation. Why would this ever occur, given that the representations are real valued and high-dimensional?\n\nIt occurs provided that attributes are categorical. This is clearly stated in the revised paper (see A4).\n\n> The text is unintelligible: \"two objects are equivalent if they are made by same sets of parts for all the pi-parameterizations of the R_l,pi decomposition relation.\" \n\nThe text has been improved in many parts. For clarity, the equivalence relation between objects in the same stratum is called 'collapsibility' in the revised paper (see change A6)\n\n> The 'ego graph patterns' in Figure 1 and 'Ego Graph  Neural Network' used in the experiments are never explained in the text, and no references are given.\n\nEgo Graph Neural Network was explained at the beginning of section 4. We improved the text (see change B) in order to make it more clear and added an illustrative example with a picture (see change A2).", "title": "Response"}, "HJIdTrbSg": {"type": "rebuttal", "replyto": "r1xXahBNl", "comment": "Thank you for the remarks.\nYour summary is correct and the description that you provided is mathematically equivalent to the one of our paper.\nWe rewrote our description in order to make it easier to follow and adopted your proposal to use the term  \"membership types\" (change A3).\nYou correctly understood that a 0-neighborhood subgraph is just a vertex. We added a footnote (change A5) with the definition of r-neighborhood subgraphs/ego-graphs.\nWe added the requested details about: network architecture and how the final classification is performed (see changes B and C).\nWe provide an illustrative example of H-hierarchical decomposition (see changes A1, A2).\n\n> Important details ... aren't provided ...\nMore details are provided in the revised version of the paper (see changes B and C).\n\n> Were other choices of decomposition/object-part structures investigated, given the generality of the shift-aggregate-extract formulation?\n\nThe investigation of other \"object-part structures\" is surely an interesting research direction, but we leave it to future works due to space limitations.\n\n> What motivated the choice of \"ego-graphs\"?\n\nWe chose to use ego graphs for two reasons:\n1) this kind of pattern is studied and used in social network analysis,\n2) the number of instances of this kind of pattern scales linearly with the size (number of nodes) of the input graphs (see also [Costa & De Grave, 2010]).\n\n> Why one-hot degrees for the initial attributes?\nWe chose to use vertex degrees since this centrality measure is easy to compute.\n", "title": "Response"}, "SJEfpH-He": {"type": "rebuttal", "replyto": "S1Y0td9ee", "comment": "A) we rewrote section 2:\n    A1) we generally improved the wording,\n    A2) we added a figure in order to exemplify H-hierarchical decompositions,\n    A3) we improved the explanation of the \\pi labels (now called \"membership types\"),\n    A4) we rewrote and clarified the assumptions under which domain compression works,\n    A5) we added a footnote with the definition of r-neighborhood subgraphs/ego-graphs,\n    A6) we introduced the term 'collapsibility' for the equivalence relation between objects in the same stratum.\n\nB) We added (in section 4) the details about the network architecture and how the classification is performed.\n\nC) We added an appendix with the number of layers and hidden units that we used in our experiments.", "title": "We provide a summary of the changes that we made after the reviewers' feedback."}, "SyLUDAIQg": {"type": "rebuttal", "replyto": "ryOAun1Xe", "comment": "1) We fixed the subscripts in the second sentence of the last paragraph of page 2. Thank you for noticing that.\n2) Although our method was conceived for social network data, it can also handle other types of graphs. For the sake of completeness, we tested SAEN on the molecule and protein datasets studied in previous works (e.g. Niepert et al. 2016) and results are in line with the current state of the art. A table with comparisons has been added to the paper.", "title": "clarification about a sentence and generality of our method"}, "ryOAun1Xe": {"type": "review", "replyto": "S1Y0td9ee", "review": "1. I found the second sentence in the last paragraph of page 2 quite confusing. Could you double check all the subscripts (i's and j's) and make sure they're correct? In particular \"o_j is part of o_j\" and then the definition of the relation inverse... is this supposed to describe the set of composite objects that o_j is a part of, or the set of composite object o_i's parts (in which case the indices seem backwards).\n2. How generic is the ego-graph decomposition you propose? Specifically, a potential drawback of your approach is that the definition and implementation the graph decomposition seems specific to the type of graph being considered. Have you thought about extending this work to additional settings, and if so, what kind of decompositions would make sense? The primary competitor technique (PSCN) also included results on a number of additional datasets (MUTAG, PCT, PROTEIN, etc.), which appear to be standard datasets for graph kernels (not an expert here). Would your technique make sense for these datasets?The paper contributes to recent work investigating how neural networks can be used on graph-structured data. As far as I can tell, the proposed approach is the following:\n\n  1. Construct a hierarchical set of \"objects\" within the graph. Each object consists of multiple \"parts\" from the set of objects in the level below. There are potentially different ways a part can be part of an object (the different \\pi labels), which I would maybe call \"membership types\". In the experiments, the objects at the bottom level are vertices. At the next level they are radius 0 (just a vertex?) and radius 1 neighborhoods around each vertex, and the membership types here are either \"root\", or \"element\" (depending on whether a vertex is the center of the neighborhood or a neighbor). At the top level there is one object consisting of all of these neighborhoods, with membership types of \"radius 0 neighborhood\" (isn't this still just a vertex?) or \"radius 1 neighborhood\".\n\n  2. Every object has a representation. Each vertex's representation is a one-hot encoding of its degree. To construct an object's representation at the next level, the following scheme is employed:\n\n    a. For each object, sum the representation of all of its parts having the same membership type.\n    b. Concatenate the sums obtained from different membership types.\n    c. Pass this vector through a multi-layer neural net.\n\nI've provided this summary mainly because the description in the paper itself is somewhat hard to follow, and relevant details are scattered throughout the text, so I'd like to verify that my understanding is correct.\n\nSome additional questions I have that weren't clear from the text: how many layers and hidden units were used? What are the dimensionalities of the representations used at each layer? How is final classification performed? What is the motivation for the chosen \"ego-graph\" representation? \n\nThe proposed approach is interesting and novel, the compression technique appears effective, and the results seem compelling. However, the clarity and structure of the writing is quite poor. It took me a while to figure out what was going on---the initial description is provided without any illustrative examples, and it required jumping around the paper to figure for example how the \\pi labels are actually used. Important details around network architecture aren't provided, and very little in the way of motivation is given for many of the choices made. Were other choices of decomposition/object-part structures investigated, given the generality of the shift-aggregate-extract formulation? What motivated the choice of \"ego-graphs\"? Why one-hot degrees for the initial attributes?\n\nOverall, I think the paper contains a useful contribution on a technical level, but the presentation needs to be significantly cleaned up before I can recommend acceptance.", "title": "Question", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1xXahBNl": {"type": "review", "replyto": "S1Y0td9ee", "review": "1. I found the second sentence in the last paragraph of page 2 quite confusing. Could you double check all the subscripts (i's and j's) and make sure they're correct? In particular \"o_j is part of o_j\" and then the definition of the relation inverse... is this supposed to describe the set of composite objects that o_j is a part of, or the set of composite object o_i's parts (in which case the indices seem backwards).\n2. How generic is the ego-graph decomposition you propose? Specifically, a potential drawback of your approach is that the definition and implementation the graph decomposition seems specific to the type of graph being considered. Have you thought about extending this work to additional settings, and if so, what kind of decompositions would make sense? The primary competitor technique (PSCN) also included results on a number of additional datasets (MUTAG, PCT, PROTEIN, etc.), which appear to be standard datasets for graph kernels (not an expert here). Would your technique make sense for these datasets?The paper contributes to recent work investigating how neural networks can be used on graph-structured data. As far as I can tell, the proposed approach is the following:\n\n  1. Construct a hierarchical set of \"objects\" within the graph. Each object consists of multiple \"parts\" from the set of objects in the level below. There are potentially different ways a part can be part of an object (the different \\pi labels), which I would maybe call \"membership types\". In the experiments, the objects at the bottom level are vertices. At the next level they are radius 0 (just a vertex?) and radius 1 neighborhoods around each vertex, and the membership types here are either \"root\", or \"element\" (depending on whether a vertex is the center of the neighborhood or a neighbor). At the top level there is one object consisting of all of these neighborhoods, with membership types of \"radius 0 neighborhood\" (isn't this still just a vertex?) or \"radius 1 neighborhood\".\n\n  2. Every object has a representation. Each vertex's representation is a one-hot encoding of its degree. To construct an object's representation at the next level, the following scheme is employed:\n\n    a. For each object, sum the representation of all of its parts having the same membership type.\n    b. Concatenate the sums obtained from different membership types.\n    c. Pass this vector through a multi-layer neural net.\n\nI've provided this summary mainly because the description in the paper itself is somewhat hard to follow, and relevant details are scattered throughout the text, so I'd like to verify that my understanding is correct.\n\nSome additional questions I have that weren't clear from the text: how many layers and hidden units were used? What are the dimensionalities of the representations used at each layer? How is final classification performed? What is the motivation for the chosen \"ego-graph\" representation? \n\nThe proposed approach is interesting and novel, the compression technique appears effective, and the results seem compelling. However, the clarity and structure of the writing is quite poor. It took me a while to figure out what was going on---the initial description is provided without any illustrative examples, and it required jumping around the paper to figure for example how the \\pi labels are actually used. Important details around network architecture aren't provided, and very little in the way of motivation is given for many of the choices made. Were other choices of decomposition/object-part structures investigated, given the generality of the shift-aggregate-extract formulation? What motivated the choice of \"ego-graphs\"? Why one-hot degrees for the initial attributes?\n\nOverall, I think the paper contains a useful contribution on a technical level, but the presentation needs to be significantly cleaned up before I can recommend acceptance.", "title": "Question", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}