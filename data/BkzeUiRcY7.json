{"paper": {"title": "M^3RL: Mind-aware Multi-agent Management Reinforcement Learning", "authors": ["Tianmin Shu", "Yuandong Tian"], "authorids": ["tianmin.shu@ucla.edu", "yuandong@fb.com"], "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.", "abstract": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.", "keywords": ["Multi-agent Reinforcement Learning", "Deep Reinforcement Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper addresses a variant of multi-agent reinforcement learning that aligns well with real-world applications - it considers the case where agents may have individual, diverging preferences. The proposed approach trains a \"manager\" agent which coordinates the self-interested worker agents by assigning them appropriate tasks and rewarding successful task completion (through contract generation). The approach is empirically validated on two grid-world domains: resource collection and crafting. The reviewers point out that this formulation is closely related to the principle-agent problem known in the economics literature, and see a key contribution of the paper in bringing this type of problem into the deep RL space.\n\nThe reviewers noted several potential weaknesses: They asked to clarify the relation to prior work, especially on the principle-agents work done in other areas, as well as connections to real world applications. In this context, they also noted that the significance of the contribution was unclear. Several modeling choices were questioned, including the choice of using rule-based agents for the empirical results presented in the main paper, and the need for using deep learning for contract generation. They asked the authors to provide additional details regarding scalability and sample complexity of the approach.\n\nThe authors carefully addressed the reviewer concerns, and the reviewers have indicated that they are satisfied with the response and updates to the paper. The consensus is to accept the paper.\n\nThe AC is particularly pleased to see that the authors plan to open source their code so that experiments can be replicated, and encourages them to do so in a timely manner. The AC also notes that the figures in the paper are very small, and often not readable in print - please increase figure and font sizes in the camera ready version to ensure the paper is legible when printed."}, "review": {"ByepuqhpUE": {"type": "rebuttal", "replyto": "SJesiGugx4", "comment": "As promised, we have released the code of this paper:\n\nhttps://github.com/facebookresearch/M3RL\n\nThanks all for the interest! ", "title": "Code released. "}, "Hye-Lo29hm": {"type": "review", "replyto": "BkzeUiRcY7", "review": "\nThis paper studies the problem of generating contracts by a principal to incentive agents to optimally accomplish multiagent tasks. The setup of the environment is that the agents have certain skills and preferences for activities, which the principal must learn to act optimally. The paper takes a combined approach of agent modeling to infer agent skills and preferences, and a deep reinforcement learning approach to generate contracts. The evaluation of the approach is fairly thorough.\n\nThe main novel contribution of the paper is to introduce the principal-agent problem to the deep multiagent reinforcement learning literature.\n\nMy concerns are:\n- The paper should perform a literature search on related work from operations research, including especially principal-agent problems, which are not currently surveyed, and perhaps also optimal scheduling problems.\n- How do the problems introduced either map onto real applications or map onto environments studied in existing literature (such as in operations research)?\n- More details should be given on the mind tracker module.\n- Is it necessary to use deep reinforcement learning for contract generation?  If the agent modeling is good, the optimal contracts look like they are probably simple to compute directly in the environments studied.\n\nOverall, the paper is somewhat interesting and relatively technically sound, but the contribution seems marginal. The problems studied seem pulled out a hat, when they could be situated in specific existing literature.\n", "title": "An interesting exploration of RL for principle-agent problems", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SyggSiwZp7": {"type": "review", "replyto": "BkzeUiRcY7", "review": "Summary:\n\nThis paper proposes a way to train a manager agent which would manage a bunch of worker agents to achieve a high-level goal. Each worker has its own set of skills and preferences and the manager tries to assign sub-tasks to these agents along with bonuses such that the agents can even perform tasks that are not preferred by them. Authors achieve this by training a manager which tracks the skills and preferences of the agents on the fly. Authors have done an extensive analysis of the proposed approach in two simple domains: resource collection and crafting.\n\nMajor comments:\n\nThis paper focuses on multi-agent settings with self-interested agents. The problem formulation and the solution are novel enough. Experiments are on toy domains with very few goals and sub-task dependencies. However, authors have done a good job in doing an extensive analysis of the proposed approach.\n\n1.\tCan you comment about the scalability of the proposed solution when the number of possible subtasks increases? When the sub-task dependency graph size increases?\n\n2.\tWhat is the reason for using rule-based agents in all the experiments? It would have been more useful if all the analysis are done with RL agents rather than rule-based agents. It would also make the paper stronger.\n\n3.\tAre the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper. I would increase my rating if the authors are willing to release the code to reproduce all the results reported in the paper.\n\n\nMinor comments:\n\n1.\tPage 3, line 9: \u201ctypical\u201d -> \u201ctypically\u201d\n2.\tPage 3, \u201cintention\u201d section: \u201cBased on the its reward ..\u201d Check grammar.\n3.\tPage 5, last line: \u201cthe total quantitative is 10\u201d check grammar.\n4.\tPage 8, conclusions, second line: \u201cnad\u201d -> \u201cand\u201d\n5.\tPage 8, conclusions, 4th line: \u201ccombing\u201d -> \u201ccombine\u201d\n", "title": "Multi agent RL with self interesting agents: New formulation and solution in simple environments.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryl9TkfK0m": {"type": "rebuttal", "replyto": "BkzeUiRcY7", "comment": "We thank all reviewers for their comments and suggestions.  We have revised our submission and also plan to release our code. In the revision, we i) discussed the connection between our work and principal-agent problems including how our work differs from the traditional approaches in classical principal-agent problems, ii) added implementation details about the mind tracker module, and iii) fixed the typos.", "title": "Submission revision"}, "rJlQL4SGC7": {"type": "rebuttal", "replyto": "HkgbGMxEnm", "comment": "Thank you for your reviews.  Here are our responses to your questions:\n\n1. Clarify how the skills of agents play a role in the problem setup\nWe clarify the definition of skills and how it influences the manager\u2019s decision as follows.  \n\ni) As defined in Section 3, an agent\u2019s skill depends on its state transition probabilities and its policy. The state transition probabilities define if a resource can be collected by an agent (i.e., whether the \u201ccollect\u201d action executed by this agent will have real effect), and it is equivalent to a binary value for each resource in Resource Collection. The agent\u2019s skill also depends on its policy because it affects how fast an agent can achieve a goal. E.g., when the agent has a suboptimal policy, it may not be able to reach a goal within the time limit even though it actually can collect the resource if given more time.\n\nii) The skills are completely hidden from the manager. It can be inferred by the manager based on the performance history, and also on the estimated worker policies by IL. However, only checking whether a goal is reached is not sufficient to determine skills. Failing to reach a goal may be a result of several reasons -- it may be because i) the bonus in the contract is too low, ii) the contract terminates prematurely before the agent can reach the goal, or iii) the assigned task depends on another task which has not been finished yet. So the manager needs to infer agents\u2019 skills, preferences, and the task dependency jointly through multiple trials.\n\n2. Is maximizing utility justified?\nMaximizing utility is actually the setup in similar problems in economics. Just like those problems (e.g., mechanism design), this paper focuses on scenarios where agents won\u2019t truthfully or clearly reveal its skills and preferences to the manager, and do not always behave optimally. As we stated in the paper, maximizing utility is more realistic, and typically the span of the decision making process of the manager is much shorter than the time needed for improving worker agents. Let\u2019s consider a simple scenario. An agent is unable to collect a certain kind of resource. By maximizing its utility, it may still accept the contract and go to that resource. Once a resource is occupied by this agent, other agents can no longer collect it according to our setting. This means that the resource will never be really collected.\n\nAs an empirical evidence,  you may compare the S2 and S3 settings with S1 in Resource Collection. In S2 and S3, workers may prefer a task that it can not perform, which should never happen in the case of maximizing return. As a result (shown in Figure 4b and Figure 4c), the training difficult significantly increases.\n\n3. Are there alternate ways to overcome maintaining the UCB explicitly, especially for the number of time-steps? \nYes, there are ways to overcome this. First, we can define small time intervals instead of maintaining statistics for each step (i.e., combining statistics in every dT consecutive steps will reduce the complexity to 1 / dT of the original size). Note that this has been done in results shown in Appendix C.1, where dT also means that for every dT steps, the manager can only change the contracts once. Second, we may define a maximum number of steps to be considered in the performance history, which can be determined by the upper bound of the execution time for a subtask, and can be smaller than the step limit of the whole episode.\n\n4. What are the units for rewards in the plots?\nIt is the average per episode. The reward is defined as in Section 5.1.1 and Section 5.1.2 without any rescaling. We have added this in the caption.\n\n5. Typos\nThank you for pointing out these typos. We will fix them in the next revision.\n", "title": "Author response"}, "S1lrcUBMA7": {"type": "rebuttal", "replyto": "SyggSiwZp7", "comment": "Thank you for your reviews and comments. We address your questions as follows.\n\n1. Scalability of the proposed solution\nFrom our current results, you may see that our approach has a decent scalability -- even though we doubled the subtasks and also introduced additional dependency in Crafting compared to Resource Collection, it does not need much more episodes for converging to optimal policies, where our agent-wise exploration plays an important role. Generally speaking, deploying more present workers coupled with our agent-wise exploration should significantly improve the learning efficiency and overcome the challenges introduced from more substasks or a larger dependency graph. In addition, the computational complexity is linear in terms of the number of agents, so our approach is also scalable when there are more agents.\n\n2. What is the reason for using rule-based agents in all the experiments?\nWe have actually used RL agents as well (Appendix C.3), and it showed that our approach also works when workers are RL agents. In the main results, we focus on rule-based agents because it is computationally demanding to train a large population of RL agents, and our focus was not about the worker policies but rather how the manager assesses the workers\u2019 mental states and encourages an optimal collaboration accordingly. In this paper, using a cheap rule-based implementation with randomness has demonstrated the effect of different components of our approach. \n\n3. Are the authors willing to release the code?\nYes, we do plan to open source our implementation. Specifically, the game environment and the worker agents were implemented in Python and it runs at a speed of more than 300 steps per second. We used PyTorch as the framework for implementing all the network modules. Typically it took < 10 hours to get a converged result by our approach on a single Nvidia Tesla V100 GPU.\n\n4. Typos\nThanks for pointing out these typos. We will fix them in the next revision.", "title": "Author response"}, "H1lU9Hrf0X": {"type": "rebuttal", "replyto": "Hye-Lo29hm", "comment": "Thank you for your comments and suggestions. We respond to your questions and concerns as follows.\n\n1. Connection with principal-agent problems.\nThank you for pointing this out. We really appreciate it. The problem we address is indeed closely connected to principal-agent problems, or moral hazard problems in economics, which considers whether the agent makes the best choice for what the principal delegates (e.g., a plumber might make more money by suggesting an overhaul rather than a short-term fix). In this setting, there are a lot of issues to be modeled, e.g., information asymmetry between principals and agents, how to setup incentive cost, how to infer agents\u2019 types and how to monitor their behaviors, etc. Traditional approaches [1] in economics build mathematical models to address these issues separately, leading to complicated models with many tunable parameters. In comparison, our paper provides a practical end-to-end computational framework to address this problem in a data-driven way, once the agents\u2019 utility function is written down as a combination of principal\u2019s request and its own preference (Eqn. 1). Moreover, this framework is adaptive to changes of agents\u2019 preferences and capabilities, which very few papers in economics have addressed. \n\nBecause of the connection to principal-agent problems and the data-driven nature of the proposed method, there could be a broad number of practical applications.\n\nWe will incorporate a more thorough literature reviews in the next revision. \n\n[1] The theory of incentives: the principal-agent model, Jean-Jacques Laffont, 2001\n\n2. More details should be given on the mind tracker module.\nWe will explain more implementation details in the appendix in the next revision. We will also release the code.\n\n3. Is it necessary to use deep reinforcement learning for contract generation?\nAs stated in the introduction, one of the main points of this work is about incomplete information. I.e., we do not know the true agent models and their mental states, and also do not assume that the task dependency is known. In real world problems, we indeed can not assume that a manager knows the exact nature of other agents. So we want to train a manager that can quickly model worker agents through observations and simultaneously generate optimal contracts. In contrast, traditional methods do not consider task dependency, and usually assume agent types are either known or follow a given distribution. Also, deep models are flexible enough to handle complicated interactions between agents and changes of settings. Thus, deep RL is a more suitable approach than traditional methods under the incomplete information setting. \n", "title": "Author response"}, "HkgbGMxEnm": {"type": "review", "replyto": "BkzeUiRcY7", "review": "This paper studies the problem of coordinating many strategic agents with private valuation to perform a series of common goals. The algorithm designer is a manager who can assign goals to various agents but cannot see their valuation or control them explicitly. The manager has a utility function for various goals and wants to maximize the total revenue. The abstract problem is well-motivated and significant and is an entire branch of study called algorithmic mechanism design. However often many assumptions have to be made to make the problem mathematically tractable. In this paper, the authors take an empirical approach by designing an RL framework that efficiently maximizes rewards across many episodes. Overall I find the problem interesting, well-motivated. The paper is well-written and contains significant experiments to support its point. However, I do not have the necessary background in the related literature to assess the significance of the methods proposed compared to prior work and thus would refrain from making a judgment on the novelty of this paper in terms of methodology. Here are some of my comments/questions to the author on this paper.\n\n\n(1) I want to clarify how the skills of the agents play a role in the problem setup. Does it show up in the expression for the manager's reward? In particular, does it affect the Indicator for whether a goal is completed Eq. (2) via a process that need not be explicitly modeled but can be observed via a feedback of whether or not the goal is completed? So in the case of resource collection example, the skill set is a binary value for each resource, whether it can be collected or not? \n\n(2) Related to the first point, the motivation for modeling the agents as maximizing their utility is the assumption that agents do not know their skills. I am wondering, is this really justified? Over the course of episodes, can the agents learn their skills based on the relationship between their intention and the goals they achieve? In the resource collection example, when they reach a resource and are not able to collect it, they understand that they do not have the corresponding skill. Is there a way to extrapolate the results from this paper to such a setting? \n\n(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps). Are there alternate ways to overcome maintaining the UCB explicitly, especially for the number of time-steps? \n\nSome minor comments on the presentation.\n\n(1) What are the units for rewards in the plots? Is it the average per episode reward? It would be good to mention this in the caption.\n\n(2) There are a few typos in the paper. Some I could catch was,\n\n- Last line in Page 5: \"quantitative\" -> \"quantity\"\n- Page 8: skills nad preferences -> skills and preferences\n- Page 8: For which we combining -> for which we combine", "title": "Multi-agent Management using RL", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}}}