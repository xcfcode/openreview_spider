{"paper": {"title": "Improved Training Techniques for Online Neural Machine Translation", "authors": ["Maha Elbayad", "Laurent Besacier", "Jakob Verbeek"], "authorids": ["maha.elbayad@inria.fr", "laurent.besacier@univ-grenoble-alpes.fr", "jakob.verbeek@inria.fr"], "summary": "Improved training of wait-k decoders for online machine translation", "abstract": "Neural sequence-to-sequence models are at the basis of state-of-the-art solutions for sequential prediction problems such as machine translation and speech recognition. The models typically assume that the entire input is available when starting target generation. In some applications, however, it is desirable to start the decoding process before the entire input is available, e.g. to reduce the latency in automatic speech recognition. We consider state-of-the-art wait-k decoders, that first read k tokens from the source and then alternate between reading tokens from the input and writing to the output. We investigate the sensitivity of such models to the value of k that is used during training and when deploying the model, and the effect of updating the hidden states in transformer models as new source tokens are read. We experiment with German-English translation on the IWSLT14 dataset and the larger WMT15 dataset. Our results significantly improve over earlier state-of-the-art results for  German-English translation on the WMT15 dataset across different latency levels.", "keywords": ["Deep learning", "natural language processing", "Machine translation"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a method of training latency-limited (wait-k) decoders for online machine translation. The authors investigate the impact of the value of k, and of recalculating the transformer's decoder hidden states when a new source token arrives. They significantly improve over state-of-the-art results for German-English translation on the WMT15 dataset, however there is limited novelty wrt previous approaches. The authors responded in depth to reviews and updated the paper with improvements, for which there was no reviewer response. The paper presents interesting results but IMO the approach is not novel enough to justify acceptance at ICLR. \n"}, "review": {"BylTPwWioS": {"type": "rebuttal", "replyto": "rke3OxSKwr", "comment": "Based on the suggestions of the reviewers, we made a few updates to the paper:\nMade the comparison to the original wait-k paper [1] clearer, highlighting the differences in the encoder side.\nAdded training time details of our models as compared to the baselines and our implementation of STACL [1].\nAdded decoding speeds on GPU and CPU with and without decoder states update as well as the decoding speed of our implementation of [1].\n\n[1] Ma et al. \u201cSTACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework.\" ACL 2019", "title": "Paper updates"}, "rJxSt6UWsr": {"type": "rebuttal", "replyto": "HJgHncMCtB", "comment": "Regarding \"The masking and using causal attention for the transformer has been proposed in previous works.\"\n\nUni-directional encoders for online machine translation were previously used with RNN-based architectures, we are not familiar with existing work that uses causal attention for transformer NMT models. In [1] the encoder was not causal which means that for every time-step the encoder states have to be updated. It would be great if you could share the references you were thinking about.\n\n\nRegarding \"The hidden state updates provide some gains for the model but also makes the decoder more expensive.\"\n\nCompared to our model with caching, we agree that the update of the decoder states is expensive. However if we\u2019re comparing to [1] we basically re-allocated the cost from updating the encoder states to updating the decoder states instead and we get better performances with this new allocation.\n\nRegarding \"The training with multiple k provides similar gain as training with one k larger than the value used at the inference time. Overall the contributions are limited.\"\n\nTraining with a single large k does not improve the performance on smaller values of k.\nIf we look for example at figure 4.c, for an average lagging of 2, there is a difference of almost 3 BLEU points between the model trained with k=9 and the one trained with k in [1,...,5].\n\nRegarding \"There is quite some room for this paper to improve its clarify, especially in terms of annotations and explaining the proposed ideas.\"\n\nPlease let us know if there are any specific annotations or concepts you think need rewriting, we would gladly make it clearer in the updated paper.\n\n[1] Ma et al. \u201cSTACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework.\" ACL 2019\n", "title": "Thank you for the review and comments"}, "r1xU1p8bsB": {"type": "rebuttal", "replyto": "B1xFB-O0FB", "comment": "Regarding \"1) in section 3.1.2, the authors mentioned two adaptations. Is the proposed AR encoder uni-directional? If the AR encoder is uni-directional, then I would be surprised that the uni-directional encoder outperforms the bi-directional encoder in the original wait-k model. \"\n\nThe original wait-k paper [2] and the subsequent paper [1,3] use an \u2018incremental encoder\u2019. It is bidirectional for all tokens before a cursor g(t). Therefore at every decoding time-step t, with increased g(t) the encoder states have to be updated. It is interesting, indeed perhaps surprising, to see that in the context of online machine translation the unidirectional encoder outperforms the bidirectional one. All of our transformer model for online translation use uni-directional encoders (MT) and when evaluated without the  \u2018update\u2019, the decoders are equivalent to the ones in [2] which suggests that the uni-directional encoders are better suited for online translation.\n\nRegarding \"For the second bullet, I think the original wait-k also did the same thing(they mentioned this in the paper clearly). So there is nothing new about bullet 2. \"\n\nIndeed, there is nothing new about the masking in the encoder-decoder interaction but we wanted to explicitly define all the masks used in the architecture, and list the differences with respect to an offline transformer model.\n\nRegarding \"2) the idea mentioned in fig 1 is very similar to [1]. I suggest the authors compare with the aforementioned methods. \"\n\nWe will compare with the method in [1]. However In [1] the authors suggest optimizing the decoding along a set of paths sampled within an area of interest (similar to the gray area in our figure 2.) but ended up optimizing along the two boundary paths. What we suggest here is to optimize the decoding in \u2018all\u2019 the cells of the gray area. This is possible with the pervasive attention architecture where the cell state is independent from the path we followed to arrive there. With the transformer model, optimizing the full area is not evident, so we ended up selecting a few wait-k paths and thus using training strategy similar to the one in [1]. There is also the fact that [1] aims to learn dynamic read/write decision with a special token added to the vocabulary to represent the \u2018Read\u2019 action. Unfortunately, [1] only reports results for Chine-English (and reverse) translation, preventing direct comparison to  our results and those of [2,3].\n\nRegarding \"3) updating the hidden state of the decoder introduces more complexity during the inference time. I recommend the authors to perform some analysis about decoding time with CPU and GPU.\"\n\nWe will include decoding times on CPU and GPU for our models and compare them to the approach in [1].\n\nRegarding \"4) it is also interesting to show more comparison between different models' training time with the original STACL. \"\n\nDuring the training of our models on a given wait-k path we do not update the encoder states nor the previous decoder states. This makes our training time comparable to an offline transformer. With [1] however, given a target sequence y of length |y| there are |y| encoder forward passes to evaluate the states associated with each context size g(t).\n\nBetween our single k training and multiple k, the training time is higher if for each sentence pair we run a separate forwards pass for each value of k. Alternatively, for each batch of sentence pairs we can sample a value of k and only use the loss for the wait-k path for that value of k for that batch. This way we end up with a comparable training times.\n\n[1] Zheng et al. \"Simultaneous Translation with Flexible Policy via Restricted Imitation Learning\" ACL 2019\n[2] Ma et al. \u201cSTACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework.\" ACL 2019\n[3] Zheng et al. \u201cSimpler and faster learning of adaptive policies for simultaneous translation.\"  EMNLP 2019\n", "title": "Thank you for the review and comments!"}, "HJgHncMCtB": {"type": "review", "replyto": "rke3OxSKwr", "review": "This work apply the wait-k decoding policy on the 2D CNN-based architecture and transformer.  In the transformer-based model the author proposed to recalculate the decoder hidden states when a new source token arrives. The author also suggested to train with multiple k at the decoder level with shared encoder output. The experiments showed that the transformer model provide the best quality on IWSLT14 En-De, De-En, and WMT15 De-EN.\n\nThe masking and using causal attention for the transformer has been proposed in previous works. The hidden state updates provide some gains for the model but also makes the decoder more expensive. The training with multiple k provides similar gain as training with one k larger than the value used at the inference time. Overall the contributions are limited.\n\nThere is quite some room for this paper to improve its clarify, especially in terms of annotations and explaining the proposed ideas.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 3}, "B1xFB-O0FB": {"type": "review", "replyto": "rke3OxSKwr", "review": "This paper extends the idea of prefix-to-prefix in STACL and proposes two different variations. The authors did some interesting experiments between caching and updating decoder.\n\nMy questions are as follows:\n\n1) in section 3.1.2, the authors mentioned two adaptations. Is the proposed AR encoder uni-directional? If the AR encoder is uni-directional, then I would be surprised that the uni-directional encoder outperforms the bi-directional encoder in the original wait-k model. For the second bullet, I think the original wait-k also did the same thing(they mentioned this in the paper clearly). So there is nothing new about bullet 2. \n\n2) the idea mentioned in fig 1 is very similar to [1]. I suggest the authors compare with the aforementioned methods. \n\n3) updating the hidden state of the decoder introduces more complexity during the inference time. I recommend the authors to perform some analysis about decoding time with CPU and GPU.\n\n4) it is also interesting to show more comparison between different models' training time with the original STACL. \n\n[1] Zheng et al. \"Simultaneous Translation with Flexible Policy via Restricted Imitation Learning\" ACL 2019", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "SyxQenqMcH": {"type": "review", "replyto": "rke3OxSKwr", "review": "Sorry, this is a very quick review.\n\nThe paper is about an improved method of training latency-limited (wait-k) decoders for transformer-based machine translation, in which the right context is limited to various numbers.  So it's a kind of augmentation method that's well matched to the test scenario.  At least, that is my understanding.\n\nI am not really an MT expert so cannot comment with much authority.  On the plus side the paper says it sets a new state of the art for latency-limited decoding for a German-English MT task, and it involves transformers, which are quite hot right now so the  attendees might find it interesting because of that connection.\nOn the minus side, it is all really quite task-specific.\nI am putting weak accept.. regular-strength accept might be my other choice.\nIt's all with low confidence.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}}}