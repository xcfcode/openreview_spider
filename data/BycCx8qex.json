{"paper": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"], "summary": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"]}, "meta": {"decision": "Reject", "comment": "This work proved to be a controversial submission. This paper has two main components: 1) a neural framework TBRU/DRAGNN, 2) experimental results on some NLP tasks. Generally there was lack of consensus about the originality of (1) and a general feeling that even if there are aspect of novelties, that the paper was lacking clarity about is contributions One reviewer was an outlier, highlighting the benefit of the ability \"incorporate dynamic recurrent connections through the definition of the transition system\" which is claimed to be really novel. Others claim this is specific to the framework used. Negative reviewers felt that (1) is probably not novel within itself and represents a slight departure from stack-lstm. The controversy here is whether DRAGNN is simple \"software engineering with no inherent \"free things\" that would lead to impact within the community. This question of impact is also inherent to (2), in particular whether the authors really got new benefit of using DragNN or whether these are \"reimplementations of things in the literature\". I felt the reviewers did seem to put in due diligence here, so the recommendation would be to put further effort into clarification and further back up of novelty claims in future versions."}, "review": {"B1dR02-Eg": {"type": "rebuttal", "replyto": "rkqw-ubNg", "comment": "Thank you for the detailed review. We want to clarify our statement about the \"goal of DRAGNN\" -- when we said \"the goal of DRAGNN is modularity\" in our comment, we should have said \"**a** goal of the DRAGNN **implementation** is modularity\". This may have led to some confusion about why we wrote this paper and what our contributions are. This purpose of this paper is not to describe a software system or a means to unify existing neural architectures: We are proposing a framework for neural architectures that allows **new** types of cross-task representation learning and dynamic neural models.\n\nIn particular:\n\nRe: The purpose of DRAGNN\n\n- The primary purpose of our framework is **not code re-use or engineering**, but (1) dynamic neural connections and (2) new, better multi-task learning through re-use of learned representations between models.  We show empirically that both of these lead to improvements over popular architectures and enable new types of stacked neural architectures:\n\n * Table 1 shows that seq2seq+attention is less accurate (and quadratic) vs. an explicit dynamic input connection (which is linear)\n * Table 2 shows that stack-propagation is better than single encoder->multiple decoders. This model is a new dynamic neural architecture enabled by our framework (backpropagation across tasks along connections as a function of shift/reduce decisions.) \n * Table 3 shows we can build new types of stacked models by including multiple TBRU's that produce dependency trees in different directions. This is another new dynamic neural architecture enabled by our framework.\n\n- Again, the primary purpose of DRAGNN is not about the implementation, nor to make it easy to re-implement other papers. The goal is to provide *new* types of multi-task learning and compositional representations. The modularity of TBRU's makes this easy to do, but it's not required if one is just using something like bi-LSTMS (see for example: https://openreview.net/pdf?id=SJZAb5cel).\n\n- We discussed the implementation because TensorFlow does not efficiently support building a compute graph dynamically. TBRU's provide a way to express dynamic connections that can be implemented efficiently with a fixed compute graph (assuming access to while loops). Like we said before, we are actively pursuing an open source release in TensorFlow, not proposing an alternative to TensorFlow, Dynet, etc. DRAGNN is a meta-framework that must be implemented on top of an existing NN toolbox; it requires automatic differentiation, and it does not compete with NN toolboxes.\n\nRe: DRAGNN vs. VW\n\nWe claimed VW is orthogonal for two reasons: (1) recurrent, dynamic neural networks and (2) multi-task learning with shared multi-task representations.\n\n- VW's credit assignment compiler is an excellent tool for learning structured predictors for a single task. Like we said, DRAGNN is aimed to enable multi-task learning and learn compositional representations. How to do this properly is still an open research question.\n\n- According to VW's github page, VW supports feed-forward networks of a particular structure, with non-differentiable links (i.e. links through the features). We apologize if we were mistaken -- can you please clarify how to implement recurrent models such as LSTMs in VW?\n\nRe: Limitations of DRAGNN\n\n- In terms of implementation, DRAGNN is built on TensorFlow, and can be interspersed with vanilla TF code. Therefore, any model implemented in TensorFlow can utilize DRAGNN, and vis versa.\n\n- Dynamic programming is not something that fits into TF easily, and unless the program can be easily mapped to a TBRU it won't fit into DRAGNN easily either. For example, if the CRF functions to compute structured attention in https://openreview.net/pdf?id=HkE0Nvqlg are not implemented in TF, you would need to port them first. However,\n\n- If you're not doing multi-task learning and/or not trying to use dynamic connections in the network, there's not much point to shoehorning your model into the DRAGNN framework to begin with. If you are, then DRAGNN provides a useful foundation to build your model. ", "title": "Reponse to final review"}, "rk4QakoQe": {"type": "rebuttal", "replyto": "HyI9jXyXe", "comment": "1. What are the limitations of the DRAGNN framework? (e.g., Can we implement seq2seq with soft attention?)\n\nThe goal of DRAGNN is to express complex neural network architectures in terms of pluggable modules; this is also the main limitation. In other words, given an arbitrary equation to implement, one needs to translate it into a TBRU specification if it doesn\u2019t fit into an existing library of modules. \n\nThat being said, seq2seq with soft attention is easily represented in DRAGNN and we actually included it in the experiments. We simply set the recurrence function to pull from all input tokens and include an attention mechanism in the network cell. We actually compare to this in our experiments; rows 3 & 4 in Table 1 correspond to soft attention\n\n2. How does DRAGNN related to Vowpal Wabbit's imperative learning-to-search learning framework? See recent paper at NIPS https://arxiv.org/abs/1406.1837 (which has been on arxiv since 2014) as well as the author's tutorial at ICML 2015. \n\nVW has a flexible approach for learning-to-search built on a fixed model structure for a single structured prediction task. In contrast, DRAGNN is a simple approach for learning-to-search built on a flexible, modular model structure that handles multiple tasks and heterogenous datasets. Note that \u201cjoint\u201d in the VW framework refers to jointly reasoning about multiple variables, not jointly reasoning about multiple tasks with different annotations/datasets/loss functions. In that sense, the VW framework is orthogonal to DRAGNN, and we can combine the learning approach described in that work to DRAGNN in the future. \n\n3. Will a framework be released accompanying paper? The examples given are a little hand-wavey, which got me wondering: How many lines of code are required to expresses each of the models? Also, How long does it take to train and run? \n\nSee answer above. We\u2019ll add more detailed descriptions of the models. \u201cLines of code\u201d is a bit hard to measure: given the set of transition systems, recurrent units, and recurrence functions we implemented, expressing the models are just filling out a configuration file (e.g. TBRU #1 are these four things, TBRU #2 are these four things.) Typically more code is required to specify the training configuration (e.g. which datasets / loss functions map to which components) than the actual model itself.\n\nTraining a parsing model like in Table #1 took about 5-6 hours on a single CPU; the summarization models took about a day to train the most complex multi-task models.\n\n4. How does one include beam search or dynamic programming inference with a DRAGNN? To implement the full Parsey McParseFace model, users need this to implement approx global normalization and decoding via beam search.\n\nDRAGNN is designed to allow batching, and beam can be considered a batch with a bit more constraints and book-keeping. The structure of computation does not change. We are currently porting the SyntaxNet style beam training to DRAGNN so we can implement a recurrent version of Parsey. \n\n5. Why limit training to maximum likelihood training, especially given that we know such training is prone to failure due to \"exposure bias\" and many good alternatives exist, including DAgger (Ross et. al, 2011), LOLS (Chang et al., 2015) and reinforcement learning algorithms (e.g., policy gradient).\n\nThis framework is extendable to these RL style algorithms and we intend to explore these in future work. We chose ML style training because it was the simplest starting point for this framework. Furthermore, since we use iterative training rather than training with a global loss for all the components, some errors of this kind have been taken care of since the previous components are dynamically unrolled (instead of requiring a joint set of gold annotations for every task on every example.)\n", "title": "response"}, "rkdgTJiQx": {"type": "rebuttal", "replyto": "S13PQBeme", "comment": "1. Explain the differences between a stack LSTM and Example 6 (compositional representation from arc-standard dependency parsing). It\u2019s clear to me that they are indeed different, but I\u2019d love to better understand the nuances there, and it seems like an important comparison to draw explicitly in the paper.\n\nThe main difference is in the composition function. Stack LSTM uses a composition function which combines the embedding of head (h), the dependent (d), and the syntactic relation (r) to be the compositional representation for the arc.  In contrast, we use dynamic recurrences to represent composition: the hidden representation of the last decision that modified the i\u2019th token is used as the compositional representation. This makes the model quite a bit simpler and more efficient without sacrificing accuracy. \n\n2. Any plans to release accompanying code? \n\nWe\u2019re actively pursuing an open-source release in TensorFlow.\n\n3. How does the concurrent ICLR submission https://arxiv.org/pdf/1611.01734v2.pdf fit into this framework, if at all?\n\nIt is quite easy to fit the deep biaffine attention into DRAGNN. Consider the following setup with three TBRU\u2019s:\n\nTBRU #1: Left-to-right shift-only LSTM\nTBRU #2: Right-to-left shift-only LSTM\nTBRU #3: Head-selection transition system, defined as follows:\n\nState = list of heads 1, \u2026, i \nDecision = select a head for i+1\nRecurrence = Representations for all tokens from Bi-LSTM models (TBRU\u2019s #1 and #2)\nNetwork cell = Biaffine score between head and all possible modifiers\n\nNote that we will include this implementation with any release of the framework, and with our framework, we can include training objectives such as POS tagging quite easily.", "title": "response"}, "S13PQBeme": {"type": "review", "replyto": "BycCx8qex", "review": "\u2014 Explain the differences between a stack LSTM and Example 6 (compositional representation from arc-standard dependency parsing). It\u2019s clear to me that they are indeed different, but I\u2019d love to better understand the nuances there, and it seems like an important comparison to draw explicitly in the paper.\n\u2014 Any plans to release accompanying code?\n\u2014 How does the concurrent ICLR submission https://arxiv.org/pdf/1611.01734v2.pdf fit into this framework, if at all?\nThe authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google\u2019s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. \n\nThe most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). \n\nThe \u201craison d\u2019etre,\u201d in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework \u2014 emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I\u2019d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. \n\nOverall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded. ", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1qE6OVVg": {"type": "review", "replyto": "BycCx8qex", "review": "\u2014 Explain the differences between a stack LSTM and Example 6 (compositional representation from arc-standard dependency parsing). It\u2019s clear to me that they are indeed different, but I\u2019d love to better understand the nuances there, and it seems like an important comparison to draw explicitly in the paper.\n\u2014 Any plans to release accompanying code?\n\u2014 How does the concurrent ICLR submission https://arxiv.org/pdf/1611.01734v2.pdf fit into this framework, if at all?\nThe authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google\u2019s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. \n\nThe most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). \n\nThe \u201craison d\u2019etre,\u201d in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework \u2014 emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I\u2019d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. \n\nOverall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded. ", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyI9jXyXe": {"type": "review", "replyto": "BycCx8qex", "review": "1. What are the limitations of the DRAGNN framework? (e.g., Can we implement seq2seq with soft attention?)\n\n2. How does DRAGNN related to Vowpal Wabbit's imperative learning-to-search learning framework?\nSee recent paper at NIPS https://arxiv.org/abs/1406.1837 (which has been on arxiv since 2014) as well as the author's tutorial at ICML 2015.\n\n3. Will a framework be released accompanying paper? The examples given are a little hand-wavey, which got me wondering: How many lines of code are required to expresses each of the models? Also, How long does it take to train and run?\n\n4. How does one include beam search or dynamic programming inference with a DRAGNN? To implement the full Parsey McParseFace model, users need this to implement approx global normalization and decoding via beam search.\n\n5. Why limit training to maximum likelihood training, especially given that we know such training is prone to failure due to \"exposure bias\" and many good alternatives exist, including DAgger (Ross et. al, 2011), LOLS (Chang et al., 2015) and reinforcement learning algorithms (e.g., policy gradient).Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement https://openreview.net/pdf?id=HkE0Nvqlg or\nhttp://www.cs.jhu.edu/~jason/papers/rastogi+al.naacl16.pdf? Can I implement the\ndynamic programming components as transition units and (importantly) would it be\nefficient?\n\n disagree that the VW framework is orthogonal, it is a *competing* way to\nimplement recurrent models. The main different to me appears to be that VW's\nimperative framework is more general, but less modular.\n\nThe experimental contribution seems useful as does the emphasis on how easy it\nis to incorporate multi-task learning.\n\nMinor:\n\n- It would be useful to see actual code snippets (possibly in an\n  appendix). Otherwise, its unclear how modular DRAGNN really are.\n\n- The introduction states that (unlike seq2seq+attention) inference remains\n  linear. Is this *necessarily* the case? Users define a transition system that\n  is quadratic, just let attention be over all previous states. I recommend that\n  authors rephrase statement more carefully.\n\n- It seems strange to use A() as in \"actions\", then use d as \"decision\" for its\n  elements.\n\n- I recommend adding i as an argument to the definition of the recurrence\n  function r(s) to make it clear that it's the subset of previous states at time\n  i, otherwise it looks like an undefined variable. A nice terse option is to\n  write r(s_i).\n\n- Real numbers should be \\mathbb{R} not \\mathcal{R}.\n\n- It's more conventional to use t for a time-step instead of i.\n\n- Example 2: \"52 feature embeddings\" -> did you mean \"52-DIMENSIONAL feature\n  embeddings\"?\n", "title": "Limitation of the framework", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkqw-ubNg": {"type": "review", "replyto": "BycCx8qex", "review": "1. What are the limitations of the DRAGNN framework? (e.g., Can we implement seq2seq with soft attention?)\n\n2. How does DRAGNN related to Vowpal Wabbit's imperative learning-to-search learning framework?\nSee recent paper at NIPS https://arxiv.org/abs/1406.1837 (which has been on arxiv since 2014) as well as the author's tutorial at ICML 2015.\n\n3. Will a framework be released accompanying paper? The examples given are a little hand-wavey, which got me wondering: How many lines of code are required to expresses each of the models? Also, How long does it take to train and run?\n\n4. How does one include beam search or dynamic programming inference with a DRAGNN? To implement the full Parsey McParseFace model, users need this to implement approx global normalization and decoding via beam search.\n\n5. Why limit training to maximum likelihood training, especially given that we know such training is prone to failure due to \"exposure bias\" and many good alternatives exist, including DAgger (Ross et. al, 2011), LOLS (Chang et al., 2015) and reinforcement learning algorithms (e.g., policy gradient).Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement https://openreview.net/pdf?id=HkE0Nvqlg or\nhttp://www.cs.jhu.edu/~jason/papers/rastogi+al.naacl16.pdf? Can I implement the\ndynamic programming components as transition units and (importantly) would it be\nefficient?\n\n disagree that the VW framework is orthogonal, it is a *competing* way to\nimplement recurrent models. The main different to me appears to be that VW's\nimperative framework is more general, but less modular.\n\nThe experimental contribution seems useful as does the emphasis on how easy it\nis to incorporate multi-task learning.\n\nMinor:\n\n- It would be useful to see actual code snippets (possibly in an\n  appendix). Otherwise, its unclear how modular DRAGNN really are.\n\n- The introduction states that (unlike seq2seq+attention) inference remains\n  linear. Is this *necessarily* the case? Users define a transition system that\n  is quadratic, just let attention be over all previous states. I recommend that\n  authors rephrase statement more carefully.\n\n- It seems strange to use A() as in \"actions\", then use d as \"decision\" for its\n  elements.\n\n- I recommend adding i as an argument to the definition of the recurrence\n  function r(s) to make it clear that it's the subset of previous states at time\n  i, otherwise it looks like an undefined variable. A nice terse option is to\n  write r(s_i).\n\n- Real numbers should be \\mathbb{R} not \\mathcal{R}.\n\n- It's more conventional to use t for a time-step instead of i.\n\n- Example 2: \"52 feature embeddings\" -> did you mean \"52-DIMENSIONAL feature\n  embeddings\"?\n", "title": "Limitation of the framework", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}