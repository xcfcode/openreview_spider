{"paper": {"title": "Variational Autoencoder with Arbitrary Conditioning", "authors": ["Oleg Ivanov", "Michael Figurnov", "Dmitry Vetrov"], "authorids": ["tigvarts@gmail.com", "michael@figurnov.ru", "vetrovd@yandex.ru"], "summary": "We propose an extension of conditional variational autoencoder that allows conditioning on an arbitrary subset of the features and sampling the remaining ones.", "abstract": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.", "keywords": ["unsupervised learning", "generative models", "conditional variational autoencoder", "variational autoencoder", "missing features multiple imputation", "inpainting"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a VAE model with arbitrary conditioning. It is a novel idea, and the model derivation and training approach are technically sound. Experiments are thoughtfully designed and include comparison with latest related works.\n\nR1 and R3 suggested the original version of the paper was lack of comparison with relevant work and the authors provided new experiments in the revision. The rebuttal also addressed a few other concerns about the novelty and clarity raised by R3.\n\nBased on the novel contribution in handling missing feature imputation with VAE, I would recommend to accept the paper. It is worth noticing that there is another submission to ICLR (https://openreview.net/forum?id=ByxLl309Ym) that shares a similar idea of constructing the inference network with binary masking, although it is designed for a pre-trained VAE model.\n\nThere are still two weaknesses pointed out by R3 that would help improve the paper by addressing them:\n1. The paper does not handle different kinds of missingness beyond missing at random.\n2. VAE model makes the trade-off between computational complexity and accuracy.\nPoint 1 would be a good direction for future research, and point 2 is a common problem for all VAE approaches. While the latter should not become a reason to reject the paper, I encourage the authors to take MCMC methods into account in the evaluation section.\n"}, "review": {"r1xSk0udiN": {"type": "rebuttal", "replyto": "H1e2RnDm5E", "comment": "Thank you for pointing your work and NADE, which we will gladly cite in the next revision.\n\nIndeed, the objective in section 4.3.1 is a special case of your model. However, our model is simpler and likely faster since it does not require LSTM-based sequential decision making.\n\nWe agree that the missing feature imputation problem can be solved as orderless autoregression. However, our approach shows that non-autoregressive imputation is also viable. We compare to an autoregressive Universal Marginalizer model in Section 5.3. We find that autoregressive model can perform better, but, unsurprisingly, is slow at test time.", "title": "RE: Missing references to prior work"}, "BJgGYt2i2X": {"type": "review", "replyto": "SyxtJh0qYm", "review": "The goal of this paper is to use deep generative models for missing data imputation. This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. First, a masking variable is sampled from a chosen prior distribution. The mask determines which features are observed. Then, the likelihood of the observed features is maximized via a lower bound. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of \"missing\" (to the generative model) features.\n\nNovelty:\nGenerative models have a long history of being used to impute missing data. e.g. http://www.cs.toronto.edu/~fritz/absps/ranzato_cvpr2011.pdf, https://arxiv.org/pdf/1610.04167.pdf,\nhttps://arxiv.org/pdf/1808.01684.pdf, https://arxiv.org/pdf/1401.4082.pdf [Appendix F]\nIt is a little difficult to guage what the novelty of this work is.\n\nClarity\nThis is a poorly written paper. Distilling the proposed methodology down to one paragraph was challenging since the text meanders through several concepts whose relevance to the overarching goal is questionable. For example, it is not clear what Section 3.2 adds to the discussion. The text describes a heuristic used in learning GSNNs only to say that the loss function used by GSNNs is not used in the experimental section for this paper -- this renders most of 4.3.2 redundant. There are issues like awkward grammar, sloppy notation, and spelling mistakes (please run spell check!) throughout the manuscript. Please use a different notation when referring to the variational distributions (do not re-use \"p\").\n\nExperimental Results\nThe model is evaluated against MICE and MissForest on UCI datasets. RMSE and accuracy of classification (from imputed data is compared). The complexity of data considered is simplistic (and may not make use of the expressivity of the deep generative model). Why not run these experiments on datasets like MNIST and Omniglot?\nBeyond that:\n(a) was there any comparison to how classification performance behaves when using another neural network based imputation baseline (e.g. the method in Yoon et. al)?\n(b) the *kind* of missingness considered here appears to be MCAR (the easiest kind to tackle) -- did you consider experiments with other kinds of missingess?\n\nThe qualitative results presented in this work are interesting. The method does appear to produce more diverse in-paintings than the method from Yeh et. al (though the examples considered are not aligned).\n\nTable 5 claims negative log-likelihood numbers on MNIST as low as 61 and 41 (I assume nats...). These numbers do not make sense. How were they computed?\n\n\nPriors on b:\nWhat kind of priors on b did you experiment with? ", "title": "Idea: Train a VAE to maximize the likelihood of a subset of the data while using the other subset for posterior inference. Poorly written paper with some interesting qualitative results.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ByxM-yCY07": {"type": "rebuttal", "replyto": "SygTsHQY0Q", "comment": "Thank you for pointing out these references! We updated the related work section.\n\nThough these models solve unconditional data generation problem, they can be easily conditioned on the arbitrary subset of missing features.\nNevertheless, these models use a framework of Markov chains which makes them more computationally expensive at the test time. They also require fully-observed training data to learn the transition operator.", "title": "RE: Missing Related work"}, "SylNszXY0Q": {"type": "rebuttal", "replyto": "BJgGYt2i2X", "comment": "We thank the reviewer for their feedback and comments. The reviewer raises several important concerns, which we address below.\n\n> It is a little difficult to guage what the novelty of this work is.\nThe novelty of the paper is the model based on variational autoencoder which can be conditioned on the arbitrary subset of the features. The model generates unobserved features from joint conditional distribution over them using a single forward pass through neural network. To our best knowledge, such model has not been proposed and explored previously. \nIn Susskind et al\u201911 (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.4604&rep=rep1&type=pdf ) authors train gated MRF at the lowest layer of DBN. Authors also propose an iterative process for filling-in missing pixels. We can claim that DBNs learning process is harder and less stable than the same for VAEs as we can see from experiments in the paper which use only 48x48 grayscale images. More importantly, the proposed method fills-in missing pixels in the single way while our paper propose a generative way to fill-in them in a number of significantly different ways. \nRezende et al\u201914 (https://arxiv.org/pdf/1401.4082.pdf) require a VAE pretrained on the fully observed data and run MCMC process where each step is forward pass through VAE in order to get one conditional sample, which is computationally expensive.\nSharir et al\u201916 (https://arxiv.org/pdf/1610.04167.pdf) also focuses mostly on the marginalization over the generated features for missing features imputation. The experiment in the paper consider only the classification quality under the missing data while in our work it is only a proxy metric which helps us to estimate the quality of imputations. \nZhang et al\u201918 (https://arxiv.org/pdf/1808.01684.pdf) is only a preprint with no code released. From the practical point of view, the proposed method requires to solve a fixed-point problem which requires much more forward passes through neural networks than in our method. During training such problem has to be solved on every E-step of EM-algorithm. From the theoretical point of view the approximations like E_{p(y | x)} p(z | x, y) \\approx p(z | x, E[y | x]) which are widely used in the paper do not sound reasonable, especially in the case when p(y | x) is multimodal which we address in our paper. We believe that in the case of multimodal dependencies such inaccurate approximations will prevent method from learning true joint conditional distribution.\n\n> For example, it is not clear what Section 3.2 adds to the discussion. The text describes a heuristic used in learning GSNNs only to say that the loss function used by GSNNs is not used in the experimental section for this paper -- this renders most of 4.3.2 redundant.\nOur paper is a follow-up of the Conditional Variational Autoencoder work, which claims that GSNN and hybrid model are important heuristics that increase model\u2019s quality. Thus we think our theoretical and experimental evaluation of this technique is valuable for the community. Most of this evaluation is provided in the appendix, while the main paper just describes the summary of our study and contains a link to appendix for those who interested.\n\n> Please use a different notation when referring to the variational distributions (do not re-use \"p\").\nWe would like to clarify that p(b | x) is not a variational distribution, but a user-defined distribution which describes the generative process of pairs (x, b) for the training stage: firstly x is sampled from the train set, then b is generated conditionally on x. Otherwise please describe the distribution you mean. We use only one variational distribution in the paper which we denote as q(z | x, b).\n\n> Why not run these experiments on datasets like MNIST and Omniglot?\nWe find that MICE and MissForest are too computationally expensive to be used for MNIST and Omniglot. For instance, running MissForest on a dataset which contains less than 40000 objects and 60 features took about 10 days. MICE which produces 10 imputations for each object works even slower.", "title": "Response to Reviewer 1 (Part 1/2)"}, "H1e4bQmYCQ": {"type": "rebuttal", "replyto": "SylNszXY0Q", "comment": "> Was there any comparison to how classification performance behaves when using another neural network based imputation baseline (e.g. the method in Yoon et. al)?\nThank you for this important question. We reimplemented their method based on the paper and the published code for MNIST inpainting. We ran the comparison on UCI datasets. We observe that GAIN works comparable with our method and for some datasets even worse than VAEAC. We can also notice that GAIN doesn\u2019t use unobserved data during training, which makes it easier to apply to the missing features imputation problem. Nevertheless, when the fully-observed training data is available it turns into a disadvantage. For example, in inpainting setting GAIN cannot learn the conditional distribution over MNIST digits given one horizontal line of the image while VAEAC can. All these results are reported in the paper and in the supplementary.\n\n> The *kind* of missingness considered here appears to be MCAR (the easiest kind to tackle) -- did you consider experiments with other kinds of missingess?\nFor missing feature imputation we did not. Nevertheless, one can see in inpainting section, that model works well with highly structured kinds of missingness such as masks containing rectangles, horizontal lines and complex pattern structures (last two rows of figure 4).\n\n> Table 5 claims negative log-likelihood numbers on MNIST as low as 61 and 41 (I assume nats...). These numbers do not make sense. How were they computed?\nIt is the log-likelihood of the unobserved part of image given the observed part, estimated using importance sampling. Surely this log-likelihood is not comparable with log-likelihood of unconditioned generative models. The first reason is that the unobserved part contains 700 pixels instead of 784 in unconditional model. The second reason is that model uses the information from the observed part.\n\n> What kind of priors on b did you experiment with?\nIn missing features imputation we used component-wise Bernoulli distribution over b. In the inpainting section we also considered the equiprobable distribution over the masks which contains\n+ one missing rectangle\n+ one missing random half of the image\n+ missing center part of the image\n+ one observed horizontal line\n+ complex missing pattern proposed in the paper \u201cContext Encoders\u201d\nIn figure 4 we used the mixture of all those distributions except horizontal line at the training stage.\nThe samples from all used distributions are presented on pictures 1-4. The detailed description of those distributions over b are available in appendices A.2 and A.3.", "title": "Response to Reviewer 1 (Part 2/2)"}, "SyxzO-XY0m": {"type": "rebuttal", "replyto": "H1lYezPC3Q", "comment": "Thank you very much for taking the time to review our work.\n\nThank you for pointing out the additional baselines for missing features imputation.\nBefore commenting them we want to emphasize that our paper focuses on the learning joint conditional density of unobserved features. This problem has a lot in common with missing features imputation, but there are some differences. The majority of missing feature imputation methods do not consider the whole distribution over missing features. Usually, they restore the average or the most probable value of unobserved features, or the conditional marginal distribution for each unobserved feature. Such imputations can provide good RMSE metrics or post-imputation classification quality, but we consider them as not satisfactory for our problem setting.\nWe comment the proposed baselines below one by one:\n1. Gondara et al\u201917 focuses on missing features multiple imputation, but not on learning of the conditional distribution. They propose to train the same neural network starting from different initialization in order to obtain multiple imputations. Such approach is quite computationally expensive when multiple imputations are required. More importantly, there are no theoretical guarantees or experimental proof that such method will restore the true joint distribution of the unobserved features instead of the averages or marginals of the features.\n2. Yoon et al\u201918 solves the same problem as we do. We reimplemented their method based on the paper and the code they published for MNIST inpainting. We ran the comparison on UCI datasets. We observe that GAIN works comparable with our method and for some datasets even worse than VAEAC. We can also notice that GAIN doesn\u2019t use unobserved data during training, which makes it easier to apply to the missing features imputation problem. Nevertheless, when the fully-observed training data is available it turns into a disadvantage. For example, in inpainting setting GAIN cannot learn the conditional distribution over MNIST digits given one horizontal line of the image while VAEAC can. All these results are reported in the paper and in the supplementary.\n3. Zhang et al\u201918 is hard to compare with, because the paper is only a preprint with no code released. From the practical point of view, the proposed method requires to solve a fixed-point problem which requires many more forward passes through neural networks than in our method. During training such problem has to be solved on every E-step of EM-algorithm. From the theoretical point of view the approximations like E_{p(y | x)} p(z | x, y) \\approx p(z | x, E[y | x]) which are widely used in the paper do not sound reasonable, especially in the case when p(y | x) is multimodal which we address in our paper. We believe that in the case of multimodal dependencies such inaccurate approximations will prevent method from learning the true joint conditional distribution.\n\n> Given the exponential number of valid masks in a general setting, one only subsamples a small portion during the training. The question is whether the model can generalize well in this regime?\nIn the general case when the distribution is arbitrary complex the answer is no. Nevertheless, if there are dependencies in the data which behaves similarly for all masks, including unseen ones, then the model would catch them. We can see exactly the described behavior on the 7th and 8th row on the figure 4. In this kind of masks each pixel has 80% probability to be missed, so there are at least 10^888 different most probable masks. Obviously, VAEAC never observes two equal masks in both training and testing stage. Nevertheless, as we see, it generalizes well for all masks generated from Bernoulli distribution.\nThe problem with generalization appears when the dependencies in the data are completely different for various kinds of masks. For example, if we learn VAEAC using Bernoulli distributed masks, it learns only short-range dependencies in the data. If after that we close, for instance, right half of the image, then the model will produce unnatural inpaintings, because long-range dependencies are required for handling such kind of masks.\nNevertheless, such cases of completely different dependencies induced by masks are rare. For instance, in our experiments the method shows to be tolerant to changing the scale of pattern mask from training to testing stage. The examples of pattern mask are presented on the last two rows of figure 4.", "title": "Response to Reviewer 3"}, "S1xrqJ7FCQ": {"type": "rebuttal", "replyto": "ryeVlLKR37", "comment": "Thank you for your kind review. \n\nIn equation (8) the proposal network q is conditioned on the concatenation of observed and unobserved parts of the object x_{1 - b} and x_b which appears to be x.", "title": "Response to Reviewer 2"}, "ryeVlLKR37": {"type": "review", "replyto": "SyxtJh0qYm", "review": "This paper introduces the VAEAC model, inspired by CVAEs, it allows conditioning on any subset\nof the latent features. This provides a model able to achieve good results on image inpainting\nand feature imputation tasks.\n\nThe paper appears to be technically sound, and the experiments are\nthoughtfully designed. The writing is clear and the model is easy to\nunderstand. The closest work to this of the Universal Marginalizer is\ncompared to well, with more compelling examples in the appendix. I\nwould have preferred if more of the experimental results were in the\nmain paper instead of in the appendix especially as the authors state\nthey chose to highlight their better results in the main paper.\n\nWhile not the first model to try to handle modeling data with missing features, it is\nstill a fairly original and elegant formulation.\n\nMinor details:\n\nIn equation (8) should x be x_b?\n", "title": "Solid work on using VAEs for feature imputation", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1lYezPC3Q": {"type": "review", "replyto": "SyxtJh0qYm", "review": "The paper presents a model for learning conditional distribution when arbitrary partitioning the input to observed and masked parts. The idea is to extend the conditional VAE framework such that the posterior is a function of an arbitrary subset of observed variables. Accordingly, reconstruction loss only penalizes the error in the reconstruction of masked (unobserved) variables. The method is compared against 1) classical approaches in missing data imputation on UCI benchmarks; 2) image inpainting against recently proposed GANS for the similar task, as well as; 3) against universal marginalizer, which learns conditional densities using a feedforward / autoregressive architecture.\n\nMy concern about the experimental results on missing data imputation is that strong competition such as Gondra et al\u201917 and Yoon et al\u201918 that report better results on UCI than classical approaches are not included. Could you please comment? See also [1,2] for other autoencoding architectures for this task.\n\nWhile the derivation of the method is principled, it assumes that either the mask is known during the training OR one could efficiently sample a distribution of masks to learn arbitrary conditional densities. Given the exponential number of valid masks in a general setting, one only subsamples a small portion during the training. The question is whether the model can generalize well in this regime? The experimental results in this setting is not very encouraging, suggesting the proposed approach is effective only when the limitted mask patterns are known in advance. \n\n[1] Gondara, Lovedeep, and Ke Wang. \"Multiple imputation using deep denoising autoencoders.\" arXiv preprint arXiv:1705.02737 (2017).\n\n[2] Zhang, Hongbao, Pengtao Xie, and Eric Xing. \"Missing Value Imputation Based on Deep Generative Models.\" arXiv preprint arXiv:1808.01684 (2018).\n", "title": "interesting paper; missing/weak experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}