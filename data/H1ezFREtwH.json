{"paper": {"title": "Composing Task-Agnostic Policies with Deep Reinforcement Learning", "authors": ["Ahmed H. Qureshi", "Jacob J. Johnson", "Yuzhe Qin", "Taylor Henderson", "Byron Boots", "Michael C. Yip"], "authorids": ["a1qureshi@ucsd.edu", "jjj025@eng.ucsd.edu", "y1qin@eng.ucsd.edu", "tjwest@ucsd.edu", "bboots@cs.washington.edu", "yip@ucsd.edu"], "summary": "We propose a novel reinforcement learning-based skill transfer and composition method that takes the agent's primitive policies to solve unseen tasks.", "abstract": "The composition of elementary behaviors to solve challenging transfer learning problems is one of the key elements in building intelligent machines. To date, there has been plenty of work on learning task-specific policies or skills but almost no focus on composing necessary, task-agnostic skills to find a solution to new problems. In this paper, we propose a novel deep reinforcement learning-based skill transfer and composition method that takes the agent's primitive policies to solve unseen tasks. We evaluate our method in difficult cases where training policy through standard reinforcement learning (RL) or even hierarchical RL is either not feasible or exhibits high sample complexity. We show that our method not only transfers skills to new problem settings but also solves the challenging environments requiring both task planning and motion control with high data efficiency.", "keywords": ["composition", "transfer learning", "deep reinforcement learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper considers deep reinforcement learning skill transfer and composition, through an attention model that weighs the contributions of several base policies conditioned on the task and state, and uses this to output an action. The method is evaluated on several Mujoco tasks.\n\nThere were two main areas of concern. The first was around issues with using equivalent primitives and training times for comparison methods. The second was around the general motivation of the paper, and also the motivation for using a BiRNN. These issues were resolved in a comprehensive discussion, leaving this as an interesting paper that should be accepted."}, "review": {"rJeMg4_gqB": {"type": "review", "replyto": "H1ezFREtwH", "review": "This paper presents an approach in which new tasks can be solved by an attention model that can weigh the contribution of different base policies conditioned on the current state of the environment and task-specific goals. The authors demonstrate their method on a selection of RL tasks, such as an ant maze navigation task and a more complicated \u201cant fall\u201d task, which requires the agent to first move a block to fill a gap in the environment before it is able to reach the goal. \n\nI found the paper interesting and well written. My primary concern is that the primitive policies are learned independently of the composite policies, which might limit the application of this approach to more complex problems. Additionally, it would be great to also see the concurrent and sequential form of skill combination for the more complex tasks, and not just the point navigation task shown in Figure 7. \n\nStandard errors on Figures 5 and 6 seem to be missing. Additionally, I was curious that in Figure 4a and Figure 6a, the composite\u2019s performance is already a lot better than the other methods after 0 training steps. Maybe the authors can elaborate on that. Maybe the performance at step 0 is just hard to make out in the graphs?\n\nI would suggest accepting the paper but there could be a more detailed analysis of how the pre-trained sub-modules are used and learning both composite and sub-policies together would make the paper stronger. \n\nAdditional comments:\n- Where is the training graph for the Composite-SAV applied to the \u201cant fall\u201d task? Maybe I missed it?\n- Algorithm 1 should probably be moved to the main text. \n\n####After rebuttal####\nThe authors' response and the revised paper address my already minor concerns. ", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "H1gDo92oiH": {"type": "rebuttal", "replyto": "H1ezFREtwH", "comment": "We thank all the reviewers for their insightful comments. We have revised our paper accordingly and provided the individual responses to each reviewer. The main changes to the paper are summarized as follows:\n\n1-We add attention weights depiction for presented scenarios and update our plots to include std errors.\n2-We revise our related work section to highlight our paper\u2019s novelty and merits compared to concurrent work [1].\n3-We include a couple of new experiments: i) a new ablated model (Sec 5.2) in our experiments (same as [1]) that takes the state and goal information, and outputs the mixture weights without conditioning on primitive actions encoding; ii) comparison with the benchmark models that were pretrained for an equal amount of time as our primitive policies (Fig. 4, Appendix C.1). iii) HIRO [2] results (Fig. 4) with both standard-mujoco-Ant (150 units torque limit) and low-torque-Ant (30 units torque limit, this is same as in [2]). iv) A side experiment (not the focus of this paper) that shows our method can learn new skills (that were missing in a given skill set) and composition simultaneously in an end-to-end manner (Appendix B).\nAll the experiments mentioned above further highlight the benefits of our approach. Our network architecture leverages all information (states, goals, primitive skills) in a structured way using bidirectional RNN and attention framework, and therefore, leads to better performance.\n\n[1] \u201cMCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\u201d NeurIPS 2019\n[2] \u201cData-efficient hierarchical reinforcement learning\u201d NeurIPS 2018", "title": "Response to all reviewers"}, "ryefEkSAFS": {"type": "review", "replyto": "H1ezFREtwH", "review": "What is the specific question/problem tackled by the paper?\nThis paper addresses the hierarchical RL problem of combining multiple primitive policies (pi_1, \u2026, pi_K) into policies for more complex tasks. Given a number of primitive skills and a new task within an environment, the paper aims to learn to pick and combine the primitives as needed to solve the new task. This problem statement is interesting and the method performs well on difficult tasks. \n\nHowever, I argue for rejecting this paper because it lacks meaningful contributions to the field. I do not see how the method presented in the paper is more than RL over  hand engineered action spaces that are better for the tasks. While this improves results, we already know that for any task, there is some best action space for performing that task. This is why most HRL work aims to also find the primitive policies in additional to composing them. Is this any better than option-critic if the options are hardcoded to be the primitives? The experiment state that the option-critic method did not work, but did you give it access to the same primitives?\n\nSummary:\nThe method presented in the paper is as follows: at each state s_t, the K primitives are queried for their action a_k ~ pi_k(s_t). Then, a biRNN reads in the actions in order from 1 to k. In parallel, the state and a goal are encoded by a network named \u201cDecoder\u201d. The encoded state and the hidden states of the RNN are used to output an attention weight over each primitive. Finally, the output action is the weighted combination of all the actions. The encoders and attention weights are trained with RL. \n\nThis method is evaluated on several mujoco tasks, such as making a cheetah jump hurdles by combining \u201cforward\u201d and \u201cjump\u201d primitives. Each environment has predefined primitives such as \u201cforward\u201d \u201cleft\u201d \u201cright\u201d etc. \n\nThis method is compared against HIRO, which does not have access to the primitive policies. It is not surprising that hand engineering primitives helps performance. \n\nIs the approach well motivated?\nThe general idea behind the approach is well motivated: using primitive skills to learn complex skills is a useful goal. The details of the method are strange.\nI would like to see a better motivation and empirical justification for the biRNN. Why should the primitive\u2019s action be encode in order? The ordering of the primitives is arbitrary and constant: a fully connected network could be used, or the attentions could be output entirely independently per primitive.\nIn fact, I do see not why the primitives\u2019 actions need to be encoded at all. It would be much simpler for the encoder to look at (s_t, g_t) and output a discrete probability over the K primitives.\nThe ablations in 5.2 are for outputting actions directly rather than mixture weights. The paper would benefit from ablations where mixture weights are output but without the biRNN or without passing in the primitive\u2019s actions.\n \n\nIs the method well placed in the literature? \nThe main idea of predicting weights over multiple experts is not novel (see \"Adaptive mixtures of local experts\u201d from 1991). In the context of RL literature, we can interpret the primitive skills as actions directly, and then the method is performing basic RL over a better action space (the better actions being \u201cgo left\u201d, \u201cjump\u201d etc. We can also interpret these as options, but unlike options a single primitive is not followed for multiple time steps with a termination condition. Functionally, this is equivalent to regular RL using domain knowledge to engineer the action space. \n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 4}, "Hkx4LknOKS": {"type": "review", "replyto": "H1ezFREtwH", "review": "Overall, I think the method has some great merit but I am not overly confident in the reproducibility of the method. Some of the comparisons (HIRO) do not agree with the results in the HIRO paper. Also, more description is needed to describe how the baselines were used in the analysis. Were they also given the pre-trained sub-policies? A more fair comparison might be to give those baseline methods no sub-policies but let them pre-train for an equivalent amount of time as the sub-policies are trained.\n\n Here are some more detailed comments: \n- Figure 1 is not very clear and does not appear to add much to the explanation of the method. More detail should be included in the caption.\n - In the paper, it is noted that HRL has high sample complexity and needs lots of training data. I find the comment about how HRL requires many more training steps than regular RL very odd. The purpose of HRL is to have better sample efficiency and learn strong polices faster. Has the author observed different? The purpose of HRL is to reduce sample complexity and search in a well suited and structured way.\n- The assumption that the sub-policies solve the underlying MDP is rather strong. How are these policies going to be trained to guarantee this? \n- I like the idea of using an attention model to help pick learn a weighting for the combination of a number of sub-policies. I am not sure if using a bi-directional LSTM is the best or simplest method to accomplish this. The authors can look at \"MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\" NeurIPS 2019 for a recent work that is similar to theirs. \n- For Figure 4: Do these methods also get to use the sub-policies that have pre-trained some version of tasks? Also, how are these component policies trained? Over what tasks are they trained? This information is very important to make sure the method is not overly biased to the composition of them. \n- I find the results in Figure 5 very odd. The baseline shows that HIRO does not learn to perform well on these tasks even though these are the tasks from the HIRO paper that it learned to solve rather well. Can this contradiction be explained? \n- For the HIRO comparison was the system also using the composite policies there were pretrained? HIRO is designed to learn the sub-policies concurrently but it seems in this case the authors are using the outputs of the composition policies as input to the HIRO low policy. \n- I do not understand the visualization in Figure 7. How to the colored paths for the agent represent the weights for the compose policy?", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 4}, "rJxJ_tPqsB": {"type": "rebuttal", "replyto": "SkgD-el5sS", "comment": "We would like to thank our reviewer for the feedback. We have revised our paper accordingly and our responses are summarized as follows:\n\nRC: None of the experiments use skill sets of variable lengths, right? \nAR: Yes, in this paper we use a fixed set of primitive policies and we further elaborate this in our paper to avoid confusion.\n\nRC: Is there some local minimum at 0.5 distance to goal that they all get stuck in? \nAR: Due to environment structures, the models are likely to get trapped in some local minima. For instance, in the ant-cross-maze, getting to the center junction is almost half of the distance (~0.5/1.0). Also, reaching any of the three goals requires upward motion to the center junction followed by further sideways or upward movements. We observed that all ablated models overfit to upward motion policy and fail to learn other motions. Similarly, in the ant-random-goal environment, we noticed that all ablated models could only acquire limited behaviors that helped them get closer to the goals limited to one-quarter or two-quarters of a circle but not the entire space.  We believe the presented environments are challenging and using advance exploration strategies might help improve the performance for all models, but it is not in the scope of this paper.\n\nRC: It\u2019s also quite strange that all the ablations perform the same as each other? \nAR: Ablated models indeed fail to perform and sometimes converge quickly to some local minima, as we have mentioned earlier such as for the ant-cross-maze. However, it is also evident that learning curves are not the same for all models. For instance, in ant-cross-maze, the variance of each plot indicates that composition-without-BRNN was able to get closer to the goal sometimes, whereas, other ablated models completely failed to get any closer to the given targets.\n\nRC: \u201caveraged over ten trials\u201d Is a trial an evaluation rollout, or is a trial an independent RL training process?\nAR: They correspond to independent RL training processes. \n\nRC: And, given the existence of MCP, which can either learn primitives or use pertained primitives, what is the motivation for this paper?\nAR:  We further elaborate on MCP in our related works (section 2) to highlight the merit of our approach. And our response is also summarized as follows:\n \nMCP [1] comprises i) a set of trainable Gaussian primitive policies that take the given state and proposes the corresponding set of action distributions and ii) a gating function that takes the extra goal information together with the state and outputs the mixture weights for composition. The primitive policies and a gating function are trained concurrently using reinforcement learning. In their transfer learning tasks [1], the primitive polices parameters are kept fixed, and the gating function is trained to output the mixture weights according to the new goal information. In our ablation studies, we show that training an MCP like-gating function that directly outputs the mixture weights without conditioning on the latent encoding of primitive actions gives inferior performance compared to our method. Our method utilizes all information (states, goals, and primitive skills) in a structured way through attention framework, and therefore, leads to better performance.\n\n[1] \u201cMCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\u201d NeurIPS 2019\n\nRC: Don\u2019t almost all comparisons be against methods that do not have any access to the primitives, these comparisons are obviously unfair.\n\nAR: To address this issue, we include a comparison with the benchmark models that were pretrained for an equal amount of time that we used to train our primitive policies (Fig. 4 Appendix C.1). Also, note that in our framework, primitive policies are trained only once, especially for Ant, and are used to solve in all related environments (ant-cross-maze, ant-random-goal, ant-u-maze, ant-push, ant-fall). \n\nRC: HIRO performs as well as Composition HIRO, unless the low-torque also significantly improves the Composition HIRO (HIRO-Low-Torque-Ant-Pretrained).\n\nAR: It does not matter much for the composition framework either it is low-torque-ant or high-torque-ant as it assumes the availability of primitive skills. However, for HIRO standard-ant, we found that even pretraining led to no improvement in the performance whereas our method can use standard ant to solve the problem. \n\nAC: How did you end up with the architecture you have?\nWe believe our ablation studies sufficiently motivates the architecture design. In our paper, we highlight that it is crucial to utilize all information (states, goals, and primitive skills) for composition, and our framework does leverage it and therefore, leads to better performance.\n\nAC: We would also like to thank our reviewer for providing some interesting ideas that we leave to our future studies such as i) leveraging variable input length at each time step, and ii) using hand-defined primitives. \n", "title": "Response to Reviewer# 1"}, "rJgTUVm5jB": {"type": "rebuttal", "replyto": "B1lHiYgciB", "comment": "We thank our reviewer for providing comprehensive and constructive feedback which helped us improve our paper. We have revised the paper to address reviewer comments. The response summaries are as follow: \n\nRC: It would be good to include more details on why your method is novel and an improvement over MCP. \n\nAR: We want to thank our reviewer for pushing us to go through the details of the MCP [1] paper carefully. We noticed that their approach is quite similar (if not the same) to our Composition-Without-BRNN model presented in the ablation study section (Section 5.2). We already show in our results that MCP like framework gives inferior performance compared to our method.  Further details are included in the paper (in related work section 2 and ablative study section 5.2) and are also summarized as follows:\n \nA recent and similar work to ours is a multiplicative composition policies (MCP) framework [1]. MCP comprises i) a set of trainable Gaussian primitive policies that take the given state and proposes the corresponding set of action distributions and ii) a gating function that takes the extra goal information together with the state and outputs the mixture weights for composition. The primitive policies and a gating function trained concurrently using reinforcement learning. In their transfer learning tasks [1], the primitive polices parameters are kept fixed, and the gating function is trained to output the mixture weights according to the new goal information. In our ablation studies, we show that training an MCP like-gating function that directly outputs the mixture weights without conditioning on the latent encoding of primitive actions gives inferior performance compared to our method. Our method utilizes all information (states, goals, and primitive skills) in a structured way through attention framework, and therefore, leads to better performance.\n\n[1] \u201cMCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\u201d NeurIPS 2019\n\n\nRC:  \"Figure 3: Do these methods also get to use the sub-policies that have pre-trained some version of tasks? \" I did not find this information in the appendix. Could you summarize this process?\nAR: We include a separate section in Appendix (C.1) to discuss pretraining. A brief summary for Fig. 3 is as follow: \nFor the non-hierarchical tasks presented in Fig. 3, the benchmark methods are not pretrained. However, to account for pretraining, the performance of other methods (TRPO, PPO, SAC) can be accessed after 1, 0.5, and 0.1 million for the ant, halfcheetah, and pusher environments, respectively. Also, notice that in Fig. 3, the pretraining will have no significant effect on the performance of TRPO and PPO in all environments and SAC in half-cheetah hurdle and cross-maze-ant environments. Furthermore, since pusher and random-goal-ant are relatively simple environments (due to no obstacles or maze around), pretrained SAC can perform similar to our composition method.\n", "title": "Response to Reviewer # 2 "}, "B1gp_Si_jB": {"type": "rebuttal", "replyto": "HJl4XUqLoH", "comment": "RC: Additionally, it would be great to also see the concurrent and sequential form of skill combination for the more complex tasks, and not just the point navigation task shown in Figure 6. \n\nAR:  We have included the depiction of attention weights for the other complicated environments in our paper (Appendix D). ", "title": "Response to Reviewer# 3 [Attention Weights]"}, "Skg7lT9UjH": {"type": "rebuttal", "replyto": "ryefEkSAFS", "comment": "RC: Reviewer Comment; AR: Authors Response \n\nRC: Is this any better than option-critic if the options are hardcoded to be the primitives? The experiment state that the option-critic method did not work, but did you give it access to the same primitives?\n\nAR: Option-critic method executes the options sequentially, and there is no concurrent composition. Furthermore, option-critic learns/needs tasks specific primitive, whereas our method can compose task-agnostic skills, both sequentially and concurrently, to solve new problems.\n\nRC: Why should the primitive\u2019s action be encode in order? \n\nAR: We use bidirectional RNN instead of uni-directional RNN to avoid ordering issues. It is also evident from the results that ordering is not a problem for our method. For instance, in cross-maze-ant, the composition has access to four primitive policies ( up, down, left, and right), and still, our method learns to not use downward policy (irrespective of order) at all as it is not needed to solve the given task.\n\nRC: The ablations in 5.2 are for outputting actions directly rather than mixture weights. The paper would benefit from ablations where mixture weights are output but without the biRNN or without passing in the primitive\u2019s actions.\n\nAR: We have included the suggested ablation. Please refer to Fig 5. Composition-without-BRNN takes the current state and outputs the mixture weights directly, which are then used to compose primitive actions. The composition-without-BRNN indeed perform better than other ablated models but still gives inferior performance than our proposed composition model. We argue that the composition model without BRNN performs poorly compared to the proposed method because it is entirely unaware of low-level continuous actions. Therefore, learning the latent encoding of primitive actions and making them a part of state-space is crucial for learning an effective composite policy. \n", "title": "Response to Reviewer # 1"}, "HJl4XUqLoH": {"type": "rebuttal", "replyto": "rJeMg4_gqB", "comment": "RC: reviewer comment; AR: author response\n\nRC: I found the paper interesting and well written. My primary concern is that the primitive policies are learned independently of the composite policies, which might limit the application of this approach to more complex problems. \n\nAR: We think it is one of the salient features of our method that it can take task-agnostic skills and can compose them for solving new transfer-learning problems. However, to address reviewer concern, we also include new skill acquisition experiments (Appendix B) in our paper. These experiments show that our method can acquire missing skills (task-dependent) and can compose them together with the existing task-agnostic skill set to solve the given problem in an end-to-end learning manner.\n\nRC: Additionally, it would be great to also see the concurrent and sequential form of skill combination for the more complex tasks, and not just the point navigation task shown in Figure 6. \n\nAR: We will add the depiction of attention weights for the other complicated tasks in a couple of days.\n\nRC: Standard errors on Figures 4 and 5 seem to be missing.\n\nAR: We have updated the figures to include standard errors. Please refer to the revised paper.  \n\nRC: Additionally, I was curious that in Figure 4a and Figure 6a, the composite\u2019s performance is already a lot better than the other methods after 0 training steps. Maybe the authors can elaborate on that. Maybe the performance at step 0 is just hard to make out in the graphs?\n\nAR: It is because other methods sometimes push the object away from the target position at early evaluation steps, leading to an increase in distance of the object from the given target than its initial distance.\n\nRC: I would suggest accepting the paper but there could be a more detailed analysis of how the pre-trained sub-modules are used and learning both composite and sub-policies together would make the paper stronger.\n\nAR: We will include the attention weights depiction soon for the other complicated tasks. Furthermore, we have added new results to show that it is possible to learn both composite and sub-level policies together in end-to-end learning using our framework.  \n\nRC: Where is the training graph for the Composite-SAV applied to the \u201cant fall\u201d task? Maybe I missed it?\n\nAR: We do not include as composite-SAC for ant-maze, ant-push, and ant-fall. These problems require complex task planning (sub-goal generation), and therefore, like other non-hierarchical RL methods (SAC, TRPO, PPO), the composition-SAC also fails to perform in these problems.\n\nRC: Algorithm 1 should probably be moved to the main text.\nAR: Due to limited space, it might not be possible to move the algorithm 1 to the main text. \n", "title": "Response to Reviewer # 3"}, "S1lytXcIir": {"type": "rebuttal", "replyto": "Hkx4LknOKS", "comment": "RC: reviewer comment; AR: author response\n\nWe provide the source code to ensure reproducibility with trained polices.  Also, we were able to reproduce HIRO results presented in their paper. In their paper, they use low-torque-ant (30 Units) that limits the action space. In contrast, in our work, we use standard-ant (150 units), so the action-space is large and makes the learning problem significantly harder as the Ant is now more prone to instability. For completeness, we now update for plots (Fig. 4) to include HIRO results with low-toque-ant (conforming with the system settings proposed in their paper).\n\nRC: Also, more description is needed to describe how the baselines were used in the analysis. Were they also given the pre-trained sub-policies? A more fair comparison might be to give those baseline methods no sub-policies but let them pre-train for an equivalent amount of time as the sub-policies are trained.\n\nAR: We have included more descriptions on the baselines\u2019 analysis in Appendix C. Furthermore, we now include HIRO baselines that were pretrained on low-torque-ant (as in HIRO paper) for the equal amount of time as the sub-policies for the composite model were trained. The new results are presented in Fig 4. The pretraining of HIRO on standard-Ant does not lead to any improvement. Therefore, to avoid clutter, we do not include those plots in Fig 4.  Furthermore, we would like to highlight that, in the case of Ant, our composition model uses the same primitive skills across all presented environments, which in some cases is not equivalent to pretraining other benchmark models every time.\n\nRC: Figure 1 is not very clear and does not appear to add much to the explanation of the method.\n\nAR: We exclude figure 1 as other reviewers think it\u2019s unnecessary. \n\nRC: In the paper, it is noted that HRL has high sample complexity and needs lots of training data. I find the comment about how HRL requires many more training steps than regular RL very odd. The purpose of HRL is to have better sample efficiency and learn strong polices faster. Has the author observed different? The purpose of HRL is to reduce sample complexity and search in a well suited and structured way.\n\nAR: We agree with the reviewer. Our statement was misleading. HRL methods perform indeed better than regular RL, and we have removed that statement. As per the author\u2019s understanding, the HRL, especially HIRO, is for complex tasks with weak reward signals that require both task and motion planning. And, due to weak reward signals and complex decision-making, these methods take a huge amount of interactive experience with the environment, which, of course, is still much less than regular RL methods. And in this aspect, our proposed composition framework further improves data-efficiency of HIRO by effectively exploiting the primitive skills.\n\nRC: The assumption that the sub-policies solve the underlying MDP is rather strong. How are these policies going to be trained to guarantee this? \n\nAR: We assume standard RL sub-level-policies policies that generally solve underlying MDPs.\n\n\nRC: The authors can look at \u201cMCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\u201d NeurIPS 2019 for a recent work that is similar to theirs. \n\nAR: Thanks for pointing us to MCP framework, we have included it in our related work section.\n\n\nRC: For Figure 3: Do these methods also get to use the sub-policies that have pre-trained some version of tasks? Also, how are these component policies trained? Over what tasks are they trained?  \n\nAR: We have included the details of training sub-level policies in Appendix C. \n\nRC: I find the results in Figure 4 very odd. The baseline shows that HIRO does not learn to perform well on these tasks even though these are the tasks from the HIRO paper that it learned to solve rather well. Can this contradiction be explained?\n\nAR: We were able to reproduce HIRO results using their system settings (low-torque-ant), please refer to Fig 4 for results.\n\nRC: For the HIRO comparison was the system also using the composite policies there were pretrained? HIRO is designed to learn the sub-policies concurrently but it seems in this case the authors are using the outputs of the composition policies as input to the HIRO low policy. \n\nAR: In our proposed work, HIRO concurrently learns the high-level policy and a composite policy (given primitive skills). \n\nRC: I do not understand the visualization in Figure 6?\n\nAR: The \u201ccolor\u201d of paths has nothing to do with the attention weights. The agent starts from the center (origin) and moves towards one of the given goals. Since primitive polices were to move straight up, down, left, and right, the diagonal motion requires the concurrent composition of low-level skills, which is indicated by the attention weights. On attention maps, the vertical axis reads from top to down, where zero means the starting position of the agent, and it takes about 30 steps to reach the target.\n", "title": "Response to Reviewer # 2"}}}