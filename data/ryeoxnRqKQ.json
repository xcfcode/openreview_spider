{"paper": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "summary": "", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"]}, "meta": {"decision": "Reject", "comment": "Although one review is favorable, it does not make a strong enough case for accepting this paper. Thus there is not sufficient support in the reviews to accept this paper.\n\nI am recommending rejecting this submission for multiple reasons.\n\nGiven that this is a \"black box\" attack formalized as an optimization problem, the method must be compared to other approaches in the large field of derivative-free optimization. There are many techniques including: Bayesian optimization, (other) evolutionary algorithms, simulated annealing, Nelder-Mead, coordinate descent, etc. Since the method of the paper does not use anything about the structure of the problem it can be applied to other derivative-free optimization problems that had the same search constraint. However, the paper does not provide evidence that it has advanced the state of the art in derivative-free optimization.\n\nThe method the paper describes does not need a new name and is an obvious variation of existing evolutionary algorithms. Someone facing the same problem could easily reinvent the exact method of the paper without reading it and this limits the value of the contribution.\n\nFinally, this paper amounts to breaking already broken defenses, which is not an activity of high value to the community at this stage and also limits the contribution of this work.\n"}, "review": {"ryxoR3ptyE": {"type": "rebuttal", "replyto": "rJxJgUsYkE", "comment": "Not sure why you are obsessed with the minimal adversarial examples. If those are what you are looking for, our paper does not provide a direct answer though you probably can derive one based on our work. \n\nKindly check Steps 1--4 in the paper which generate valid adversarial examples whose differences from the original images are imperceptible up to the thresholds $\\tao_p, p=2 or \\infty$.", "title": "Steps 1--4 generate valid adversarial examples"}, "SklmUtNFJE": {"type": "rebuttal", "replyto": "r1eZ4M7N14", "comment": "If you referred to a parametric distribution by the \"more powerful distribution\", we learn the parameters to specify the distribution as we described earlier. Once we reach such a distribution, we are supposed to either sample from it or use the mode to generate an adversarial example following Steps 1--4 in the paper. The mean of Gaussian overlaps with the Gaussian's mode.", "title": "Mode is not the mean"}, "BylsJD_90X": {"type": "rebuttal", "replyto": "ByeQ2E_dAQ", "comment": "Regarding the \"whole\" wording, we are fine to remove the \"whole\" because \n\"the whole population of adversarial examples per image\" \nis actually equivalent to \n\"the population of adversarial examples per image\". \nGiven an image, all its adversarial examples comprise the population. We use a Gaussian distribution to model this population in this work. In the future, other multi-variate continuous distributions, like GMM or uniform, may be found a better fit to the population. Additionally, one may also consider to capture the population by non-parametric distributions. No matter which one --- including the Gaussian, what it models is the population per image and not the local region. \n\nThanks to the above, we formalize our problem as minimizing the expected loss under the Gaussian distribution. This problem formulation is different from any problem formulations of the existing white-box attack methods. In contrast, BPDA and QL are built upon PGD (and CW for BPDA). PGD is the basic framework for them and they only (and yet non-trivially) replace the true gradients by the estimated ones. In this sense, we said we did not employ any white-box attack methods. We also agree with the reviewer that there is no sharp contrast between BIM and MIM because both are based on the similar principle of solving the following optimization problem: $min_{perturbation} Loss$. In contrast, ours is $min_{Gaussian mean} Expectation Loss$. ", "title": "Understanding our approach"}, "BJe1JZucAm": {"type": "rebuttal", "replyto": "ByeQ2E_dAQ", "comment": "We have tuned the hyper-parameters of the competing methods (BPDA, QL, ZOO, D-based) in order to achieve the best performances they could have. The ES part is to approximate an expectation by a sample mean, so we believe it is fair to fix the sample size for QL and our algorithm --- we did tune the other hyper-parameters in QL such as the learning rate and number of iterations. (QL actually doubles the sample size by reversing the signs of the samples.)\n\nFor all of the competing methods but the decision-based, we used the code released by the original authors in the experiments. We did not find the official implementation of the decision-based method (D-based) due to the deadline rush; instead, this implementation (https://github.com/greentfrapp/boundary-attack) was employed. Thanks to the reviewer's question, we tested this implementation using the evaluation metric reported in the original publication and only found it failed to re-produce the reported results. Upon a second search, we found the \"foolbox\" implementation (https://github.com/bethgelab/foolbox) of D-based. With it, we re-produced the reported results and obtained 66% success rate on attacking INPUT-TRANS (ours: 100%, BPDA: 100%, QL: 66.5%, and ZOO: 38.3%). Regarding the D-based experiments for generating the $\\ell_infty$ bounded adversarial examples, we re-wrote the norm function in the two implementations and did not observe any good results. We agree with the reviewer that, since D-based was particularly tailored for the $\\ell_2$ bounded adversarial examples, that simple change of norm is not good enough to improve D-based for handling the $\\ell_infty$ metric. More careful work has to be done to modify the D-based method to fit the $\\ell_infty$ context; for example, the projection to the $\\ell_2$ sphere has to be updated by the projection to the $\\ell_infty$ polygon. We have updated the PDF.", "title": "Regarding the experiments"}, "Hyxdoxb8CX": {"type": "rebuttal", "replyto": "ryeoxnRqKQ", "comment": "Denote by QL (Ilyas et al., 2018)\u2019s approach. We have added to the revised PDF \n+ new results on attacking Adv-Train (Madry et al., 2018) (Table 1), \n+ a new paragraph to draw readers\u2019 attention to QL upfront (cf. the highlighted text in the introduction), \n+ new results of QL on attacking the 10 defense methods (Table 1), \n+ a new section (Section 3.1.3) to carefully investigate the factors that contribute to the inferior performance of QL algorithm. The results reveal that, in order to improve its attack success rates, it is vital to get rid of PGD (projection and the sign of the gradients), which is the foundation upon which QL is built, and meanwhile to couple the $\\ell_infty$ clip with the tanh transformation. \n\nThanks to the careful experimental investigation, we make the following conclusion. \n\n1) QL is hinged on the white-box PGD attack --- in terms of methodology, it is actually closer to [1] than ours because both QL and [1] essentially approximate the gradients for PGD. As a result, the quality of the estimated gradients in QL is a big deal. Unfortunately, ES does not give rise to stable gradients due to the sampling step and PGD\u2019s projection and sign functions, especially when the gradients are \"obfuscated\". On the contrary, we do not employ any white-box attack methods at all in developing our algorithm. The Gaussian mean is more important than the gradients in our approach. Whereas ES is a natural choice to search for the Gaussian mean, some derivative-free methods [2] are also good alternatives. \n\n2) The seemingly subtle algorithmic distinction between QL and ours actually leads to significantly different attack success rates. In order to improve QL\u2019s performance, it is vital to remove PGD, the foundation upon which QL is built. \n\n[1] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. \n[2] Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization:  a review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3):1247\u20131293,2013.", "title": "Summary of changes in the new PDF"}, "rygk8TgICX": {"type": "rebuttal", "replyto": "SyepS_mP3m", "comment": "Q: the attack introduced here is actually equivalent to (Ilyas et al., 2018). \n\nA: We believe the above is a mis-interpretation of our responses. Denote by QL (Ilyas et al., 2018). QL is hinged on the white-box PGD attack --- in terms of the methodology, it is actually closer to [1] than ours because both QL and [1] essentially approximate the gradients for PGD. As a result, the quality of the estimated gradients in QL is a big deal. Unfortunately, ES does not give rise to stable gradients due to the sampling step and PGD\u2019s projection and sign functions. Indeed, after we remove the PGD step in QL, there is a significant performance boost (cf. Table 2 in the revised PDF). On the contrary, we do not employ any white-box attack methods at all in developing our algorithm. The Gaussian mean is more important than the gradients in our approach. Whereas ES is a natural choice to search for the Gaussian mean, some derivative-free methods [2] are also good alternatives. \n\nPlease see Section 3.1.3 for a more detailed investigation about QL.\n\n\nQ: \"existing black-box attacks are weaker than their white-box counterparts\u201d is simply not true.\n\nA: It is actually unclear how strong the existing black-box methods are on attacking the defended neural networks --- most experiments reported in the original publications are conducted on vanilla neural networks. Our own experiments do show that ZOO [1] and the decision-based attack [2] fail to perform well on attacking all the 10 defense methods (cf. Table 1 in the PDF) --- as the decision-based attack consumes a lot of run time, we have to include the complete results later. We have toned down the description about prior black-box attack in the introduction (cf. the text highlighted in the blue color). \n\n[1] Chen, Pin-Yu, et al. \"Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models.\" Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 2017.\n\n[2] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.\n\n\nQ: The concept of adversarial distributions is not new\n\nA: We have to point out the distribution over adversarial examples per image is different from the distribution over the transformations for a physical adversarial in the real world. In order to photograph a real-world adversarial, it is natural to consider all the conditions (location, background, lighting, etc.) as a distribution of transformations. In contrast, it is not so obvious to model by a distribution the adversarial examples for every single image. To the best of our knowledge, this work is the first to capture the whole population of adversarial examples per image.\n\n", "title": "PDF updated & responses to questions"}, "HyxHfJWUCm": {"type": "rebuttal", "replyto": "SklRyhS5nQ", "comment": "Here are the success rates on attacking the vanilla PGD training (Madry et al., 2018) on CIFAR10: \nBPDA: 46.9%\nOurs: 47.9%\n\nThe classification accuracy of the PGD-defended network is 87.3% on CIFAR10.\n\nConclusion 1: The vanilla PGD training is strong.\nConclusion 2: The vanilla PGD training sacrifices the performance to certain degree on the original classification task, so do some other defense techniques (cf. Table 1 in the PDF).", "title": "Results"}, "rkxO_clI07": {"type": "rebuttal", "replyto": "S1g3Hhj_hm", "comment": "Q: The paper would have to change significantly in order to relate it properly to (Ilyas et al., 2018).\n\nA: Denote by QL (Ilyas et al., 2018)\u2019s approach. We have added to the revised PDF \n+ a new paragraph (in the Introduction section) to draw readers\u2019 attention to QL upfront, \n+ new results of QL on attacking the 10 defense methods (Table 1), and \n+ a new section (Section 3.1.3) to carefully investigate the factors that contribute to the inferior performance of QL algorithm. The results reveal that, in order to improve its attack success rates, it is vital to get rid of PGD (projection and the sign of the gradients), which is the foundation upon which QL is built, and meanwhile to couple the $\\ell_infty$ clip with the tanh transformation. \n\nGiven the above changes, it seems like feasible to extensively discuss QL and yet not completely re-write the paper. \n\nAdditionally, we wanted to emphasize that the Gaussian mean is more important than the gradients in our approach. Whereas ES is a natural choice to search for the Gaussian mean, some derivative-free methods [2] are also good afternatives. In sharp contrast, QL is hinged on the white-box PGD attack --- in terms of the methodology, it is actually closer to [1] than ours because both QL and [1] essentially approximate the gradients for PGD. As a result, the quality of the estimated gradients in QL is a big deal. Unfortunately, ES does not give rise to stable gradients due to the sampling step and PGD\u2019s projection and sign functions. Indeed, after we remove the PGD step in QL, there is a significant performance boost (cf. Table 2). \n\n[1]\tAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. \n[2] Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization:  a review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3):1247\u20131293,2013.\n\n\nQ: Example adversarial examples to baseline the figure:\n\nA: Sorry for the confusion about Figure 1. First of all, we did not include all the defense methods in Figure 1 due to the heavy run time on ImageNet. Besides, for each attack method, we had removed all the examples of which it failed to change the labels. Our intention was to compare the relative convergences when their last steps are aligned. Upon reading your comments, however, we think this alignment is actually unnecessary and should be removed. In the revised PDF submission, you can see that some of the attack methods fail to reach 100% success rate.\n\nWe will add some example adversarial examples in the appendix, but the adversarial examples in $\\ell_\\infty = 0.031$ are hardly differentiable from the benign ones. \n", "title": "PDF revised with extensive discussion on [1]; Curves updated"}, "BkeY7txIR7": {"type": "rebuttal", "replyto": "r1lSC_3T2m", "comment": "Thank you for the encouraging comments! Regarding the name of the curve, we have removed \u201cROC\u201d and now simply call it the curve of success rate vs. number of evolution iterations. We will continue to polish the text.", "title": "Appreciated; Have re-named the curve"}, "r1lSC_3T2m": {"type": "review", "replyto": "ryeoxnRqKQ", "review": "In this paper, authors propose a \"universal\" Gaussian balck-box adversarial attack.\nOriginal and well-written (although there are a few grammar mistakes that would require some revision) and structured. Having followed the comments and discussion I am convinced that the proposed method is state of the art and interesting enough fro ICLR.\nTo the best of my knowledge, the study is technically sound.\nIt fairly accounts for recent literature in the field.\nExperiments are convincing.\nOne thing I am not so convinced about is the naming of the evaluation curve as \"a new ROC curve\". I understand the appeal of pairing the proposed evaluation curve with the ROC curve but, beyond an arguable resemblance, they have no much in common, really.", "title": "original", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1g3Hhj_hm": {"type": "review", "replyto": "ryeoxnRqKQ", "review": "Summary: In this paper the authors discuss a black-box method to learn\nadversarial inputs to DNNs which are \"close\" to some nominal example\nbut nevertheless get misclassified. The algorithm essentially tries to\nlearn the mean of a joint Gaussian distribution over image\nperturbations so that the perturbed image has high likelihood of being\nmisclassified. The method takes the form of zero-th order gradient\nupdates on an objective measuring to what degree the perturbed example\nis misclassified. The authors test their method against 10 recent DNN\ndefense mechanisms, which showed higher attack-success rates than\nother methods. Additionally the authors looked at transferrability of\nthe learned adversarial examples.\n\nFeedback: As noted before, this paper shares many similarities with\n\n[1] \"Black-box Adversarial Attacks with Limited Queries and Information\" (https://arxiv.org/abs/1804.08598)\n\nand the authors have responded to those similarities in two follow-ups. I have reviewed these results and their \nmethod does appear to improve over [1]. However, I am still reluctant to admit these additions to the original submission, \nmainly because dropping [1] in the original submission seems to be a fairly major omission of one of the most relevant competitors out there. In its current form, the apparent redundancies distract significantly from the paper, and to remedy this, the paper would have to change significantly in order to relate it properly to [1] clear is needed. I'd be curious on the ACs thoughts on this. \n\nI appreciate the authors' claim that their method can breach many of the popular defense methods out there, but we \nalso see that many of the percentages in Figure 1 converge  to 1. On the one hand this suggests that all defense methods \nare in some sense equally bad, but on the other, it could also just reflect on the fact that the thresholds are chosen \n\"too large\". I understand that many of the thresholds were inherited from previous work, but it would nevertheless help if the authors showed some example adversarial images to help baseline this Figure.  ", "title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SyepS_mP3m": {"type": "review", "replyto": "ryeoxnRqKQ", "review": "In this work the authors use a score-based adversarial attack (based on the natural evolution strategy (NES)) to successfully attack a multitude of defended networks, with success rates rivalling the best gradient-based attacks.\n\nAs confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]:\n\n* Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability.\n* Different motivation/derivation of NES.\n* Concept of adversarial distributions.\n* Regression network for good initialization.\n* Introduction of accuracy-iterations plots.\n\nMy main concerns are as follows:\n* The review of the prior literature, in particular on score-based and decision-based defences (the latter of which are not even mentioned), is very limited and is framed wrongly. In particular, the statement \u201cHowever, existing black-box attacks are weaker than their white-box counterparts\u201d is simply not true: as an example, the most prominent decision-based attack [2] rivals white-box attacks on vanilla DNNs as well as defended networks [3].\n* The concept of adversarial distributions is not new but is common in the literature of real-world adversarials that are robust to transformations and perturbations (like gaussian noise), check for example [4]. In [4] the concept of _Expectation Over Transformation (EOT)_ is introduced, which is basically the generalised concept of the expectation over gaussian perturbations introduced in this work.\n* While I like the idea of accuracy-iterations plots, the idea is not new, see e.g. the accuracy-iterations plot in [2] (sample-based, Figure 6), the loss-iterations plot in [5] or the accuracy-distortion plots in [3]. However, I agree that these type of visualisation or metric is not as widespread as it should be.\n\nHence, in summary the main contribution of the paper is the application of NES against different defence models, datasets and Lp metrics as well as the use of a regression network for initialisation. Along this second point it would be great if the authors would be able to demonstrate substantial gains in the accuracy-query metric. In any case, in the light of previous literature a major revision of the manuscript will be necessary.\n\n[1] Ilyas et al. (2018) \u201cBlack-box Adversarial Attacks with Limited Queries and Information\u201d (https://arxiv.org/abs/1804.08598) \n[2] Brendel et al. (2018) \u201cDecision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\u201d (https://arxiv.org/abs/1712.04248)\n[3] Schott et al. (2018) \u201cTowards the first adversarially robust neural network model on MNIST\u201d (https://arxiv.org/abs/1805.09190)\n[4] Athalye et al. (2017) \u201cSynthesizing Robust Adversarial Examples\u201d (https://arxiv.org/pdf/1707.07397.pdf)\n[5] Madry et al. {2017) \u201cTowards Deep Learning Models Resistant to Adversarial Attacks\u201d (https://arxiv.org/pdf/1706.06083.pdf)", "title": "Good evaluation but important prior work was missed which substantially reduces novelty and makes a major rewrite necessary", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SklRyhS5nQ": {"type": "rebuttal", "replyto": "ByeeVjH92m", "comment": "Got it. Thanks for the pointers! ", "title": "Got it"}, "HJe1uvS52Q": {"type": "rebuttal", "replyto": "H1eCOHrqhQ", "comment": "Oops, I misunderstood your earlier question.. We are running experiments against the vanilla PGD defended CNN. As Athalye et al. (2018) did not release this very strong model, we had to train it ourselves. Actually, we will ask them for that model by email now.. Stay tuned please.", "title": "Will do"}, "S1ekDzBc2m": {"type": "rebuttal", "replyto": "SkleNxHqhQ", "comment": "Since Therm discretizes the input, it prevents one from simply applying the gradient projection in PGD, not mentioning that the gradients are estimated through other methods like BPDA or DGA (Buckman et al., 2018). Did you mean that DGA is a more faithful application of (Madry et al., 2018)'s defense to Therm than LS-PGA? However, DGA has been shown a weak attack so it likely cannot lead to strong defense (e.g., called Therm-Adv-DGA) either.", "title": "Response to \"Vanilla PGD training\""}, "BylxbaE9nX": {"type": "rebuttal", "replyto": "S1gc1YNc3Q", "comment": "Regarding (2), thank @Nicholas for the catch! We will cite both (Buckman et al., 2018) and (Madry et al., 2018) in the revised paper. It is worth noting that vanilla PGD does not apply to Therm. As a result, the LS-PGA enhanced Therm-Adv is probably one of the best one can do in order to apply Madry et al. (2018)'s defense principle to Therm. \n\nRegarding (1), you may consider the Gaussian distribution along with the steps 1--4 in the paper as the threat model. We do not use any substitute network in our approach. The black-box setting: We query a black-box network by an input and obtain its output probability vector.  This setting is as standard as many existing works'.", "title": "Answers to (1) & (2)"}, "BkgZeTW5hX": {"type": "rebuttal", "replyto": "Bkx94ECt2m", "comment": "Thanks for asking. We will clarify our previous responses (mainly the paragraph below) by answering three of your questions.\n\n----------------------------------------\nOn the algorithmic aspect, both ours and Ilyas et al. (2018)\u2019s employ NES as the optimization algorithm. However, we arrive at it via different routes and for different purposes. We assume a probabilistic generation process of the adversarial examples (Steps 1\u20134, Section 2), which finds an adversarial example by a one-step addition to the input. In contrast, Ilyas et al. (2018)\u2019s modeling assumption is that an adversarial example can be found by PGD, which iteratively updates the original input with a small learning rate until it becomes adversarial. To this end, we use NES to estimate the parameters of the distribution, while Ilyas et al. (2018) use NES to replace the true (stochastic) gradients in PGD. We contend that, due to the non-differentiable clip and projection operations and the fairly large Gaussian covariance, NES is *not* an efficient (and possibly a biased) estimator of the true gradients \u2014 we are running experiments to empirically verify if this is true or not.  \n--------------------------------------------------------------\n\n== Q1: specify exactly what the difference between NES and your attack is? ==\n\nUsing our notation, the pseudo code below sketches our algorithm and Ilyas et al. (2018)\u2019s.\n\nOurs, which searches for the Gaussian from which more than one adversarial examples can be generated.\nIterate until convergence:\n1. Draw a sample {\\epsilon} from the normal distribution\n2. Transform it to a sample of Gaussian by {z=\\theta + \\sigma * \\epsilon}\n3. Generate current adversarial examples from {z} by steps 1\u20134\n4. Compute the losses {J(z)}\n5. Compute the search gradients {g} by equation (5)\n6. Update the Gaussian mean: \\theta = \\theta - r * g \nReturn \\theta\n\nIlyas et al. (2018)\u2019s, which searches for a single adversarial example.\nIterate until convergence:\n1. Draw a sample {e} from the normal distribution\n2. Transform it to a sample of zero-mean Gaussian by {z=0 + \\sigma * \\epsilon}\n3. Generate current adversarial examples by {x + z} and {x - z}\n4. Compute the losses {J(z)}\n5. Compute the search gradients {g} by equation (5)\n6. x = Projection(x - r * sign(g))\nReturn x\n\n\nThe differences start from the second line, where we transform the normal sample to a sample of the Gaussian N(\\theta, sigma^2) while Ilyas et al. (2018) transform it following a zero-mean Gaussian N(\\theta, sigma^2). \n\nLine 3: The difference is on how to generate the adversarial examples. \n\nLine 4: Slightly different loss functions are used in the two methods. This is not vital. \n\nLine 5 is the same for the two methods. \n\nLine 6: While we update the Gaussian mean by a gradient descent step, Ilyas et al. (2018) update the adversarial example by PGD.\n\n== Q2: a smaller standard deviation for sampling ==\nBy using the same setting for the NES component of our algorithm and Ilyas et al. (2018)\u2019s, including the same sample size and standard deviation, we obtain the comparison results below. Ours still performs better. We will complete the experiments with all the defense methods studied in our paper. \n\nTable 1: Success rate on attacking Randomization (ImageNet)\n# of iterations          30       90       150      210       270    300     360     400\nours                         21.54  78.58  90.02   95.41   95.5    95.5    95.5    95.5\nIlyas et al. (2018)'s  20.5    46.37  53.33   53.33   53.33  53.33  53.33  53.33 \n\n\n== Q3: you don't perform clipping ==\nWe did perform clipping in steps 1--4 of the paper, where we generate adversarial examples from a Gaussian distribution. In contrast, Ilyas et al. (2018)\u2019s performs the clipping of gradients due its employment of PGD attack. \n", "title": "Clarification"}, "BkxNNaW_hX": {"type": "rebuttal", "replyto": "HJg9cjD6o7", "comment": "With the open-source code released by Ilyas et al. (2018), we have evaluated their method on attacking three defense methods: SAP and Therm for CIFAR10 and Randomization for ImageNet. The results (success rate vs. number of optimization iterations) are shown in the tables below. We have also tested larger sample size and higher number of iterations for NES, and yet the results remain about the same. \n\nIlyas A, Engstrom L, Athalye A, Lin J. Black-box Adversarial Attacks with Limited Queries and Information. arXiv preprint arXiv:1804.08598. 2018 Apr 23.\n\nThe inferior attacking results of (Ilyas et al., 2018) verify our conjecture above, i.e., due to the non-differentiable clip and projection operations and the fairly large Gaussian covariance, NES is *not* an efficient (and possibly a biased) estimator of the true gradients of PGD. As a result, NES is not able to approach PGD\u2019s strong attack  performance. \n\nTable 1: Success rate on attacking SAP (CIFAR10)\n# of iterations           30       90       150      210       270     300      360      400\nOurs                          45.13  96.21  99.00   99.54   99.81   100      100      100\nIlyas et al. (2018)'s  33.36  34.51  36.03   37.36   37.36   37.36   37.36   37.36\n\nTable 2: Success rate on attacking Therm (CIFAR10)\n# of iterations            30       90       150      210       270     300       360     400\nOurs                          67.38   96.38  98.92   99.53   99.74   99.89   100     100\nIlyas et al. (2018)'s  59.22   83.32  83.82   84.32   85.33   85.33   85.33   85.33\n\nTable 3: Success rate on attacking Randomization (ImageNet)\n# of iterations          30       90       150      210       270    300    360    400\nOurs                         21.54  78.58  90.02   95.41   95.5    95.5   95.5   95.5\nIlyas et al. (2018)'s  3.33    4.56    6.77     8.5       8.5      8.5     8.5     8.5\n", "title": "Experimental comparison with \"Black-box Adversarial Attacks with Limited Queries and Information\" "}, "HJg9cjD6o7": {"type": "rebuttal", "replyto": "HkxngDtniX", "comment": "That\u2019s a great catch. Thank you very much! We should have read the paper before\u2026 It is intriguing (and yet disappointing for us) to see that a similar approach has been proposed (Ilyas et al. 2018) by also resorting to the natural evolution strategy (NES), but it is not surprising. After all, derivative-free methods, such as NES, REINFORCE, and the zero-th order algorithms, are a natural choice for the blackbox attack. \n\nWhile we mainly attack up to 10 recently published defense methods by the proposed approach, Ilyas et al. (2018) focus on attacking a vanilla neural network under the constraints of limited queries and information (e.g., top k entries as opposed to the full output vector). \n\nOn the algorithmic aspect, both ours and Ilyas et al. (2018)\u2019s employ NES as the optimization algorithm. However, we arrive at it via different routes and for different purposes. We assume a probabilistic generation process of the adversarial examples (Steps 1\u20134, Section 2), which finds an adversarial example by a one-step addition to the input. In contrast, Ilyas et al. (2018)\u2019s modeling assumption is that an adversarial example can be found by PGD, which iteratively updates the original input with a small learning rate until it becomes adversarial. To this end, we use NES to estimate the parameters of the distribution, while Ilyas et al. (2018) use NES to replace the true (stochastic) gradients in PGD. We contend that, due to the non-differentiable clip and projection operations and the fairly large Gaussian covariance, NES is *not* an efficient (and possibly a biased) estimator of the true gradients \u2014 we are running experiments to empirically verify if this is true or not.  \n\nIt is a conceptual change from the traditional attack methods (e.g., PGD) to the way of modeling the adversarial examples by a distribution. This change may enable some exciting future works. For instance, we can draw samples from the distribution to characterize the adversarial boundaries, efficiently do adversarial training, etc.\n\nAnother notable difference from (Ilyas et al. 2018)\u2019s is that we train a regression neural network to find a good initialization for NES. Experiments verify the benefit of this regression network. \n\nOn the experimental aspect, we attack the recently proposed defense methods following the protocols set up in the original papers. As a result, we experiment with both CIFAR10 and ImageNet, both the $\\ell_2$ and $\\ell_infty$ distances, and different types of defenses (e.g., input randomization and discretization, ensembeling, denoising, etc.). In contrast, Ilyas et al. (2018) experiment with ImageNet with an $\\ell_\\infty$ distance. In addition, we examine the adversarial examples\u2019 transferabilities across different defense methods. Unlike the findings about the transferability across vanilla neural networks, our results indicate several unique characteristics of the transferability of our adversarial examples for the defended neural networks (cf. Section 3.3). Finally, we plot the curves of the attack success rates versus the iteration numbers, a new evaluation scheme which is complementary to the final attack success rates. ", "title": "Differences from \"Black-box Adversarial Attacks with Limited Queries and Information\""}}}