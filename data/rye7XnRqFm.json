{"paper": {"title": "Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning", "authors": ["Fabio Pardo", "Vitaly Levdik", "Petar Kormushev"], "authorids": ["f.pardo@imperial.ac.uk", "v.levdik@imperial.ac.uk", "p.kormushev@imperial.ac.uk"], "summary": "Q-map is a reinforcement learning agent that uses a convolutional autoencoder-like architecture to efficiently learn to navigate its environment.", "abstract": "Goal-oriented learning has become a core concept in reinforcement learning (RL), extending the reward signal as a sole way to define tasks. However, as parameterizing value functions with goals increases the learning complexity, efficiently reusing past experience to update estimates towards several goals at once becomes desirable but usually requires independent updates per goal.\nConsidering that a significant number of RL environments can support spatial coordinates as goals, such as on-screen location of the character in ATARI or SNES games, we propose a novel goal-oriented agent called Q-map that utilizes an autoencoder-like neural network to predict the minimum number of steps towards each coordinate in a single forward pass. This architecture is similar to Horde with parameter sharing and allows the agent to discover correlations between visual patterns and navigation. For example learning how to use a ladder in a game could be transferred to other ladders later.\nWe show how this network can be efficiently trained with a 3D variant of Q-learning to update the estimates towards all goals at once. While the Q-map agent could be used for a wide range of applications, we propose a novel exploration mechanism in place of epsilon-greedy that relies on goal selection at a desired distance followed by several steps taken towards it, allowing long and coherent exploratory steps in the environment.\nWe demonstrate the accuracy and generalization qualities of the Q-map agent on a grid-world environment and then demonstrate the efficiency of the proposed exploration mechanism on the notoriously difficult Montezuma's Revenge and Super Mario All-Stars games.", "keywords": ["reinforcement learning", "goal-oriented", "convolutions", "off-policy"]}, "meta": {"decision": "Reject", "comment": "The paper proposes to use a convolutional/de-convolutional Q function over on-screen goal locations, and applied to the problem of structured exploration. Reviewers pointed out the similarity to the UNREAL architecture, the difference being that the auxiliary Q functions learned are actually used to act in this case.\n\nReviewers raised concerns regarding novelty, the formality of the writing, a lack of comparisons to other exploration methods, and the need for ground truth about the sprite location at training time. A minor revision to the text was made, but the reviewers did not feel their main criticisms were addressed. While the method shows promise, given that the authors acknowledge that the method is somewhat incremental, a more thorough quantitative and ablative study would be necessary in order to recommend acceptance."}, "review": {"HJx0bKkKhQ": {"type": "review", "replyto": "rye7XnRqFm", "review": "Focus on navigation problems, this paper proposes Q-map, a neural network that estimates the number of steps (in terms of the discount factor gamma) required to reach any position on the observable screen/window. Moreover, it is shown that Q-map can be applied for exploration, by trying to reach randomly selected goal.\n\nPros\n1. Novel goal-based exploration scheme\n\nCons\n1. Similar idea has been proposed before\nFor example, Dayan (1993) estimates the number of steps to reach any position on the map using successor representations. Discussion about this field (successor representations/features) is completely missing in the paper.\nRef:\n- Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613\u2013624, 1993.\n- Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, David Silver, and Hado van Hasselt. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4058\u20134068, 2017.\n- Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning, pp. 510\u2013519, 2018.\n\n2. Comparison to existing methods is only vaguely discussed\nFor example, it is claimed multiple times that UVFA requires the goal coordinates, but Q-map also requires coordinates when doing the exploration.\n\n3. The network architecture is not clearly presented\nFor example, the output of the network needs to be clipped, which suggests that there is no output transform. Since the predicted output is in [0,1], it would make sense to use Sigmoid transform for each pixel and use logistic loss.\n\n4. The proposed exploration scheme could be unnecessarily complicated\nSec.3.1 provides lengthy discussion about the drawback of eps-greedy exploration. Then in Sec.3.2, \\epsilon_r is basically the same as the eps-greedy algorithm, using to randomly select an action. Isn't this a \"bad\" thing as suggested in Sec.3.1? Moreover, the new exploration scheme requires two more hyper-parameters (min/max distance threshold), which will add more complication to the already very complicated deep RL learning procedure.\n\n5. Experiment results are limited\nFor the toy experiment in Sec.2.3, the map are relatively simple. The example of Dayan (1993) with an agent surrounded by walls is an interesting scenario and should be included. The proposed Q-map (ConvNet) could fail because it is hard to learn geodesic distance with only local information. More importantly, there is no comparison to similar methods in Sec.3. UVFA can replace Q-map to do similar exploration.\n\n6. Writing can be greatly improved\nThere are many grammar errors. To name a few, \"agent capable to produce\", \"the gridworld consist of\", \"in the thrist level\".\n\nMinors\n- UFV should be UVF in the introduction\n- Citation in Sec.3 is not consistent with the rest of the paper. Use \\citep or \\citet properly.", "title": "Do not have enough comparison to existing works; need to improve writing", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1lOBG5KCX": {"type": "rebuttal", "replyto": "HJx0bKkKhQ", "comment": "1. Successor features, a generalization of Dayan\u2019s successor representation, propose a framework for transfer learning when the reward function changes between tasks but not the environment\u2019s dynamics. In Dyan (1993), the experiment shows how the successor representations predict the future state occupancy under the current policy when trained to reach a particular goal and describes how the learning is affected when the goal location is changed. We believe this literature is quite different from Q-map which directly and simultaneously learn how to reach every possible goals and is task-independent.\n\n2. While UVFA requires a goal to be provided in input of the neural network, Q-map doesn't as it produces the Q-values towards all possible goals at once in output. This implies a few algorithmic differences between the two approaches when used for the proposed exploration: 1) During the goal-selection step or training, the values for all goals are queried or updated in one pass through the network while would require as many passes as there are goals with UVFA. 2) When trying to reach a given goal, the Q-values at the proper location in output are used for Q-map while this goal would just be provided in input for UVFA.\n\n3. We have tested regression with various non-linearities in output but have found them to perform worse. For example, sigmoids tend to squeeze values to either 0 (the goal can't be reached) or 1 (the goal can be reached in one step). Furthermore, clipping is only performed when creating the target Q-frames as always clipping the output of the network would not give any gradient for values outside of (0, 1).\n\n4. We have retained a minimal amount of purely random actions for several reasons: 1) They are necessary for Q-map's own exploration 2) They allow DQN to discover actions which may not be helpful for navigating the environment, such as hitting blocks to gain coins in Mario 3) The proportion of random actions used is significantly smaller than what is used in the baseline, thus the drawbacks, such as \u201cwasteful\u201d actions, are reduced.\n\n5. We agree that such environments would have been an interesting test for the Q-map. Given the time available for the rebuttal we will have to consider these for future work. Yes, UVFA could be used instead of Q-map, but it would likely be computationally slower as every possible goal would need to be passed in input and have worse learning performance due to the lack of deconvolutional architecture to facilitate generalization. Such a comparison could also be worthwhile for future work.", "title": "Response to AnonReviewer1"}, "rJxSZbqFCm": {"type": "rebuttal", "replyto": "BklVETbY3Q", "comment": "We agree with some of the pointed similarities with UNREAL, and now reference it in the paper. The autoencoder architecture and Q-learning used for its pixel-control auxiliary task are indeed similar. However, the meaning of the review's use of the term \"spatial goals\" is not very clear to us, as the pixel-control auxiliary task's purpose is to maximize the on-screen pixel value change, and has no notion of goal-oriented RL. Furthermore, the learned values are not used in any practical manner. Q-map on the other hand, is trained to minimize the number of steps towards all goal coordinates which can be used for a variety of applications, such as exploration as shown in the paper, goal-oriented control (e.g. if the task is to reach some coordinates), or hierarchical RL.\n\nWhile we agree that the necessity to localize the agent or a target object in the environment is significant, we would like to point out that it is a common assumption in goal-oriented RL, and is not impractical for certain areas of research, such as robotics. We chose to use Montezuma\u2019s Revenge and Mario for their complexity and their role in various previous papers on exploration. We do not believe it was worthwhile showing performance chart for Montezuma\u2019s Revenge, as the baseline random exploration never reached the key and we did not use environmental rewards.", "title": "Response to AnonReviewer3"}, "rklgYx5Y07": {"type": "rebuttal", "replyto": "SJgnyWWqh7", "comment": "First, we would like to clarify that this paper makes two main contributions: 1) Q-map: a way to simultaneously learn to reach coordinates and 2) DQN + Q-map: a way to use Q-map for exploration. Unfortunately the review\u2019s points did not address 1).\n\n(ii) We do reference these works in section 3.1, however, as most of them still use epsilon-greedy as part of their algorithms, our proposed method can be directly integrated with them. To isolate the impact of taking multiple steps in the direction of a goal versus random actions, we chose to only use a standard DQN agent.\n\n(iii) We unfortunately do not have results with the exact same experimental setup without goal biasing but during preliminary experiments we found that a goal biasing of 50% gave a performance boost on Mario. The experiment with Montezuma's Revenge does not use any biasing however as no reward was used and thus no DQN was trained.\n\n(iv) By exploratory actions we mean individual actions that are not greedy for the task (completely random or goal-directed). To have a fair comparison between epsilon-greedy exploration and the proposed exploration using Q-map, we ensure that these exploratory actions are following the same schedule, linearly decaying through the training.\n\n(v) The quoted training method was specifically used for the gridworld environment that was designed to evaluate the training of the Q-map under ideal conditions with a nearly uniform coverage of all transitions. For the experiments with Mario and Montezuma's Revenge the goal was to evaluate the proposed exploration algorithm, we therefore used the original starting states at the beginning of the levels.\n\n(vi) We added a new experiment using a Q-map trained first on level 1.1 and then on level 2.1. We noticed faster training and some notions of generalization even though the two levels use different tilesets and backgrounds. The videos and code are available on the website.", "title": "Response to AnonReviewer2"}, "SJxwFycY0Q": {"type": "rebuttal", "replyto": "rye7XnRqFm", "comment": "We thank the reviewers for their constructive feedback. It has helped us improve the quality of the paper and gave us directions for future work.\n\nSince the original submission, we have updated the paper and improved the website https://sites.google.com/view/q-map-rl with some new videos and cleaner source code.", "title": "General comment"}, "SJgnyWWqh7": {"type": "review", "replyto": "rye7XnRqFm", "review": "Authors propose to overcome the sparse reward problem using an exploration strategy that incentivizes the agent to visit different parts of the game screen. This is done by building Q-maps, a 3D tensor that measures the value of the agent's current state (defined as the position of the agent) and action in reaching other (x, y) locations in the map. Each 2D slice of the Q-map measures the value at different (x, y) locations for one action. Such 2D slices (i.e. channels) are stacked together to form the Q-map. Taking the max across the channels, thus, provides the Q-value for the optimal action. \n\nA policy for maximizing the rewards is trained using DQN. The Q-map based exploration is used as a replacement for \\epsilon-greedy exploration. \n\nThe Q-map is used for exploration in the following way:\n(a) Chose a random action with probability \\epsilon_r. \n(b) If neither a random action nor a \"goal\" is chosen, a new goal is chosen with probability \\epislon_g. The goal is a (x, y) location, chosen so that is not too hard or too easy to reach it (i.e. Q-map values are neither too high or low; intuitively [1 - Q-map(x, y, a)] (for normalized/clipped Q) is a measure of distance of the goal).  \n     -- If a \"goal\" is chosen, the greedy action to go towards the goal is chosen. \n(c) If neither a goal or random action is chosen, DQN is used to chose the greedy exploration. \n\nAuthors also bias the goal selection to match DQN's greedy action. This is done as following -- from a set of goals that satisfy (b) above; chose the goal for which Q-map selected action matches the DQN's greedy action. \n\nResults are presented on simple 2D maze environments, Mario and Montezuma's revenge. \n\nI have multiple concerns with the papers:\n(i) The writing is informal and the ideas are not well explained. It would really benefit -- if authors introduce an algorithm box or talk about the method as a sequence of points. Right now, the ideas are scattered throughout the paper. I am still confused by figure 3 -- when are random goals chosen? Do random goals correspond to (b) above? Also, when the Horde architecture, GVF and UVF are mentioned, the references are missing -- I would love for the authors to include the corresponding  references.  \n\n(ii) The idea of reaching as many states as possible has been explored in count based visitation (Bellemare et al, Tang et al) \u2014 but no comparisons have been made to any previous work. Its always good to put a new work in the perspective of old work with similar ideas. \n\n(iii) The authors propose biased and random goal sampling \u2014 I would love to see how much improvement does biased goal sampling offer over random goal sampling. \n\n(iv) \u201c\u2026compare the performance of our proposed agent and a baseline DQN with a similar proportion of exploratory actions\u201d .. I don\u2019t agree with this a metric \u2014 I think the total number of steps is a good metric. Exploration is part of the agent\u2019s algorithm to find the goal, we shouldn\u2019t compare against DQN by matching the number of exploratory actions. \n\n(v) \u201cThe Q-map is trained with transitions generated by randomly starting from any free locations in the environment and taking a random action.\u201d Does this mean that when the agent is trained with Mario \u2014 the game is reset after every episode and the agent is placed a random starting location? If yes, then this is not a realistic assumption. \n\n(vi) I would like to see \u2014 how do Q-maps generalize across levels of Mario or Montezuma\u2019s revenge? Does Q-map trained on level-1 help in good exploration on future levels without any further fine-tuning? \n\nOverall, I like the idea of incentivizing exploration without changing the reward function as is done in multiple prior works. However, I think more thorough quantitative evaluation is required and it will be interesting to see transfer of Q-maps outside the 2D-domains. I am happy to increase my score if such evidence is provided. \n\nOther references worth including:\n(a) Strategies for goal generation: Automatic Goal Generation for Reinforcement Learning Agents (https://arxiv.org/abs/1705.06366) ", "title": "Interesting Idea, but not well evaluated", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BklVETbY3Q": {"type": "review", "replyto": "rye7XnRqFm", "review": "The main idea in the paper is to use on-screen locations as goals for an RL agent. Using a de-convolutional network to parameterize the Q-function allows all goals to be updated at once and correlations between nearby or similar goal locations could be modelled. The paper explores how this type of goal space can be used for better exploration showing modest improvement in scores on Super Mario.\n\nClarity - The paper is well written and easy to follow. The Q-map architecture is well motivated and intuitive and the exploration strategy based on Q-maps is interesting.\n\nNovelty - The idea of using spatial goals combined with a de-convolutional architecture is not new and goes back at least to \u201cReinforcement Learning with Unsupervised Auxiliary Tasks\u201d by Jaderberg et al.. The UNREAL agent used the same type of de-convolutional \u201cQ-map\u201d to update a spatial grid of goals all at once. The main difference is that the UNREAL agent learns about spatial goals as an auxiliary task and does not execute/act on the goals like the Q-map agent. Nevertheless, the type of architecture and algorithm (called 3D Q-learning in this paper) is essentially the same.\n\nSignificance - The Q-map architecture requires access to the position of the avatar on the screen at training time. I would expect that using such a significant part of the agent\u2019s true state during training should lead to a significant improvement in performance at test time. Why not evaluate the proposed exploration strategy on well known hard exploration tasks? The results on Montezuma\u2019s Revenge are only qualitative. There Q-map agent did outperform an epsilon-greedy DQN baseline on Super Mario but the improvement does not seem very significant given how much prior knowledge Q-map was given compared to the baseline. It is also not clear how much of the improvement comes from training the Q-map as an auxiliary task and how much of it comes from better exploration.\n\nOverall quality - Given that the architecture is not very novel and requires the avatar\u2019s position to train I did not find the qualitative or quantitative results compelling enough. Perhaps the authors could show that the exploration strategy works well on several difficult exploration games. Another possibility would be to showcase other ways to use the Q-map, for example in an HRL setup.\n\nMinor comment - Some sections seem to be missing references. For example, the second paragraph of the introduction discusses GVFs and the Horde architecture without any references.", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}