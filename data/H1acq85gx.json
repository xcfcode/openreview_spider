{"paper": {"title": "Maximum Entropy Flow Networks", "authors": ["Gabriel Loaiza-Ganem *", "Yuanjun Gao *", "John P. Cunningham"], "authorids": ["gl2480@columbia.edu", "yg2312@columbia.edu", "jpc2181@columbia.edu"], "summary": "", "abstract": "Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, rather than the traditional method of optimizing over the continuous density directly, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself.  By exploiting recent developments in normalizing flow networks, we cast the maximum entropy problem into a finite-dimensional constrained optimization, and solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "This paper is an interesting application to maximum entropy problems, which are not widely considered in deep learning. The reviewers have concerns over the novelty of this method and the ultimate applicability and scope of these methods. But this method should be of interest, especially in connecting deep learning to the wider community on information theory and should make an interesting contribution to this year's proceedings."}, "review": {"HyL6veZJb": {"type": "rebuttal", "replyto": "H1acq85gx", "comment": "Added more data as shown on the conference poster, per audience request.", "title": "Revision"}, "r1zGLUAUl": {"type": "rebuttal", "replyto": "H1acq85gx", "comment": "Due to the upcoming end of the rebuttal period this Friday, we kindly remind our reviewers of the recent major update that we did to the paper, and discussions that we added. We feel that we addressed the main concerns raised by the reviewers and would be glad if the reviewers could have a look and share any feedback. Thank you very much.", "title": "Friendly reminder"}, "ByTL0QVLg": {"type": "rebuttal", "replyto": "H1acq85gx", "comment": "We thank AnonReviewer4 for the input.  We respond to your points individually:\n\nFirst, you make an interesting connection with the KLD, but that is not quite applicable here: our formulation has c_k(E T_k)^2 instead of c_k E(T_k ^2), which can't be cast straightforwardly in a VI framework. Having this term is useful because:\n\n+ The true optimum has log density sum eta_i T_i (eta is as in equation 2), with no c_i T_i^2 term. If we didn't have the last term, we would be minimizing the KLD to a log density sum lambda_i T_i, which would only recover the maximum entropy distribution if lambda actually corresponded to eta. Like we mentioned on the paper, computing eta is computationally intractable.\n\n+ Having this term enables the use of the augmented Lagrangian method, giving us the theoretical guarantees discussed on our previous answers, which ensures that we recover the maximum entropy distribution (up to regularity and expressivity of the model class, as discussed).\n\nSecond, you commented about novelty in the paper.  We respectfully disagree.  For example, the above KLD example is in fact different than suggested, so the MEFN is not the typical variational inference setup.  Further, we think the introduction of the ME problem is novel to the deep learning community, and conversely, we have introduced deep learning to the ME problem, a literature in which there is substantial need for estimation and sampling techniques.  Mechanically, constrained optimization is rarely used in the deep learning community, and we introduce new steps there.  Finally, per your and others\u2019 requests, we think our new expanded results section adds novelty and state of the art performance to those problem domains (see below).\n\nThird, you asked for more complex data.  Thank you.  We have significantly addressed this concern in the new version of the paper (see updated pdf).  Section 4.3 now details a texture synthesis problem, and demonstrates that the MEFN performs consistently with state of the art in terms of matching texture-net moment constraints, all while outperforming substantially in terms of sample variability.  As the purpose of this class of implicit generative models is to generate a diverse sample set of plausible images, having the ME component in this problem domain is critical and adds to the novelty and impact of our work. \n\nTo your point about step 8 of our algorithm, it simply corresponds to the stochastic version of the usual augmented Lagrangian method, namely lambda_{k+1} = lambda_k + c_k * T(phi_k). This basically corresponds to a gradient step in lambda with step size c_k. The step size is justified by the theoretical derivation of the augmented Lagrangian method, which can be read in detail in Bertsekas (2014).", "title": "Last Review Answer"}, "Bkk0aXELe": {"type": "rebuttal", "replyto": "H1acq85gx", "comment": "We have significantly revised the paper in response to reviewer and other comments.  Major changes include:\n\n+ Most significantly, we added new experiments applying MEFN to texture synthesis (Section 4.3).  Current state-of-the-art (Ulyanov et al 2016) defines a complicated texture loss and then learns a network that generates images with a small texture loss, with excellent results.  One potential downside in this construction, common to many state of the art deep learning approaches, is that there is no objective encouraging sample diversity; this implies the extreme pathological case where the distribution is simply a point mass on the training image.   We apply the real NVP (Dinh et al 2016), a recently proposed flow tailored to image applications, as our network structure in an MEFN framework.  The MEFN provides images with qualitative textures at least as good as existing state of the art (and this is quantitatively supported), and critically the MEFN produces samples with significantly more diversity (as quantified in a variety of ways).  This experiment highlights the difficulty of obtaining the max entropy distribution in high dimensions and with complicated constraints, and our MEFN handles this setting well and improves the current state-of-the-art in texture synthesis.  As such we feel that this enhanced results section speaks to any concerns about experiments and further increases the novelty and utility of the MEFN framework.\n\n+ In line with the point about sample diversity, we also augmented our Dirichlet experiments of Section 4.1 to show the danger of not considering the entropy term: there moment matching is achieved per the objective, but sample diversity is significantly lost.  This result foreshadows the more important implication of this fact seen in Section 4.3.\n\n+ We have thoroughly revised the text to respond to other reviewer comments and to clarify the contributions and novelties introduced in this work, which we and the reviewers felt were not adequately emphasized before.", "title": "Significant Paper Update"}, "r1sa-EwEe": {"type": "rebuttal", "replyto": "H1acq85gx", "comment": "We thank our reviewers for their input. We did some minor updates to the manuscript. Here are our answers to the points raised in the reviews:\n\n1. Augmented Lagrangian with stochastic objective and constraints, and how our use of a hypothesis test helps (AnonReviewer2 and AnonReviewer1):\n\nThe augmented Lagrangian method transforms a constrained optimization problem into a sequence of unconstrained optimization problems (similar to a log barrier or other interior point methods).  Thus, as long as we are confident in convergence for each unconstrained problem, the overall convergence of the augmented Lagrangian is inherited. This fact remains the case regardless of the underlying optimizer, be it a standard noiseless gradient method, or (as is in this case) an SGD method.  This explanation certainly is somewhat informal, but our experience empirically is that there are no incremental problems with a series of SGD unconstrained optimizations save the minor issue addressed below.\n\nA potential minor issue with the augmented Lagrangian method is that if c is too large, although theoretically not an issue, in practice this will make the unconstrained problems ill-conditioned thus making it hard to solve them numerically. The update rules for c are designed to address this issue. In our experiments we found that sometimes the random nature of our constraint estimates caused c to be updated when it shouldn't (this was the case for the Dirichlet experiments, not the financial ones). Our hypothesis test solution aims to have a more conservative updating rule in order to avoid this issue, and it performed well in practice. It should also be noted that when the number of samples used for the hypothesis test goes to infinity, the hypothesis test becomes the regular update rule (i.e. noiseless).\n\nTo the best of our knowledge, there are few sources in the literature on constrained stochastic optimization. There is research addressing constrained optimization when we only have access to a single sample ([1]), but this is not our case as we have access to as many samples as desired. There is a paper ([2]) in which it is proved that alternating a gradient step with a lambda update will work (similar to what we do: alternating optimizing with many gradient steps and performing a lambda update), but the issue of updating c is not touched there, it is just assumed that c is large enough. This does however, at least partially, justify our augmented Lagrangian approach.\n\n\n2. Higher-dimensional Experiments (AnonReviewer2 and AnonReviewer1):\n\nFirst, we note that evaluating the baseline in higher-dimensional settings is not trivial: recovering the Gibbs distribution is computationally intractable.  That said, we are currently working on applying our method to generate texture images. We believe this might be a particularly interesting higher dimensional application, as the networks trained to accomplish this task are usually trained to match some statistics, having no guarantee that the generated images are \"fair\" samples of the objective texture instead of simply very similar images to the input one.  To that end, another paper currently under review at ICLR, \"What does it take to generate natural textures?\" by Ivan Ustyuzhaninov, Wieland Brendel, Leon Gatys and Matthias Bethge (https://openreview.net/forum?id=BJhZeLsxx), is asking directly this question that our method can address: one reviewer asked, \"since it is clear that only reproducing an image statistics is meaningless (as the best possible result is the original image), can the authors discuss how this family of texture generation methods should, at least in principle, be tested?\", to which the authors replied: \"In principle, one would need to estimate the entropy of the samples. This is hard for optimization-based texture synthesis models\", which highlights the effect our method could potentially have for this high-dimensional application.\n\nWe expect to have results for this setting before the end of the review period.\n\n\n3. Novelty (AnonReviewer2):\n\nWe believe the following points make our approach a meaningfully novel contribution:\n\n+Maximum entropy modeling has not been addressed in the Deep Learning literature.\n+Constrained optimization has also been largely untouched in the Deep Learning literature.\n+Our work identifies the role that invertible networks (with easily computable Jacobians) can have for entropy estimation.\n+There is a large community that uses maximum entropy models; for this group, a method which allows one to trivially sample from the ME distribution is highly novel and impactful.\n+Our method could achieve the sample diversity that some other network methods don't currently have (e.g. texture modeling, see previous comment re high-d experiments).\n\n\n4. Additional comments:\n\n-AnonReviewer2 asked for more details about how the bottom right panel of figure 2 was obtained:\n\nWe know the parameter of the p* Dirichlet. We placed a grid around that specific parameter value and for each of those, we computed the corresponding KL to p* as well as the MMD test p-value.\n\n-AnonReviewer1 asked about the appropriateness of when to use maximum entropy modeling:\n\nAs mentioned in the review, ME modeling can be used as an alternative to maximum likelihood: instead of selecting model parameter values based on data, the constraint values can be computed from data and then the ME distribution matching those constraints can be estimated. ME modeling is also frequently used in the following scenario: Suppose we want to know if a statistic T is enough to summarize the data. We can compute T for the data, obtain the corresponding ME distribution and generate synthetic data from it. Then, we can perform a hypothesis test to know if the synthetic data comes from the same distribution as the original data. If it does, T does summarize the data.\n\n-AnonReviewer1 inquired about the support constraint:\n\nThe way we deal with the support constraint on the paper is indeed by adding a final layer with an easy-to-compute Jacobian that maps to the desired support. We feel that this can be done in most applications, for example, it is easy to enforce positivity, any kind of coordinate-wise boundedness, being on the simplex, matrices being positive-definite, etc. If for a specific problem this can't be achieved, an additional term could be added to the augmented Lagrangian to enforce as well the support constraint.\n\n[1] Wang W, Ahmed S. Sample average approximation of expected value constrained stochastic programs. Operations Research Letters. 2008 Sep 30;36(5):515-9.\n\n[2] Polyak BT. Methods for solving constrained extremum problems in the presence of random noise. USSR Computational Mathematics and Mathematical Physics. 1979 Dec 31;19(1):72-81.", "title": "Reviews answers"}, "Sy2Zsig4g": {"type": "rebuttal", "replyto": "rJCZQnpXe", "comment": "We added a short discussion about the regularity conditions on the paper. We give a more thorough one here. The conditions are:\n\n(i) There exists a strict local optimum phi* of the optimization problem:\n\nIf the function class is rich enough that it contains a true solver  of the maximum entropy problem, then a global optimum exists. Although not rigorous, we would expect that even in the finite expressivity case that a global optimum remains, and indeed, recent theoretical work ([1,2]) has gotten close to proving this.\n\n(ii) phi* is a regular point of the optimization problem, that is, the rows of the Jacobian of T at phi* are linearly independent:\n\nAgain, this is not formal, but we should not expect this to cause any issues. This clearly depends on the specific form of T, but the condition basically says that there should not be redundant constraints at the optimum, so if T is reasonable this shouldn't happen.\n\n(iii) H(p_phi) and T(phi) are twice continuously differentiable on a neighborhood around phi*:\n\nThis holds by the smoothness of the normalizing flows.\n\n(iv) y'Hy>0 for every non-zero y such that J'y=0, where H is the Hessian of the Lagrangian (not the augmented Lagrangian) at phi* and J is the Jacobian of T at phi*:\n\nThis condition is harder to justify. It would appear it is just asking that the Lagrangian be strictly convex in feasible directions, but it is actually stronger than this and some simple functions might not satisfy the property. For example, if the function we are optimizing was x^4 and we had no constraints, the Lagrangian's Hessian would be 12x^2, which is 0 at x*=0 thus not satisfying the condition. Importantly, these conditions are sufficient but not necessary, so even if this doesn't hold the augmented Lagrangian method might work (it certainly would for x^4). Because of this and the non-rigorous justifications of conditions i-ii, we did not go into more details about these conditions on the paper and relied instead on the empirical performance to justify that we are indeed recovering the maximum entropy distribution.\n\nIf all of these conditions hold, the augmented Lagrangian (for large enough c and lambda close enough to the real Lagrange multiplier) has a unique optimum in a neighborhood around phi* that is close to phi* (as c goes to infinity, it converges to phi*) and its hessian at this optimum is positive-definite. This implies that gradient descent (with the usual caveats of being started close enough to the solution and with the right steps) will correctly recover phi* using the augmented Lagrangian method.  This of course just guarantees convergence to a local optimum, but if there are no additional assumptions such as convexity, it can be very hard to ensure that it is indeed a global optimum. Some recent research has attempted to explain why optimization algorithms perform so well for neural networks ([3,4]), but we leave such attempts for our case for future research.\n\n[1] Raghu M, Poole B, Kleinberg J, Ganguli S, Sohl-Dickstein J. On the expressive power of deep neural networks. arXiv preprint arXiv:1606.05336. 2016 Jun 16.\n\n[2] Poole B, Lahiri S, Raghu M, Sohl-Dickstein J, Ganguli S. Exponential expressivity in deep neural networks through transient chaos. InAdvances in Neural Information Processing Systems 2016 (pp. 3360-3368).\n\n[3] Choromanska A, Henaff M, Mathieu M, Arous GB, LeCun Y. The Loss Surfaces of Multilayer Networks. InAISTATS 2015.\n\n[4] Kawaguchi K. Deep Learning without Poor Local Minima. arXiv preprint arXiv:1605.07110. 2016 May 23.", "title": "Augmented Lagrangian conditions"}, "rJCZQnpXe": {"type": "review", "replyto": "H1acq85gx", "review": "You refer to the regularity conditions for solving (6), but do not say anything about how those conditions do (or don't) hold in your scenario. Can you say a little about this?Much existing deep learning literature focuses on likelihood based models. However maximum entropy approaches are an equally valid modelling scenario, where information is given in terms of constraints rather than data. That there is limited work in flexible maximum entropy neural models is surprising, but  is due to the fact that optimizing a maximum entropy model requires (a) establishing the effect of the constraints on some distribution, and formulating the entropy of that complex distribution. There is no unbiased estimator of entropy from samples alone, and so an explicit model for the density is needed. This challenge limits approaches. The authors have identified that invertible neural models provide a powerful class of models for solving the maximum entropy network problem, and this paper goes on to establish this approach. The contributions of this paper are (a) recognising that, because normalising flows provide an explicit model for the density, they can be used to provide unbiased estimators for the entropy (b) that the resulting Lagrangian can be implemented as a relaxation of a augmented Lagrangian (c) establishing the practical issues in doing the augmented Lagrangian optimization. As far as the reviewer is aware this work is novel \u2013 this approach is natural and sensible, and is demonstrated on an number of models where clear evaluation can be done. Enough experiments have been done to establish this is an appropriate method, though not that it is entirely necessary \u2013 it would be good to have an example where the benefits of the flexible flow transformation were much clearer. Further discussion of the computational and scaling aspects would be valuable. I am guessing this approach is probably appropriate for model learning, but less appropriate for inferential settings where a known model is then conditioned on particular instance based constraints? Some discussion of appropriate use cases would be good. The issue of match to the theory via the regularity conditions has been brought up, but it is clear that this can be described well, and exceeds most of the theoretical discussions that occur regarding the numerical methods in other papers in this field.\n\nQuality: Good sound paper providing a novel basis for flexible maximum entropy models.\nClarity: Good.\nOriginality: Refreshing.\nSignificance: Significant in model development terms. Whether it will be an oft-used method is not clear at this stage.\n\nMinor issues\n\nPlease label all equations. Others might wish to refer to them even if you don\u2019t.\nTop of page 4: algorithm 1\u2192 Algorithm 1.\nThe update for c to overcome stability appears slightly opaque and is mildly worrying.  I assume there are still residual stability issues? Can you comment on why this solves all the problems?\nThe issue of the support of p is glossed over a little. Is the support in 5 an additional condition on the support of p? If so, that seems hard to encode, and indeed does not turn up in (6). I guess for a Gaussian p0 and invertible unbounded transformations, if the support happens to be R^d, then this is trivial, but for more general settings this seems to be an issue that you have not dealt? Indeed in your Dirichlet example, you explicitly map to the required support, but for more complex constraints this may be non trivial to do with invertible models with known Jacobian? It would be nice to include this in the more general treatment rather than just relegating it to the specific example.\n\nOverall I am very pleased to see someone tackling this question with a very natural approach.", "title": "Berteskas 2014 conditions", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryivmuWNg": {"type": "review", "replyto": "H1acq85gx", "review": "You refer to the regularity conditions for solving (6), but do not say anything about how those conditions do (or don't) hold in your scenario. Can you say a little about this?Much existing deep learning literature focuses on likelihood based models. However maximum entropy approaches are an equally valid modelling scenario, where information is given in terms of constraints rather than data. That there is limited work in flexible maximum entropy neural models is surprising, but  is due to the fact that optimizing a maximum entropy model requires (a) establishing the effect of the constraints on some distribution, and formulating the entropy of that complex distribution. There is no unbiased estimator of entropy from samples alone, and so an explicit model for the density is needed. This challenge limits approaches. The authors have identified that invertible neural models provide a powerful class of models for solving the maximum entropy network problem, and this paper goes on to establish this approach. The contributions of this paper are (a) recognising that, because normalising flows provide an explicit model for the density, they can be used to provide unbiased estimators for the entropy (b) that the resulting Lagrangian can be implemented as a relaxation of a augmented Lagrangian (c) establishing the practical issues in doing the augmented Lagrangian optimization. As far as the reviewer is aware this work is novel \u2013 this approach is natural and sensible, and is demonstrated on an number of models where clear evaluation can be done. Enough experiments have been done to establish this is an appropriate method, though not that it is entirely necessary \u2013 it would be good to have an example where the benefits of the flexible flow transformation were much clearer. Further discussion of the computational and scaling aspects would be valuable. I am guessing this approach is probably appropriate for model learning, but less appropriate for inferential settings where a known model is then conditioned on particular instance based constraints? Some discussion of appropriate use cases would be good. The issue of match to the theory via the regularity conditions has been brought up, but it is clear that this can be described well, and exceeds most of the theoretical discussions that occur regarding the numerical methods in other papers in this field.\n\nQuality: Good sound paper providing a novel basis for flexible maximum entropy models.\nClarity: Good.\nOriginality: Refreshing.\nSignificance: Significant in model development terms. Whether it will be an oft-used method is not clear at this stage.\n\nMinor issues\n\nPlease label all equations. Others might wish to refer to them even if you don\u2019t.\nTop of page 4: algorithm 1\u2192 Algorithm 1.\nThe update for c to overcome stability appears slightly opaque and is mildly worrying.  I assume there are still residual stability issues? Can you comment on why this solves all the problems?\nThe issue of the support of p is glossed over a little. Is the support in 5 an additional condition on the support of p? If so, that seems hard to encode, and indeed does not turn up in (6). I guess for a Gaussian p0 and invertible unbounded transformations, if the support happens to be R^d, then this is trivial, but for more general settings this seems to be an issue that you have not dealt? Indeed in your Dirichlet example, you explicitly map to the required support, but for more complex constraints this may be non trivial to do with invertible models with known Jacobian? It would be nice to include this in the more general treatment rather than just relegating it to the specific example.\n\nOverall I am very pleased to see someone tackling this question with a very natural approach.", "title": "Berteskas 2014 conditions", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkCcx3S7e": {"type": "rebuttal", "replyto": "SyPFdDZXg", "comment": "We added the Q-Q plot in figure 3. We also added the the following analysis about the Q-Q plot:\nIn the middle panel we show a Q-Q plot of the quantiles of the MEFN and Gibbs distributions. We can see that the quantile pairs match the identity closely, which should happen if both methods recovered the exact same distribution. This highlights the effectiveness of MEFN.  There does exist a small mismatch in the tails: the distribution inferred by MEFN has slightly heavier tails. This mismatch is difficult to interpret: given that both the Gibbs and MEFN distributions are fit with option price data (and given that one can observe at most one value from the distribution, namely the stock price at expiration), it is fundamentally unclear which distribution is superior, in the sense of better capturing the true ME distribution\u2019s tails.", "title": "Q-Q Plot"}, "SyPFdDZXg": {"type": "review", "replyto": "H1acq85gx", "review": "In the experiment with financial data in section 4.2, how does the Gibbs method and the MEFN differ in the modeling of the tails of the price density? From figure 3 it seems that MEFN produces slightly heavier tails. Perhaps the authors could report QQ plots for bot distributions and the observed quantiles in the test data.The authors propose a new approach for estimating maximum entropy distributions\nsubject to expectation constraints. Their approach is based on using\nnormalizing flow networks to non-linearly transform samples from a tractable\ndensity function using invertible transformations. This allows access to the\ndensity of the resulting distribution. The parameters of the normalizing flow\nnetwork are learned by maximizing a stochastic estimate of the entropy\nobtained by sampling and evaluating the log-density on the obtained samples.\nThis stochastic optimization problem includes constraints on expectations with\nrespect to samples from the normalizing flow network. These constraints are\napproximated in practice by sampling and are therefore stochastic. The\noptimization problem is solved by using the augmented Lagrangian method. The\nproposed method is validated on a toy problem with a Dirichlet distribution and\non a financial problem involving the estimation of price changes from option\nprice data.\n\nQuality:\n\nThe paper seems to be technically sound. My only concern would the the approach\nfollowed to apply the augmented Lagrangian method when the objective and the\nconstraints are stochastic. The authors propose their own solution to this\nproblem, based on a hypothesis test, but I think it is likely that this has\nalready been addressed before in the literature. It would be good if the\nauthors could comment on this.\n\nThe experiments performed show that the proposed approach can outperform Gibbs\nsampling from the exact optimal distribution or at least be equivalent, with\nthe advantage of having a closed form solution for the density.\n\nI am concern about the difficulty of he problems considered.\nThe Dirichlet distributions are relatively smooth and the distribution in the\nfinancial problem is one-dimensional (in this case you can use numerical\nmethods to compute the normalization constant and plot the exact density).\nThey seem to be very easy and do not show how the method would perform in more\nchallenging settings: high-dimensions, more complicated non-linear constraints,\netc...\n\nClarity:\n\nThe paper is clearly written and easy to follow.\n\nOriginality:\n\nThe proposed method is not very original since it is based on applying an\nexisting technique (normalizing flow networks) to a specific problem: that of\nfinding a maximum entropy distribution. The methodological contributions are\nalmost non-existing. One could only mention the combination of the normalizing\nflow networks with the augmented Lagrangian method. \n\nSignificance:\n\nThe results seem to be significant in the sense that the authors are able to\nfind densities of maximum entropy distributions, something which did not seem\nto be possible before. However, it is not clearly how useful this can be in\npractice. The problem that they address with real-world data (financial data)\ncould have been solved as well by using 1-dimensional quadrature. The authors\nshould consider more challenging problems which have a clear practical\ninterest.\n\nMinor comments:\n\nMore details should be given about how the plot in the bottom right of Figure 2 has been obtained.\n\n\"a Dirichlet whose KL to the true p\u2217 is small\": what do you mean by this? Can you give more details on how you choose that Dirichlet?\n\nI changed updated my review score after having a look at the last version of the paper submitted by the authors, which includes new experiments.", "title": "Performance of MEFN in the modeling of the tails", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByL7eIWEx": {"type": "review", "replyto": "H1acq85gx", "review": "In the experiment with financial data in section 4.2, how does the Gibbs method and the MEFN differ in the modeling of the tails of the price density? From figure 3 it seems that MEFN produces slightly heavier tails. Perhaps the authors could report QQ plots for bot distributions and the observed quantiles in the test data.The authors propose a new approach for estimating maximum entropy distributions\nsubject to expectation constraints. Their approach is based on using\nnormalizing flow networks to non-linearly transform samples from a tractable\ndensity function using invertible transformations. This allows access to the\ndensity of the resulting distribution. The parameters of the normalizing flow\nnetwork are learned by maximizing a stochastic estimate of the entropy\nobtained by sampling and evaluating the log-density on the obtained samples.\nThis stochastic optimization problem includes constraints on expectations with\nrespect to samples from the normalizing flow network. These constraints are\napproximated in practice by sampling and are therefore stochastic. The\noptimization problem is solved by using the augmented Lagrangian method. The\nproposed method is validated on a toy problem with a Dirichlet distribution and\non a financial problem involving the estimation of price changes from option\nprice data.\n\nQuality:\n\nThe paper seems to be technically sound. My only concern would the the approach\nfollowed to apply the augmented Lagrangian method when the objective and the\nconstraints are stochastic. The authors propose their own solution to this\nproblem, based on a hypothesis test, but I think it is likely that this has\nalready been addressed before in the literature. It would be good if the\nauthors could comment on this.\n\nThe experiments performed show that the proposed approach can outperform Gibbs\nsampling from the exact optimal distribution or at least be equivalent, with\nthe advantage of having a closed form solution for the density.\n\nI am concern about the difficulty of he problems considered.\nThe Dirichlet distributions are relatively smooth and the distribution in the\nfinancial problem is one-dimensional (in this case you can use numerical\nmethods to compute the normalization constant and plot the exact density).\nThey seem to be very easy and do not show how the method would perform in more\nchallenging settings: high-dimensions, more complicated non-linear constraints,\netc...\n\nClarity:\n\nThe paper is clearly written and easy to follow.\n\nOriginality:\n\nThe proposed method is not very original since it is based on applying an\nexisting technique (normalizing flow networks) to a specific problem: that of\nfinding a maximum entropy distribution. The methodological contributions are\nalmost non-existing. One could only mention the combination of the normalizing\nflow networks with the augmented Lagrangian method. \n\nSignificance:\n\nThe results seem to be significant in the sense that the authors are able to\nfind densities of maximum entropy distributions, something which did not seem\nto be possible before. However, it is not clearly how useful this can be in\npractice. The problem that they address with real-world data (financial data)\ncould have been solved as well by using 1-dimensional quadrature. The authors\nshould consider more challenging problems which have a clear practical\ninterest.\n\nMinor comments:\n\nMore details should be given about how the plot in the bottom right of Figure 2 has been obtained.\n\n\"a Dirichlet whose KL to the true p\u2217 is small\": what do you mean by this? Can you give more details on how you choose that Dirichlet?\n\nI changed updated my review score after having a look at the last version of the paper submitted by the authors, which includes new experiments.", "title": "Performance of MEFN in the modeling of the tails", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}