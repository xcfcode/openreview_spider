{"paper": {"title": "Attention-based Graph Neural Network for Semi-supervised Learning", "authors": ["Kiran K. Thekumparampil", "Sewoong Oh", "Chong Wang", "Li-Jia Li"], "authorids": ["kirankoshy@gmail.com", "sewoong79@gmail.com", "chongw@google.com", "lijiali@cs.stanford.edu"], "summary": "We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets", "abstract": "Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.", "keywords": ["Graph Neural Network", "Attention", "Semi-supervised Learning"]}, "meta": {"decision": "Reject", "comment": "A version of GCNs of Kipf and Welling is introduced with (1) no non-linearity; (2) a basic form of (softmax) attention over neighbors where the attention scores are computed as the cosine of endpoints' representations (scaled with a single learned scalar). There is a moderate improvement on Citeseer, Cora, Pubmed.\n\nSince the use of gates with GCNs / Graph neural networks is becoming increasingly common (starting perhaps with GGSNNs of Li et al, ICLR 2016)) and using attention in graph neural networks is also not new  (see reviews and comments for references), the novelty is very limited.  In order to make the submission more convincing the authors could: (1) present results on harder datasets; (2)  carefully evaluate against other forms of attention (i.e. previous work).\n\nAs it stands, though it is interesting to see that such simple model performs well on the three datasets, I do not see it as an ICLR paper.\n\nPros:\n-- a simple model, achieves results close / on par with state of the art\n\nCons:\n-- limited originality\n-- either results on harder datasets or / and evaluation agains other forms of attention (i.e. previous work) are needed\n\n\n"}, "review": {"S1Z9bmyZf": {"type": "review", "replyto": "rJg4YGWRb", "review": "SUMMARY.\n\nThe paper presents an extension of graph convolutional networks.\nGraph convolutional networks are able to model nodes in a graph taking into consideration the structure of the graph.\nThe authors propose two extensions of GCNs, they first remove intermediate non-linearities from the GCN computation, and then they add an attention mechanism in the aggregation layer, in order to weight the contribution of neighboring nodes in the creation of the new node representation.\nInterestingly, the proposed linear model obtains results that are on-par with the state-of-the-art model, and the linear model with attention outperforms the state-of-the-art models on several standard benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe paper is, for the most part, clear, although some improvement on the presentation would be good (see below).\nAn important issue the authors should address is the notation consistency, the indexes i and j are used for defining nodes and labels, please use another index for labels.\nIt is very interesting that stripping standard GCN out of nonlinearities gives pretty much the same results, I would appreciate if the authors could give some insights of why this is the case.\nIt seems to me that an important experiment is missing here, have the authors tried to apply the attention model with the standard GCN?\nI like the idea of using a very minimal attention mechanism. The similarity function used for the attention (cosine) is symmetric, this means that if two nodes are connected in both directions, they will be equally important for each other. But intuitively this is not true in general. It would be interesting if the authors could elaborate a bit more on the choice of the similarity function.\n\n\n----------\n\nDETAILED COMMENTS\nPage 2. I do not understand the point of so many details on Graph Laplacian Regularization.\nPage 2. The use of the term 'skip-grams' is somewhat odd, it is not clear what the authors mean with that.\nPage 3. 'the natural random walk' ???\nBottom of page 4. When the authors introduce the attention based network also introduce the input/embedding layer, I believe there is a better place to do so instead of that together with the most important contribution of the paper.\n", "title": "few questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJmKbdIgM": {"type": "review", "replyto": "rJg4YGWRb", "review": "The paper proposes graph-based neural network in which weights from neighboring nodes are adaptively determined. The paper shows importance of propagation layer while showing the non-linear layer does not have significant effect. Further the proposed method also provides class relation based on the edge-wise relevance.\n\nThe paper is easy to follow and the idea would be reasonable. \n\nImportance of the propagation layer than the non-linear layer is interesting, and I think it is worth showing.\n\nVariance of results of AGNN is comparable or even smaller than GLN. This is a bit surprising because AGNN would be more complicated computation than GLN. Is there any good explanation of this low variance of AGNN?\n\nInterpretation of Figure 2 is not clear. All colored nodes except for the thick circle are labeled node? I couldn't judge those predictions are appropriate or not.", "title": "idea would be reasonable and constains interesting insight", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "HJvS2zhgz": {"type": "review", "replyto": "rJg4YGWRb", "review": "The paper proposes a semi supervised learning algorithm for graph node classification. The Algorithm is inspired from Graph Neural Networks and more precisely graph convolutional NNs recently proposed by ref (Kipf et al  2016)) in the paper.  These NNs alternate 2 types of layers:  non linear projection and diffusion, the latter incorporates the graph relational information by constraining neighbor nodes to have close representations according to some \u201cgraph metrics\u201d. The authors propose a model with simplified projection layers and more sophisticated diffusion ones, incorporating a simple attention mechanism. Experiments are performed on citation textual datasets. Comparisons with published results on the same datasets are presented.\n\nThe paper is clear and develops interesting ideas relevant to semi-supervised graph node classification. One finding is that simple models perform as well as more complex ones in this setting where labeled data is scarce. Another one is the importance of integrating relational information for classifying nodes when it is available. The attention mechanism itself is extremely simple, and learns one parameter per diffusion layers. One parameter weights correlations between node embeddings in a diffusion layer. I understand that you tried more complex attention mechanisms, but the one finally selected is barely an attention mechanism and rather a simple \u201cimportance\u201d weight. This is not a criticism, but this makes the title somewhat misleading. The experiments show that the proposed model is state of the art for graph node classification. The performance is on par with some other recent models according to table 2. The other tests are also interesting, but the comparison could have been extended to other models e.g. GCN.\nYou advocate the role of the diffusion layers, and in the experiments you stack 3 to 4 such layers. It would be interesting to have indications on the compromise performance/ number of diffusion layers and on the evolution of these performances when adding such layers.\nThe bibliography on semi-supervised learning in graphs for classification is light and should be enhanced.\nOverall this is an interesting paper with nice findings. The originality is however relatively limited in a field where many recent papers have been proposed, and the experiments need to be completed.\n", "title": "Interesting paper with nice findings. The originality is however relatively limited in a field where many recent papers have been proposed, and the experiments need to be completed.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1rCoFiQz": {"type": "rebuttal", "replyto": "rJg4YGWRb", "comment": "We thank the reviewers and the other commenters for helping us improve our work and its presentation. Taking the reviews and comments to heart we have made several changes which, we believe, greatly improve our paper. We added comparison of performance of AGNN with different number of propagation layers in Appendix C. In the Appendix D, we added experimental results of GCN on both random splits and cross-validation settings. Further, we have expanded the bibliography in the Sections 2 and 4.1. As per the reviews we have also made changes to notations and presentation style in Sections 3 and 4. In Section 5.2 and Appendix A, we corrected the order of class names. We improved the caption and marked training set nodes in Figures 2, 4, 5 and 6. Finally, we made some minor changes in the text.", "title": "Changes made to the paper"}, "Hk129YimG": {"type": "rebuttal", "replyto": "S1Z9bmyZf", "comment": "We are thankful for your review and insightful comments.\n\n1. Confusing notation is corrected: In the revised version $c$ indexes a label.\n\n2. Why GLN works:  For semi-supervised learning, we believe that the primary gain of using graph neural network comes from the \u201cAveraging\u201d effect. Similar to denoising pixels in images, by averaging neighbors features, we get a denoised version of current nodes\u2019 features. This gives significant gain over those estimations without denoising (such as Mulit-Layer Perceptron in Table 2). This, we believe, is why GLN is already achieving the state-of-the-art performance. The focus of this paper is how to get the next remaining gain, which we achieve by proposing asymmetric averaging using \u201cattention\u201d. So far, we did not see any noticeable gain in non-linear activation for semi-supervised learning. However, we believe such non-linearity can be important for other applications, such as graph classification tasks on molecular networks. \n\n3. Attention in GCN: GCN with attention did not give gain over our AGNN architecture, which is somewhat expected as GCN and GLN have comparable performances, within the error margin of each other. Note that from the architecture complexity perspective AGNN is simpler than GCN with attention, meaning that AGNN might have a better chance explaining the data.\n\n4. Symmetric attention: Even though the scaled cosine similarity would be symmetric between two connected nodes $i$ and $j$, the attention value itself can be different due to the fact that softmax computations are calculated on different neighborhoods: $N(i)$ and $N(j)$ respectively.\nBut we agree that attention mechanism has an element of symmetry and this might be alleviated by using more complex attention mechanism. As the reviewer pointed out, we chose the simple attention mechanism here; we tried various attention mechanisms with varying degrees of complexity, and found the simple attention mechanism to give the best performance. Training complex attention is challenging, and we would like to explore more complex ones in our future work.\n\nResponse to detailed comments:\n\n1. Details on Graph Laplacian Regularization: We added details about Laplacian regularization for completeness of discussion of previous work and because Laplacian regularizations closely related to the propagations layers used in almost all Graph Neural Network papers.\n2. \u2018Skip-grams\u2019: We added some clarification on the use of \u2018skip-grams\u2019 in the revised version.\n3. \u2018Natural random walk\u2019 on a graph is random walk where one move from a node to one of its neighbors selected with uniform probability. We have clarified this in the revised version.\n4. Presentation of the Attention-based Graph Neural Network: Thanks for pointing this out. We have made some changes to the presentation style.\n", "title": "Response to AnonReviewer4"}, "H1w_FFiQz": {"type": "rebuttal", "replyto": "HJvS2zhgz", "comment": "Thank you for reviewing our paper and pointing out missed experiments and inconsistencies.\n\n1. Attention mechanism:  It is true as the reviewer pointed out that our attention mechanism is very simple. We settled on this choice after training/testing several attention mechanisms, most of which are more complex than the one we propose. The proposed simple attention mechanism gave the best performance, among those we tried. We believe this is due to the fact that complex attention mechanisms are harder to train as there are more parameters to learn.\n\n2. GCN on other training sets: The reason we do not report GCN performance in tables 2 and 3 is that we made it our rule not to run other researcher\u2019s algorithms ourselves, at the fear of not doing justice in the hyperparameters we need to choose.   However, given the interest in the numerical comparisons, as the reviewer pointed out, in the revised version, we run these experiments  and reported the performance of GCN in the appendix D (as it might give the wrong impression that those results are performed by the authors of GCN, if we put it in the table in the main text).\n\n3. Choice of number of diffusion layers: Thanks for pointing this out. We have added a table in the appendix C which contains testing accuracies of AGNN model with different number of diffusion layers.\n\n4. Regarding bibliography: We have expanded the bibliography on semi-supervised learning using graphs. Please see the section 2 in the revised manuscript.\n", "title": "Response to AnonReviewer3: Compared AGNN with different number of layers and added experiments with GCN"}, "HyJAutiQM": {"type": "rebuttal", "replyto": "rJmKbdIgM", "comment": "Thank you for your time, review and valuable comments.\n\n1. Regarding the similar variance of results of AGNN and GLN: In Table 2 of the original version we don\u2019t report the variance or standard-deviation of accuracies of the trials, but we report (as mentioned in paragraph 1 on page 3 of original version) standard-error which defined as standard-deviation/square-root(number of trials) (https://en.wikipedia.org/wiki/Standard_error).  That being said, when the training data is fixed (as is the case for Table 2), the variance of GLN is smaller than that of AGNN as predicted by the reviewer, as the only source of randomness is the initialization of the neural network weights. On the other hand, when the training data is chosen randomly (As is the case for Tables 3 and 4), there are two sources of randomness and the variance of GLN and AGNN are harder to predict and compare. We could not predict how different choices of the training data affects the accuracy, and it can happen that GLN has larger variance than AGNN.\n\n2. Regarding Figure 2.: We apologize for the lack of clarity in its caption. The thick nodes are from the test set whose labels are not known to the model at training time. For clarification, we have now added `*\u2019 (asterisk) to mark nodes from the training set whose labels were revealed to the model during training (e.g. Figure 4). Coincidentally none of the neighborhood in Figure 2 have any nodes from the training set.\n", "title": "Response to AnonReviewer2: Improved Figure 2 for qualitative analysis of Attention"}, "rk5cCabxf": {"type": "rebuttal", "replyto": "H1buZ4xlz", "comment": "We agree that graph classification is another exciting application where graph neural networks are making breakthroughs. There are several key differences in the dataset (from citation networks) and we have not tried the idea of linear architecture for the molecular dataset yet. For example, the edges have attributes. There are straight forward ways to incorporate such information into GNNs, but we have not pursued this direction yet. I do agree the experiments you suggested will both (a) clarify what the gain is in non-linear activation; and (b) give insights on how different datasets (and applications) might require different architectures. \n\nFor the linear model, we did not tune the hyper parameters and the same hyper parameters are used as your (Kipf and Welling) original GCN. We made a small change in the stopping criteria  to take the best model in validation error out of all epochs. We did not see any significant change when we use the same stopping criteria as GCN. We will make this explicit during the revision process. Overall, there was no hyperparameter tuning for the linear model, and all the numbers should provide fair comparisons.\n\nThank you for the references, we will surely include and discuss all the great prior work you pointed out. \n\n", "title": "Thank you for the insightful comment."}, "rkWt6QPyG": {"type": "rebuttal", "replyto": "rk8rhql1G", "comment": "Thank you for your interest in our paper and bringing the VAIN model to our attention.\n\nWe see that VAIN uses attention between multiple-agents in a system. We were not aware of this line of literature when we submitted the paper. We will cite this line of work in our final version. Below is a comparison between VAIN and Attention-based-Graph Neural Network.\n\nMain similarities and differences between VAIN and Attention-based Graph Neural Network (AGNN) are as follows:\n1. Experimental results: AGNN is tested on semi-supervised classification of nodes on a graph where as VAIN is tested on prediction of future state in multi-agent systems.\n\n2. Side information: AGNN is graph processing neural network, but VAIN model does not take graph as an input as initially proposed (although it could).  In VAIN, it is assumed that every agent can possibly interact with every other agent in the system. Where as in AGNN we have a known graph in which real world first order interaction between two nodes are represented as edges and attention is computed only for these first order interactions. VAIN clubs all the higher order (long range) interactions into a single attention mechanism, where as AGNN computes higher order interactions through multiple hops of first order attention mechanism.\n", "title": "Thank you for pointing out the missing reference."}}}