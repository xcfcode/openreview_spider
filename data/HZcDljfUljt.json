{"paper": {"title": "Filter pre-pruning for improved fine-tuning of quantized deep neural networks", "authors": ["Jun Nishikawa", "Ryoji Ikegaya"], "authorids": ["~Jun_Nishikawa2", "~Ryoji_Ikegaya1"], "summary": "We propose a new pruning method for quantization and a new quantization workflow for high performance with low-bits.", "abstract": "Deep Neural Networks(DNNs) have many parameters and activation data, and these both are expensive to implement. One method to reduce the size of the DNN is to quantize the pre-trained model by using a low-bit expression for weights and activations, using fine-tuning to recover the drop in accuracy. However, it is generally difficult to train neural networks which use low-bit expressions. One reason is that the weights in the middle layer of the DNN have a wide dynamic range and so when quantizing the wide dynamic range into a few bits, the step size becomes large, which leads to a large quantization error and finally a large degradation in accuracy. To solve this problem, this paper makes the following three contributions without using any additional learning parameters and hyper-parameters. First, we analyze how batch normalization, which causes the aforementioned problem, disturbs the fine-tuning of the quantized DNN. Second, based on these results, we propose a new pruning method called Pruning for Quantization (PfQ) which removes the filters that disturb the fine-tuning of the DNN while not affecting the inferred result as far as possible. Third, we propose a workflow of fine-tuning for quantized DNNs using the proposed pruning method(PfQ). Experiments using well-known models and datasets confirmed that the proposed method achieves higher performance with a similar model size than conventional quantization methods including fine-tuning.", "keywords": ["Deep Neural Networks", "Quantization", "Quantize", "Pruning", "MobileNet", "compression"]}, "meta": {"decision": "Reject", "comment": "Four reviewers rate this article borderline. R3 finds the paper clearly presented and the method effective, but misses quantitative analysis of the dynamic range problem as well as novelty. Following the discussion and revision, she/he considers the paper improved and updated the score to 5, still being concerned about the novelty. R1 considers the paper makes an important observation but has concerns about experiments, rating it 6. R2 considers that the paper contributes a clear idea, but indicates that more analysis and supporting results are needed. She/he indicated a number of shortcomings in the initial review, and found the update good, hence tending to rate the paper higher after the responses, 6. R4 considers the paper well motivated and the method valid. However, he/she found the writing poor and over-claiming results, and that more rigorous mathematical notation would help. After the discussion and revision, he/she found the paper better and increased the score to 5, but still found issues preventing the paper from being accepted. In summary, the reviewers agree that the paper contains an interesting and well motivated method, but they also point at a number of shortcomings. The revision improved several of them but others persisted. Although the ratings improved after the discussion, the overall rating is borderline. This is a very competitive call, and hence I have to recommend reject at this time. "}, "review": {"2q3M5RTvEY4": {"type": "review", "replyto": "HZcDljfUljt", "review": "### Overall\nThis work present a Pruning mechanism for Quantization scenario. Duo to the low-bits effects, the quantized network is hard to train properly. Therefore, authors provide a new method call Pruning for Quantization (PfQ) and a workflow to solve the model compression problem practically. Comparing to some current quantization methods, PfQ obtains some gain from solution to the performance on benchmark datasets such as ImageNet and CIFAR100.\n\n### Pros\nClear idea about PfQ and the mathematical results provide the reason why the authors want to do so.\n\n### Cons\n1) Generally, the joint quantization (Q) and pruning (P) framework is not novel at present. For example, the following papers have tried to solve the model compression problem with P and Q jointly:\nTung, Frederick, and Greg Mori. \"Clip-q: Deep network compression learning by in-parallel pruning-quantization.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\nTung, Frederick, and Greg Mori. \"Deep neural network compression by in-parallel pruning-quantization.\" IEEE transactions on pattern analysis and machine intelligence (2018).\nWang, Ying, Yadong Lu, and Tijmen Blankevoort. \"Differentiable Joint Pruning and Quantization for Hardware Efficiency.\" European Conference on Computer Vision. Springer, Cham, 2020.\nAnd compression with the BN mechanisms are also available:\nGao, Xitong, et al. \"Dynamic channel pruning: Feature boosting and suppression.\" arXiv preprint arXiv:1810.05331 (2018).\nLiu, Yuan, et al. \"Local Normalization Based BN Layer Pruning.\" International Conference on Artificial Neural Networks. Springer, Cham, 2019.\nKang, Minsoo, and Bohyung Han. \"Operation-Aware Soft Channel Pruning using Differentiable Masks.\" arXiv preprint arXiv:2007.03938 (2020).\nTo support the claim that PfQ has the SOTA performance on benchmark datasets, first, PfQ should outperforms other BN based pruning techniques under the quantization settings. Then, PfQ should be better than those Pruning and Quatization Joint optimal methods for model compression. Especially, the readers would like to know why the variance based pruning approach is better than those BN weights based methods. It is not convincible enough for readers to follow the idea without analysis into details.\n2) It is lack of explanation why the workflow is needed for the model compression with PfQ. The reason why multiple PfQs are executed is missing. A more clear algorithm would be better for the presentation of the \"workflow\".\n3) Ablation studies: since the author design multiple round of PfQs, the paper is expected to show the studies on what will happen if the workflow just contains PfQ and reasonable finetune. Why the performance could not beat the current \"workflow\"? How would the variance distribution be after the \"workflow\"?\n4) More proof-reading will make the paper look better. For example,  in page 7, section 4.3.1, \"a single GPU (2080ti)\" => \"a single GPU (2080Ti)\". In the same sentence, \"for the learning in cifar100\" => \"for the learning in CIFAR-100\". \n", "title": "Clear idea. More analysis and results are needed to support the idea", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Ks2tW-5vOZ9": {"type": "review", "replyto": "HZcDljfUljt", "review": "This paper studies the effect of quantization during training together with batch normalization in quantized deep neural networks. The compound effect of convolution and batch normalization on the dynamic range of activations has implications on the progress of training. The authors propose a protocol for training a quantized neural network combining filter pruning, fine tuning and bias correction. The experiments show that the model size is reduced significantly, while keeping or improving the accuracy.\n\nStrengths\n- The paper is clearly presented, and the effect of batch normalization in the dynamic range of a quantize layer is interesting.\n- The method, in the paper settings, is effective in reducing the model size and sometimes improving accuracy.\n\nWeaknesses\n- There is not qualitative analysis of the problem of the dynamic range and how the proposed method alleviates it. \n- The novelty is limited in my opinion. The paper combines typical practices used in other works. For example, pruning filters with small magnitude is a common practice. it is also common to fine tune a network after pruning or compression, which not only reduces model size but often also improves the accuracy due to lower overfitting. And bias correction to account for the reconstruction error is also commonly used in similar cases (e.g. DFQ, Finkelstein2019, Masana2017). \n- The number of iterations in the experiments is a fixed number. It would be more convincing using a validation set with early stopping.\n- The pruning+fine tuning seems to be done once (twice in the proposed workflow). In that case the comparison may not be fair without several iterations of pruning+fine tuning, to compare models when they cannot be further pruned. \n\nOverall, I think the novelty of the paper is limited, with concerns about the experiments.\n\nQuestions\nPlease address weaknesses\n\nFinkelstein et al., Fighting Quantization Bias With Bias, ECV@CVPR 019 \nMasana et al., Domain-adaptive deep network compression, ICCV2017\n\n-- Post rebuttal\n\nI appreciate the response by the authors and the new experiments. I also read the other reviews and responses. I think the paper has improved in the revised version. However, I'm still concerned about the novelty, which still remains relatively incremental, as also pointed by other reviewers. I update my rating to 5.", "title": "Effective idea with limited novelty", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "QKNhB_nddwr": {"type": "review", "replyto": "HZcDljfUljt", "review": "This paper proposes a pruning for quantization (PfQ) scheme to improve the fine-tuning of the quantized network. PfQ removes the filters that disturb the fine-tuning of the DNN while retaining good performance. Some experiments are conducted to demonstrate the effectiveness of the proposed PfQ.\n\nPros:\n- The paper is well motivated and the method seems valid.\n\nCons:\n- The writing quality is poor. in its current form, a general readership will struggle to understand it. An expert familiar with the details of the field (not just the general area) can probably disentangle the text, but otherwise, it's too convoluted.\n- This paper is over-claimed. For example, how BatchNorm disturbs the fine-tuning of the quantized network has been pointed out by [1]. The analysis presented by the authors is too incremental.\n\nSome remarks:\n- Please note that BN can not be absorbed into the previous convolution layer in the training stage. The mean and variance in Eq.(2) should be running-moving average statistics, which should be pointed out in the beginning.\n- To improve the quality of this paper, more rigorous mathematical notations and some visual experiments should be provided.\n- It would be better to show some ablation studies to show how your method works.\n\n[1] Data-free quantization through weight equalization and bias correction. Markus Nagel et al.\n\n**********After rebuttal\n\nThe revised version has a better shape. In particular, I like the analytical experiment (Fig.2), which demonstrates that the proposed scheme can improve the wide dynamic range. Overall, this paper observes that BN with small variance influence quantization and proposes a protocol for training a quantized neural network combining filter pruning. \n\nSome issues still prevent it from being accepted. For example, PfQ is proposed to reduce the dynamic range. However, there is even no definition of the dynamic range in the paper which may make readers hard to understand the mechanism of PfQ.  Besides, the author claim that the weight widening the dynamic range in quantization is theoretically analyzed. But the analysis of Eqn. (14-16) is less rigorous. The readers may expect to see how weights in the case of $V_{c}^{L,\\tau} \\approx 0$ increase the dynamic range according to its definition compared to those weights in the other cases ($V_{c}^{L,\\tau} \\gg 0$).\n\nThe paper proposes an effective approach of quantization, which reduces the model size and improves accuracy. I would like to increase my rating to 5.", "title": "Review of Paper 609", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "mEAfbVuxm1C": {"type": "rebuttal", "replyto": "uFbhq-nk9V7", "comment": "Answer for Some remarks 1:\n - We revised some sentence about that to clear below the equation (2) in subsection 2.2.\n\nAnswer for Some remarks 2:\n - We understood as that the one is about the too many indexes in subsection 2.2 and 3.1 and another is that we should visually describe the experiments.\n   Therefore, we revised some equations in subsection 2.2 and 3.1 to reduce indexes and clear and visually show the results of solving the problem of dynamic range by PfQ in subsection 4.2.2 and some channels output of BN to describe the difference between the running variances in appendix A.2.\n\nAnswer for Some remarks 3:\n - We added the results of ablation study of our quantization workflow in subsection 4.2.3.", "title": "paper updated and we made some experiments"}, "3Ut_aYznT8l": {"type": "rebuttal", "replyto": "m9Sl3h07sFk", "comment": "Additional comment for weakness 2:\n - \"The reason why multiple PfQs are executed is missing.\"\n  We revised subsection 3.3 to give more explanation of our quantization workflow.\n - \"A more clear algorithm would be better for the presentation of the \"workflow\".\"\n  We revised our paper and the more clear algorithm was described in subsection 3.3.\n  \nAdditional comment for weakness 3:\n - We made the ablation study regarding our quantization workflow and the results was described in subsection 4.2.3.", "title": "paper updated and we made some experiments"}, "9FLSS66FBv7": {"type": "rebuttal", "replyto": "_WswB4aE17N", "comment": "We update our paper.\nThe changes are commented above.\nCould you check our paper.", "title": "paper updated"}, "VPsI1EE5QG6": {"type": "rebuttal", "replyto": "q37Kk4RpcGL", "comment": "We made the new experiments and the results were described in subsection 4.2.2 for weakness 1 and appendix A.1 for weakness 3.\nCould you confirm the results.", "title": "paper updated"}, "rh8jxSAvhjf": {"type": "rebuttal", "replyto": "HZcDljfUljt", "comment": "\nThank you for your comments and reading our paper.\nWe update the paper.\nThis update contains the following changes.\n - Revising the structure of subsection 4.2.\n   The title of 4.2 is changed to \"Ablation Study\" and subsection 4.2.1 is \"Effect of Disturbing Weights\" (i.e. old 4.2), 4.2.2 is a new subsection \"Solving Dynamic Range Problem\" and 4.2.3 is a new subsection \"Effect of Proposed Quantization Workflow\".\n - Addition of Appendix about the two additional experiments.\n   The first result is regarding early stopping using validation set.\n   The second result is visualizing the difference between the channels whose running variance is close to zero and the channels whose running variance is not close to zero.\n - Revising the description of our quantization workflow in subsection 3.3.\n - Simplifying some mathematical notations in subsection 2.2, 3.1, 3.2.\n - Addition of the additional results in Table 3 (old Table 2) about MobileNetV1.\n - Moving figure 1 above section 2.\n - Addition of hyperlinks in each references.", "title": "Paper updated"}, "uFbhq-nk9V7": {"type": "rebuttal", "replyto": "QKNhB_nddwr", "comment": "Thanks for your kind comments. Please find our reply below.\n\nAnswer for Cons 1:\n - We will scrutinize and revice this paper so that readers who are not familiar with this field will understand it.\n   If possible, could you tell me where you felt that?\n\nAnswer for Cons 2:\n - The claim in [1] is about the dynamic range of the weights.\n   In this paper, we claim that BatchNorm has yet another problem described in subsection 3.1 after we point out that the problem of the dynamic range is claimed by [1].\n\nAnswer for Some remarks 1:\n - Thank you for your clear advice.\n   It is described in the sentence under (4) of Subsection 2.2.\n\nAnswer for Some remarks 2:\n - We think this is probably related to Cons 1.\n   We scrutinize and revise this paper.\n   If possible, could you tell me where you felt that?\n\nAnswer for Some remarks 3:\n - As pointed out by other reviewers, we will make the experiments about the ablation studies for workflow.\n   We will also consider if there are any other ablation studies to confirm.\n   What kind of ablation study did you feel you should do?", "title": "Answer to reviewer4"}, "m9Sl3h07sFk": {"type": "rebuttal", "replyto": "2q3M5RTvEY4", "comment": "Thanks for your kind comments. Please find our reply below.\n\nAnswer for weakness 1:\n - we appreciate that you presented several important papers.\n   However, the presented mathods are inappropriate as comparative methods to ours.\n   PfQ is the simplest pre-processing method to solve the problem of quantization that is discussed in this paper (i.e., certain weights disturb the fine-tuning of the quantized DNN).\n   For this reason, quantization methods that do not consider joint quantization and pruning are chosen as our comparative methods.\n   Specifically, we do not discuss the effectiveness of PfQ as pruning.\n   The main contribution of this paper is showing the effectiveness of removing certain weights that disturb the fine-tuning.\n   Therefore, comparing with the methods that consider effective joint quantization and pruning is unfair.\n\n\nAnswer for weakness 2:\n - \"It is lack of explanation why the workflow is needed for the model compression with PfQ.\"\n  PfQ is independent of the workflow.\n    The workflow is proposed to improve the performance after the fine-tuning of the quantized DNN.\n    In general, DNN quantization is performed for the activations and weights simultaneously for float pre-trained model.\n    However, the fine-tuning may not proceed well by this method since the variation from the pre-trained model is too large.\n    To deal with this problem, the proposed workflow is introduced to fine-tune progressively that helps the fine-tuning process.\n    Also, our quantization workflow can get the effect of BN.\n    In general, for DNN quantization, BN layers are folded into the convolution before quantization to reduce the weights that have to be quantized.\n    However, our proposed workflow allow us to use BN during the first fine-tuning since the weights are treated as float values in the first fine-tuning.\n    Therefore, the effect of BN (i.e., the statistical information of the layers can be used) is obtained during the first fine-tuning and improvements in the performance can be expected than the previous methods.\n - \"The reason why multiple PfQs are executed is missing.\"\n  During the first fine-tuning for quantized activations only in our quantization workflow, the model contains BN layers.\n    This may cause that the filters that disturb the fine-tuning for quantized DNN are newly produced after the first fine-tuning.\n    To solve this problem, the second PfQ is performed to remove these filters.\n - \"A more clear algorithm would be better for the presentation of the \"workflow\".\"\n  We appreciate your precise advice. \"Proposal of Quantization Workflow\" in Section 3.3 was modified to show a more clear algorithm.\n\nAnswer for weakness 3:\n - \"since the author design multiple round of PfQs, the paper is expected to show the studies on what will happen if the workflow just contains PfQ and reasonable finetune. \"\n  We appreciate your precise advice.\n    As commented, the ablation study is necessary to show the importance of our proposed workflow.\n    We will make the additional experiments, and write the results in the paper update.\n - \"Why the performance could not beat the current \"workflow\"?\"\n  We will make the experiments as the above ablation study.\n - \"How would the variance distribution be after the \"workflow\"?\"\n  The variance distribution after the \"workflow\" did not bave any noticeable characteristics as overall.\n    However, there was almost no variance close to zero.\n    For example, the filters close to zero existed only 0.3% in MobileNetV2 after the \"workflow\" for CIFAR-100.", "title": "Essence of this paper is the new problem of quantization (equation (16))"}, "_WswB4aE17N": {"type": "rebuttal", "replyto": "A9PzPA_joMn", "comment": "Thanks for your kind comments. Please find our reply below.\n\nAnswer for your comment 'Table 1 is to show the benefits of removing disturbing weights. Why use the activation quantization instead of weights quantization? It's better to analyze weight quantization here too.':\nThere is a reason why we did not experiment with weight quantization.\nA filter with a small variance of BN causes two problems: \nproblem (1) that increases the dynamic range of the weights and problem (2) that disturbs the fine-tuning of the quantized DNN.\nProblem (1) is claimed in [1] below, and problem (2) is our claim.\nWhen we fine-tune the DNN with quantized weights, problem (1) must be solved by some way. (Otherwise, the fine-tuning may not work.)\nTherefore, in the experiment in Table 1, we made the experiment with only activation quantization in order to focus only on problem (2).\n[1]: A Quantization-Friendly Separable Convolution for MobileNets (https://arxiv.org/abs/1803.08607)\n\nAnswer for question 1:\n- \"Will per-channel quantization have the problem discussed in this paper?\"\n  It is theoretically possible.\n  The above problem (2) is caused by the quantization error and gradient approximation error of the second and fourth terms of equation (16).\n  By per-channel quantization, if the errors are very small, the problem may not occur, but if the errors become large due to low-bit quantization, etc., the problem may occur.\n- \"What about the results of using / not using PfQ for per-channel quantization?\"\n  It works, but the benefits are minor compared to per-layers.\n- \"The min-max quantization scale (eq 20) is used in this paper, what if we use learnable quantization (e.g., LSQ [1])? Is it still suffer from the channels having small variance?\"\n  The performance may be improved.\n  However, it is likely that the performance will be lower than the case using PfQ, because the training will also have to work to the problem of small channel variance.\n\nAnswer for question 2:\n  The following [2] is cited. We forgot to add a link. I'm so sorry.\n  [2]: Data-Free Quantization Through Weight Equalization and Bias Correction (https://arxiv.org/abs/1906.04721)\n\nAnswer for question 3:\n  Our method currently applies only to DNNs with BN layer. How to apply it to a model without BN layer has not yet been considered.\n  The essence of the problem claimed in this paper is that some channels whose values do not change for any input data in the model disturb the quantization fine-tuning.\n  Therefore, we consider that the essential claim of this paper can be applied if such channels can be detected without BN.\n\nAnswer for other comments:\n  Thank you your kind comments. We will move the Figure 1 just before Section 2.", "title": "Answer to reviewer 1"}, "q37Kk4RpcGL": {"type": "rebuttal", "replyto": "Ks2tW-5vOZ9", "comment": "Thanks for your kind comments. Please find our reply below.\n\nAnswer for weakness 1: \nWe completed some experiments which show that PfQ suppresses the dynamic range of the weights.\nWe will add these results in the update of our paper.\n\nAnswer for weakness 2: \nThis paper claims a new problem in quantization (see Equation (16) and its explanation in section 3.1) and the simplest solution to solve it and improve the performance.\nSo it's not just a combination of the traditional methods.\n\nAnswer for weakness 3: \nThank you for your helpful suggestions. We will do some experiments to make our paper more convincing. We will include these additional results in the update of our paper.\n\nAnswer for weakness 4:\nThe fine-tuning in our paper is done for quantized activations and weights, not for pruning. Therefore, the number of pruning does not lose fairness.", "title": "Our main novelity is to find and solve the new problem of quantization (equation (16))"}, "A9PzPA_joMn": {"type": "review", "replyto": "HZcDljfUljt", "review": "This paper proposed to prune certain channels to improve the accuracy of quantized DNN model. The motivation comes from the observation that the channels which have small variance are actually harmful to the quantization-aware training. The authors show that these channels with small variance can be pruned without significant influence to the neural network, and the accuracy loss can be easily recovered with fine-tuning. \n\nPros:\n\nThis paper makes an important observation that channels with small variance lead to a problem that the filter weights will have a wide range after fusing the BN layer into convolution. Directly performing quantization on the weight tensor will lead to a large quantization scale and large quantization error.\n\nThe authors find that the channels with small variance can actually be removed. Based on their following BN operations, these channels can be seen as constant channels approximately, and these constants can be folded into the bias of the next layer. By this way, the neural network is not only more suitable to quantize, but also has less model size and computation.\n\nThe paper gives a study showing that removing the channels with small variance improves the accuracy of quantized model. The proposed method is also compared with several other quantization methods.\n\nCons:\n\nOn ImageNet, the experiments are only did on MobileNetV2, and only a few methods are compared. Although the current experiments show the benefits of PfQ, it is better to have more experiments to make the validation part more solid.\n\nTable 1 is to show the benefits of removing disturbing weights. Why use the activation quantization instead of weights quantization? It's better to analyze weight quantization here too.\n\nThere are some questions which are not clear in the paper:\n\n1. Will per-channel quantization have the problem discussed in this paper? What about the results of using / not using PfQ for per-channel quantization? The min-max quantization scale (eq 20) is used in this paper, what if we use learnable quantization (e.g., LSQ [1])? Is it still suffer from the channels having small variance?\n\n2. DFQ is used as the quantization method. Is this a quantization method cited in this paper? I didn't find the reference and the full name for DFQ.\n\n3. Is the proposed method only applied to DNNs which have BN layers? Is it possible to apply the proposed method to models without BN layers?\n\nOther comments:\nIt's easier for people to understand the key idea of this paper if moving Figure 1 to the first few pages.\n\nIn general I think this paper is a good work on quantization and model compression. It makes good observation of factors that influence quantization, and shows that pruning can actually help quantization in some cases. There are some issues on the experiment (See cons), it's better to validate the proposed method on more DNN architectures and compare with more quantization methods.\n\nReferences\n\n[1] Esser, S.K., McKinstry, J.L., Bablani, D., Appuswamy, R. and Modha, D.S., 2019. Learned step size quantization. arXiv preprint arXiv:1902.08153.", "title": "borderline, tend to accept", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}