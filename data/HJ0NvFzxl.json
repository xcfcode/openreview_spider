{"paper": {"title": "Learning Graphical State Transitions", "authors": ["Daniel D. Johnson"], "authorids": ["ddjohnson@hmc.edu"], "summary": "I introduce a set of differentiable graph transformations, and use them to build a model with a graphical internal state that can extract structured data from text and use it to answer queries.", "abstract": "Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that produces sequences from graph-structured input. In this work I introduce the Gated Graph Transformer Neural Network (GGT-NN), an extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and modify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a family of Turing machines.", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning", "Structured prediction"]}, "meta": {"decision": "Accept (Oral)", "comment": "The idea of building a graph-based differentiable memory is very good. The proposed approach is quite complex, but it is likely to lead to future developments and extensions. The paper has been much improved since the original submission. The results could be strengthened, with more comparisons to existing results on bAbI and baselines on the experiments here. Exploring how it performs with less supervision, and different types of supervision, from entirely labeled graphs versus just node labels, would be valuable."}, "review": {"SJxB-hVSg": {"type": "rebuttal", "replyto": "rkPx9lere", "comment": "Thank you for your comments.\n\n1) Experiments on training with suboptimal graphs have not been performed. Such experiments could involve either training on a smaller subset of accurate graphs or training using a set that includes some inaccurate graphs.\n\nIf accurate graphs were provided for a small subset of the training data only, and if no graphs were provided for the rest, this situation would essentially be a mix of strong and weak supervision, such as was suggested in the previous comment by Daniel Tarlow (see below). It seems possible that the network could learn to generalize the graph structure acquired from the graph-annotated data and apply it to the rest of the data without suffering in accuracy, which would be an interesting and exciting result.\n\nIf graphs were provided for all inputs, but some of the graphs were incorrect, this could be considered a form of \"class noise\", since the creation of the correct graph can be seen as a problem of classifying the potential graph nodes and edges into their correct types, and incorrect graphs can then be interpreted as improperly classified examples. It seems likely that the performance of the model would depend on the frequency of the errors, with the model displaying less test accuracy as the inaccuracy in the training data increased. Bekker and Goldberger studied the behavior of deep neural network models when subjected to class noise, and found that accuracy deteriorates rapidly with increasing class noise. They also propose a method for improving network performance, based on estimating the distribution of the incorrect classes [1]. It is possible that such a method could be adapted for use with the GGT-NN model. However, GGT-NNs may be more sensitive to incorrect examples than other supervised learning algorithms, since the provided graphs are used both as targets for the graph construction and as inputs for answering queries.\n\n2) In my preliminary experiments, the results were significantly different when intermediate graph supervision was not applied. In particular, the model tended to quickly overfit to the training data, and fail to solve the overall task. Additionally, the graph structure learned by the model was unusual in that there were very few clearly defined nodes or edges. Instead, each node and edge had fractional strength and did not have a specific well-defined type. This seems to indicate that the model was exploiting the extension of the discrete graph structure into a continuous matrix representation, and using it to memorize particular training examples.\n\nOne approach that might prevent this behavior and enable the model to learn a useful graph representation on its own is to use the \"concrete distribution\" proposed by Maddison et al. in their submission to ICLR 2017 [2]. Introducing this into the node-addition and edge-update transformations might force the model to create a discrete graph representation by introducing noise into the model. I plan to investigate this extension of the GGT-NN in the future. If successful, this could enable GGT-NN models to be trained without graph-level supervision.\n\n3) The amount of work required likely depends greatly on the particular tasks used. In this case, the bAbI tasks are generated by a script that actually uses a knowledge-graph object to produce the stories. By modifying the script to save this graph in addition to the stories, I was able to give the model additional supervision with minimal work.\n\n4) Thank you for your suggestion. The next version of the paper will include a mention of the use of strong supervision in the introduction of the paper.\n\n[1] Bekker, Alan Joseph, and Jacob Goldberger. \"Training deep neural-networks based on unreliable labels.\" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016.\n[2] Maddison, Chris J., Andriy Mnih, and Yee Whye Teh. \"The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables.\" arXiv preprint arXiv:1611.00712 (2016). https://openreview.net/forum?id=S1jE5L5gl\n", "title": "Re: Loss function requires strong supervision"}, "rkPx9lere": {"type": "rebuttal", "replyto": "HJ0NvFzxl", "comment": "Section 4.1 Supervision mentioned that the proposed method requires additional supervision by providing the correct graph at each timestep. \n\nI have some questions about whether this is practical: \n\n1. In some applications, human may or may not be able to provide the \"correct graph\" at each timestep. What if the human supervisor can only provide suboptimal graphs during training? How would that affect the results?\n\n2. What was the result of not providing such intermediate supervision? Was the result slightly worse than the current result, or significantly different?\n\n3. Such additional supervision would also require more \"work\" by the human. How can the amount of work be quantified, so the reader understands the implications of having to provide additional supervision?\n\n4. I would appreciate if the need of such strong supervision was mentioned earlier in the paper, perhaps during the introduction or literature review, to give the reader some \"warning\". Some literature review/comparison of other approaches that require strong supervision would also be appreciated.\n\nThanks for the good work, I enjoyed reading it!\n\n\n", "title": "Loss function requires strong supervision"}, "SJRBhuOEg": {"type": "rebuttal", "replyto": "HJ0NvFzxl", "comment": "The paper has been updated with the following changes:\n\n- Fixed typo in the equations given in B.2 and B.2.1.\n- Added a reference to work done by Giles et al. in Section 6.\n- Clarified the operation of direct reference in Sections 3 and 4.\n- Added a link to the source code for the model (in Appendix B).\n- Minor wording changes in the Abstract, Introduction, and Sections 3 and 4.", "title": "Revision uploaded"}, "B1of3__Ex": {"type": "rebuttal", "replyto": "SkibszLEx", "comment": "Thank you very much for your review and comments.\n\nFor the Rule 30 task, the model was trained on examples that only included seven steps of the automaton, and was then evaluated on three variants of the task: one with seven steps, one with 20 steps, and one with 30 steps. Although it is true that the model was unable to generalize perfectly to inputs longer than those seen in training data, the model did achieve 100% accuracy on inputs of the same length as it was trained on (7 steps). It is likely that the model could be trained to correctly produce longer sequences if it was trained with longer examples.", "title": "Re: review"}, "HyNl3ddEx": {"type": "rebuttal", "replyto": "HkMx83V4l", "comment": "Thank you very much for your review and comments.\n\nAlthough the GGT-NN model was evaluated on the bAbI tasks, which involve representing a dialog, it can also be used for non-dialog tasks. The cellular automaton and Turing machine tasks described in section 5.2 are two examples of tasks where the output is a graph instead of a textual answer. I also briefly describe alternate operation configurations in Section 4.2 and Appendix D.\n\nThe direct reference update step (T_h,direct) is based on the presence of input words that correspond directly to specific node types. If none of those words appear in a particular input sentence, then the direct reference update will contain no information. In all of the bAbI tasks, some entity is directly described in each sentence, so it is true that in this case the node state update step (T_h) may not be necessary. However, the node state update step is necessary whenever an input sentence does not reference any node type. As one example, this occurs with the \"run\" and \"simulate\" commands for the cellular automaton and Turing machine tasks, described in Appendix C.\n\nThe importance of direct reference on GGT-NN performance is certainly very interesting. For some tasks, a simpler method such as \"match type\" (as described in \"LEARNING END-TO-END GOAL-ORIENTED DIALOG\" by Bordes et al., submitted to ICLR'17) combined with memory networks may be sufficient. However, other tasks seem to benefit greatly from a graph representation. For instance, bAbI task 19 requires finding a path between a source and a destination, and was easily solved using the GGT-NN model. Although direct reference (and possibly match type) can identify the source and the destination, the intermediate nodes are not mentioned in the query, so it is the graph operations that enable the path between them to be found. The graph operations are also a natural fit for tasks that require graph-structured output, which seem difficult to formulate using simpler models.\n\nThe time and size issue described in the paper arises both during learning and in inference, and is caused primarily by the overhead of storing all possible edge connections. During inference the usage could be potentially reduced with sparse matrix operations, but this has not yet been explored.\n\nThe source code is available at https://github.com/hexahedria/gated-graph-transformer-network (and a link has been added in the most recent revision of the paper). In essence, all of the transformations are implemented as functions that take in Theano tensors and return new Theano tensors. These transformations are combined into a Theano computation graph that implements Algorithm 1 and also computes the loss between the final output and the target. The gradient is then back-propagated using the built-in Theano automatic differentiation functions.\n\nThank you for pointing out the typo in appendices B.2 and B.2.1; it has been fixed in the newest revision of the paper.\n\nThe work of Lee Giles is indeed relevant and interesting. I have added a reference to and brief discussion of \"Learning and extracting finite state automata with second-order recurrent neural networks\" by Giles et al. (1992) since this seemed most relevant to the GGT-NN model.", "title": "Re: review"}, "Sk18sOO4g": {"type": "rebuttal", "replyto": "Hk_mPh-4e", "comment": "Thank you very much for your review and comments.\n\nFuture work on training the GGT-NN model may enable it to solve complex problems where full graph information is not available. Potential avenues for exploration include providing graph states for a subset of the training data (suggested below in the comment by Daniel Tarlow) and introducing additional regularization to allow the network to be trained end-to-end without any graph information.", "title": "Re: review"}, "SkC4fyCQe": {"type": "rebuttal", "replyto": "HJ0NvFzxl", "comment": "Based on the reviewer comments, I have uploaded a new revision of the paper, with the following changes:\n\n- Added figures depicting the differentiable graph format and each of the graph transformations\n- Added a section comparing the GGT-NN model to existing works\n- Included baselines for the bAbI tasks, with discussion of the relative performance of the GGT-NN model\n- Clarified the feasibility of alternative network configurations (relative to Algorithm 1)\n- Moved implementation details of the graph transformations into the appendix\n- Clearly separated and simplified the background information section\n- Clarified the training procedure for the GGT-NN model\n\nThank you very much for your feedback.", "title": "Revision uploaded"}, "S1A4xRT7l": {"type": "rebuttal", "replyto": "ByDNuaaXx", "comment": "I have revised the paper since my original submission. The current revision includes a comparison of GGT-NNs to other network architectures, including memory networks, in section 6.\n\nI use Theano to compile the network, but I only have to compile the network once, so it is quite fast. The graph structure is encoded as a set of matrices (depicted in Figure 1) so the same Theano function can be used for multiple sample dialogs.", "title": "Construction of network"}, "HyqrCn27e": {"type": "rebuttal", "replyto": "H1tlYin7e", "comment": "Thank you for your comments.\n\nThe parameters of the model are not learned from only one dialog. Instead, they are learned based on a large corpus of data. U and W are not the trainable parameters of the graph; those are the trainable parameters associated with a single GRU cell, described by Cho et al. (2014). In Algorithm 1, each graph transformation (indicated with a capital T) has its own set of trainable parameters, based on its internal structure (described in detail in Appendix B). Additionally, the GRU layer used to produce i(k) also has trainable parameters.\n\nIt is true that each sample dialog corresponds to an independent graph structure, which starts as the empty graph for each dialog. However, the parameters used by each graph are shared between all dialogs, and determine how the graph is updated and how information flows between nodes. The graph structure does not represent the network itself, so there are no parameters actually attached to the graph. Instead, the parameters of the model are used to modify the graph. Thus when running on different dialogs, the same parameters are used, but end up producing a different graphical structure.\n\nIt can be useful to think of the graph not as an actual part of the model, but instead as a type of memory. The model itself uses its parameters to read and modify the data stored in the graph and change the graph's structure. This is similar to how a Differentiable Neural Computer, for instance, can use its read-write heads to read and modify the data stored in memory.\n\nI am not confident that I understand your question about learning \"to perform the inference\". In all cases, the parameters are learned to maximize the probability that the graph is built correctly and that the correct answer is chosen for the query. At inference time, the model is then used to build a graph and produce an answer to a query. However, even so, the parameters are trained across all dialogs.\n\nIf I have misunderstood your questions or if this answer does not fully address them, please let me know.", "title": "Re: Inference vs. learning"}, "H1tlYin7e": {"type": "review", "replyto": "HJ0NvFzxl", "review": "Dear Author,\n\nAlgorithm 1 applies to a single dialog with K sentences: the graph represents a memory of the dialog that is later used to answer a query.  As the graph starts from 0 at step 1, shall I assume that all the trainable parameters of the graph (U and W)  are only learnt from one sample dialog, basically only learnt to perform the inference? If this is true, what are the trainable parameters that encode knowledge from other dialogs? The parameters used to extract representation i(k)? If I am wrong, how are the U and W parameters passed from previous sample dialogs?This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.\n\nThe preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see http://www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf). This is still clearly promising.\n\n The sequence of transformation in algorithm 1 looks sensible, though the authors do not discuss any other operation ordering. In particular, it is not clear to me that you need the node state update step T_h if you have the direct reference update step T_h,direct. \n\nIt is striking that the only trick that is essential for proper performance is the \u2018direct reference\u2019 , which actually has nothing to do with the graph building process, but is rather an attention mechanism for the graph input: attention is focused on words that are relevant to the node type rather than the whole sentence. So the question \u201chow useful are all these graph operations\u201d remain. A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR'17 (see match type in \"LEARNING END-TO-END GOAL-ORIENTED DIALOG\" by Bordes et al)\n\n\nThe authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both? A description of the actual implementation would help  (no pointer to open source code is provide). The author mentions Theano in one of my questions: how are the transformations compiled in advance as units? How is the gradient back-propagated through the graph is this one is only described at runtime?\n\n\nTypo: in the appendices B.2 and B.2.1, the right side of the equation that applies the update gate has h\u2019_nu while it should be h_nu.\n\nIn the references, the author could mention the pioneering work  of Lee Giles on representing graphs with  RNNs.\n\nRevision: I have improved my rating for the following reasons:\n- Pointers to an highly readable and well structured Theano source is provided.\n- The delta improvement of the paper has been impressive over the review process, and I am confident this will be an impactful paper.\n- Much simpler alternatives approaches such as Memory Networks seem to be plateauing for problems such as dialog modeling, we need alternatives.\n- The architecture is this work is still too complex, but this is often as we start with DNNs, and then find simplifications that actually improve performance\n", "title": "Inference vs. learning", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkMx83V4l": {"type": "review", "replyto": "HJ0NvFzxl", "review": "Dear Author,\n\nAlgorithm 1 applies to a single dialog with K sentences: the graph represents a memory of the dialog that is later used to answer a query.  As the graph starts from 0 at step 1, shall I assume that all the trainable parameters of the graph (U and W)  are only learnt from one sample dialog, basically only learnt to perform the inference? If this is true, what are the trainable parameters that encode knowledge from other dialogs? The parameters used to extract representation i(k)? If I am wrong, how are the U and W parameters passed from previous sample dialogs?This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.\n\nThe preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see http://www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf). This is still clearly promising.\n\n The sequence of transformation in algorithm 1 looks sensible, though the authors do not discuss any other operation ordering. In particular, it is not clear to me that you need the node state update step T_h if you have the direct reference update step T_h,direct. \n\nIt is striking that the only trick that is essential for proper performance is the \u2018direct reference\u2019 , which actually has nothing to do with the graph building process, but is rather an attention mechanism for the graph input: attention is focused on words that are relevant to the node type rather than the whole sentence. So the question \u201chow useful are all these graph operations\u201d remain. A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR'17 (see match type in \"LEARNING END-TO-END GOAL-ORIENTED DIALOG\" by Bordes et al)\n\n\nThe authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both? A description of the actual implementation would help  (no pointer to open source code is provide). The author mentions Theano in one of my questions: how are the transformations compiled in advance as units? How is the gradient back-propagated through the graph is this one is only described at runtime?\n\n\nTypo: in the appendices B.2 and B.2.1, the right side of the equation that applies the update gate has h\u2019_nu while it should be h_nu.\n\nIn the references, the author could mention the pioneering work  of Lee Giles on representing graphs with  RNNs.\n\nRevision: I have improved my rating for the following reasons:\n- Pointers to an highly readable and well structured Theano source is provided.\n- The delta improvement of the paper has been impressive over the review process, and I am confident this will be an impactful paper.\n- Much simpler alternatives approaches such as Memory Networks seem to be plateauing for problems such as dialog modeling, we need alternatives.\n- The architecture is this work is still too complex, but this is often as we start with DNNs, and then find simplifications that actually improve performance\n", "title": "Inference vs. learning", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJJ2BgM7x": {"type": "rebuttal", "replyto": "BJ1YgITzx", "comment": "Thank you for the helpful suggestions. A related-work section and baseline data for the bAbI tasks will be included in the next revision of the paper.\n\nI have considered baselines for the experiments described in 4.2. However, it is difficult to represent the input and output in a way that makes sense for existing models, especially with the Turing machine example. I attempted to use a sequence-to-sequence model using a textual description of the graph format, but found it difficult to train due to the length and complexity of the textual representation. Although baselines could conceivably be constructed that do not use the graphical structure (perhaps simply using a sequence of symbols to represent the tape of the Turing machine), the experiments in 4.2 were meant to demonstrate that GGT-NNs can naturally manipulate complex graphical structures, and were not intended to show superiority of the GGT-NN model over non-graphical models using non-graphical representations.\n\nDue to space constraints, it did not seem reasonable to explore additional tasks for which the results could be compared to existing literature. However, this is certainly an important area to consider in future work. Similarly, using a mixture of strong and weak supervision is a great idea that I will attempt to investigate further in the future.", "title": "Context and baselines"}, "r1swHlM7g": {"type": "rebuttal", "replyto": "BJKOnF1Xl", "comment": "Thank you for your valuable input. The next revision of the paper will include an additional section comparing this model to existing works, as well as a summary and clearer explanation of the transformation operations.\n\nOverall, the primary strengths of this model relative to other models are:\n\n1) the input to the network does not need to be structured. This is in contrast to GGS-NNs or other existing graph-based approaches, which require input in graphical format, or to lifted relational networks, which require input in terms of predicate-based facts and hand-specified rules.\n\n2) the internal state of the network is an explicit graph, and can potentially be extracted, analyzed, or used for downstream applications. This is in contrast to the internal state of other architectures with differentiable internal state, such as memory networks or differentiable neural computers: although they can store graphical relationships implicitly, they do not have an explicit graphical structure. (Note that differentiable neural computers do have connections based on the temporal access patterns of the data, but these do not necessarily correspond to edges between nodes in the underlying structure of the data.)\n\nThe structure described in Algorithm 1 utilizes all of the graph transformations in a particular order that is well-suited to a question-answering task. This is also one of the most powerful structures that I considered. However, subsets of the graph transformations could be used to construct a model for a different type of task. For instance, the node addition transformation can be removed if the set of nodes is fixed, and the query processing steps can be removed if the desired output is a graph. Appendix B also shows how the structure can be modified to answer the query based on a sequence of graphs, rather than a single graph.", "title": "Clarifications and improving paper clarity"}, "BJKOnF1Xl": {"type": "review", "replyto": "HJ0NvFzxl", "review": "\nDear authors,\n\n First, I would like to say that the topic of the article is clearly interesting, but the paper is very hard to follow since the writing is a little bit rough. The paper could clearly benefit to some additional figures for example when presenting the model, and to separate the presentation of the different allowed operations with the \u201ctechnical\u201d computations.\n\n My first question is about the positioning of the paper and the links between your approach and other approaches like Relational Neural networks (or lifed relational neural networks)  for example or even differentiable neural computers. Particularly, having a discussion about the expressiveness of the different approaches would be very nice (if feasuible) \nCould you please discuss a little bit more the difference/advantages/drawbacks of your approach with existing works ? \n\nMy second question concerns the way the structure is defined in the experimental part (Algorithm 1). It is not clear to me if other structures could be proposed. Could you explain a little bit more this point ? And particularly the advantage of the chosen structure w.r.t other possibilities and for the particular tasks studied here ? The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.\n\nThe approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus easily learnable through gradient-descent techniques. It can be seen as a succesfull attempt to mix continuous and symbolic representations. It moreover seems more general that the recent attempts made to add some 'symbolic' stuffs in differentiable models (Memory networks, NTM, etc...) since the shape of the state is not fixed here and can evolve. My main concerns is about the way the model is trained i.e by providing the state of the graph at each timestep which can be done for particular tasks (e.g Babi) only, and cannot be the solution for more complex problems. My other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format, making the article still difficult to read due to its density. ", "title": "Hard paper...", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hk_mPh-4e": {"type": "review", "replyto": "HJ0NvFzxl", "review": "\nDear authors,\n\n First, I would like to say that the topic of the article is clearly interesting, but the paper is very hard to follow since the writing is a little bit rough. The paper could clearly benefit to some additional figures for example when presenting the model, and to separate the presentation of the different allowed operations with the \u201ctechnical\u201d computations.\n\n My first question is about the positioning of the paper and the links between your approach and other approaches like Relational Neural networks (or lifed relational neural networks)  for example or even differentiable neural computers. Particularly, having a discussion about the expressiveness of the different approaches would be very nice (if feasuible) \nCould you please discuss a little bit more the difference/advantages/drawbacks of your approach with existing works ? \n\nMy second question concerns the way the structure is defined in the experimental part (Algorithm 1). It is not clear to me if other structures could be proposed. Could you explain a little bit more this point ? And particularly the advantage of the chosen structure w.r.t other possibilities and for the particular tasks studied here ? The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.\n\nThe approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus easily learnable through gradient-descent techniques. It can be seen as a succesfull attempt to mix continuous and symbolic representations. It moreover seems more general that the recent attempts made to add some 'symbolic' stuffs in differentiable models (Memory networks, NTM, etc...) since the shape of the state is not fixed here and can evolve. My main concerns is about the way the model is trained i.e by providing the state of the graph at each timestep which can be done for particular tasks (e.g Babi) only, and cannot be the solution for more complex problems. My other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format, making the article still difficult to read due to its density. ", "title": "Hard paper...", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJ1YgITzx": {"type": "rebuttal", "replyto": "HJ0NvFzxl", "comment": "Great job with the paper. I love the idea.\n\nIt seems the main weakness is the lack of broader context. Specifically,\n\nBaselines: \n- it'd be nice to see bAbI results from other papers in Table 1, to quickly get a sense for how your results compare\n- have you thought about baseline methods for the experiments in 4.2?\n- are there any other tasks like bAbI where you can compare GGT-NN to existing results from the literature?\n\nRelated work:\n- I'd appreciate a related work section with discussion of the similarities and differences to other models with more or less structured memory representations including, e.g., Memory Networks, Hierarchical Attentive Memory, and Differentiable Neural Computers (DNC).\n\nFinally, have you considered training your model with a mixture of strong and weak supervision? Maybe you could get away with just a few instances labeled with the strong supervision described in 3.1, with the rest labeled only by the correct answer?", "title": "Cool paper!"}}}