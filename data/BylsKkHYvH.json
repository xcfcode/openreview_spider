{"paper": {"title": "Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks", "authors": ["Joonyoung Yi", "Juhyuk Lee", "Kwang Joon Kim", "Sung Ju Hwang", "Eunho Yang"], "authorids": ["joonyoung.yi@kaist.ac.kr", "sehkmg@kaist.ac.kr", "preppie@yuhs.ac", "sjhwang82@kaist.ac.kr", "eunhoy@kaist.ac.kr"], "summary": "", "abstract": "Handling missing data is one of the most fundamental problems in machine learning. Among many approaches, the simplest and most intuitive way is zero imputation, which treats the value of a missing entry simply as zero. However, many studies have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks. Yet, none of the existing work has explained what brings such performance degradations. In this paper, we introduce the variable sparsity problem (VSP), which describes a phenomenon where the output of a predictive model largely varies with respect to the rate of missingness in the given input, and show that it adversarially affects the model performance. We first theoretically analyze this phenomenon and propose a simple yet effective technique to handle missingness, which we refer to as Sparsity Normalization (SN), that directly targets and resolves the VSP. We further experimentally validate SN on diverse benchmark datasets, to show that debiasing the effect of input-level sparsity improves the performance and stabilizes the training of neural networks.", "keywords": ["Missing Data", "Collaborative Filtering", "Health Care", "Tabular Data", "High Dimensional Data", "Deep Learning", "Neural Networks"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper investigates the problem of using zero imputation when input features are missing. The authors study this problem, propose a solution, and evaluate on several benchmark datasets. The reviewers were generally positive about the paper, but had some questions and concerns about the experimental results. The authors addressed these concerns in the rebuttal. The reviewers are generally satisfied and believe that the paper should be accepted."}, "review": {"r1lmuyH9tr": {"type": "review", "replyto": "BylsKkHYvH", "review": "Zero imputation is studied from a different view by investigating its impact on prediction variability and a normalization scheme is proposed to alleviate this variation. This normalization scales the input neural network so that the output would not be affected much. While such simple yet helpful algorithms are plausible there are number of remaining issues:\n1-\tZero imputation, as authors mentioned, is not an acceptable algorithm for imputation and improving on that via the normalization proposed in the paper cannot be counted as an exciting move in this area unless an extensive comparison shows it\u2019s benefits over the many other existing techniques. I am interested to see how would the results be if you compare this simple algorithm with more complicated ones like GAIN or MisGAN. It is argued in the paper that with high dimensional data, your algorithm is more acceptable, but how would it be with in other cases?\n2-\tYour algorithm is only explained with neural net framework, how can we extend it to the other machine learning models?\n3-\tIs batch normalization used in your experiments? Scaling the activation in one layer to reduce its impact on the next layer is somehow similar to what happens in batch normalization, and I am wondering if BN makes any similar effect?\n4-\tPlease provide labels for the x-axes in the figures.\n\n------------------------------------------\nAfter rebuttal:\nThanks for adding the extra experiments.\nLooking at Table 9 in appendix, I am bit surprised to see that sometimes mean imputation works better than MICE (GAIN usually works good with large data). Maybe it attributes to the missing features. How did you choose to apply 20% missingness? randomly?", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}, "SkxqwIb3or": {"type": "rebuttal", "replyto": "Bkl5QuoYoH", "comment": "We additionally investigate how the model outputs change with the number of known entries of the input when each normalization is applied (as we show in Figure 1). Since BN and LN do not specifically target VSP in the first place, they do not completely solve the bias caused by input sparsity levels. In particular, this phenomenon becomes more pronounced when L2 weight decay is not applied (or weight decay parameter lambda is small), hence we have large absolute values of weights. The details can be found in Appendix H.1.2. ", "title": "Additional Comment of Response to Reviewer #1 [Part 3/4]"}, "Bkl5QuoYoH": {"type": "rebuttal", "replyto": "r1lmuyH9tr", "comment": "[3. Does Batch Normalization make similar effect to SN?]\n\n- We thank for the interesting suggestion. We performed experimental comparison against both BN and Layer Normalization (LN) (since BN could stabilize statistics of hidden layer but it does not consider instance wise characteristics). Please see Appendix H for the results and discussions.\n\nIn our new experiments, SN significantly outperforms both LN and BN in most cases (or yields at least comparable performance in all cases).  Note that, in certain settings, LN and BN perform even worse than vanilla zero imputation with a large margin. For instance, on Movielens 100K (AutoRec, item vector encoding), RMSE of vanilla zero imputation, LN, BN as follows: 0.8835 \u00b1 0.0003 (zero imputation w/o SN) vs. 0.9396 \u00b1 0.0141 (LN) vs. 0.9205 \u00b1 0.0081 (BN). Thus neither BN or LN seem to be effective in solving the VSP problem as SN. \n\nIn all our previous experiments, we inadvertently did not consider Batch Normalization (BN) simply because BN is not widely used in dealing with tabular datasets despite its universality on vision tasks.", "title": "Response to Reviewer #1 [Part 3/4]"}, "Bkl49OsYoS": {"type": "rebuttal", "replyto": "r1lmuyH9tr", "comment": "[2. Your algorithm is only explained with neural net framework, how can we extend it to the other machine learning models?]\n\n- In this paper, we analyze the variable sparsity problem with the focus of neural networks. Our theoretical analysis can be seamlessly applied to certain non-neural network models (e.g. shallow linear regression is a special case of our analysis without hidden layers). However,  we need further research to confirm whether VSP occurs for all machine learning models in general. \n\nWe believe that VSP for each model needs to be studied separately. Even within the neural network framework we are focusing on, there are different results depending on the type of activation functions and other details as you can see in Theorem 1-3. ", "title": "Response to Reviewer #1 [Part 2/4] "}, "SkeTlustsB": {"type": "rebuttal", "replyto": "r1lmuyH9tr", "comment": "\n[4. Please provide labels for the x-axes in the figures.]\n- We apologize for the confusion. We fixed this issue on our revised paper. \n", "title": "Response to Reviewer #1 [Part 4/4]"}, "Hkx5MFotjH": {"type": "rebuttal", "replyto": "rkgNqJ7iKS", "comment": "\n[4. Does your model assume all input values are numerical but not categorical?]\n- There is no restriction about the type of inputs in our analysis and the construction of our algorithm. In fact, CF-NADE and CF-UIcA for collaborative filtering datasets in our experiments, only allow categorical values for their inputs where SN successfully achieves the performance improvement. Another example of using SN for categorical input is density estimation tasks (binarized MNIST) in Section 4.5. \n", "title": "Response to Reviewer #3 [Part 4/4]"}, "Hkg_4YiFiH": {"type": "rebuttal", "replyto": "rkgNqJ7iKS", "comment": "\n[3. MCAR assumption]\n- We also fully agree that MCAR assumption is the one that are generally not well established in real cases. But, this assumption drastically simplifies our statements (as we know, theoretical analysis always requires some simplified assumptions and hence there's some gap with reality). Without this assumption, we can't get such neat statements since we have to worry about some worst cases in our analysis, but this does not mean SN is ineffective even in theory; we can still see that SN can reduce the dependency on sparsity level to some extent, although in a much more complex form. In order to make up for having such a simplified assumption, we experimentally show that variable sparsity problems actually exist in various real-world datasets even where the MCAR assumption does not hold, and that SN can relieve this problem.\n", "title": "Response to Reviewer #3 [Part 3/4]"}, "rylbIKoFsH": {"type": "rebuttal", "replyto": "rkgNqJ7iKS", "comment": "\n[2. Comparison with other missing handling techniques]\n- First of all, the main contribution of our paper is to provide a deeper understanding and the corresponding solution about the issue that the zero imputation, the simplest and most intuitive way of handling missing data, degrades the performance in training neural networks. Hence, we only considered the vanilla zero imputation as our baseline in the submission since we do not claim that our corrected zero imputation (with SN) is the best for all tasks. \n\nHowever, some reviewers wanted to see direct comparisons against other state-of-the-art imputation techniques such as GAIN [1] and GMMC [2]. Hence, we performed additional comparisons against them on tasks considered in our paper as well as available tasks in [1] and [2].  Interestingly (and thanks to the reviewers who raised this issue), our corrected zero imputation (with SN), even with its simplicity, shows at least comparable or significantly better performances over all baselines, on all considered tasks. Here only two cases (Movielens 100K for high dimensional/missing rate case; NHIS dataset for low dimensional/missing rate case) are shown as examples and the rest are described in Appendix H:\n\n(Movielens 100K using item vector encoding) \n----------------------------------------------------------------\n                  Model                  |         Test RMSE\n----------------------------------------------------------------\n  Zero Imputation w/o SN   |     0.8835 \u00b1 0.0003\n  Zero Imputation w/ SN     |     0.8809 \u00b1 0.0011\n----------------------------------------------------------------\n  Zero Imputation w/ BN     |     0.9205 \u00b1 0.0081\n  Zero Imputation w/ LN     |     0.9396 \u00b1 0.0141\n  Dropout                               |     0.9268 \u00b1 0.0261\n  Mean Imputation              |     0.9206 \u00b1 0.0012\n  Median Imputation           |     0.9196 \u00b1 0.0017\n  kNN                                     |     0.9133 \u00b1 0.0011\n  MiCE                                    |     0.9209 \u00b1 0.0022\n  SoftImpute                        |     0.8867 \u00b1 0.0007\n  GMMC                                |     0.9109 \u00b1 0.0166\n  GAIN                                   |     1.0354 \u00b1 0.0101\n----------------------------------------------------------------\n(NHIS dataset diabetes identification task) \n----------------------------------------------------------------\n                  Model                  |         Test AUROC\n----------------------------------------------------------------\n  Zero Imputation w/o SN   |     0.9121 \u00b1 0.0097\n  Zero Imputation w/ SN     |     0.9283 \u00b1 0.0011\n----------------------------------------------------------------\n  Zero Imputation w/ BN     |     0.9026 \u00b1 0.0105\n  Zero Imputation w/ LN     |     0.9127 \u00b1 0.0056\n  Dropout                               |     0.9101 \u00b1 0.0054\n  Mean Imputation              |     0.9117 \u00b1 0.0075\n  Median Imputation           |     0.8975 \u00b1 0.0060\n  kNN                                     |     0.9107 \u00b1 0.0075\n  MiCE                                    |     0.9224 \u00b1 0.0021\n  SoftImpute                        |     0.9224 \u00b1 0.0019\n  GMMC                                |     0.9109 \u00b1 0.0045\n  GAIN                                   |     0.9091 \u00b1 0.0067\n----------------------------------------------------------------\nNote that the evaluation metrics are different for above two cases (RMSE for Movielens and AUROC for NHIS). In the most low dimensional and low missing rate cases, the problem is relatively easy, so all imputation methods work comparably well. \n\nWe still believe that each imputation technique has its own advantages and disadvantages, and we do not claim that our corrected zero imputation (with SN) is always the best. However, we do believe that these new experiments show SN is a sufficiently competitive technique.\n\n[1] Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar.  Gain: Missing data imputation using generative adversarial nets.  In Proceedings of the 35th International Conference on Machine Learning-Volume 71, 2018\n[2] Marek \u0301Smieja, \u0141ukasz Struski, Jacek Tabor, Bartosz Zieli \u0301nski, and Przemys\u0142aw Spurek. Processing of missing data by neural networks. In Advances in Neural Information Processing Systems, pp.2719\u20132729, 2018.\n", "title": "Response to Reviewer #3 [Part 2/4]"}, "SJeshdoFoH": {"type": "rebuttal", "replyto": "r1lmuyH9tr", "comment": "We thank the reviewer for thoughtful and constructive feedback.\n\n[1. Comparison with other missing handling techniques]\n- First of all, the main contribution of our paper is to provide a deeper understanding and the corresponding solution about the issue that the zero imputation, the simplest and most intuitive way of handling missing data, degrades the performance in training neural networks. Hence, we only considered the vanilla zero imputation as our baseline in the submission since we do not claim that our corrected zero imputation (with SN) is the best for all tasks. \n\nHowever, some reviewers wanted to see direct comparisons against other state-of-the-art imputation techniques such as GAIN [1] and GMMC [2]. Hence, we performed additional comparisons against them on tasks considered in our paper as well as available tasks in [1] and [2].  Interestingly (and thanks to the reviewers who raised this issue), our corrected zero imputation (with SN), even with its simplicity, shows at least comparable or significantly better performances over all baselines, on all considered tasks. Here only two cases (Movielens 100K for high dimensional/missing rate case; NHIS dataset for low dimensional/missing rate case) are shown as examples and the rest are described in Appendix H:\n\n(Movielens 100K using item vector encoding) \n----------------------------------------------------------------\n                  Model                  |         Test RMSE\n----------------------------------------------------------------\n  Zero Imputation w/o SN   |     0.8835 \u00b1 0.0003\n  Zero Imputation w/ SN     |     0.8809 \u00b1 0.0011\n----------------------------------------------------------------\n  Zero Imputation w/ BN     |     0.9205 \u00b1 0.0081\n  Zero Imputation w/ LN     |     0.9396 \u00b1 0.0141\n  Dropout                               |     0.9268 \u00b1 0.0261\n  Mean Imputation              |     0.9206 \u00b1 0.0012\n  Median Imputation           |     0.9196 \u00b1 0.0017\n  kNN                                     |     0.9133 \u00b1 0.0011\n  MiCE                                    |     0.9209 \u00b1 0.0022\n  SoftImpute                        |     0.8867 \u00b1 0.0007\n  GMMC                                |     0.9109 \u00b1 0.0166\n  GAIN                                   |     1.0354 \u00b1 0.0101\n----------------------------------------------------------------\n(NHIS dataset diabetes identification task) \n----------------------------------------------------------------\n                  Model                  |         Test AUROC\n----------------------------------------------------------------\n  Zero Imputation w/o SN   |     0.9121 \u00b1 0.0097\n  Zero Imputation w/ SN     |     0.9283 \u00b1 0.0011\n----------------------------------------------------------------\n  Zero Imputation w/ BN     |     0.9026 \u00b1 0.0105\n  Zero Imputation w/ LN     |     0.9127 \u00b1 0.0056\n  Dropout                               |     0.9101 \u00b1 0.0054\n  Mean Imputation              |     0.9117 \u00b1 0.0075\n  Median Imputation           |     0.8975 \u00b1 0.0060\n  kNN                                     |     0.9107 \u00b1 0.0075\n  MiCE                                    |     0.9224 \u00b1 0.0021\n  SoftImpute                        |     0.9224 \u00b1 0.0019\n  GMMC                                |     0.9109 \u00b1 0.0045\n  GAIN                                   |     0.9091 \u00b1 0.0067\n----------------------------------------------------------------\nNote that the evaluation metrics are different for above two cases (RMSE for Movielens and AUROC for NHIS). In the most low dimensional and low missing rate cases, the problem is relatively easy, so all imputation methods work comparably well. \n\nWe still believe that each imputation technique has its own advantages and disadvantages, and we do not claim that our corrected zero imputation (with SN) is always the best. However, we do believe that these new experiments show SN is a sufficiently competitive technique.\n\n[1] Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar.  Gain: Missing data imputation using generative adversarial nets.  In Proceedings of the 35th International Conference on Machine Learning-Volume 71, 2018\n[2] Marek \u0301Smieja, \u0141ukasz Struski, Jacek Tabor, Bartosz Zieli \u0301nski, and Przemys\u0142aw Spurek. Processing of missing data by neural networks. In Advances in Neural Information Processing Systems, pp.2719\u20132729, 2018.\n", "title": "Response to Reviewer #1 [Part 1/4] "}, "HJxNcFitiH": {"type": "rebuttal", "replyto": "rkgNqJ7iKS", "comment": "We thank the reviewer for thoughtful and constructive feedback.\n\n[1. Powerful backbone architecture on collaborative filtering datasets]\n- We use AutoRec (Sedhain et al., 2015) and its variant CF-NADE (Zheng et al., 2016) without any intention simply because many modern nn-based models are in fact variants of AutoRec. But, following the reviewers\u2019 valuable suggestion, we consider CF-UIcA [3], one of the current state-of-the-arts, as a new backbone on collaborative filtering datasets, and consistently achieve even stronger performances (in terms of RMSE):\n    - Movielens 100K: 0.8945 \u00b1 0.0024 (w/o SN) vs. 0.8793 \u00b1 0.0017 (w/ SN)\n    - Movielens 1M: 0.8223 \u00b1 0.0016 (w/o SN) vs. 0.8178 \u00b1 0.0007 (w/ SN)\n\nNote that we do not test for Movielens 10M because the authors of CF-UIcA did not provide the results for it due to the complexity of the model. \n\n[3] Du, C., Li, C., Zheng, Y., Zhu, J., & Zhang, B. (2018, April). Collaborative filtering with user-item co-autoregressive models. In Thirty-Second AAAI Conference on Artificial Intelligence.\n", "title": "Response to Reviewer #3 [Part 1/4]"}, "BJgXpqjYsS": {"type": "rebuttal", "replyto": "S1g-T-zX5B", "comment": "Thank you for your comments. Please refer to the answer to R1 or R3. \n\nThere is a minor note about the dropout method in the GMMC [2] paper you mentioned. The dropout method uses a single drop (missing) probability uniformly across all instances of the dataset. On the other hand, our algorithm normalizes each data instance with its own missing rate. The global drop probability does not solve the variable sparsity problem, so it shows poor performance as seen in Appendix H. For example, on Movielens 100K dataset (AutoRec with item vector encoding), the RMSE of dropout and SN is as follows: 0.9268 \u00b1 0.0261 (dropout) vs 0.8809 \u00b1 0.0011 (zero imputation w/ SN). For other various datasets and models, SN also shows similar or better performance compared to the dropout.\n\n[2] Marek \u0301Smieja, \u0141ukasz Struski, Jacek Tabor, Bartosz Zieli \u0301nski, and Przemys\u0142aw Spurek. Processing of missing data by neural networks. In Advances in Neural Information Processing Systems, pp.2719\u20132729, 2018.\n", "title": "Response for Jaeyoon Yoo"}, "Bkei_wiFoB": {"type": "rebuttal", "replyto": "r1eWppVRFr", "comment": "We thank the reviewer for thoughtful and constructive feedback.\n\n[Powerful backbone architecture on collaborative filtering datasets]\n- We use AutoRec (Sedhain et al., 2015) and its variant CF-NADE (Zheng et al., 2016) without any intention simply because many modern nn-based models are in fact variants of AutoRec. But, following the reviewers\u2019 valuable suggestion, we consider CF-UIcA [3], one of the current state-of-the-arts, as a new backbone on collaborative filtering datasets, and consistently achieve even stronger performances (in terms of RMSE):\n    - Movielens 100K: 0.8945 \u00b1 0.0024 (w/o SN) vs. 0.8793 \u00b1 0.0017 (w/ SN)\n    - Movielens 1M: 0.8223 \u00b1 0.0016 (w/o SN) vs. 0.8178 \u00b1 0.0007 (w/ SN)\n\nNote that we do not test for Movielens 10M because the authors of CF-UIcA did not provide the results for it due to the complexity of the model. \n\n[3] Du, C., Li, C., Zheng, Y., Zhu, J., & Zhang, B. (2018, April). Collaborative filtering with user-item co-autoregressive models. In Thirty-Second AAAI Conference on Artificial Intelligence.\n", "title": "Response to Reviewer #2"}, "rkgNqJ7iKS": {"type": "review", "replyto": "BylsKkHYvH", "review": "This paper provides a novel solution to the variable sparsity problem, where the output of neural networks biased with respect to the number of missing inputs. The authors proposed a sparsity normalization algorithm to process the input vectors to encounter the bias. In experiments, the authors evaluated the proposed sparsity normalization model on multiple datasets: collaborative filtering datasets, electric medical records datasets, single-cell RNA sequence datasets and UCI datasets. Results show that the proposed normalization method improves the prediction performance and the predicted values of the neural network is more uniformly distributed according to the number of missing entries.\n\nThe paper describes a clear and specific machine learning problem. Then the authors demonstrate a simple normalization strategy is capable of fixing the issue of biased prediction. The paper has a well-organized structure to convey the motivation. Therefore, my opinion on this paper leans to an acceptation. My questions are mainly on the experiment section:\n\n1) As shown in Table 2, there are various new collaborative filtering methods proposed after 2015, why the authors chose to extend AutoRec (Sedhain et al., 2015) but not other new methods?\n\n2) In the experiments, you compare your model with zero imputation (Please correct me if w/o SN is not zero imputation). However, I think it is a common practice in machine learning that we perform imputation with mean or median values. I'm interested in knowing whether filling with mean/median values work with these datasets.\n\n3) In section 4.5, you mentioned that \"SN is effective\neven when MCAR assumption is not established\". However, I'm still not clear about the reason. I believe many machine learning datasets have NMAR (not missing at random) type of missing data, but not MCAR. So this is an important issue for me.\n\n4) Does your model assume all input values are numerical but not categorical? ", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 1}, "r1eWppVRFr": {"type": "review", "replyto": "BylsKkHYvH", "review": "This paper studies a very interesting phenomena in machine learning called VSP, that is the output of the model is highly affected via the level of missing values in its input.  The authors demonstrate the existence of such phenomena empirically, analyze the root cause for it theoretically, and propose a simple yet effective normalization method to tackle the problem. Several experiments demonstrate the effectiveness of this method.\n\nIn general I think the paper is descent and elegant. It is motivated from real-world pain-point, gives a rigorous study towards the root cause, and the proposed method is very effective. To the best of my knowledge there is no prior work looking deep into this area and this paper does bring new insights to the community. As a result I would vote for its acceptance.\n\nOne issue is that I find the backbone methods in experiments are somehow out-of-date. For example, AutoRec (2015) and CF-NADE (2016). I admit that I\u2019m not an expert in the field of recommendation but still think that more recent, and powerful baseline algorithms should be applied on to further demonstrate the true effectiveness of Sparsity Normalization.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}}}