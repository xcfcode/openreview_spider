{"paper": {"title": "Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition", "authors": ["Yangming Li", "lemao liu", "Shuming Shi"], "authorids": ["~Yangming_Li1", "~lemao_liu1", "~Shuming_Shi1"], "summary": "This work studys what are the impacts of unlabeled entity problem on NER models and how to effectively eliminate them by a general method.", "abstract": "In many scenarios, named entity recognition (NER) models severely suffer from unlabeled entity problem, where the entities of a sentence may not be fully annotated. Through empirical studies performed on synthetic datasets, we find two causes of performance degradation. One is the reduction of annotated entities and the other is treating unlabeled entities as negative instances. The first cause has less impact than the second one and can be mitigated by adopting pretraining language models. The second cause seriously misguides a model in training and greatly affects its performances. Based on the above observations, we propose a general approach, which can almost eliminate the misguidance brought by unlabeled entities. The key idea is to use negative sampling that, to a large extent, avoids training NER models with unlabeled entities. Experiments on synthetic datasets and real-world datasets show that our model is robust to unlabeled entity problem and surpasses prior baselines. On well-annotated datasets, our model is competitive with the state-of-the-art method.", "keywords": ["Named Entity Recognition", "Unlabeled Entity Problem", "Negative Sampling"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper studies the unlabeled entity problem in NER. Specifically, performance degradation in training of NER models due to unlabeled entities. It analyzes the reason through evaluation on synthetic datasets and finds that it is due to the fact that all the unlabeled entities are treated as negative examples. To cope with the problem, it proposes a negative sampling method which considers the use of only a small subset of unlabeled entities. Experimental results show that the proposed method achieves better performances than the baselines on real-world datasets and achieves competitive performances compared with the state-of-the-art methods on well-annotated datasets.\n\nPros\n\u2022\tThe paper is clearly written.\n\u2022\tThe proposed method appears to be technically sound.\n\u2022\tExperimental results support the main claims.\n\u2022\tThe findings in the paper are useful for the field.\n\nCons\n\u2022\tNovelty of the work might not be enough.\n\nThe authors have addressed some clarity and reference issues pointed out by the reviewers in the rebuttal.  Discussions have been made among the reviewers.\n"}, "review": {"3E-03klkV0": {"type": "review", "replyto": "5jRVa89sZk", "review": "This paper focuses on the unlabeled entity problem in NER, where the entities of a sentence are incomplete annotated. Since some entities may not be annotated, the performance of models can be degraded. This paper analyzes the performance degradation by evaluating synthetic datasets and finds that all the unlabeled entities are treated as negative instances is the main factor of the performance degradation. To alleviate the performance degradation, this paper proposes a negative sampling approach that considers only a small subset of unlabeled entities in order to reduce the impacts of unlabeled entities. The experimental results show that the proposed method achieves better performances compared to previous studies on real-world datasets and achieves competitive performances compared to the state-of-the-art methods on well-annotated datasets.\n\nStrong points:\n+ The analysis using two proposed metrics to find causes of the performance degradation of the unlabeled entity problem is quite interesting.\n+ The idea of using the negative sampling under the data imbalance situation, especially in NER, seems reasonable.\n+ The paper is well-written and easy to follow. It provides comprehensive experiments on several datasets.\n\nConcerns: \n1. The key concern about this paper is the missing of some references. There are some previous studies related to the unlabeled entity problem. Particularity, this problem is quite related to or almost the same as the \u201cIncomplete annotations\u201d at [1] or \u201cPartially annotated training data\u201d at [2]. In addition, the unlabeled entity problem has a little relevance to the data imbalances [3] if we assume that all unlabeled spans are negative instances. Therefore, the proposed method should be compared with these previous studies experimentally and/or theoretically.\n[1] Jie et al., Better Modeling of Incomplete Annotations for Named Entity Recognition, NAACL 2019.\n[2] Mayhew et al., Named Entity Recognition with Partially Annotated Training Data, CONLL 2019.\n[3] Li et al., Dice Loss for Data-imbalanced NLP Tasks, ACL 2020.\n2. In the experiments on the real-word dataset, the proposed method only compared with weak baselines. The reviewer suggests the authors to evaluate with the state-of-the-art NER methods such as BERT-MRC or BERT-Biaffine Model.\n3. At the inference time, every span should be input to the MLP layer to obtain the predicted score. That is, the proposed method takes more time (O(n^2)) to infer new sentences compared to other methods (O(n)). Furthermore, this paper uses heuristic(s) when the spans of inferred entities intersects. However, there is no discussion about them.\n\nMinor comments:\n1. In the section 3.2, the figures and the contents about the figures are inconsistent. That is, the middle parts of the figures are for the change in misguidance rates and the right-hand ones are for the change of the erosion rates.\n2. For batching with sampling, it would be better to provide more details about it.\n3. It would be nice to show a model convergence or loss convergence graph because this paper uses random sampling.\n4. It would be better to show the correlation in numbers between the f1 scores and the proposed rates.", "title": "A plausible approach but a weak comparison", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "7arLl5w8LMO": {"type": "rebuttal", "replyto": "3E-03klkV0", "comment": "Thanks for your valuable comments.\n\nComment-1: The key concern about this paper is the missing of some references. There are some previous studies related to the unlabeled entity problem. Particularity, this problem is quite related to or almost the same as the \u201cIncomplete annotations\u201d at [1] or \u201cPartially annotated training data\u201d at [2]. In addition, the unlabeled entity problem has a little relevance to the data imbalances [3] if we assume that all unlabeled spans are negative instances. Therefore, the proposed method should be compared with these previous studies experimentally and/or theoretically. [1] Jie et al., Better Modeling of Incomplete Annotations for Named Entity Recognition, NAACL 2019. [2] Mayhew et al., Named Entity Recognition with Partially Annotated Training Data, CONLL 2019. [3] Li et al., Dice Loss for Data-imbalanced NLP Tasks, ACL 2020.\n\nAnswer-1: We are sorry for the missing references. [1] further optimizes Partial CRF [4] and [2] is based on PU Learning [5]. In fact, both [4] and [5] are cited in our paper. Partial CRF demands well-annotated data to be trained with true negative instances and PU Learning relies on prior information and heuristics. They both lack enough flexibility compared with our method. To make comparisons on performances, we run the open source codes of [1] (https://github.com/allanj/ner_incomplete_annotation) and [5] (https://github.com/v-mipeng/LexiconNER) on the two real-world datasets (Note that [4] is already compared in our paper and the source code of [2] is not available). The results are shown in the table below.\n\nMethod\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0       EC\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0NEWS\n\nPartial CRF (Jie et al., 2019) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 61.75\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a078.64\n\nPU Learning\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0       61.22\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a077.98\n\nOur Model w/o BERT w/ BiLSTM\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0   64.68\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a082.11\n\nFrom the table, we can see that our proposed method has notably outperformed both Partial CRF and PU Learning. The improvements over both baselines are at least 4.74% on EC and 4.41% on NEWS. Your comment about data imbalances is very interesting and we will explore it in future work. We will add the experiment results and discussions above in the revised version. \n\n[1] Jie et al., Better Modeling of Incomplete Annotations for Named Entity Recognition, NAACL-2019\n\n[2] Mayhew et al., Named Entity Recognition with Partially Annotated Training Data, CONLL-2019\n\n[3] Li et al., Dice Loss for Data-imbalanced NLP Tasks, ACL 2020\n\n[4] Yang et al., Distantly supervised ner with partial annotation learning and reinforcement learning, COLING-2018\n\n[5] Peng et al., Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning, ACL-2019\n\nComment-2: In the experiments on the real-word dataset, the proposed method only compared with weak baselines. The reviewer suggests the authors to evaluate with the state-of-the-art NER methods such as BERT-MRC or BERT-Biaffine Model.\n\nAnswer-2: According to your suggestion, we additionally run three models (BERT-MRC, BERT-Biaffine, and BERT + Partial CRF) on the two real-world datasets. The results are demonstrated in the table below.\n\nMethod\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 EC\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0NEWS\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\nBERT-MRC\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 55.72\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 74.55\n\nBERT-Biaffine\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 55.99\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 74.57\n\nBERT + Partial CRF\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 63.12\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 81.24\n\nOur Model\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 66.17\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a085.39\n\nWe will add these experiment results in the revised version.\n\nComment-3: At the inference time, every span should be input to the MLP layer to obtain the predicted score. That is, the proposed method takes more time (O(n^2)) to infer new sentences compared to other methods (O(n)). Furthermore, this paper uses heuristic(s) when the spans of inferred entities intersects. However, there is no discussion about them.\n\nAnswer-3: The theoretical time complexity O(n^2) is due to the span selection procedure of our model. In fact, its running time is much less than that of the forward computation of neural networks (even for LSTM). In experiments, we find the latter is 10 times longer than the former. The heuristic used to solve span conflicts is simple yet effective. Empirically, the probability of our strategy making mistakes (the labeled span with higher score is not the true entity) is lower than 0.07%. We will clarify these facts in the revised version of our paper.", "title": "Response to AnonReview4 (First Three Comments)"}, "m5fDH_kmtZj": {"type": "rebuttal", "replyto": "7arLl5w8LMO", "comment": "Comment-4: In the section 3.2, the figures and the contents about the figures are inconsistent. That is, the middle parts of the figures are for the change in misguidance rates and the right-hand ones are for the change of the erosion rates.\n\nAnswer-4: we will rearrange the figures so that they are consistent with the contents.\n\nComment-5: For batching with sampling, it would be better to provide more details about it.\n\nAnswer-5: We will provide more details in the revised version and make detailed comments in the released source code.\n\nComment-6: It would be nice to show a model convergence or loss convergence graph because this paper uses random sampling.\n\nAnswer-6: We will add them in the revised version.\n\nComment-7: It would be better to show the correlation in numbers between the f1 scores and the proposed rates.\n\nAnswer-7: According to your suggestion, we calculate the Pearson Correlation Coefficient for both baselines (i.e., BERT Tagging and LSTM-tagging). The results are in the table below.\n\n\n1. Result for BERT Tagging\u00a0\u00a0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Erosion Rate\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Misguidance Rate\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\nCoNLL-2003\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0-0.94\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-1.00\n\nOntoNotes 5.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0-0.85\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-1.00\n\u00a0\n\n2. Result for LSTM Tagging\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Erosion Rate\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Misguidance Rate\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\nCoNLL-2003\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0-0.90\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-0.96\n\nOntoNotes 5.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0-0.82\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-0.98\n\nWe will add these results in the revised version.", "title": "Response to AnonReview4 (Last Four Comments)"}, "8dzm3IQqHKi": {"type": "rebuttal", "replyto": "JZDh9tCbsgS", "comment": "Thanks for your valuable feedbacks.\n\nComment-1: The way of constructing synthetic datasets should be explained clearly (Section 3.1).\u00a0\n\nAnswer-1: Synthetic datasets help us understand how unlabeled entity problem impacts NER models. A synthetic dataset is constructed from a well-annotated dataset (e.g., CoNLL-2003) as follows.\u00a0Given a probability p (say p=0.3), for each annotated entity in a sentence from a well-annotated dataset, we randomly sample a number \\mu (0<\\mu<1) from the uniform distribution. If \\mu <= 0.3, then we drop this entity for the underlying sentence; otherwise we retain this entity. We will explain it more clearly in the revised version. \n\nComment-2: It is not clear how does the negative sampling applied on the dataset (in sentence level? entity level? token level?).\n\nAnswer-2: Given a training case (i.e., a sentence and annotated entities), negative sampling is to randomly sample a subset of unlabeled spans (that are not the spans of any annotated entities) as the negative instances for training. Therefore, negative sampling is applied at entity level (more precisely, span level). Eq. (6) and Eq. (7) also indicate this fact. We will make it more clear in the revised version.", "title": "Response to AnonReview2"}, "3JCcwPG4ho6": {"type": "rebuttal", "replyto": "wM0Cz3H5hlb", "comment": "Thanks for your valuable comments.\n\nQuestion-1: The first question is about 4.2 Training via Negative sampling on page 5. I am not quite sure about the procedure. Negative instance candidates are randomly selected from original sentences. You use \\hat{y}, which is a subset of randomly selected span to replace a missed entity set defined in Eq. (2)?\n\nAnswer-1: We are sorry for the confusion. Actually, negative instance candidates denote all spans except manually labeled entities. Since these negative instance candidates may contain some real entities which are just neglected by annotators, we sample a subset \\hat{y} from these candidates as the negative instances for training (see Eq. (7)). \\tilde{y} in Eq. (2) is different from \\hat{y} in Eq. (7). The former denotes a set of named entities neglected by annotators while the latter is a set of negative instances for training. \n\nFor example, x= [Jack is from New York] and y = [(2, 3\uff0cLOC)] is the entity set annotated by human. We can see that human neglected an entity (0, 0, PER) and thus \\tilde{y} = [(0, 0, PER)]. \n\nIn this way, negative instance candidates set include all spans expect (2, 3\uff0cLOC), i.e., it is [(0, 0, O), (0, 1, O), (0, 2, O), (0, 3, O), (0, 4, O), (1, 1, O), (1, 2, O), (1, 3, O), (1, 4, O), (2, 1, O), (2, 2, O), (2, 4, O), (3, 3, O), (3, 4, O), (4, 4, O)]. \n\nIn addition, if we sample four negative instances as \\hat{y}, then \\hat{y} may be [(0, 3, O), (4, 4, O), (1, 1, O), (2, 4, O)].\n\nWe will add this example to explain these notations more clearly in our revised paper. \n\nQuestion-2: Could you expand more about Equation 8 to add more details?\n\nAnswer-2: Eq. (8) theoretically proves that, through negative sampling, the probability of treating a neglected entity as the negative instance is very low. To verify this conclusion in real scenarios, for every synthetic dataset, we calculate the percentage of neglected entities (i.e., the entities in \\tilde{y}) in the set of sampled negative instances (i.e., \\hat{y}). The results are demonstrated in the table below.\n\nDataset\\Masking Prob.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.4\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.6\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.9\n\nCoNLL-2003\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0   \u00a0 0.17%\u00a0\u00a0 \u00a0 \u00a00.41%\u00a0 \u00a0 \u00a0 \u00a0\u00a00.62%\u00a0 \u00a0 \u00a0\u00a00.83%\u00a0\u00a0 \u00a0 \u00a0\u00a01.12%\u00a0\u00a0 \u00a0 \u00a0\u00a01.41%\u00a0 \u00a0 \u00a0\u00a01.67%\u00a0\u00a0 \u00a0 \u00a0\u00a01.66%\u00a0\u00a0 \u00a0 \u00a0\u00a01.78%\n\nOntoNotes 5.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  0.07%\u00a0 \u00a0 \u00a0\u00a00.15%\u00a0 \u00a0 \u00a0 \u00a0\u00a00.21%\u00a0 \u00a0 \u00a0\u00a00.27%\u00a0\u00a0 \u00a0 \u00a0\u00a00.35%\u00a0 \u00a0 \u00a0\u00a00.42%\u00a0\u00a0 \u00a0\u00a00.48%\u00a0\u00a0 \u00a0 \u00a0\u00a00.56%\u00a0 \u00a0 \u00a0\u00a00.63%\n\nNote that all the percentage scores are averaged over 10 epochs and the \\lambda ratio (see Section 4.2) is set as 0.9. From the table, we can see that, empirically, the possibility of our negative sampling making mistakes (treating a neglected entity as a negative instance) is very low. These results further confirm the effectiveness of our negative sampling. We will add this experiment and make a deep analysis in the revised version.", "title": "Response to AnonReview3 (First Two Questions)"}, "5bXkHm8JYQT": {"type": "rebuttal", "replyto": "3JCcwPG4ho6", "comment": "Question-3: The unlabeled entity problem is most serious in the distant supervision setting. However, the distant supervision setting suffers from entity ambiguation and unlabeled entity problem simultaneously. How do you think your design to tackle entity ambiguation problem?\n\nAnswer-3: This paper focuses on analyzing and solving the unlabeled entity problem. Experiments show that our proposed method can almost eliminate the misguidance brought by unlabeled entities during training (see Section 6.2 and Figure 4). As for the entity ambiguation problem, a feasible idea is, in the preprocessing stage, using the context information to disambiguate entity labels.\n\nQuestion-4: Moreover, in the distant supervision experiment in Table 3, how will you model compare with other distant supervision models like AutoNER?\n\nAnswer-4: Besides Partial CRF, other related methods mainly include AutoNER [1] and PU Learning [2,3]. AutoNER allows learning from high-quality phrases that may be neglected entities. Because obtaining these phrases demands external resources, we can\u2019t offer direct comparisons. Note that our negative sampling doesn\u2019t rely on any external resource, which is more flexible. PU Learning assigns low weights to false negative instances in the loss function. LexiconNER [2] is based on PU Learning and has released its source code at https://github.com/v-mipeng/LexiconNER. We run the code on two real-world datasets (described in Section 6.4) and the results are shown in the table below.\n\nMethod\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      EC\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 NEWS\n\nPU Learning\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0     61.22\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a077.98\n\nOur Model w/o BERT w/ BiLSTM\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a064.68\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a082.11\n\nFrom the table, we can see that our proposed model has notably outperformed PU Learning on both datasets. The improvements of F1 scores are respectively 5.65% and 5.30% on EC and NEWS. We will add these results and make\u00a0a\u00a0detailed\u00a0analysis in the revised version.\n\n[1] Shang et al., Learning Named Entity Tagger using Domain-Specific Dictionary, EMNLP-2018\n\n[2] Peng et al., Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning, ACL-2019\n\n[3] Mayhew et al., Named Entity Recognition with Partially Annotated Training Data, CoNLL-2019", "title": "Response to AnonReview3 (Last Two Questions)"}, "IbfieAbz2Yp": {"type": "rebuttal", "replyto": "dN5WewHl75m", "comment": "Thanks for your valuable feedbacks.\n\nComment-1: We wonder what would give a classical distillation process on this task. [even if relevant comparison are made with results from the litterature]\n\nAnswer-1: Mainstream approaches to the problem include AutoNER [1], Partial CRF [2,3], and PU Learning [4,5]. AutoNER supports learning from high-quality phrases that may be unlabeled entities. However, mining these phrases demands external domain-specific corpora, which is not flexible. Partial CRF marginalizes the loss over all possible complete label sequences that are compatible with the incomplete annotation. However, it still demands well-annotated data to be trained with true negative instances. In experiments, our proposed methods have significantly outperformed Partial CRF on both real-world datasets (see Section 6.4 and Table 3). PU Learning assigns low weights to false negative instances in the loss function. However, obtaining these weights relies on prior information and heuristics, which is also inflexible.\n\nA work [4] based on PU Learning has released its source code [https://github.com/v-mipeng/LexiconNER]. We run the code on two real-world datasets to make comparisons with PU Learning. The results are shown in the table below.\n\nMethod\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      EC\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0NEWS\n\nPU Learning\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0    61.22\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a077.98\n\nOur Model w/o BERT w/ BiLSTM\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0   64.68\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a082.11\n\nWe will add these results and make a clear comparison in the revised version.\n\nComment-2: Given the architecture, we wonder what is the detailed learning procedure: it seems clear that the network is first trained on the real ground truth and then refined using the distillation loss. Section 3.1 is a little bit short on this point.\n\nAnswer-2: We didn\u2019t adopt multi-stage training strategies. On every synthetic dataset, we evaluate the performances of models with adjusted loss (see Eq. (2)) and MLE loss, separately. The two performances are used to show how unlabeled entity problem affects NER models. For example, the left part of Figure 1 demonstrates the results of four different settings (BERT Tagging w/ MLE, BERT Tagging w/ Adjusted Loss, LSTM Tagging w/ MLE, and LSTM Tagging w/ Adjusted Loss) on the synthetic datasets derived from CoNLL-2003. Each model is trained from scratch on a dataset. We will make this more clear in the revised version.\n\nComment-3: Regarding the model, equation (4) is not discussed nor analysed.\n\nAnswer-3: The design of Eq. (4) is inspired by ESIM model (see Eq. (14) in [6]). We also tried other options such as bi-affine scoring [7], but we found Eq. (4) performs the best. We will clarify this in the revised version.\n\n[1] Shang et al., Learning Named Entity Tagger using Domain-Specific Dictionary, EMNLP-2018\n\n[2] Yang et al., Distantly supervised ner with partial annotation learning and reinforcement learning, COLING-2018\n\n[3] Jie et al., Better Modeling of Incomplete Annotations for Named Entity Recognition, NAACL-2019\n\n[4] Peng et al., Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning, ACL-2019\n\n[5] Mayhew et al., Named Entity Recognition with Partially Annotated Training Data, CoNLL-2019\n\n[6] Chen et al., Enhanced LSTM for Natural Language Inference, ACL-2017\n\n[7] Dozat et al., DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING, ICLR-2017", "title": "Response to AnonReview1"}, "JZDh9tCbsgS": {"type": "review", "replyto": "5jRVa89sZk", "review": "This paper conducted an empirical analysis on the unlabeled entity problem in the NER task. It concluded that there are two reasons to affect the NER model's performance:  the reduction of annotated entities, and treating unlabeled entities as negative instances. Experiments showed that the latter reason gave a much more negative impact on the NER models. \n\nThis empirical study was conducted on the synthetic datasets which were extracted from two English NER datasets using negative sampling. The way of constructing synthetic datasets should be explained clearly (Section 3.1). It is not clear how does the negative sampling applied on the dataset (in sentence level? entity level? token level?). ", "title": "Practical, interesting results", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "wM0Cz3H5hlb": {"type": "review", "replyto": "5jRVa89sZk", "review": "This paper investigates the unlabeled entity problem, which is generally observed in the manual annotation setting and distant supervision as well. The unlabeled problem is important and some existing works focus on solving the problems using partial CRF setting or data selector. The main observation of this paper lies in two aspects: 1) comparison between the reduction of annotated entities or treating unlabeled entities as negative instances.  Most interestingly, the authors show the observed difference between pre-trained language models and LSTM-based models.  Based on the observations, they propose a general approach to eliminate the misguidance brought by unlabeled entities and such a simple design shows good performances.  \n\nThe Paper is overall well written and easy to follow. But  I still have a few questions and want to get answers from authors. \n\nQuestions:\n\n1) The first question is about 4.2 Training via Negative sampling on page 5. I am not quite sure about the procedure. Negative instance candidates are randomly selected from original sentences.  You use \\hat{y}, which is a subset of randomly selected span to replace a missed entity set defined in Eq. (2)?\n\n2) Could you expand more about Equation 8 to add more details?\n\n3) The unlabeled entity problem is most serious in the distant supervision setting. However, the distant supervision setting suffers from entity ambiguation and unlabeled entity problem simultaneously.  How do you think your design to tackle entity ambiguation problem? Moreover, in the distant supervision experiment in Table  3, how will you model compare with other distant supervision models like AutoNER?\n\n", "title": "Generally good, a few questions ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "dN5WewHl75m": {"type": "review", "replyto": "5jRVa89sZk", "review": "This article deals with the problem of partially labeled dataset: if some entities are missing, how SOTA approaches are going to behave? To answer this question, the authors degrade CoNLL classical dataset by masking a pourcentage of the labeled data. Then, they wonder which part of the missing performance is due to the lack of labels and which part is due to the incorrect labelling of discarded supervision.\nThe experiments are well explained and interesting on synthetic dataset. Then the authors propose a new cross entropy loss to test their hypothesis on real data by sampling high confidense negative samples as ground truth. It is a way of performing distillation on the model using negative sampling.\n\nConsistant & relevant work that deserves to be publised in ICLR.\n\n* We wonder what would give a classical distillation process on this task. [even if relevant comparison are made with results from the litterature]\n\n* Given the architecture, we wonder what is the detailed learning procedure: it seems clear that the network is first trained on the real ground truth and then refined using the distillation loss. Section 3.1 is a little bit short on this point.\n\n* Regarding the model, equation (4) is not discussed nor analysed.\n\n* The approach is rather simple but elegant.", "title": "a good idea supported by consistent experiments", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}