{"paper": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"], "summary": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "keywords": ["Deep learning", "Reinforcement Learning", "Games"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper presents an approach to structured exploration in StarCraft micromanagement policies (essentially small tasks in the game). From an application standpoint, this is a notable advance, since the authors are tackling a challenging domain and in doing so develop a novel exploration algorithm that seems to work quite well here.\n \n The main downside of the paper is that the proposed algorithm does seem fairly ad-hoc: certain approximations in the algorithm, like ignoring the argmax term in computing the backprop, are not really justified except in that it \"seems to work well in practice\" (a quote from the paper), so it's really unclear whether these represent general techniques or just a method that happens to work well on the target domain for poorly understood reasons.\n \n Despite this, however, I think the strength of the application is sufficient here. Deep learning has been alternatively pushed forward by more algorithmic/mathematical advances and more applied advances, with many of the major breakthroughs coming from seemingly ad-hoc strategies applied to challenging problems. This paper falls in that later category: the ZO algorithm may or may not lead to something slightly more disciplined in the future, but for now the compelling results on StarCraft are I believe enough to warrant accepting the paper.\n \n Pros:\n + Substantial performance improvement (over basic techniques like Q-learning) on a challenging task\n + Nice intuitive justification of a new exploration approach\n \n Cons:\n - Proposed algorithm seems rather ad-hoc, making some (admittedly not theoretically justified) approximations simply because they seem to work well in practice"}, "review": {"ryLbDa0Qg": {"type": "rebuttal", "replyto": "Syv17Wy7l", "comment": "We thank the reviewer for these constructive comments. We updated section 6 and the algorithm significantly to clarify the zero-order + backprop heuristic.\n\nAbout the combinatorial action space, it scales exponentially with the number of units. In practice, good strategies should implement some form of local consistency (both in space and time), which should make exploration feasible.\n\nAbout homogenous unit behavior: maps large number of units (e.g. m15v16 and w15v17) specifically test for the units to be coordinated around a strategy, but not take exactly the same action: we can see that focus firing with overkill gives poor results. Zero-order correctly learns not to overkill in such maps. As to formations, we did not observe such behavior. This is likely because the scenarios do not require this kind of strategies, but also because the model does not seem to be well-suited to learn coordinated movements (as we mentioned in the conclusion). For \"large\" groups of units (e.g. 15+ here), learning formations directly on atomic moves through RL appears to be a real challenge (e.g., because bad moves are extremely detrimental and there is no immediate feedback to guide exploration).", "title": "Re: coordinated exploration, and algorithm clarifications"}, "SJWYWpCXx": {"type": "rebuttal", "replyto": "B1VKKmeXl", "comment": "We thank the reviewer for the helpful comment. We removed the bold .79 for ZO in m15v16, which is, indeed, not comparable to .81 for the baseline heuristics. All the algorithms (heuristics and RL) were tested on 1000 battles. The numbers .99 and 1. (m5v5 DQN and ZO respectively) are a difference of 6 games. The standard deviation is about 10 games out of 1000.", "title": "Re: Table 2 uncertainty"}, "B1VKKmeXl": {"type": "review", "replyto": "r1LXit5ee", "review": "Why are multiple values bolded per row? I suppose these are statistically insignificant differences? If so: how was this decided... and can the details be noted (i.e. 95% confidence intervals of +/- value) ?The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem.\n\nThe main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this.\n\nThere are a few seemingly arbitrary choices that are justified only by \"it worked in practice\". For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: \"Also we neglected the argmax operation that chooses the actions\". I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as \"We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important\" and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper?\n\nThe presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(\\tilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: \"because we neglected that a single u is sampled for an entire episode\"; actually, no, you did mention this in the text above and it's clear from the pseudo-code too.\n\n\"perturbated\" -> \"perturbed\"\n\n--- After response period: \n\nNo rebuttal entered, therefore review remains unchanged.", "title": "Table 2 uncertainty", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJcQahSEg": {"type": "review", "replyto": "r1LXit5ee", "review": "Why are multiple values bolded per row? I suppose these are statistically insignificant differences? If so: how was this decided... and can the details be noted (i.e. 95% confidence intervals of +/- value) ?The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem.\n\nThe main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this.\n\nThere are a few seemingly arbitrary choices that are justified only by \"it worked in practice\". For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: \"Also we neglected the argmax operation that chooses the actions\". I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as \"We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important\" and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper?\n\nThe presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(\\tilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: \"because we neglected that a single u is sampled for an entire episode\"; actually, no, you did mention this in the text above and it's clear from the pseudo-code too.\n\n\"perturbated\" -> \"perturbed\"\n\n--- After response period: \n\nNo rebuttal entered, therefore review remains unchanged.", "title": "Table 2 uncertainty", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Syv17Wy7l": {"type": "review", "replyto": "r1LXit5ee", "review": "I'm intrigued by your comments about how the combinatorial action space (controlling many units) affects exploration, in particular when coordination is necessary for meaningful feedback. My hunch is that in a pure form, this difficulty would scale exponentially with the number of units? Do you have more insights on this?\n\nI like your solution of turning the slate-action into a (longer) sequence of single-unit actions, it encourages coordinated behavior in a very direct way. I'm just worried that it might be almost too strong as a bias toward homogeneous unit behavior, because something like forming sub-formations that execute a flanking attack is very unlikely to emerge this way? Have you observed such behavior, or could you augment your set of tasks to specifically test for non-homogeneous strategy?\n\n[edited because I did not realize I only get one question]\n\nI would also like to understand the actual algorithm better: in its current form (revision of Nov 23) it is scattered across sections 4, 6, pseudocode and appendix, with inconsistent notation (e.g. g-hat is not used in the text, R and n-bar might refer to the same? etc). The clearest I found was the normalized-reward-with-adaptive-discount formulation from the appendix: I would embrace that to simplify everything else. Also, make the pseudo-code complete by inserting all the (currently mismatched) reference equations. The use of \"u\" in the pseudo-code is also rather odd: u is a constant across the episode, so g-hat could just be computed as u * return at the final step, instead of incrementally at each time-step? Actually, I'm still confused about the ZO algorithm: I'd strongly suggest rewriting section 6 and the pseudo-code.This is a very interesting and timely paper, with multiple contributions. \n- it proposes a setup for dealing with combinatorial perception and action-spaces that generalizes to an arbitrary number of units and opponent units,\n- it establishes some deep RL baseline results on a collection of Starcraft subdomains,\n- it proposes a new algorithm that is a hybrid between black-box optimization REINFORCE, and which facilitates consistent exploration.\n\n\nAs mentioned in an earlier comment, I don\u2019t see why the \u201cgradient of the average cumulative reward\u201d is a reasonable choice, as compared to just the average reward? This over-weights late rewards at the expense of early ones, so the updates are not matching the measured objective. The authors state that they \u201cdid not observe a large difference in preliminary experiments\u201d -- so if that is the case, then why not choose the correct objective?\n\nDPQ is characterized incorrectly: despite its name, it does not \u201ccollect traces by following deterministic policies\u201d, instead it follows a stochastic behavior policy and learns off-policy about the deterministic policy. Please revise this. \n\nGradient-free optimization is also characterized incorrectly (\u201cit only scales to few parameters\u201d), recent work has shown that this can be overcome (e.g. the TORCS paper by Koutnik et al, 2013). This also suggests that your \u201cpreliminary experiments with direct exploration in the parameter space\u201d may not have followed best practices in neuroevolution? Did you try out some of the recent variants of NEAT for example, which have been applied to similar domains in the past?\n\nOn the specific results, I\u2019m wondering about the DQN transfer from m15v16 to m5v5, obtaining the best win rate of 96% in transfer, despite only reaching 13% (the worst) on the training domain? Is this a typo, or how can you explain that?", "title": "coordinated exploration, and algorithm clarifications", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1WuMnSVx": {"type": "review", "replyto": "r1LXit5ee", "review": "I'm intrigued by your comments about how the combinatorial action space (controlling many units) affects exploration, in particular when coordination is necessary for meaningful feedback. My hunch is that in a pure form, this difficulty would scale exponentially with the number of units? Do you have more insights on this?\n\nI like your solution of turning the slate-action into a (longer) sequence of single-unit actions, it encourages coordinated behavior in a very direct way. I'm just worried that it might be almost too strong as a bias toward homogeneous unit behavior, because something like forming sub-formations that execute a flanking attack is very unlikely to emerge this way? Have you observed such behavior, or could you augment your set of tasks to specifically test for non-homogeneous strategy?\n\n[edited because I did not realize I only get one question]\n\nI would also like to understand the actual algorithm better: in its current form (revision of Nov 23) it is scattered across sections 4, 6, pseudocode and appendix, with inconsistent notation (e.g. g-hat is not used in the text, R and n-bar might refer to the same? etc). The clearest I found was the normalized-reward-with-adaptive-discount formulation from the appendix: I would embrace that to simplify everything else. Also, make the pseudo-code complete by inserting all the (currently mismatched) reference equations. The use of \"u\" in the pseudo-code is also rather odd: u is a constant across the episode, so g-hat could just be computed as u * return at the final step, instead of incrementally at each time-step? Actually, I'm still confused about the ZO algorithm: I'd strongly suggest rewriting section 6 and the pseudo-code.This is a very interesting and timely paper, with multiple contributions. \n- it proposes a setup for dealing with combinatorial perception and action-spaces that generalizes to an arbitrary number of units and opponent units,\n- it establishes some deep RL baseline results on a collection of Starcraft subdomains,\n- it proposes a new algorithm that is a hybrid between black-box optimization REINFORCE, and which facilitates consistent exploration.\n\n\nAs mentioned in an earlier comment, I don\u2019t see why the \u201cgradient of the average cumulative reward\u201d is a reasonable choice, as compared to just the average reward? This over-weights late rewards at the expense of early ones, so the updates are not matching the measured objective. The authors state that they \u201cdid not observe a large difference in preliminary experiments\u201d -- so if that is the case, then why not choose the correct objective?\n\nDPQ is characterized incorrectly: despite its name, it does not \u201ccollect traces by following deterministic policies\u201d, instead it follows a stochastic behavior policy and learns off-policy about the deterministic policy. Please revise this. \n\nGradient-free optimization is also characterized incorrectly (\u201cit only scales to few parameters\u201d), recent work has shown that this can be overcome (e.g. the TORCS paper by Koutnik et al, 2013). This also suggests that your \u201cpreliminary experiments with direct exploration in the parameter space\u201d may not have followed best practices in neuroevolution? Did you try out some of the recent variants of NEAT for example, which have been applied to similar domains in the past?\n\nOn the specific results, I\u2019m wondering about the DQN transfer from m15v16 to m5v5, obtaining the best win rate of 96% in transfer, despite only reaching 13% (the worst) on the training domain? Is this a typo, or how can you explain that?", "title": "coordinated exploration, and algorithm clarifications", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}