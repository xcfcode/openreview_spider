{"paper": {"title": "Demystifying ResNet", "authors": ["Sihan Li", "Jiantao Jiao", "Yanjun Han", "Tsachy Weissman"], "authorids": ["lisihan13@mails.tsinghua.edu.cn", "jiantao@stanford.edu", "yjhan@stanford.edu", "tsachy@stanford.edu"], "summary": "", "abstract": "We provide a theoretical explanation for the superb performance of ResNet via the study of deep linear networks and some nonlinear variants. We show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depth-invariant, which makes training very deep models no more difficult than shallow ones. Shortcuts of higher depth result in an extremely flat (high-order) stationary point initially, from which the optimization algorithm is hard to escape. The 1-shortcut, however, is essentially equivalent to no shortcuts. Extensive experiments are provided accompanying our theoretical results. We show that initializing the network to small weights with 2-shortcuts achieves significantly better results than random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of deeper depth, from various perspectives ranging from final loss, learning dynamics and stability, to the behavior of the Hessian along the learning process.", "keywords": ["Deep learning", "Optimization", "Theory"]}, "meta": {"decision": "Reject", "comment": "This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance."}, "review": {"S13MXAZEg": {"type": "rebuttal", "replyto": "r1VnOs1Ql", "comment": "Dear AnonReviewer3,\n\nWe would like to add a few comments regarding why the architecture of ResNet makes the zero initial point particularly important, and why the Hessian at the zero initial point may be particularly insightful for the ResNet architecture. \n\nIndeed, as argued in ICLR submission ``Identity Matters in Deep Learning'' by Hardt and Ma, it is a simple observation to see that for any PSD matrix A, one can decompose it as the multiplication of a series of operators A_i where A_i = A^{1/n}, and the number n is the number of layers. It is clear that for very large n, the operator A_i would be very close to the identity mapping. It clearly shows that for very deep linear Residual networks, it suffices to only consider parameters that are close to zero, and one can find the global minimum there. Concretely, one just need to consider those variables matrices within each shortcut to have spectrum norm less than order 1/n. The work of Hardt and Ma extended this observation to matrices with positive determinants. \n\nWe believe this intuition should extend to the nonlinear case, and it is our ongoing work. However, our work goes deeper than this observation: this observation may suggest that the 1-shortcut network could work, but the work of He et al.'15 has already showed that 1-shortcut does not work. The experiments of Hardt and Ma also used 2-shortcuts. It is because the previous simple argument only shows that the 1-shortcut linear residual network has the capacity to approximate any linear mapping while keeping the variable matrix within each shortcut having short norm, it does not show that it is easy to optimize. Although linear 1-shortcut network has very special properties [as shown in Hardt and Ma, there is no spurious local minimum for searching around zero initial point], the failure of 1-shortcut in nonlinear Residual network as argued in He et al'15 clearly shows that this special property does not extend to the nonlinear case. Our argument on the Hessian at the initial point, however, is independent of whether the network is linear or not. We think the idea of 2-shortcut is a beautiful balance between approximation and optimization: it still maintains the property that one suffices to only consider variable matrices to have small spectral norm, and also it is easy to optimize. The property it loses compared to 1-short linear network is that there are critical points that are not global minimum, but as we argued before, this property only belongs to the 1-short linear network but not the nonlinear version, hence is not a big loss. ", "title": "More intuitions regarding looking at the Hessian at zero initial point"}, "HJLyQ0bNg": {"type": "rebuttal", "replyto": "rJRD3BVQg", "comment": "Dear AnonReviewer2,\n\nWe would like to add a few comments regarding why the architecture of ResNet makes the zero initial point particularly important, and why the Hessian at the zero initial point may be particularly insightful for the ResNet architecture. \n\nIndeed, as argued in ICLR submission ``Identity Matters in Deep Learning'' by Hardt and Ma, it is a simple observation to see that for any PSD matrix A, one can decompose it as the multiplication of a series of operators A_i where A_i = A^{1/n}, and the number n is the number of layers. It is clear that for very large n, the operator A_i would be very close to the identity mapping. It clearly shows that for very deep linear Residual networks, it suffices to only consider parameters that are close to zero, and one can find the global minimum there. Concretely, one just need to consider those variables matrices within each shortcut to have spectrum norm less than order 1/n. The work of Hardt and Ma extended this observation to matrices with positive determinants. \n\nWe believe this intuition should extend to the nonlinear case, and it is our ongoing work. However, our work goes deeper than this observation: this observation may suggest that the 1-shortcut network could work, but the work of He et al.'15 has already showed that 1-shortcut does not work. The experiments of Hardt and Ma also used 2-shortcuts. It is because the previous simple argument only shows that the 1-shortcut linear residual network has the capacity to approximate any linear mapping while keeping the variable matrix within each shortcut having short norm, it does not show that it is easy to optimize. Although linear 1-shortcut network has very special properties [as shown in Hardt and Ma, there is no spurious local minimum for searching around zero initial point], the failure of 1-shortcut in nonlinear Residual network as argued in He et al'15 clearly shows that this special property does not extend to the nonlinear case. Our argument on the Hessian at the initial point, however, is independent of whether the network is linear or not. We think the idea of 2-shortcut is a beautiful balance between approximation and optimization: it still maintains the property that one suffices to only consider variable matrices to have small spectral norm, and also it is easy to optimize. The property it loses compared to 1-short linear network is that there are critical points that are not global minimum, but as we argued before, this property only belongs to the 1-short linear network but not the nonlinear version, hence is not a big loss. ", "title": "More intuitions regarding looking at the Hessian at zero initial point"}, "rkFFjhJEe": {"type": "rebuttal", "replyto": "S12HqcyEg", "comment": "Dear AnonReviewer1,\n\nMany thanks for your review. We agree that there is much room for more work on both theoretical and practical side of our work, but we do not believe your review reflects an accurate evaluation of our contribution. Concretely,\n\n1. linear vs non-linear\n\nBoth of our Theorem 1 and figures plotted later apply equally to linear and non-linear networks [some figures for nonlinear networks are to be added in the revised version]. Linear networks are not special in our results. \n\n2. On studying the Hessian\n\nYou seem to suggest that the spectrum of the Hessian does not imply the increase or decrease of optimization difficulties in deep learning, which somehow renders it useless. Although we agree that the loss surface of a non-convex function may be extremely complicated, we still think it is of crucial value to study the spectrum of Hessian, and the less the condition number (or more precisely, the better shape the spectrum is, since we also do not consider the zero eigenvalues of the Hessian), the easier we can optimize the function. It is not only supported strongly by convex optimization theory, and is also by the fact that locally a non-convex function admits a quadratic approximation, and the shape of the quadratic function should be a crucial asset to give us insights about the optimization difficulty. \n\n3. On studying the Hessian at zero initial point\n\nIn fact, we believe that one main contribution of ResNet is that it makes the study of loss function at zero initial point more important than before. It is because now we have strong belief that one should able to find a point with decent training loss near the zero initial point, which was not the case before ResNet. For example, it was proved in the ICLR submission ``Identity Matters in Deep Learning'' that for linear networks there exist a global optimizer whose corresponding mapping in each layer has vanishing norm. It strongly motivates us to study the behavior of the loss function at the zero initial point, and gives us more confidence that we should be able to find a point with close-to-optimum training loss near the zero initial point. \n\n4. Shortcuts of different depth\n\nThe main contribution of our work is to differentiate the subtle differences caused by shortcuts of different depths. Given the empirical observation that shortcuts of length 1 and length 3 do not work, but shortcuts of length 2 work, it is intriguing to find out a theoretical justification of the difference caused only by this variation. It is exciting to us that the analysis of the loss function at the zero initial point beautifully explains this bewildering phenomenon, which was the first explanation in the literature. Crucially, our explanation does not depend on other design principles of the networks, which we view as a big advantage. We emphasize again that we do not attempt to explain why general architectures of deep learning would work [and in fact some architectures may not always work, which makes a theoretical justification nearly impossible]---it is our sole aim to justify why the shortcut approach and the specific depth 2 leads to performance improvements. \n\n5. Hessian along the training curve\n\nIndeed, due to the heavy computational burden incurred by computing the Hessian, the main computations are done on the MNIST dataset after whitening and PCA. Moreover, the Hessian along the training curve in fact depends on the specific training algorithm used, which is not our focus in this paper. However, we still clearly see interesting phenomena such as the gradual increase of condition number along the training curve, the quick escape from the initial point along the negative curvature direction, and the generally quick optimization speed for the zero initialization. Please note that this paper is not written as a heavily experimental paper---it is a paper to provide theoretical insights for better design of deep learning architectures. It is indeed our ongoing work to generalize the insights obtained from this paper to areas that the design principle of ResNet hasn't had a great influence, and our initial experiments already show very promising results. ", "title": "Reply to the review of AnonReviewer1"}, "rkhMZh4mg": {"type": "rebuttal", "replyto": "rJRD3BVQg", "comment": "Dear Reviewer2,\n\nMany thanks for your comments. We will for sure elaborate more on the proofs in the next revision, and adopt some other edits such as using \\emph instead of boldface. \n\nWe agree that we should also make the argument of 2-shortcut clearer. Indeed, as you suggested, with 1-shortcut we essentially haven't done anything unusual, with 2-shortcut we have a depth-invariant condition number, which also means that locally our loss function has a nice quadratic curvature, which is very important for gradient descent to work. For shortcuts of higher depth initially the loss function does not have any quadratic curvature, which makes the learning algorithm very hard to escape from without using higher order information. \n\nWe agree that it is improper to compare the amount of difficulty caused by A or B. The sole reason we put the sentence is that we observed the same optimization difficulties with both linear and non-linear structures, which means that linear structures can also exhibit optimization difficulties. We will revise it accordingly. \n\nYour last point is very interesting and can be decomposed into two parts. First, how does the condition number change as one moves from the initialization? It is a fantastic question that we found theory quite incapable of handling. It heavily depends on the exact network structure, the optimization algorithm one uses, and even the dataset. If one were able to analyze that, one should definitely already have a deep and clear understanding of the contributions of each element of the model to the training process, which we currently find a daunting task. Also, it may not be what we wanted: for a network with so many hand-crafted design ideas, we are really not sure which ideas are significantly contributing to the success, and which ideas are even hindering the success. It is the reason that we started to focus on specific aspect of the network, which is the local behavior of the loss function at the initialization, which is quite independent of other design elements of the network. However, it would be a fantastic work if one can clearly characterize how the condition number variation path depends on various design elements of the network such as activation functions, network topology, and learning algorithm, and draw insights on how we can improve the network. It is in fact the initial direction of our work, which we ultimately changed since we wanted to obtain something within current reach. Second, regarding the condition number at initializations. For this we have strong theoretical and empirical evidence that the Xavier, orthogonal, and He initializations all have condition numbers exploding to infinity as the number of layers tends to infinity (we have plots in the paper demonstrating that). It may sound surprising especially for orthogonal initializations, but if one digs in more carefully, one will see that orthogonal initialization only guarantees that the mapping of each layer has a nice spectrum, but what really dictates the optimization difficulty is the behavior of the whole loss function as a function of parameters to be learned. Our work clearly shows that the intuition for orthogonal initialization is not true for the loss function one wants to minimize, which we also found very surprising when we discovered it. This also shows that even if we relax a bit from the ambitious goal we set at the beginning and only analyze theoretically the initial point, some non-trivial observations can still pop up. :)\n\nBest regards,\nJiantao", "title": "Pre-review questions"}, "rJRD3BVQg": {"type": "review", "replyto": "SJAr0QFxe", "review": "Hi, \n\n Sorry for the delay in posting my question. Before a question a comment. I think you need to explain Lemma 1 and Lemma 2 better. I think is quite hard (at least it was for me) to understand what they claimed (not necessarily the proof itself). Being a bit more verbose could help. I'm also thinking that the proof should be in the main text as it is the main contribution imho of the work. But I can see space is tight for that. Also regarding writing style, I find epithets like superb maybe not necessary and I would use \\emph instead of bold face. \n\n\nRegarding the claims. I think the paper is interesting .. there are a few things though that I don't agree with. The only reasons this thing works with initialization 0, is because there are only 2 layers skipped with an identity, which results in an stable point from which you can escape. If you skip over more than 2 and initialize with 0 you wouldn't be able to learn! I think this needs to made clearer in the text. This is an unusual operation regime !\n\nSecondly I don't think the results say anything about how hard an optimization problem we have due to the nonlinearity. Anecdotal evidence is not enough to say that the main difficulty of training is not from the nonlinearity. To be honest I would not even know how to compare the amount of difficulty cause by A or B. I think that sentence is misleading. \n\nAlso I think looking at the Hessian at a fixed point might also be misleading. How does the condition number deteriorates as I move from this initialization? How fast? Also just because this number doesn't depend on depth at that point (but it does anywhere in vicinity) doesn't necessary mean that the condition number is better than the one obtain by say Xavier initialization or orthogonal initialization. Condition number for orthogonal initialization (at initialization) is also 1 and doesn't depend on depth, right?  I know there are empirical evidence .. I just I'm not sure, it feels there are a few things missing from the argument. \nI think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.", "title": "Pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJ3emeV4e": {"type": "review", "replyto": "SJAr0QFxe", "review": "Hi, \n\n Sorry for the delay in posting my question. Before a question a comment. I think you need to explain Lemma 1 and Lemma 2 better. I think is quite hard (at least it was for me) to understand what they claimed (not necessarily the proof itself). Being a bit more verbose could help. I'm also thinking that the proof should be in the main text as it is the main contribution imho of the work. But I can see space is tight for that. Also regarding writing style, I find epithets like superb maybe not necessary and I would use \\emph instead of bold face. \n\n\nRegarding the claims. I think the paper is interesting .. there are a few things though that I don't agree with. The only reasons this thing works with initialization 0, is because there are only 2 layers skipped with an identity, which results in an stable point from which you can escape. If you skip over more than 2 and initialize with 0 you wouldn't be able to learn! I think this needs to made clearer in the text. This is an unusual operation regime !\n\nSecondly I don't think the results say anything about how hard an optimization problem we have due to the nonlinearity. Anecdotal evidence is not enough to say that the main difficulty of training is not from the nonlinearity. To be honest I would not even know how to compare the amount of difficulty cause by A or B. I think that sentence is misleading. \n\nAlso I think looking at the Hessian at a fixed point might also be misleading. How does the condition number deteriorates as I move from this initialization? How fast? Also just because this number doesn't depend on depth at that point (but it does anywhere in vicinity) doesn't necessary mean that the condition number is better than the one obtain by say Xavier initialization or orthogonal initialization. Condition number for orthogonal initialization (at initialization) is also 1 and doesn't depend on depth, right?  I know there are empirical evidence .. I just I'm not sure, it feels there are a few things missing from the argument. \nI think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.", "title": "Pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJKllQX7x": {"type": "rebuttal", "replyto": "r1VnOs1Ql", "comment": "Many thanks for your comment.\n\n1. This issue has been resolved, please refer to the latest version of the paper.\n2. First, Hessian at zero is easy to calculate, but we have also calculated the Hessian along the learning curve in our experiments (Fig. 4). Also, our intuition is that in order to prevent the output variance of each layer from explosion, the mapping of each layer should be close to identity. Thus the Hessian around zero should have an important impact on the overall learning process. Lastly, now we widely believe that there are lots of stationary points in a deep network, and most of them have small losses. Thus our expectation is that there are some low-loss points not far from the initial point, so we will not go too far from the low-condition-number regime (as shown in Fig. 4). \n3. We consider one of the causes of hard learning to be that a bad initial point (where the form of Hessian is extremely biased) is hard to correct later in the optimization, both in linear case and in nonlinear case.\n\nBest regards,\nSihan", "title": "preliminary questions"}, "HyoFizX7e": {"type": "rebuttal", "replyto": "ByTuTsy7e", "comment": "Many thanks for your comment.\n\n1. We argue that nonlinearities are not the main causes of hard learning, because we believe that optimization difficulty has already occurred in linear networks, which is similar to what we have observed in nonlinear ones. We support this view by 1) Theorem 1, which applies to both linear case and nonlinear case (see the revision of the paper for the explicit expression of condition number in nonlinear case), and 2) our experiments, where the difficulty of training of linear networks is similar to that of nonlinear ones.\n2. We are preparing for the other figures, but they are close to Fig. 4. Please look forward to updates of the paper.\n\nBest regards,\nSihan", "title": "Linear vs non-linear, are they really similar?"}, "ByTuTsy7e": {"type": "review", "replyto": "SJAr0QFxe", "review": "1- Why could one conclude \"non-linearities are not the causes of hard learning\" based on a single plot on MNIST dataset?\n2- Did you try to get similar results to figure 4 for non-linear networks?ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization. ", "title": "Linear vs non-linear, are they really similar?", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S12HqcyEg": {"type": "review", "replyto": "SJAr0QFxe", "review": "1- Why could one conclude \"non-linearities are not the causes of hard learning\" based on a single plot on MNIST dataset?\n2- Did you try to get similar results to figure 4 for non-linear networks?ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization. ", "title": "Linear vs non-linear, are they really similar?", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1VnOs1Ql": {"type": "review", "replyto": "SJAr0QFxe", "review": "I have so far taken a quick pass over the paper. Some questions:\n\n1. In Theorem 1:  you showed condition number of 2-shortcut networks are depth-invariant, and gave the condition number in (13). How do the condition number behave for nonlinear activations, would it be larger or smaller than the linear case, what quantity of the activation does the condition number depends on?  I have not looked at the appendix, but it would be good to have some discussion in the main text.\n\n2. Why only study the Hessian at zero initialization? I understand your point that it is easier to escape the saddle point for n=2 in this regime. But does it have implication on how it behaves after leaving this regime (and for vast majority of the time, we probably are not in that regime)?\n\n3. You said in the end of section 2 that \"nonlinearities are not the main cause of hard learning\", then in your opinion, what are the causes?This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. \n\nThis result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:\n\n1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?\n\n2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^\\prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point.\n\n3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well?\n\nMinor: \n- sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error\n- sec 4.1 what do you mean by \"zero initialization with small random perturbations\"? why not exactly zero initialization, how large is the random perturbation?\n\n", "title": "preliminary questions", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1oRglzEg": {"type": "review", "replyto": "SJAr0QFxe", "review": "I have so far taken a quick pass over the paper. Some questions:\n\n1. In Theorem 1:  you showed condition number of 2-shortcut networks are depth-invariant, and gave the condition number in (13). How do the condition number behave for nonlinear activations, would it be larger or smaller than the linear case, what quantity of the activation does the condition number depends on?  I have not looked at the appendix, but it would be good to have some discussion in the main text.\n\n2. Why only study the Hessian at zero initialization? I understand your point that it is easier to escape the saddle point for n=2 in this regime. But does it have implication on how it behaves after leaving this regime (and for vast majority of the time, we probably are not in that regime)?\n\n3. You said in the end of section 2 that \"nonlinearities are not the main cause of hard learning\", then in your opinion, what are the causes?This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. \n\nThis result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:\n\n1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?\n\n2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^\\prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point.\n\n3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well?\n\nMinor: \n- sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error\n- sec 4.1 what do you mean by \"zero initialization with small random perturbations\"? why not exactly zero initialization, how large is the random perturbation?\n\n", "title": "preliminary questions", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkB6yqAMg": {"type": "rebuttal", "replyto": "B1Z_FMpzg", "comment": "Dear flynn,\n\nMany thanks for your comment. Please note that we are also studying deep networks with both linear and nonlinear activation functions. \n\nPlease note that the bottleneck design itself does not ease the optimization process. As mentioned in He et al. (2015), \"... the usage of bottleneck designs is mainly due to practical considerations. We further note that the degradation problem\nof plain nets is also witnessed for the bottleneck designs. \" Thus our work is not intended to explain the bottleneck structure.\n\nSecond, MSRA initialization are designed for neural networks with ReLUs. Our work only studied networks with ReLUs in the \"Learning Results\" section, as exact Hessians for them are much harder to be calculated. Moreover, the MSRA initialization for ReLUs are following the same rationale as the Xavier initialization for linear networks, and we have observed in additional experiments that similar phenomena (such as the condition number explosion as depth increases) hold for both the MSRA initialization and the Xavier initialization. \n\nFinally, notice that our model is much simpler than ResNet. It is because we intend to compute the Hessian along the learning process, but a dataset of the size of ImageNet would prohibit such computations. We do appreciate your input and it is our ongoing work to utilize the intuitions and theoretical discoveries we obtained in this work to improve the performance of state-of-the-art deep learning systems. \n\nBest regards,\nSihan", "title": "experiments"}, "B1Z_FMpzg": {"type": "rebuttal", "replyto": "rJ8F_JGMx", "comment": "In this paper,the experiments show that that initializing the network to small weights with 2-shortcuts achieves significantly better results than random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of deeper depth.But we know the resnet networks of deeper depth like 152 ,200 and all use bottleneck Residual Units,how to explain this contributed to success?on the other hand,you ignore msra,another way to initialize;finally your experiments only use mnist,it is not reasonable enough(It may be reasonable that doing some experiments on imagenet dataset ,which resnet achieves success).", "title": "experiments"}, "H14O7FfMe": {"type": "rebuttal", "replyto": "rJ8F_JGMx", "comment": "Dear Ashok,\n\nMany thanks for your comment. Please note that since the Hessian H is a symmetric matrix, the definition of condition number in our paper, $|\\lambda|_max / |\\lambda|_min$, is exactly equal to the ratio between the maximum singular value and the minimum singular value of H. It is because the max and min are defined with respect to the moduli but not the signed values. It can be easily proved using the eigenvalue decomposition of the matrix. \n\nSecond, one should note that at the zero initial point, shortcuts of length 2 not only result in a depth-invariant condition number, but also a nice spectrum. This spectrum is really what one needs to escape from this saddle point. Precisely, it is a strict saddle point that is easy to escape from. \n\nmany thanks,\nJiantao", "title": "Definition of condition number"}, "rJ8F_JGMx": {"type": "rebuttal", "replyto": "SJAr0QFxe", "comment": "Hi Authors,\n\nI went through your paper in detail and it was very good linking the Hessian at the zero initial point to the success of the ResNet. However, I am a little unsure of the definition of the condition number in the paper. Across several standard refernces, the condition number is defined as the ratio the maximum and minimum singular values where as you defined it as $ |\\lambda|_max / |\\lambda|_min  $.  Unless the matrix, which is the Hessian H in your case, is positive semi-definite, these two are not the same and the spectra of the singular values and the eigen values can be quite different in general. Especially, for n=2, the initial point zero is a saddle point as stated in your theorem and as such the Hessian is not positive-definite. Please let me know if I am missing out on something and whether or not this modification affects the analysis in the paper. Thanks!", "title": "Definition of condition number"}}}