{"paper": {"title": "Semi-supervised deep learning by metric embedding", "authors": ["Elad Hoffer", "Nir Ailon"], "authorids": ["ehoffer@tx.technion.ac.il", "nailon@cs.technion.ac.il"], "summary": "", "abstract": "Deep networks are successfully used as classification models yielding state-of-the-art results when trained on a large number of labeled samples. These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data. In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data. This criterion is based on a deep metric embedding over distance relations within the set of labeled samples, together with constraints over the embeddings of the unlabeled set. The final learned representations are discriminative in euclidean space, and hence can be used with subsequent nearest-neighbor classification using the labeled samples.", "keywords": ["Deep learning", "Semi-Supervised Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "This paper studies semi-supervised learning by combining ideas from metric embedding and entropy. The model attempts to embed samples with visible labels into corresponding clusters, while embedding samples without labels close to some of the cluster centroids. Strong numerical results are reported on small-scale semi-supervised image tasks. \n \n The reviewers acknowledged the effectiveness of the method and the strong numerical results, but expressed concerns about the lack of originality with respect to previous works. \n \n The AC found the work pleasantly well presented and strikingly simple, yet with excellent numerical performance. These two qualities make this work actually refreshing relative to the ever-increasing complexity of current deep learning architectures. In particular, this seems to be the *proper* way to perform semi-supervised learning. \n However, the model appears extremely similar to [Grandvalet & Bengio, '04], not just in the entropy-minimization regularization for unlabeled examples, but also for the cross-entropy minimization of labeled examples and the semi-supervised objective. The only difference I perceive is that [G & B '04] present a generic recognition model p( z | x), whereas the present submission uses a classification layer of the form \n (1) p(z | x ) = softmax( -|| Phi(x) - Phi(y_k) ||^2 ), \n where the y_k are randomly chosen examples of class respectively k. p(z | x) is thus random relative to the choice of 'anchors' y_k. \n Thus, the present model is rather a particular case, unless I misunderstood. In that case, the main question is to understand how much the excellent numerical performance relies on this particular choice of classification layer. The numerical section should consider a baseline where (1) is replaced by a 'standard' classification softmax of the form p(z | x ) = softmax( < v_k, Phi(x) > ), where v_k are class-specific parameters. \n \n For these reasons,we recommend inviting this submission to the workshop track, and we encourage the authors to address the previous remark."}, "review": {"BkOPbNzIe": {"type": "rebuttal", "replyto": "S1ZMS80Qx", "comment": "Thank you for the review, we appreciate your remarks and incorporated them into latest revision.\nAs you point out, the objective introduced in this work corresponds to a sum of distance ratio loss (Hoffer 2015) and entropy minimization (Grandvalet 2004). While these are not new, this is the first attempt to our knowledge, of combining these ideas for semi-supervised metric learning. We will add an additional information regarding the differences from these earlier works.\nSzegedy 2015 uses targets that are \u201csoftened\u201d by weighted average between the true-target, and a random sample from a noise distribution u(k) (uniform in the paper). As these are sampled separately for each iteration, this corresponds to random label noise. We will add an additional explanation in our paper for this fact.\nThe choices for \\lambda_{1,2} and k-NN parameter were made using a validation set. We did not found any substantial difference between the values we explored, so they were usually left as the default value.\nWe will add an additional figure depicting the labels at the start of training as you suggested. We will also try to shed some light on the interaction between the losses used.", "title": "comment"}, "B1LB-VzLg": {"type": "rebuttal", "replyto": "ByfUT5b4x", "comment": "Thank you for the review, we appreciate your remarks and incorporated them into latest revision.\nAs you point out, this work is based on the method of Hoffer and Ailon (2015) of metric learning using embedded distance ratios. However, this work is the first, as far as we know, to leverage this method to learn in a semi-supervised regime. This is done by using an additional entropy minimization objective over the distribution of embedded distance ratios. \nAn additional discussion will be added to the paper to try and address when this method will break down.", "title": "comment"}, "rynM-Vz8x": {"type": "rebuttal", "replyto": "rkAXNP7Ex", "comment": "Thank you for the review, we appreciate your remarks and incorporated them into latest revision.\nSpecifically, we wish to address your concerns for novelty of this work. Comparing with the works of Hadsell and Weston, this work uses a novel objective which is composed of a distance ratio measure (unlike the contrastive, hinge based loss used before), and an entropy minimization on the distance measure to labeled samples. Although distance ratio loss (Hoffer 2015) and entropy minimization (Grandvalet 2004) are not new, this is the first attempt to our knowledge, of combining these ideas for semi-supervised metric learning. Furthermore, we would argue that the interaction between these criterions is a form of balancing (which can be weighted), that corresponds with your observation for the need of a balancing constraint.\nWe would also like to point that the neural network models themselves are very simple and the baseline achieved by training them with the standard cross-entropy has significantly lower accuracy. Therefore, we are certain that the high performance achieved is due to the proposed objective, and not the network architecture.", "title": "comment"}, "Sk_SV6cml": {"type": "rebuttal", "replyto": "BJzbomkQg", "comment": "Thank you for you question.\nThe proposed objective can be affected by class imbalanced data set, which we believe can be mitigated by proportional weighting of the loss function. Further investigations are needed to quantify the affect of such phenomena.", "title": "Answer"}}}