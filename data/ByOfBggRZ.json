{"paper": {"title": "Detecting Statistical Interactions from Neural Network Weights", "authors": ["Michael Tsang", "Dehua Cheng", "Yan Liu"], "authorids": ["tsangm@usc.edu", "dehuache@usc.edu", "yanliu.cs@usc.edu"], "summary": "We detect statistical interactions captured by a feedforward multilayer neural network by directly interpreting its learned weights.", "abstract": "Interpreting neural networks is a crucial and challenging task in machine learning. In this paper, we develop a novel framework for detecting statistical interactions captured by a feedforward multilayer neural network by directly interpreting its learned weights. Depending on the desired interactions, our method can achieve significantly better or similar interaction detection performance compared to the state-of-the-art without searching an exponential solution space of possible interactions. We obtain this accuracy and efficiency by observing that interactions between input features are created by the non-additive effect of nonlinear activation functions, and that interacting paths are encoded in weight matrices. We demonstrate the performance of our method and the importance of discovered interactions via experimental results on both synthetic datasets and real-world application datasets. ", "keywords": ["statistical interaction detection", "multilayer perceptron", "generalized additive model"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a way of detecting statistical interactions in a dataset based on the weights learned by a DNN. The idea is interesting and quite useful as is showcased in the experiments. The reviewers feel that the paper is also quite well written and easy to follow."}, "review": {"SyQM6W5gz": {"type": "review", "replyto": "ByOfBggRZ", "review": "This paper presents a method to identify high-order interactions from the weights of feedforward neural networks.  \n\nThe main benefits of the method are:\n1)\tCan detect high order interactions and there\u2019s no need to specify the order (unlike, for example, in lasso-based methods).\n2)\tCan detect interactions appearing inside of non-linear function (e.g. sin(x1 * x2))\n\nThe method is interesting, in particular if benefit #2 holds experimentally. Unfortunately, there are too many gaps in the experimental evaluation of this paper to warrant this claim right now.\n\nMajor:\n\n1)\tArguably, point 1 is not a particularly interesting setting. The order of the interactions tested is mainly driven by the sample size of the dataset considered, so in some sense the inability to restrict the order of the interaction found can actually be a problem in real settings. \nBecause of this, it would be very helpful to separate the evaluation of benefit 1 and 2 at least in the simulation setting. For example, simulate a synthetic function with no interactions appearing in non-linearities (e.g. x1+x2x3x4+x4x6) and evaluate the different methods at different sample sizes (e.g. 100 samples to 1e5 samples). The proposed method might show high type-1 error under this setting. Do the same for the synthetic functions already in the paper. By the way, what is the sample size of the current set of synthetic experiments?\n2)\tThe authors claim that the proposed method identifies interactions \u201cwithout searching an exponential solution space of possible interactions\u201d. This is misleading, because the search of the exponential space of interactions happens during training by moving around in the latent space identified by the intermediate layers. It could perhaps be rephrased as \u201cefficiently\u201d.\n3)\tIt\u2019s not clear from the text whether ANOVA and HierLasso are only looking for second order interactions. If so, why not include a lasso with n-order interactions as a baseline?\n4)\tWhy aren\u2019t the baselines evaluated on the real datasets and heatmaps similar to figure 5 are produced?\n5)\tIs it possible to include the ROC curves corresponding to table 2?\n\n\nMinor:\n\n1)\tHave the authors thought about statistical testing in this framework? The proposed method only gives a ranking of possible interactions, but does not give p-values or similar (e.g. FDRs).\n2)\t12 pages of text. Text is often repetitive and can be shortened without loss of understanding or reproducibility.\n", "title": "Interesting method, more work to do on the experimental side", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hy8XC6Kxf": {"type": "review", "replyto": "ByOfBggRZ", "review": "This paper develops a novel method to use a neural network to infer statistical interactions between input variables without assuming any explicit interaction form or order. First the paper describes that an 'interaction strength' would be captured through a simple multiplication of the aggregated weight and the weights of the first hidden layers. Then, two simple networks for the main and interaction effects are modeled separately, and learned jointly with posing L1-regularization only on the interaction part to cancel out the main effect as much as possible. The automatic cutoff determination is also proposed by using a GAM fitting based on these two networks. A nice series of experimental validations demonstrate the various types of interactions can be detected, while it also fairly clarifies the limitations.\n\nIn addition to the related work mentioned in the manuscript, interaction detection is also originated from so-called AID, literally intended for 'automatic interaction detector' (Morgan & Sonquist, 1963), which is also the origin of CHAID and CART, thus the tree-based methods like Additive Groves would be the one of main methods for this. But given the flexibility of function representations, the use of neural networks would be worth rethinking, and this work would give one clear example.\n\nI liked the overall ideas which is clean and simple, but also found several points still confusing and unclear.\n\n1) One of the keys behind this method is the architecture described in 4.1. But this part sounds quite heuristic, and it is unclear to me how this can affect to the facts such as Theorem 4 and Algorithm 1. Absorbing the main effect is not critical to these facts? In a standard sense of statistics, interaction would be something like residuals after removing the main (additive) effect. (like a standard test by a likelihood ratio test for models with vs without interactions)\n\n2) the description about the neural network for the main effect is a bit unclear. For example, what does exactly mean the 'networks with univariate inputs for each input variable'? Is my guessing that it is a 1-10-10-10-1 network (in the experiments) correct...? Also, do g_i and g_i' in the GAM model (sec 4.3) correspond to the two networks for the main and interaction effects respectively?\n\n3) mu is finally fixed at min function, and I'm not sure why this is abstracted throughout the manuscript. Is it for considering the requirements for any possible criteria?\n\nPros:\n- detecting (any order / any form of) statistical interactions by neural networks is provided.\n- nice experimental setup and evaluations with comparisons to relevant baselines by ANOVA, HierLasso, and Additive Groves.\n\nCons:\n- some parts of explanations to support the idea has unclear relationship to what was actually done, in particular, for how to cancel out the main effect.\n- the neural network architecture with L1 regularization is a bit heuristic, and I'm not surely confident that this architecture can capture only the interaction effect by cancelling out the main effect.\n\n", "title": "An interesting use case of neural networks for a traditional statistical problem!", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hy9gtVoxz": {"type": "review", "replyto": "ByOfBggRZ", "review": "Based on a hierarchical hereditary assumption, this paper identifies pairwise and high-order feature interactions by re-interpreting neural network weights, assuming higher-order interactions exist only if all its induced lower-order interactions exist. Using a multiplication of the absolute values of all neural network weight matrices on top of the first hidden layer, this paper defines the aggregated strength z_r of each hidden unit r contributing to the final target output y. Multiplying z_r by some statistics of weights connecting a subset of input features to r and summing over r results in final interaction strength of each feature interaction subsets, with feature interaction order equal to the size of each feature subset. \n\nMain issues:\n\n1. Aggregating neural network weights to identify feature interactions is very interesting. However, completely ignoring \nactivation functions makes the method quite crude. \n\n2. High-order interacting features must share some common hidden unit somewhere in a hidden layer within a deep neural network. Restricting to the first hidden layer in Algorithm 1 inevitably misses some important feature interactions.\n\n3. The neural network weights heavily depends on the l1-regularized neural network training, but a group lasso penalty makes much more sense. See Group Sparse Regularization for Deep Neural Networks (https://arxiv.org/pdf/1607.00485.pdf).\n\n4. The experiments are only conducted on some synthetic datasets with very small feature dimensionality p. Large-scale experiments are needed.\n\n5. There are some important references missing. For example, RuleFit is a good baseline method for identifying feature interactions based on random forest and l1-logistic regression (Friedman and Popescu, 2005, Predictive learning via rule ensembles); Relaxing strict hierarchical hereditary constraints, high-order l1-logistic regression based on tree-structured feature expansion identifies pairwise and high-order multiplicative feature interactions (Min et al. 2014, Interpretable Sparse High-Order Boltzmann Machines); Without any hereditary constraint, feature interaction matrix factorization with l1 regularization identifies pairwise feature interactions on datasets with high-dimensional features (Purushotham et al. 2014, Factorized Sparse Learning Models with Interpretable High Order Feature Interactions). \n\n6. At least, RuleFit (Random Forest regression for getting rules + l1-regularized regression) should be used as a baseline in the experiments.\n\nMinor issues:\n\nRanking of feature interactions in Algorithm 1 should be explained in more details.\n\nOn page 3: b^{(l)} \\in R^{p_l}, l should be from 1, .., L. You have b^y.\n\n\nIn summary, the idea of using neural networks for screening pairwise and high-order feature interactions is novel, significant, and interesting.  However, I strongly encourage the authors to perform additional experiments with careful experiment design to address some common concerns in the reviews/comments for the acceptance of this paper.\n \n========\nThe additional experimental results are convincing, so I updated my rating score.\n  ", "title": "Feature interaction identification by multiplying |neural network weight matrices|", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HJf2U17mz": {"type": "rebuttal", "replyto": "Hy9gtVoxz", "comment": "5 & 6. >> On the remaining baseline references\n\nWe have added references for the remaining baselines as you suggested (on Page 2 and in Appendix H). The references are Interpretable Sparse High-Order Boltzmann Machines, Min et al. 2014 and Factorized Sparse Learning Models with Interpretable High Order Feature Interactions, Purushotham et al. 2014. \n\nWe performed experiments comparing our method to the \"Shooter\" and \"FHIM\" baselines from the references. The details and results of our experiments are shown in Appendix H.  We found that indeed, the Shooter baseline benefits from relaxing strict hierarchical hereditary constraints ", "title": "rebuttal (cont'd)"}, "BkR-FYvzf": {"type": "rebuttal", "replyto": "rJZYeXMzG", "comment": "Thank you for clarifying your suggestion on group lasso. We have conducted experiments on group lasso as you suggested. After tuning regularization strengths, we found that indeed the average AUC of group lasso with input groups is slightly better than vanilla lasso, but the difference is not statistically significant. The details of additional experiments with group lasso are included in Appendix G.\n\nWe also plan to share our code in the future so that readers can replicate the experiments.", "title": "authors' response"}, "S1xmKLxzM": {"type": "rebuttal", "replyto": "Hy8XC6Kxf", "comment": "Thank you for your comments and suggestions. \n\n>> \u201cit is unclear to me how this can affect to the facts such as Theorem 4 and Algorithm 1.\u201d\nThe existence of the univariate networks does not affect Algorithm 1 or Theorem 4. The univariate networks are meant to reduce the modeling of spurious interactions in the main fully-connected network to improve interaction detection performance. \n\n>>  On the architecture of the univariate and GAM networks\nYour understanding is correct.  \n\n>> Regarding the abstraction of mu, \nWe did not define mu until later in the paper because mu was determined by experiments", "title": "authors' response"}, "Bk7UD8gMf": {"type": "rebuttal", "replyto": "Hy9gtVoxz", "comment": "Thank you for your comments and suggestions to improve the paper. Below are our responses to the main points of your comments:  \n\n1. >> \u201cHowever, completely ignoring activation functions makes the method quite crude.\u201d\nOur approach to aggregating weight matrices depends on the activation functions in two ways: 1) the use of matrix multiplications is based on Lemma 3, which depends on the activation functions being 1-Lipschitz, and 2) the averaging of weights is empirically determined from neural networks with ReLU activation. \n\n2. >> \u201cRestricting to the first hidden layer in Algorithm 1 inevitably misses some important feature interactions.\u201d\nThis is an interesting point. We did consider it before. However, it is not straightforward how to incorporate the idea of common hidden units at intermediate layers to get better interaction detection performance. Our previous studies show that naively using the intermediate hidden layers to suggest new interactions have resulted in worse performance in interaction detection because the connections between input features and intermediate layers are not direct. \n\n3. >> \u201ca group lasso penalty makes much more sense\u201d \nIn general, group lasso requires specifying groupings a priori. It is unclear how to tailor the group lasso penalty to discover interactions, but group lasso might offer an alternative way of finding a cutoff on interaction rankings. \n\n4. >> \u201cLarge-scale experiments are needed.\u201d\nWe have conducted experiments with large scale p (p=1000, 950 pairwise interactions) as you suggested and obtained a pairwise interaction strength AUC of 0.984. The full experimental setting can be found in our updated paper in Appendix F, which follows Purushotham et al. 2014 on how to generate large p noisy data.\n\n5 & 6. >> \u201c, RuleFit should be used as a baseline in the experiments.\u201d\nWe have added experiments with RuleFit into Table 2 as you suggested. Our approach outperforms RuleFit. This is consistent with previous work by Lou et al. 2013, \u201cAccurate and Intelligible Models with Pairwise Interactions\u201d, which found that RuleFit did not perform better than Additive Groves, our main baseline.\n", "title": "rebuttal"}, "ry4GOIgGM": {"type": "rebuttal", "replyto": "SyQM6W5gz", "comment": "Thank you for your comments and suggestions. We conducted some experiments based on your suggestions and provide our responses to your major points below. We have also included additional results in Appendices E and F.   \n   \n1) Our proposed method strongly relies on the assumption that the neural network fit well to data, since we are extracting interactions from the learned network weights. The number of data points available plays a critical role here because a small amount of data can cause the neural network to overfit, causing our method to miss true interactions and find spurious ones instead. To avoid this scenario, we employed modern tricks to help our neural network fit well to the data (e.g. early stopping, regularization).  That being said, we advise against using our framework when the number of data samples, n, is too small for normal neural networks, e.g., when n < p, where p is the number of features. Under such scenarios, one might need to impose much stronger assumptions on the data, which goes against our proposal of a general interaction detection algorithm.\n\nFor assurance, we conducted the experiments that you requested and confirmed that our approach does well on the multiplicative synthetic function x1+x2x3x4+x4x6 for datasets of sizes [1e2, 1e3, 1e4, 1e5], obtaining average interaction ranking AUCs of [0.99, 1.0, 1.0, 1.0], respectively. The average AUCs for our 10 synthetic functions with nonlinearities (combined) are [0.57,0.83,0.92,0.94] respectively. The baseline methods are specified to find multiplicative interactions, so their AUC is 1 for the multiplicative synthetic function. Note that we can only obtain interactions accurately when there is enough data to train the model, as seen in the improving scores with more data samples.  In our synthetic experiments, we used 10k training samples (and 10k valid/10k test), and this has been updated in our paper. We also updated our paper with a large-p experiment on multiplicative interactions in Appendix F, where we obtained an AUC of 0.98.\n\n2) While the neural network is technically searching interactions during training, the cost of this implicit exponential search is faster than an explicit exponential search of the space of interaction candidates. Our work avoids this explicit search.\n\n3) We originally did not include higher-order detection experiments with ANOVA and Lasso because they are mis-specified to handle detecting the general non-additive form of interactions. For assurance, we ran experiments on ANOVA and Lasso and got average top-rank recall scores of 0.47 and 0.44 respectively, which are much lower than the 0.65 average obtained by our approach.\n\n", "title": "rebuttal"}}}