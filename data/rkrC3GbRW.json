{"paper": {"title": "Learning a Generative Model for Validity in Complex Discrete Structures", "authors": ["Dave Janz", "Jos van der Westhuizen", "Brooks Paige", "Matt Kusner", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"], "authorids": ["david.janz93@gmail.com", "josvdwest@gmail.com", "tbpaige@gmail.com", "matt.kusner@gmail.com", "jmh233@cam.ac.uk"], "summary": "", "abstract": "Deep generative models have been successfully used to learn representations for high-dimensional discrete spaces by representing discrete objects as sequences and employing powerful sequence-based deep models. Unfortunately, these sequence-based models often produce invalid sequences: sequences which do not represent any underlying discrete structure; invalid sequences hinder the utility of such models. As a step towards solving this problem, we propose to learn a deep recurrent validator model, which can estimate whether a partial sequence can function as the beginning of a full, valid sequence. This validator provides insight as to how individual sequence elements influence the validity of the overall sequence, and can be used to constrain sequence based models to generate valid sequences \u2014 and thus faithfully model discrete objects. Our approach is inspired by reinforcement learning, where an oracle which can evaluate validity of complete sequences provides a sparse reward signal. We demonstrate its effectiveness as a generative model of Python 3 source code for mathematical expressions, and in improving the ability of a variational autoencoder trained on SMILES strings to decode valid molecular structures.", "keywords": ["Active learning", "Reinforcement learning", "Molecules"]}, "meta": {"decision": "Accept (Poster)", "comment": "Viewing the problem of determining the validity of high-dimensional discrete sequences as a sequential decision problem, the authors propose learning a Q function that indicates whether the current sequence prefix can lead to a valid sequence. The paper is fairly well written and contains several interesting ideas. The experimental results appear promising but would be considerably more informative if more baselines were included. In particular, it would be good to compare the proposed approach (both conceptually and empirically) to learning a generative model of sequences. Also, given that your method is based on learning a Q function, you need to explain its exact relationship to classic Q-learning, which would also make for a good baseline."}, "review": {"SJzxBpKeM": {"type": "review", "replyto": "rkrC3GbRW", "review": "SUMMARY:\nThis work is about learning the validity of a sequences in specific application domains like SMILES strings for chemical compounds. In particular, the main emphasis is on predicting if a prefix sequence could possibly be extended to a complete valid sequence. In other words, one tries to predict if there exists a valid suffix sequence, and based on these predictions, the goal is to train a generative model that always produces valid sequences.  In the proposed reinforcement learning setting, a neural network models the probability that a certain action (adding a symbol) will result in a valid full sequence. For training the network, a large set of (validity-)labelled sequences would be needed. To overcome this problem, the authors introduce an active learning strategy, where the information gain is re-expressed as the conditional mutual information between the the label y and the network weights w, and this mutual information is maximized in a greedy sequential manner.    \nEVALUATION:\nCLARITY & NOVELTY: In principle, the paper is easy to read. Unfortunately, however, for the reader is is not easy to find out what the authors consider their most relevant contribution. Every single part of the model seems to be quite standard (basically a network that predicts the probability of a valid sequence and an information-gain based active learning strategy) - so is the specific application to SMILES strings what makes the difference here?   Or is is the specific greedy approximation to the mutual information criterion in the active learning part? Or is it the way how you augment the dataset? All these aspects might be interesting, but somehow I am missing a coherent picture.\nSIGNIFICANCE: it is not entirely clear to me if the proposed \"pruning\" strategy for the completion of prefix sequences can indeed be generally applied to sequence modelling problems, because in more general domains it might be very difficult to come up with reasonable validity estimates for prefixes that are significantly shorter than the whole sequence. I am not so familiar with SMILES strings -- but could it be that the experimental success reported here is mainly a result of the very specific structure of valid SMILES strings?  But then, what can be learned for general sequence validation problems?\n     \nUPDATE: Honestly, outside the scope of SMILES strings, I still have some concerns regarding reasonable validity estimates for prefixes that are significantly shorter than the whole sequence...  \n\n", "title": "An interesting paper about a truly relevant problem. The proposed model seems to work well for SMILES strings representing moleculs, but its general applicability is a bit unclear to me. Further, I am not fully convinced about the novelty of the approach taken.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1bjT3VgM": {"type": "review", "replyto": "rkrC3GbRW", "review": "The authors use a recurrent neural network to build generative models of sequences in domains where the vast majority of sequences is invalid. The basic idea, outlined in Eq. 2, is moderately straightforward: at each step, use an approximation of the Q function for subsequences of the appropriate length to pick a valid extension. There are numerous details to get right. The writing is mostly clear, and the examples are moderately convincing. I wish the paper had more detailed arguments and discussions.\n\nI question the appropriateness of Eq. 2 as a target. A correctly learned model will put positive weight on valid sequences, but it may be an arbitrarily slow way to generate diverse sequences, depending on the domain. For instance, imagine a domain of binary strings where the valid sequences are the all 1 sequence, or any sequence beginning with a 0. Half the generated sequences would be all 1's in this situation, right? And it's easy to construct further examples that are much worse than this?\n\nThe use of Bayesian active learning to generate the training set feels like an elegant idea. However, I wish there were more clarity about what was ad hoc and what wasn't. For instance, I think the use of  dropout to get q is suspect (see for instance https://arxiv.org/abs/1711.02989), and I'd prefer a little more detail on statements like \"The nonlinearity of g(\u00b7) means that our Monte\nCarlo approximation is biased, but still consistent.\" Do we have any way of quantifying the bias? Is the statement about K=16 being reasonable a statement about bias, variance, or both?\n\nFor Python strings: \n- Should we view the fact that high values of tau give a validity of 1.0 as indicative that the domain's constraints are fairly easy to learn?\n- \"The use of a Boltzmann policy allows us to tune the temperature parameter to identify policies\nwhich hit high levels of accuracy for any learned Q-function approximation.\" This is only true to the extent the domain is sufficiently \"easy\" right? Is the argument that even in very hard domains, you might get this by just having an RNN which memorized a single valid sequence (assuming at least one could be found)?\n- What's the best explanation for *why* the active model has much higher diversity? I understand that the active model is picking examples that tell us more about the uncertainty in w, but it's not obvious to me that means higher diversity. Do we think this is a universal property of domains?\n- The highest temperature active model is exploring about half of valid sequences (modulo the non-tightness of the bound)? Have you tried gaining some insight by generating thousands of valid sequences manually and seeing which ones the model is rejecting?\n- The coverage bound is used only for for Python expressions, right? Why not just randomly sample a few thousand positives and use that to get a better estimate of coverage? Since you can sample from the true positive set, it seems that your argument from the appendix about the validation set being \"too similar to the training set\" doesn't apply?\n- It would be better to see a comparison to a strong non-NN baseline. For instance, I could easily make a PCFG over Python math expressions, and use rejection sampling to get rid of those that aren't exactly length 25, etc.?\n\nI question how easy the Python strings example is. In particular, it might be that it's quite an easy example (compared to the SMILES) example. For SMILES, it seems like the Bayesian active learning technique is not by itself sufficient to create a good model? It is interesting that in the solubility domain the active model outperforms, but it would be nice to see more discussion / explanation.\n\nMinor note: The incidence of valid strings in the Python expressions domain is (I believe) > 1/5000, although I guess 1 in 10,000 is still the right order of magnitude.\n\nIf I could score between \"marginal accept\" and \"accept\" I would. ", "title": "Interesting paper, raises as many questions as it answers.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1odDD8gM": {"type": "review", "replyto": "rkrC3GbRW", "review": "Overall: Authors casted discrete structure generation as a planning task and they used Q-learning + RNNs to solve for an optimal policy to generate valid sequences. They used RNN for sequential state representation and Q-learning for encoding expected value of sub-actions across trajectory - constraining each step's action to valid subsequences that could reach a final sequence with positive reward (valid whole sequences).\n\nEvaluation: The approach centers around fitting a Q function with an oracle that validates sub-sequences. The Q function is supported by a sequence model for state representation. Though the approach seems novel and well crafted, the experiments and results can't inform me which part of the modeling was critical to the results, e.g. was it the (1) LSTM, (2) Q-function fitting? Are there other simpler baseline approaches to compare against the proposed method? Was RL really necessary for the planning task? The lack of a baseline approach for comparison makes it hard to judge both results on Python Expressions and SMILES. The Python table gives me a sense that the active learning training data generation approach provides competitive validity scores with increased discrete space coverage. However the SMILES data set is a little mixed for active vs passive - authors should try to shed some light into that as well.\n\nIn conclusion, the approach seems novel and seem to fit well with the RL planning framework. But the lack of baseline results make it hard to judge significance of the work.", "title": "Novel Approach to Generate Discrete Structure using RL", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1frSrpXf": {"type": "rebuttal", "replyto": "r1bjT3VgM", "comment": "While the proposed target distribution is not uniform over $\\mathcal{X}_+$, it has the following advantages:\n\n(a) It functions as an indicator of validity, giving zero probability mass to invalid sequences.\n(b) It can be combined with generative models trained on real-world data which do not generate uniform samples. The proposed method can then be used to eliminate, at each step during the sequence generation in such models, those next actions (characters) that will lead to invalid sequences, improving the validity of the sequences generated.\n(c) It is invariant to changes in the training data distribution (active learning strategy).\n(d) It can handle padded sequences with no extra effort.\n(d) It is numerically stable -- with the output at each step being in the range [0, 1], rather than perhaps the ratio of sequences with that prefix that can lead to valid sequences, which tends to 0 with increasing sequence length and has typical scale that varies with step t.\n\nWe also considered and tested as target a distribution that would be uniform over all sequences if trained with data distributed uniformly from $\\mathcal{X}$. We found however that: (a) this only held for fixed length sequences and was not appropriate for padded sequences; (b) the requirement of uniform data from $\\mathcal{X}$ prevented us from using active learning or any already existing data and; (c) the resulting method suffered from severe numerical/optimisation issues.\n", "title": "Appropriateness of eq. (2) as a target"}, "BkYQBH67f": {"type": "rebuttal", "replyto": "r1bjT3VgM", "comment": "The paper https://arxiv.org/abs/1711.02989 refers to variational Gaussian dropout. We use Bernoulli dropout, which is a theoretically-grounded way of obtaining uncertainty estimates in neural networks. This method has already been used to obtain uncertainty estimates in Bayesian neural networks in several previous works:\n\nhttps://arxiv.org/abs/1506.02142\nhttps://arxiv.org/abs/1512.05287\nhttps://arxiv.org/abs/1703.02910\n", "title": "Regarding the use of dropout"}, "SyvGSB6Xz": {"type": "rebuttal", "replyto": "r1bjT3VgM", "comment": "We have investigated the quality of the biased Monte Carlo information gain estimator. For active learning, the bias would only matter if it affects the relative ordering of different choices. The bias here preserves ordering. That is, if info_gain(x_1) > info_gain(x_2) then E[ info_gain_MC(x_1)] > E[info_gain_MC(x_2)]. K=16 was a statement regarding variance \u2013 note that some variance isn\u2019t much of an issue for us, after all we are intentionally \u2018injecting\u2019 noise at the Boltzmann sampling stage, in order to obtain diverse samples.", "title": "Regarding the bias in the Monte Carlo approximation"}, "SJHWHrT7z": {"type": "rebuttal", "replyto": "r1bjT3VgM", "comment": "We have updated and extended our SMILES experiments. We now provide comparisons of our work with a state-of-art context-free grammar based approach [Kusner et al. (2017)].\n\nFor python expressions, the active model sees a lot more valid sequences during training, and thus gets better at modelling a large range of those. The passive one doesn\u2019t see as many examples of valid sequences, and so doesn\u2019t learn their general properties as well. Looking at the generated data, both methods struggle with correlated changes like brackets. One possible fix is to use variable length sequences to learn the usage of brackets from shorter sequences, which can then be generalised to longer sequences.\n\nAbout the claim regarding Boltzmann sampling being used to generate high validity samples at low enough temperatures, indeed, we mean that it can just generate the same one valid sequence with no sequence diversity. Tau at 1.0 validity could perhaps give an indication of the difficulty of the problem domain.\n", "title": "Regarding Experiments"}, "SypyHBT7z": {"type": "rebuttal", "replyto": "r1bjT3VgM", "comment": "In our new SMILES experiments, instead of the active learning we propose a data augmentation strategy which generates informative negative samples. Table 4 shows that this strategy allows us to outperform previous state-of-the-art results [Kusner et al. (2017)].\n\nThe reason for not using our active learning strategy with SMILES is because, while it does learn to discover strings that are technically valid, they are not chemically-realistic. This is why our initial active learning results in the SMILES domain were mixed. By instead augmenting an existing set of realistic molecules, we are able to more efficiently explore the space of realistic SMILES strings.\n", "title": "Regarding the worse performance of active learning with SMILES"}, "HJ4T4Spmz": {"type": "rebuttal", "replyto": "B1odDD8gM", "comment": "Our main contribution is the formulation of the problem as learning a Q function. To learn this function, however, we need informative data. For Python strings, where no positive data is available, we propose an active learning strategy to learn efficiently. For SMILES, where existing positive data is available, we propose a data augmentation strategy which allows us to obtain informative negative samples. We chose to describe our Q function with a recurrent neural network (LSTM), but any other similar model (GRU) could have been used as well.\n", "title": "Clarifying most relevant contribution"}, "H1HcVrTmM": {"type": "rebuttal", "replyto": "B1odDD8gM", "comment": "We have updated and extended our SMILES experiments. We now provide comparisons of our work with a state-of-art context-free grammar based approach [Kusner et al. (2017)]. This more clearly demonstrates the significance of our contribution.\n", "title": "Regarding experiments/baseline approaches"}, "H1iONBaXG": {"type": "rebuttal", "replyto": "B1odDD8gM", "comment": "The reason for not using our active learning strategy with SMILES is because while it does learn to discover strings that are technically valid, they are not chemically-realistic. This is why our initial active learning results in the SMILES domain were mixed. By instead augmenting an existing set of realistic molecules, we are able to more efficiently explore the space of realistic SMILES strings.\n\nIn our new SMILES experiments, instead of the active learning we propose a data augmentation strategy which generates informative negative samples. Table 4 shows that this strategy allows us to outperform previous state-of-the-art results [Kusner et al. (2017)].\n", "title": "SMILES results mixed for active vs passive "}, "Hy_SEST7G": {"type": "rebuttal", "replyto": "SJzxBpKeM", "comment": "The proposed approach is applicable to any sequence validity problem in which our Q function is learnable from data. We considered the problems of learning the validity of python expressions and SMILES sequences because these are relatively simple problems that are also useful in practice and challenging for existing methods.", "title": "Is the specific application to SMILES strings what makes the difference?"}, "S1C74HpXf": {"type": "rebuttal", "replyto": "SJzxBpKeM", "comment": "Solving the validity learning problem in arbitrary domains can at worst be highly intractable. We believe practical solutions are only available when the validity rules are simple enough to be learned from data. Our approach is able to learn validity models in two very different domains, Python expressions and SMILES strings, demonstrating its capacity for generalization.  \n\nThe proposed active learning method is expected to be more beneficial in domains where shorter sequences demonstrate rules of validity that also apply to longer strings \u2014 that is, the nature of the governing validity rules does not change a great deal as sequences get longer.", "title": "Significance and generalizability"}, "H1U0mSp7M": {"type": "rebuttal", "replyto": "SJzxBpKeM", "comment": "Our main contribution is the formulation of the problem as learning a Q function. To learn this function, however, we need informative data. For Python strings, where no positive data is available, we propose an active learning strategy to learn efficiently. For SMILES, where existing positive data is available, we propose a data augmentation strategy which allows us to obtain informative negative samples. We chose to describe our Q function with a recurrent neural network (LSTM), but any other similar model (GRU) could have been used as well.\n\nTo further demonstrate the the importance of our contribution, we\u2019ve updated the SMILES experiments to include a comparison with previous work on validity of samples from VAE prior \u2013 a challenging domain where benchmarks exist. Here, our model sets the new state-of-the-art. \n", "title": "Clarifying most relevant contribution"}}}