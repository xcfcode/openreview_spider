{"paper": {"title": "Importance and Coherence: Methods for Evaluating Modularity in Neural Networks", "authors": ["Shlomi Hod", "Stephen Casper", "Daniel Filan", "Cody Wild", "Andrew Critch", "Stuart Russell"], "authorids": ["~Shlomi_Hod1", "~Stephen_Casper1", "~Daniel_Filan1", "~Cody_Wild1", "~Andrew_Critch1", "~Stuart_Russell1"], "summary": "Toward better tools for interpretability, we develop methods for evaluating modularity in neural networks and apply them to partitions of neurons from a graph-based clustering algorithm.", "abstract": "As deep neural networks become more advanced and widely-used, it is important to understand their inner workings. Toward this goal, modular interpretations are appealing because they offer flexible levels of abstraction aside from standard architectural building blocks (e.g., neurons, channels, layers). In this paper, we consider the problem of assessing how functionally interpretable a given partitioning of neurons is. We propose two proxies for this: importance which reflects how crucial sets of neurons are to network performance, and coherence which reflects how consistently their neurons associate with input/output features. To measure these proxies, we develop a set of statistical methods based on techniques that have conventionally been used for the interpretation of individual neurons. We apply these methods on partitionings generated by a spectral clustering algorithm which uses a graph representation of the network's neurons and weights. We show that despite our partitioning algorithm using neither activations nor gradients, it reveals clusters with a surprising amount of importance and coherence. Together, these results support the use of modular interpretations, and graph-based partitionings in particular, for interpretability.", "keywords": ["interpretability", "modularity"]}, "meta": {"decision": "Reject", "comment": "The paper present an approach for defending for, and search for, 'modularity' in neural networks, as a step to better interpretations of their functional structure. This is an interesting, and highly original approach, as recognised by the reviewers. However,  there was also some discussion about what exactly can be learned from the derived clusters/modules, and if and how they will lead to a better understanding of neural networks, or provide concrete ways of improving them.  While the authors addressed some issues during the review process, and provided additional results, the consensus (of all three reviewers) was finally that the paper did not reach the quality standards required by ICLR. I share this view-- the paper provides a refreshing perspective, but I still am not convinced that I see a clear, compelling 'use case' for their approach.  "}, "review": {"7LXScpW7tW6": {"type": "review", "replyto": "4qgEGwOtxU", "review": "The manuscript introduces an approach, based on importance and coherence, for evaluation whether a partitioning of a network exhibits modular characteristics. \nImportance refers to how crucial is a neuron , or set of neurons, to the performance of a network on a given task, e.g. classification.\nCoherence refers to how consistently the neuron(s) in question are related to specific features.\nExperiments are conducted by considering sets of neurons identified via a spectral clustering algorithm.\n\nThe manuscript proposes a method to verify to what extent a partitioning of a network follows modular characteristics. To a good extent the proposed method is grounded on proper theoretical foundations which is highly desirable. The only part where this cannot be fully verified is its dependence on the method from [Anonymous, 2021] which cannot be verified.\n\nMy main concerns with the manuscripts are the following:\n\n- When conducting spectral clustering, the number of clusters is set to 12. Is there a procedure to set this value in a principled manner? is there an indication on the effect of this parameter? the manuscript would benefit from analyzing the effect of this parameter in the observation made on the reported experiments?\n\n- Visualizations discussed in Sec. 3.1.1 (Fig.1) are quite subjective. While in some cases some patterns are indeed visible, in other cases it is hard to make sense of what is being presented. Is it possible to evaluate the produced visualizations in a more objective manner?\nIn recent years, several methods ( Bau et al, 2016, Oramas et al. ICLR'19, Yang and Kim, arXiv:1907.09701 ) for quantitative evaluation of methods for visual interpretation and explanation have been processed. Perhaps one of these could be adopted in the manuscript with the goal of objectively evaluating the visualizations/explanations presented in Fig. 1.\n\n- In some cases design decision are made that seem to favor observations expected in some experiments. For instance, in order to favor clusterability small MLPs are pruned, to improve visualization MLPs are trained with dropout; and other factors relevant to the proposed method. Therefore my question by ensuring that some of these properties, e.g. cleaner cluster, clearer visualizations, don't you favor the measurement capabilities of the proposed importance/coherence metrics?\n\n- When analyzing the \"importance\" metric on the lesion tests (Sec. 3.2) there are new conditions that are applied to the clusters being considered in the analysis, e.g. the size of the cluster, minimum effect of the cluster on accuracy, etc. Keeping this present, my questions are: i) Were these conditions also applied when analyzing \"coherence\", and ii) why these type of condition were not applied in the experiments of Sec. 3.1? Ideally, some level of consistency is expected among the experiments. Otherwise it is hard to assess properly the origin of observations made on the results of the experiments.\n\n- At the end of Sec. 4, it is stated that the conducted experiments, combining spectral clustering with feature visualization,\nhighlight the usefulness of combining multiple interpretability methods in order to build an improved set of tools for rigorously understanding systems. However, from the observations made on the experiments I do not see the added value that the proposed method could bring to interpretability/explainability of the analyzed models (networks).\n\n- Very related, one paragraph later, it is stated that having modular networks is useful both for interpretability and for building better models. However, from the content of the manuscript it is not clear how having a modular network/representation does contribute with the two listed aspects.\n\n- Significant parts of the manuscript are delegated to the supplementary material. In addition, the third part of the proposed method, i.e. intrinsic partition evaluation, is part of another manuscript [Anonymous,2021] that does not seem to be published. For these reasons, to a good extent, the manuscript is not self-contained. ", "title": "Interesting work,  some aspects need to be polished", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "MaRfTci6mK9": {"type": "rebuttal", "replyto": "4qgEGwOtxU", "comment": "We thank our reviewers for engaging in the review process in the last two weeks.\nWe have added updates concerning three experiments:\n\n1. We added a plot for VGG trained on CIFAR into Figure 2, where the sub-clusters are visualized according to the results of the lesion test. In particular, we see that also for this network there is diversity in the sub-cluster according to our descriptive taxonomy.\n\n2. We added a new appendix section, Appendix 5, focusing on untrained networks. We perform feature visualization experiments but not lesion-based experiments because an untrained network will only have accuracy at the random guess baseline with and without lesions. The results suggest that these trained networks are considerably more input coherent than untrained ones. \n\n3. We added to what is now Appendix 8 analysis of the variance of activations for units in true and random sub-clusters when their visualizations are passed through the network. We also analyze the coefficients of variation for the activations in true sub-clusters. The results suggest that in some networks, the neurons in true sub-clusters tend to be activated by their visualizations with lower variance, and random sub-clusters tend to be activated by their visualizations with higher variance. This is a sign that in these networks, coherence tends to happen via positive rather than negative associations. Meanwhile, coefficients of variation are sometimes relatively low, including for ImageNet models, but are often over 1 for other models. \n", "title": "Updates for last revision "}, "eMhFzLsChol": {"type": "rebuttal", "replyto": "lmbNMjZrG15", "comment": "We thank Reviewer 1 for the additional comments.\n\n(1) +(6)  Indeed, we use only spectral clustering as a partition-generation method. Although our approach may be related to disentangled representations, we are using standard training methods and partitioning the network purely based on the weight values, and are unaware of prior research suggesting that the resulting partitioning of the network will have properties that would be useful for model interpretation and explanation. Similarly, while Ghorbani et al find visual concepts that networks depend on, they do not locate regions in the network that respond to these concepts, nor do they propose ways of evaluating regions for modularity or find any interesting results from this evaluation. Their criteria of importance and coherence is a property of a visual concept (input) and not of groups of neurons.\n\n(4) In updates to the paper, we rewrote the fourth paragraph of section 3 to clarify our approach to pruning and dropout. In summary, pruning was used for MLPs, and dropout was used to produce correlation-based visualization examples. The only other time in which we used dropout was for the regularized CIFAR-10 VGGs whose results are presented alongside the unregularized ones. \n", "title": "Response to Reviewer 1"}, "-aNZK9b_h7": {"type": "rebuttal", "replyto": "uSawEznuE-", "comment": "We thank Reviewer 3 for clarifying this point.\nMartin et al. discuss heavy tail distributions of _singular values_ as a way to model the distribution of weight metrics. We are not aware of an empirical work that shows that weight distributions of trained neural networks are heavy-tailed.\n", "title": "Response to Reviewer 3"}, "u39tsuibbVy": {"type": "rebuttal", "replyto": "fD6fBvPSC6r", "comment": "We thank Reviewer 3 for the additional comments and feedback.\n\n(Re 1:) We are a bit confused regarding the sentence: \u201cFurther, it is not clear to me that what you are picking up on are really clusters or heavy tails / spikes in a high-D distribution over activations.\u201d\nOur method does not cluster activations, but rather neurons based on the network's weight only. The statistical hypothesis testing procedure allows us to establish a link between the partition and the network performance. Indeed, it does not help us to understand the inner-working of a cluster, but it does show that these clusters, which were found without any use of data, are important and coherent (compared to the control partitions), and it suggests them as candidates for further interpretability investigation.\nAlso, since the hypothesis tests were based on percentiles which could only be as low as 0.1, our tests have a type natural robustness to outliers in a heavy tail.\n\n(Re: 3) We are producing a similar plot as Figure 2 for VGG models trained on CIFAR-10 and Imagenet. We\u2019ll update when the new version of the paper is uploaded. Figure 6 in the appendix (p. 19) shows what we call \u201caccuracy profile\u201d, plots of overall and per-class accuracies for the lesioning of true sub-cluster as well as multiple random sub-clusters. Although it is not shown as a histogram format, it does demonstrate the distribution of accuracies of the random shuffle control. \n", "title": "Reply to Reviewer 3"}, "c2TOtugPSdb": {"type": "rebuttal", "replyto": "ghzQRTMGy0y", "comment": "We thank Reviewer 4 for the additional comments and feedback.\n\n(1) We agree with the reviewer that this is a relevant question. Nevertheless, this paper focuses on developing the methods, and applying that one possible partition generation algorithm.\nWe note that in the concurrent submission we analyzed the effect of various regularization and initialization on _structural_ modularity, i.e., without relation to the network performance as we did in this paper.\n\n(2) + (3) Re: We thank the reviewer for this suggestion. We think it is an interesting direction for future work.\n\n(4) Re: Thank you for pointing out these papers. We have added them to the related work.\n\n(7) Re: Currently we are running experiments on randomly-initialized VGGs. We\u2019ll update when the new version of the paper is uploaded.", "title": "Reply to Reviewer 4 "}, "ZeduGWY7C2": {"type": "rebuttal", "replyto": "4qgEGwOtxU", "comment": "In response to feedback, we have posted an updated version of the paper. The key updates are as follows:\n* Section 2, first paragraph: we clarified that intrinsic partition evaluation is not relevant to the methods used in this paper. \n* Section 2, final paragraph: we added an explanation of why we chose k=12 clusters. \n* Section 3 paragraph 4: we clarified the explanation of when pruning and dropout were used. \n* Section 3.1.1 paragraph 1: we clarified that the purpose of this section is to provide visual examples and build intuition preceding the quantitative results in the following subsections.\n* Section 3.2, Importance portion: we switched the order in which we present the taxonomy and the network wide hypothesis tests for importance so as not to suggest that we filtered for sub-clusters in the hypothesis tests based on the descriptive taxonomical criteria. \n* Section 4, paragraph 2: we now mention earth mover distance and intersection information in related works. \n* Appendix 5: Lesion test results for k=8 (50% fewer) and k=18 (50% more) clusters. They are highly similar to Table 1b. \n* Appendix 6: multiple testing corrections of table 1 using both the Bonferrroni-Holm and Benjamini-Hochberg techniques. Both show that we find that most results survive the corrections for multiple comparisons. \n\nWe do not yet but will soon have updates on dispersion analysis (see reviewer 4\u2019s comments) for input construction experiments. We will post another update soon. \n", "title": "Updates for new revision"}, "R30mqifim5o": {"type": "rebuttal", "replyto": "pgsBVKNObcP", "comment": "We thank reviewer 4 for the constructive feedback. \n\n(1) Re: \u201cthe way results are presented does not easily allow to establish whether we can identify a series of robust findings that are valid across all architectures / tasks.\u201d\nIt is true that we find evidence of modular sub-clusters in some networks but not in others. However (a) we don\u2019t believe that finding different trends across different networks should be disqualifying, and (b) while we show that spectral clustering often identifies modular sub-clusters, the the most central purpose of this paper is to develop ways to measure and use importance and coherence for modularity evaluation.\n\n(2) Re: \u201cIt would also be valuable to better assess the relationship between modularity and regularization techniques, such as dropout, L2 and pruning.\u201d\nWhile we do not study this thoroughly, note that our comparisons between the regularized and unregularized CIFAR-10 VGGs relate to this. Nonetheless, we believe that further work focusing on this question and other ways to promote modularity architecturally or via training procedure will be valuable.\n\n(3) Re: \u201cit would strengthen the paper if the proposed method is put in relation to other approaches that have proven effective to address this kind of question\u2026.For example, it would be nice to see if clustering coefficient and average path length...\u201d and \u201cWhen comparing \u201ctrue sub-clusters\u201d with \u201crandom sub-clusters\u201d, a useful control analysis would be to create random sub-clusters by matching some connectivity property\u201d\nOverall, we agree that additional controls would be interesting, but we do argue that controlling for location (layer) and size are sufficient to evaluate importance and coherence. In this paper, we do not aim to compare partition generation methods. As a side note, spectral clustering has a random-walk interpretation and is very closely related with commute-time/average path length. See von Luxburg (2007).\n\n(4) Re: Earth-mover distance and intersection information.\nWe agree that earth mover distance could be used as a measure of coherence in 3.1.1. Other methods from 3.1.2 and 3.2 could as well. But we intended for 3.1.1. to give visual examples preceding more comprehensive results in 3.1.2 and 3.2. We also agree that intersection information could be used as a measure of coherence. However, to our understanding, because intersection information measures the association between a stimulus and output, this is closely related to our experiments with lesions. Is this consistent with your understanding?\n\n(5) Re: \u201cWhat is the rationale for setting the number of clusters to 12?\u201d\nWe will change section 2 to clarify this. We wanted a number greater than 10 (the number of classes in MNIST/CIFAR), but small relative to the number of filters in the networks' layers. Note that finding results with k=12 in networks ranging from the MNIST to ImageNet scale suggests that these phenomena are robust to different choices of the ratio between $k$ and the size of the network. We have explored using higher $k$, but much larger values lead to slow runtimes for ImageNet experiments. We are currently running lesion experiments for k=8 and k=18 which we will report here soon.\n\n(6) Re: \u201cRegarding the gradient-based method discussed in Olah et al. (2017), it would be useful to have some dispersion measure over the final optimization score\u201d\nWe agree this may be interesting. We are running some experiments to get a sense of dispersion and will hopefully be able to report this soon. \n\n(7) Re: \u201cIt would be interesting to include as baseline some analysis on randomly connected networks.\u201d\nWe agree that this would be interesting for understanding how much sub-cluster importance and coherence result from training, but we do not see a strong motivation for this involving the development of interpretability methods. \n\n(8) Re:  Where the p-values corrected for multiple comparisons?\nNo. But we agree that also reporting multiple testing corrections in the Supplement would be helpful. We are working on this now and will report results soon.\n\n(9) Re: \u201cI guess there are several cases where modular systems (or in general systems with localized representations) can develop complex emergent dynamics that still prevent interpretability.\u201d\nWe agree. We believe that future work will be needed for clarifying the association between interpretability and modularity. Note that this is a _motivation_ for our work though because developing methods for identifying modules and validating their boundaries is a prerequisite to answering this question. \n\n[1] Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395\u2013416, 2007.\n", "title": "Reply to reviewer 4"}, "NXju7UC4mN6": {"type": "rebuttal", "replyto": "7LXScpW7tW6", "comment": "We thank reviewer 1 for the constructive feedback. \n\n(1) Re: \u201cThe only part where this cannot be fully verified is its dependence on the method from [Anonymous, 2021] which cannot be verified... the third part of the proposed method, i.e. intrinsic partition evaluation, is part of another manuscript [Anonymous,2021] that does not seem to be published\u201d\nWhile the other paper investigates spectral clustering in depth, this paper uses spectral clustering as an example of a partition-generating procedure. In that sense, this paper can stand alone. Regarding intrinsic partition evaluation, we mention this for completeness, but it is not relevant to the experiments we conduct in this paper. However, we are hoping to improve clarity here. Do you have any improvements in mind? For example, is it confusing to mention Intrinsic Partition Evaluation in section 2? \n\n(2) Re: \u201cWhen conducting spectral clustering, the number of clusters is set to 12.\u201d\nWe will change section 2 to clarify this. We wanted a number greater than 10 (the number of classes in MNIST/CIFAR), but small relative to the number of filters in the networks' layers. Note that finding results with k=12 in networks ranging from the MNIST to ImageNet scale suggests that results are robust to different choices of $k$. We have explored using higher $k$, but much larger values lead to slow runtimes for ImageNet experiments. We are currently running lesion experiments for k=8 and k=18 which we will report here soon.\n\n(3) Re: \u201cVisualizations discussed in Sec. 3.1.1 (Fig.1) are quite subjective...Is it possible to evaluate the produced visualizations in a more objective manner?\u201d\nWe intended section 3.1.1. to provide simple visual examples before the more rigorous quantitative results in the rest of section 3. Note that in 3.1.2 and 3.2, we also perform experiments on the same types of networks which are visualized in 3.1.1. \n\n(4) Re: \u201cIn some cases design decision are made that seem to favor observations expected in some experiments...don't you favor the measurement capabilities of the proposed importance/coherence metrics\u201d\nWould you be able to further clarify if/how this is an issue? We do not see an inherent issue with tuning our training process for clearer results. We will clarify in section 2 our approach with pruning and dropout. Put more simply, we use pruning in MLPs and no other networks. We use dropout in the regularized CIFAR-10 VGGs and also in the MLPs used for correlation-based visualization (with the exception of the network trained on halves-diff data in figure 4d). Also bear in mind that the baselines we compare all results to are random sub-clusters, not subclusters of unregularized networks.\n\n(5) Re: \u201cWhen analyzing the \"importance\" metric on the lesion tests (Sec. 3.2) there are new conditions that are applied to the clusters being considered in the analysis...i) Were these conditions also applied when analyzing \"coherence\", and ii) why these type of condition were not applied in the experiments of Sec. 3.1?\u201d\nWe will rewrite some of these explanations for clarity. In all experiments with importance and coherence from which Table 1 is produced, we omit sub-clusters that are extremely small or large (details in supplement), but we do not otherwise select for clusters. We ONLY introduce new conditions for classifying sub-clusters for the sake of producing Figure 2 and providing an example taxonomy that demonstrates the diversity among sub-clusters in size, importance, and importance relative to random sub-clusters.\n\n(6) Re: \u201cit is not clear how having a modular network/representation does contribute with the two listed aspects\u201d\nOur argument is that this work motivates building networks that better lend themselves to modular deconstructions because this level of abstraction is useful for interpretability. The argument that modularity leads to better models made via discussion and citation of works in the final paragraph of related works. \n\n(7) Re: \u201cI do not see the added value that the proposed method could bring to interpretability/explainability of the analyzed models.\u201d\nOur argument is that the advantage of combining clustering with other interpretability methods is that it provides a method of generating partitions based on weight connectivity which doesn't involve activations or gradients. The key thing that weight-based clustering brings to the table is non-redundancy with methods based on runtime analysis. \n", "title": "Reply to reviewer 1"}, "pP96MWqcgLH": {"type": "rebuttal", "replyto": "IKY7SYg-6Cy", "comment": "We thank reviewer 3 for the constructive feedback. \n\n(1) Re: \u201c Largely qualitative and anecdotal...The results shown in this paper are thin and qualitative\u201d\nSection 3.1.1 and Figure 1 are indeed qualitative. Our goal for them was to provide visual examples with a simple dataset like MNIST. More importantly though, 3.1.2 and 3.2 are built entirely around quantitative results. We would like to emphasize that having quantitative results based on statistical hypothesis testing at all is actually something which sets this paper apart from related work using similar methods such as Bau et al. (2017), Watanabe (2019), and Carter et al. (2019).\n\n(2) Re: \u201cThe results [in Figure 1] are limited to black and white images (MNIST and fashion-MNIST), and not all examples look great.\u201d\nWhat we want to emphasize in Figure 1is that these sub-clusters, despite being based only on weights, systematically exhibit coherence w.r.t. testing data. We find this interesting because it shows that we can uncover runtime properties of a network without making queries to it. And we believe this is useful because interpreting units at the sub-cluster level gives us a flexible level of abstraction and pairs well with data-sensitive interpretability methods. \n\n(3) Re: \u201cOnly 2 figures are shown in the main paper, with a lot of other details shoved into the supplement. Thus, the writing and presentation could be improved...\u201d\nWould it be possible to provide more detailed suggestions about what figures and details from the supplement should be emphasized more and included in the main paper? For example, should we incorporate hypothesis testing details from the supplement into the main paper? \n\n(4) Re: \u201cThe results crucially rely on a second paper which was concurrently submitted and can't be reviewed because it is anonymized.\u201d\nWhile the other paper investigates spectral clustering in depth, this paper only relies on spectral clustering as an example of a partition-generating procedure for neurons. In that sense, this paper stands alone. However, we are hoping to improve clarity here. What kind of details would you like to see? For example, is it confusing to mention Intrinsic Partition Evaluation in section 2? \n\n(5) Re: \u201cThe paper does not generate testable predictions or practical insights...\u201d\nWe agree that our main contribution is not a set of methods that will be immediately useful to practitioners, but this was not our goal. Our approach focused on understanding phenomena and introducing tools which evaluate whether a given neuron partitioning reflects the network functionality, and thus supports the existence of modularity. \n\n(6) Re: \u201cThe only takeaway point for me was that some neurons / units show correlated representations\u201d\nThis is not how we would characterize our contributions. While a cluster of units being highly correlated with one another w.r.t. a data distribution would be sufficient to show input coherence, it would not be necessary, nor would it establish importance or output coherence. Note that it is also novel that spectral clustering can find sets of neurons which exhibit these properties at all. We argue that the main takeaways from this paper are the concepts of importance and coherence, statistical methods for measuring them, and demonstrating that spectral clustering, which takes only the network\u2019s weights as an input, often reveals important and input coherent sub-clusters. \n\n[1] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6541\u20136549, 2017.\n[2] Chihiro Watanabe. Interpreting layered neural networks via hierarchical modular representation. In International Conference on Neural Information Processing, pp. 376\u2013388. Springer, 2019.\n[3] Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah. Activation atlas. Distill, 4(3):e15, 2019.\n", "title": "Reply to reviewer 3"}, "tfIR4X6HUQG": {"type": "rebuttal", "replyto": "4qgEGwOtxU", "comment": "We would like to thank the reviewers for their thoughtful comments and efforts towards improving our paper. We responded to each reviewer in a separate comment, in addition, in this comment we address some of the most important points from the reviewers. Nevertheless, please refer to the individual comment for a detailed reply.\n\nOur main results are summarized in table 1 and discussed in sections 3.1.2 and 3.2. These results are built entirely around a quantitative approach based on statistical hypothesis testing, and it is what sets this paper apart from related work using similar methods such as Bau et al. (2017), Watanabe (2019), and Carter et al. (2019). Indeed, section 3.1.1 and Figure 1 are qualitative, and they are intended to provide visual examples with a simple dataset like MNIST in analyzing clusters.\n\nThe reasoning that guided us in choosing the number of clusters to be $k=12$ is that we wanted a number greater than 10 (the number of classes in MNIST/CIFAR), but small relative to the number of filters in the networks' layers. We are currently running lesion experiments for k=8 and k=18 which we will report here soon.\n\nOn top of that, we run new experiments that measure dispersion over the final optimization score for the input cohorision test, following the suggestion of Reviewer #4. We are also adding to the paper analysis of p values under a multiple testing framework. We will also report on these soon. \n\nIn this paper we cite a concurrent submission about partition-generating using spectral clustering. All the relevant details are explained also in this paper, and we consider it self-contained in this sense, as the proposed modularity evaluation method can be applied to any partitioning of a neural network. We would like to ask the reviewers whether there is any detail that is missing.\n\nFinally, we believe that modularity could be one possible path, in addition to others, for achieving mechanistic transparency of neural networks. The proposed methods in this paper for evaluating a network partitioning, are one step in this direction. We find it exciting that networks display modularity that can be found via spectral clustering. We think that the most valuable implications of this work in the future will involve modularity evaluation in networks with more explicitly modular architectures and training procedures.\n\n[1] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6541\u20136549, 2017.\n[2] Chihiro Watanabe. Interpreting layered neural networks via hierarchical modular representation. In International Conference on Neural Information Processing, pp. 376\u2013388. Springer, 2019.\n[3] Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah. Activation atlas. Distill, 4(3):e15, 2019.\n", "title": "Comment to all reviwers: key points"}, "pgsBVKNObcP": {"type": "review", "replyto": "4qgEGwOtxU", "review": "This paper explores the application of spectral clustering methods to assess modular organization in the emergent architecture of deep networks. In particular, sub-modules identified by spectral clustering are evaluated in terms of \"importance\" and \"coherence\", two metrics defined by the authors with the goal of capturing how crucial the neurons in the sub-module are to the classification accuracy, and how consistent their activation is across input and output patterns.\nOverall, the paper addresses important questions related to the way structural properties in a deep network might support the emergence of functional properties, which is a key issue given the relatively poor theoretical understanding we have about these self-organizing systems. The paper is comprehensible, though the general structure and the writing could be improved to improve readability. For example, in Section 3 it is not immediately clear how the importance and coherence metrics relate to the specific technique adopted for feature visualization, or to the lesioning method applied. The \u201cRelated Work\u201d section should be moved at the beginning of the paper, and the contribution should be better framed in the context of other existing approaches.\nAlthough I appreciate the wide range of networks tested by the authors, I think that the way results are presented does not easily allow to establish whether we can identify a series of robust findings that are valid across all architectures / tasks, or whether specific cases entail peculiar findings. It would also be valuable to better assess the relationship between modularity and regularization techniques, such as dropout, L2 and pruning: I think that this is a very important point that should deserve further investigation, since it could give important insights about the role of regularizers in shaping the final network architecture.\nFinally, it would strengthen the paper if the proposed method is put in relation to other approaches that have proven effective to address this kind of question. This would greatly improve the robustness of the results, since the current baseline is basically constituted by a comparison with random sub-modules. For example, it would be nice to see if clustering coefficient and average path length (as defined in [1]) can provide useful information also for the analyses proposed by the authors. Note that in [1] the authors investigated deep networks with similar architectures (MLPs, CNNs, ResNets) trained on similar tasks (CIFAR-10, ImageNet).\n\nOther comments:\n- Pg. 3: the technique used to visualize a sub-cluster by creating an aggregate measure of the learned features can be discussed in relation to the method based on Earth-mover distance proposed by [2], where the authors also discuss other graph-based metrics that might be useful in the present setting.\n- Could the \u201cintersection information\u201d approach presented in [3] can be exploited also in the analyses of the sub-modules detected by the spectral clustering? Note that in [3] the authors also investigate \u201clesion tests\u201d by means of interventional techniques, which would make that approach very interesting as a further benchmark.\n- What is the rationale for setting the number of clusters to 12? If this value is not theoretically motivated, further analyses should show that the results are robust to variations in this value.\n- When comparing \u201ctrue sub-clusters\u201d with \u201crandom sub-clusters\u201d, a useful control analysis would be to create random sub-clusters by matching some connectivity property (e.g., same average strength, clustering coefficient and/or average path length).\n- Regarding the gradient-based method discussed in Olah et al. (2017), it would be useful to have some dispersion measure over the final optimization score, in order to better assess whether all neurons in the sub-cluster where in fact similarly activated by the optimized image.\n- It would be interesting to include as baseline some analysis on randomly connected networks, since it has been shown that subgraphs in large random networks can in fact support accurate task performance even without ever training the weight values [4].\n- Where the p-values corrected for multiple comparisons?\n- The authors consider \u201cmodularity  as an organizing principle to achieve mechanistic transparency\u201d. Though I sympathize with this statement, I guess there are several cases where modular systems (or in general systems with localized representations) can develop complex emergent dynamics that still prevent interpretability.\n\nReferences\n[1]\tJ. You, J. Leskovec, K. He, and S. Xie, \u201cGraph Structure of Neural Networks,\u201d in International Conference on Machine Learning, 2020.\n[2]\tA. Testolin, M. Piccolini, and S. Suweis, \u201cDeep learning systems as complex networks,\u201d J. Complex Networks, vol. 0000, no. 1, pp. 1\u201321, Jun. 2019.\n[3]\tS. Panzeri, C. D. Harvey, E. Piasini, P. E. Latham, and T. Fellin, \u201cCracking the Neural Code for Sensory Perception by Combining Statistics, Intervention, and Behavior,\u201d Neuron, vol. 93, no. 3, pp. 491\u2013507, 2017.\n[4]\tV. Ramanujan, M. Wortsman, A. Kembhavi, A. Farhadi, and M. Rastegari, \u201cWhat\u2019s Hidden in a Randomly Weighted Neural Network?,\u201d In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11893-11902. 2020.", "title": "Interesting proposal to inspect the emergent structure in deep networks, which should be refined and compared with alternative methods based on graph-theory", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "IKY7SYg-6Cy": {"type": "review", "replyto": "4qgEGwOtxU", "review": "The authors identify putative clusters of units/neurons in deep networks using spectral clustering on a graph defined by synaptic weights. The authors then argue that these structurally defined clusters of neurons have similar *functional representations*. Finding interpretable relationships between weight matrices and functional modules is challenging, and the authors should be applauded for attempting to tackle this challenging problem that few research groups are devoting energy to.\n\nDespite these positive notes, I have reservations about the presentation and results of the paper. My main concerns are:\n\n(1) The results are largely qualitative and anecdotal. In figure 1, for example, the authors show slightly higher contrast in their identified clusters than random clusters. The results are limited to black and white images (MNIST and fashion-MNIST), and not all examples look great. Only 2 figures are shown in the main paper, with a lot of other details shoved into the supplement. Thus, the writing and presentation could be improved to highlight the most exciting and surprising findings.\n\n(2) The results crucially rely on a second paper which was concurrently submitted and can't be reviewed because it is anonymized. The results shown in this paper are thin and qualitative (see point 1), so in my view these two paper should be combined into a single paper which overall might tell a more comprehensive and compelling story.\n\n(3) The paper does not generate testable predictions or practical insights that could be used by used by practitioners. The only takeaway point for me was that some neurons / units show correlated representations, which is arguably already known (e.g. Csordas et al 2020). How to exploit this modularity to develop human-interpretable explanations of network function remains unclear to me.", "title": "Worthwhile ideas and concept, but feels incomplete", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}