{"paper": {"title": "Lossless Compression of Structured Convolutional Models via Lifting", "authors": ["Gustav Sourek", "Filip Zelezny", "Ondrej Kuzelka"], "authorids": ["~Gustav_Sourek1", "zelezny@fel.cvut.cz", "~Ondrej_Kuzelka1"], "summary": "Speeding up weight-sharing dynamic neural computation graphs, such as GNNs, with lifted inference.", "abstract": "Lifting is an efficient technique to scale up graphical models generalized to relational domains by exploiting the underlying symmetries. Concurrently, neural models are continuously expanding from grid-like tensor data into structured representations, such as various attributed graphs and relational databases. To address the irregular structure of the data, the models typically extrapolate on the idea of convolution, effectively introducing parameter sharing in their, dynamically unfolded, computation graphs. The computation graphs themselves then reflect the symmetries of the underlying data, similarly to the lifted graphical models. Inspired by lifting, we introduce a simple and efficient technique to detect the symmetries and compress the neural models without loss of any information. We demonstrate through experiments that such compression can lead to significant speedups of structured convolutional models, such as various Graph Neural Networks, across various tasks, such as molecule classification and knowledge-base completion.", "keywords": ["weight sharing", "graph neural networks", "lifted inference", "relational learning", "dynamic computation graphs", "convolutional models"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper shows that show that methods for probabilstic lifted inference can also be used to \"compress symmetries\" in convolutional models over structured data. The resulting structured convolutional models are then shown to yield speed ups for learning graph neural networks, too. This is highly interesting since the existing literature rather considered how to make use of weisfeiler lehman for classification of graphs, both in neural and a kernel way. This paper, however, shows how to compress the computations. And it paves the way to connect equivariant learning lifted inference by exploiting the connection between lifted probabilistic inference / weisfeiler lehman and their algebraic formulations. "}, "review": {"VMA9sqBXsAE": {"type": "review", "replyto": "oxnp2q-PGL4", "review": "The paper proposes an approach to make learning deep neural networks more efficient using ideas from lifted inference for relational probabilistic models. Specifically, symmetries in the computation graph are identified and then the neural network is compressed into an equivalent model.\n\nThe idea of using symmetries for improving scalability has been successfully used in statistical relational models. Therefore, it seems to be a nice direction for improving scalability in neural networks such as GCNs. Thus, large computation graphs can be reduced to smaller graphs for effective computation.\n\nThe main weakness with the paper is that the compression algorithm was not very clear to me. Particularly, the problem of identifying isomorphisms is a hard problem and several lifted inference techniques have typically used approximate methods to identify symmetries in graphical models [e.g. Niepert UAI 2012, Bui et. al. UAI 2013, Holtzen et al. UAI 2019, etc.]. I was not sure why or how the identification of exact symmetries is not hard in this case. Is the type of computation graph restricted to some form? If so, a more detailed description of the properties of this graph is useful. Since the main focus of the paper is on \u201cexact\u201d symmetries, I think in general, a more detailed analysis of the proposed approach will help in general.\n\nRegarding the experiments, they show that the proposed techniques can help speed up different types of deep models based on GNNs. One aspect that was not very clear was how large were the computation graphs? Also, when the compression is exact would the accuracies still vary between the compressed and uncompressed model?\n\nIn general, I like the idea of using symmetries to compress graph-based neural networks. I am not too clear on the details of the proposed approach, particularly their applicability for general graph structures.\n\nFrom the discussions, the authors make it clear that \"compressing\" the computation graph is possible without the need for expensive operations (as is the case in typical \"lifted\" inference literature). The approach does seem to be simple to implement , maybe a bit more detailed analysis and clarity as suggested by others as well could strengthen the paper further.", "title": "An approach to use symmetries to improve scalability of learning in graph based neural networks. The idea seems promising and has been successful in the graphical models community but some details were unclear in the paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "5bzWkZ0KINB": {"type": "rebuttal", "replyto": "YPPs7sCftYl", "comment": "Thank you for the review!\n\n> *1. Although I think this work is good for small datasets like molecular networks, i am skeptic about its transferability in large scale graphs. The reason behind that is I don't assume there would be a lot of structurally similar nodes in case of a large scale real life graph. Authors did not provide any information on the size of the graphs used for the experiments.*\n\nIt is true that the effectiveness of the compression depends on the amount of symmetries in the computation graphs, which stem from the input graph symmetries together with the model symmetries (weight-sharing). It is however not dependent on the size of the graphs.\n\nThe individual input graphs vary from the smallest with app. 25 nodes and 50 bonds per a single molecule, to medium-size knowledge bases with app. 20,000 triples over hundreds of objects and relations. We note that the molecular data are not \"small datasets\", as there are thousands of molecules in each (and so the overall dataset size can be actually bigger than some \"large\" graphs). \n\nWe also note that these are real-life datasets, and the molecular data represent one of the primary GNN applications (Zhou, Jie, et al, 2018).\n\nThe computation graphs themselves are then in the orders of 10^2-10^5 of nodes.\n\nWe added the information on graph sizes into the paper.\n\n>*2. Authors proposed two different algorithm, exact and non-exact. There are no comparisons between these two methods are shown in the result. Proper analysis on time and performance of the algorithms are also absent. The motivation behind using exact or non-exact is not clear.*\n\nThese two algorithms are not to be considered as competing alternatives in terms of performance. The practical difference is that the exact algorithm provides guarantees on lossless compression, while the non-exact does not. However in the experiments, we demonstrate that even the non-exact algorithm achieves lossless compression in practice, given simply a high enough numeric precision to be used in the value comparisons (Fig. 4). The results of the two algorithms are then equivalent in practice.\n\nThe non-exact algorithm is slightly faster, since it skips the structural equivalence check, nevertheless the asymptotic complexity of both is the same and is linear in the size of the computation graph (i.e. the same as traversing the graph for simple evaluation - see end of Section 4).\n\nThe main motivation behind the exact vs. non-exact algorithms were the structural vs. functional symmetries (see beginning of Section 4). Without having the exact algorithm, we could not show that the non-exact algorithm behaves as an exact one in practice (given sufficiently many bits of precision). \n\n>*3. On Table 1, the time of the different experiments are given. There are no analysis on time vs performance metric in this table. Does all the baseline show similar performance as the baseline model? The analysis is not present.*\n\nYes, each of the models achieves the same accuracy (performs equivalent computations) across all the frameworks (see Sec. 5.1., p. 8). The computation time is thus all that matters. A more detailed analysis and comparison of the frameworks can be found in the work of (Sourek et. al., 2020).\n\n>*4. It would also be interesting to see an analysis on the effect of the algorithms on different graph types based on their characteristics (degree distribution, density, veracity etc).*\n\nWe generally agree, but the compression performance of the algorithm(s) depends on the input graph and model properties (symmetries) in a more complicated manner. Consequently, one could surely select or simulate input graphs of very different properties leading all to extreme compression rates using the right combination of particular node types and patterns of their interconnections extracted by the respective models. \nInstead, in this paper we provide experiments over varying real-world datasets and models from actual practice to reflect the expected real-life performance.\n\n\n\n------\n\n\nZhou, Jie, et al. \"Graph neural networks: A review of methods and applications.\" arXiv preprint arXiv:1812.08434 (2018).\n\nSourek, Gustav, Filip Zelezny, and Ondrej Kuzelka. \"Beyond Graph Neural Networks with Lifted Relational Neural Networks.\" arXiv preprint arXiv:2007.06286 (2020).", "title": "Author response"}, "2_CeJ7TE8mo": {"type": "rebuttal", "replyto": "VMA9sqBXsAE", "comment": "Thank you for the review!\n\n> *The main weakness with the paper is that the compression algorithm was not very clear to me. Particularly, the problem of identifying isomorphisms is a hard problem and several lifted inference techniques have typically used approximate methods to identify symmetries in graphical models [e.g. Niepert UAI 2012, Bui et. al. UAI 2013, Holtzen et al. UAI 2019, etc.]. I was not sure why or how the identification of exact symmetries is not hard in this case. Is the type of computation graph restricted to some form? If so, a more detailed description of the properties of this graph is useful. Since the main focus of the paper is on \u201cexact\u201d symmetries, I think in general, a more detailed analysis of the proposed approach will help in general.*\n\nThe type of the computation graphs is not restricted, but the algorithm does not promise to detect isomorphisms or automorphisms in arbitrary graphs.\n\nThe \u201ctrick\u201d is that our method does not actually need to be able to detect isomorphism or find automorphisms, which is why it can sidestep the computational complexity obstacles. Automorphisms are important in some other problems (such as probabilistic inference in some of the references you mention) but they are not necessary for compressing computational graphs, as we argue below. The description below applies to the exact algorithm (which is the focus of the reviewer\u2019s comment).\n \nConsider the following computation graph (using the notation used in the paper where we have (node1, node2, edge label)): (1, 3, 1), (2,3,1), (4,5,1), (3,6,1), (5,6,1)\n \nThis can be visualized as (v1 -> v3), (v2 -> v3), (v4 -> v5), (v3 -> v6), (v5 -> v6) and all edges are labelled with the same weight w_1.\n \nSuppose that we have the constant activation functions for the leaves of the graph: f_1 = 1, f_2= 2, f_4 = 1. And the other activation functions are, say, a sigmoid.\n \nThe output of the exact algorithm will be a graph corresponding to:\n \n(v1 -> v3), (v2 -> v3), (v1 -> v5), (v3 -> v6), (v5 -> v6)\n \nIn this case, we only removed the node v4 and replaced it by v1 (so the compression gain is not high but that is not the point of this example). The point of this example is to illustrate that we merged v1 and v4 despite the fact that there is no automorphism mapping v1 to v4 (in the graph theoretical sense). \n \nHopefully, this example helps to illustrate why we do not have to worry about complexity of detecting symmetries such as graph automorphisms - our algorithm simply does not use them. \n \nWe have added a new paragraph explaining in more detail how structural-equivalence differs from detecting automorphism (page 4).\n\n\n\n> *Regarding the experiments, they show that the proposed techniques can help speed up different types of deep models based on GNNs. One aspect that was not very clear was how large were the computation graphs?* \n\nThe computation graph sizes are in the orders of 10^2-10^5 of nodes, reflecting the input graphs ranging from small molecules to knowledge bases. We added the information into the paper.\n\n\n\n> *Also, when the compression is exact would the accuracies still vary between the compressed and uncompressed model?*\n\nNo, they are practically the same, as one would expect from a lossless compression. This was the very purpose of experiments targeting the proposed experimental Question 3. (p. 6), the results of which are in Figure 6 (and also in Fig. 4 - left).", "title": "Author response"}, "QrOSSoWVVht": {"type": "rebuttal", "replyto": "y9p8c6MVc1E", "comment": "Thank you for the review!\n\n> *However, I think the presentation could be improved. In particular, it is not clear the evalutation settings that they are using. For knowledge completion how do they compute accuracy? Do they use golden triples?*\n\nWe used a standard process of measuring accuracy with golden triples and negative samples generated by corrupting the golden triples (Wang, Quan, et al., 2017). We do not elaborate much on accuracy computation in the paper as the focus here is on computing performance (speedup) and not accuracy of the models.\nI.e. the achieved accuracy values themselves are not important, they are only shown to demonstrate the relative effect of the compression on accuracy (i.e. no effect in the case of lossless compression).\n\n\nWang, Quan, et al. \"Knowledge graph embedding: A survey of approaches and applications.\" IEEE Transactions on Knowledge and Data Engineering 29.12 (2017): 2724-2743.", "title": "Author response"}, "SkswAksN2nj": {"type": "rebuttal", "replyto": "Qj1aOaDYCaY", "comment": "Thank you for the reference, we were not aware of this recent work. We agree that is it very different, but shares the same goal of a lossless computation compression. Added to related work.", "title": "related work updated"}, "YPPs7sCftYl": {"type": "review", "replyto": "oxnp2q-PGL4", "review": "##########################################################################\n\nSummary:\n\n \nThe paper provides an interesting work in the scale/speed up of structured convolutional models. In particular, it proposes an idea using a technique named lifting which is used in scaling up of graphical models to detect the symmetries and compress the neural model such as Graph Neural Network. Authors show that this compression can lead to speedups of the models in many tasks.\n\n##########################################################################\n\nPros:\n \n1. The paper takes one of the most important issue of structured convolutional models: scale/speed up. I think this is an area with real world implication where we need more works.\n\n \n2. I think this method can be very useful for networks using lots of identical nodes like molecular network. It can leverage from the repetition in nodes and compress the network into some smaller network and save time and memory.\n \n3. Overall the paper is well written. I liked the illustrations for explaining the methods. Result section is also well structured. It clearly shows the effectiveness of the algorithm over other methods.\n \n##########################################################################\n\nCons: \n\n \n1. Although I think this work is good for small datasets like molecular networks, i am skeptic about its transferability in large scale graphs. The reason behind that is I don't assume there would be a lot of structurally similar nodes in case of a large scale real life graph. Authors did not provide any information on the size of the graphs used for the experiments. \n \n2. Authors proposed two different algorithm, exact and non-exact. There are no comparisons between these two methods are shown in the result. Proper analysis on time and performance of the algorithms are also absent. The motivation behind using exact or non-exact is not clear.\n\n3. On Table 1, the time of the different experiments are given. There are no analysis on time vs performance metric in this table. Does all the baseline show similar performance as the baseline model? The analysis is not present.\n\n4. It would also be interesting to see an analysis on the effect of the algorithms on different graph types based on their characteristics (degree distribution, density, veracity etc). \n \n##########################################################################\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n \n#########################################################################\n\n", "title": "Room for improvement", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "y9p8c6MVc1E": {"type": "review", "replyto": "oxnp2q-PGL4", "review": "The authors of this paper propose an compression technique for GNNs that was inspired by lifted inference. The compression consists of removing asymmetries by merging nodes. They define two algorithms for compression: a non-exact algorithm that merges two nodes that are \"functional\" equivalent and an exact algorithm that merges two nodes that are structurally equivalent.\n\nThe approach seem quite novel, pretty interesting and it could interest a wide audience that works on graph models. However, I think the presentation could be improved. In particular, it is not clear the evalutation settings that they are using. For knowledge completion how do they compute accuracy? Do they use golden triples?\n\nIn conclusion I think section 5 should be improved and writte more clearly but overall I think it is a good paper and should be considered for acceptance.", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}}}