{"paper": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "summary": "", "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"]}, "meta": {"decision": "Reject", "comment": "This work explores weight pruning for BERT in three broad regimes of transfer learning: low, medium and high.\n\nOverall, the paper is well written and explained and the goal of efficient training and inference is meaningful. Reviewers have major concerns about this work is its technical innovation and value to the community: a reuse of pruning to BERT is not new in technical perspective, the marginal improvement in pruning ratio compared to other compression method for BERT, and the introduced sparsity that hinders efficient computation for modern hardware such as GPU. The rebuttal failed to answer a majority of these important concerns.\n\nHence I recommend rejection."}, "review": {"S1lvPpVDsr": {"type": "rebuttal", "replyto": "HJeqpvgaYr", "comment": "Thank you for taking the time to review our paper! We will try to answer your questions in order:\n\n1a. How did we pick 3 epochs for fine-tuning? Would a different amount of fine-tuning help?\n\n3 epochs was the amount of fine-tuning used in the original BERT paper. However, we also tried fine-tuning between 1 and 12 epochs before pruning 60%. The results in Figure 3 (right) show that the development accuracy does not improve whether you train for 1 epoch or 12 before pruning.\n\nMore importantly, however, is that the weights pruned do not change much whether we fine-tune for 1 epoch or 12 epochs. We observed in Figure 4 (right) that weights quickly settle into a new sorting order within the first epoch of fine-tuning. This implies that no matter how much longer we fine-tune, the weights selected for pruning will not change much.\n\n1b. Why is pruning after fine-tuning worse than pruning during pre-training?\n\nThis is really only true around 60% pruning and above. We believe the explanation for this is that we continue training after pruning to recover accuracy. If we prune during pre-training, this allows the model to recover some of the deleted pre-training information in the remaining weights. If we prune during fine-tuning, however, that information is no longer accessible to the model, so it cannot recover as well.\n\n2. Would examining the pruning thresholds tell us whether some pruning ratio is reasonable?\n\nWe completely agree that this is an interesting signal. It is difficult, however, to interpret the threshold without knowing the distribution of weights in each matrix. For example, a threshold of 0.01 might be reasonable if the standard deviation of weights is 1, but not if the standard deviation is 0.001.\n\nExamining the percentage of total magnitudes pruned might be a better choice here, since it is also easy to compute given the weights. This can be found in Figure 5 in the Appendix.\n\n3a. Why were models pruned after fine-tuning trained until training losses were comparable?\n\nWe wanted to make sure that the pruned models fit the downstream data just as well as models pruned during pre-training. This would imply that the models pruned after fine-tuning \u201clearned\u201d the downstream data just as well as the others.\n\n3b. Doesn\u2019t it look like the models are over-fitting?\n\nWe don\u2019t think so. The un-pruned models in each experiment have both the lowest training loss out of all the models and also the highest development accuracy. Pruning, which acts like a regularizer, decreases model complexity and increases training loss. In this case, it also happened to decrease the development accuracy.\n\nIt is known that neural networks can sometimes fit the data perfectly and still generalize well. This has been called the \u201cdouble-descent risk curve,\u201d[1] and we conjecture that it may explain the role of pre-training in generalization as well as some of our results.\n\n[1] https://arxiv.org/abs/1812.11118", "title": "Response to Review"}, "HkxT-iVDor": {"type": "rebuttal", "replyto": "BJeAGC3pYS", "comment": "Thank you for taking the time to review our paper! We understand your concerns\nabout the practicality/novelty of the pruning method we used. To summarize:\n\n1. Pruning does not seem practically useful.\n  a. Random sparsity is difficult to accelerate on GPUs; structured sparsity might be better.\n  b. Some methods under submission achieve higher compression rates.\n  c. We cannot prune much (30-40%) without losing accuracy.\n\n2. We do not propose any new technical methods.\n\nWe would like to emphasize, however, that these practical questions are\northogonal to the thesis of our paper. Our main contribution is a scientific\ninvestigation of a general question: how does compressing a universal feature\nextractor affect task-specific problems? We believe this paper merits being\nshared with the community on a scientific basis alone. We will, however, try to\nclarify some of the practical aspects as well.\n\n1a. Random sparsity is difficult to accelerate on GPUs. Structured sparsity might be better.\n\nAccelerating unstructured sparse matrix multiplication is an active area of\nresearch in which recent progress has been made. Bank-balanced sparsity (which\nis closely related to unstructured sparsity) achieves near-ideal speed-ups while\nrequiring a minimal deviation from unstructured sparsity.[1] We believe our\nresults transfer to this technique, since bank-balanced sparsity preserves\nalmost 95% of weight magnitudes when compared with unstructured pruning. On the\nsystems side, adaptive sparse matrix multiplication has shown promising results\non GPUs.[2]\n\nStructured pruning, on the other hand, is not currently practical. Block sparse\npruning imposes optimization / model constraints that quickly degrade\naccuracy.[1][3] It is also not clear whether other types of structured pruning\n(attention head, etc.) are orthogonal to weight pruning (explored in Section 6),\nso it may make sense to do both on a single model.\n\n1b. Some methods currently under submission achieve higher compression rates.\n\nThese papers [4][5][6] utilize some combination of knowledge distillation, word\nembedding factorization, and parameter sharing. However, these methods are not\nwell understood, which makes it difficult to use them to answer scientific\nquestions. Weight magnitude pruning, on the other hand, is simple and\nwell-motivated. It\u2019s known that when an over-parameterized neural network\nachieves a global minimum, many subnetworks have zero weights [8]. We should\nalso note that many of these other techniques also do not show a practical\ninference speed improvement.\n\n1c. We cannot prune much (30-40%) without losing accuracy.\n\nAs Reviewer #1 points out, 30-40% pruning is not practically useful. However,\nthe specific numbers are not important to our work. Before we started, we did\nnot know BERT was prunable and why performance would degrade, if it did. Our\nwork has shown the existence of three distinct regimes of pruning: most of\nBERT\u2019s capacity encodes the pre-training inductive bias, and only a small\nfraction is needed to fit downstream data. This implies that the size of the\npre-training dataset is the limiting factor in model compression, which should\ndrive future work towards understanding the nature of that inductive bias.\n\nAlso, several application domains demand very memory constrained models.\nPractitioners in these domain will accept \u201clossy\u201d compression (60-70%), as long\nas they can quantify the memory / accuracy trade-off.\n\n2. We do not propose any new technical methods.\n\nWe are interested in exploring the previously unexplored question of how\ncompression affects transfer learning. For this purpose, we choose to use a\nwell-understood technique. Weight magnitude pruning is old, but it has recently\nbeen validated as one of the most effective and fine-grained pruning\ntechniques.[7] While other compression methods may achieve smaller model sizes,\nthis is orthogonal to our main contributions.\n\nAlso, some of our conclusions are independent of magnitude weight pruning:\n\n- Fine-tuning does not change the weight distribution much, giving further\n  evidence for focusing on compressing during pre-training rather than for\n  specific tasks.\n\n- Ablating BERT's inductive bias affects different tasks at different rates.\n  This provides an additional lens into why language model pre-training helps\n  other tasks, which is particularly interesting to the natural language\n  processing community, since we lack a philosophical justification for LM\n  pre-training.\n\nAgain, we thank you for your reviews and hope you will consider allowing us to\npresent this work at ICLR 2020.\n\n[1] https://arxiv.org/abs/1811.00206 / https://dl.acm.org/citation.cfm?doid=3289602.3293898\n[2] https://dl.acm.org/citation.cfm?doid=3293883.3295701\n[3] https://openreview.net/forum?id=HJaDJZ-0W\n[4] https://openreview.net/forum?id=H1eA7AEtvS\n[5] https://openreview.net/forum?id=rJx0Q6EFPB\n[6] https://openreview.net/forum?id=SJxjVaNKwB\n[7] https://arxiv.org/abs/1902.09574\n[8] https://tinyurl.com/yjj33x45 ", "title": "Response to Review #2 and Review #3"}, "rklUVnVDiH": {"type": "rebuttal", "replyto": "H1xSiSF6tr", "comment": "Thank you so much for reviewing our paper!\n\nOur response to this review has been merged with the response to Review #2, since your concerns have a large overlap. Please view it here: https://openreview.net/forum?id=SJlPOCEKvH&noteId=HkxT-iVDor", "title": "Response to Review"}, "HJeqpvgaYr": {"type": "review", "replyto": "SJlPOCEKvH", "review": "The paper conducts a series of interesting experiments on compressing BERT and makes several conclusions. The compression technique is magnitude weight pruning based on an existing work. The paper mainly tested different compression rates and the stages when the compression can be applied. Compared to the existing work, one main contribution of the paper is to show that the BERT model can be pruned prior to fine-tuning any specific downstream tasks by 30%-40% without affecting all tested downstream tasks much. The paper is well motivated and presents interesting experimental results and conclusions. I have some concerns on their experiment details, which needs some clarification.\n\n1. the observation in 3.4 is a little counter-intuitive to me. The model has all pre-trained weights and should be able to determine, during fine-tuning, which weights to decrease to nearly zero or to abandon. However, the experimental results show that the pruning at that point produces a worse dev accuracy. For the experiments, 3 epochs is used for fine-tuning and then the pruning is applied. I was wondering what happen if you first fine-tune the model to get the best dev accuracy and prune the weights at that point. How did you choose the number 3? I am guessing that the pruning in the middle of fine-tuning process may throw away useful information too early. \u00a0 \n2. It will be helpful to show the thresholds of pruning and how these thresholds relate to the training loss and accuracy. I think the value of the thresholds can tell whether some pruning ratios are reasonable.\n3. when the authors continue training the model, for example in 3.4, the training stops when the training losses are comparable. Why did the training loss is used as the metric instead of the dev accuracy? Figure 1 right seems to show that those models are overfitting.\u00a0", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}, "H1xSiSF6tr": {"type": "review", "replyto": "SJlPOCEKvH", "review": "This work is an empirical study of testing how pruning at the pre-training stage affects subsequent transfer learning (through fine-tuning) stage. The main idea is to carefully control the amount of sparsity injected into BERT through weight magnitude pruning and study the impact on accuracy. The experimental setup is mostly well done, especially the part that disentangles the complexity restriction and information deletion. During the exploration, the authors made several interesting observations, such as 30-40% model weights do not encode any useful inductive bias, which could help shed some light for future work on both training and compressing BERT-like models.\n\nOverall, the paper is well written and explained. The goal is meaningful, and this is a sensible contribution to the ongoing interests of compressing BERT-like large models for efficient training and inference. \n\nMy major concern is on its novelty and how directly it can provide benefit to computation.  First, although the findings are interesting, the methods used in this paper are not new.  Various pruning techniques have been explored in prior work, which makes the novelty contribution of this paper somewhat limited. \n\nFurthermore, the study has mostly focused on the impact of random sparsity to accuracy. However, as it is known that it is really difficult for modern hardware to benefit from random sparsity because it leads to irregular memory accesses, which negatively impact the performance. It has been observed that speedups are very limited or can be negative even the random sparsity is >95% [1]. Therefore, it is hard to judge how inference or training can benefit from 30-40% weight sparsity. Going forward, the authors are encouraged to choose pruning methods that lead to regular memory access to avoid adversely impacting practical acceleration in modern hardware platforms.\n\n[1] Learning Structured Sparsity in Deep Neural Networks. Wen et al. NeurIPS 2016", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "BJeAGC3pYS": {"type": "review", "replyto": "SJlPOCEKvH", "review": "\nThis work explores weight pruning for BERT. It finds that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of\npruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation.\n\nMy major concern about this work is its technical innovation and value to the community.\n1. This is simply a study of model pruning for BERT. There is nothing new technically.\n\n2. It shows BERT can be pruned for 30-40% parameters. Actually, this is not surprising; instead I'm even disappointed about this result. 30-40% weight reduction does not really speed up inference much or save model size much. Besides, to handle sparse weight matrixes, one may need additional operations to use the pruned models on a modern GPU.\n\n3. Several other submissions show that BERT models can be compressed for 5-10x without accuracy loss. Comparing with this work, this paper seems to tell me that pruning is not suitable for BERT.   ", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 4}}}