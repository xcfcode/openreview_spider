{"paper": {"title": "Adaptive Generation of Unrestricted Adversarial Inputs", "authors": ["Isaac Dunn", "Hadrien Pouget", "Tom Melham", "Daniel Kroening"], "authorids": ["isaac.dunn@cs.ox.ac.uk", "hadrien.pouget@cs.ox.ac.uk", "tom.melham@cs.ox.ac.uk", "kroening@cs.ox.ac.uk"], "summary": "Training GANs to generate unrestricted adversarial examples", "abstract": "Neural networks are vulnerable to adversarially-constructed perturbations of their inputs. Most research so far has considered perturbations of a fixed magnitude under some $l_p$ norm. Although studying these attacks is valuable, there has been increasing interest in the construction of\u2014and robustness to\u2014unrestricted attacks, which are not constrained to a small and rather artificial subset of all possible adversarial inputs. We introduce a novel algorithm for generating such unrestricted adversarial inputs which, unlike prior work, is adaptive: it is able to tune its attacks to the classifier being targeted. It also offers a 400\u20132,000\u00d7 speedup over the existing state of the art. We demonstrate our approach by generating unrestricted adversarial inputs that fool classifiers robust to perturbation-based attacks. We also show that, by virtue of being adaptive and unrestricted, our attack is able to bypass adversarial training against it.", "keywords": ["Adversarial Examples", "Adversarial Robustness", "Generative Adversarial Networks", "Image Classification"]}, "meta": {"decision": "Reject", "comment": "This paper presents an interesting method for creating adversarial examples using a GAN.  Reviewers are concerned that ImageNet Results, while successfully evading a classifier, do not appear to be natural images.  Furthermore, the attacks are demonstrated on fairly weak baseline classifiers that are known to be easily broken.  They attack Resnet50 (without adv training), for which Lp-bounded attacks empirically seem to produce more convincing images.  For MNIST, they attack Wong and Kolter\u2019s \"certifiable\" defense, which is empirically much weaker than an adversarially trained network, and also weaker than more recent certifiable baselines.\n"}, "review": {"rJeOnu0jYr": {"type": "review", "replyto": "rJg46kHYwH", "review": "The paper proposes using GANs to generate unrestricted adversarial examples. They seek to generate examples that are adversarial for a specific classifier, and they do so by using class-conditional GANs and a fine-tuning loss. The fine-tuning loss consists of both the ordinary GAN loss (to fool the discriminator) as well as an adversarial loss (which rewards the GAN for generating examples misclassified by the specific classifier). The authors perform various experiments on their generated examples to check for realism and how adversarial the generated images are.\n\nI would reject this paper for two key reasons. First, I feel that the contributions are not significant enough (in comparison to the prior work of Song et. al). Second, I feel that some of the methods (and some of the writing) are not too principled.\n\nIn my opinion, unrestricted adversarial examples are significant if they can be made to be realistic. If our current deep learning models often mislabeled very realistic images, that would properly expose a big failure mode of our current models. However, if our machine learning models perform poorly on images that look fake/generated 40% of the time (which is what the authors state) and don\u2019t look too realistic to humans, it is less worrying.\n\nIn comparison to Song et. al, the authors state that their methods result in very similar results in terms of realism and how adversarial their images are (arguably, Song et. al actually produces better results in terms of being adversarial). In my opinion, the authors\u2019 claimed improvements are not significant enough, because I think realism should be the primary metric to evaluate this field. Improving speed of generation is nice, and being able to bypass a simple adversarial training procedure is interesting but not significant unless this insight is expanded upon. The results on MNIST in Fig. 5 and Fig. 6 are not too convincing, as simpler attacks that generate (arguably) more realistic images like translations and rotations [1] or L1/L2 attacks [2] (since the networks are trained for L_inf robustness) can also degrade accuracy. Finally, I can also think of another reasonable baseline that I would have liked to see the authors compare their method against. Because the authors want to attack a specific network, they could have (1) generated realistic images using a pre-trained GAN (2) used a norm-bounded attack on the specific classifier and the generated GAN images. These images could be even more realistic if the norm-bound of the attack is fairly small, and would still be able to attack specific classifiers.\n\nFinally, I am confused by the comparison to a not-fine-tuned GAN in Fig. 14/Fig. 15 and would appreciate a clarification so that I can understand the results. For example, what does it mean for intended true label = 9, target label = 0 to have 90% success in Fig. 15? Does this mean that when you try to generate a 9 with the GAN, the classifier misclassifies it as a 0 90% of the time? In particular, I\u2019m struggling to understand what the target label is for the case of the not-fine-tuned GAN.\n\nSecondly, I feel that there are many instances in the paper where the methods used are not explained in a principled way. For example, one of the key parts of this work is the fine-tuning loss function. Why does the loss function involve multiplying the ordinary GAN loss (with some additional transformation applied to it which seems unnecessary) with the adversarial loss? It seems most reasonable add the adversarial loss and the ordinary GAN loss (without the additional transformation). Is the stochastic loss selection procedure necessary? If all these peculiarities of the method are necessary, it seems that the success of this method is quite brittle.\n\nAdditional feedback:\n\n- In the intro, I think citing [3] in addition to Xu et. al is more appropriate.\n- You should refer to Figure 1 somewhere in the text of your work\n- In section 3.2, you can use \u201ccosine similarity\u201d to describe what you are doing faster.\n- When you talk about \u201cglobal optima of realistic adversarial examples\u201d and \u201clocal optimal of unrealistic adversarial examples,\u201d it sounds weird. I would try to reword this because I don\u2019t think you are trying to make a precise mathematical statement but it sounds like one when you write it this way.\n- In Table 1, I would format the numbers better to be vertically aligned\n- You should provide a citation for MixTrain on page 5\n\n[1] https://arxiv.org/abs/1712.02779,\n[2] https://arxiv.org/abs/1905.01034\n[3] https://arxiv.org/abs/1802.00420", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 2}, "rJxptTW6tH": {"type": "review", "replyto": "rJg46kHYwH", "review": "======== update ========\nI have read the authors' response and it has addressed most of my concerns. I am glad to see the authors' experiments on online adversarial training. \n\nHowever, there is one additional concern that I didn't realize previously. Currently the performance of adversarial training is measured in \"success rates\". However it seems to me this success rates were not computed using human evaluation (since the authors claim once the classifier is finished training, the attack success rate can be larger than 99%). I would have changed my score to 8 if either 1) some adversarial images from the generator were included after finishing adversarial training or 2) success rates using human evaluation is reported. Unfortunately, I only realized this after the author rebuttal period, and the authors didn't have the chance to address this.\n\nThat being said, I feel this paper still presents interesting contribution to the field. I am still largely in favor of the acceptance of this paper, and will remain my rating of 6 for now. If this paper gets accepted, I strongly encourage the authors to address the concern I mentioned above in their camera ready.\n\n\n======== original reviews ========\n\nThis paper proposes a novel method on generating unrestricted adversarial examples by finetuning GANs. The authors have conducted comprehensive experiments on evaluating the advantages of their approach. They demonstrated that their attack is harder to mitigate using adversarial training, produces unrestricted adversarial examples faster than existing methods, and can generate some unrestricted adversarial examples for complex high-dimensional datasets such as ImageNet.\n\nI feel although the approach is straightforward, the authors have done a good job in motivating the method and have demonstrated its advantages via a good cohort of experiments. I like how the authors motivated finetuning in section 3.2, and I am glad that the authors have conducted ablative experiments to support their arguments in section 4.4. The experiments on adversarial training are especially interesting, since previous work hasn't considered this straightforward defense against unrestricted adversarial attacks. I am also glad that the authors can generate unrestricted adversarial examples for data as complicated as ImageNet images using the latent technique in GANs. Although still not perfect, some of the unrestricted adversarial examples on ImageNet are surprisingly good to the sense that they may be used as practical attacks.\n\nThe writing is great, and it is a pleasure to read this paper. \n\nI do have some suggestions and questions for further improvement of the paper, and I strongly recommend the authors to address those before publication.\n\n- Section 3 is lacking an explicit form of the combined objective function. Currently some loss functions such as $l_ordinary$, $l_targeted$, $l_d$ and $l_finetune$ are only defined in Figure 1 but not in the main text. It is not clear their explicit mathematical form.\n\n- In section 3.2, it is better to also mention the ablative study you did later in section 4.4. \n\n- In section 4.1, the authors showed nearest neighbors to some of the unrestricted adversarial examples they generated. It is more convincing to have some quantitive results of that. For example, what is the average minimum distance to training data for a group of 10000 unrestricted adversarial examples? In addition, what is the distance function used in computing nearest neighbors? Did you use Euclidean distance? If so, it would be better to also have results using distances computed in the feature space of a pre-trained convolutional network.\n\n- In section 4.2, the adversarial training was done by alternating two phases of training rounds. I am wondering whether this makes the classifier harder to adapt to the newly generated unrestricted adversarial examples? Can you use some procedure more similar to traditional adversarial training, i.e., the attacker and the classifier are learned together at each step? \n\n- Song et al. require 100-500 iterations to generate an adversarial example, whereas your approach only need one iteration. Why is your approach 400 to 2000x more efficient? What is the additional reason that speeds up your approach?\n\n- In section 4.5 line 1, the word \"replies\" was repeated twice.\n\n\n\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "SkgDv7mhoS": {"type": "rebuttal", "replyto": "rJxomm72or", "comment": "Adaptivity is Significant\n\nIf we interpret your comment correctly, you state that adaptivity is a natural consequence of generating unrestricted adversarial examples. This is not the case: it is possible to use a fixed, easy-to-mitigate attack which is not constrained to norm-balls around test points. (Note that \u2018unrestricted\u2019 currently means just \u2018not restricted to an $L_p$ norm ball\u2019.) Our experiments show that Song et al.\u2019s method is such an easy-to-mitigate method, and that ours is not.\n\nAnother perspective on this is that all adversarial example algorithms so far involve a certain search procedure in image space for an example that fools the classifier, whereas our approach entails an optimisation over the weights of a generator, in effect searching for an adversarial example generation procedure which is effective against the target network.\n\nAlthough there is room for improvement regarding the realism of the generated images, it seems likely to us that further tuning and development of the procedure - and possibly scaling up the compute used - will remedy this. The tremendous pace of improvement in GAN image quality since 2014 is strong evidence in favour of this hypothesis.\n\nIn short, the significance of introducing the first adaptive method for unrestricted adversarial example generation outweighs any current minor realism limitations imposed by GAN training difficulties.\n", "title": "(Part 2)"}, "rJxomm72or": {"type": "rebuttal", "replyto": "HJeH1uLssB", "comment": "Thank you for taking the time to receptively read and reflect upon our comments - we are of course very glad that you feel able to raise your score.\n\nWe are also grateful that you have been so specific and clear about your reasons for not yet recommending acceptance, which once again makes it easy for us to improve our paper and respond to any points of disagreement. In particular, we believe we can allay your remaining concerns.\n\nImageNet Realism: Lower-Quality BigGAN Checkpoint\n\nYou are correct to point out that our ImageNet images are much less realistic than those published in the BigGAN paper. However, much of this is due to the underlying GAN that we are using. While we are using BigGAN, we can only use the publicly available code and checkpoints [1], which are not the same as those used to produce the images in the published BigGAN paper. In particular, there are two checkpoints available, both taken during a single training run: one after 100,000 iterations, and another taken later, \u201cjust before collapse\u201d. This later checkpoint achieves an inception score of 97, considerably less than the score of 166.5 reported in the paper. However, we actually use the earlier checkpoint, with even worse performance, since the author recommends that it \u201cmay be easier to fine-tune\u201d. Note that the resolution for these checkpoints is also 128x128, whereas the impressive images in the BigGAN paper are 512x512.\n\nImageNet Realism: Hardware Limitations\n\nIn addition, the codebase we use is intended for \u201c4-8 GPUs\u201d [1]. However, we only have access to one 16GB GPU. This means that the 15 is the greatest minibatch size we can use without running out of memory. This causes problems, since \u201ca small batch leads to inaccurate estimation of the batch statistics, and reducing batch normalisation\u2019s batch size increases the model error dramatically\u201d [2]. This has caused problems for others attempting to use the same codebase [3], and the author has warned that using a smaller batch size \u201cwill likely negatively impact model performance\u201d [4] and for this reason \u201cthis is not really a model for small hardware\u201d [5]. Unfortunately, this is true for all state-of-the-art ImageNet GANs.\n\nIn an attempt to clarify to what extent the unrealistic ImageNet results are a product of adversarial finetuning, rather than these external limitations, we have added images to the paper generated by the BigGAN on our machine after training (not adversarial finetuning) for 10 gradient steps. These can now be found in Appendix A. To our eyes, it appears that these images are little better than our adversarial examples, and so adversarial finetuning is not the primary cause of the unrealistic images (such as deformed dogs).\n \nMNIST Realism: Interpretation of Results\n\nYou correctly point out that MNIST is a simple dataset. However, this in fact makes our results more impressive, not less impressive. The simplicity of classification means that the adversarially-robust classifiers are much better than for any other dataset, so finding adversarial examples is more challenging. Equally importantly, the highly-structured images (black background with simple white figures) makes it relatively easy to spot deviations from the usual data distribution. To be clear, our 50% figure is not that 50% of the time, human judges do not think the image looks realistic, but rather that 50% of the time, human judges are unable to distinguish our adversarial examples from examples in the dataset. This is not a trivial achievement.\n\nOur key message regarding realism is that unrestricted adversarial examples are valuable not only when they are indistinguishable from real data (for us, 50% of the time on MNIST), but also when they are unambiguous inputs for which a human could give a meaningful answer (as you point out). We achieve this on MNIST, and although you are correct to identify that GAN training issues hinder this on ImageNet, we still feel that these results are a significant step in a useful direction.\n\n[1] https://github.com/ajbrock/BigGAN-PyTorch\n[2] https://arxiv.org/abs/1803.08494\n[3] https://github.com/ajbrock/BigGAN-PyTorch/issues/40\n[4] https://github.com/ajbrock/BigGAN-PyTorch/issues/39\n[5] https://github.com/ajbrock/BigGAN-PyTorch/issues/31\n", "title": "Thank you - we believe we can address your further concerns (part 1)"}, "ryeTAIYdsr": {"type": "rebuttal", "replyto": "rJg46kHYwH", "comment": "Thank you to all three reviewers for your thoughtful and constructive comments. We have responded in detail to each point made, and have uploaded an updated version of our paper incorporating your feedback. The key changes that have been made are:\n- Removal of out-of-date and confusing explanatory diagram identified by reviewers #1 and #2; redrafting of exposition in section 3.1 which we hope is a much clearer explanation of our method.\n- Addition of additional adversarial training experiment (4.2) suggested by reviewer #1.\n- Addition of further ablative experiments (4.4) for strategies outlined in section 3.3 to address concerns of reviewer #3.\n- Addition of baseline (4.4) suggested by reviewer #3.\n- Smaller corrections and writing improvements.\n\nWe hope that this improved paper together with our responses to your individual comments will reassure you and allow you to increase your scores.\n", "title": "Summary of Improvements Made"}, "SylNpBFusr": {"type": "rebuttal", "replyto": "Syl3VrKOsB", "comment": "Ablation Study for Section 3.3\n\nYou correctly point out that we have not made clear how necessary each training strategy is. We have therefore carried out ablative experiments demonstrating the effect of omitting each in turn, reported in section 4.4 and appendix L. In short, pretraining and use of an attack rate other than 1 are not strictly necessary, but improve performance. Use of the naive loss function of simply summing the two loss terms degrades performance so badly as to be unusable. GANs are notoriously difficult to train at the best of times; adding an extra loss term which conflicts with the ordinary loss makes this even more difficult; strategies to improve training are helpful.\n\nOther Baseline\n\nThank you for the suggestion to compare to norm-bounded perturbation attacks on images generated by a pretrained GAN. We have implemented this baseline, and found that the results are only slightly more effective than norm-bounded perturbations on the test set; this makes sense, since the pre-trained generator is supposed to have learnt this data distribution.\n\nComparison to Non-Finetuned GAN\n\nWe apologise for the confusion regarding the pretrained-only baseline - our description was unclear. To carry out the baseline, we first generate many examples with a particular intended true label, then filter these to keep only those which match the \u2018target label\u2019, then report the proportion of these are judged to indeed visually resemble the intended true label. We have updated the paper clarify this: please do let us know if we have still caused confusion.\n\nAdditional Feedback\n\nThe revision we have uploaded includes all of your suggestions of minor improvements to writing, referencing and formatting, for which we are grateful.\n\nWe believe that this response fully addresses all the concerns you have raised - we look forward to hearing from you, either to raise your score or to continue the conversation with any further concerns you have.\n", "title": "Response to Reviewer #3 (Part 2)"}, "Syl3VrKOsB": {"type": "rebuttal", "replyto": "rJeOnu0jYr", "comment": "Thank you for your detailed and thoughtful review. Although we are disappointed about your recommendation, we are grateful for the specificity and cogency of your feedback, which makes it especially easy to either improve our paper or respond to any points of disagreement.\n\nAs we understand it, you have raised two specific concerns regarding the significance of our work: that unrestricted adversarial examples are significant only if they are realistic, and that our work is too incremental in comparison to the prior work of Song et al. We address these separately.\n\nSignificance of Results: Realism of Unrestricted Adversarial Examples\n\nIn criticising the realism of our generated adversarial examples, we believe you are directly addressing an important fundamental question: what is the purpose of adversarial examples research? Gilmer et al.\u2019s seminal paper [1] on this subject identifies two motivations: security concerns, and improving understanding and capabilities of our models.\n\nLet us first consider security, for which it is essential to specify the precise threat model being used; Gilmer et al. suggest a compelling taxonomy of these. For \u2018non-suspicious attacks\u2019, the requirement is that the attack input must not be identifiable as being adversarial. Our experiments find that human judges are unable to identify which image is an unrestricted adversarial example 50% of the time on average (Fig. 5 in revised paper). Although this is not perfect (90%), this is still a far higher attack success rate than is desirable.\n\nHowever, this is not the only security threat model of interest. For Gilmer et al.\u2019s \u2018attacks with content constraints\u2019 and \u2018attacks without input constraints\u2019 threat models, there is no requirement that the input be realistic, just that it fools the target system and maintains the correct semantics. All of the successful unrestricted adversarial examples in the paper would be a threat in such scenarios.\n\nBut security implications are not the only motivation. Another framing of an \u2018adversarial example\u2019 is an input for which the network generalises in a different way to a human, therefore reaching the \u2018incorrect\u2019 answer. It would be desirable to have models that generalise correctly on all inputs, not merely on inputs indistinguishable from training data; unrestricted approaches such as ours need not have this property to identify interesting failures in generalisation which can then be studied by the community.\n\nIn short, unrestricted adversarial examples need not be so realistic as to be indistinguishable from training data in order for them to represent a failure of generalisation which we would like to correct. Our contribution is not an improvement in realism or success rate, but a novel method with other advantages.\n\nSignificance of Results: Comparison to Prior Work\n\nOur work is not incremental over Song et al. since it presents an entirely new method, with several important advantages.\n\nThe most fundamental of these is that our method is adaptive. While $L_1$/$L_2$ attacks, rotation/translation attacks and Song et al.\u2019s attack are all able to attack a network defended against $L_\\infty$ attacks, they all share a weakness: they are not adaptive. That is, their attack procedure is fixed, and does not depend on the target network. It seems likely that all such attacks can be mitigated by adversarial training, since the classifier can learn not to rely on features targeted by that particular threat. Empirical studies show that this is true for $L_p$ perturbations [2], translations/rotations [3], and Song et al. (section 4.2).\n\nConversely, our method is adaptive, since the generator is essentially finetuned to find a set of features to attack that the target classifier is reliant upon; this search is not constrained by an $L_p$ norm or any other requirement, and so the classifier is unable to anticipate all kinds of attack that the generator may learn next. Our preliminary attempts at defence against our attack have failed; we believe this challenge to be significant enough to be left as a fruitful direction for future work.\n\nAs described in section 5.1, our method is also three orders of magnitude more efficient, demonstrably scales to a dataset orders of magnitude more complex than Song et al, and allows any existing GAN to be used out-of-the-box.\n\n[1] https://arxiv.org/abs/1807.06732\n[2] https://arxiv.org/abs/1908.08016\n[3] https://arxiv.org/pdf/1905.01034\n", "title": "Response to Reviewer #3 (Part 1)"}, "HklaLEY_iS": {"type": "rebuttal", "replyto": "rkgqAJxpFr", "comment": "Thank you for your detailed and thoughtful review. We are especially glad to read that you find the experimental evaluation extensive and particularly enjoyed the demonstration that our method is able to find new ways of fooling standard adversarial training (which is very effective at mitigating the state of the art).\n\nWe are sorry to hear that some parts of our exposition - especially Figure 1, which was indeed out-of-date - caused you (and another reviewer) some confusion. We have removed Figure 1 and rewritten our exposition in light of your feedback, and hope that this revision greatly improves the clarity of this point.\n\nTo address your concern directly, any conditional GAN can be used with our method, with no other restrictions on the architecture. Although it is completely standard for a contemporary GAN to be class-conditional, we have clarified this condition in our revision.\n\nWe have also incorporated your minor writing improvements, for which we are grateful.\n\nWe believe that this response fully addresses all the concerns you have raised - we look forward to hearing from you, either to raise your score or to continue the conversation with any further concerns you have.\n", "title": "Response to Reviewer #2"}, "rJgbx4Y_sH": {"type": "rebuttal", "replyto": "rJxptTW6tH", "comment": "Thank you for your detailed and thoughtful review. We are especially glad to read that you find the method to be well-motivated, the empirical evaluation to be comprehensive, and the writing to be lucid.\n\nWe are grateful for your suggestions of minor improvements to the clarity of the paper. We have uploaded a revision of the paper with these changes incorporated, including clarification of the overall objective function.\n \nYou are correct to point out that Song et al. require 100-500 iterations to generate an adversarial example, yet we claim a 400-2000x efficiency improvement. The extra factor of four is because our method requires only a single forward pass through the generator network, while each iteration of Song et al. requires both forward and backward passes through both the generator and classifier networks. We hope that our revised wording makes this more explicit.\n\nWhile it could be interesting to repeat the nearest-neighbour calculations with a larger sample size, ten handpicked images are sufficient for a sanity check that our examples really are unrestricted. We believe a sanity check is all that is required: the only case in which our generated adversarial examples would be within a typical $L_p$ norm radius is if the generator were simply memorising dataset images. As a result we have prioritised other experiments and improvements during this time-limited response period.\n\nYour proposal of an adversarial training procedure in which the classifier is trained simultaneously online with the GAN finetuning is very sensible. We have implemented this experiment (see section 4.2), and found that our method is again able to easily evade this adversarial training procedure; the generator is always able to find a new way of fooling the classifier, since it has no restrictions.\n\nWe believe that this response fully addresses all the concerns you have raised - we look forward to hearing from you, either to raise your score or to continue the conversation with any further concerns you have.\n", "title": "Response to Reviewer #1"}, "rkgqAJxpFr": {"type": "review", "replyto": "rJg46kHYwH", "review": "This paper presents a GAN architecture that generates realistic adversarial\ninputs that fool a targeted classifier.  Adversarial inputs are unrestricted:\nthey may be any realistic images that humans will often classify as real\nexamples of the intended class, whereas the target model misclassifies them.\nThe novelty is that they finetune the generator itself during training, the\nmethod can be applied to a variety of GAN architectures, and the method is fast.\n\nTricks used to successfully train the GAN are clearly described, and the\nexperimental evaluation was of good scope, covering a good selection of\nexperiments.  I particularly enjoyed the short Section 4.2 and Fig 7a+b, where\nthey show that a local defense can always be fooled somewhere else along\nthe input manifold of that class.\n\nWhile the modifications to existing solutions may at first seem minor, they\nhave significant impact in applicability, effectiveness and speed of generating\nunrestricted adversarial images.  So I think this paper can be accepted.\n\nI had a bit of avoidable confusion in the introductory sections.  Figure 1\ndescribing the GAN is never referred to.  It includes components not exactly\nagreeing with my naive expectations from surrounding text.  Are any Fig. 1 features\noptional?  It would help to highlight the novel elements in Fig. 1.  Or does\nFig.1 correspond perhaps to the combined GAN elements in Section 4 (\"In our\nexperiments, we combine three ...\").  My uncertainty was really relieved only\nby the time I got to Related Work and Appendix E :(\n\nThe main claims seemed well supported by experiments, apart from claim 3\n(applicability to \"any\" checkpointed GAN codebase). Might the scope of their\napproach also be clarified by clearly identifying required and optional GAN\ncomponents in Fig. 1?\n\n---- misc comments ----\nSome sentences were long and difficult to parse:\n- 4.1: \"Our method generates..., else ....\"  Perhaps make the else clause a second sentence.\n- 4.2: \"Image quality as measured ....\" length and references made this difficult to read. Can you\nrewrite as separate shorter sentences?\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}}}