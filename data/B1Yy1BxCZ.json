{"paper": {"title": "Don't Decay the Learning Rate, Increase the Batch Size", "authors": ["Samuel L. Smith", "Pieter-Jan Kindermans", "Chris Ying", "Quoc V. Le"], "authorids": ["slsmith@google.com", "pikinder@google.com", "chrisying@google.com", "qvl@google.com"], "summary": "Decaying the learning rate and increasing the batch size during training are equivalent.", "abstract": "It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate $\\epsilon$ and scaling the batch size $B \\propto \\epsilon$. Finally, one can increase the momentum coefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to 76.1% validation accuracy in under 30 minutes.", "keywords": ["batch size", "learning rate", "simulated annealing", "large batch training", "scaling rules", "stochastic gradient descent", "sgd", "imagenet", "optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "Pros:\n+ Nice demonstration of the equivalence between scaling the learning rate and increasing the batch size in SGD optimization.\n\nCons:\n- While reporting convergence as a function of number of parameter updates is consistent, the paper would be more compelling if wall-clock times were given in some cases, as that will help to illustrate the utility of the approach.\n- The paper would be stronger if additional experimental results, which the authors appear to have at hand (based on their comments in the discussion) were included as supplemental material.\n- The results are not all that surprising in light of other recent papers on the subject.\n"}, "review": {"SyoR0j7BG": {"type": "rebuttal", "replyto": "H17TutI4G", "comment": "We apologize for any confusion. We do not claim that most big data problems can be solved using single machine SGD; our experiments use distributed (but synchronous) SGD. We will edit the text to clarify that the incentive to use asynchronous training is reduced when the synchronous batch size can be scaled to a substantial fraction of the training set size (as observed in our paper). Although we expect this to be commonly the case, we acknowledge that it is not guaranteed to occur for all models/datasets.\n\nWe will add our additional results reporting wall-clock times on ResNet-50 (likely in an appendix). We will also update the section 5.2 to provide median of 5 runs for each case shown, and add experiments at a wider range of learning rate values to the appendix.\n\nWe state that parameter updates provide a measure of the training speed if one assumes near-perfect parallelism, since this clarifies how readers should interpret our results depending on their own hardware/scaling efficiency. We note that a number of recent works have achieved extremely good scaling. Eg Goyal et al. [1] report large batch scaling efficiency of 90% while Akiba et al. [2] report scaling efficiency of 80%; both compared to a single node of 8 GPUs. On our own hardware we can increase the batch size from 256 to 16k with >95% scaling efficiency.\n\n[1] \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\", Goyal et al., 2017, arXiv:1706.02677\n[2] \"Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes\", Akiba et al., 2017, arXiv:1711.04325", "title": "Clarifications"}, "r1SNNxFlf": {"type": "review", "replyto": "B1Yy1BxCZ", "review": "The paper analyzes the the effect of increasing the batch size in stochastic gradient descent as an alternative to reducing the learning rate, while keeping the number of training epochs constant. This has the advantage that the training process can be better parallelized, allowing for faster training if hundreds of GPUs are available for a short time. The theory part of the paper briefly reviews the relationship between learning rate, batch size, momentum coefficient, and the noise scale in stochastic gradient descent. In the experimental part, it is shown that the loss function and test accuracy depend only on the schedule of the decaying noise scale over training time, and are independent of whether this decaying noise schedule is achieved by a decaying learning rate or an increasing batch size. It is shown that simultaneously increasing the momentum parameter and the batch size also allows for fewer parameters, albeit at the price of some loss in performance.\n\nCOMMENTS:\n\nThe paper presents a simple observation that seems very relevant especially as computing resources are becoming increasingly available for rent on short time scales. The observation is explained well and substantiated by clear experimental evidence. The main issue I have is with the part about momentum. The paragraph below Eq. 7 provides a possible explanation for the performance drop when $m$ is increased. It is stated that at the beginning of the training, or after increasing the batch size, the magnitude of parameter updates is suppressed because $A$ has to accumulate gradient signals over a time scale $B/(N(1-m))$. The conclusion in the paper is that training at high momentum requires additional training epochs before $A$ reaches its equilibrium value. This effect is well known, but it can easily be remedied. For example, the update equations in Adam were specifically designed to correct for this effect. The mechanism is called \"bias-corrected moment estimate\" in the Adam paper, arXiv:1412.6980. The correction requires only two extra multiplications per model parameter and update step. Couldn't the same or a very similar trick be used to correctly rescale $A$ every time one increases the batch size? It would be great to see the equivalent of Figure 7 with correctly rescaled $A$.\n\nMinor issues:\n* The last paragraph of Section 5 refers to a figure 8, which appears to be missing.\n* In Eqs. 4 & 5, the momentum parameter $m$ is not yet defined (it will be defined in Eqs. 6 & 7 below).\n* It appears that a minus sign is missing in Eq. 7. The update steps describe gradient ascent.\n* Figure 3 suggests that most of the time between the first and second change of the noise scale (approx. epochs 60 to 120) are spent on overfitting. This suggests that the number of updates in this segment was chosen unnecessarily large to begin with. It is therefore not surprising that reducing the number of updates does not deteriorate the test set accuracy.\n* It would be interesting to see a version of figure 5 where the horizontal axis is the number of epochs. While reducing the number of updates allows for faster training if a large number of parallel hardware instances are available, the total cost of training is still governed by the number of training epochs.\n* It appears like the beginning of the second paragraph in Section 5.2 describes figure 1. Is this correct?", "title": "A simple, relevant observation as computing resources are becoming increasingly available for rent", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1i1vxqgz": {"type": "review", "replyto": "B1Yy1BxCZ", "review": "The paper represents an empirical validation of the well-known idea (it was published several times before) \nto increase the batch size over time. Inspired by recent works on large-batch studies, the paper suggests to adapt the learning rate as a function of the batch size.\n\nI am interested in the following experiment to see how useful it is to increase the batch size compared to fixed batch size settings. \n\n1) The total budget / number of training samples is fixed. \n2) Batch size is scheduled to change between B_min and B_max\n3) Different setting of B_min and B_max>=B_min are considered, e.g., among [64, 128, 256, 512, ...] or [64, 256, 1024, ...] if it is too expensive.\n4) Drops of the learning rates are scheduled to happen at certain times represented in terms of the number of training samples passed so far (not parameter updates).\n5) Learning rates and their drops should be rescaled taking into account the schedule of the batch size and the rules to adapt learning rates in large-scale settings as by Goyal. ", "title": "Useful empirical validation.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJJrhg5lf": {"type": "review", "replyto": "B1Yy1BxCZ", "review": "## Review Summary\n\nOverall, the paper's paper core claim, that increasing batch sizes at a linear\nrate during training is as effective as decaying learning rates, is\ninteresting but doesn't seem to be too surprising given other recent work in\nthis space. The most useful part of the paper is the empirical evidence to\nbackup this claim, which I can't easily find in previous literature. I wish\nthe paper had explored a wider variety of dataset tasks and models to better\nshow how well this claim generalizes, better situated the practical benefits\nof the approach (how much wallclock time is actually saved? how well can it be\nintegrated into a distributed workflow?), and included some comparisons with\nother recent recommended ways to increase batch size over time.\n\n\n## Pros / Strengths\n\n+ effort to assess momentum / Adam / other modern methods\n\n+ effort to compare to previous experimental setups\n\n\n## Cons / Limitations\n\n- lack of wallclock measurements in experiments\n\n- only ~2 models / datasets examined, so difficult to assess generalization\n\n- lack of discussion about distributed/asynchronous SGD\n\n\n## Significance\n\nMany recent previous efforts have looked at the importance of batch sizes\nduring training, so topic is relevant to the community. Smith and Le (2017)\npresent a differential equation model for the scale of gradients in SGD,\nfinding a linear scaling rule proportional to eps N/B, where eps = learning\nrate, N = training set size, and B = batch size. Goyal et al (2017) show how\nto train deep models on ImageNet effectively with large (but fixed) batch\nsizes by using a linear scaling rule.\n\nA few recent works have directly tested increasing batch sizes during\ntraining. De et al (AISTATS 2017) have a method for gradually increasing batch\nsizes, as do Friedlander and Schmidt (2012). Thus, it is already reasonable to\npractitioners that the proposed linear scaling of batch sizes during training\nwould be effective.\n\nWhile increasing batch size at the proposed linear scale is simple and seems\nto be effective, a careful reader will be curious how much more could be\ngained from the backtracking line search method proposed in De et al.\n\n\n## Quality\n\nOverall, only single training runs from a random initialization are used. It\nwould be better to take the best of many runs or to somehow show error bars,\nto avoid the reader wondering whether gains are due to changes in algorithm or\nto poor exploration due to bad initialization. This happens a lot in Sec. 5.2.\n\nSome of the experimental setting seem a bit haphazard and not very systematic.\nIn Sec. 5.2, only two learning rate scales are tested (0.1 and 0.5). Why not\nexamine a more thorough range of values?\n\nWhy not report actual wallclock times? Of course having reduced number of\nparameter updates is useful, but it's difficult to tell how big of a win this\ncould be.\n\nWhat about distributed SGD or asyncronous SGD (hogwild)? Small batch sizes\nsometimes make it easier for many machines to be working simultaneously. If we\nscale up to batch sizes of ~ N/10, we can only get 10x speedups in\nparallelization (in terms of number of parameter updates). I think there is\nsome subtle but important discussion needed on how this framework fits into\nmodern distributed systems for SGD.\n\n\n## Clarity\n\nOverall the paper reads reasonably well.\n\nOffering a related work \"feature matrix\" that helps readers keep track of how\nprevious efforts scale learning rates or minibatch sizes for specific\nexperiments could be valueable. Right now, lots of this information is just\nprovided in text, so it's not easy to make head-to-head comparisons.\n\nSeveral figure captions should be updated to clarify which model and dataset\nare studied. For example, when skimming Fig. 3's caption there is no such\ninformation.\n\n## Paper Summary\n\nThe paper examines the influence of batch size on the behavior of stochastic\ngradient descent to minimize cost functions. The central thesis is that\ninstead of the \"conventional wisdom\" to fix the batch size during training and\ndecay the learning rate, it is equally effective (in terms of training/test\nerror reached) to gradually increase batch size during training while fixing\nthe learning rate. These two strategies are thus \"equivalent\". Furthermore,\nusing larger batches means fewer parameter updates per epoch, so training is\npotentially much faster.\n\nSection 2 motivates the suggested linear scaling using previous SGD analysis\nfrom Smith and Le (2017). Section 3 makes connections to previous work on\nfinding optimal batch sizes to close the generaization gap. Section 4 extends\nanalysis to include SGD methods with momentum.\n\nIn Section 5.1, experiments training a 16-4 ResNet on CIFAR-10 compare three\npossible SGD schedules: * increasing batch size * decaying learning rate *\nhybrid (increasing batch size and decaying learning rate) Fig. 2, 3 and 4 show\nthat across a range of SGD variants (+/- momentum, etc) these three schedules\nhave similar error vs. epoch curves. This is the core claimed contribution:\nempirical evidence that these strategies are \"equivalent\".\n\nIn Section 5.3, experiments look at Inception-ResNet-V2 on ImageNet, showing\nthe proposed approach can reach comparable accuracies to previous work at even\nfewer parameter updates (2500 here, vs. \u223c14000 for Goyal et al 2007)\n", "title": "reasonable empirical evidence for a not-too-surprising claim / could improve with more diverse set of tasks, wallclock metrics", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkCOZEamG": {"type": "rebuttal", "replyto": "B1Yy1BxCZ", "comment": "We have uploaded an updated manuscript, responding to the comments of the referees. We were delighted that all three reviewers recommended the paper be accepted. As well as fixing some minor typos, the main changes are:\n\n1) We have edited the final paragraph of section 4, to clarify that the performance losses with large batches/momentum coefficients are not resolved by using initialization bias correction suggested by reviewer 3. When the momentum coefficient is too large, it takes many epochs for the accumulation to forget old gradients, and this prevents SGD from responding to changes in the loss landscape.\n\n2) We clarify in section 5.3 that we chose not to include wall-clock times, since these are not comparable across different hardware/software frameworks. As we stated in response to reviewer 2, we have confirmed that increasing batch sizes can be used to reduce wall-clock time.\n\n3) We include a brief discussion of asynchronous SGD in the related work section.\n", "title": "Updated manuscript"}, "SJdPl-KMf": {"type": "rebuttal", "replyto": "r1SNNxFlf", "comment": "We thank the reviewer for their positive review, \n\nWe will edit our discussion of momentum in section 4 to explain the problem more clearly. We are currently running experiments to double check, but we do not believe that the \u201cbias-corrected moment estimate\u201d trick will remove the performance gap when training at very large momentum coefficient. This is for two reasons: \n\n1) When one uses momentum, one introduces a new timescale into the dynamics, the time required for the direction of the parameter updates to change/forget old gradients. When one trains with large batch sizes and large momentum coefficients, this timescale becomes several epochs long. This invalidates the scaling rules, which assume this timescale is negligible. This issue arises throughout training, not just at initialization/after changing the noise scale. \n\n2) The \u201cbias-corrected moment estimate\u201d ensures that the expected magnitude of the parameter update at the start of training is correct, but it does not ensure that the variance in this parameter update is correct. As a result, bias correction introduces a very large noise scale at the start of training, which decays as the bias correction term falls. The same issue will arise if we used bias correction to reset the accumulation during training at a noise scale step; in fact it would temporarily increase the noise scale every time we try to reduce it.\n\nResponding to the minor issues raised: \ni) Our apologies, this should be figure 7b, we will fix it. \nii) The momentum coefficient is defined in the first line of the paragraph following eqns 4/5. \niii) Yes, we will fix this. \niv) We will check our conclusions hold when we reduce the number of epochs here, however we keep to pre-existing schedules in the paper to emphasize that our techniques can be applied without hyper-parameter tuning. \nv) All curves in figure 5 saw the same number of training epochs. \nvi) The first two schedules described in this paragraph match figure 1, however the following two schedules are new.", "title": "Response to review"}, "HkYkRxtzz": {"type": "rebuttal", "replyto": "SJJrhg5lf", "comment": "We thank the reviewer for their positive assessment of our work. \nTo respond to the comments raised: \n\nThe wall clock time is primarily determined by the hardware researchers have at their disposal; not the quality of the research/engineering they have done. In the paper we choose to focus on the number of parameter updates, because we believe this is the simplest and most meaningful scientific measure of the speed of training. Assuming one can achieve perfect parallelism, the number of parameter updates and the wall clock time are identical. However we can confirm here that, using the increasing batch size trick, we were able to train ResNet-50 to 76.1% validation accuracy on ImageNet in 29 minutes. With a constant batch size, we achieve comparable accuracy in 44 minutes (replicating the set-up of Goyal et al.). This significantly under-estimates the gains available, as we only increased the batch size to 16k in these experiments, not 64k as in the paper. \n\nOne of the goals of large batch training is to remove the need for asynchronous SGD, which tends to slightly reduce test set accuracies. Since we are now able to scale the batch size to several thousand training examples and train accurate ImageNet models in under an hour with synchronous SGD, the incentive to use asynchronous training is much reduced. Intuitively, asynchronous SGD behaves somewhat like an increased momentum coefficient, averaging the gradient over recent parameter values. \n\nWe chose to focus on clarity, rather than including many equivalent experiments under different architectures, however we have checked that our claims are also valid for a DNN on MNIST and ResNet-50 on ImageNet. This is also why we do not present an exhaustive range of learning rate scales in section 5.2; we wanted to keep the figures clean and easy to interpret. It's worth noting that our observations also match theoretical predictions. We will update the figure captions to clarify which model/dataset they refer to.", "title": "Response to review"}, "B1FJsgFfG": {"type": "rebuttal", "replyto": "B1i1vxqgz", "comment": "We thank the reviewer for their positive review. \n\nWe'd like to emphasize that our paper verifies a stronger claim than previous works. While previous papers have proposed increasing the batch size over time instead of decaying the learning rate, our work demonstrates that we can directly convert decaying learning rate schedules into increasing batch size schedules and vice-versa; obtaining identical learning curves on both training and test sets for the same number of training epochs seen. To do so, we replace decaying the learning rate by a factor q by increasing the batch size by the same factor q. This strategy allows us to convert between small and large batch training schedules without hyper-parameter tuning, which enabled us to achieve efficient large batch training, with batches of 65,000 examples on ImageNet. \n\nWe may have misunderstood, but we believe that we provided the experiment suggested in the review in section 5.1 (figures 1,2 and 3). We consider three schedules, each of which decay the noise scale by a factor of 5 after ~60, ~120 and ~160 epochs. Each schedule sees the same number of training examples. The \u201cdecaying learning rate schedule\u201d achieves this by using a constant batch size of 128 and decaying the learning rate by a factor of 5 at each step. The \u201cincreasing batch schedule\u201d holds the learning rate fixed and increases the batch size by a factor of 5 at the same steps. Finally the \u201chybrid\u201d schedule is mix of the two strategies. All three curves achieve identical training curves in terms of number of examples seen (figure 2a), and achieve identical final test accuracy (figure 3a). In this sense, decaying the learning rate and increasing the batch size are identical; they require the same amount of computation to reach the same training/test accuracies. However if one increases the batch size one can benefit from greater parallelism to reduce wall clock time.", "title": "Response to review"}, "BJtuH2Qlf": {"type": "rebuttal", "replyto": "rJIErsI1M", "comment": "Thank you for your interest in our work!\n\nWe would like to emphasize that the method of increasing the batch size during training instead of decaying the learning rate is not an alternative to large batch training, it is complimentary. Large batch training is achieved by increasing the initial learning rate and linearly scaling the batch size, thus holding the SGD noise scale constant. Meanwhile we propose increasing the batch size during training at constant learning rate, in order to maintain the same noise scale progression obtained by a decaying learning rate. \n\nWe showed in figure 5 that we could simultaneously increase the initial learning rate and batch size by a factor of 5, and also replace a decaying learning rate with an increasing batch size schedule. This did not cause any reduction in test performance, achieving final test accuracy of 94.5%. In response to your question, we attempted to instead increase the initial learning rate and batch size by a factor of 25 with a constant batch size and decaying learning rate schedule. The test set accuracy drops to 93.2%. We will add these results to the appendix after the review process. \n\nBest wishes,", "title": "Extra results in appendix after review"}}}