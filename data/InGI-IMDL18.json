{"paper": {"title": "Secure Federated Learning of User Verification Models", "authors": ["Hossein Hosseini", "Hyunsin Park", "Sungrack Yun", "Christos Louizos", "Joseph Soriaga", "Max Welling"], "authorids": ["~Hossein_Hosseini4", "hyunsinp@qti.qualcomm.com", "~Sungrack_Yun1", "~Christos_Louizos1", "jsoriaga@qti.qualcomm.com", "mwelling@qti.qualcomm.com"], "summary": "We propose a private and secure method for training user verification models in federated setup.", "abstract": "We consider the problem of training User Verification (UV) models in federated setup, where the conventional loss functions are not applicable due to the constraints that each user has access to the data of only one class and user embeddings cannot be shared with the server or other users.  To address this problem, we propose Federated User Verification (FedUV), a framework for private and secure training of UV models. In FedUV, users jointly learn a set of vectors and maximize the  correlation  of  their  instance  embeddings  with  a  secret  user-defined  linear combination of those vectors. We show that choosing the linear combinations from the codewords of an error-correcting code allows users to collaboratively train the model without revealing their embedding vectors.  We present the experimental results for user verification with voice, face, and handwriting data and show that FedUV is on par with existing approaches, while not sharing the embeddings with other users or the server.", "keywords": ["Federated learning", "User verification models"]}, "meta": {"decision": "Reject", "comment": "In this paper, the authors propose to adapt the recent paper by Yu et al. (ICML 2020), namely FedAwS. In that paper, the authors solved a potential failure mode in federated learning, when all the users only have access to one class in their devices. In this paper, the authors extend FedAwS to a setting in which federated learning is used for User Verification (UV), namely FedUV. The authors argue that the previous paper could not be the solution to learning UV because FedAwS share the embedding vectors with the server.\n \nThe authors then show a procedure in which they can learn a classifier in which the embedding vectors are not needed to be shared with the classifier. They use error-correcting codes to make the mapping sufficiently different and that allows the training to succeed without sharing the embedding. The proposed change is only marginally worse than FedAwS and centralized learning. This is the part of the paper that has attracted positive comments and is praised by all the reviewers.\n \nThe authors take as given that by not sharing the embedding vectors and by using randomly generated error-correcting codes, the whole procedure is privacy-preserving and secure. The 4th reviewer indicates that these guarantees need to be proven and points out several references that hint toward flaws in the argument by the authors. Reviewer 4th does say that not sharing the embeddings might not be enough, but that self-evident arguments are not enough.\n \nThis paper provides a significant improvement for a federated machine learning algorithm that deserves publication, but the rationale of the paper is flawed from a privacy and security viewpoint. I think if the paper is published as is, especially with the proposed title, it will create a negative reaction by the security and privacy community for not adhering to their standards. We cannot lower those standards. \n \nI suggest to the authors that they can follow two potential paths for publishing this work:\n \n1 Change the scope of their algorithm. For example, I can imagine that by not sharing the embedding the communication load with the server might be significantly reduced or that adding new users with new classes can be easier.\n \n2 Follow the recommendation from Reviewer 4 and show that the proposed method is robust against the different attacks.\n \nMinor comments:\n \nFor a paper that is trying to solve the AU problem, I would expect a discussion about why learning is better than a private algorithm. In a way, learning is sharing, and that increases the risk of mischief by malicious users.   \n \nThe discussion about error-correcting codes and the minimum distance is quite old fashion. In high dimensions, the minimum distance is not the whole story. LDPC codes make sense when we stop focusing on minimum distance codes and minimum distance decoding. I would recommend having a look at the Berlekamp\u2019s Bat discussion in David MacKay\u2019s book (Chapter 13)."}, "review": {"DwH11bJXjQv": {"type": "review", "replyto": "InGI-IMDL18", "review": "**Summary**\nFederated learning takes advantage of the fact that private user data does not need to be transferred and shared across devices or servers. This makes FL particularly attractive for the user verification scenario, where privacy-sensitive biometric data are used to train verification models. One crucial hurdle in this scenario is that per device, only positive data are present, potentially turning the device-wise training objective ill-posed (all embedding are likely to collapse to a single point). As a way to introduce negative examples, FEDAWS has been developed and presented at ICML 2020. This paper recognizes a crucial security risk in the FEDAWS system, that embeddings of user data are transferred to the server, and proposes a more secure training methodology, FEDUV, that involves the error-correcting codes. FEDUV enjoys stronger security guarantees while showing comparable ROC curves as FEDAWS at nearly identical computational costs (though not entirely sure about the computational cost bit ;) ).\n\n**Pros**\nThe motivation is spot on. Having to see any form of negative samples is the itchy point of the FL-based user verification system. FEDUV magically solves this issue by pre-defining a unique prototype vector for each user, which are not shared across users and are by design far apart from each other (this is the crucial trick!) by employing a technique in error-correcting codes (ECC). As a result, each user's endeavour to get closer to the own prototype vector ensures the maximisation of distance from the others' prototype vectors. \n\nThree experiments that are quite close to real-world scenarios (speaker, face, and handwriting-based verification) show that the performance of FEDUV is comparable to FEDAWS, the state of the art framework from ICML 2020 with weaker security guarantees.\n\nWriting is nearly flawless. Highly enjoyable paper.\n\n**Cons**\nNo major cons. Perhaps explain in a bit more depth on the BCH code to illustrate (at least a high-level, hand-wavy description) how it assigns the codes in a distance-maximizing manner. Section 2.3 only explains the desiderata for BCH, rather than *how* BCH achieves it. Please also confirm that FEDUV spends nearly identical computational cost as FEDAWS. Somehow I got this from the paper, but have not found a solid reference that confirms this (if not, please explain, too).\n\nNits: Please add grid lines and row titles (training set, test set with known users, test set with unknown users) in Figure 2 plots. Baslines --> Baselines. Flatten the last part of Section 1 as paragraphs rather than itemize? Yu et al. 2020 (FEDAWS) is an ICML paper, not arXiv - please fix the reference.\n\n**Key reasons for the rating**\nI don't find any major rationale to reject this paper. However, its novelty is also eclipsed by the Yu et al. 2020 (FEDAWS) paper. Though I really like this paper, I believe the best scores should be reserved for more innovative papers.\n\n**After rebuttal & discussion**\nI still tend to think that the paper's scope can be adjusted relatively easily (it is not too difficult to insert more disclaimers and change the title), and we can force apply the adjustment by conferring a conditional acceptance.\n\nBut I'm sold on the point that there is a lack of argumentation on whether undisclosing the user-specific embedding will improve the privacy guarantee. I had taken this argument as granted, but this is indeed not so obvious, given that there exist many attacks that are applicable in this kind of scenario, as R4 has argued. It would be great if the authors could quantify the improved privacy guarantee.\n\nI'm okay with rejecting the paper then. I still like the paper quite a lot, but rejecting it will also give the authors a good chance to assimilate more points of views in the paper.", "title": "Nearly flawless paper", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "uJ0ZU8BwNRx": {"type": "rebuttal", "replyto": "InGI-IMDL18", "comment": "We would like to thank the reviewers for their valuable feedback. We have updated the paper to address the comments. \n\nThe major changes made to the paper are as follows:\n* We explicitly stated that the goal of the paper is to train embedding-based classifiers in federated setup with only the positive loss term. We removed any statement that implied our method addresses the problem of poisoning or evasion attacks in FL and added that \"Our proposed framework trains the model without sharing the secret embedding vector with the server or other users. It is, however, not designed to defend against generic attacks in the federated setup, such as the poisoning and backdoor attacks presented in (Bhagoji et al., 2019; Bagdasaryan et al., 2020).\"\n* We added a figure to Appendix showing the accuracy with and without $l_{neg}$ and explained that including $l_{neg}$ in training helps with accuracy at early epochs but does not have significant impact on the final accuracy.\n* We added AUC of the ROC curves of Figure 2 to the appendix. \n", "title": "Paper is revised"}, "_Ni7uGjhXk": {"type": "rebuttal", "replyto": "nXb4gSY8F-L", "comment": "Thank you for your comments. Our responses are provided in the following. \n\n* About attacks. Please note that our method is designed to provide robustness against the attacks on user verification (UV) models. Studying generic attacks on federated learning is out of scope of the paper. We will make this clearer in the revision. \n\n* About backdoor attack by the server. Generic backdoor attacks can be, indeed, performed, e.g., to force the model to map a fake example $x$ into a fake embedding vector $y$. Such an attack, however, will not negatively affect the verification performance of the model. The reason is that the server does not have access to the input data or the embedding vectors of users and cannot design the fake input or the fake embedding vector in a way to harm a target user. We will make this clearer in the revision.\n\n* About evasion attack. \n  * As we mentioned in previous responses, in the context of user verification models, evasion attacks can be used to force the model to verify fake examples. You also mentioned that the attacker can start \u201cfrom a fake example and gradually adjust it until the model classifies it as the target user.\u201d \n  * In our proposed method, the verification is done according to Equ. (6), by comparing the output of the model with a secret binary vector $v$. That is, the model returns a binary value (accept or reject) for each query. In experiments, we used codewords of length $127$, $255$, and $511$ as vector $v$. \n  * Since the vector $v$ is not known to the attacker, gradient-based attacks cannot be performed. As you mentioned, an alternative approach for the attacker is to just query the model. However, the probability of forcing the model to verify a perturbed input is very small, as there are $2^v$ number of possible vectors. Please note that UV models are usually deployed on edge devices for applications such as unlocking the phone and it is not possible to make large number of queries to them in real-world settings. We will include this discussion in the revised version of the paper to clarify our threat model.  ", "title": "Response to Security Issues"}, "sEHqaEqb8qc": {"type": "review", "replyto": "InGI-IMDL18", "review": "The paper leverages federated learning to train user verification models. The authors claim that their new federated learning addresses the security and privacy issues of previous methods. In particular, for privacy, the users do not need to send their class embedding vectors to server nor other users. For security, the paper claims that the proposed method is secure against poisoning attacks and evasion attacks. \n\nStrengths\n\nI think the major strength of the paper is to design a loss function and a way of modeling embedding vectors for users such that the embedding models can be learnt without sharing the embedded vectors to the server nor other users. \n\nWeaknesses\n\nThe paper is weak on its security and privacy claims. \n\n1. For privacy, can you quantify the privacy leakage of sharing embedded vectors with the server? Without a formal quantification, it is hard to claim your method is more private. \n\n2. Poisoning attack. I don't think the paper addresses the poisoning attacks. The paper considers that the server may poison the learnt model. However, in the proposed method, the server can still poison the model. In particular, the server can send arbitrary new model to each user. In general, it is hard to defend against malicious server who performs poison attacks. \n\nAlso, malicious users can poison the model training, which are more realistic poisoning attacks. But such poisoning attacks are not considered. I don't see how the proposed method can address these poisoning attacks.  Some references on poisoning attacks:\n\nhttps://arxiv.org/abs/1807.00459\n\nhttps://arxiv.org/abs/1911.11815\n\nhttps://openreview.net/forum?id=rkgyS0VFvr\n\n3. Evasion attack. The proposed cannot address evasion attack at all. \n\n4. Experimetal details. Can you add more details on experimental details, e.g., learning rate. How is experiment on softmax loss function implemented. \n\n5. Can you also report AUC to compare different methods, since you already show the true positive rate vs. false positive rate curves?", "title": "unclear security and privacy guarantees", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "lPLFyJesrad": {"type": "rebuttal", "replyto": "SWPjdV5QZiw", "comment": "Thank you for your comments. Our responses are provided in the following. \n\n* About privacy and security focus of the work. We outlined the privacy and security requirements of the user verification (UV) applications in the first two paragraphs of the Section 3-1. It is mentioned that the input biometric data are privacy-sensitive, and the embedding vectors are security-sensitive. We used federated learning to address the privacy problem of sharing the input data and proposed a method based on error-correcting codes to tackle the security problem of sharing embedding vectors. \n\n* About poisoning and evasion attacks. In the third paragraph of the Section 3-1, we explained the security threats of the leakage of the embedding vector. We also provided examples of the relevant poisoning and evasion attacks, where the attacker uses the knowledge of the embedding vector to run the attack. The goal of the paper is to train the UV model without sharing the embedding vector, so that the model will be robust against such attacks that need access to the embedding vector. \n\n* About backdoor attach by the server. Can you please explain how a backdoor attack can be performed without the knowledge of the embedding vectors of the users?\n\n* About poisoning attack by malicious users. Can you please explain how malicious users can perform a poisoning attack without the knowledge of the embedding vectors of other users?\n\n* About evasion attack. You mentioned that the attacker can start \u201cfrom a fake example and gradually adjust it until the model classifies it as the target user.\u201d Can you please explain how the attack can be performed without the knowledge of the embedding vector of the target user?", "title": "Response to Security/Privacy Concerns"}, "Gg869m9mtCz": {"type": "rebuttal", "replyto": "DwH11bJXjQv", "comment": "Thank you for your comments. Our responses are provided in the following. \n\n* On BCH code. We will add a description of how the BCH codewords are constructed to the paper. Please, however, note that the choice of the coding algorithm is not crucial to our work and our method works with any error-correcting coding algorithm. We used BCH code because it provides codes with a wide range of message and code lengths. \n\n* On computational cost. FedUV has similar computational cost as FedAWS on the client-side. On the server-side, however, FedUV is more efficient, since, unlike FedAWS, it does not require the server to do any processing beyond averaging the gradients. We will add a more detailed discussion on the computational cost comparisons to the experimental results section. \n\n* Thanks for your helpful edit suggestions. We will revise the paper accordingly.\n\n", "title": "Response to Reviewer 2 "}, "esb_ss0OE3i": {"type": "rebuttal", "replyto": "sEHqaEqb8qc", "comment": "Thank you for your comments. Our responses are provided in the following. \n\n* On privacy quantification. Our paper focuses on the security problems associated with the leakage of the embedding vector. Please note that, unlike the input biometric data, the embedding vector is not private per se. It is, however, highly security-sensitive since it will be used for user verification. In our paper, we proposed a method to train the model without sharing the embedding vector with the server (or other users). As a result, our method provides robustness against the attacks that require access to the embedding vector.\n\n* On security claims. Please note that the goal of our paper is not designing a model robust against generic evasion and poisoning attacks, and, indeed, as you mentioned, a model trained with FedUV will certainly be vulnerable against adversarial examples and poisoning attacks like any other deep neural network. Our goal, instead, is to prevent the attacks that specifically result from the leakage of the embedding vector. We provided examples of how sharing the embedding with the server allows the server to run special types of poisoning and evasion attacks, and showed that our method provides robustness against such attacks.\n\n* About poisoning attack:\n  * Response to \u201cthe server can send arbitrary new model to each user\u201d: such an attack can be detected by users since they will notice that the accuracy of new global models on their validation data is not improving as the training progresses. \n  * Response to poisoning attack by malicious users: in poisoning attacks, the attacker usually aims to manipulate the training data such that the model misclassifies specific test examples or classifies them into a target class. In our problem of user verification, each user is associated with one class. Hence, for the server or users to run poisoning attack, they need to force the model to either (i) reject the correct examples of a target user, or (ii) verify some fake examples as true inputs of the target user. Attacks (i) and (ii) need access to input examples and the embedding vector of the target user, respectively. The use of federated learning eliminates the need to share input examples and our method, FedUV, eliminates the need to share embedding vectors. Hence, our method is robust against such poisoning attacks. \n\n* About evasion attack. In the context of user verification models, evasion attack can be used to force the model to verify fake examples. The attack can be described as follows: manipulate a fake example such that the model will verify it as a true example generated by a target user. To do so, the attacker needs to modify the fake example such that the model will output the embedding vector of the target user. The embedding vectors, however, are not shared with the server of other users. Hence, such an attack is not feasible. \n\n* On experimental details:\n  * We train UV models using the FedAvg method with one local epoch and $20,000$ rounds with $0.01$ of users selected at each round. The models are trained with SGD optimizer with learning rate of $0.1$ and learning rate decay of $0.01$. We will provide more details on the experimental setup in the paper. \n  * The softmax loss function corresponds to the regular training of a multi-class classifier, i.e., each user is assigned a class ID and they collaboratively train the entire model (including the last FC layer as embedding vectors). \n\n* On reporting AUCs. Thanks for your suggestion. We will add the AUC of different methods in the revised version of the paper. ", "title": "Response to Reviewer 4"}, "_6eD_-5pAR0": {"type": "rebuttal", "replyto": "mk2LVZOWfha", "comment": "Thank you for your comments. Our responses are provided in the following. \n\n* About $d_{min}$: \n     * Larger code length results in larger $d_{min}$, which is the minimum difference between any two codewords. In communication systems, the difference of $d_{min}$ between codewords can be used to correct up to $\\lfloor(d_{min}-1)/2\\rfloor$ errors (for binary codewords). For example, for $d_{min}=25$, if a received codeword contains up to $12$ errors, it can be correctly decoded since the true codeword is the closest one to it. \n     * In our application, we used codewords as output representations of the model. Hence, the model can be viewed as a communication channel that makes errors at test time. If the minimum distance between the codewords is larger, the model can make more errors and still correctly classify the input, which in turn results in higher test accuracy.  \n\n* About baselines. The model architectures used for each dataset are provided in Appendix A. The embedding size of softmax and FedAWS is $1024$ in all cases. The softmax baseline is the regular training using the FedAvg method and without any security constraint. In FedAWS, users send their embedding vectors to the server and the server maximizes the pairwise distances between embeddings in addition to gradient averaging. \n\n* About assumption of $\\|\\|z\\|\\|=\\sqrt{c}$. The network is trained to maximize the correlation of the instance embedding (model output) with the codeword. To make the optimization easier, the model output is scaled to have the same norm as the codeword. The normalization is important for the proofs but is also crucial in experiments as we observed that it increases the convergence rate and also improves the final accuracy. \n\n* About $l_{pos}$ and $l_{neg}$. It will indeed help to use $l_{neg}$ for training especially at early training rounds, but the effect of $l_{neg}$ gradually vanishes as $l_{pos}$ becomes smaller and eventually gets close to zero. To illustrate this, we show the training and test accuracy with and without $l_{neg}$ for MNIST-UV dataset (Please note that for the sake of simplicity, here we show the accuracy rather than the TPR and FPR). The figure can be found [[here]](https://drive.google.com/file/d/1BosJLUymhVNJqFli70iQkk3Wdi8WeRFT/view?usp=sharing). As can be seen, using $l_{neg}$ results in better accuracy at early epochs but does not have significant impact on the final accuracy. \n\n* On the relationship of $l_r$ and the minimum distance: The codewords are constructed as $v=C(m)$, where $C$ is the coding algorithm and $m=b\\|\\|r$ is the message vector, in which $b$ is a unique binary vector assigned by server to each user and $r$ is a random binary vector chosen by each user. Let $c$, $l_m$, $l_b$ and $l_r$ be the lengths of the codeword, message vector, unique ID vector and random vector, respectively. There are $2^{l_m}$ distinct codewords. With larger $l_m$ and the same $c$, there will be more codewords in the same space size and so their pairwise distance decreases. Increasing $l_r$ increases $l_m$ and, hence, reduces the minimum distance of the code. We will add more details about this to the paper. \n\n\n\n", "title": "Response to Reviewer 1"}, "3OsnLjalnpR": {"type": "review", "replyto": "InGI-IMDL18", "review": "In this paper, the authors focus on designing a federated user verification solution. Specifically, the authors address two fundamental challenges associated with user verification, i.e., one-class data (positive data only), and privacy protection (i.e., the raw data and the embeddings of the users and class). Technically, the authors extend a very recent work called FedAWS by (Yu et al., 2020), and introduce a user-specific codewords, which not only protect users' privacy (i.e., not sharing the embedding with other users or the server) but also do not need the negative samples (i.e., the two loss functions in Eq.(5) reduces to one due to equivalence shown in Theorem 1). We can see that the main idea of re-writing the Eq.(2) into two loss functions in Eq.(4) and Eq.(5), and introducing codewords are novel and effective, which also address the two challenges well.\n\nEmpirical studies on three user verification cases show the effectiveness of the proposed solution FedUV.\n\nOverall, the technique is novel (and I like this idea) and the paper is well presented. I recommend acceptance.\n", "title": "Secure Federated Learning of User Verification Models", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "mk2LVZOWfha": {"type": "review", "replyto": "InGI-IMDL18", "review": "The authors propose a method that allows training of UV methods without sharing any user (exemplar or class) embeddings with the server or other uses. Models are trained using gradient averaging on the server, so any leakage through that is not addressed in this work. The paper shows experimental results on speaker identification, face and handwriting verification tasks. The authors argue that this is the first work that considers secure training in a federated setup, with neither raw inputs nor exemplar or class embeddings being shared with the server or other users.\n\n#### Pros\n\n* The paper is clearly written and the derivations are sound (for the most part, see questions below). \n* The idea appears to be novel and a significant delta compared to the SoTa in terms of security and the novelty of a secure embedding learning protocol in the federated setup were only (one) positive classes are available for training.\n* The experimental results are promising albeit can't compete with existing less secure methods. \n\n#### Cons\n\n- Clarity of experiments\n  - Especially for the face verification task the code length seems to play a major role. Any discussion giving an understanding of this would be appreciated. Specifically, how and why does $d_{min}$ affect the accuracy. Bottom of page 5 mentions that increasing the code-words and presumably $d_{min}$ increases the performance, but no reasoning is provided.\n  - Additional insights of how the baselines (softmax, FedAws) were trained and what the emedding sizes are would be helpful. Is the embedding size ~64 in all cases?\n\n#### Questions & Comments\n\n- The assumption of $||z|| = \\sqrt{c}$ should be put into context. What are the practical applications for this assumption. Is it merely there for the math to work out?\n- The theorems show that $l_{neg}$ is redundant for when $l_{pos}=0$, however, it is not clear to me that minimizing $l_{pos}$ also corresponds to minimizing $l_{neg}$. In practice, $l_{pos}$ will likely never reach $0$ and a negative loss term could have a significant contribution to the loss surface.\n- Page 6 mentions that increasing $l_r$ reduces the minimum distance of the code for a given code length. Why is this the case? Is it because $r_u$ is sampled by the clients and no guarantees can be made? A more detailed discussion would be helpful.\n\nThis work proposes a new idea that allows training embeddings for verification with only positive classes in a federated setting, while ensuring security. Some areas could be clarified in the paper, especially why it is sufficient to proof the redundancy of the negative loss term only for the global minimum of when $l_{pos}=0$. Assuming the authors can provide a satisfying explanation, I recommend accepting this work.", "title": "Secure federated learning user verification model training", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}