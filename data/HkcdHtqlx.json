{"paper": {"title": "Gated-Attention Readers for Text Comprehension", "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "Zhilin Yang", "William W. Cohen", "Ruslan Salakhutdinov"], "authorids": ["bdhingra@cs.cmu.edu", "hanxiaol@cs.cmu.edu", "zhiliny@cs.cmu.edu", "wcohen@cs.cmu.edu", "rsalakhu@cs.cmu.edu"], "summary": "", "abstract": "In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. ", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"]}, "meta": {"decision": "Reject", "comment": "The paper proposes several extensions to popular attention-enhanced models for cloze-style QA. The results are near state of the art, and an ablation study hows that the different features (multiplicative interaction, gating) contribute to the model's performance. The main concern is the limited applicability of the model to other machine reading problems. The authors claim that unpublished results show applicability to other problems, but that is not sufficient defence against these concerns in the context of this paper. That said, the authors have addressed most of the other concerns brought up by the reviewers (e.g. controlling for number of hops) in the revised version. Overall however, the PCs believe that this contribution is not broad and not novel enough; we encourage the authors to resubmit."}, "review": {"rk_Jym38x": {"type": "rebuttal", "replyto": "HkcdHtqlx", "comment": "Respected Reviewers,\n\nWe would like to thank you again for the comments and suggestions. We believe that we have addressed most of the concerns raised in the revision posted on 12-21-2016. \n\nWe request you to kindly review the changes and our rebuttal below, and reconsider your scores if satisfied. Otherwise, kindly let us know how we can improve the paper further (before the review period ends on Jan 20 this Friday).\n\nBest Regards,\nAuthors.", "title": "Request the Reviewers for Feedback"}, "rJt62pu4l": {"type": "rebuttal", "replyto": "BJCy-Ie4g", "comment": "We thank the reviewer for the valuable feedback. The GA Reader trained with fixed word embeddings achieves state of the art results on CBT-NE but not so on CBT-CN (please see the revisions after 12/2/16). In general, we observed that the small size of the dataset, combined with the open-domain nature of children\u2019s book stories suggests that overfitting was an issue with these datasets. Our model, though conceptually simple, is highly expressive, but a consequence of that is that larger amounts of training data may be required to obtain improvements.", "title": "Response to Reviewer2"}, "HkoY36_4e": {"type": "rebuttal", "replyto": "rk0An2xNx", "comment": "We thank the reviewer for the valuable feedback. We agree that a deeper analysis of the performance gain w.r.t. different components in the model is important in order to better understand the architecture. To this end, we have added more analyses to Tables 4 and 6 in the revised draft, and related discussions at the end of section 4.4. Below we address the specific points you raised:\n\n1. While the experiments and architecture presented in the current paper are indeed for the specific task of cloze-style QA, we believe the main idea of GA is applicable in other settings where a sequence needs to be processed conditioned on external context. One possible application is QA over videos. There is already some evidence of the utility of multiplicative interactions in visual QA, see for example [1]. \n\nWe would also like to point out that other researchers have already successfully applied the preprint version GA Reader-- to challenging datasets for other tasks such as language modeling (LAMBADA [2]) and extractive question answering (SQuAD [3]), with SOTA or competitive results, indicating its generality. The current paper adds further improvements to that unpublished version.\n\n2. Table 6 presents an ablation study on the components added on top of GA Reader--. Character embeddings are the least important, and were not applied for CNN/Daily Mail datasets anyway due to anonymization (Section 4.2). The use of token-specific attentions and glove embeddings are both important additions. Since the strongest baseline, NSE, also uses Glove embeddings we believe that the improvement provided by our main contribution, the GA module (with token-specific attentions), is important. Further, the significance of this module is established in section 4.4.\n\n3. Our new empirical results in Table 4 show that increasing the number of hops leads to substantial increase in performance up to K=3, which plateaus beyond that. Intuitively, visualization of the attention at intermediate layers in Figures 3-7 shows that the model tends to focus on distinct salient tokens in the query after each layer. This indicates that multiple hops allow the model to combine multiple pieces of information from the query, mimicking a shallow reasoning process. We agree that a deeper analysis on the effect of K on different types of queries would be most informative, unfortunately this is beyond the scope of this work since such query type annotations are not readily available nor easily constructed for any of the datasets we consider. \n\n[1] Fukui, Akira, et al. \"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.\" arXiv preprint arXiv:1606.01847 (2016).\n[2] Chu, Zewei, et al. \"Broad Context Language Modeling as Reading Comprehension.\" arXiv preprint arXiv:1610.08431 (2016).\n[3] Yang, Zhilin, et al. \"Words or Characters? Fine-grained Gating for Reading Comprehension.\" arXiv preprint arXiv:1611.01724 (2016).\n", "title": "Response to Reviewer3"}, "rkCVhTdNx": {"type": "rebuttal", "replyto": "S1oYzFbNl", "comment": "We thank the reviewers for the valuable feedback. We agree that a deeper analysis of the gain of performance due to the different components in the model is important in order to better understand the architecture. To this end, we have added more analyses to Tables 4 and 6 in the revised draft, and a discussion of these at the end of section 4.4. Below we address the specific points you raised:\n\n1. (1) We have added results on the effect of varying the number of hops K to table 4. We can see that there is a significant and stable increase in performance as we go from K=1 (which is the AS reader) to K=3 hops, which then plateaus at K=4 hops. This is in line with our expectation from the visualizations presented in section 4.5 (Figures 3-7), where we show that after each layer the model typically attends to distinct tokens in the query to derive the correct answer. \n\n(2) It is certainly possible to share parameters for different components of the system, including the query encoders at each layer. However, the decision to do so is dependent on the particular application; in particular for the datasets we consider we found no benefit in doing so (please see the comment on 4 Dec below). In the paper we present the more general formulation of not sharing the parameters, since that allows the query representations, and hence the filters applied at each layer to be different.\n\n2. Table 6 presents an ablation study on the different components added on top of GA Reader--. We see that even without C(w), the proposed model, despite its relative simplicity, achieves SOTA results on WDW. We would also like to emphasize that C(w) was not used for CNN and Daily Mail datasets where again GA has SOTA performance. The main focus of the paper is the GA module, which we show in section 4.4 has a statistically significant impact on the final performance.\n", "title": "Response to Reviewer1"}, "HkrkhpdVl": {"type": "rebuttal", "replyto": "HkcdHtqlx", "comment": "Based on the reviewer feedback we have added the following:\n\n1. Analysis of the effect of number of hops K on the performance to Table 4.\n2. Ablation study of the components in the current version added on top of GA Reader-- to Table 6.\n\nA discussion of these is added to section 4.4, and also provided in response to the reviews here.\n\nNote - Mistakenly pushed the same revision twice on 12/21/2016.", "title": "Revision #2 (12/21/2016)"}, "S1qX2cxXx": {"type": "rebuttal", "replyto": "rJOg5jyXg", "comment": "(A) The query GRUs (and hence the query representations) in our model are different in each layer, i.e. they do not share parameters. In this way, each layer can focus on a particular semantic aspect and/or granularity for the gating operation. We figure that adding diversity to query representations is more effective than reinforcing the same information at every layer repetitively. In fact, we compared the two schemes (share v.s. not share) experimentally and found them to perform similarly for the datasets we consider. In the paper we present the more general formulation of not sharing parameters.\n\n(B) Thank you for the suggestion. Applying the GA module selectively between the layers is equivalent to replacing the single-layer document GRUs with multilayer versions, and then retuning both the total number of hops (K) and the GRU depth at each hop. We agree this is an interesting direction to explore, though beyond the primary focus of this paper. As shown in our ablation study in Sec 4.4, a statistically significant performance drop is observed when all the GA modules are omitted, which justifies the effectiveness of the gated-attention mechanism. We leave the study regarding the optimum depth of each document GRU for future research.", "title": "On query GRUs and GA placement"}, "rJOg5jyXg": {"type": "review", "replyto": "HkcdHtqlx", "review": "(A)\nAs described in equation (3) and Fig. 1, there is a Bi-GRU for the query Q at each layer indexed by k, i.e. GRU_Q^(k). \n1. Are these K GRU_Qs shared or different from each other? \n2. If different, why did the authors choose to specialize these query GRUs? Since the input (Y) to these GRUs is identical, it would be interesting to explore how is the performance affected by sharing these GRUs (i.e., using a single GRU_Q instead of K specialized ones).\n\n(B)\nThe GA is applied at all the layers. How important is it to apply it to each layer? Can the authors comment on how the performance is affected if GA is omitted in the lower-layers (or higher layer)? A thorough experimental evaluation would be most insightful.Summary:\n\nThe authors propose a multi-hop \"gated attention\" model, which models the interactions between query and document representations, for answering cloze-style questions. The document representation is attended to sequentially over multiple-hops using similarity with the query representation (using a dot-product) as the scoring/attention function. \nThe proposed method improves upon (CNN, Daily Mail, Who-Did-What datasets) or is comparable to (CBT dataset) the state-of-the-art results.\n\n\nPros:\n\n1. Nice idea on heirarchical attention for modulating the context (document) representation by the task-specific (query) representation.\n2. The presentation is clear with thorough experimental comparison with the latest results.\n\n\nComments:\n\n1. The overall system presents a number of architectural elements: (1) attention at multiple layers (multi-hop), (2) query based attention for the context (or gated attention), (3) encoding the query vector at each layer independently.\nIt is important to breakdown the gain in performance due to the above factors: the ablation study presented in section 4.4 helps establish the importance of Gated Attention (#2 above). However, it is not clear:\n\n  (1) how much multiple-hops of gated-attention contribute to the performance.\n  (2) how important is it to have a specialized query encoder for each layer.\n\nUnderstanding the above better, will help simplify the architecture.\n\n\n2. The tokens are represented using L(w) and C(w). It is not clear if C(w) is crucial for the performance of the proposed method.\nThere is a significant performance drop when C(w) is absent (e.g. in \"GA Reader--\"; although there are other changes in \"GA Reader--\" which could affect the performance). Hence, it is not clear how much does the main idea, i.e., gated attention contributes towards the superior performance of the proposed method.\n", "title": "Query GRUs and GA Placement", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1oYzFbNl": {"type": "review", "replyto": "HkcdHtqlx", "review": "(A)\nAs described in equation (3) and Fig. 1, there is a Bi-GRU for the query Q at each layer indexed by k, i.e. GRU_Q^(k). \n1. Are these K GRU_Qs shared or different from each other? \n2. If different, why did the authors choose to specialize these query GRUs? Since the input (Y) to these GRUs is identical, it would be interesting to explore how is the performance affected by sharing these GRUs (i.e., using a single GRU_Q instead of K specialized ones).\n\n(B)\nThe GA is applied at all the layers. How important is it to apply it to each layer? Can the authors comment on how the performance is affected if GA is omitted in the lower-layers (or higher layer)? A thorough experimental evaluation would be most insightful.Summary:\n\nThe authors propose a multi-hop \"gated attention\" model, which models the interactions between query and document representations, for answering cloze-style questions. The document representation is attended to sequentially over multiple-hops using similarity with the query representation (using a dot-product) as the scoring/attention function. \nThe proposed method improves upon (CNN, Daily Mail, Who-Did-What datasets) or is comparable to (CBT dataset) the state-of-the-art results.\n\n\nPros:\n\n1. Nice idea on heirarchical attention for modulating the context (document) representation by the task-specific (query) representation.\n2. The presentation is clear with thorough experimental comparison with the latest results.\n\n\nComments:\n\n1. The overall system presents a number of architectural elements: (1) attention at multiple layers (multi-hop), (2) query based attention for the context (or gated attention), (3) encoding the query vector at each layer independently.\nIt is important to breakdown the gain in performance due to the above factors: the ablation study presented in section 4.4 helps establish the importance of Gated Attention (#2 above). However, it is not clear:\n\n  (1) how much multiple-hops of gated-attention contribute to the performance.\n  (2) how important is it to have a specialized query encoder for each layer.\n\nUnderstanding the above better, will help simplify the architecture.\n\n\n2. The tokens are represented using L(w) and C(w). It is not clear if C(w) is crucial for the performance of the proposed method.\nThere is a significant performance drop when C(w) is absent (e.g. in \"GA Reader--\"; although there are other changes in \"GA Reader--\" which could affect the performance). Hence, it is not clear how much does the main idea, i.e., gated attention contributes towards the superior performance of the proposed method.\n", "title": "Query GRUs and GA Placement", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyS_eE1Xx": {"type": "rebuttal", "replyto": "SktUKpAfl", "comment": "We do not shuffle the entities each time a training article is loaded. Please note that the test set is loaded only once, with a fixed random order of entities. Hence, the question is whether shuffling (or not) can help during the training process \u2013 which is an empirical question. One would expect that shuffling would lead to less overfitting, but other researchers have done these experiments (see Table 3 in  revisions 1 (11/4) and 2 (11/21) of another ICLR submission \u2013 https://openreview.net/revisions?id=HJ0UKP9ge along with the accompanying comment on 11/21), with no clear conclusion as to which approach is better. ", "title": "on shuffling entities"}, "SktUKpAfl": {"type": "rebuttal", "replyto": "S1daknQ-x", "comment": "Hi, I am looking into the released code; if you are using kcho's data, do you randomly shuffle the entities when you load the articles?", "title": "shuffling entities"}, "Sy45dg0fe": {"type": "rebuttal", "replyto": "HkcdHtqlx", "comment": "Updated Tables 3,4,5 with new comparisons of GA Reader's performance when word embeddings are either held fixed or updated during training. We observe that for smaller datasets (WDW and CBT) keeping the word embeddings fixed leads to substantial improvements (>1%). Also, retuned the character GRU hidden state size to 25 for WDW and CBT, which was previously 50.", "title": "Revision #1 (12-01-2016)"}, "S1R4BJRfx": {"type": "rebuttal", "replyto": "rk02Tunzg", "comment": "There is some previous work showing the effectiveness of recurrent neural networks for modeling characters for named entity recognition [1,2]. Hence we used an RNN (specifically GRU) as the character encoder for WDW, which has named entities as answers, and CBT, which has either named entities or common nouns as answers. While both RNNs and CNNs are applicable for character-level feature extraction, we cannot make a general statement whether RNNs or CNNs are more suited for this task, and that itself may be an interesting direction for future research.\n\n[1] Yang et al, 2016, Multi-Task Cross-Lingual Sequence Tagging from Scratch, arxiv.org\n[2] Lample et al, 2016. Neural architectures for named entity recognition, arxiv.org", "title": "On RNN v CNN for character modeling"}, "rk02Tunzg": {"type": "review", "replyto": "HkcdHtqlx", "review": "Hi,\nis there a specific reason why you used a character GRU instead of a convolutional neural network?\nDo you have any intuitions on whether the two approaches are equivalent or one can be better than the other?SUMMARY.\n\nThe paper proposes a machine reading approach for cloze-style question answering.\nThe proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).\nGA calculates the compatibility of each word in the document and the query as a probability distribution.\nFor each word in the document a gate is calculated weighting the query representation according to the word compatibility.\nUltimately, the gate is applied to the gru-encoded document word.\nThe resulting word vectors are re-encoded with a bidirectional GRU.\nThis process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.\nThe probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.\n\nThe proposed model is tested on 4 different dataset. \nThe authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.\nThe paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.\nThe GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.\nI would have liked to see some discussion on why the model works less well on the CBT dataset, though.\n\n\n----------\n\nDETAILED COMMENTS\n\nminor. In the introduction, Weston et al., 2014 do not use any attention mechanism.", "title": "character-level embeddings", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJCy-Ie4g": {"type": "review", "replyto": "HkcdHtqlx", "review": "Hi,\nis there a specific reason why you used a character GRU instead of a convolutional neural network?\nDo you have any intuitions on whether the two approaches are equivalent or one can be better than the other?SUMMARY.\n\nThe paper proposes a machine reading approach for cloze-style question answering.\nThe proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).\nGA calculates the compatibility of each word in the document and the query as a probability distribution.\nFor each word in the document a gate is calculated weighting the query representation according to the word compatibility.\nUltimately, the gate is applied to the gru-encoded document word.\nThe resulting word vectors are re-encoded with a bidirectional GRU.\nThis process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.\nThe probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.\n\nThe proposed model is tested on 4 different dataset. \nThe authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.\nThe paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.\nThe GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.\nI would have liked to see some discussion on why the model works less well on the CBT dataset, though.\n\n\n----------\n\nDETAILED COMMENTS\n\nminor. In the introduction, Weston et al., 2014 do not use any attention mechanism.", "title": "character-level embeddings", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJ3_t6o-x": {"type": "rebuttal", "replyto": "HyOkwUsZl", "comment": "Thanks for suggesting the more explicit notation. \n\nRegarding sigmoid vs tanh nonlinearities, our preliminary experiments on using sigmoid nonlinearity over the query embedding led to slightly worse performance. Note that in the current setup both query and document embeddings are outputs of a tanh operation. Hence, these vectors lie in the same space. Our conjecture is that the elementwise-product is acting as a \u201csemantic matching\u201d operation, for example a negative sign in the output indicates a mismatch along that dimension. This emphasizes the similarity or difference between query and document embeddings on a per-dimension basis. ", "title": "Regarding sigmoid vs tanh"}, "HyOkwUsZl": {"type": "rebuttal", "replyto": "BJUhGSjZg", "comment": "ok. I found the inner product notation in (5) confusing.  I would have written\n\nalpha_{i,j} = softmax_j  (Q_j^T D_i)\n\ntilde{q}_i = sum_j alpha_{i,j} Q_j\n\nPhysicists make all the indexes explicit when many indexes are involved (Einstein notation).\n\nI'm not sure this is the place for this kind of comment, but in (6) tilde{q}_i is used as a gating vector. Since tilde{q}_i is a convex combination of Q_j it seems that Q_j should be viewed as a gating vector.  Q_j is the hidden state of two GRUs. GRUs produce hidden states with tanh rather than sigma.  But shouldn't gating vectors be produced by sigma?  Just a thought.\n\nOne might call this attention-over-gating in contrast with attention-over-attention.\n\n", "title": "ok and a thought."}, "BJUhGSjZg": {"type": "rebuttal", "replyto": "rkH6bQi-l", "comment": "It is not a typo. D^k, Q^k in equations (2), (3) are matrices of size 2n_h x |D| and 2n_h x |Q| (we call these the full output of a GRU, as indicated in eq (1)). In equation (5), d_i is one column of D^k, of size 2n_h. Hence, Q^T d_i has size |Q| which is a vector. \\alpha_i will be a vector of the same length.\n\nThanks for your comments!", "title": "Clarification of eq. (5) and figure 3."}, "rkH6bQi-l": {"type": "rebuttal", "replyto": "H1oaSB5Wx", "comment": "Equations (2), (3) and (5) together imply that alpha_i is a scalar.  The argument to the softmax in (5) is clearly a scalar.  Is there a typo?", "title": "Clarification of eq. (5) and figure 3."}, "H1oaSB5Wx": {"type": "rebuttal", "replyto": "SksquzqWg", "comment": "Thank you for the comment. To clarify, each \\alpha_i in eq. (5) is a vector of the same length as the question, where \\alpha_ij denotes the attention over the j-th token in the question from the perspective of the i-th token in the document (i = 1,2,...,|D|, j = 1,2,...,|Q|). The intuition here is to allow different parts of the document to focus on different aspects of the question. The collection of all \\alpha_ij's forms a |D|-by-|Q| matrix, and is visualized by the heat maps.", "title": "Clarifications about eq. (5) and fig. 3"}, "SksquzqWg": {"type": "rebuttal", "replyto": "HkcdHtqlx", "comment": "I am confused by the relationship between equation (5) and the heat map in figure 3 (and the other heat maps in the appendix).  In equation (5) the attention alpha_i is indexed by document positions.  But the heat map in figure 3 is indexed by question positions.  I don't understand.", "title": "Confusion relating equation (5) to figure 3."}, "rJ1FpTu-e": {"type": "rebuttal", "replyto": "SytXQdP-e", "comment": "Thank you for the questions.\n\n1. If the candidate has multiple tokens the attention-sum layer aggregates over all mentions of all tokens in the candidate. This is explained after equation 8 in the paper -- \u201cwhere I(c, d) is the set of positions where a token in c appears in the document d\u201d.\n\n2. While designing these improvements over the previous GA Reader, we did some preliminary ablation studies. Most of the improvement in performance comes from the new query-attention mechanism. Character embeddings are only applied to CBT and WDW datasets in this paper, where they lead to a small (~0.5%) improvement. The focus of this paper is to study the effect of Gated-Attention which is discussed in detail in section 4.4.\n\n3. Since the model in this paper is different from the previous GA Reader, the hyper-parameters were re-tuned. In particular, there are two differences \u2013 we use K=3 layers for each dataset, and dropout p=0.1 for Daily Mail. Empirically we found K=3 leads to an improvement of ~1% on average over using K=2. Similarly, changing the dropout also leads to ~1-2% difference in performance, depending on the dataset. In general, we find that for larger datasets smaller dropout rates work better.", "title": "Experimental Details"}, "SytXQdP-e": {"type": "rebuttal", "replyto": "HkcdHtqlx", "comment": "1: in WDW dataset, each candidate answer might has several tokens, how did you deal with it?\n2: Different from the previous GA readers, you have three differences, did you do ablation study on the attention mechanism and char embedding?\n3: I observed the experiment hyper parameters are different from previous GA readers, how much does this hyper parameters matters?\n ", "title": "Question about the experimental details"}, "S1daknQ-x": {"type": "rebuttal", "replyto": "S1tsrjQWg", "comment": "Thank you for the question. We used the preprocessed data \u2014 http://cs.nyu.edu/~kcho/DMQA/", "title": "dataset"}, "S1tsrjQWg": {"type": "rebuttal", "replyto": "HkcdHtqlx", "comment": "Hi, for CNN/DailyMail, did you obtain the data via DeepMind's extraction script, or did you use Kyunghyun Cho's preprocessed data?", "title": "which data used"}}}