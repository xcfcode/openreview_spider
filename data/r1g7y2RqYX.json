{"paper": {"title": "Label Propagation Networks", "authors": ["Kojin Oshiba", "Nir Rosenfeld", "Amir Globerson"], "authorids": ["kojinoshiba@college.harvard.edu", "nirr@g.harvard.edu", "amir.globerson@gmail.com"], "summary": "Neural net for graph-based semi-supervised learning; revisits the classics and propagates *labels* rather than feature representations", "abstract": "Graph networks have recently attracted considerable interest, and in particular in the context of semi-supervised learning. These methods typically work by generating node representations that are propagated throughout a given weighted graph.\n\nHere we argue that for semi-supervised learning, it is more natural to consider propagating labels in the graph instead. Towards this end, we propose a differentiable neural version of the classic Label Propagation (LP) algorithm. This formulation can be used for learning edge weights, unlike other methods where weights are set heuristically. Starting from a layer implementing a single iteration of LP, we proceed by adding several important non-linear steps that significantly enhance the label-propagating mechanism.\n\nExperiments in two distinct settings demonstrate the utility of our approach.\n", "keywords": ["semi supervised learning", "graph networks", "deep learning architectures"]}, "meta": {"decision": "Reject", "comment": "This  paper is on graph based semi-supervised learning where the goal is to develop an approach to jointly the node labeling function together with the edge weights. A natural way to formulate this problem as a bi-level optimization problem. However, the authors claim that this approach introduces two main difficulties: (a)  the \"upper\" objective function is itself the solution to the \"lower\" optimization problem (Eq. (2)), and (b) optimization is challenging (Eq. (3)). The AC disagrees. Firstly, there is a close connection between the constrained version and the regression version of the problem (e.g., Belkin, Matveeva and Niyogi) -- the former is infact a special case of the latter for a certain choice of regularization parameter. The latter reduces to an linear system. The outer problem can be optimized using standard gradient descent using the implicit function theorem trick common in bilevel optimization. Reviewers have also raised concerns about clarity, and experimental support in this paper and comparisons with related work.  "}, "review": {"BJeOQ0NqCm": {"type": "rebuttal", "replyto": "SklckBGU3X", "comment": "Thank you for your comments, please see our responses below.\n\n\u201cConvergence issues; the algorithm may go wrong; bifurcation rate can be too slow/fast:\u201d\nIndeed, a limited number of layers might give an exact solution to the quadratic criterion in Eq. (2). However, our results imply that using few iterations with learned weights outperforms a converged solution using heuristic weights.\nBecause our model optimizes accuracy, each point point in Fig, 4 corresponds not only to a different T but also to different learned weights, making it difficult to compare convergence across points.\nWhile bifurcation can potentially change the rate of convergence, it does not have to. Since the bifurcation parameters (\\theta^\\tau) are learned, and since \\theta^\\tau = 0 implies no change in rates, bifurcation will be used (by learning that \\theta^\\tau != 0) only if it results in better performance.\n\n\u201cIs introducing entropy always helpful?\u201d:\nSince the entropy parameters (\\theta^e) are learned, and because \\theta^e = 0 implies uniform weights (like in LP), the model will learn to use entropy only if it results in improved performance. The same applies to KL divergence.\n\n\u201cDifference in experimental setting and results from GCN:\u201d\nThe main differences between our setup and GCN are the number of labeled nodes and how they can be used. In GCN labeled nodes are partitioned into training (20 nodes per class) and validation (500 additional nodes). While training sets are kept small (3.6% for Citeseer, 5.2% for CoRA, and 0.3% for pubmed), the total number of labeled nodes (training+validation) is rather large (18.6%, 23.6%, and 2.8% of all nodes, respectively), and a huge portion of labeled nodes is pre-allocated for validation (81%, 78%, and 89% of labeled data, respectively) and so can only be used for tuning, not training. This puts methods that require little or no tuning (such as LP, over which our method is built) at an immediate disadvantage. \nIn our view, methods should be free to choose how to best use the available labeled data, be it for training, tuning, or other. We have experimented in the GCN setting, allowing our model to use all labeled data for training. While our model outperforms GCN (83.4 vs. 79.6 on CoRA and 69.8 vs. 67.5 on Citeseer, averaged over 10 random splits), this may also seem unfair, since other baselines might also benefit from a allocation of the labeled budget. There also several other issues with the \u201cstandard\u201d setting used in GCN - see the recent paper by Shchur et al. (2018) [1] for details.\nDue to the above, our solution was to revert to the classic SSL experimental setting used in numerous papers, where a fixed percentage of labeled nodes are drawn uniformly at random. We used 1% as it is a reasonable number in the range of the GCN setting, and allowed us to fully train all baselines for all settings and datasets over 10 random splits in a reasonable amount of time.\n\n\u201cNumber of GCN layers:\u201d\nWe use the published GCN code which has one graph-convolution layer and is used in their paper.\n\n\u201cToo many hyperparameters to tune\u201d:\nPlease note that we have only *one* network-related hyper-parameter that requires tuning - the number of layers (T). As Fig. 4 shows, choosing T can be made robust by using bifurcation. All other model parameters (denoted by \\theta) are learned. The regularization coefficient \\lambda is chosen by standard cross validation.\n\n\u201cMinor points\u201d:\nThank you for these, we will fix them.\n\n[1] Shchur, O., Mumme, M., Bojchevski, A., & G\u00fcnnemann, S. (2018). Pitfalls of Graph Neural Network Evaluation. arXiv preprint arXiv:1811.05868.", "title": "Convergence, parameters and hyper-parameters, and experimental setting"}, "S1l6GGSqR7": {"type": "rebuttal", "replyto": "HJxISgAn2m", "comment": "Thank you for your comments. Below please find details describing our modeling and experimental choices. The updated paper includes an enriched related materials section and GAT as a baseline. Fig. 4 quantifies the added value of the bifurcation component.\n\n\u201cTotal number of parameters, and compared to other methods?\u201d\nThe total number of parameters is between 12 and 44, depending on dataset and experimental setting. Based on their published codes, GCN has 23,040 and GAT has 92,391 for CoRA. We use 38. These include:\n- Weights: 30 edge features, some are per-class (Appendix B)\n- Attention: 2 parameters per class, one for entropy and one for divergence (Sec. 3.1)\n- Bifurcation: 2 parameters (Eq. (13))\n\n\u201cRelation to papers by Saha et al. and Cilberto et al.\u201d:\nThe above papers propose methods for multi-task learning (Saha et. al for online, Ciliberto et al. for batch), and consider relations between tasks, which are fixed and given as input. Our paper focuses on semi-supervised learning, and considers weighted relations between examples, which are learned.\n\n\u201cDiscern contribution of learning the weights vs propagating labels instead of embeddings\u201d:\nThe LP baseline, which we generalize, propagates labels with fixed weights. Fig. 4 shows the how adding bifurcation (LPN_bif) compares to only learning weights (LPN_nobif).\n\n\u201cPlease specify that \\theta are learned\u201d:\nWe will clarify this.\n\n\u201cEntropy and divergence - inversely proportional?\u201d:\nWe use *negative* entropy and *negative* divergence (see Eq. (11) and above). This aligns with your intuition.\n\n\u201cUse threshold for rounding instead of bifurcation\u201d:\n\u201cHard\u201d rounding is non-differentiable, and cannot be used efficiently with back-propagation. Bifurcation is differentiable, and much more expressive than simple rounding. It can interpolate between \u201crounding up\u201d (large \\tau) and \u201crounding down\u201d to uniform (\\tau-->zero), or result in no rounding (\\tau=1). \n\n\u201cAre a, b tuned using cross-validation? Can't we learn them?\u201d:\nBoth a and b (=\\theta^\\tau) are learned, not tuned.\n\n\u201cCan\u2019t the loss in Eq. (14) be replaced by the standard empirical loss?\u201d:\nUnfortunately, no. With the standard empirical loss, Eq. (14) becomes degenerate. This is because the it compares the true and predicted labels of the labeled nodes. As in LP, predicted labels of labeled nodes are set to their true labels, so the loss is always 0. This is also noted in Zhang & Lee (2007).\n\n\u201cIf available, how are node features used?\u201d:\nNode features are used to parameterize edge weights (Eq. (7) & appendix B). Sec. 2.2 now includes more details.\n\n\u201cWhy is k chosen to be equal to 1%?\u201d:\nWe chose 1% as it is a reasonable number in the range of those used in GCN and others (3.6%, 5.2%, and 0.3%). Note that we re-train all baselines on all datasets over 10 random splits in two experimental settings, which requires considerable computational resources.\n\n\u201cReduce features to avoid overfitting\u201d:\nThe overall number of parameters we use is very small, especially compared to other methods. We observed that reducing the number of features only degrades performance.\n\n\u201cDifferences from GCN\u201d:\nThere are several notable differences, most of which become apparent when comparing the form of classifiers proposed by each method. The classifier of GCN is f(x, W; \\theta), while ours is f(y; W(x; \\theta)). This means that:\n- GCN operates on features, while we propagate labels. The benefit of propagating labels is that labeled information is used not only to penalize wrong predictions (in the loss), but also to *generate* predictions. This is the hallmark of the LP algorithm, which we adopt.\n- GCN assumes edge weights W are given as input. These are typically set heuristically. In contrast, our method learns weights by optimizing predictive accuracy.\n- GCN uses node features x to generate embeddings, and hence does not apply to tasks where node features are not available. For our method, when features are available, we use them to parametrize W. When they are not available, we use the information-gated attention mechanism.\n\n\u201cNo comparison to GraphSage:\u201d\nGraphSage applies to an inductive learning setting. Our method is designed for a transductive learning setting.\n\n\u201cExplain \u2018LPN_nobif degrades with large T\u2019\u201d:\nFig. 4 shows that without bifurcation, accuracy can be sensitive to T. Adding bifurcation provides robustness. This is true even when the effects of bifurcation are subtle (\\tau~=1).\n\n\u201cTrend in Table 1; why is performance poor for Flickr?\u201d:\nFlickr is dense compared to others (edge/node ratio of 60:1, vs. between 1.5:1 and 5.3:1). To reduce the computational load, we sparsify the graph, which may explain the low accuracy.\n\n\u201cUncertainty estimates\u201d:\nNow added.", "title": "Clarifications, justifications, and updates"}, "rkg5ufBcAQ": {"type": "rebuttal", "replyto": "rJxDdu4A37", "comment": "Thank you for your comments, please see our responses below.\n\n\u201cNot sure I understand dynamic weights\u201d:\nYes, this is correct. The attention mechanism turns incoming soft labels (h^t) into edge weights (a^{t+1}). For given parameters \\theta^\\alpha, the attention function \\alpha is indeed fixed, but since soft labels change as they pass through the layers, so do the edge weights. When viewed as a label-propagation mechanism, weights can be thought of as changing over time.\n\n\u201cWhat is \\theta^\\tau in Eq. (13):\n\\theta^\\tau is defined just above Eq. (13) - it is simply the concatenation of a and b. The left hand side can be written as \\tau(t;a,b). In general, we use \\theta to denote parameters, and the corresponding superscript to denote what they parameterize. We will make this clearer.\n\n\u201cThe term \u2018time\u2019 is misleading\u201d:\nWe apologize for this inclarity. Indeed, we use time, iterations, and number of layers (or depth) interchangeably. This is because the network\u2019s layers simulate the iterations of LP, which we think of as applied over time. We will clarify this.\n\n\u201cHow are raw features incorporated in the loss?\u201d:\nWe use \u201craw\u201d features to parameterize edge weights. This means that the weight w_{ij} of an edge (i,j) is a function of various edge measures \\phi_{ij}, some of which are derived from raw node features (see appendix B). The function is parameterized by \\theta^\\phi - the exact form is given in Eq. (7). Edge weights then determine the predicted labels (through the label propagation mechanism), and predictions are plugged into the loss function in Eq. (14), where they are evaluated against the ground-truth labels.\n", "title": "Comments"}, "HJxISgAn2m": {"type": "review", "replyto": "r1g7y2RqYX", "review": "**** After Revision ***********\nI thank the authors for diligently revising the paper according to the reviewers' suggestions. I have increased my score for the paper. I still think the experimental evaluation can be more thorough. For example, it would be good to show the effect of varying the \\tau parameter and the number of available labels (k). It would also be good to experiment with the Flickr graph without any sparsification and to add uncertainty estimates to the results in Table 1. \n**** After Revision ***********\n\nThis paper proposes a framework for non-linear label propagation where the weights are learned simultaneously. There are model specific and experimental setup design decisions that require justification. There also needs to be a number of ablation studies to justify the effectiveness of the different components of this framework.  Finally, there seems to be an insufficient comparison (both experimentally and theoretically) to the large amount of related literature. \n- What is the total number of parameters in the proposed network? Please clarify how this is \"relatively few parameters\" as compared to other methods. \n- Please compare how your method for learning weights relates to the following papers and the references therein [1,2]\n[1] Online Learning of Multiple Tasks and Their Relationships. Saha et al, AISTATS, 2011. \n[2] Convex Learning of Multiple Tasks and their Structure, Cilberto et al, 2015. \n- It would be good to have an ablation study in order to discern what is the contribution of learning the weights vs propagating labels (instead of embeddings). \n- For clarity, please specify that \\theta are the parameters to be learned. \n- Please explain the intuition of using entropy and KL divergence for the attention weights. Shouldn't the attention for an edge be inversely proportional to the entropy i.e. the attention should be higher if the neighboring node's label is more certain?\n- Instead of the bifurcation mechanism proposed in section 3.2, isn't it possible to use a threshold to round the resulting prediction to a hard label?\n- In equation 13, are the hyper-parameters a, b tuned using cross-validation? Can't we learn the \\tau in the same training procedure? Please justify this design decision?\n- What is the performance if the loss in equation 14 is replaced by the standard empirical loss? There needs to be an ablation study on this. \n- If the node features are available, how are they used in this framework?\n- In the experimental section, why is k chosen to be equal to 1%? Please show results while varying this. \n- Please justify the line \"parameterizes w using a small number (~20) of informative features based on the raw features (e.g., dimensionality reduction), the graph (e.g., edge betweenness), and the labeled set (e.g., distance from labeled nodes). \" Isn't it possible to get similar performance by reducing the number of parameters so that model doesn't overfit?\n- Please clearly state what is the difference in the framework from the Kipf and Welling, 2016 paper?\n- Why isn't there a comparison to methods like Graph-Sage?\n- Please explain this line \"LPNnobif degrades with large T, and even \\tau slightly above 1 makes a difference\"\n- Finally, please explain the trend in the results in Table 1. For example, why is the performance of the proposed method poor on the Flickr dataset, but better on the DBLP dataset?\n- It would good to have uncertainty estimates for the results reported in Table 1. ", "title": "Method for non-linear label propagation while learning the network weights simultaneously. Insufficient comparison to related methods, insufficient experimental evidence and explanation for the results. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJxDdu4A37": {"type": "review", "replyto": "r1g7y2RqYX", "review": "This paper presents an interesting idea for the following task: given a graph and a subset of labelled nodes, infer the labels on the remaining nodes. Here the authors will make prediction for absent labels based on local averages on the graph of the neighbouring soft labels. The main originality is that the local average is weighted and the weights are learnt. \n\nI had trouble understanding the details of the algorithm and the authors should be more careful in their description of the algorithm. Some points to clarify:\n- section 3.1, I am not sure to understand the 'dynamic weights'. The main point here seems to be the use of an attention mechanism (which does not vary in time) applied to inputs varying in time?\n- section 3.2, I do not understand equation (13). What is \\theta^\\tau, it does not appear in the right-hand term?\n\nI think that using the term time is misleading. Time might refer to epochs in an optimization process, whereas time in Section 3 seems to refer to a number of layers as described in equation (6).\n\nPlease, be more explicit on the use of raw features. How are the similarities described in appendix B incorporated in the loss?\n\nOverall, I think this paper requires a lot of clarification before being published.", "title": "deep learning architecture for graph semi-supervised learning", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "SklckBGU3X": {"type": "review", "replyto": "r1g7y2RqYX", "review": "Summary\nThis paper proposes label propagation network (LPN), a neural network to learn label prediction and similarity measure (weights) between data points simultaneously in semi-supervised setting. The proposed method simulates label propagation steps with the forward pass of LPN, enabling backpropagation through label propagation steps.\n\nStrong points\n- Learning both weights and label predictions in SSL seems to be novel (provided that the author's claim in the related work section is right).\n- Good performance.\n- The paper is generally well written.\n\nConcerns\n- Replacing the label propagation by forward pass of a neural network is an attractive idea, but because of that the convergence guarantee is lost.  As Figure 4 shows, LPN without bifurcation mechanism seems to suffer from convergence issue as the number of evaluation step grows. I guess that the algorithm may go wrong even with bifurcation mechanism for some data, for example if the bifurcation rate grows too fast/slow.\n- The original label propagation works with weights without entropy. Does introducing entropy term (e(h_i;theta)) is always helpful? For instance, if some data points erroneously get certain during initial iterations, the whole algorithm may fail.\n- The performance reported for GCN is quite different from what is presented in the GCN paper, and authors explain that this is due to the different experimental setting. For me the performance gap is quite significant to be originated from different experimental setting. Could you elaborate on this? Also, how many GCN layers were used?\n- Too many hyperparameters to tune.\n\nMinor points\n- I think the line above Eq (4) should be like \\tilde w_ij = w_ij / sum_k w_ik.\n- Eq (10) is quite misleading. The original weight w_ij should be symmetric (w_ij = w_ji), but this is not. Also, considering the intuition behind the label propagation, I think Eq (10) should be like alpha_ij(h_i, h_j) = exp(e(h_j) + d(h_i, h_j)), not e(h_i) as written the paper.\n- In the experiments setting, the authors calling their algorithm as DeepLP_alpha and DeepLP_phi. I guess these should be LPN_alpha and LPN_phi.\n", "title": "Interesting idea, but heuristical.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1gZxggrcQ": {"type": "rebuttal", "replyto": "SJgzy2ogcm", "comment": "Thank you for your constructive comments!\n\n1) Our experimental setting follows the classic graph-SSL evaluation scheme used in numerous works (*). The setting used in Yang et al. (2016) and in several other following papers is slightly different. The main difference lies in how data points are partitioned into train, validation, and test sets. In the classic setting, the labeled set is created by sampling k% of the data uniformly at random. The labeled set can then be used for training, validation, or any other usage deemed appropriate, and all other unlabeled points are used for evaluation. In contrast, Yang et al. (2016) uses three distinct sets: (a) a small, class-balanced train set, (b) a designated (and fairly large) validation set, and (c) a test set comprised of only a subset of the unlabeled points.\n\nAs recently pointed out in Oliver et al. (2018), SSL evaluation should be done with care. Based on the above (and other subtle differences), we believe the classic setting is better suited as an evaluation scheme for our paper.\n\n2) Thank you for pointing out the EP paper. We will add it to the related work discussed in the paper. Our approach is different from EP, since our focus is on propagating class labels (as opposed to graph labels that are the focus of EP), and as such the aggregation and non-linearities we employ are tailored for points on the simplex, and our training loss is discriminative. It is quite likely however that the two methods can be combined.\n\n(*) Notable examples include the SSL book Chapelle et al. (2006) and, among others, papers by Zhu et al. (2003); Zhou et al. (2004); Zhang & Lee (2007); Perozzi et al. (2014); Grover & Leskovec (2016); Tang et al. (2015); Monti et al. (2017).\n", "title": "Comment on experimental setting + EP paper"}}}