{"paper": {"title": "cGANs with Projection Discriminator", "authors": ["Takeru Miyato", "Masanori Koyama"], "authorids": ["miyato@preferred.jp", "koyama.masanori@gmail.com"], "summary": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model.", "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. \nThis approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. \nWith this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. \nWe were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. \nThis new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.", "keywords": ["Generative Adversarial Networks", "GANs", "conditional GANs", "Generative models", "Projection"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a simple modification to conditional GANs, where the discriminator involves an inner product term between the condition vector y and the feature vector of x. This formulation is reasonable and well motivated from popular models (e.g., log-linear, Gaussians). Experimentally, the proposed method is evaluated on conditional image generation and super-resolution tasks, demonstrating improved qualitative and qualitative performance over the existing state-of-the-art (AC-GAN).\n"}, "review": {"BkTgv3WUG": {"type": "rebuttal", "replyto": "ByS1VpgRZ", "comment": "The code for reproducing the results in this paper has been uploaded at \nhttps://github.com/pfnet-research/sngan_projection.\nAlso we have uploaded other materials (pretrainied models, generated images and movies) at https://drive.google.com/drive/folders/1GnDuF02F3a_zNEwiA74DnaG7OQ3-Co3N.\nPlease go to the links if you are interested in our work.", "title": "The code for reproducing the results"}, "BJSIkW61f": {"type": "review", "replyto": "ByS1VpgRZ", "review": "\nI thank the authors for the thoughtful response and updated manuscript. After reading through both, my review score remains unchanged.\n\n=================\n\nThe authors describe a new variant of a generative adversarial network (GAN) for generating images. This model employs a 'projection discriminator' in order to incorporate image labels and demonstrate that the resulting model outperforms state-of-the-art GAN models.\n\nMajor comments:\n1) Spatial resolution. What spatial resolution is the model generating images at? The AC-GAN work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the Inception score versus naively resizing the image. It is not clear how much the gains of this model is due to generating better lower resolution images and performing simple upscaling. It would be great to see the authors address this issue in a serious manner.\n\n2) FID in real data. The numbers in Table 1 appear favorable to the projection model. Please add error bars (based on Figure 4, I would imagine they are quite large). Additionally, would it be possible to compute this statistic for *real* images? I would be curious to know what the FID looks like as a 'gold standard'.\n\n3) Conditional batch normalization.  I am not clear how much of the gains arose from employing conditional batch normalization versus the proposed method for incorporating the projection based discriminator. The former has been seen to be quite powerful in accomodating multi-modal tasks (e.g. https://arxiv.org/abs/1709.07871, https://arxiv.org/abs/1610.07629\n). If the authors could provide some evidence highlighting the marginal gains of one technique, that would be extremely helpful.\n\nMinor comments:\n- I believe you have the incorrect reference for conditional batch normalization on Page 5.\nA Learned Representation For Artistic Style\nDumoulin, Shlens and Kudlur (2017)\nhttps://arxiv.org/abs/1610.07629\n\n- Please enlarge images in Figure 5-8. Hard to see the detail of 128x128 images.\n\n- Please add citations for Figures 1a-1b. Do these correspond with some known models?\n\nDepending on how the authors respond to the reviews, I would consider upgrading the score of my review.", "title": "Review of cGAN with projection discriminator", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkyTeFweM": {"type": "review", "replyto": "ByS1VpgRZ", "review": "The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset. Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) . This implementation basically restricts the conditional distribution p(y|x) to be really simple and seems to be posing a good prior leading to great empirical results.\n\n+ Quality:\n- Simple method leading to great results on ImageNet!\n- While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work. One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes.\n- Appropriate comparison with existing conditional models: AC-GANs and PPGNs.\n- Appropriate (extensive) metrics were used (Inception score/accuracy, MS-SSIM, FID)\n\n+ Clarity:\n- Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2).\n- PPG should be PPGNs.\n\n+ Originality:\nThis work proposes a simple method that is original compared existing GANs.\n\n+ Significance:\nWhile the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger.\n\nOverall, I really enjoy reading this paper and recommend for acceptance!\n\n\n\n\n", "title": "simple, interesting GAN modification; great results", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SynfBlcgf": {"type": "review", "replyto": "ByS1VpgRZ", "review": "This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network.  It motivates the method by examining the form of the log density ratio in the continuous and discrete cases.\n\nThis paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores). \n\nWhat bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline. In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience. It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward.\n\nThe sentence containing \"assume that the network model can be shared\" had me puzzled for a few minutes. I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer.", "title": "An unusually thorough GAN paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rk3jNQPGf": {"type": "rebuttal", "replyto": "BJYUqi--G", "comment": "We are sorry but we forgot to note the reference information of \"Gulrajani et al. (2017)\" in the previous comment.\n\nReference:\nImproved training of Wasserstein GANs\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin and Aaron Courville\nIn NIPS2017", "title": "Reference"}, "HJ-udwIGf": {"type": "rebuttal", "replyto": "SynfBlcgf", "comment": "We reflected your suggestion on our revision and conducted the hyper-parameter search on CIFAR-100 about the Adam hyper-parameters (learning rate $\\alpha$ and 1st order momentum $\\beta_1$).\nNamely, we varied each one of these parameters while keeping the other constant, and reported the inception scores for all methods including several versions of \u201cconcat\u201d architectures to compare. \nMore specifically, we tested with concatenation module introduced at (a) input layer, (b) hidden layer, and at (c) output layer.  The results of this complementary experiment are now provided in the of the appendix section A of the revised paper.\n\nAs we can see in Figure 11, our \u201cprojection\u201d architecture excelled over all other architectures for all choice of the hyper-parameters, and achieved the inception score of 9.53.  Meanwhile,  concat architectures were able to achieve all 8.82 at most.  \nThe best concat model in term of the inception score on CIFAR-100 was the hidden concat with $\\alpha$=0.0002 and $\\beta_1$ = 0, which turns out to be the very choice of the parameters we picked for the original ImageNet experiment. Unfortunately,  we were not able to secure the time for the parameter search on ImageNet experiment. However, from the way the outcomes look for the CIFAR-100, we speculate the same to happen.  \n", "title": "We conducted hyper parameter search on CIFAR-100 dataset, and confirmed our projection model achieved better performance on all choice of the hyper-parameters."}, "BJERLD8fz": {"type": "rebuttal", "replyto": "ByS1VpgRZ", "comment": "We owe great thanks to all reviewers for helpful comments to improve our manuscripts. \nWe revised our manuscript based on the reviewer\u2019s comments and uploaded the revision.  \nFor an important note, we re-calculated the inception scores and FID with the original evaluation code written in TensorFlow,  because the results slightly differed from our rendition written in Chainer. \nPlease rest assured, however, because the newly computed results does not affect any claims we have made on the original version of our paper.\n\nAlso, to show the efficacy of our method on smaller benchmark datasets,  we added the results on CIFAR-10 and CIFAR-100 datasets. \nOur projection model was able to eclipse the comparative models (concat discriminator models and AC-GANs) on these datasets as well.  \nPlease see Appendix A for details. \n", "title": "Uploaded the revision"}, "BJYUqi--G": {"type": "rebuttal", "replyto": "SynfBlcgf", "comment": "\n>What bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline. In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience. \n\nWe did not perform any hyper-parameter optimization for the Adam optimizer and the number of critic updates, etc\u2026 \nWe just used the same hyper-parameters used in Gulrajani et al. (2017, https://github.com/igul222/improved_wgan_training/blob/master/gan_cifar_resnet.py ), because we adopted the practically the same architecture used in the very paper.   \nWe must admit that we simply could not spare enough time for the parameter search for the ImageNet experiments.  However, we plan to do the search for (beta1, alpha of Adam) on CIFAR 10 or CIFAR 100 and compare the performance against \"AC-GANs\", \"concat\" and \"projection\".\n\n\n>The sentence containing \"assume that the network model can be shared\" had me puzzled for a few minutes. I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer.\n\nThank you very much! We concur with you in your views and we will reflect the suggestion on this part of the revision.   \n \n\n", "title": "Response to AnonReviewer2"}, "Byeml2ZWM": {"type": "rebuttal", "replyto": "BJSIkW61f", "comment": "\n> 1) Spatial resolution. What spatial resolution is the model generating images at?\n \nWe are sorry for the lack of the spatial resolution information. The model generates at \"128x128\" spatial resolution. \n\n\n> 1) Spatial resolution,  3) Conditional batch normalization,\n\nThe goal of our paper is to show the efficacy of our \"projection\" model for the discriminator, so all our experiments use same architecture for the generator.  In all our experiments, we are equipping the generator with conditional BN. This includes our experiments with \"AC-GANs\", as well as \"concat\" model and \"projection\" model.  \nWe can indeed explore the same result with generators equipped with a different way to introduce the conditional information (such as label concatenation); however, we intend not to make this (generator structure) the focus of our paper. On the base of our theoretical motivations, (Section 3) we also believe that our way will perform well even with a different way of label conditionalization of the generator.  \nWe would also like to emphasize that our projection model is prevailing on the super-resolution task as well, suggesting that our success is not the model and task specific. \nLastly, as interesting as it is, we would not find a way to include the dependence of the performance on the image resolution into the scope of our paper. \n\n\n>2) \n>FID in real data. The numbers in Table 1 appear favorable to the projection model. Please add error bars (based on Figure 4, I would imagine they are quite large).\n\nWe are sorry, but we are little confused about this suggestion. First of all, we are dealing with images from different classes in our experiments. The difficulty of image generation differs across each class, and the intra FID shall depend on the dataset of each class.  We, therefore, found no particular need for showing the size of its variance (error bar) in this experiment.  The goal of our Figure 4 here is to simply show that our projection method outperforms \"concat\" and \"AC-GANs\" on \"most of the classes\",  and we felt it more appropriate to visualize our claim with scatter plot. \n\n\n>Additionally, would it be possible to compute this statistic for *real* images? I would be curious to know what the FID looks like as a 'gold standard.'\n\nPlease take a look at the definition of FID(p5, (Heusel et al., 2017)) .  FID is a measure of a difference between two distributions. If there are infinitely many 'real' images, the FID between 'real' images against 'real' images is trivially 0.  In our paper, we are comparing the empirical distribution of generated samples over 5000 samples against the that of the training 'real' images.  If we compute the empirical distribution of 'real' images against another empirical distribution of the 'real' images,  we are bound to observe some nonzero FID value.  However, we find no particular importance in computing such value.  \n\n\n> Minor comments:\n>- I believe you have the incorrect reference for conditional batch normalization on Page 5.\n>- Please enlarge images in Figure 5-8. Hard to see the detail of 128x128 images.\n>- Please add citations for Figures 1a-1b. Do these correspond with some known models?\n\nThanks for pointing out the incorrect references! We would revise the designated citations accordingly. We would also like to modify the figure images to improve the visuality.\n", "title": "Response to AnonReviewer3"}, "HJd7sibbz": {"type": "rebuttal", "replyto": "rkyTeFweM", "comment": "We are very glad to hear that you enjoy our manuscript!\n\n> While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work. One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes.\n>While the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger.\n\nAblation study was in fact a vexing issue in our paper, and we are still unsure of a way to theoretically back up our results. We may attempt your suggestion, and meanwhile continue looking for still other convincing experiments.\n\n\n> Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2).\n> PPG should be PPGNs.\n\nThanks for pointing out the mistakes! We will make changes accordingly in the revised version.\n  \n", "title": "Response to AnonReviewer1"}, "ByeD_ibZz": {"type": "rebuttal", "replyto": "ByS1VpgRZ", "comment": "We thank all three reviewers for thorough reading of our manuscript and their comments and suggestions.\nWe responded to all the suggestions and made corrections for each reviewer\u2019s comment separately.", "title": "Thank you so much for the reviews!"}, "HJP0Zr3ef": {"type": "rebuttal", "replyto": "r1-ygQixf", "comment": "We were clearly making typos in the reference. \nThe reference you mentioned is the very reference we intended to cite.  \nAs for the use of the word  \u201cFiLM\u201d, we would like to stick for now to the \u201cconditional batch normalization\u201d to make it easy for the readers to readily catch the framework of our algorithm.  ", "title": "Thanks for pointing out the mistake!"}}}