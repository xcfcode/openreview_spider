{"paper": {"title": "Robust Local Features for Improving the Generalization of Adversarial Training", "authors": ["Chuanbiao Song", "Kun He", "Jiadong Lin", "Liwei Wang", "John E. Hopcroft"], "authorids": ["cbsong@hust.edu.cn", "brooklet60@hust.edu.cn", "jdlin@hust.edu.cn", "wanglw@cis.pku.edu.cn", "jeh@cs.cornell.edu"], "summary": "We propose a new stream of adversarial training approach called Robust Local Features for Adversarial Training (RLFAT) that significantly improves both the adversarially robust generalization and the standard generalization.", "abstract": "Adversarial training has been demonstrated as one of the most effective methods for training robust models to defend against adversarial examples. However, adversarially trained models often lack adversarially robust generalization on unseen testing data. Recent works show that adversarially trained models are more biased towards global structure features. Instead, in this work, we would like to investigate the relationship between the generalization of adversarial training and the robust local features, as the robust local features generalize well for unseen shape variation. To learn the robust local features, we develop a Random Block Shuffle (RBS) transformation to break up the global structure features on normal adversarial examples. We continue to propose a new approach called Robust Local Features for Adversarial Training (RLFAT), which first learns the robust local features by adversarial training on the RBS-transformed adversarial examples, and then transfers the robust local features into the training of normal adversarial examples. To demonstrate the generality of our argument, we implement RLFAT in currently state-of-the-art adversarial training frameworks. Extensive experiments on STL-10, CIFAR-10 and CIFAR-100 show that RLFAT significantly improves both the adversarially robust generalization and the standard generalization of adversarial training. Additionally, we demonstrate that our models capture more local features of the object on the images, aligning better with human perception.", "keywords": ["adversarial robustness", "adversarial training", "adversarial example", "deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "Earlier work suggests that adversarial examples exploit local features and that more robust models rely on global features. The authors propose to exploit this insight by performing data augmentation in adversarial training, by cutting and reshuffling image block. They demonstrate the idea empirically and witness interesting gains. I think the technique is an interesting contribution, but empirically and as a tool.\n"}, "review": {"Byg-szbhtH": {"type": "review", "replyto": "H1lZJpVFvr", "review": "In this paper, the authors proposed a new approach to improve the robustness of CNNs against adversarial examples.\nThe recent studies show that CNNs capture local features, which can be easily affected by the adversarial perturbations.\nThus, in the paper, the authors proposed to train CNNs so that they can capture local features that are robust against the adversarial perturbations.\nThe difficulty here is that existing adversarial training algorithms tend to bias CNNs to ignore local features and to capture only global features.\nTo avoid this unfavorable property of the adversarial training, the authors proposed the random block shuffle (RBS) that intensionally destroys the global feature of the images.\nThe authors demonstrated that combining RBS with the existing adversarial training algorithms can lead to robust CNNs.\n\nI found the paper well-written and the idea is easy to follow.\nEspecially, the use of RBS seems to be an interesting idea.\nAs a small downside, the proposed approach looks rather straightforward, and I expect to see any theoretical foundations if possible.\n\n### Updated after author response ###\nIn summary, the contribution of this study is in twofolds.\n1. Proposed an algorithm for learning robust local features.\n2. Demonstrated that learning robust local features is effective to improve the robustness of the model.\nThe possible downside is\n3. The proposed approach looks straightforward.\nOverall, I like the paper (especially for the reason 2 above), and therefore keep my score.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}, "rJlBQbyjir": {"type": "rebuttal", "replyto": "B1gR0K4ViH", "comment": "Dear reviewer #1,  we believe we have addressed your concerns and clarified your points in the rebuttal. Do you have an updated assessment (or concerns) of our paper? Thanks for your consideration. ", "title": "Thanks for your attention."}, "B1egLD4EiH": {"type": "rebuttal", "replyto": "H1lZJpVFvr", "comment": "We deeply appreciate all reviewers for the thorough comments and valuable suggestions, which definitely help the improvement of our paper. We would like to briefly summarize our modification in the updated paper and provide specific response in the individual comment.\n\nOur main modifications are as follows:\n\n- Polish the overall writing; emphasize our motivation and clarify the intuition.\n\n- In Section 4.3, we have reorganized the content and replaced the performance analysis on uniform noise with the performance analysis on brightness perturbation.\n\n- To make the results more convincing, we have supplied the training codes of PGDAT and TRADES in the initial code link (i.e., the files \"train_pgdat.py\" and \"train_trades.py\")\n\n- In Section 5 (Conclusion and Future Work), we indicate the possible directions for future research based on our observations and also the suggestion of Reviewer 2 on the benefit of robust local features.\n\nWe provide strong empirical support for our hypothesis that robust local features can improve the generalization of adversarial training, and we believe that our observations on the benefit of robust local features in adversarial training are very useful and interesting to the community, as it opens a new avenue for improving adversarial training from the perspective of features.\n\nWe hope all our effort can make our paper more comprehensive and address most of your concerns. Thank you very much!\n\n\nBest regards,\n\nAuthors\n\n \n", "title": "General response"}, "HklhB5cqjS": {"type": "rebuttal", "replyto": "Hkxict-tsr", "comment": "Thank you very much for the quick reply and raising a possible future research direction for us. Following your constructive suggestion, we have performed revision in the conclusion section.\n\nIn this work, in order to explore the relationship between the generalization of adversarial training and the robust local features, we advocate for exploring robust local features by reducing the impact of global information, which is necessary to validate our main hypothesis in this work. However, as you indicated, it does not mean that the global information is useless, as the global information is useful for some tasks, e.g. as you suggested, answering \"How many apples in the image\".\n\nWe agree that training a robust model that can capture both global and local information will be an important direction. From a methodological standpoint, the discovered relationship may also serve as an inspiration for new adversarial defenses, where not only the robust local features but also the global information is taken into account. One way of introducing the global information would be through adding the term of \"adversarial training on original adversarial examples\", whereas it is beyond the scope of our main theme (i.e., on the relationship between the generalization of adversarial training and the robust local features) in this work. These questions are indeed worth investigation in future work, and we hope that our observations on the benefit of robust local features will inspire more future development.\n\n", "title": "RE: RE: RE: Review #2"}, "H1gjudE4oS": {"type": "rebuttal", "replyto": "H1lZuixYcS", "comment": "We deeply appreciate the reviewer for the positive, insightful comments and constructive suggestions. We have revised our paper accordingly.\n\nA1. This suggestion is great and realistic, which focuses more on the adversarial robustness in the physical world. In Section 4.3, LOSS SENSITIVITY UNDER DISTRIBUTION SHIFT, we replace the \u201cuniform noise addition\u201d to the \u201cbrightness perturbation\u201d. We adopt the brightness perturbation with varied daylight intensity, and present the results in Table 2a in the revision. We observe that, as compared to PGDAT and TRADES, both RLFAT_P and RLFAT_T show lower gradients of the models on different data distributions, which we can directly attribute to the robust local features.\n\n\nA2. We report the running time of 400 iterations of training for RLFL_p with or without line 7, on a single Titan X GPU.\n\n-----------------------------------------------------------------------\n| Dataset\t|     RLFL_p     | RLFL_p without line 7  |\n-----------------------------------------------------------------------\n| STL-10\t        |\t681s\t|\t680s\t                   |\n| CIFAR-10\t|\t720s\t|\t719s\t                   |\n| CIFAR-100\t|\t541S\t|\t540s\t                   |\n-----------------------------------------------------------------------\n\nWe see that the computational cost is mainly concentrated on the generation of adversarial examples and the optimization of the loss function, and the cost of RBS transformation is negligible in the total computational cost.\n\n\nA3. Thanks for introducing the multiple instance setting [1]. Multiple instance setting usually combines the features of different instances in a bag, where the instances share the label of the original image.\n\nThe similarity between the two is that the original image need to be split into several blocks or patches. But intrinsically they are different. First, the goal of multiple instance learning (MIL) is to reduce the effect of noisy labels introduced by data augmentation, whereas the goal of our approach is to explore the robust local features. Second, MIL has a large computational cost, as MIL has multiple input instances in the bag. In contrast, the cost of RBS can be ignored. Third, our objective function differs to MIL that considers the probabilities of the instances, which is more like an ensemble learning. While our approach utilizes the random shuffled blocks as a whole to learn the robust local features. \n\n[1] Multiple Instance Learning Convolutional Neural Networks for Object Recognition. ICPR 2017. \n\n", "title": "RE: Review #3"}, "B1gR0K4ViH": {"type": "rebuttal", "replyto": "rkxiT9QatH", "comment": "Thank you for your insightful comments.  We have performed the corresponding revisions based on your constructive suggestions. And our responses to your concerns are as follows: \n\n\nA1. We have strengthened the motivation of the feature transfer term in the revision, Section 3.2: \u201cSince the type of input images in the training phase and the inference phase is different (RBS transformed images for training, versus original images for inference), we consider to transfer the knowledge of the robust local features learned by RBSAT to the normal adversarial examples.\u201d With this purpose, we insert the feature transfer term in the objective function.\n\nIn this work, we advocate for learning the robust local features. The difference of \"minimizing the loss on original PGD image\" and \"the feature transfer term\" is the supervised information. Instead of using our proposed feature transfer term, if we directly minimize the loss on the original PGD images, the supervised information is the true label of the images. But as [1] suggests, then the model might explore the global structural features. In contrast, our term uses the robust local feature representation learned from RBS adversarial training as the supervised information for the original PGD images.\n\n\nA2. Sorry for the confusion, and we will try to explain as follows:\n\nReproducing the results with our released codes might provide more intuitive understanding on the performance. Our code builds on the code framework of Madry\u2019s. To make an apples-to-apples comparison, we re-implement the loss function of TRADES with Tensorflow, and use the same training hyper-parameters for all the methods. The implementation code and all the models are released with the submission. \n\n(a)\tThe difference of performance for PGDAT:\n\nThe iteration step of PGD in Madry's paper is set to 7. For CIFAR10, following the settings in TRADES, the iteration step of PGD to generate adversarial examples is set to 10 in our paper, as we want to use the same parameters for all methods for a fair comparison. In the literature, many researchers also use different steps like 10 [2], or 20 [3] for PGD, as long as they keep the same parameters for all methods. Using 10 indicates that we have a stronger training but at the same time a stronger attack that may reduce the robust accuracy. \n\n(b)\tThe difference of performance for TRADES:\n\nFor TRADES, the authors reported the \u2018best\u2019 result of 1/\u03bb=6 rather than the \u2018mean\u2019 result of 1/\u03bb=6. Our result (50.95%, 1/\u03bb=6\uff09is slightly higher than the mean result (50.64%) of 1/\u03bb=5 in TRADES\u2019s paper (Table 4). The mean robust accuracy increases slowly by the increasing of 1/\u03bb. Thus, we think the mean result of 1/\u03bb=6 is supposed to be slight higher than 50.64%, which is close to our result of 50.95%. \n\n \nA3. Local features are like the local texture and local edges of the object in an image, while global features are like the global shape of the object and some details are ignored. \n\nLocal features seems to be well-generalized on unseen shape variation [4] but less robust against adversarial perturbation. We use RBS transformation to break up the global structure information of the adversarial images, and training on the reshuffled adversarial images can force the model to explore the robust local features. In addition, the random shuffle is a way of augmenting the adversarial examples that can help the generalization.\n\nTo provide more intuition, we investigate the features of the input images that the models are focused by the visualization of SmoothGrad. Our model captures more local feature information of the object, aligning better with human perception. \n\n[1]\tInterpreting adversarially trained convolutional neural networks. ICML 2019.\n[2]\tAre Labels Required for Improving Adversarial Robustness. NeurIPS 2019.\n[3]\tAdversarial Training for Free! NeurIPS 2019.\n[4]\tImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. ICLR 2019. \n\n \n", "title": "RE: Review #1"}, "SyxxxKE4oH": {"type": "rebuttal", "replyto": "Byg-szbhtH", "comment": "We appreciate your time in reviewing our paper and your positive comments. We will try to address your concern as follows.\n\nWe thank you for raising the question of formal theoretical foundation. The theoretical foundation of the proposed approach is highly dependent on the theory of the feature disentanglement. The disentanglement of the features from images is a rising direction, such as how to disentangle the robust local features, the features strongly related to label or the features aligning better with human perception from images, and actually there is little disentanglement work [1] in the field of deep learning for adversarial robustness. \n\nWhile this work shows the general relationship between the generalization of adversarial training and the robust local features, there remains several open questions: how to explicitly disentangle the robust local features? what is the best way to leverage the robust local features for adversarially robust generalization? These are also our future work to make further investigation. We hope this work inspires more work on the theoretical side of the adversarial robustness, and analysis on the disentanglement of robust features.\n\n[1] Adversarial Examples Are Not Bugs, They Are Features. NeurIPS 2019.\n \n", "title": "RE: Review #2"}, "rkxiT9QatH": {"type": "review", "replyto": "H1lZJpVFvr", "review": "The work suggests reshuffle images blocks of adversarial examples during adversarial training, in order to improve the generalization performance on benign and adversarial test samples.  The main method is based on the hypothesis in [Zhang et al 2019], [Ilyas et al 2019].  The assumption claims that robust models rely on global structural features, and non-robust models rely on local features. Thus, the work tries to learn local robust features, by cutting and reshuffling the image blocks. Overall the idea is interesting and the paper is well written .\n\nHowever, there are some concerns about the presentation and the main methodology:\n1.\tCan the paper give more explanation on the purpose of inserting the feature transfer term in the objective function? What is the difference of the proposed one with directly minimizing the loss on both original PGD image and reshuffled image?\n2.\tFor CIFAR10, TRADEs and PGDAT\u2019s performance in the result is not as good as the ones shown in their original works, which is comparable to the performance of the proposed RLFAT method. More discussions are needed, otherwise the experimental results are not convincing. \n3.\tMore intuitions are needed  on what local and global features are, and why training on the reshuffled images can help learn generalizable robust local features. \n\nOverall the paper is easy to understand, but we suggest that more insight should be given on the success of the proposed method.\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 2}, "H1lZuixYcS": {"type": "review", "replyto": "H1lZJpVFvr", "review": "The paper is interested in robustness w.r.t. adversarial exemples. \n\nThe authors note that:\n* features reflecting the global structure are more robust wrt adversarial perturbations, but generalize less;\n* features reflecting the local structure generalize well, but are less robust wrt adversarial perturbations. \nIn hindsight, these claims are intuitive: adversarial perturbations and unseen shape variations are of the same flavor; one should resist to both or handle both, with the difference that the latter is bound to occur (and should be handled) and the former is undesired (and should be resisted). \n\nThe goal thus becomes to define local features that are robust. \n\nThe proposed approach is based on \n* enforcing the invariance of the intermediate representation through shuffling the blocks of the training images; \n* building normal adversarial images x' and deriving the block shuffling RBS such that the x' and RBS(x') are most similar w.r.t. the logit layer\n* adding these RBS(x') to the training set;\n\nThe idea is nice; the experiments are well conducted and convincing (except for the addition of uniform noise, which is unrealistic; you might consider instead systematic noise mimicking a change of light);\nI'd like more details about:\n* The computational cost of line 7 in algo (deriving the best RBS).\n\nYou might want to discuss the relationship between the proposed approach and the multiple instance setting (as if the image was a bunch of patches). ", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 2}}}