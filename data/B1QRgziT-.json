{"paper": {"title": "Spectral Normalization for Generative Adversarial Networks", "authors": ["Takeru Miyato", "Toshiki Kataoka", "Masanori Koyama", "Yuichi Yoshida"], "authorids": ["miyato@preferred.jp", "kataoka@preferred.jp", "koyama.masanori@gmail.com", "yyoshida@nii.ac.jp"], "summary": "We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.", "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ", "keywords": ["Generative Adversarial Networks", "Deep Generative Models", "Unsupervised Learning"]}, "meta": {"decision": "Accept (Oral)", "comment": "This paper presents impressive results on scaling GANs to ILSVRC2012 dataset containing a large number of classes. To achieve this, the authors propose \"spectral normalization\" to normalize weights and stabilize training which turns out to help in overcoming mode collapse issues.  The presented methodology is principled and well written. The authors did a good job in addressing reviewer's comments and added more comparative results on related approaches to demonstrate the superiority of the proposed methodology. The reviewers agree that this is a great step towards improving the training of GANs.  I recommend acceptance."}, "review": {"Skl_XvAyym": {"type": "rebuttal", "replyto": "rybjgmM0M", "comment": "Hi,  thanks for your interest in our work! \n\nWe actually propagate the gradient through the spectral norm.\n\nThe line :https://github.com/pfnet-research/sngan_projection/blob/master/source/functions/max_sv.py#L30\ncalculates the spectral norm given W. \nWhen we apply the backprop, the gradient will propagate to W, which is the weight parameter of linear / conv layer.\n\n", "title": "Re: This is projected gradient "}, "Skch8h-Iz": {"type": "rebuttal", "replyto": "B1QRgziT-", "comment": "The code for reproducing the results in this paper has been uploaded at \nhttps://github.com/pfnet-research/sngan_projection.\nAlso we have uploaded other materials (pretrainied models, generated images and movies) at https://drive.google.com/drive/folders/1GnDuF02F3a_zNEwiA74DnaG7OQ3-Co3N.\nPlease go to the links if you are interested in our work.", "title": "The code for reproducing the results"}, "SkQdbLclM": {"type": "review", "replyto": "B1QRgziT-", "review": "This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives. The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator. This Lipschitz property has already been proposed by recent methods and has showed some success. However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator. This is demonstrated in comparison to weight normalization in Figure 4. The experimental results are very good and give strong support for the proposed normalization.\n\n\nWhile the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs. The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models. I am recommending acceptance, though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art. More details in the comments below.\n\nComments:\n1. One concern about this paper is that it doesn\u2019t fully answer the reasons why this normalization works better. I found the discussion about rank to be very intuitive, however this intuition is not fully tested.  Figure 4 reports layer spectra for SN and WN. The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency. I would like to see the same spectra included. \n2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge? One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments. What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win?\n3. Section 4 needs some careful editing for language and grammar.\n", "title": "Standard idea, great results ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1xyfspez": {"type": "review", "replyto": "B1QRgziT-", "review": "This paper proposes \"spectral normalization\" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function. The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient. Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods.\n\nOverall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach. The experimental results seem solid and seem to support the authors' claims. I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer. Like the anonymous commenter, I also initially thought that the proposed \"spectral normalization \" is basically the same as \"spectral norm regularization\", but given the authors' feedback on this I think the differences should be made more explicit in the paper.\n\nOverall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication.\n\nSmall Nits: \n\nSection 4: \"In order to evaluate the efficacy of our experiment\": I think you mean \"approach\".\n\nThere are a few colloquial English usages which made me smile, e.g. \n * Sec 4.1.1. \"As we prophesied ...\", and in the paragraph below \n * \"... is a tad slower ...\".", "title": "Nice step forward in improving training of GANs", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJH-EWkWM": {"type": "review", "replyto": "B1QRgziT-", "review": "The paper is motivated by the fact that in GAN training, it is beneficial to constrain the Lipschitz continuity of the discriminator. The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a \"spectrally normalized\" objective.\n\nI think the methodology presented in this paper is neat and the experimental results are encouraging. However, I do have some comments on the presentation of the paper:\n\n1. Using power method to approximate matrix largest singular value is a very old idea, and I think the authors should cite some more classical references in addition to (Yoshida and Miyato). For example,\n\nMatrix Analysis, book by Bhatia\nMatrix computation, book by Golub and Van Loan.\n\nSome recent work in theory of (noisy) power method might also be helpful and should be cited, for example,\nhttps://arxiv.org/abs/1311.2495\n\n2. I think the matrix spectral norm is not really differentiable; hence the gradients the authors calculate in the paper should really be subgradients. Please clarify this.\n\n3. It should be noted that even with the product of gradient norm, the resulting normalizer is still only an upper bound on the actual Lipschitz constant of the discriminator. Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?", "title": "Paper review: the methodology is neat, but presentation can be improved", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "r1onL2xXM": {"type": "rebuttal", "replyto": "B1QRgziT-", "comment": "We owe great thanks to all reviewers for helpful comments toward improving our manuscripts. \nWe revised our manuscript based on the reviewer\u2019s comments (including the ones that were visible to us by mistake because of the administrator\u2019s technical problem) and uploaded the revision.\n\nFirstly,  we conducted additional comparative study against orthonormal regularization, and showed the advantage of our algorithm over the orthonormal regularization.  \n\nSecondly,  we responded to the AnonReviewer2\u2019s comment by running still another experiment with weight clipping (Arjovsky et al. 2017) and compared the results on CIFAR10 and STL10.  \nWe confirmed that, as we have noted in Section 3, weight clipping also suffered from the rank degeneracy and its performance turned out to be much worse than our spectral normalization.\n\nThirdly, for the ImageNet, we re-calculated the inception scores for all methods using the original tensorflow implementation and replaced the scores on the table, because we were using Chainer instead of Tensorflow exclusively for the ImageNet results, and there were some numerical variations. The newly computed values do not affect any of our claims regarding the advantages and the superiority of our algorithm.\n", "title": "Uploaded the revision."}, "SJpmh_17f": {"type": "rebuttal", "replyto": "BkOctTAGf", "comment": "Hi, thanks for your comment.\n\nWe will share the reproducing code after the acceptance notification.\nWe will announce the link to the code here when we make the code public.", "title": "Thanks for the comment."}, "HJxGRNvMz": {"type": "rebuttal", "replyto": "SJok1XB-f", "comment": "We need to remember that the paper you designated uses the classic loss function for both \u201cgenerator\u201d and discriminator updates.  As we explain in the experiment section, we are using the modified generator updated rule proposed by Goodfellow et al (2014), which uses softplus function -log sigmoid(f(x)) = log  (1 + exp(-f(x))) := softplus(-f(x)) in place of log (1- sigmoid(f(x))) so that one can maintain the learning process.  Note that softplus(-f(x)) is approximately -f(x)  when f(x) < 0 (In fact, on the bulk of the support of the generator,  f(x) tends to be negative. ). Thus, the generator will be looking at the gradient of f(x) on the course of its training.  As such, we need to keep our eyes on the gradient of f(x), which can blow up outside of the support of p or q (see Eq (4) ) without any gradient regularization as a countermeasure. \nSo far, this is our current postulate on the importance of Lipschitz constant in GAN. The gest of our paper is that, WGAN-GP and our spectral normalization can constrain the norm of the gradient so that this will not be a problem. \n \nAlso, we shall make it clear that our method is not designed specifically for the purpose of preventing mode-collapse.  However, it is not hard to imagine that the control of the Lipschitz constant of the discriminator would prevent the training process of the generator to plateau prematurely because of the critical gradient problem we have described above.  \n", "title": "Thanks for the comment!"}, "BJAcWZobM": {"type": "rebuttal", "replyto": "H1xyfspez", "comment": "\nThank you so much for the review!\n\n>I also initially thought that the proposed \"spectral normalization \" is basically the same as \"spectral norm regularization\", but given the authors' feedback on this I think the differences should be made more explicit in the paper.\n\nThanks for the suggestion; we will emphasize the difference between spectral norm regularization and our spectral normalization in the revised manuscript.\n\n\nAnd thanks for pointing out the colloquialism, we will relax it :-)\n\n", "title": "Response to AnonReviewer 1"}, "BkIYgWs-z": {"type": "rebuttal", "replyto": "HJH-EWkWM", "comment": "\nThank you so much for the review!\n\n\n>The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a \"spectrally normalized\" objective.\n\nThank you very much for the comments;  however we would like to emphasize that we are controlling the spectral norm of the operators, not their gradient.  Also, unlike what we refer to as  \u201cgradient penalty method\u201d,  we are not modifying the objective function in any means. We are still using the same objective function as the classic GAN; we are just looking for the candidate discriminator from the normalized set of functions.\n\n\n> 1. I think the authors should cite some more classical references in addition to (Yoshida and Miyato). \n\nThanks for the remark, and yes we should have cited some of the classic references. We will add them to the revised manuscripts.\n\n\n>2. I think the matrix spectral norm is not really differentiable; hence the gradients the authors calculate in the paper should really be subgradients. Please clarify this.\n\nIndeed,  when the spectrum has multiplicities, we would be looking at subgradients, and technically we should have said so. However, the probability of this happening is zero (almost surely), and we assumed we can continue discussions without giving considerations to such events.  We will make note of this fact in the revised version.  Thanks!\n\n\n>3.  Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?\n\nWe would like to remind that the gest of our paper is not about the accuracy of the Lipschitz constant;  we do not intend to claim that our spectral normalization better controls the Lipschitz constant than the gradient penalty method. \nAs we claim in Section 3, an advantage of our normalization over the gradient penalty based method (WGAN-GP) is that we can control the Lipschitz constant even outside the neighborhoods of the observed datapoint. \nFurthermore, spectral normalization can be carried out with less computational cost. Please see the discussions in the designated section for more detail.\n\n\n", "title": "Response to AnonReviewer3"}, "rynszWibz": {"type": "rebuttal", "replyto": "SkQdbLclM", "comment": "\nThank you so much for the review!\n\n>This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives. \n\nThank you very much for the comments;  we however would like to remind that the spectral normalization presented in this paper is very much different from spectral \u2018norm\u2019 regularization introduced in Yoshida and Miyato (2017). \nAlso,  unlike what we refer to as  \u201cgradient penalty method\u201d,  we are not regularizing the objective function in any means, so \u201cnormalize GAN objectives\u201c is an inaccurate keyword for our paper. \nWe are still using the same objective function as the classic GAN; we are just looking for the candidate discriminator from the normalized set of functions.\nWe will emphasize these points in the revised manuscript, since these confusions seem to be recurring issues. \n\n\n>1.  The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency. I would like to see the same spectra included. \n\nThanks for the suggestion. We plan to test with the weight clipping method  (Arjovsky et al. 2017) and report the results in the revised manuscript.\n\n\n>2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge? One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments. What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win?\n\nThat sounds like a good suggestion. Should there be ample rooms in the computational resource and time, we might try the experiment with CIFAR 10.  \n\n\n>3. Section 4 needs some careful editing for language and grammar.\nThanks, we will proofread the document once again. \n\n", "title": "Response to AnonReviewer2 "}, "B1zjIDhef": {"type": "rebuttal", "replyto": "r1skYxEgG", "comment": "Thanks for your comments, and thank you very much for letting us know about the closely related work.  Our stance on this matter is that, mathematically, spectral normalization and orthonormality constraints are quite different, because the orthonormality constraint destroys the information about the spectrum by setting all the singular values to one.  On the other hand, spectral normalization only scales the spectrum so that its maximum will be one.  Therefore it is probably hard to say which method is more superior than the other, and their utility depends on the task and the dataset.  \n\nFor an additional set of comparative study, we implemented GANs with orthonormality constraint. For CIFAR10, spectral normalization outperformed the orthonormality constraint in terms of the inception score with most of the hyper-parameter settings.  On STL, our method seems to be losing with many hyper-parameter settings.  We summarize the inception scores achieved by the orthonormality method on CIFAR10 and STL. The values inside the parenthesis are the inception scores achieved by spectral normalization. \n(A-F corresponds to the different settings of the hyper-parameters introduced on the paper, Table 1)\n-CIFAR10\nA 6.37 (7.03) B 6.13 (7.20) C 6.71 (7.42) D 7.40 (7.29) E 6.68(7.16) F 6.30 (6.43)\n-STL10\nA 8.04 (6.37) B 8.39 (8.29) C 8.56 (8.14) D 7.99 (7.87) E 2.24(7.80) F 6.55 (5.73)\nThe experiment on the ImageNet is still running, and we will be getting results in a week or two.  \n\n>Besides, spectral normalization (SN) is proposed by (https://arxiv.org/abs/1705.10941).\nWe would like to emphasize that our method is different from spectral norm regularization.  Unlike our method, their method penalizes the spectral norm by additional regularization term (Eq.(1), https://arxiv.org/abs/1705.10941). Their method is fundamentally different from our method in that they do not make an attempt to \u2018set\u2019 the spectral norm to a designated value.  Also, as we mention in Eq.(11) and (12), when we reorganize the derivative of our normalized cost function, we can see that our method is in fact practically imposing a \u201csample data dependent\u201d regularization on the cost function.  \nSpectral norm regularization, on the other hand, imposes \u201csample data independent \u201d regularization on the cost function,  just like L2 regularization and Lasso. \nIn fact, when we trained GANs with 'spectral norm regularization', each layer\u2019s spectral norm quickly shrunk to 0 and the training practically stopped. \n\nWe will momentarily add all these discussions and results to our revised version. \n", "title": "Thanks for your comments!"}, "Hkci0r3lM": {"type": "rebuttal", "replyto": "Hkgbu7Qgz", "comment": "1)\nIndeed, u and v are both functions of W, and we technically have to backprop through these vectors as well.  However,  in our implementation, we ignored the dependency of u and v on W for the sake of computational efficiency, and we were still able to maintain the Lipschitz constraint.\nIn fact, to be on the safe side, we ran experiments with backprop on u and v as a separate experiment.  We were not able to observe any notable improvement.  \n\n2)\nTo make the long story short,  sigma(W) and sigma(B) may and may not differ depending on the padding and stride size.  We briefly discuss this matter on the second footnote in page 5.  Let us elaborate on this a little further.  For the sake of argument, let us assume that the input image is infinite dimensional in both directions. If the stride size is 1,  the value on each output pixel will be computed from the outputs of exactly same number (say, m) of filter blocks.  The same holds also when the stride size divides the dimension of the filter block. In such cases, sigma(W) and sigma(B) will be off by the root m, and the dominant vectors will be exactly same. \nWhen the stride size does not divide the dimension of the dimension of the filter block, however, there will be some output pixels that are computed from the outputs of more filter blocks than other. In such cases, the relationship between sigma(W) and sigma(B) appears complex; at least so complex that we decided not to elaborate further on our paper. \nFor our experiment, we made sure that the stride size divides the dimension of the filter block so that, even after taking the padding size into consideration, the dominant direction will not be too much off from what we mathematically intended. \n", "title": "backprop, the spectral norm of the convoluation operation"}, "SyTXZU2xz": {"type": "rebuttal", "replyto": "ryjuZSQlG", "comment": "\n>I don't see where the square comes in.  If you flatten $W$, it should be of shape $d_{out}d_{in}hw$, right?  I am also interested to hear more about the semantics of the spectral norm of this object (flattened filterbank), which Ian asked about below.\n\nYes, it's a typo. We meant to write 2-D, not square.  \nAs for the spectral norm of convolutional operator, please take a look at our response to Ian\u2019s comment. \n\n>Relatedly, I think there is a typo in the caption of Table 6:\n\"we replaced the usual batch normalization layer in the ResBlock of the with the conditional batch normalization layer\"\n\nWe are sorry for the confusion, and you are correct about our typo in the caption of Table 6. We meant to write \n\u201cwe replaced the usual batch normalization layer in the ResBlock of the '''generator''' with the conditional batch normalization layer\". \nWe introduced the conditional batch normalization layer to the generators of ALL the GANs. \n", "title": "Thanks for the comments!"}, "SJmRwz7xG": {"type": "rebuttal", "replyto": "HJdKxkGxf", "comment": "Thanks for the comments and remarks!\nLet me try to resolve the concerns one by one. \n\n>I'm not sure which time step I'm meant to take u and v from when computing the spectral norm. Here I chose to use the *new* value of both u and v, so that get u^T W v for free when I compute the normalizing constant for the new value of u.\n\nI am not sure if I am understanding the question clearly, but at each forward propagation, we prepare new u and v from the same set of u.     \nBy the way, we would like to note that we didn't propagate gradients thorough new_u and new_v .\nIf we write our code in Tensorflow, our implementation is like:\n       new_v = tf.nn.l2_normalize(tf.matmul(self.u, W), 1)\n       new_u = tf.nn.l2_normalize(tf.matmul(new_v, tf.transpose(W)), 1)\n       new_u = tf.stop_gradient(new_u)\n       new_v = tf.stop_gradient(new_v)\n       spectral_norm = tf.reduce_sum(new_u * tf.transpose(tf.matmul(W, tf.transpos e(new_v))), 1)\n       power_method_update =  tf.assign(self.u, new_u)\n\n>For convolution, I *think* I'm meant to use convolution and convolution transpose on a 4-D tensor, based on the comment in the paper about the sparse matrix, but I wasn't totally sure if I should do this or reshape the kernels into a matrix and use matrix-vector products.\n\nIn our implementations, we reshaped the 4D convolutional kernel into a 2-D matrix for the computation of the spectral norm. So, to be honest, our \u201cspectral norm\u201d does not include the parameters like padding and stride size.  We did away with these parameters just for the ease of computation.   So far however, this way is yielding satisfactory results.\n\nYour implementation is mathematically more faithful to our theoretical statement in that it is approximating the honest-to-goodness operator norm of the convolutional operator that includes these parameters.  We cannot say for 100% sure, but your speculate your way of computation shall work just fine.\n\n>I'm not 100% sure when I'm meant to run the `power_method_update` op. Should I just run this once per gradient step or do I need to run it several times to get u close to optimal before I start running SGD?\n\nIn our experiment, we applied the power method update operation only one time per gradient step. It turned out that one power iteration was enough. \nTo check how good we are doing with one application of the power method,  we used SVD to compute the spectral norm of the convolution kernel normalized with our method (AppendixC.1)  Note that our method is doing just fine with  one power method. \n", "title": "Thanks for the comments and remarks!"}}}