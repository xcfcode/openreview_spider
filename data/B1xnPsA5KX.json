{"paper": {"title": "Modular Deep Probabilistic Programming", "authors": ["Zhenwen Dai", "Eric Meissner", "Neil D. Lawrence"], "authorids": ["zhenwend@amazon.com", "erimeiss@amazon.com", "lawrennd@amazon.com"], "summary": "", "abstract": "Modularity is a key feature of deep learning libraries but has not been fully exploited for probabilistic programming. We propose to improve modularity of probabilistic programming language by offering not only plain probabilistic distributions but also sophisticated probabilistic model such as Bayesian non-parametric models as fundamental building blocks. We demonstrate this idea by presenting a modular probabilistic programming language MXFusion, which includes a new type of re-usable building blocks, called probabilistic modules. A probabilistic module consists of a set of random variables with associated probabilistic distributions and dedicated inference methods. Under the framework of variational inference, the pre-specified inference methods of individual probabilistic modules can be transparently used for inference of the whole probabilistic model. We demonstrate the power and convenience of probabilistic modules in MXFusion with various examples of Gaussian process models, which are evaluated with experiments on real data.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper presents a probabilistic programming language where models are constructed out of building blocks which specify both the distribution and an inference procedure. As a demonstration, they show how a GP-LVM can be implemented.\n\nThe paper spends a lot of space arguing for the benefits of modularity. Modularity is of course hard to argue with, and the benefits are already understood in the PPL community. But, as the reviewers point out, various other PPLs have already adopted various strategies to enable modular definition of models, and (in cases like Venture) special-purpose higher-level inference algorithms. This paper contains little discussion of other PPLs and how the specific design decisions relate to theirs, so it's hard to judge whether this paper really covers new ground. Such discussion wasn't added to the revised paper, even though multiple reviewers asked for it. I can't recommend acceptance.\n"}, "review": {"Skxbu8JcAX": {"type": "rebuttal", "replyto": "HJeHr0F0K7", "comment": "Thanks for your comments.\n\n> Details of how a GP fits within the probabilistic programming framework is missing. Should be more discussion of results.\n\nWe will include more details about the model in the examples and the experiments.\n\n> Authors should cite recent works in PP.\n\nThanks for listing the relevant works. We will cite them.\n", "title": "Re: Somewhat important direction of research, but not entirely novel and lacking discussion"}, "HJgAnB19C7": {"type": "rebuttal", "replyto": "rkegcHEfsm", "comment": "Thanks for your suggestions and feedbacks.\n\n> the actual content of the paper describes an extension of an existing system. Such an extension is certainly important, but the paper does not provide much more information.\n\nWe will include more details of the proposed approach.\n\n> the results of the experimental test do not seem to me to be able to support the main objective of the extension\n\nWe will include more examples/experiments to show case our library.\n\n> As far as the execution of the system is concerned, is this extension able to improve the scalability or reduce the walltime? Is this visible in the presented test (at least in terms of speed up)? Or is the convenience of this approach the simpler way to define distributions?\n\nOur library provides a more convenient way to define sophisticated distributions. By taking the advantage of GPU acceleration, which is provided via the underlying MXNet library, the Gaussian process models implemented in our library is faster than the previous implementations in pure Python (in GPy).\n", "title": "Re: A good work on modularisation of probabilistic programming languages"}, "Hyl4gE19AQ": {"type": "rebuttal", "replyto": "HklYg8l93X", "comment": "Thanks for your suggestions and feedbacks.\n\n> when we have a (often complex) combination of several probabilistic modules, how do you then leverage the tailored solvers? What is it that guarantees that these are relevant in the new combined construction?\n\nFor variational inference, the guarantee for a combination of several probabilistic modules comes from the fact that, replacing a part of a prior distribution in a variational lower bound with another variational lower bound results into a further lower bound. Therefore, we can easily put together multiple probabilistic modules for a combined model. A good example will be deep Gaussian processes, e.g. (Damianou&Lawrence 2013), which can be constructed by putting together multiple variational sparse Gaussian processes.\n\n> This is of course true and it is also part of some existing PPLs, for example Birch via their so-called \"delayed sampling\". Why is it better to insist on hard-coding this for each probabilistic module? and how can you guarantee smooth functioning when several probabilistic modules are combined in complex ways?\n\nThanks for pointing out the relevant work. We will cite it. In the case of conjugate models, the hard-coding approach would not be able to automatically choose the best approach. Instead, it relies on users to make the right choice. Although the hard-coding approach is not as smart, it is a more generic approach which can be applied to the cases beyond conjugate models.\n\n> At the same time, is not one of the key reasons for using PPLs compared to probabilistic graphical models that it offers a richer model class compared to probabilistic graphical models?\n\nPPLs can describe models beyond probabilistic graphical models, however, a big portion of real world use cases of PPLs are about Bayesian inference on probabilistic graphical models, e.g., Bayesian statistics with Stan. Our library starts with this restrictive use case and may extend in future.\n\n> Why do you remove this possibility by insisting on a specific point estimate? or is this just a particular choice of this example and not a general design choice?\n\nDoing point estimates in the example is a particular (common) choice of the model in this example. We will include examples with Bayesian inference on hyper-parameters.\n", "title": "Re: The main idea is the introduction of a new building block-probabilistic modules-into probabilistic programming with the aspiration to improve the modularity of the language."}, "Hygi62CKAX": {"type": "rebuttal", "replyto": "BJlhDyz5nX", "comment": "Thanks for the suggestions and feedbacks.\n\n> Why the paradigm of encapsulating inference methods in probabilistic modules is legitimate for constructing complex probabilistic models? \n\nA dedicated inference method for a specific model typically outperforms a generic black-box inference method. With the emphasis on flexibility, PPLs mostly build on generic inference methods such as black-box variational inference, which leads to a performance gap between the model with a dedicated implementation and the model implemented in a PPL. Encapsulating inference methods is an approach that can bridge the performance gap.\n\n\n> What inference methods and probabilistic models can we use as building blocks?\n\nWe focused on variational inference in our library, but other inference methods can also be implemented under a similar idea, e.g., Rainforth (2018) proposes nested probabilistic programming for MCMC methods. For variational inference, probabilistic models that can benefit from an efficient variational lower bound are good candidates for building probabilistic modules.\n\n> Do we need to be aware of specific inference methods that are encapsulated or we can use any blocks in any order as we do in deep learning frameworks?\n\nBy nesting a encapsulated variational lower bound into the external lower bound, the external inference method does not need to be aware of a specific choice of encapsulated inference method. In practice, a user may need to be aware of specific tuning parameters.\n\n> But the application area of MXFusion remains unclear.\n\nWe will include better examples to show cases the difference.\n", "title": "Re: The paper proposes a new probabilistic programming language, but has a lack of scientific novelty"}, "rkegcHEfsm": {"type": "review", "replyto": "B1xnPsA5KX", "review": "The paper presents an extension of the MXFusion language that allows the use of probabilistic modules. These modules are defined as a set of random variables and a specific probabilistic distribution. The modules also contain dedicated inference methods. Using these modules, one can use probabilistic distributions with inference methods tailored to the distribution, which are usually more efficient than generic inference systems.\nThe paper presents several examples using Gaussian process models, evaluated by comparison with GPy and the standard spare gaussian process method implemented in MXFusion.\n\nOverall, the paper is well written and clear, and all claims are justified. The idea of modularization is not really new (as other systems implement something similar) but this approach tries to be general, in order not to pose constraints on the specification of modules. The related work section provides a good positioning of the approach.\nI have not found any specific problems in the paper, the quality is rather high. However, the actual content of the paper describes an extension of an existing system. Such an extension is certainly important, but the paper does not provide much more information.\nMoreover, the results of the experimental test do not seem to me to be able to support the main objective of the extension, which is to give the possibility to exploit more specific probabilistic model and inference methods to achieve better results than an approach using general methods.\nAs far as the execution of the system is concerned, is this extension able to improve the scalability or reduce the walltime? Is this visible in the presented test (at least in terms of speed up)? Or is the convenience of this approach the simpler way to define distributions?\n\nAs for minor issues that I can point out, one concerns the definition of shape in the Variable of m.sigma2 (figures 1, 2, 3). I do not know the used in MXFusion, thus this might not be an error, but it seems that in the shape definition something is missing. It is written that shape=(1,), is it correct or is there an error? In case of absence of error, what does the empty argument mean?\n\nThe power benchmark is not described.\n\nIn references, Thomas V. Wiecki is mentioned with and without the first letter of middle name. I suggest to uniform the references.\n\nTypos:\n- Abstract: \"... but also sophisticated probabilistic model*s* such as ...\"\n- Sec. 1, first row of page 2: The sentence \"this would bring the a lot of benefits ...\". The \"the\" word should be deleted.\n- Sec. 1 refers to a section after 4 which does not exist in the paper.\n- Page 5: remove the full stop before the colon in the 4th row.\n- Page 5: \"The log_pdf method of the SGPR module compute*s* the above variational lower bound\"\n- Sec. 6: the sentence \"MXFusion aims at closing the gap between having specialized, highly performant algorithms and generic, easily maintained generic algorithms by introducing probabilistic modules.\" should be corrected.\n\n\nPros\n- The extension allows the use of modules that define specific probabilistic distribution/inference methods\n- It seems easy to extend the system with other modules\n- Its a really useful extension...\n\nCons\n- The performance presented in the paper is not entirely convincing\n- ... but it is just an extension of an existing system", "title": "A good work on modularisation of probabilistic programming languages", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJlhDyz5nX": {"type": "review", "replyto": "B1xnPsA5KX", "review": "In this paper authors present a new Probabilistic Programming Language (PPL) MXFusion. Similarly to the languages for the deep learning (TensorFlow, PyTorch, etc.), this language introduce probabilistic modules that are used as building blocks for complex probabilistic models. Introducing modularity to the probabilistic programming, raises the problem of inference for probabilistic models. Since, we cannot obtain the exact solution on practice we have to resort to approximate inference methods. The approximate inference methods can be either generic, thus, being suitable for many probabilistic models but resulting in poor approximation, or specific, thus, having good approximation quality, but only for specific probabilistic models. Authors propose to address this problem by encapsulating specific inference methods in corresponding probabilistic modules. Doing so, one can perform approximate inference for every module with the best suitable inference technique. Authors demonstrate interface of MXFusion for three well known probabilistic models: Bayesian linear regression, deep kernel learning, Bayesian Gaussian process latent variable model.\n\nApproaching the problem of building complex probabilistic models by introducing modular PPL is an important direction of study. But, regarding this paper I have the following concerns.\n- In my opinion, the structure of the paper can be greatly improved. From general words about modularity and approximate inference authors dive to the very specific cases of probabilistic models. Following such structure, authors don\u2019t give a clear answer to the following questions. Why the paradigm of encapsulating inference methods in probabilistic modules is legitimate for constructing complex probabilistic models? What inference methods and probabilistic models can we use as building blocks? Do we need to be aware of specific inference methods that are encapsulated or we can use any blocks in any order as we do in deep learning frameworks?\n- Novelty of that paper is the new design of PPL. That is an interesting and important question for the community, but maybe ICLR paper is not the best format to present such kind of novelty. \n- From the specific examples in the paper, legitimacy of such modular structure is clear only for variational inference (that seems to be a common knowledge) and variational approximation of gaussian processes. But the application area of MXFusion remains unclear. Verbatim examples of code for the specific examples doesn\u2019t make the difference between MXFusion and other PPLs clear, because it can be treated as encapsulation of the code into some classes, that can be implemented in other languages as well.\n- Comparison with other frameworks can be improved. In experimental section authors provide comparison with GPy framework in terms of RMSE and log-likelihood for gaussian process with 50 inducing points. As I understood both frameworks use the same inference methods and achieve the same performance, so the experiment can be considered as sanity check for MXFusion. The paper could benefit from comparison between different inference methods and providing benchmarks for inference time.\n\nOverall, the paper proposes a new PPL that is an important direction of study, but have several drawbacks and conference paper format is not the best way to present such kind of novelty.\n\nTypos:\n- Page 1, \u201cdespite the different of DNNs\u2026\u201d -> \u201cdespite the difference of DNNs\u2026\u201d?\n- Page 2, missing reference of the section\n- Page 2, section 3, \u201c... sightly different form.\u201d -> \u201c... slightly different form\u201d?", "title": "The paper proposes a new probabilistic programming language, but has a lack of scientific novelty", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HklYg8l93X": {"type": "review", "replyto": "B1xnPsA5KX", "review": "The paper works with the modularization of PPLs with natural inspiration for the successful modularization recently introduced in all deep learning softwares. \n\n* Within your so-called probabilistic modules you package dedicated inference methods that are tailored for this particular class of problems and argues that this will perform better than using a general purpose solver. For each specific case this does of course make a lot of sense. However, when it comes to the relevant case (especially within probabilistic programming) when we have a (often complex) combination of several probabilistic modules, how do you then leverage the tailored solvers? What is it that guarantees that these are relevant in the new combined construction? \n\n* Related to the above you write in your conclusion that \"Once an inference algorithm is chosen, it remains the same across a probabilistic model. However, given a specific probabilistic model, e.g., a conjugate model, a specialized inference algorithm that exploits the mathematical properties of that particular model will always produce inference results that are as good or better than the generic inference in terms of both accuracy and efficiency.\" This is of course true and it is also part of some existing PPLs, for example Birch via their so-called \"delayed sampling\": \nhttp://proceedings.mlr.press/v84/murray18a/murray18a.pdf\nThe implementation there is very different from what you propose. As far as I can understand you require hard-coding of each specific model, whereas in the paper mentioned above they seem to automate att conjugate gradient calculations to a much greater extent. Why is it better to insist on hard-coding this for each probabilistic module? and how can you guarantee smooth functioning when several probabilistic modules are combined in complex ways?\n\n* In the inference method that you briefly sketch in Section 3 you make use of VI and the intractable integrals that results are then handled using Monte Carlo. What is the gain of using VI + Monte Carlo compared to direct use of Monte Carlo? Via direct use of some kind of Monte Carlo method you would be able to guarantee performance and do proper analysis, whereas with VI you loose that capability. However, VI does of course have other pros, but my question arises due to the fact that you end up using Monte Carlo anyway.\n\n* You write that \"In PPLs, a probabilistic model is often presented as a graph of random variables...\". This is certainly true and the word \"often\" is very important in this sentence. At the same time, is not one of the key reasons for using PPLs compared to probabilistic graphical models that it offers a richer model class compared to probabilistic graphical models? While I perfectly respect you choice to specifying models in MXFusion using using probabilistic graphical models I do find this quite restrictive and it seems to miss some of the key possibilities with PPLs.\n\n* In your BLR example (which is very instructive by the way) you compute the solution via MAP. This is also find rather puzzling since that removes another great feature of PPLs, namely to work with probability distributions throughout the entire inference stage. The user can then of course choose to extract whatever point estimate might be needed in the end. Why do you remove this possibility by insisting on a specific point estimate? or is this just a particular choice of this example and not a general design choice?\n\n\nThe paper contains a lot of issues related to the use of the English language and would benefit from proper proofreading.\n", "title": "The main idea is the introduction of a new building block-probabilistic modules-into probabilistic programming with the aspiration to improve the modularity of the language.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}