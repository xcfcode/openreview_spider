{"paper": {"title": "Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning", "authors": ["Arsenii Ashukha", "Alexander Lyzhov", "Dmitry Molchanov", "Dmitry Vetrov"], "authorids": ["ars.ashuha@gmail.com", "alex.grig.lyzhov@gmail.com", "dmolch111@gmail.com", "vetrovd@yandex.ru"], "summary": "We highlight the problems with common metrics of in-domain uncertainty and perform a broad study of modern ensembling techniques.", "abstract": "Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty estimation is one of the main benchmarks for assessment of ensembling performance. At the same time, deep learning ensembles have provided state-of-the-art results in uncertainty estimation. In this work, we focus on in-domain uncertainty for image classification. We explore the standards for its quantification and point out pitfalls of existing metrics. Avoiding these pitfalls, we perform a broad study of different ensembling techniques. To provide more insight in this study, we introduce the deep ensemble equivalent score (DEE) and show that many sophisticated ensembling techniques are equivalent to an ensemble of only few independently trained networks in terms of test performance.", "keywords": ["uncertainty", "in-domain uncertainty", "deep ensembles", "ensemble learning", "deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper points out pitfalls of existing metrics for in-domain uncertainty quantification, and also studies different strategies for ensembling techniques.\n\nThe authors also satisfactorily addressed the reviewers' questions during the rebuttal phase. In the end, all the reviewers agreed that this is a valuable contribution and paper deserves to be accepted. \n\nNice work!"}, "review": {"AycpGQEQrjz": {"type": "rebuttal", "replyto": "BJxI5gHKDr", "comment": "Additional links: \n\n- Blog: https://senya-ashukha.github.io/pitfalls-uncertainty&ensembling\n    ", "title": "Additional links"}, "rJg0iIgAFH": {"type": "review", "replyto": "BJxI5gHKDr", "review": "The authors response on 13th nov regarding the main concerns I have are valid. They make sense. I thank the authors for the detailed explanations. I went back and did another thorough read of the work. As it stands, I am OK to change my review to weak accept. \n\n---------------------------------\n\nThe authors evaluate a variety of ensemble models in terms of their ability to capture in-domain uncertainity. A set of metrics are used to perform these evaluations and in turn, using deep ensenble as a reference, the authors study the behaviour/capacity of the rest of the methods. \n\nAlthough the motivation for the work is sensible, there are several critical issues with the paper and the summary is not necessarily conclusive in terms of gaining any new insights. \n1. Much of the evaluations rely on the choice/nature of the optimal temperature, which would be different for different models. The authors suggest to use the model-specific optimal values when comparing instead of fixing the temperature? Why is this the case? Further, if we take this into account (i.e., allow for comparing different temperatures) then much of the differences between DEE and others cannot be directly interpreted. This is the case when using log-likelihood and Brier scores. \n2. AUC can be transformed into a normalized probability distribution (CDF), and hence in principle it is model/hyperparameters agnostic. This is one of the reasons i is used as information criterion in Bayesian model selection. Area of AUC is a valid metric as well. To that end, why do the authors suggest that it cannot be used as criteria for comparison across models? \n3. From section 3.5 it is not clear how test time cross validation is tackling temperature scaling? \n4. In section 4.1, the hypothesis on #independent trained networks is great and it makes sense? How is this translating ito the evaluations? None of the results actually talk about this aspect directly? Or am I missing something here?\n5. Setting the evaluations with DEE as reference is problematic because we already know from random sampling theory that deep ensemble is better than the normal ensembles (tech results on random sampling for model fitting and RL etc. optimization results on mode finding with single mode vs. multi model methods also say similar things) and in fact that was the main motivation. Also normal regularization (like dropout or K-facL are more towards overfitting than ensembling) are not really an ensemble. Putting these together, most of the conclusions and the lots (figure 3 in particular) is by definition true. Nothing surprising. \n\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "BkxCKUsFsH": {"type": "rebuttal", "replyto": "ryxCquU9OH", "comment": "Thank you. This work appears relevant and we will cite it in the next revision of our paper.", "title": "Response to \"A related work\""}, "Skli45uFsB": {"type": "rebuttal", "replyto": "rkgh0fP3FS", "comment": "> 16.\tThe issues of uncalibrated log-likelihood and TACE are clearly shown in the paper, whereas the issues with misclassification detection are only verbally discussed. An illustrative example, at least a toy thought example, could really improve the paper here\n\nAUROC/AUPR for misclassification detection plainly provides numbers that can not be compared across different models. We will try to come up with a convincing illustrative example, but it is not yet clear for us how to make it more convincing than the verbal discussion.\n\n> 17.\tThe chosen main performance metric is not very convincingly motivated. It is clear why it is based on calibrated log-likelihood, but it is not very convincing why one cannot just used calibrated log-likelihood as a performance metric, why one should base the metric on deep ensembles instead. Also from the long-term perspective, if the community comes up with methods clearly outperforming deep ensembles, the metric would need to be based on one of these new methods \n\nDEE is basically a more convenient way to visualize the calibrated log-likelihood. The calibrated log-likelihood does indeed seem to be a great absolute measure of performance. However, it is not very convenient if one wants to compare the performance of different ensembling techniques. Different models and datasets have different base values of calibrated log-likelihood, and its dependence on the number of samples is non-trivial. DEE is model- and dataset-agnostic and provides some useful insights that can be difficult to visualize using the calibrated log-likelihood alone.\n\nIn the long run, the DEE metric might still be useful and insightful. While the DEE curve of deep ensembles is an identity function, superior methods would typically result in a higher DEE curve. We would only run into problems if this new superior method would outperform extremely large deep ensembles (say hundreds or thousands of samples) using just a handful of samples. However, we do not expect such a drastic gap to appear soon.\n\n> 18.\tThere is an indirect uncertainty metric that is not mentioned in the paper \u2013 uncertainty used in active learning (see, e.g., Hern\u00e1ndez-Lobato and Adams, 2015. Probabilistic backpropagation for scalable learning of Bayesian neural networks)\n\nWe do mention active learning in the related works section, but this reference is indeed very relevant here. Thank you.\n\n> 20.\t\u201cthe original PyTorch implementations of SWA\u201d \u2013 SWA is not considered in the paper\n\nWhile we do not use SWA in our experiments, our codebase is heavily based on the original implementation of SWA since it allowed to easily reproduce the training of different models and was easy to modify for our needs. We will articulate the reference more clearly in the next revision of the paper.", "title": "Response to Review #2 (part 2/2) "}, "BJebZqOFjS": {"type": "rebuttal", "replyto": "rkgh0fP3FS", "comment": "We would like to sincerely thank you for your thoughtful and thorough remarks. They will allow us to significantly improve the quality of our paper in its next revision.\n\nWe address the major questions below:\n\n> 7.\t\u201cIn that case, both objects and targets of induced binary classification problems remain fixed for all models\u201d \u2013 do the authors consider in this case all out-of-domain objects as having a positive class and all in-domain objects as having a negative class? Because the models are still going to make individual misclassification mistakes\n\nYes, in the case of out-of-domain detection the out-of-domain objects have the positive class and the in-domain objects have the negative class. Because of that, both the objects and the labels of the auxiliary binary classification problems of out-of-domain detection remain the same across different models.\n\n> 9.\tIn eq. (4) and (5) subscript DE is not defined\n\nWe define CLL_m where m stands for the name of the ensembling technique. DE stands for deep ensemble. We will clarify this in the future revision of the paper.\n\n> 11.\t\u2026 Similar to cSGLD, is seems that SWAG was not applied on ImageNet. Why is that if that is the case? And it should be clearly stated at least in experimental setup in Supplementary. \u2026\n> 23.\t\u2026 Why was dropout applied only for limited number of architectures and not applied on ImageNet at all? ...\n\nDue to limited computational resources we have prioritised SSE over cSGLD and FGE over SWAG since they are closely related to each other and achieve similar performance on CIFAR10/100 datasets. Since these techniques are quite similar to each other, we expect these results to translate well to ImageNet. Also, we have only applied dropout to the VGG model and the WideResNet model since plain ResNets are conventionally trained without dropout, and naive application of dropout hurts the final predictive performance.\n\n> 13.\tMissing details of what kind of augmentation is used in Section 4.3. Is it the same as training augmentation specified in Supplementary? It would require a reference to Supplementary\n\nFor test-time augmentation we use the same data augmentation as used during training. We will add the reference to Supplementary there.\n\n> 15.\t\u201cOur experiments demonstrate that ensembles may be severely miscalibrated by default while still providing superior predictive performance after calibration.\u201d \u2013 unclear which experiments demonstrate this and superior in comparison to what\n\nThe most drastic difference can be observed in Figure 1. Deep ensembles + augmentation (DE+aug) on ImageNet has exteremely poor calibration and perform worse than plain DE, whereas calibrated DE+aug outperforms plain DE (and other techniques as well).", "title": "Response to Review #2 (part 1/2)"}, "SJlB-P_tjB": {"type": "rebuttal", "replyto": "rJg0iIgAFH", "comment": "> 4. In section 4.1, the hypothesis on #independent trained networks is great and it makes sense? How is this translating ito the evaluations? None of the results actually talk about this aspect directly? Or am I missing something here?\n\nThis point presumably refers to the following sentence in section 4.1: \u201cWhat number of independently trained networks yields the same performance as a particular ensembling method?\u201d This is not intended to be a presentation of hypothesis. This question only sets the stage for the introduction of the deep ensemble equivalent (DEE) metric which directly answers the question when evaluated. DEE plays a major role in our benchmark and we have quantitative results concerning it (e.g. Figure 3 in the main text). We discuss the results related to DEE in Section 4.2 and Section 5 and provide more detailed results in Appendix E.\n\nOn the other hand, if the comment refers to the following sentence \"Deep ensembles, ..., which can intuitively result in a better ensemble.\", it was just a motivation to consider deep ensembles as a potentially strong baseline.\n\n> 5. Setting the evaluations with DEE as reference is problematic because we already know from random sampling theory that deep ensemble is better than the normal ensembles (tech results on random sampling for model fitting and RL etc. optimization results on mode finding with single mode vs. multi model methods also say similar things) and in fact that was the main motivation. Also normal regularization (like dropout or K-facL are more towards overfitting than ensembling) are not really an ensemble. Putting these together, most of the conclusions and the lots (figure 3 in particular) is by definition true. Nothing surprising.\n\nDid you mean DE (deep ensembles) instead of the DEE score here? The superior performance of deep ensembles is indeed not surprising. Highlighting this fact is not the main purpose of this paper. Instead, our study is largely aimed at comparing ensembling methods in a fair and interpretable way to gain insights in the fields of ensembling and uncertainty estimation.\n\nMethods that are based on stochastic computation graphs, e.g., MC-dropout, K-FAC Laplace and variational inference, are commonly regarded as ensembling techniques in the bayesian deep learning literature and are frequently used as a baseline in ensembling-based uncertainty estimation research (Gal and Ghahramani 2016, Lakshminarayanan et al 2017, Louizos and Welling 2017, Maddox et al 2019). Deep ensembles, in contrast to dropout, are rarely considered as a baseline in the bayesian deep learning literature, but we believe that overcoming the DE baseline is a strong challenge for the community.\n\nWe would highly appreciate it if you could provide links for the papers mentioned in your review since they seem to be relevant to our study.\n\nLouizos C, Welling M. Multiplicative normalizing flows for variational bayesian neural networks, ICML 2017. \nLakshminarayanan B, Pritzel A, Blundell C. Simple and scalable predictive uncertainty estimation using deep ensembles, NeurIPS 2017.\nMaddox W, Garipov T, Izmailov P, Vetrov D, Wilson AG. A simple baseline for bayesian uncertainty in deep learning, NeurIPS 2019.\nGal Y, Ghahramani Z. Dropout as a bayesian approximation: Representing model uncertainty in deep learning, ICML 2016.", "title": "Response to Review #1 (part 2/2)"}, "rkglRIOFsS": {"type": "rebuttal", "replyto": "rJg0iIgAFH", "comment": "We would like to sincerely thank you for your thoughtful remarks and questions.\n\nWe address your concerns below:\n\n> 1. Much of the evaluations rely on the choice/nature of the optimal temperature, which would be different for different models. The authors suggest to use the model-specific optimal values when comparing instead of fixing the temperature? Why is this the case? Further, if we take this into account (i.e., allow for comparing different temperatures) then much of the differences between DEE and others cannot be directly interpreted. This is the case when using log-likelihood and Brier scores. \n\nMost of the modern deep learning techniques are overconfident in their predictions, i.e. the effective temperature is lower than optimal. Moreover, it does not seem possible for now to determine the optimal temperature relying only on the training data. Validation-based temperature scaling is a simple yet powerful calibration technique that allows to improve many predictive performance metrics, e.g. the test log-likelihood, post-hoc for all methods and models. We would like to reduce the influence of the non-optimal temperature on the predictive performance, so we see the temperature scaling as an essential step in training the model. As we show in Figure 1, comparing different techniques without temperature scaling can yield misleading results. E.g., deep ensembles with test-time data augmentation (DE+augment) seem to perform worse than deep ensembles without data augmentation (DE) in terms of the log-likelihood, whereas after temperature scaling DE+augment outperforms plain DE.\n\nComparing methods at different temperatures is fair since the procedure for temperature scaling is the same for all methods, as described in Section 3.5. Moreover, we stress that this setting is more reasonable compared to using the same temperature for all methods in the benchmark since different methods have different optimal temperatures on hold-out data. \n\n> 2. AUC can be transformed into a normalized probability distribution (CDF), and hence in principle it is model/hyperparameters agnostic. This is one of the reasons i is used as information criterion in Bayesian model selection. Area of AUC is a valid metric as well. To that end, why do the authors suggest that it cannot be used as criteria for comparison across models? \n\nMetrics like AUROC / AUPR cannot be used for a particular problem of misclassification detection. Let us summarize the argument. We aim to compare different models---trained on the same data---in terms of ability to distinguish between correct and wrong classifications. The prior literature suggests the following: i) every prediction of a particular model receives a confidence score, ii) the score then is treated as an output of a binary classifier that detects misclassifications. AUROC / AUPR of these binary classifiers is used for comparison between different models.\n\nSuch a comparison, however, is not correct. Every model has its own correct and wrong predictions, and thus poses its own misclassification detection problem (binary classification of correctly classified vs. incorrectly classified examples). Particularly, in the case of comparison between K models, we have K different misclassification detection datasets comprised of pairs (original object, \u201ccorrectly classified\u201d / \u201cincorrectly classified\u201d binary label) with a different labeling for each model. The described comparison procedure essentially corresponds to a comparison of performance of classifiers that solve different classification problems. Such metrics are incomparable.\n\n> 3. From section 3.5 it is not clear how test time cross validation is tackling temperature scaling?\n\nThe \u201ctest-time cross-validation\u201d method for evaluating metrics at an optimal temperature is organized as follows: \n1. A test set is randomly shuffled and divided into K folds of the same size\n2. A temperature T* is adjusted by K-1 folds, T* = argmax_T LL(Model(T), Data(K-1 folds))\n3. The model at the optimal temperature T* is used to evaluate metrics on the Kth fold.\n4. The steps 1-3 are repeated several times, the metric values are averaged.\nIn our experiments we use K=2.\n\nIn the step 2 we solve a 1D optimization problem that optimizes LL on K-1 folds w.r.t. a scalar temperature T. The result of step 2 may differ depending on the particular data split. Strictly speaking, the described algorithm estimates expectation of test metrics w.r.t. the distribution of optimal temperatures induced by the random data splits. In practice, we noticed that the optimal temperatures did not differ much on different splits.", "title": "Response to Review #1 (part 1/2)"}, "HkgUlEdFsB": {"type": "rebuttal", "replyto": "S1lvoIaRtH", "comment": "We would like to sincerely thank you for your thoughtful remarks and questions.\n\nWe address your concerns below:\n\n> On page 6, the authors mention that the prior in Eq. 3 is taken to be a Gaussian N(\\mu, diag(\\sigma^2)) for Bayesian neural networks, however, many other choices of a prior distribution are available in the literature. What is the impact of changing prior distributions on the quality of uncertainty estimates in the case of variational inference? \n\nThe prior distribution is a part of the underlying probabilistic model, whereas most ensembling techniques can be considered as approximate inference techniques under such a model. Aiming for a fair comparison, we set the probabilistic model (and, therefore, the prior distribution) to be the same across all ensembling techniques. We use the Gaussian prior, induced by the optimizer favored in recent literature since it is simple and provides reasonably high performance. However, it would indeed be interesting to see how the choice of the prior influences different ensembling techniques.\n\n> Data augmentation is commonly used for improving model performance. However, I find the results presented in Sect 4.3 are not clear enough, note that for a given ensembling method in Table 1, the negative calibrated log-likelihood may increase or decrease when using different networks (VGG, ResNet, etc.). I think it would be interesting to elaborate a bit more on the influence of model complexity.\n\nThe effect appears due to std of the negative calibrated log-likelihood (nCLL). In all the cases where nCLL may increases or decreases within one method the difference has the order of ~1e-3 or less and lies within std interval. We will correct the tables and add stds.\n\nOn CIFAR datasets test-time data augmentation helps \u201cweak\u201d ensembling methods like dropout, K-FAC Laplace and variational inference, whereas on ImageNet we observe the improvement for all techniques. We hypothesize that this is caused by a more diverse data augmentation on ImageNet as compared to CIFAR. We will move the ImageNet results (Table 9) into the main section of the paper as they seem to be more representative than CIFAR results. It would be interesting to see whether the use of more diverse data augmentation (e.g. rotations, color transformation, etc.) improves stronger ensembles as well.\n\n> On page 15, in Eq. 12, the choice of the variance parameter \\sigma_p^2=1/(N*wd) seems unclear and should be better explained.\n\nIt is the same prior as defined in eq. 9 on page 14. The weight decay parameter wd is the coefficient before the L2 regularizer in the objective. In most deep learning frameworks, one computes the *average* loss in the minibatch instead of the *sum* across all objects. Therefore, one needs to rescale this coefficient by the size of the training set to obtain the underlying prior distribution. We will update the paper with a more clear explanation.", "title": "Response to Review #3"}, "rkgh0fP3FS": {"type": "review", "replyto": "BJxI5gHKDr", "review": "The paper provide an extensive review of current advances in uncertainty estimation in neural networks with the analysis of drawbacks of currently used uncertainty metrics and comparison on scale the recent method to estimate uncertainty. The paper covers a lot of uncertainty metrics and a wide range of methods. The paper focuses on in-domain uncertainty estimation complementing the recent similar review on out-of-domain uncertainty estimation.\n\nIt seems that the paper provides the analysis missing in the current literature. Whereas as mentioned Yukun Ding in a public comment there is a related work on identifying issues with popular uncertainty metrics, the mentioned paper is missing the through comparison of the methods for estimating uncertainty. \n\nSuch kind of thorough analysis (especially performed on scale on large datasets) and comparison is of obvious interest to the community as well as objective comparison of the current state-of-the-art. \n\nThe paper is clearly written and easy to follow.\n\nBased on this, I believe this is a strong technical paper and it should be accepted. However, the analysis in the paper is not overwhelmingly exhaustive. Some of the arguments on that are listed below.\n\nBelow is the list of comments/thoughts for potential improvement of the paper:\n1.\t\u201cIn this case, a model is expected to provide correct probability estimates:\u201d \u2013 may be not the best choice of words, because for out of domain uncertainty estimation we still expect a model to provide correct probability estimates\n2.\tThe first paragraph on page 3 seems to better fit in Section 2, for example, on the very beginning of Section 2.\n3.\t\u201cComparison of the log-likelihood should only be performed at the optimal temperature.\u201d and others alike \u2013 personally, I do not support this kind of formatting for a scientific paper\n4.\t\u201ccan produce an arbitrary ranking of different methods. <\u2026> Empirically,\u201d \u2013 in the current form it seems that the first statement is somehow theoretically justified and then additionally it is confirmed empirically in this paper. I believe that the authors use empirical observation itself as the justification of the first statement, if that the case it should be reworded here. For example, \u201ccan produce an arbitrary ranking of different methods as we show below/ as we show empirically. We demonstrate that the overall \u2026\u201d If my belief is incorrect and there are other grounds that justify the first statement that it is required a reference after this statement.\n5.\tItalic and non-italic LL usage is unclear\n6.\tHaving \u201cBrier score\u201d emphasised as a paragraph, it seems that there should be a paragraph log-likelihood as well\n7.\t\u201cIn that case, both objects and targets of induced binary classification problems remain fixed for all models\u201d \u2013 do the authors consider in this case all out-of-domain objects as having a positive class and all in-domain objects as having a negative class? Because the models are still going to make individual misclassification mistakes\n8.\tFigure 2 \u2013 legend occupies too much space of the plot occluding almost a third part of the plot. Maybe taking the legend out of the plot to the right and squeezing the plot to make a room for the legend would be a better solution\n9.\tIn eq. (4) and (5) subscript DE is not defined\n10.\t\u201cSSE and cSGLD outperform all other techniques except deep ensembles\u201d \u2013cSGLD was not applied on ImageNet, therefore this statement is a bit misleading\n11.\tColour of SWAG in Figure 3 is not very clear. Only excluding other colours I can determine which line is SWAG. Similar to cSGLD, is seems that SWAG was not applied on ImageNet. Why is that if that is the case? And it should be clearly stated at least in experimental setup in Supplementary  \nFor colours in general, lines in legends are very thin and it is difficult to assess their colour. I appreciate the authors compare a lot of methods and therefore have to use a lot of colours, but it is quite difficult to assess them even on screen not to mention if the paper is printed out. Could the authors please use thicker lines in legends at least?\n12.\t\u201cBeing more \u201clocal\u201d methods\u201d \u2013 without any context in the main paper this referral to \u201clocal\u201d methods is unclear. Also it is good to add a reference to Appendix review of the considered methods in the main text.\n13.\tMissing details of what kind of augmentation is used in Section 4.3. Is it the same as training augmentation specified in Supplementary? It would require a reference to Supplementary\n14.\t\u201c(Figure 1, Table REF)\u201d \u2013 missing number for Table\n15.\t\u201cOur experiments demonstrate that ensembles may be severely miscalibrated by default while still providing superior predictive performance after calibration.\u201d \u2013 unclear which experiments demonstrate this and superior in comparison to what\n16.\tThe issues of uncalibrated log-likelihood and TACE are clearly shown in the paper, whereas the issues with misclassification detection are only verbally discussed. An illustrative example, at least a toy thought example, could really improve the paper here\n17.\tThe chosen main performance metric is not very convincingly motivated. It is clear why it is based on calibrated log-likelihood, but it is not very convincing why one cannot just used calibrated log-likelihood as a performance metric, why one should base the metric on deep ensembles instead. Also from the long-term perspective, if the community comes up with methods clearly outperforming deep ensembles, the metric would need to be based on one of these new methods \n18.\tThere is an indirect uncertainty metric that is not mentioned in the paper \u2013 uncertainty used in active learning (see, e.g., Hern\u00e1ndez-Lobato and Adams, 2015. Probabilistic backpropagation for scalable learning of Bayesian neural networks)\n19.\tFigures 4 and 5 are too small\n20.\t\u201cthe original PyTorch implementations of SWA\u201d \u2013 SWA is not considered in the paper\n21.\t\u201chidden inside an optimizer \u2026 The actual underlying optimization problem\u201d \u2013 it seems that the ICLR audience should be familiar with \u201cactual optimization problems\u201d rather than using blindly the optimizer. It is always good to explicitly write down an equation that is used in a paper, but this wording seems a bit off for ICLR \n22.\t\u201c\\hat{p}(y^\u2217_i = j | x_i, w) denotes the probability that a neural network with parameters w assigns to class j when evaluated on object x_i\u201d \u2013 it should be \\hat{p}(y_i = j | x_i, w), y^*_i is observed\n23.\t\na.\tWhy was dropout applied only for limited number of architectures and not applied on ImageNet at all?\nb.\tWhy wasn\u2019t cSGLD applied on ImageNet\n24.\t\u201cOn CIFAR-10/100 parameters from the original paper are reused\u201d \u2013 it is better to repeat the reference here\n\n\nMinor:\n1.\tFont size in eq. (10) should be the same as the rest of the paper\n2.\t\u201cOr models achived top-1 error of\u201d: \u201cOr\u201d - ?, \u201cachived\u201d -> achieved \n3.\t\u201cfor a 45 epoch form a per-trained model\u201d: \u201cform\u201d -> \u201cfrom\u201d?", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 3}, "S1lvoIaRtH": {"type": "review", "replyto": "BJxI5gHKDr", "review": "Summary:\nThis paper mainly concerns the quality of in-domain uncertainty for image classification. After exploring common standards for uncertainty quantification, the authors point out pitfalls of existing metrics by investigating different ensembling techniques and introduce a novel metric called deep ensemble equivalent (DEE) that essentially measures the number of independent models in an ensemble of DNNs. Based on the DEE score, a detailed evaluation of modern DNN ensembles is performed on CIFAR-10/100 and ImageNet datasets.\n\nStrengths:\nThe paper is well written and easy to follow. The relationship to previous works is also well described. Overall, I think this is a good paper, which gives a detailed overview of existing metrics for accessing the quality in in-domain uncertainty estimation. The idea behind the proposed DEE score is nice and simple, clearly showing the quality of different ensembling methods (in Fig. 3). Given the importance of uncertainty analysis to deep learning, I believe this work will have a positive impact on the community.\n\nWeaknesses:\n- On page 6, the authors mention that the prior in Eq. 3 is taken to be a Gaussian N(\\mu, diag(\\sigma^2)) for Bayesian neural networks, however, many other choices of a prior distribution are available in the literature. What is the impact of changing prior distributions on the quality of uncertainty estimates in the case of variational inference? \n- Data augmentation is commonly used for improving model performance. However, I find the results presented in Sect 4.3 are not clear enough, note that for a given ensembling method in Table 1, the negative calibrated log-likelihood may increase or decrease when using different networks (VGG, ResNet, etc.). I think it would be interesting to elaborate a bit more on the influence of model complexity.\n- On page 15, in Eq. 12, the choice of the variance parameter \\sigma_p^2=1/(N*wd) seems unclear and should be better explained.\n\nMinor comments:\nThe size of some figures appears too small, for example Fig. 4 and Fig. 5, which may hinder readability.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}}}