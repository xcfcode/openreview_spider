{"paper": {"title": "Why do deep convolutional networks generalize so poorly to small image transformations?", "authors": ["Aharon Azulay", "Yair Weiss"], "authorids": ["aharon.azulay@mail.huji.ac.il", "yweiss@cs.huji.ac.il"], "summary": "Modern deep CNNs are not invariant to translations, scalings and other realistic image transformations, and this lack of invariance is related to the subsampling operation and the biases contained in image datasets.", "abstract": "Deep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations. In this paper we show that modern CNNs (VGG16, ResNet50, and InceptionResNetV2) can drastically change their output when an image is translated in the image plane by a few pixels, and that this failure of generalization also happens with other realistic small image transformations. Furthermore,  we see these failures to generalize more frequently in more modern networks. We show that these failures are related to the fact that the architecture of modern CNNs ignores the classical sampling theorem so that generalization is not guaranteed. We also show that biases in the statistics of commonly used image datasets makes it unlikely that CNNs will learn to be invariant to these transformations. Taken together our results suggest that the performance of CNNs in object recognition falls far short of the generalization capabilities of humans.", "keywords": ["Convolutional neural networks", "The sampling theorem", "Sensitivity to small image transformations", "Dataset bias", "Shiftability"]}, "meta": {"decision": "Reject", "comment": "This paper attempts to answer its suggestive title by arguing that this generic lack of invariance in large CNN architectures is due to aliasing introduced during the downsampling stages. \nThis paper received mixed reviews. Positive aspects include the clarity and exhaustive empirical setups, whereas negative aspects focused on the lack of substance behind some of the claims. Ultimately, the AC took these considerations into account and made his/her own assessment, summarized here.\n\nThe main claim of this paper implies the following: modern CNNs are unable to build invariance to small shifts, but somehow are able to learn far more complex invariances involving lighting, pose, texture, etc. This must be empirically verified beyond reasonable doubt, and the AC thinks that the current experimental setup does not achieve this threshold. As mentioned by reviewers and by public comments, the preprocessing pipeline is a key factor that may be confounding the analysis, and this should be better analysed. For example, as mentioned in the reviews below, the shift in the image can be either done by inpainting, cropping, or using a fixed background. The authors claim that there are no qualitative differences between those preprocessing choices, but by inspecting Figures 2B and 8C, the AC notices a severe change in 'jaggedness'; in other words, the choice of preprocessing *does* affect the quantitative measures of (un)stability, even though the qualitative assessment (unstable in all setups) is the same. In particular, using non-centered crops should be the default setup, since it requires no preprocessing. It is confusing that it appears in the appendix instead of the inpainting version of figure 2b. This is important, since it implies that the analysis is mixing two perturbations: the actual action of the translation group and the choice of preprocessing, and that the latter is by no means negligible. I would suggest the authors to perform the following experiment to disentangle the effect of translation by the effect of preprocessing. Since the translation forms a group, for any shift applied to the image, one can 'undo' it by applying the inverse shift. Say one applies a shift to image x of d pixels and obtains x'=T(x,+d) as a result (by using whatever border handling procedure). If border effects were negligible, then x''=T(x',-d) should give us back x, so a good measure of how unstable the network is is to measure the difference in prediction between x,x' and x''. If predicting x'' is as unstable as predicting x', it follows that the network is actually unstable to the border effect introduced by T. \n\nGiven this, the AC recommends rejection at this time, and encourages the authors to resubmit their work by addressing the above point. "}, "review": {"rZZPLoACJc": {"type": "rebuttal", "replyto": "HJxYwiC5tm", "comment": "A new version can be found in the following link:\n\nhttp://jmlr.org/papers/v20/19-519.html", "title": "A new version of the paper"}, "Sye2D_EhaX": {"type": "rebuttal", "replyto": "B1lUFfAjpQ", "comment": "Thank you for your comment. Indeed we had a correspondence about this issue following a posting of a previous version of our paper on ArXiv. \n\nThe reviewer is referring to one particular result shown at the bottom of figure 1. Contrary to the title of the reviewer\u2019s comment, he/she acknowledged in our correspondence that given the input frames shown in the bottom of figure 1, the output of the network that we show is reproducible and it varies drastically even between almost identical frames. \n\nThe issue that the reviewer is addressing is how to go from the YouTube video to the network input since the video has an aspect ratio very different from one. Indeed for this specific video, this \u201cprocessing pipeline\u201d makes a difference. However, and as we already mentioned in the correspondence, we easily found other YouTube videos of polar bears that give jagged predictions with the reviewer\u2019s suggested pipeline.\n\nAs a result of the same correspondence, we added a footnote at the bottom of page 3 that explicitly says that different preprocessing pipelines give different results. \nImportantly, following the reviewer's comments, we verified that all the other results do not depend on the pipeline and all the graphs shown in the paper are with a pipeline that maintains the aspect ratio of the original image.\n\nThat being said, the question of \"the right pipeline\" is far from obvious. Using a center crop as suggested by the reviewer means that you will necessarily miss polar bears that do not appear in the center (an extreme case of taking advantage of photographer's bias). In fact, the default preprocessing suggested by Keras (https://keras.io/applications/)  prefers to reshape the full image (as we did in the bottom of figure 1) rather than to perform a center crop.\n\nFinally, we want to stress again that the three frames shown in the bottom of figure 1 are exactly the frames given to the network and given these frames anyone can reproduce our reported network output. The difference between these frames is a very small deformation that is imperceptible to humans and yet makes a huge difference to the output of the network. The goal of our paper is to explain why this happens.\n", "title": "These problems have already been addressed in the text"}, "H1epCO9zAX": {"type": "rebuttal", "replyto": "BJeBZeoN27", "comment": "Thank you for the helpful comments.\n\n\"Theoretical arguments\". We will clarify the proof and notation in the next version. Regarding piecewise constant transformations: many interesting transformations (e.g. small rotations and scalings) can be approximated by a piecewise constant transformation. Figure 4 in the paper shows that scalings of the image behave as one would expect according to this extension of our analysis.\n\n\u201cThe theoretical argument that translation invariance is not guaranteed because of the stride (subsampling) is not fully convincing and needs further explanation and experimental verification\u201d\n\nWe prove in the paper (observation at bottom of page 4) that networks with no stride will be translation invariant. The proof ignores edge effects but we have also verified empirically that networks with the same architectures that are used in the modern CNNs but with no stride are invariant to the range of  translations that we consider in the paper. We will mention this empirical verification in the next version. \n\n\"Empirical evidence:\". As mentioned in our comments to R1, we now have quantitative results (jaggedness and recognition performance) for VGG16 and ResNet50 trained with larger pooling regions and we will include this in the final version. Also, as mentioned in our response to R1, we tried three different types of transformations: inpainting (figure 2), cropping (figure 8) and black border (not shown). All three gave the same results. \nRegarding the photographer's bias, we believe the presence of this strong bias allows the network to do well in the training set and test set without learning to be invariant to all single pixel shifts (without the bias, the number of shifts to memorize would be prohibitive). We will clarify this in the final version. \nWe very much agree that \"feature maps of the CNNs that the authors consider do indeed contain many high frequencies.\u201d and our paper argues that this aliasing is the reason for the lack of invariance: we show analytically that if the feature maps did not contain high frequencies then global pooling would be invariant, and we show empirically that feature maps of modern CNNs do not obey the conditions required for shiftability (figure 10).\n\n\n\"References and phrasing:\". Thank you again for these comments. We will revise the final version accordingly.\n\n", "title": "Response"}, "BJlhBuqGRQ": {"type": "rebuttal", "replyto": "H1g7rqLdnm", "comment": "Thank you for the helpful comments. \n\nTo establish that our results are not a side effect of our inpainting setup we repeated the same experiment with three different setups: cropping the image (figure 8 from the appendix), inpainted background (figure 2), and black background (not shown). All setups give qualitatively similar results, suggesting that the phenomenon we observe isn\u2019t a mere artifact of the choice of setup.  \nWe thank the reviewer for pointing out that are figures are too small and we will fix this on the next version.\n", "title": "Response"}, "HJlraD5MAQ": {"type": "rebuttal", "replyto": "B1lXL9e32Q", "comment": "Thank you for the useful comments.\n\n\u201chard to assess whether the reported results are \"accidents\" found in certain images or are general. \u201c\n\nAs we write in the text (third paragraph, page 3)  for approximately 30% of the randomly selected images, there exists a one pixel shift that removes the correct prediction from the top-5 (or vice versa).  Alternatively, we can measure the number of one pixel shifts that change the top 1 prediction of the network, and here the numbers are even larger: for a randomly selected image from ImageNet  embedded at a random vertical location and a fixed horizontal location,  a one pixel vertical translation will change the prediction of the InceptionResNetV2 network with a probability greater than 25%. Yet another way of saying the same thing: For a randomly selected image from ImageNet, we found on average 18.5 (25% of 74 translations) different locations where we can vertically translate the location of the embedded image by one pixel and get a change in prediction.\n\nThese are clearly general results, not accidents. We will add the top-1 numbers to the next version.\n\n\"Few quantiative results on the sensitivity to 1-pixel shift, most results are qualitative.\"\n\nResults in figure 2 show the results on 100 randomly selected images and for each one we tested 74 different 1-pixel shifts so figure 2 summarizes 7400 different experiments for each architecture. Similarly, figures 8 and 9 summarize another 7400 different experiments for each architecture.\n\nAs we write in the text (caption of figure 2) \"Images (rows) are randomly chosen from the ImageNet dataset\" and indeed we verified that performance on this subset of images is about the same as the overall reported performance of each architecture. The size of the embedded image is 150 and figure 6 shows how the size of the embedded image affects performance. Following the reviewer\u2019s suggestion, we measured the effect of the embedding size on the jaggedness and found that smaller size increases jaggedness as expected. We will add this to the next version of the paper. \n\n\n\"results on these networks with larger pooling regions, are all qualitative.\"\n\nWe now have quantitative results (jaggedness and recognition performance) for VGG16 and ResNet50 trained with larger pooling regions (accuracy goes down by about a factor of two, as does jaggedness). We will include these in the final version.\n\n\"The mathematical proof is done for average pooling, which is rarely used nowadays\".\n\nTo our knowledge, *global* average pooling is the common choice in modern CNNs. One example from the ResNet paper: \u201cThe network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax.\u201d\n\n\u201cAlso, the aforementioned experiment in which the size of the pooling region is increased, is it max pooling or average pooling?\u201d \nAverage pooling. From the captions of figure 4: \"Right: VGG16 where every 2x2 max pooling layer was replaced by a 6x6 average pooling layer.\"\n\n\"Limited results on the ImageNet bias. These results are reported in one image category (Figure 5), how general are them? \"\n\nAs written in the paper, we have actually performed this experiment on all ImageNet categories. From page 7: \"To be more quantitative, we used the available bounding-box labels, and extracted the center point of the bounding-box and its height as proxies for the object position and size respectively. We then applied a statistical significance test to ask whether object location and object sizes were uniform for that category. For more than 900 out of the 1000 categories we found that location and size were highly non uniform\"\n\n\"The paper assumes that shifting an image embedded an object in a black background is equivalent to shifting an object in a static background.\"\n\nNote that we do not use a black background, but rather we use an inpainting procedure to precisely address the reviewer's concern and make sure that no artificial boundaries are created. We also show (figure 8) that results are similar when we use cropping from a larger image, i.e. without inpainting or black borders. We have also run the experiments with black background and achieved similar results.", "title": "Response"}, "SJxCFsEbA7": {"type": "rebuttal", "replyto": "HJgzBmsyAm", "comment": "Thank you for your comment.\n \n\nPerhaps it makes sense to take this particular discussion about videos offline since we do not seem to be converging despite many rounds of back and forth (maybe a phone call will be helpful).\n\nThe important thing to remember is that except for the bottom of Figure 1, all the other results use ImageNet images,  NOT YouTube videos. Thus the statement in the reviewer's comment \"we were unable to find the source videos for the other videos shown in Figure 1\" seems to suggest a misunderstanding by the reviewer. As the text clearly describes, we took an image from imageNet, resized it (while preserving aspect ratio) , embedded it in a square image that is the size expected by the network and synthetically moved it around or rescaled it. As we also explain in the text, we experimented with different ways to embed the image into the square image (black background, inpainting, cropping) and we found a lack of invariance using all these hundreds of images\n\nThe following anonymous dropbox folder contains YouTube videos that use exactly the same pipeline as suggested by the reviewer and the network outputs are highly jagged. \nhttps://www.dropbox.com/sh/wvis40zd3sltf31/AACn480D7DB14FtneBaIMWPoa?dl=0  \n We are going to upload a code to github with the original frames to reproduce these results. We are also happy to send it to the reviewer.\n\n Again, we emphasize that figures 2 and 8 in the paper summarize experiments with thousands of presentations of images to the different networks, and all of these images are from ImageNet and do not require the reviewer's suggested pipeline nor the pipeline we describe in the appendix.", "title": "misunderstanding?"}, "B1lXL9e32Q": {"type": "review", "replyto": "HJxYwiC5tm", "review": "This paper studies the lack of shift invariance in state-of-the-art neural networks, namely, the paper introduces results that show that state-of-the-art deep neural networks are affected by 1-pixel shifts because the convolutional layers in the network poorly sample the feature maps. The topic addressed in the paper is critical for many computer vision systems, as lack of shift invariance is a catastrophic failure mode. The arguments of the paper help clarifying why the networks are so sensitive to small shifts of the objects (poor subsampling) but generalize well (there is a bias in the location of the objects in the dataset).  Both of these arguments and the sensitivity of the networks to small shifts are well known in the literature, but it is great to see a paper that puts them together and tests these arguments in state-of-the-art deep nets for ImageNet.\n\nHowever, the paper could do a much better job providing evidence to support the arguments:\n\n*Few quantiative results on the sensitivity to 1-pixel shift, most results are qualitative. This makes hard to assess whether the reported results are \"accidents\" found in certain images or are general. The results that support that 1-pixel shifts affect state-of-the-art neural networks are in Figure 2b. Yet, these results are unclear, eg. is \"400 Jaggedness\" a lot?, what is the size of the embedded image?, How are the 100 images selected? Is the network performing well in those images? How does the size of the embedded image change the \"Jaggedness\"? \n\n*Something that could help to strengthen the results would be to add networks with better sampling + larger pooling regions and see how this solves the lack of shift invariance. Now it has only been tested increasing the pooling regions, which misses the main point of the paper. Also, the results on these networks with larger pooling regions, are all qualitative.\n\n*The mathematical proof is done for average pooling, which is rarely used nowadays. I would suggest using max pooling. Also, the aforementioned experiment in which the size of the pooling region is increased, is it max pooling or average pooling?\n\n*Limited results on the ImageNet bias. These results are reported in one image category (Figure 5), how general are them?\n\n*The paper assumes that shifting an image embedded an object in a black background is equivalent to shifting an object in a static background. A hypothesis would be that the embedding of the image in the black background creates artificial boundaries that make the network more fragile to 1-pixel shifts than for natural images.\n\nIn summary, I think this is a paper that may arise a lot of interest, although the different arguments are known and the experiments are poorly executed.\n\n", "title": "Critical topic, but limited novelty and results", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1g7rqLdnm": {"type": "review", "replyto": "HJxYwiC5tm", "review": "This paper describes an empirical study of translation and scale\ninvariance properties of modern CNN architectures. The authors conduct\na thorough study of translation invariance in VGG16, ResNet50, and\nInceptionResNet with respect to the Nyquist frequency and shift-\nversus translation-invariance properties of network layers as a\nfunction of depth and subsampling rate. Empirical observations are\nquantified using a variety of metric to measure the stability of\nfeature maps under geometric transformations of the input.\n\nThe paper has the following strong points:\n\n 1. It tells an interesting (and engaging) story about a largely\n    empirical study, and while doing this never pretends to be more\n    than it is.\n 2. Empirical observations are supported by quantitative measures that\n    give compelling evidence for most observations in the paper. The\n    discussion about shiftability versus translatability is\n    particularly interesting with its link to nonlinearity, smoothing\n    and Nyquist limits.\n\nThe paper has the following weak points:\n\n 1. The reliance on inpainting for almost all experiments is somewhat\n    worrying. It is not clear that this procedure isn't introducing\n    its own biases affecting translation and scale invariance. The\n    authors make reference to a separate protocol reported in the\n    appendices, but it isn't clear which results in the appendices\n    they are referring to. A more thorough control study seems in\n    order to verify that inpainting is a reasonable simulation.\n 2. Some figures are scaled down to the limits of legibility.\n\nIn summary: I like this paper a lot, and I think it adds useful\nelements and analytical tools (both theoretical and empirical) to the\ndiscussion on invariants in modeern CNNs.\n", "title": "Nice empirical study of invariants in modern CNNs, with quantitative support at all key points.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJeBZeoN27": {"type": "review", "replyto": "HJxYwiC5tm", "review": "Paper summary: \n\nAs is made clear in the title, this paper sets out to answer the following question: \u201cWhy do deep convolutional networks generalize so poorly to small image transformations?\u201d. It focuses on natural image transformations on translation and scaling (rotation is missing though).\n\nThe paper proposes two main explanations: \n-\tStrided convolution, called subsampling in the paper, ignores the classical sampling theorem,\n-\tCNNs will not learn invariance because of the (photographers') biases contained in the datasets.\n\nOn a general level, the paper is a good read, it is well written and the figures clearly convey the message they\u2019re intended to. Adversarial attacks and robustness of CNNs in general is a very interesting and important topic in ML. The originality of this work is in the approach of the problem, the paper tries to explain the reasons why CNNs are vulnerable. Related works put more emphasis on coming up with novel attacks/defense strategies. Considering natural attacks as done in this submission is particularly interesting as it is probably a more surprising shortcoming of CNNs compared to optimally designed attacks or highly unnatural perturbations. The argument about subsampling (stride) being the reason of not having translational invariance is nice, especially the theoretical insight with the Shannon-Nyquist theorem and the more figurative example on part detectors. There are nevertheless a few major concerns about this work:\n\nMajor Concerns:\n\nTheoretical arguments:\nThe theoretical argument made in this paper is interesting but to make the point stronger a more in-depth explanation would be needed.\n-\tThe step from Eq (2) to Eq (3) is not entirely clear \u201cK does not depend on x_i\u201d, maybe one extra sentence to explain this step would be useful. \n-\tTerms introduced such as the basis function B and the set of transformations T could be better defined.\n-\tFor the extension to other types of transformations \u201cWhile the claim focuses on global translation, it can also be extended to piecewise constant transformations.\u201d it would be important to point out what type of natural transformations can be included in this set.\n\nEmpirical evidence:\nExperiments are not fully convincing. Additional empirical evidence would be beneficial and necessary to support the claims of this:\n-\t\u201cA natural criticism of these results is that they are somehow related to the image resizing and inpainting procedures that we used.\u201d This is a very good point and the authors following arguments are not fully convincing. Results with different transformation procedures mentioned in the rest of the paragraph (and probably more) should be included to convince the reader.\n-\tThe theoretical argument that translation invariance is not guaranteed because of the stride (subsampling) is not fully convincing and needs further explanation and experimental verification. In fact, feature maps of the CNNs that the authors consider do indeed contain many high frequencies.\n-\tThe argument made in part 4 about the photographer\u2019s bias seems valid for general natural transformations, but it does not apply to small transformations such as 1-pixel translations presented in the paper. Also, evidence that datasets without (or less) photographers' bias are less susceptible to natural attacks would make the argument in the paper a lot stronger. \n-\tWhen using 6x6 avg pooling for the VGG16 architecture \u201drecognition performance decreases somewhat\u201d . Results are only preliminary in the paper, but this statement needs a more thorough experimental backing. It should come with convincing quantitative evidence.\n-\tPlease include some results or citation on other work about test time augmentation to support the statement \u201cstill only provides partial invariance\u201d.\n\nReferences and phrasing:\nGenerally previous work is well referenced in this paper, although there are some formulations that can be slightly modified to make a clear distinction between what is novel and what is previous work:\n-\tAs is very well shown in the introduction, there is a lot of work on generating adversarial examples that drastically change the output of a CNN. This should be made clear in the abstract, in fact the sentence \u201cIn this paper we show that modern CNNs [...] also happens with other realistic small image transformations\u201d  seems to indicate that this is the novel work in the paper. This is also why I believe the first sentence \u201cDeep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations.\u201d is somewhat contestable. \n-\t\u201cWe find that modern deep CNNs are not invariant to translations, scalings and other realistic image transformations\u201d as the paper points out earlier this is not a novel finding, so I would use a formulation that makes that clear and gives more emphasis to your own arguments as of why this happens.\n\nFurther Comments :\n-\tPart 5 \"Implications for Practical Systems\" could be moved to discussion as there is no new point and it seems more a reflection on what was already stated.\n-\tThe final sentence of the abstract \u201cTaken together our results suggest that the performance of CNNs in object recognition falls far short of the generalization capabilities of humans.\u201d is not necessary, this is clearly true but it isn\u2019t really contested in the ML community.\n-\t\u201cdespite the architecture being explicitly designed to provide such invariances\u201d I agree that this has motivated the use and design of CNNs in the first place, but modern architectures are mostly designed to surpass the results on the common benchmarks rather than to provide such invariances.\n-\t\u201djaggedness is greater for the modern, deeper, networks compared to the less modern VGG16 network\u201d might be worth interesting to consider if the residuals have anything to do with it.\n", "title": "Although the general idea is interesting, experimental evaluation is not convincing. Similarly, some explanations like the photographer's bias as reason for susceptability to very small image transformation is not entirely convincing. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}