{"paper": {"title": "Improving Neural Language Models with a Continuous Cache", "authors": ["Edouard Grave", "Armand Joulin", "Nicolas Usunier"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "usunier@fb.com"], "summary": "", "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.", "keywords": ["Natural language processing"]}, "meta": {"decision": "Accept (Poster)", "comment": "Reviewers agree that this paper is based on a \"trick\" to build memory without requiring long-distance backprop. This method allows the model to utilize a cache-like mechanism, simply by storing previous states. Everyone agrees that this roughly works (although there could be stronger experimental evidence), and provides long-term memory to simple models. Reviewers/authors also agree that it might not work as well as other pointer-network like method, but there is controversy over whether that is necessary. \n \n - Further discussion indicated a sense by some reviewers that this method could be quite impactful, even if it was not a huge technical contribution, due to its speed and computational benefits over pointer methods. \n \n - The clarity of the writing and good use of references was appreciated \n \n - This paper is a nice complement/rebuttal to \"Frustratingly Short Attention Spans in Neural Language Modeling\".\n \n Including the discussion about this paper as it might be helpful as it was controversial: \n \n \"\"\"\n The technical contribution may appear \"limited\" but I feel part of that is necessary to ensure the method can scale to both large datasets and long term dependencies. For me, this is similar to simpler machine learning methods being able to scale to more data (though replacing \"data\" with \"timesteps\"). More complex methods may do better with a small number of data/timesteps but they won't be able to scale, where other specific advantages may come in to play.\n \n (Timesteps)\n \n Looking back 2000 timesteps is something I've not seen done and speaks to a broader aspect of language modeling - properly capturing recent article level context. Most language models limit BPTT to around 35 timesteps, with some even arguing we don't need that much (i.e. \"Frustratingly Short Attention Spans in Neural Language Modeling\" that's under review for ICLR). From a general perspective, this is vaguely mad given many sentences are longer than 35 timesteps, yet we know both intuitively and from the evidence they present that the rest of an article is very likely to help modeling the following words, especially for PTB or WikiText.\n \n This paper introduces a technique that not only allows for utilizing dependencies far further back than 35 timesteps but shows it consistently helps, even when thrown against a larger number of timesteps, a larger dataset, or a larger vocabulary. Given it is also a post-processing step that can be applied to any vaguely RNN type model, it's widely applicable and trivial to train in comparison to any more complicated models.\n \n (Data)\n \n Speaking to AnonReviewer1's comment, \"A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)\"\n \n Existing pointer network approaches for language modeling are very slow to train - or at least more optimal methods are yet to be discovered - and has such limited the BPTT length they tackle. Merity et al. use 100 at most and that's the only pointer method for language modeling attending to article style text that I am aware of. Merity et al. also have a section of their paper specifically discussing the training speed complications that come from integrating the pointer network. There is a comparison to Merity et al. in Table 1 and Table 2.\n \n The scaling becomes more obvious on the WikiText datasets which have a more realistic long tail vocabulary than PTB's 10k. For WikiText-2, at a cache size of 100, Merity et al. get 80.8 with their pointer network method while the neural cache model get 81.6. Increasing the neural model cache size to 2000 however gives quite a substantial drop to 68.9. They're also able to apply their method to WikiText-103, a far larger dataset than PTB or WikiText-2, and show that it still provides improvements even when there is more data and a larger vocabulary. Scaling to this dataset is only sanely possible as the neural cache model doesn't add to the training time of the base neural model at all - that it's equivalent to training a standard LSTM.\n \"\"\""}, "review": {"rJIKOwrNl": {"type": "rebuttal", "replyto": "SJv48zBNl", "comment": "Thank you for your review and questions.\n\nThe main message of this paper is to show that a simple method for augmenting RNN with memory is very competitive with more complex approaches, on the task of language modeling. We do not claim that it is the best way to do so (compared e.g. to pointer networks), but that it is the most efficient. In particular, our method is drastically faster at train time (no overhead compared to training a model without memory), or can even be applied to pre-trained models, for free. This allows our method to scale to much larger cache sizes and datasets, leading to much better performance than using more complicated models.\n\nRegarding comparisons to memory augmented models, we do compare to Pointer Networks (c.f. Table 2) and Memory Networks (c.f. Table 3), which were tailored for language modeling. More precisely, on the WikiText-2 dataset (c.f. Table 2), our approach outperforms pointer networks (Merity et al., 2016), with a 14.7% reduction in perplexity. We were also able to apply our model on the larger WikiText-103 dataset (we believe training pointer networks cannot scale to this dataset), leading to further reduction in perplexity (WikiText-2 and WikiText-103 share the same validation & test sets). We also compare our method to Memory Networks (Sukhbaatar et al., 2015) on the text8 dataset (c.f. Table 3), where we observe a 32% reduction in perplexity. We thus believe that our claim that our simple approach is competitive with more complicated models is well supported by results reported in the paper.\n\n- \"In the experiment results, for your neural cache model, are those results with linear interpolation or global normalization, or the best model? Can you show results for both?\"\n\nAs stated in the paragraph \"Results\" of section 5.1, we report results with linear interpolation (except in Figure 2 & 3, where it is shown that both methods obtain similar results on PTB and WikiText-2, with linear interpolation being easier to apply).\n\n- \"Why is the neural cache model worse than LSTM on Ctrl (Lambada dataset)? Please also show accuracy on this dataset.\"\n\nAs explained in section 5.3 (& Figure 5), performance on the control set of Lambada degrades when increasing the value of the interpolation parameter of the cache. This is because only a small number of examples of the control set contain the target word in the context (and therefore, the cache model in not useful for these examples).\n\n- \"It is also interesting that the authors mentioned that training the cache component instead of only using it at test time gives little improvements. Are the results about the same or worse? \"\n\nThe results were very similar, while it is much easier to train without the cache component.", "title": "re: review"}, "BJCdudaQx": {"type": "rebuttal", "replyto": "SJE4wLpme", "comment": "Thank you for your answer. I think it's important to report the accuracy, because this is the metric that the dataset authors had in mind, and also because this number can be referred to in subsequent work. You should not shy the seemingly low number 17% because the dataset is challenging. Also, as far as I know, your performance is SOTA, because all the better results I am aware of were obtained using a trick, whereby the model was forced to select a word from the context. If you achieved 17% without this trick, this is definitely worth reporting.", "title": "re: re: Accuracy on LAMBADA"}, "SJE4wLpme": {"type": "rebuttal", "replyto": "BkHDNXcfe", "comment": "Thank you for your comment.\n\nWe performed early experiments and obtained around 17% accuracy on the LAMBADA dataset. As far as we know, this is the state-of-the-art for language models (which only read the passage from left to right, and not from right to left). We focused on perplexity in the paper, as we are mostly interested in language modeling applications. We need to run additional experiments to get a definitive accuracy number on the LAMBADA dataset.\n", "title": "re: Accuracy on LAMBADA"}, "ryJvm8a7g": {"type": "rebuttal", "replyto": "SJuL9N1me", "comment": "Thank you for your comment.\n\n(1) Early experiments with training the cache component showed little improvement over only applying it at test time. It also makes the learning more complicated (because gradients must be back-propagated through the cache), especially for very large caches. We thus decided to only apply it at test time.\n\n(2) On the PTB dataset, the best results are obtained for a cache of size 500. This is probably the case because articles from the Wall Street Journal are shorter than Wikipedia articles (and thus, a shorter cache is enough to store full articles).\n\n(3) We performed early experiments and obtained around 17% accuracy on the LAMBADA dataset. As far as we know, this is the state-of-the-art for language models (which only read the passage from left to right, and not from right to left). We focused on perplexity in the paper, as we are mostly interested in language modeling applications.", "title": "re: comments"}, "r1fa4Lpmg": {"type": "rebuttal", "replyto": "rJhNxHy7l", "comment": "Thank you for your comment.\n\nContrary to our model which uses h_t as a representation for x_{t+1}, the model of Merity et al. uses h_t as a representation for x_t. This requires to learn an additional transformation between the current activation and those in the cache. This transformation is then trained jointly with the rest of the model, limiting the size of cache (because the BPTT algorithm is performed for L time-steps, where L is the cache size). Contrary to our approach, Merity et al. uses dynamic interpolation (through the sentinel vector in the pointer softmax).\n\nOur method is more scalable at train time, since it does not require to perform BPTT over L time-steps, where L is the size of the cache (contrary to Merity et al.). We can thus scale to much larger cache size easily.\n\nMerity et al. only reported results for cache of size 100.", "title": "re: Questions"}, "Syj3I86Xe": {"type": "rebuttal", "replyto": "rkTpYikXx", "comment": "Thank you for your comment.\n\nIn Table 1, we use a cache of size 500 (we report validation perplexity in Figure 2 and test perplexity in Table 1). A cache of size 100 gets a test perplexity of 74.2 on the Penn TreeBank dataset.\n\nWe do not really have further insights beyond dataset sizes as the difference in performance improvements. We believe that with more training data, the difference between models tends to be less important. It was already observed by Goodman (2001) that the improvement due to cache models is decreasing when the training set size increases.\n\nWhen training on the WikiText-103 dataset, we use the hierarchical softmax because of the large vocabulary.  As of now, our implementation only support linear interpolation when using the hierarchical softmax. Hence, we could only generate the left subfigure of Fig. 3 for WikiText-103, which unfortunately does not provide interesting insight. It should be noted that the hyper-parameters theta and lambda were chosen on the validation set.", "title": "re: Cache size on PTB, WikiText vocabulary differences, and Lambada question"}, "rkTpYikXx": {"type": "review", "replyto": "B184E5qee", "review": "Thanks for the paper - it's an intelligent method that can be applied relatively easily to existing RNN models with a good performance boost. I imagine this may find a variety of uses.\n\nMaybe I am missing it, but it doesn't seem immediately clear the cache size used for the Penn Treebank model? The Pointer Sentinel LSTM work used a window history of 100 and Figure 2 notes that the cache size is 500 to get a perplexity of 74.6 - what was the cache size used to produce the PTB number in Table 1? Maybe adding that in the table, as in Table 2, and also a comparable number (100, 2000, X) would be useful for a glance comparison between the tables?\n\nThe result comparison between WikiText-2 and WikiText-103 was quite helpful. Citing the importance of testing on relatively large datasets is hugely important given Penn Treebank seems to be the standard dataset for language modeling even though it's quite small in both text and vocabulary. Have you any further insights beyond dataset sizes as to difference in performance improvements? The vocabulary size difference between WikiText-2 and WikiText-103 is quite massive - is it possible that the vocabulary size difference may make the cache memory model less advantageous?\n\nFor the Lambada dataset, the insight regarding the lambda parameter for development and control sets were interesting. As you note, it seems the interpolation parameter would need to be flexible to ensure strong generative ability as to beat the LSTM even when the cache memory aspect is not helpful. Might a similar issue have occurred with WikiText-103 in order to decrease the improvement the cache model may give? I don't see a Figure 3 style chart for WikiText-103 - likely as it's very expensive to produce - but it'd be possible that might provide that insight.This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.\n\nThey illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously.\nI recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.\n\nI recommend this interesting and well analyzed paper be accepted.", "title": "Cache size on PTB, WikiText vocabulary differences, and Lambada question", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1YGZUMNx": {"type": "review", "replyto": "B184E5qee", "review": "Thanks for the paper - it's an intelligent method that can be applied relatively easily to existing RNN models with a good performance boost. I imagine this may find a variety of uses.\n\nMaybe I am missing it, but it doesn't seem immediately clear the cache size used for the Penn Treebank model? The Pointer Sentinel LSTM work used a window history of 100 and Figure 2 notes that the cache size is 500 to get a perplexity of 74.6 - what was the cache size used to produce the PTB number in Table 1? Maybe adding that in the table, as in Table 2, and also a comparable number (100, 2000, X) would be useful for a glance comparison between the tables?\n\nThe result comparison between WikiText-2 and WikiText-103 was quite helpful. Citing the importance of testing on relatively large datasets is hugely important given Penn Treebank seems to be the standard dataset for language modeling even though it's quite small in both text and vocabulary. Have you any further insights beyond dataset sizes as to difference in performance improvements? The vocabulary size difference between WikiText-2 and WikiText-103 is quite massive - is it possible that the vocabulary size difference may make the cache memory model less advantageous?\n\nFor the Lambada dataset, the insight regarding the lambda parameter for development and control sets were interesting. As you note, it seems the interpolation parameter would need to be flexible to ensure strong generative ability as to beat the LSTM even when the cache memory aspect is not helpful. Might a similar issue have occurred with WikiText-103 in order to decrease the improvement the cache model may give? I don't see a Figure 3 style chart for WikiText-103 - likely as it's very expensive to produce - but it'd be possible that might provide that insight.This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.\n\nThey illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously.\nI recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.\n\nI recommend this interesting and well analyzed paper be accepted.", "title": "Cache size on PTB, WikiText vocabulary differences, and Lambada question", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJhNxHy7l": {"type": "review", "replyto": "B184E5qee", "review": "The authors mention the similarity of this method to a copying mechanism applied to language modeling (i.e. CopyNet), pointer sentinel mixture models (Merity et al.), and the attention sum reader. Can the authors elaborate more on the difference with Merity et al.? Since both methods require computing attention over the previous timesteps, why is this method more scalable than that one? What performance does the pointer sentinel model obtain with a cache size of 1000?The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes. They demonstrate good improvements on language modeling by adding this cache to RNN baselines.\n\nThe main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop. This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing.\n\nThe basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks.\n\nMy main criticism of this work is its simplicity and incrementality when compared to previously existing literature. As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference. However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication.", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BkjpniLEg": {"type": "review", "replyto": "B184E5qee", "review": "The authors mention the similarity of this method to a copying mechanism applied to language modeling (i.e. CopyNet), pointer sentinel mixture models (Merity et al.), and the attention sum reader. Can the authors elaborate more on the difference with Merity et al.? Since both methods require computing attention over the previous timesteps, why is this method more scalable than that one? What performance does the pointer sentinel model obtain with a cache size of 1000?The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes. They demonstrate good improvements on language modeling by adding this cache to RNN baselines.\n\nThe main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop. This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing.\n\nThe basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks.\n\nMy main criticism of this work is its simplicity and incrementality when compared to previously existing literature. As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference. However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication.", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJuL9N1me": {"type": "review", "replyto": "B184E5qee", "review": "(1) Could you please comment on why you decided to train without the cache component and apply it only at test time?\n(2) On the PTB dataset, do the results get better if you increase the cache size, or is the dataset too small and it starts to overfit?\n(3) Similar to the below comment, I also would like to know the accuracy on the LAMBADA dataset.\n\nThanks!This paper proposes a simple extension to a neural network language model by adding a cache component. \nThe model stores <previous hidden state, word> pairs in memory cells and uses the current hidden state to control the lookup. \nThe final probability of a word is a linear interpolation between a standard language model and the cache language model. \nAdditionally, an alternative that uses global normalization instead of linear interpolation is also presented. \nExperiments on PTB, Wikitext, and LAMBADA datasets show that the cache model improves over standard LSTM language model.\n\nThere is a lot of similar work on memory-augmented/pointer neural language models, and the main difference is that the proposed method is simple and scales to a large cache size.\nHowever, since the technical contribution is rather limited, the experiments need to be more thorough and conclusive. \nWhile it is obvious from the results that adding a cache component improves over language models without memory, it is still unclear that this is the best way to do it (instead of, e.g., using pointer networks). \nA side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)\n\nSome questions:\n- In the experiment results, for your neural cache model, are those results with linear interpolation or global normalization, or the best model? Can you show results for both? \n- Why is the neural cache model worse than LSTM on Ctrl (Lambada dataset)? Please also show accuracy on this dataset. \n- It is also interesting that the authors mentioned that training the cache component instead of only using it at test time gives little improvements. Are the results about the same or worse?", "title": "comments", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJv48zBNl": {"type": "review", "replyto": "B184E5qee", "review": "(1) Could you please comment on why you decided to train without the cache component and apply it only at test time?\n(2) On the PTB dataset, do the results get better if you increase the cache size, or is the dataset too small and it starts to overfit?\n(3) Similar to the below comment, I also would like to know the accuracy on the LAMBADA dataset.\n\nThanks!This paper proposes a simple extension to a neural network language model by adding a cache component. \nThe model stores <previous hidden state, word> pairs in memory cells and uses the current hidden state to control the lookup. \nThe final probability of a word is a linear interpolation between a standard language model and the cache language model. \nAdditionally, an alternative that uses global normalization instead of linear interpolation is also presented. \nExperiments on PTB, Wikitext, and LAMBADA datasets show that the cache model improves over standard LSTM language model.\n\nThere is a lot of similar work on memory-augmented/pointer neural language models, and the main difference is that the proposed method is simple and scales to a large cache size.\nHowever, since the technical contribution is rather limited, the experiments need to be more thorough and conclusive. \nWhile it is obvious from the results that adding a cache component improves over language models without memory, it is still unclear that this is the best way to do it (instead of, e.g., using pointer networks). \nA side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)\n\nSome questions:\n- In the experiment results, for your neural cache model, are those results with linear interpolation or global normalization, or the best model? Can you show results for both? \n- Why is the neural cache model worse than LSTM on Ctrl (Lambada dataset)? Please also show accuracy on this dataset. \n- It is also interesting that the authors mentioned that training the cache component instead of only using it at test time gives little improvements. Are the results about the same or worse?", "title": "comments", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkHDNXcfe": {"type": "rebuttal", "replyto": "B184E5qee", "comment": "Hi, great paper! \n\nCould you please also report the accuracy on LAMBADA dataset, not just perplexity?", "title": "Accuracy on LAMBADA"}}}