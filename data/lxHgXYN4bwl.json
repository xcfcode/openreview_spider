{"paper": {"title": "Expressive Power of Invariant and Equivariant Graph Neural Networks", "authors": ["Waiss Azizian", "marc lelarge"], "authorids": ["waiss.azizian@ens.fr", "~marc_lelarge1"], "summary": "", "abstract": "Various classes of Graph Neural Networks (GNN) have been proposed and shown to be successful in a wide range of applications with graph structured data. In this paper, we propose a theoretical framework able to compare the expressive power of these GNN architectures. The current universality theorems only apply to intractable classes of GNNs. Here, we prove the first approximation guarantees for practical GNNs, paving the way for a better understanding of their generalization. Our theoretical results are proved for invariant GNNs computing a graph embedding (permutation of the nodes of the input graph does not affect the output) and equivariant GNNs computing an embedding of the nodes (permutation of the input permutes the output). We show that Folklore Graph Neural Networks (FGNN), which are tensor based GNNs augmented with matrix multiplication are the most expressive architectures proposed so far for a given tensor order. We illustrate our results on the Quadratic Assignment Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to learn how to solve the problem, leading to much better average performances than existing algorithms (based on spectral, SDP or other GNNs architectures). On a practical side, we also implement masked tensors to handle batches of graphs of varying sizes. ", "keywords": ["Graph Neural Network", "Universality", "Approximation"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "This paper is concerned with the ongoing research program of mapping the approximation power of different GNN architectures. It provides significant advances in the study of equivariant GNNs and nice extensions in the invariant case by closing existing gaps between distinct GNN families. \nAll reviewers agreed that this is a strong submission with substantial new theoretical results. The AC recommends a strong acceptance. "}, "review": {"EbmT1m2UmY": {"type": "review", "replyto": "lxHgXYN4bwl", "review": "#### Goal\n\nThe paper describes the approximation power of certain types of graph neural networks.  It considers Message Passing GNNs (MGNNs), and two GNN-type methods proposed by Maron et al., k-FGNN and k-LGNN.\n\nThe main challenge with some of the current expressiveness analysis of neural networks is their reliance on discrete attributes. This line of work shows how these can be extended to a general setting.\n\n#### Quality\n\n- This is a great paper, well written, nice appendix, on an interesting topic. The only drawback is that the approach was narrowly applied to k-FGNN and k-LGNN, comparing them to k-WL approaches.\n\n- My main concern is being too narrowly focused on k-FGNN and k-LGNN without a proper empirical comparison with other more expressive competing methods. For instance, it does not even discuss alternative approaches using group averaging. I think the paper would significantly broaden its audience if there is a comparison with novel k-ary group averaging approaches (e.g., Chen et al., 2020, Murphy et al. 2019) in the empirical section. \n\n#### Clarity\n\nThe paper is clear and well-written. The notation follows the standard notation used in prior work.\nIn one aspect the notation could improve: \\subset should be replaced by \\subseteq to avoid confusion, and be more in line with the use of \\subsetneq. I was not sure what \\subseteq meant until I saw \\subsetneq. It is unfortunately but different branches of mathematics use \\subseteq differently and the symbol should be avoided altogether. \nThe appendix is well organized, with an index that helps the reader find proofs and definitions.\n\n#### Originality\n- The work follows from prior work of Maehara & Hoang (2019), Chen et al., 2019, Geerts (2020a), etc.\n- There is some novelty in the proof approach\n- New results related to k-FGNN and k-LGNN.\n\n#### Significance\n\nThe work is of interest to a narrow sub-community working on k-FGNN and k-LGNN. The MGNN results were known but recovered here in a different way (i.e., there is value). The k-FGNN and k-LGNN results are new but also very narrow due to the sub-community size.\n\nThe empirical results essentially do not compare with any other alternative approach. Until the community is convinced that k-FGNN and k-LGNN can empirically compete with other approaches (e.g., Morris et al. 2019, Permutation group averages like \u00a0Chen et al. 2020, Murphy et al. 2019, among others), these methods will not see broad applicability. \n\nChen, Zhengdao, Lei Chen, Soledad Villar, and Joan Bruna. \"Can graph neural networks count substructures?.\"\u00a0NeurIPS\u00a0(2020).\n\nMurphy, Ryan L., Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. \"Relational pooling for graph representations.\"\u00a0ICLR (2019).\n\n#### Pros\n- Solid theoretical work showing the expressiveness of k-FGNN and k-LGNN\n- Interesting ports of ideas from other papers to improve proof techniques of current GNN papers\n\n#### Cons\n\n- The authors seem unfamiliar with group average approaches that increase the expressiveness of MGNNs. These can also be universal approximations. Permutation group averages like \u00a0Chen et al. 2020, Murphy et al. 2019, among others, have increased expressiveness of MGNN using the same MGNN architecture.  Since there has been a lot of recent activity related to these methods, this is a blind spot of this work. For instance, \u201cNote also, that if the nodes are given distinct features, MGNNs become much more expressive Loukas (2019) but this is meaningless in some problems such as our graph alignment problem.\u201d does not point to the fact that a simple group average approach would fix the issue. This way, the work feels unnecessarily narrow.\n\n#### Typos:\n- \"test to express the discriminatory power of equivariant architectures For this\" => \"test to express the discriminatory power of equivariant architectures. For this\"\n- \"Maron et al. (2018) Geerts (2020a)\" => \"Maron et al. (2018); Geerts (2020a)\"\n\n----------------------------------------\n\nAfter rebuttal:\n\nThanks for the answers. I have raised my score even though I still think the paper could have done a better job at comparing against other methods.\n\n- Regarding group-averaging methods for the equivariant case: It is a trivial extension, specially the approach of giving GNNs unique IDs and then averaging their representation, which (Loukas, 2020) shows it is universal (but (Loukas, 2020) did not consider averaging). Regarding training, it is always performed stochastically via data augmentation and Monte Carlo estimated in test. For the generalization error of the stochastic optimization, it is still unknown (some new results show promise (Chen et al. 2020) and (Lyle et al. 2020)) in the same way that the generalization performance of other universal methods is still unknown.\n\n- Chen, S., Dobriban, E., & Lee, J. H. (2020). Invariance reduces Variance: Understanding Data Augmentation in Deep Learning and Beyond.\n- Lyle, Clare, Mark van der Wilk, Marta Kwiatkowska, Yarin Gal, and Benjamin Bloem-Reddy. \"On the Benefits of Invariance in Neural Networks.\" arXiv preprint arXiv:2005.00178 (2020).\n- Loukas, Andreas. \"How hard is to distinguish graphs with graph neural networks?.\" Advances in Neural Information Processing Systems 33 (2020).", "title": "Interesting paper with fixable deficiencies (Likely to advocate for it)", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "PSP6pWkYfov": {"type": "review", "replyto": "lxHgXYN4bwl", "review": "1. Summary:\n\nThis paper compares the expressive power of three types of invariant and equivariant GNNs against the Weisfeiler-Lehman (WL) tests, proves function approximation results for these GNNs, and demonstrates that 2-FGNN_E works well for the quadratic assignment problem.\n\n2. Clearly state the recommendation (accept or reject) with one or two key reasons for this choice:\n\nI am slightly leaning towards recommending a rejection to this paper, with the main reason being that I am not sure if the contributions are sufficiently significant, as I will elaborate below.\n\n3. Arguments for the recommendation and questions for the authors:\n\na) It is nice to establish and summarize a thorough comparison of the expressive powers of different GNN families as well as k-WL, and also to incorporate not only invariant but also equivariant GNNs into consideration. However, among the results in Section 4.2, the only novel one is the equivalence between k-FGNN and (k+1)-WL, which, one may argue, is also not surprising considering the equivalence between k-FWL and (k+1)-WL. The value of 2-FGNN has also been demonstrated by the work of [1].\n\nb) The overall framework for the comparison of expressive power as well as function approximation resembles that of [2], which characterizes the expressive power of GNNs and WL tests by the sigma algebra they induce on the space of graphs, and also establishes an equivalence between the ability to distinguish non-isomorphic graphs and the ability to approximate invariant functions. Hence, the authors may want to elaborate more on the connection between this work and [2].\n\n4. Additional comments:\n\nThe authors claim that 3-LGNN or 2-FGNN can count the number of 6-cycles, which is an improvement upon the result in [3], which implies that 6-LGNN can count 6-cycles. Technically, however, it is only known (by [4], to my knowledge) that 3-WL can count the number of times 6-cycles appear as subgraphs of a graph, but not induced subgraphs, whereas [3] is concerned with the counting of both subgraphs and induced subgraphs. Therefore, I would recommend that the authors make this statement more precise.\n\nReferences:\n\n[1] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks.\n\n[2] Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph isomorphism testing and function approximation with GNNs.\n\n[3] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures?\n\n[4] Martin Fu \u0308rer. On the Combinatorial Power of the Weisfeiler-Lehman Algorithm.\n\n\n=== Post Rebuttal ===\n\nI appreciate the careful response provided by the authors, which reminds me of the significance of extending existing theoretical results from invariant to equivariant models. Therefore I have raised my score by 1 point.", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "PlNBGug-HLp": {"type": "review", "replyto": "lxHgXYN4bwl", "review": "Summary:\nThe authors prove several statements about the expressiveness of different classes of graph neural nets (GNNs): conventional message passing networks, linear GNNs (LGNN) and \u201cfolklore GNNs\u201d (FGNN). The novel theoretical contributions include analysis of expressiveness of FGNNs that use tensors of arbitrary order in terms of comparison to the Weisfeiler-Lehman tests; a characterization of the functions that these classes of networks can approximate; universality of FGNN as the tensor order goes to infinity. The results are based on a general Stone-Weierstrass-like theorem for equivariant functions. Prior universality results can be recovered as special cases. The authors have a simple experiment that show in a limited setting that a practical implementation agrees with the theory.\n\nStrengths:\n-\tThe paper and appendix are very well written and relatively well understandable for me, unfamiliar with universality proofs. Particular examples of clear writing include: the statement of Theorem 4 is clearly explained below the theorem; providing Example 16 directly after Corollary 15 aids exposition; using example 17 to motivate prop 18. \n-\tThe authors use a very general statement (Thm 20) to derive their results, making them generally applicable.\n-\tThe authors derive a substantial number of expressiveness results from the general theory.\n\nWeaknesses & suggestions for improvement:\n-\tThe main paper only sets up the problem and states the main results, while all theoretical contributions are done in the appendix. The main paper would be more self-contained if some more intuition for the proofs was given in the main paper.\n-\tThe experiments seem to not compare to LGNN. Adding this comparison would help making an empirical argument for why FGNN is best. \n\nRecommendation:\nAlthough I am not very familiar with the field of universality proofs, the paper appears to me to be a very solid contribution to the field and I recommend publication.\n \nMinor points / suggestions:\n-\tIn several instances, the authors write \u201ca compact\u201d without a noun. Is this conventional language?\n-\tBelow Eq 2, there is an F without subscript. Is this the same for all layers?\n-\tIn App C.3, in the second line of the equation, are there suffices missing on the left-hand side?\n-\tSec 4.3, first sentence, \u201cthe set invariant\u201d, missing \u201cof\u201d \n-\tSec 4.3, typo \u201cTo clarify the meaning of theses statements\u201d\n-\tApp D.3: typo \u201cas every function satisfy\u201d should be \u201csatisfies\u201d\n-\tExample 16, typo \u201cis able to learn function\u201d should be plural\n-\tApp D.3, proof of Corollary 15: define when a class separates a set around \u201cclearly separates X_\\mathcal{F}\u201d. Presumably means same as separates points?\n-\tEq 17, Define S \\cdot \\mathcal{F}, presumably as the scalar-vector product of the outputs of the functions? \n-\tCorollary 19, assumption 3, what does \u201cpairwise distinct coordinates\u201d mean?\n-\tApp D.3, typo \u201cFor an equivariant function, for any\u201d missing $f$\n-\tExample 21, \\mathcal{C}_{eq} should be \\mathcal{C}_E? Happens later more.\n-\tLemma 24, define R{X_1, .., X_p] as polynomials\n\n### Post rebuttal\nI thank the authors for their response. My previous rating still applies.", "title": "Solid theoretical contribution to universality proofs of equivariant networks", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "PQwpJwfudCn": {"type": "review", "replyto": "lxHgXYN4bwl", "review": "post-discussion:\nI read the author's response and other reviews. I still think this is a very strong submission and would like to see it accepted.\nI encourage the authors to add a section discussing their generalization of SW theorem + Theorems 31,32 as the authors suggested. another small point: I couldn't easily find the definition of \"stable by concatenation\".\n\n\nSummary: \n\nThe paper studies the expressive power of several classes of recently suggested models: message passing neural networks, Linear GNNs, and Folklore GNNs. Understanding the expressivity of GNNs is a timely and important topic since unlike other neural architectures as fully connected networks, the most widely-used models today, message-passing networks, are not universal. The paper first formally defines the three model classes mentioned above. The paper then reviews previous results on the separation power of these models (can a model tell the difference between two non-isomorphic graphs) and summarizes them, as well as new results they obtain, in prop. 3. Theorem 4 then connects these separation results to the approximation power of the model classes by a novel adaptation of a recently introduced generalization of the Stone-Weierstrass theorem.  The authors conclude the paper with an experimental section showing that folklore GNNs perform significantly better than other models in solving the quadratic assignment problem, which supports the theoretical claims\n\n\nStrong points:\n\nNew strong results: the paper closes the following knowledge gaps: (1) Folklore GNNs: k Folklore GNNs are as strong as k+1 WL, (2) Equivariant separation: connecting the separation power of equivariant (rather than invariant) models to their equivariant WL counterpart (3) Link separation results to approximation results: the authors suggest a generalization of the stone Weierstrass theorem and use it to show new approximation results.    \nThe paper succinctly summarizes the current results on the separation power of various GNN classes.\nThe paper Introduces new tools from approximation theory. These tools seem important and will be useful for future papers targeting the expressive power of invariant and equivariant deep models\nInteresting experimental results. Showing for the first time that more expressive models  (2-FWL) substantially outperform previous (2-WL) models on an important task.\n\nWeak points:\n\nNone. I have several comments on the exposition. See below. \nIn addition, I would like to ask the authors to explicitly and succinctly state their generalization of the Stone-Weierstrass theorem for general symmetries and deep models so it would be easy for future work to use it. Please do so for both invariant and equivariant case, and consider stating it in the paper itself.\n\n\n\nRecommendation: \n\nThis is a high-quality paper with several strong contributions as listed above. Most importantly, the paper proves several important results on the expressive power of GNNs and introduces useful mathematical tools that I am sure will be used by the community. Hence, I strongly encourage the acceptance of this paper (and would also recommend it as a spotlight/oral presentation).\n\nMinor comments:\n\n* Can the authors comment on \u201cBuilding powerful and equivariant graph neural networks with structural message-passing\u201d (Vignac et al. 2020). Specifically, Theorem 2 shows that their models can separate any pair of non-isomorphic graphs while using only second-order tensors\n* \u201cSo in terms of separating power, when restricted to tensors of order k, k-FGNN is the most powerful architecture. \u201d => Among the architectures considered in this paper.\n* \u201cFor regular-grid graphs, they match classical convolutional networks which by design can only approximate translation-invariant functions and hence have limited expressive power. In this paper, we focus instead on more expressive architectures.\u201d => I believe that mentioning the relation between GNNs and 2-WL here is a better example of the limited expressive power of GNNS.\n* \u201cNote that 2-WL solves k-cliques for k <= 6 (Fu\u0308rer (2017)) so that these networks are probably not comparable to 2-WL.\u201d => As far as I know 2WL cannot detect triangles. Please explain.\n* k-FWL: maybe write 2 FGNN layers explicitly and discuss matrix multiplication connection?\n* Appendix D is written really nicely with good examples\n* Algebras are closed under multiplication but sets of neural networks are not (although products can be easily approximated in most cases). I believe this gap is filled in Appendix D but would like to ask the authors to explain it in the main text.\n* The paper has several typos.\n", "title": "Interesting topic and multiple strong contributions", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "NNEPxGwAUg7": {"type": "rebuttal", "replyto": "PQwpJwfudCn", "comment": "We warmly thank the reviewer for his/her comments and constructive suggestions.\n- \u201cIn addition, I would like to ask the authors to explicitly and succinctly state their generalization of the Stone-Weierstrass theorem for general symmetries and deep models so it would be easy for future work to use it. Please do so for both invariant and equivariant case, and consider stating it in the paper itself.\u201d\nWe propose to add Theorems 31 & 32 in the paper itself. These are the results we directly applied to MGNNs, FGNNs and LGNNs to prove our main results.\nHowever, for the equivariant case, we use explicitly that the group of symmetries is $\\mathcal S_n$ to prove Theorem 32 using Theorem 20. So, for another group of symmetries, on has to come back to Thm. 20 and adapt our proof of Thm. 32. For now, the generalization of Thm. 32 to arbitrary groups of symmetries is left as future work. If you think this is relevant, we can try to add Thm. 20 too to the main paper (subject to space constraints).\n-\u201cCan the authors comment on \u201cBuilding powerful and equivariant graph neural networks with structural message-passing\u201d (Vignac et al. 2020). Specifically, Theorem 2 shows that their models can separate any pair of non-isomorphic graphs while using only second-order tensors\u201d\nThis work seems indeed very promising. However, concerning their Thm. 2, the authors themselves explain some shortcomings of their result  in their appendix B.4. \nWe quote their appendix B.4 : \u201cThe main assumption of our proof is that the aggregation and update functions can exactly compute any function of their input \u2014 this is impossible in practice. An extension of our argument to a universal approximation statement would entail substituting the aggregation and update functions by appropriate universal approximators. In particular, the aggregation function manipulates a set of n \u00d7 c matrices, which can be represented as a n \u00d7 n \u00d7 c tensor with some lines zeroed out. [...] Therefore, proving that a given parametrization of an SMP can be used to approximately reconstruct the adjacency matrix hinges on the identification of a simple universal approximator of equivariant functions on n \u00d7 n \u00d7 c tensors.\u201d\nEssentially, their proof relies on an universal equivariant approximator for tensors of order 2. However, no such tractable approximators have been found yet. The only way to do this for now is to use tensors of arbitrary high order. Therefore, to be effectively implemented as described in their Thm 2, tensors of arbitrary high order are needed.\nOn the contrary, in this work, we study tractable and practical architectures, which are not universal approximators and characterize their approximation power.\nNevertheless, we agree that his recent architecture is promising and look forward to a theoretical analysis of a tractable version of SMP. As explained in our response to Reviewer 4, once the separating power of such version of SMP is known, our theorems will apply to characterize its approximation power.\n-\u201cFor regular-grid graphs, they match classical convolutional networks which by design can only approximate translation-invariant functions and hence have limited expressive power. In this paper, we focus instead on more expressive architectures.\u201d => I believe that mentioning the relation between GNNs and 2-WL here is a better example of the limited expressive power of GNNS.\nHere we wanted to discuss the connection with GCN like (Defferrard et al., 2016). In this work, the architecture is stacking message-passing layers with pooling layers so that the separating power of such GCN is not known because the pooling layer does not fall into our framework.\n-\u201cNote that 2-WL solves k-cliques for k <= 6 (Fu\u0308rer (2017)) so that these networks are probably not comparable to 2-WL.\u201d => As far as I know 2WL cannot detect triangles. Please explain.\u201d\nThank you for highlighting our mistake here. F\u00fcrer (2017) indeed refers to 2-FWL, which is equivalent to 3-WL, and not 2-WL. We will correct it.\n-\"k-FWL: maybe write 2 FGNN layers explicitly and discuss matrix multiplication connection?\"\nWe agree that the definition of k-FGNN is much easier for the particular case k=2 and if place permit, we will add a discussion about the relation between 2-FGNN layers and matrix multiplication (feature wise).\n\u201cAlgebras are closed under multiplication but sets of neural networks are not (although products can be easily approximated in most cases). I believe this gap is filled in Appendix D but would like to ask the authors to explain it in the main text.\u201d\nIndeed, this is addressed in appendix D.9 precisely. The idea is that, in the theoretical analysis, it is almost as if the MLPs in the final layers of GNNs can be replaced by continuous functions, as MLPs are universal approximators. This new \u201cideal\u201d class of models is now stable by multiplication. We will add this clarification to the main paper.\n\nWe thank the reviewer for the other suggestions that we will take into account as best we can.\n", "title": "Author response to Reviewer 3"}, "-ascgBvSUw_": {"type": "rebuttal", "replyto": "PSP6pWkYfov", "comment": "We sincerely thank the reviewer for its constructive criticism. We will respond to each point in turn.\n- \u201ca) It is nice to establish and summarize a thorough comparison of the expressive powers of different GNN families as well as k-WL, and also to incorporate not only invariant but also equivariant GNNs into consideration. However, among the results in Section 4.2, the only novel one is the equivalence between k-FGNN and (k+1)-WL, which, one may argue, is also not surprising considering the equivalence between k-FWL and (k+1)-WL. The value of 2-FGNN has also been demonstrated by the work of [1].\u201d\nWe respectfully disagree on this point: to the best of our knowledge, the separating power of GNN was only studied in the invariant case.. In Proposition 3, only the left column of results (4) (5) (6) were known, we stated them for completeness. We do not know any previous result about separating power for equivariant GNNs.\nMoreover, studying the separating powers of GNNs is not the main objective of our work, which is to characterize the approximation power of realistic GNN classes.\nWe agree that the empirical value of 2-FGNN was already shown in [1] but only for the invariant case. We explicitly refer to [1] for experimental results in the invariant case (graph classification task, graph regression\u2026), see the \u201cEmpirical results for the Quadratic Assignment Problem\u201d paragraph in the Introduction. In our paper, we only look at experimental results validating 2-FGNN in the equivariant case.\n- \u201cb) The overall framework for the comparison of expressive power as well as function approximation resembles that of [2], which characterizes the expressive power of GNNs and WL tests by the sigma algebra they induce on the space of graphs, and also establishes an equivalence between the ability to distinguish non-isomorphic graphs and the ability to approximate invariant functions. Hence, the authors may want to elaborate more on the connection between this work and [2].\u201d\nWe agree that our results resemble that of [2], indeed as stated in our paper, our results allow us to recover the theoretical results in [2]. However, we believe results in [2] are much more limited than ours. First, [2] only deals with the invariant case whereas we deal with the equivariant case too which is crucial for our applications with Graph alignment. Second, theoretical results in [2] only deal with universality results, which are not relevant for practical GNN classes, whereas we derive approximation results even for architectures that are not universal (i.e. dealing with fixed order tensors).\nFinally, the framework of representation power based on sigma-algebra they introduce only makes sense for finite input spaces, while the notion of discriminatory power is much more flexible. However, we agree that,  when the input space $X$ is finite, these two notions can be linked. The separating power of a class of function $\\mathcal F$ indeed characterizes the sigma algebra generated by $\\mathcal F$, denoted by $\\sigma(\\mathcal F)$ : one can shows that, $\\sigma(\\mathcal F) = \\sigma(X/\\rho(\\mathcal F))$, where $\\sigma(X/\\rho(\\mathcal F))$ is to be understood as in [2], i.e. as the sigma algebra generated by the equivalence classes of $\\rho(\\mathcal F)$. If the reviewers think this remark is relevant we can add this remark to the appendix.\n  (Note that, here, \u201csigma algebra\u201d has nothing to do with the term \u201calgebra\u201d used in our work : here, a sigma algebra refers to the measure theory concept. )\n- \u201cThe authors claim that 3-LGNN or 2-FGNN can count the number of 6-cycles, which is an improvement upon the result in [3], which implies that 6-LGNN can count 6-cycles. Technically, however, it is only known (by [4], to my knowledge) that 3-WL can count the number of times 6-cycles appear as subgraphs of a graph, but not induced subgraphs, whereas [3] is concerned with the counting of both subgraphs and induced subgraphs. \u201d\nThank you for pointing out this imprecision, we will clarify this.\n\nReferences:\n[1] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks.\n[2] Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph isomorphism testing and function approximation with GNNs.\n[3] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures?\n[4] Martin Fu \u0308rer. On the Combinatorial Power of the Weisfeiler-Lehman Algorithm.\n", "title": "Author response to Reviewer 2"}, "e3LXpVFuKQN": {"type": "rebuttal", "replyto": "EbmT1m2UmY", "comment": "We sincerely thank the reviewer for the constructive comments and criticism. We will respond to each point in turn.\n\n- \u201cIn one aspect the notation could improve: \\subset should be replaced by \\subseteq to avoid confusion, and be more in line with the use of \\subsetneq. I was not sure what \\subseteq meant until I saw \\subsetneq.\u201c\nThank you for your suggestion, we will update our notations.\n- \u201cIt does not even discuss alternative approaches using group averaging. I think the paper would significantly broaden its audience if there is a comparison with novel k-ary group averaging approaches (e.g., Chen et al., 2020, Murphy et al. 2019) in the empirical section.\u201d\nFor our theoretical results, we decided to focus on what we consider to be the most studied architectures theoretically: MGNN, LGNN and FGNN. We agree that the relational pooling introduced in Murphy et al. and studied in Chen et al. is a very interesting idea but to the best of our knowledge, this technique is only valid for invariant GNNs. We do not see any simple extension of this idea to the equivariant case which is the main focus of our empirical study. We will add this remark in our paper.\n-\"The work is of interest to a narrow sub-community working on k-FGNN and k-LGNN. The MGNN results were known but recovered here in a different way (i.e., there is value). The k-FGNN and k-LGNN results are new but also very narrow due to the sub-community size.\nThe empirical results essentially do not compare with any other alternative approach. \"\nFor our empirical results, we did compare to MGNN, and two \u2018non learning\u2019 algorithms based on SDP and spectral methods. \n- \"Until the community is convinced that k-FGNN and k-LGNN can empirically compete with other approaches (e.g., Morris et al. 2019, Permutation group averages like  Chen et al. 2020, Murphy et al. 2019, among others), these methods will not see broad applicability.\"\nFor machine learning task requiring an invariant representation of the graph (like graph classification, graph regression \u2026), we agree with the reviewer that FGNNs will probably be useful only in very particular cases as this is an already \u2018mature\u2019 field where MGNNs have been customized for specific tasks/datasets and are both computationally efficient and state of the art. However, we believe the situation is rather different for machine learning tasks requiring an equivariant representation of the graph, i.e. a node embedding as in our QAP application. For such tasks, we believe 2-FGNN can empirically compete and even outperform standard MGNN. We are currently working on more applications and got encouraging preliminary results in this direction.\n- \u201cThe authors seem unfamiliar with group average approaches that increase the expressiveness of MGNNs. These can also be universal approximations. Permutation group averages like  Chen et al. 2020, Murphy et al. 2019, among others, have increased expressiveness of MGNN using the same MGNN architecture. \u201c\nRegarding Relational Pooling (RP) and our theoretical analysis : \nAs far as we know, the ideal RP, which is a universal approximator, cannot be used for large graph due to its complexity of $O(|V|!)$.\nRegarding the other classes of RP GNN, our theorems in the invariant case can be almost immediately applied to them to characterize their approximation power, but the result involve their separating powers, which have not been studied much yet. Consider, for instance, $k$-ary RP by Murphy et al. 2019, with an additional final MLP. This is an invariant class of GNNs which actually satisfies the assumptions of Theorem 31. Therefore, the set of function these GNNs can approximate is, (informally),\n$$\\{f \\in \\mathcal{C}_I(X, Y): \\rho(k\\text{-RP-GNN}) \\subset \\rho(f)\\}$$\nHowever, $\\rho(k\\text{-RP-GNN})$ is not yet known. Thus, the study of the separating power of these classes of GNN is still an interesting question for the community, but, once it is known, our results will automatically give their approximation power.\nMoreover, in this work we were interested in theoretically studying both invariant and equivariant GNN, especially since QAP needs an equivariant architecture. As far as we know, RP-based architecture have not yet been extended to the equivariant case.\nFurthermore, we do not see how a group average approach combined with Loukas (2019) would fix the issue. \n\nChen, Zhengdao, Lei Chen, Soledad Villar, and Joan Bruna. \"Can graph neural networks count substructures?.\" NeurIPS (2020).\nMurphy, Ryan L., Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. \"Relational pooling for graph representations.\" ICLR (2019).\n", "title": "Author response to Reviewer 1"}, "jVBV_DJnICn": {"type": "rebuttal", "replyto": "PlNBGug-HLp", "comment": "We thank the reviewer for the constructive comments and are grateful for the numerous and helpful suggestions. We will respond to each point in turn.\n- \u201cThe main paper only sets up the problem and states the main results, while all theoretical contributions are done in the appendix. The main paper would be more self-contained if some more intuition for the proofs was given in the main paper.\u201d\nIf accepted, we will use the additional page for the camera ready version of the paper to include part of the theoretical contributions with examples that are now in the appendix.\nFollowing the demands of Reviewer 3, we propose to add Theorems 31 & 32 in the paper itself with some of the explanations in D. These are the results we directly applied to MGNNs, FGNNs and LGNNs to prove our main results.\n- \u201cThe experiments seem to not compare to LGNN. Adding this comparison would help making an empirical argument for why FGNN is best.\u201d\nMGNNs seem much more popular than LGNNs in practice.  They have the same expressive power as MGNN when dealing with tensors of order 2, in particular, they are useless for our graph alignment problem with regular graphs. Note also that we compare our implementation of FGNN directly with existing results in the literature with MGNN, SDP and spectral algorithms. This makes sure that alternative implementations have been carefully designed. \n    Moreover, (Maron 2019) empirically compared the equivariant versions of LGNN and FGNN and found FGNN performed better.\n    Therefore, we believe that for our QAP problem, LGNN will have similar results as MGNN ( we are ready to implement LGNN if required).\n- \u201cIn several instances, the authors write \u201ca compact\u201d without a noun. Is this conventional language?\u201d\n    After looking it up, it seems to be an abuse of language. We will add \u201csubset\u201d or \u201cset\u201d for clarity.\n- \u201cBelow Eq 2, there is an F without subscript. Is this the same for all layers?\u201d\nNo it depends on the layer, we will fix this.\n- \u201cIn App C.3, in the second line of the equation, are there suffices missing on the left-hand side?\u201d\nIndeed, subscripts are missing. The left-hand side of the second equation at the bottom of page 18 should be $I^k(G)_{\\mathbf{i}, r, s, e+2}$. This actually makes us realize that $\\mathbb{F}_1$ defined above should be $\\mathbb{R}^{k^2 \\times (e+2)}$. We will improve the clarity of this paragraph by harmonizing the notations with Lemma 8, i.e. by using $p_0$ instead of $e+1$.\n- \u201cApp D.3, proof of Corollary 15: define when a class separates a set around \u201cclearly separates X_\\mathcal{F}\u201d. Presumably means same as separates points?\u201d\nIndeed, it means that for any $x, y \\in X_{\\mathcal{F}}$ there exists $f \\in \\hat{\\mathcal{F}}$ such that $f(x) \\neq f(y)$. We will clarify this at the beginning of appendix D.\n- \u201cEq 17, Define S \\cdot \\mathcal{F}, presumably as the scalar-vector product of the outputs of the functions? \u201c\nIndeed, we will clarify in D.1\n- \u201cCorollary 19, assumption 3, what does \u201cpairwise distinct coordinates\u201d mean?\u201d\n$f(x)$ is a vector $\\mathbb R^p$, so, by \u201cpairwise distinct coordinates\u201d we mean that, for any indices $i$, $j$ with $i \\neq j$, the coordinates $i$ and $j$ of $f$ differ, i.e. $f(x)_i \\neq f(x)_j$.\n\nWe warmly thank the reviewer for listing  the other typos and suggestions.\n\n\n", "title": "Author response to Reviewer 4"}}}