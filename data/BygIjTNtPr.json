{"paper": {"title": "ODE Analysis of Stochastic Gradient Methods with Optimism and Anchoring  for Minimax Problems and GANs", "authors": ["Ernest K. Ryu", "Kun Yuan", "Wotao Yin"], "authorids": ["eryu@math.ucla.edu", "kunyuan@ucla.edu", "wotaoyin@math.ucla.edu"], "summary": "Convergence proof of stochastic sub-gradients method and variations on convex-concave minimax problems", "abstract": "Despite remarkable empirical success, the training dynamics of generative adversarial networks (GAN), which involves solving a minimax game using stochastic gradients, is still poorly understood. In this work, we analyze last-iterate convergence of simultaneous gradient descent (simGD) and its variants under the assumption of convex-concavity, guided by a continuous-time analysis with differential equations. First, we show that simGD, as is, converges with stochastic sub-gradients under strict convexity in the primal variable. Second, we generalize optimistic simGD to accommodate an optimism rate separate from the learning rate and show its convergence with full gradients. Finally, we present anchored simGD, a new method, and show convergence with stochastic subgradients.", "keywords": ["GAN", "minimax problems", "stochastic gradients"]}, "meta": {"decision": "Reject", "comment": "Motivated by GANs, the authors study the convergence of stochastic subgradient                                                     \ndescent on convex-concave minimax games.                                                                                           \nThey introduced an improved \"anchored\" SGD variant, that provably converges                                                        \nunder milder assumptions that the base algorithm.                                                                                  \nIt is applied to training GANs on MNIST and CIFAR-10, partially showing                                                            \nimprovements over alternative training methods.                                                                                    \n                                                                                                                                   \nA main point of criticism that the reviewers identify is the strength of the                                                       \nassumptions needed for the analysis.                                                                                               \nFurthermore, the experimental results were deemed weak as the reported scores                                                      \nare far away from the SOTA, and only simple baselines were compared against.  "}, "review": {"BJezYAMhsH": {"type": "rebuttal", "replyto": "BygIjTNtPr", "comment": "This comment is in response to comments from Reviewers #2 and #3.\n\nIn our analysis, we assume convex-concavity, even though GANs are not convex-concave. However, prior theoretical papers make assumptions that are equally or further unrealistic, as we discuss in the introduction. Common assumptions are:\n - GAN is quadratic in the discriminator and generator parameters.\n - Use full gradient, not stochastic gradients.\n(Examples of recent published work under such assumptions include: Mescheder et al., ICML, 2019; Daskalakis and Panageas, NeurIPS, 2018; Gidel et al., AISTATS, 2019.) Compared to these assumptions, convex-concavity is no more unrealistic. We believe the analyses under the different (unrealistic) assumptions provide complementary insights into the training dynamics of GANs.\n\nIn the past, the analysis of SGD for minimization in ML initially focused on the convex case. The highly cited AdaGrad paper by Duchi et al. in 2011 is one such example\nhttp://jmlr.org/papers/v12/duchi11a.html\nAs our understanding of SGD matured, attention shifted towards more realistic non-convex ML minimization setups. We believe the analysis of SGD-type methods for minimax problems will take a similar course. Currently, the minimax theory relies on unrealistic assumptions, but once our understanding matures, the field will be able to move on to more realistic setups.\n\nWe believe the validity of the convex-concavity assumption should be judged in comparison to the prior theoretical works, rather than how well the assumption matches the empirical practice. Our assumption of convex-concave (but non-differentiable and stochastic gradients) is very reasonable compared to the assumptions used in prior theoretical works.\n", "title": "Convex-concavity assumption is, comparatively, not unreasonable"}, "BkeJ3AM2iS": {"type": "rebuttal", "replyto": "Byx_SfI1qS", "comment": "We thank Reviewer #3 for the thoughtful feedback.\n\nPlease see our other comment on the convex-concavity assumption.\n\nWe agree the experiments are at best suggestive, and far from the state-of-the-art since it is a simple architecture without any modern techniques, and we appreciate that Reviewer #3 is recognizing that the main contribution of this paper is theoretical. If the reviewers believe the experiments are a distraction, we are happy to remove them. We ask the reviewers to consider the theoretical contribution in their decision.\n\nIn the revision, we have clarified which training method generated Figure 3.\n\nWe will make sure to exclude acknowledgments in future double-blind submissions. We apologize for the mistake.", "title": "Thank you for the thoughtful comments"}, "ryxY5AGhoS": {"type": "rebuttal", "replyto": "H1e3787jYB", "comment": "We agree with Reviewer #2 that the main contribution of this work is the theory, while the experiments are at best suggestive. (Reviewer #2 offers COLT, the more theoretical venue, as a more appropriate venue.)\n\nWe firmly disagree with the statement that our theoretical contribution is not suited for ICLR. We cite several ICLR papers with similar theoretical contributions and simple experiments:\n- Daskalakis et al., ICLR, 2018.\n- Gidel et al., ICLR, 2019.\n- Mertikopoulos et al., ICLR, 2019.\nWe decided to submit to ICLR as our paper continues this line of work by offering stronger theoretical results. If the reviewers believe the experiments are a distraction, we are happy to remove them.\n\nWe ask the following question to Reviewer #2: Are theory papers without a significant experimental component welcome at ICLR?\nWe would appreciate a clear statement, even if we disagree. Clarifying the basis of the decision on the public forum will be of value to the ML community, as it will serve as a guideline for future authors in determining whether ICLR is an appropriate venue for their theoretical work.\n\nFinally, please see our other comment on the convex-concavity assumption.", "title": "Theory papers do not belong in ICLR?"}, "S1l_PRfnsr": {"type": "rebuttal", "replyto": "BJx2N57Ctr", "comment": "We thank Reviewer #1 for the thoughtful feedback. In our revision, we have followed Reviewer #1's suggestions to improve the presentation of the paper. We respond to the individual comments below.\n\n\"[Section 3.1 and 4.1 are] basically the proof of the convergence ... Stating the result before the proof would help the reader to understand where the authors want to go.\"\n\nThank you for the suggestion. We have updated the structure to improve the readability. We avoided saying \"Theorem\" or \"convergence proof\"  to avoid discussing the existence and differentiability of solutions to the continuous-time ODEs. (We do not see such rigorous treatment of the continuous-time setup to be important since the continuous-time analysis serves merely as an illustration and inspiration for the discrete-time algorithm.)\n\n\n\"For the stochastic version of SimGD-A a new parameter is introduced without any comment or description on why it is necessary. Question) Is the condition only necessary for the proof? Does work in practice? If no, what is the best value for?\"\n\nDue to page limitations, we deferred the discussion on this matter to the appendix. The proof requires $\\varepsilon>0$, but we believe this is an artifact of the proof. In particular, we conjecture that Lemma 17 holds with $o(s/\\tau)$ rather than $\\mathcal{O}(s/\\tau)$, and, if so, it is possible to establish convergence with $\\varepsilon=0$. In practice, we observed that $\\varepsilon=0$ to work well.\n\n\"A FID above 40 for MNIST is very far from standard results.\"\n\nIndeed, our experiments are far from the state-of-the-art since it is a simple architecture without any modern techniques (e.g. spectral normalization). Our experiments only demonstrate that anchoring helps in some setups, not necessarily state-of-the-art setups. If the reviewers believe the experiments are a distraction, we are happy to remove them. We ask the reviewers to consider the theoretical contribution in their decision.\n\n\"Question) When you cite Lassale Principle you mention that if $z_\\infty$ is a limit point of $z(t)$ then starting at $z_\\infty$, you stay at a constant distance to $z_*$. But in the bilinear example you give any point in the circle is not a limit point (size no dynamics converge to it). I guess when you said limit point you meant adherent point?\"\n\nThank you for pointing this out. Indeed, \"limit point\" is not the correct word. We should have said \"adherent point\" or, since we are talking about sequences,  \"cluster point\" as defined in\nhttps://en.wikipedia.org/wiki/Limit_point#For_sequences_and_nets", "title": "Thank you for the thoughtful comments"}, "H1e3787jYB": {"type": "review", "replyto": "BygIjTNtPr", "review": "The paper studies last-iterate convergence of simultaneous gradient descent and related algorithms in (convex-concave) GANs.\n\nThe experiments are very weak. Figure 4 shows that anchored Adam outperforms Adam and optimistic Adam in terms of FID on MNIST, but not CIFAR-10. \n\nThe authors cite many of the vast number of algorithms that have been proposed to train GANs (Heusel, Mescheder, Gidel, Gemp, and on and on ... and on\u2026) and discuss some of them in the analysis. However, when it comes to experiments, they only compare against vanilla Adam (!) and Optimism. I would summarize the experiments as *suggesting* that anchoring doesn\u2019t hurt on MNIST or CIFAR-10. The paper doesn\u2019t tune beta and gamma, so it\u2019s hard to be sure. \n\nThe motivation for the paper is GANs, but GANs are not convex-concave. The analysis is therefore not directly relevant. From the experiments, it is not clear *at all* whether anchored Adam is *actually* an improvement in practice over any of the alternative algorithms that the authors discuss or cite. \n\nIn short, the contribution is unclear. The analysis is better suited to a venue like COLT. To be a reasonable ICLR submission, the paper needs to compare against more baselines at a bare minimum.\n", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 3}, "BJx2N57Ctr": {"type": "review", "replyto": "BygIjTNtPr", "review": "*Summary*\nThis paper provides the analysis of three algorithms in the context of minmax convex concave games: Simultaneous stochastic subgradient method, Simultaneous gradient with optimism and Simultaneous gradient with anchoring. These three algorithm are first analysed in continuous time with an ODE perspective and then leverage these intuitions and techniques to analyze the discrete time versions.\n\nI think that these contributions are of interest of ICLR community, however, I have some concerns regarding the presentations of the results. \n\n*Decision*\nI vote for a weak accept that could move to an accept if the authors improve the presentation of the paper:\nThe paragraph \u201cRegularized dynamics and convergence\u2019 in Section 3.1 in quite hard to follow. This subsection is basically the proof of the convergence of the continuous version of  GD-O. Stating the result before the proof would help the reader to understand where the authors want to go. \nVery same point for The subsection 4.1\nFor the stochastic version of SimGD-A a new parameter $\\epsilon$ is introduced without any comment or description on why it is necessary. \nA FID above 40 for MNIST is very far from standard results (that are below 1). Thus I am not convinced by the practical advantage of Anchored Adam on these models that have performances results very far from the standard ones.  (for instance between 20 and 25 for CIFAR10 is very reasonable). It is maybe because you do not use convolutional layers in your architecture. I think that using a DGAN architecture (for instance the one from the pytorch tutorial https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) would give the expected results (i.e. FID close to 0).\n\n*Questions*\n\n- When you cite Lassale Principle you mention that if $z_\\infty$ is a limit point of $z(t)$ then starting at $z_\\infty$, you stay at a constant distance to $z_*$. But in the bilinear example you give any point in the circle is not a limit point (size no dynamics converge to it). I guess when you said limit point you meant adherent point ?\n- Is the condition $\\epsilon$ only necessary for the proof ? Does $\\epsilon =1$ work in practice ? \nIf no, what is the best value for $\\epsilon$ ?\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 4}, "Byx_SfI1qS": {"type": "review", "replyto": "BygIjTNtPr", "review": "This paper analyzes the dynamics of stochastic gradient descent when applied to convex-concave games (motivated by the game used to train GANs, which is typically not convex-concave), as well as the previously proposed GD with optimism and a new anchored GD algorithm that provably converges under weaker assumptions than SGD or SGD with optimism.\n\nThis seems like a nice contribution to the GAN theory literature, and the anchored SSSGD algorithm might well have significant practical value (although more thorough experiments would be needed to make this claim). As such I recommend acceptance.\n\nThe obvious critique to raise of this kind of work is that the GAN problems that motivate it are clearly not convex-concave, and so it is unclear how or whether the results can or should inform practice. The simplest way to make the case for that kind of relevance is empirically, so I'd recommend that the authors consider what kinds of GAN experiments would support (or falsify) the claim that their theoretical results have some hope of generalizing to the non-convex-concave setting. Figure 4 is suggestive, but it doesn't say much except that anchored Adam might occasionally be a good choice (and sometimes isn't). The samples in Figure 3 are pretty far from the state of the art, and in any case Figure 3 doesn't even say which training method generated them.\n\nFinally, PLEASE don't include acknowledgments in a paper that's under double-blind review; it compromises your anonymity, which in principle could be grounds for a desk rejection.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}}}