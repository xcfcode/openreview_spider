{"paper": {"title": "Out-of-distribution Prediction with Invariant Risk Minimization: The Limitation and An Effective Fix", "authors": ["Ruocheng Guo", "Pengchuan Zhang", "Hao Liu", "Emre Kiciman"], "authorids": ["~Ruocheng_Guo1", "~Pengchuan_Zhang1", "~Hao_Liu2", "~Emre_Kiciman1"], "summary": "We find a limitation of Invariant Risk Minimization under a specific type of strong spuriousness and propose an effective fix for Out-of-distribution Prediction.", "abstract": "This work considers the out-of-distribution (OOD) prediction problem where (1)~the training data are from multiple domains and (2)~the test domain is unseen in the training. DNNs fail in OOD prediction because they are prone to pick up spurious correlations. Recently, Invariant Risk Minimization (IRM) is proposed to address this issue. Its effectiveness has been demonstrated in the colored MNIST experiment. Nevertheless, we find that the performance of IRM can be dramatically degraded under \\emph{strong $\\Lambda$ spuriousness} -- when the spurious correlation between the spurious features and the class label is strong due to the strong causal influence of their common cause, the domain label, on both of them (see Fig. 1). In this work, we try to answer the questions: why does IRM fail in the aforementioned setting? Why does IRM work for the original colored MNIST dataset? Then, we propose a simple and effective approach to fix the problem of IRM. We combine IRM with conditional distribution matching to avoid a specific type of spurious correlation under strong $\\Lambda$ spuriousness. Empirically, we design a series of semi synthetic datasets -- the colored MNIST plus, which exposes the problems of IRM and demonstrates the efficacy of the proposed method.", "keywords": ["Invariant Risk Minimization", "Causal Machine Learning", "Out-of-distribution Prediction"]}, "meta": {"decision": "Reject", "comment": "Loosely, while IRM aims to find a feature mapping Phi s.t. response Y given Phi(X) is independent of the environment variables E, they suggest that when E is strongly correlated with Y, then it is possible for Phi obtained via IRM to involve environment variables. They motivate this by suggesting that if there exists a feature mapping Phi(X) = E, it would satisfy the IRM aim, but that this is undesirable.\n\nThey suggest instead requiring Phi(X)|Y being invariant to the environment.\n\nThe reviewers bring up a couple of concerns. The first is that it is not clear outside some simple examples when Y given Phi(X) being independent of E does not suffice. The second is that the authors also do not empirically validate their fix outside a single simple dataset. Moreover, what are the pitfalls of having Phi(X) given Y being independent of E?\n\nOverall, this is an interesting kernel of an idea; it just needs to be fleshed out a bit more.\n"}, "review": {"2J3rKotCOx": {"type": "review", "replyto": "q-qxdClTs0d", "review": "### Summary of Paper\n\nThis paper identifies and tries to fix a limitation of the recent work of Invariant Risk Minimization (Arjovsky et al., '19). IRM is a solution framework for the OoD prediction problem, where one has to learn a classifier based on data from multiple domains, hoping to generalize to unseen domains. \n\n1. This paper extends the Colored-MNIST dataset (where MNIST images are spurious colored according to the label) to a new and more difficult dataset called CMNIST+ where they empirically show that IRM fails to generalize to the new domain. The key idea behind the construction of the CMNIST+ dataset is the introduction of a correlation between the spurious features and the _domain_ label during training time (besides the training-time correlation between the spurious features and the class label that is already there in Colored-MNIST). This spurious-feature-domain-label correlation can be introduced by making Domain 1 largely contain points of class 1 and Domain 2 largely contain class 2. They call this sort of correlation as \"$\\Lambda$-spuriousness\". \n\n2. The paper provides an intuitive argument for why IRM does not work on this dataset: the IRM constraint is equivalent to \"class label independent of domain label given feature representation\", such a constraint does not preclude the classifier from learning \"feature representation = domain label\". Such a classifier however would not generalize well to an unseen domain where the domain label is not correlated with the class label. \n\n3. Finally, the paper proposes a fix for IRM which essentially adds a \"conditional distribution matching\" constraint to the IRM constraint. This constraint forces the distribution of the feature representation for any class label to be invariant across domains. By implementing this through (a) an MMD approach and (b) an adversarial learning approach, they show a 10% improvement in OoD accuracy on CMNIST+.\n\n### Strengths\n\n1. The paper is strong on novelty: the problem identified, the explanation provided, the dataset proposed, and the solution proposed are all novel. Overall, the paper piqued my curiosity and I enjoyed reading it.\n\n2. The problem of OoD prediction is practically important. Furthermore, this paper exposes the limitations of an existing solution under a very natural kind of spurious correlation that could occur in real life (i.e., domain-class correlation). \n\n3. The paper also provides a synthetic dataset that will be quite valuable to future work that aims to develop better OoD prediction algorithms. \n\n4. The \"failure\" dataset and the solution proposed are all founded on a solid, intuitive argument. \n\n\n### Weaknesses + important clarification questions\n\n5. I think the paper will benefit greatly from at least one other empirical example both in terms of showing failure of IRM and in terms of showing that IRM-MMD/ACDM improves. This could either be on a synthetic dataset similar to (but not) CMNIST+  (maybe MNIST but with some other spurious feature; or maybe some other dataset with the same spurious feature). Even better, it'd be great (but not absolutely necessary) to verify the performance of IRM-MMD/ACDM on a practical benchmark. \n\n\n6.  I was confused about the definition of $\\Lambda$-spuriousness. The introduction defines this to be the existence of a correlation between label and color during training. Isn't this the same correlation that exists in CMNIST? Is this a typo? I suspect that the $\\Lambda$-spuriousness refers to strong correlation between the class label, the domain label and the spurious features. Or did I completely misunderstand this?\n\n7. Could you explain why you had to resort to using three channels/colors as against just two like in CMNIST? I wonder if this is the point that was addressed by the following line in the paper:\n> The Colored MNIST (CMNIST) dataset cannot expose the limitation of IRM under strong \u039b spuriousness. This is because its two training domains are quite similar.\n\n**Update:**   You provided an example 2-color dataset to argue why you can't really see whether IRM fails in the test domain. However, this example doesn't seem to be the correct analog of the example 3-color dataset in your paper? In your 2-color example, domain 1 is mostly Y=1, and most of those datapoints are colored G. Domain 2 is mostly Y=0 and most of those datapoints are colored G too. But the analog of the 3-color dataset would be one where in domain 2 most datapoints have Y=0 and those datapoints are mostly colored B (or R, but not G). Then, consider a test domain where you've an equal proportion of Y=0 and Y=1, but all Y=0 are colored G and Y=1 are colored by  B.  It seems like under this case, if IRM were to use the color, it would have a poorer test accuracy than 0.5. Perhaps I'm missing something here. Nevertheless, it seems like considering a dataset like this, and/or fleshing out a corresponding Table 1 for such a dataset would be critical to substantiate this argument. \n\n\n\n \n\n8. The argument that label balancing does not fix IRM's issue is a crucial argument to justify the fix given here. But I would have appreciated a bit more elaboration on this. The text says \"Theoretical analysis shows that this [label balancing] is an invalid solution\". Could you explain how I can infer this from Table 2? Which column here would correspond to the performance of IRM under label balancing, and why?\n\n\n### Overall opinion\nThe paper identifies a novel gap in an existing algorithm for an important problem, provides an intuitive explanation as to why that gap exists, and also proceeds to provide a reasonable, intuitively-grounded fix for it. Overall, this makes a complete, coherent paper worth publishing.\n\n#### Minor suggestions\n- For completeness, it would be nice to provide a concrete argument and discussion (in the appendix) for the connection between the original IRM constraint and the constraint ``\"Y \\condindep E | F(X)\"\n- In Page 3, there is a footnote against the symbol \"E\" which might be better positioned elsewhere.\n- In Fig 2 (c), $K_{IRM} = 0$ could be misleading (since it can be interpreted as adding the constraint right from the first step). Perhaps $K_{IRM} = -1$ or $K_{IRM} = \\infty$ would be more appropriate.\n\n\n#### References\n\nMartin Arjovsky, Le \u0301on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\narXiv preprint arXiv:1907.02893, 2019.\n\n\n**Update**: Thanks to the authors for clarifying most of my clarification questions.  I'm not sure I was able to fully follow your argument about why these results can't be adapted to a two-color dataset (see above), but to indicate that you've clarified many of my questions I've increased my confidence score to a 4.  Good luck to the authors.\n\n\n**Further updates**: I'd like to elaborate on my thoughts a bit more with the hope that the authors may find it useful for future versions of the paper. \n\n- First, I really appreciate the authors for performing additional experiments with EIIL during the response phase. I wish to emphasize that, after a long discussion with R3, I've some strong disagreements with their review regarding the \"simple fix\":\n   -   I personally think EIIL is out-of-scope as it is a recent algorithm. It doesn't sound like a \"simple fix\" to me. However, it's great that the authors were able to show that their algorithm works better. \n   -  I don't think pooling all datapoints and splitting it will work. You'll end up with a dataset with label-color correlation in both domains, and both domains will be identical. So IRM won't work here.\n  - One can always come up with some hacks like \"pool all environments and carefully split them back\" that work under the assumption that there's $\\Lambda$-spurious correlation. But those hacks would be sub-optimal if there were no $\\Lambda$-spurious correlation.  Therefore, this is an unfairly powerful \"overfit\" hack, and does not make a good baseline. You want an elegant solution that works whether or not there's $\\Lambda$-spurious correlation as you won't know whether that sort of a spurious correlation exists in practice.\n  - As a side note, in light of the above point, I think it's important that the authors also demonstrate that the CDM constraint added preserves the performance of IRM on the original CMNIST dataset. \n  - Hopefully the authors can keep the EIIL results for future versions of the paper as it only makes the paper stronger.\n\n- I don't think the paper should be heavily penalized for the lack of a realistic dataset, because it's hard to verify $\\Lambda$ spuriousness on realistic benchmarks. However:\n  - I'd strongly encourage that you consider trying similar experiments on a dataset like say Rotated-MNIST (or Rotated-MNIST+ to be more precise, if at all possible). \n  - Even better, you could consider whether similar experiments can be done with Celeb-A where you have access to image attributes like hair color etc., (See Fig 2 https://arxiv.org/abs/2005.04345) and you could try creating different environments by sampling differently in each.\n  - If you think that's it's impossible to create a $\\Lambda$-spurious dataset, you might want to explain in future versions of the paper as to why that's not possible.\n\n- I understand R3's main concern which is that the algorithm in this paper requires that the distribution of the causal features $X_{causal} | Y$ to be the similar across all environments. It seems like IRM doesn't expect this sort of invariance, while algorithms prior to IRM do require something of this sort (including CDM, DANN etc.,). I think one actionable way to address this concern would be to show that there are datasets where IRM + CDM does better than CDM (just like how IRM+CDM does better than IRM in CMNIST+). This way we can see why combining IRM and CDM offers something unique.\n\n\n- Finally, I want to appreciate your efforts in trying to clarify all the reviewers' concerns (at least I found them helpful) and also to update the paper accordingly, add experiments etc., ", "title": "Valuable study of and improvement over invariant risk minimization", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "-CseCzHzPB": {"type": "review", "replyto": "q-qxdClTs0d", "review": "Summary:\nIn this work, the authors focus on the out-of-distribution generalization problem. The input is dataset from multiple environments and the goal is to learn a model that generalizes well to an unseen test environment. The work is based on recent line of works on invariant risk minimization (IRM) (Arjovsky et al.). The authors show that under a certain type of structure for the generative model, where the domain/environment label itself has a strong correlation with the spurious factors and the target label, IRM fails. The authors propose an extension of the colored MNIST dataset to highlight this problem. Finally, the authors build a method that works better than IRM on the extension of colored MNIST dataset. \n\nPros:\n I appreciate the authors have tried to highlight how the IRM directly applied to datasets from multiple environments will not always work and one has to be careful about the environment induced correlations themselves. \n\nCons: \nI divide my concerns into different subsections below. \n\na)\tIncorrect connection between IRM (Arjovsky et al.) and conditional independence made by the authors: \n\nConditional independence (Y \\perp E | F(X)) is a necessary condition but not sufficient for the theory of IRM to work.  Therefore, analyzing any F that satisfies this property is not sufficient. \nSuppose we have two training environments, E=1 and E=2.  Asssume that we are only interested in binary classification for now. The main condition that is assumed in Arjovsky et al. (Page 9 Definition 7) for the success of IRM is that there exists a representation F*(X) such that \n\nP(Y|F*(X),E=0) = P(Y|F*(X), E=1) = P(Y|F*(X))\n\nThe above condition implies that Y \\perp E | F*(X). In the above expression equating conditionals, there is already an assumption made about F*(X), which is that for both environments E=0 and E=1, the support of F*(X)|E=0 and support of F*(X)|E=1 are equal. Suppose the supports were not equal, then the conditionals can only be equated over the intersection of the supports.  \nIn the driving example used in the paper, i.e., F*(X)=E, the support of the two conditionals F*(X)|E=0 -->E=0 and F*(X)|E=1-->E=1 do not intersect.  Therefore, what authors claim is a problem with IRM is not really a problem but a data generating environment for which the theory of IRM is not guaranteed to be successful. The right claim to make is that CMNIST+ does not satisfy the assumptions IRM makes for the method to be successful. However, this is easy to fix as I explain soon. \n\nBefore moving to the next section, I would like to also make another important remark. The authors investigate any predictor that satisfies the conditional independence condition. This is also not correct because IRM and other IRM based methods select one of the invariant predictors and not all (thus there can always be bad invariant predictors, which does not mean that they will be selected). For a complete characterization of invariant predictors in terms of conditional independences please refer to Koyama et al.\n\nUpdate post discussions: This point a) was corrected by the authors.\n\nb)   Why does IRM not work on the CMNIST+?\n\nWe now turn to providing the explanation why IRM did not work on CMNIST+ as the explanation provided by the authors is not accurate. As we explained in the last section, F(X)=E is a representation that does not satisfy the criterion that the theory of IRM requires. One intuitive way to think is that the success of IRM assumes that representation F that we search over have an overlapping support across the environments. The IRM optimization procedure fails because it does not enforce this assumption in any way and F(X)=E can lead to a better predictor than F(X)=S, where S is the true causal feature. \nIn CMNIST+, the authors have created two environments, where the environment label itself is strongly correlated with the label. Say in E=0, the majority of the labels are 0, and in E=1 the majority of the labels are 1.  Suppose the IRM optimization (Arjovsky et al.) is given a representation F(X)=E as input. The support of F(X) gets partitioned into two disjoint sets X0 = F^{-1}(0) and X1 = F^{-1}(1). E=0 learns a predictor over the set X0 and E=1 learns a predictor over the set X1. A predictor that labels all points in X0 as 0 and X1 as 1 actually satisfies the definition of invariant predictor because it simultaneously minimizes the error in the two environments. If the error of this predictor is actually less than the error of the predictor based on causal features, then this predictor can be selected by the IRM optimization, which is exactly the case in the CMNIST+ dataset. \n\nThe main reason IRM was designed was to make the predictors from different environments be compared when the sets X0 and X1 overlap to some extent at least, i.e. the image of F over the feature distributions in the two environments has to overlap. \nIf the image of F over feature distribution does not overlap at all (as is the case in example considered in the paper), a trivial invariant predictor which is not robust to distribution shifts will exist.\n\n\nc)\tThere is a simple alternate fix for the entire problem:\nThe space of problems that authors want to fix abstractly stated are when the environment label (domain label) itself is so strongly correlated with the label that the IRM is encouraged to use environment as a representation.\nThe fix goes as follows:  Mix the data from the two environments. Take the mixed data and divide into two completely new environments. A manual way to construct these new environments is to divide the data in such a way that the proportion of the colors in the two environments marginally different as was the case in colored MNIST. For an algorithmic approach  to construct these new environments  use this approach (paper http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-045.pdf) (code available at https://github.com/ecreager/eiil). The above paper shows that a single dataset can be divided into multiple environments and retains the gains shown in IRM. \nThe two new environments obtained from the algorithm in the above paper or even through a manual division as explained above would lead to a dataset that is very similar to the original CMNIST. Observe that by mixing and creating two new environments, we are automatically ensuring that the support overlap assumptions required by IRM are satisfied.\nI believe that these approaches can bring the performance back to 68 percent level. \n\nAlso, note that the fix I am proposing is not the simple fix based on label balancing that the authors show does not work. By mixing the environments, we are destroying the spurious environment based correlation that exist. \n\nTherefore, whenever there is a strong environment based spurious correlation, i.e., each environment has a stark difference in the marginal distribution of labels, then the prudent thing to do is to mix the environments destroy the spurious correlation and then construct environments for IRM either manually or through the algorithm I shared above. \n\nUpdate post discussions: The authors did more experiments to show their method works on CMNIST+ better than these baselines. However, I have major concerns with the principle proposed by the authors as a search criterion. I believe the authors approach happens to work on CMNIST+ but is not based on the right principle. More on this in my point d) below.\n\nd)\tThe fix proposed by authors has theoretical problems:\nThe authors have proposed to do conditional distribution matching which constraints that the representation learned has to be independent of the environment conditional on the label. For the sake of discussion, let us consider the structural equation model (SEM) used in Theorem 9 in Arjovsky et al. If we assume that the lambda-spuriousness condition holds for the SEM under consideration. Say for the two environments, the support of the feature distributions do not intersect, the support of the label distributions do not intersect. In this case of lambda-spuriousness, IRM continues to be able to recover the ideal invariant predictor and does not fail (as no assumption in Theorem 9 is violated). However, the representation used by the ideal predictor does not have to satisfy the CDM condition stated by the authors. Therefore, the CDM condition proposed by the authors holds in the extreme case of the example discussed by the authors but does not hold in general. \n\nIn other words, author should show how imposing CDM works in a broad range of settings and not just one example that they use. Based on my argument I above, I highly doubt that CDM condition is actually a necessary condition for the success of IRM under lambda-spuriousness.  \n\ne)\tColored MNIST + issues: \nThe authors never explained in detail how the data in the different environments is generated. The Table 1 is an incomplete definition. It only lays down the conditional distributions. Conditional distributions are not sufficient to decipher the underlying structural equation model.  What authors call lambda-spuriousness is not really a lambda structure. In CMNIST+, there has to be an arrow from the label Y to the spurious features Xs as well, which does not appear in lambda structure. \n\nUpdate based on discussion: The authors explained the data generation process. \n\nf)\tA simple comparison with balance in generation time:\nWhy did the authors not compare their method when P(Y=1|E)=0.5 in both environments. It seems for high values of rho and P(Y=1|E)=0.5 a direct application of IRM does not work.  My suspicion is when  P(Y=1|E)=0.5, IRM does not work because we need more than two training environments, as the number of colors that enter the equation of spurious correlations go from two to three. \n\ng) Comparison with Koyama et al.: Koyama et al. had worked with a lambda type structure and introduced an extension of colored MNIST where environment label plays an important role. A comparison with that work would have been useful.\n\nUpdate based on discussion: The authors clarified that this paper was posted on arxiv on Aug 4 and the ICLR policy requires them to compare only until Aug 2. I am ok with authors not including a comparison with this paper.\n\nQuality: Unfortunately, the paper is not good quality. A lot more work is needed to really justify why what they propose is really a problem with IRM and why the simple fixes I propose won't already solve the problem. \n\nSignificance: The area of OoD generalization is quite significant. However, the problem proposed by the authors and the approach taken by the authors is not of much significance. \n\nOriginality: The authors have proposed a new CMNIST dataset and a new algorithm to fix it. The authors should get credit for proposing the dataset but besides that I don't think the algorithm proposed as a fix is needed. \n\n\n**Final update:** The main criterion used by the authors to search invariant predictors is not correct and is in fact not satisfied by the invariant predictors. For this my suggestion to authors is to modify their criterion in a way that it is at least satisfied by the ideal model you want to learn.\n\nReferences:\n\nKoyama et al.   \"Out-of-distribution generalization with maximal invariant predictor.\" arXiv preprint arXiv:2008.01883 (2020).\n\n\n\n\n", "title": "Paper proposes a new dataset extending CMNIST to highlight flaws of IRM  and proposes a fix. Existing works that extend IRM already suffice to fix this problem. Several theoretical issues with fix proposed by the authors.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "W2QtFvpvm58": {"type": "rebuttal", "replyto": "yZc5LirNWAc", "comment": "1. The claims made by our paper are (1) IRM suffers from the non-overlapping issue of $F(X)$ from different domains under strong $\\Lambda$ spuriousness with deep models, and (2) IRM-CDM is a simple and effective fix to this non-overlapping issue. We empirically show that the proposed IRM-CDM improves IRM on CMNIST+. **We never claim that it has theoretical guarantees on recovering some-sort of causal representations, for example, the causal representation defined by a SEM. However, the reviewer wrote: *If your claim is you can prove that $f()$ minimizes a regularized loss for a certain $\\beta$, then please show a proof.* It may not be okay for the reviewer to first make an assumption for our work and then to attack the assumption made by him/herself.**\n\n2. Every method has its assumptions and domains to work, so does the proposed IRM-CDM. IRM-CDM is a generalized version of the vanilla IRM. IRM can be treated as IRM-CDM with $\\beta = 0$. IRM-CDM is applicable to the cases where IRM is supposed to be applicable. Our results show solid improvements over IRM in the cases where IRM suffers from the non-overlapping issue. We agree that it is an important problem to theoretically quantify classes of problems where the proposed method has theoretical guarantee.  We leave it as a future work. We welcome the reviewer to propose **concrete examples** that the method fails (there exist such examples) and we would love to explicitly mention those cases in the next version of the paper. However, based on the reviewer\u2019s, we have not seen such a formally defined class of problems that IRM-CDM fails. The reviewer first proposed the linear model in Theorem 9 [1]. But later we agreed that it does not apply to the deep invariant feature learning setting that this paper focuses on. After that, **the reviewer only wrote the following argument, which lacks formal specification and contradicts with the problem setting of IRM and this work. The reviewer wrote: In the simple system with $Y_e\u2190f(X_e)+n$, with some additional regularity conditions, it can be shown that $f()$ is the ideal invariant model one should learn. Do you agree to this or not?** In IRM, the goal is to learn a representation $F(X)$ that elicits an invariant optimal predictor for all domains [1]. This implies that it is unlikely, in practice, that we can learn the invariant function $f$ that directly maps raw features $X_e$ to the label $Y$, even if it exists. Instead, we need to learn a feature representation $F(X)$, which captures the *causal features* that elicits an invariant predictor mapping $F(X)$ to the label $Y$ across domains. In addition, in the reviewer\u2019s question, he/she did not specify (1) what is the *simple nonlinear system* or (2) what are the *additional regularity conditions*. If the reviewer can provide a formal definition of the aforementioned terminologies along with the derivation to support his/her claim since the reviewer wrote **it can be shown ...**.We cannot agree or disagree with the reviewer when a solid formulation of the reviewer\u2019s claim is absent.\n\n3. Please note that **the regularization formulation of IRM-CDM is the method proposed in both the first and current version of our paper.** We will try to improve the presentation of the motivation since the reviewer seems to misunderstand it.\n\n\n[1] Arjovsky, Martin, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. \"Invariant risk minimization.\" arXiv preprint arXiv:1907.02893 (2019).\n", "title": "Thanks for the reviewer's response! We justify our disagreement with the reviewer's claims."}, "IMK3qmMYRGI": {"type": "rebuttal", "replyto": "SrkQnnG55qg", "comment": "### Response to 1.\n \nWe strongly disagree with the reviewer\u2019s claim \u201cThe main criterion and theory motivating the approach itself **seem wrong**.\u201d In the previous reply, we provide concrete reasons for why Theorem 9 in IRM [1] and the linear model proposed by the reviewer do not invalidate our proposed method. We hope the reviewer can read our previous response carefully. \n\nWe hope that the reviewer could have a good understanding of the difference between the hard constraint $F(x) \\perp E | Y$ and the soft CDM regularization. As mentioned in our paper, the regularization weight $\\beta$ controls how much the distributions should be close to each other (overlapping), instead of being strictly equal. The regularization weight $\\beta$ should be tuned case by case because how much the hidden feature distributions should overlap differs case by case.\n\nIn the case $Y_e = f(X_e) + n$, the reviewer mentioned that ideally one wants to learn $f$ even though $X_e$ and $X_{e\u2019}$ do not overlap. First of all, in this case, with neural network modeling $f$, IRM will just learn $f$ independently in each environment. Such independent $f$ functions are not the desired invariant predictor. Second, we hope that the reviewer can understand the meaning of **invariant feature learning**. In this problem, we want to learn deep feature representations $F(X)$, that can bring data from different environments ($X_e$ and $X_{e\u2019}$) into the same hidden space. We make use of the prior information **there is some invariance across domains** by encouraging their features overlapping and enforcing that they share one common predictor (e.g., a linear layer) on the overlap. If their features do not overlap, then to learn a universal approximator $f$ (e.g., neural network) is essentially to learn $f$ independently in each environment/domain, which does not make use of the prior information **there is some invariance across domains** at all.\n\n### Response to 2.\n\nThe proposed method **IRM-ACDM outperforms the EIIL consistently** in terms of both (higher) mean and (lower) standard deviation in test accuracy.\n\n### Response to 3.\n\nThis reviewer\u2019s claim 3. is not correct. If possible, could the reviewer point out which part of [3] supports the claim? Generally speaking, $P(Y|X^c)$ is invariant across domains is not a sufficient condition for $X^c$ to be exactly the set of causes for $Y$. For the simplest example, $X^c$ can be just a subset of the causes of $Y$ which are exogenous. In fact, in traditional causal inference, even if we know $X^c$ is a cause of $Y$, we do not have information about whether $P(Y|X^c)$ is invariant across domains. One example is a non-stationary system [2] where $P(Y|X^c)$ can change over time even if $X^c$ is exactly the set of causes of $Y$. \n\n[1] Arjovsky, Martin, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. \"Invariant risk minimization.\" arXiv preprint arXiv:1907.02893 (2019).\n\n[2] Huang, Biwei, Kun Zhang, Mingming Gong, and Clark Glymour. \"Causal discovery and forecasting in nonstationary environments with state-space models.\" Proceedings of machine learning research 97 (2019): 2901.\n\n[3] Bareinboim, E., Brito, C., & Pearl, J. (2012). Local characterizations of causal Bayesian networks. In Graph Structures for Knowledge Representation and Reasoning (pp. 1-17). Springer, Berlin, Heidelberg.\n", "title": "We thank the reviewer's response but disagree with the three claims from the reviewer."}, "nvjwGFPxTrw": {"type": "rebuttal", "replyto": "q-qxdClTs0d", "comment": "We thank reviewers for their insightful feedback and for appreciating our work! We have revised the paper with the following major changes to incorporate the comments.\n\nBased on R2\u2019s suggestions, we add a paragraph in Appendix C.2 to explain why using two colors to set up a colored MNIST dataset under strong $\\Lambda$ spuriousness is possible but not ideal.\n\nBased on R3\u2019s suggestion, in Section 4, we add experimental results of the baseline EIIL [1] on CMNIST+, which show EIIL suffers from large standard deviation even if it can achieve comparable mean test accuracy to the proposed method IRM-CDM. We also provide a detailed explanation why it is unstable in Appendix C3.\n\nBased on R3 and R4\u2019s suggestions, in Appendix A, we add a detailed description of the relationship between IRM and the conditional independence $Y \\perp E | F(X)$.\n\nBased on R3\u2019s suggestions, we add a detailed description of the data generating process of CMNIST+ in Appendix C.2. We also update the causal graph in Fig. 1 to make sure it is well aligned with the data generating process of CMNIST+. In Appendix C.2, we explain why they align with each other.\n\nBased on R2\u2019s suggestions, we rewrote the confusing sentence *Specifically, we consider the situation where strong spurious correlation exists between the spurious features and the class label only in the training set. We name this strong \u039b spuriousness...* in the third paragraph of Section 1 as *Specifically, we consider the situation where strong spurious correlations among the spurious features, the class label and the domain label only hold for training data.*\n\nBased on R4\u2019s suggestion, we changed *theoretical results* to *analysis results* in Section 3.1 and Appendix B as they are specific for the CMNIST+ dataset.\n\n[1] Creager, Elliot, J\u00f6rn-Henrik Jacobsen, and Richard Zemel. \"Environment Inference for Invariant Learning.\" In ICML Workshop on Uncertainty and Robustness. 2020.\n", "title": "Response and revision "}, "jNeH91hYOb": {"type": "rebuttal", "replyto": "LW-im5tzPuu", "comment": "We thank the reviewer for the thoughtful suggestions and detailed reviews.\n\nWe agree it would be better if we can evaluate IRM-CDM against IRM, CDM and ERM in a real-world scenario. However, it can be challenging to confirm that there exists strong $\\Lambda$ spuriousness in a real-world dataset. We will try to add such an experiment in the next version of this work.\n\nIn this paper, we specifically focus on the limitation of IRM under strong $\\Lambda$ spuriousness instead of general domain transfer learning. The expected performance of the proposed method in domain transfer learning tasks can vary by the problem settings. Our method is specifically designed for cases where the causal relationships $P(Y|X^c)$ is invariant while statistical associations $P(Y|X^s)$, $P(X^c|E)$ and $P(X^s|E)$ can change across domains.\n\nWe also conjecture that invariant causal features can be useful in meta learning tasks under proper conditions. Since this topic is not very relevant to our paper, we would like to let the reviewer find answers in [1,2]. \n\nAs shown in Table 10 in Appendix B.2, the original CMNIST dataset has two training domains and a test domain. The detailed setup ($P(Y|E)$ and $P(C|Y,E)$) can be found in Table 10.\n\n[1] Zhang, Marvin, Henrik Marklund, Abhishek Gupta, Sergey Levine, and Chelsea Finn. \"Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift.\" arXiv preprint arXiv:2007.02931 (2020).\n[2] Yue, Zhongqi, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. \"Interventional few-shot learning.\" Advances in Neural Information Processing Systems 33 (2020).\n", "title": "About evaluation, application to domain transfer learning and meta learning."}, "OxgGyww2QJN": {"type": "rebuttal", "replyto": "WG0F978s1V", "comment": "### Justification on CDM\n\nFor OOD prediction under strong $\\Lambda$ spuriousness, IRM-CDM can be justified by (1) the IRM constraints can lead to undesired feature representations that (partially) fit the domain label and (2) CDM can compensate for IRM to make it more difficult to (partially) fit the domain label. In addition, considering an invertible function $f$ as the invariant classifier that maps $F(X)$ to $Y$, then given the label $Y=y$, through $f^{-1}$, we should expect to find the feature representations of two instances from the same class to be sampled from the same distribution $P(F(X)|Y)$ without knowing which domains they are from. Note that, when it is not the extreme case, having information of the domain label in the feature representations is also undesired as $P(Y|E)$ is not an invariant relationship across domains.\n\n### Theoretical results on pp 4\n\nWe agree that our \u201ctheoretical results\u201d are not the traditional type of theoretical results. We will replace \u201ctheoretical results\u201d with more proper terminology. However, the way we perform the theoretical analysis can be extended to other datasets. \n\n### Insufficient experimental validation\n\nWe agree that it would be better if we add more benchmarks. However, we must argue that the core arguments of this paper are that (1) IRM fails under strong $\\Lambda$ spuriousness as it can be satisfied by fitting the domain label and (2) IRM-CDM can fix the problem of IRM in this scenario as it can exclude undesired solutions that fit the domain label. The experiments included in the paper considering various strengths of $\\Lambda$ spuriousness have already shown enough evidence to support our arguments.\n\n### Comparison with DANN\n\nDANN is a method to align feature representations across domains, which imposes unconditional distribution matching, i.e., $P(F(X)|E) = P(F(X)|E\u2019) = P(F(X))$. Unconditional distribution matching is not a solution to OOD prediction as it fails when P(Y|E) varies across domains.\n\n### Minor issues\n\nWe understand that OOD is also used in the anomaly detection literature, but in causal machine learning, OOD is widely used to refer to the problem we discuss in this paper [1,2].\nWe will update Fig. 1 to make it easier to depict and understand the difference between strong $\\Lambda$ spuriousness and the scenario of the original CMNIST.\n\n\n[1] Arjovsky, Martin, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. \"Invariant risk minimization.\" arXiv preprint arXiv:1907.02893 (2019).\n\n[2] Krueger, David, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Remi Le Priol, and Aaron Courville. \"Out-of-distribution generalization via risk extrapolation (rex).\" arXiv preprint arXiv:2003.00688 (2020).", "title": "Justification on CDM, Theoretical results on pp 4, Insufficient experimental validation and Comparison with DANN"}, "HBIbcrtJ1M6": {"type": "rebuttal", "replyto": "Pz4iN87dQlg", "comment": "We thank the reviewer for the insightful suggestions. We really appreciate the time and effort you paid in the review.\n\n### Response to (c): Results of EIIL\nWe evaluate EIIL [3] (https://github.com/ecreager/eiil) on CMNIST+. EIIL gets test accuracy $43.40 \\pm 10.32\\%$, $43.24 \\pm 13.03 \\%$, $40.93 \\pm 13.03 \\%$ on CMNIST+ with $\\rho=0.8,0.85,0.9$. Compared to IRM-ACDM $57.23 \\pm 4.34\\%$, $44.83 \\pm 4.65 \\%$, $42.85 \\pm 3.09 \\%$ and IRM-MMD $52.91 \\pm 4.56\\%$, $40.83 \\pm 2.50 \\%$, $37.96 \\pm 6.97 \\%$, EIIL has comparable mean accuracies when $\\rho = 0.85,0.9$, but is not stable. There are two reasons. (1) EIIL relies on IRM and the soft domain weight $q(E|X,Y)$. When $q(E|X,Y)$ takes values s.t. strong $\\Lambda$ spuriousness exists, it makes IRM fail. (2) It has issues in model selection. EIIL can only select models by validation accuracy as $q(E|X,Y)$ is a scalar $q_i$ for instance $i$. It is not learned for the validation or test, so validation loss is not computable. Note that validation accuracy can be high when the model picks up spurious features. In contrast, model selection with validation loss considers the regularizers (IRM and CDM), which approximates how well models fit causal features. We add these results in Section 4 and Appendix C3.\n\n### Response to (e): The data generating process (DGP) of CMNIST+ and causal graph (Fig. 1).\n\nThe DGP is as follows:\n\n0. In MNIST, shapes $X^c$ decide digit labels.\n1. We get true labels $Y^*$ of instances by digit labels (0-9).\n2. We split the data into test and training. \n3. We assign the training instances to the training domains based on $Y^*$ and $P(Y^*|E)$ in Table 1. This introduces correlations between $Y^*$ and E. In training domains, we split data into training and validation.\n4. We generate noisy labels $Y$ by randomly flipping them with 25% probability.\n5. Given $P(C|Y,E)$, the $Y$ and $E$, we assign colors. This step introduces correlations among $X^s$, $Y$ and $E$.\n\nThere is a difference in what causal relationships mean in traditional causal inference and in OOD prediction. In OOD prediction, the definition of causal relationships is different from a traditional one. Traditionally, $X^c \\rightarrow Y$ means the generation of $Y$ is influenced by $X^c$. It does not necessarily mean $P(Y|X^c)$ remains the same across domains [2]. However, in OOD prediction, we say there exists a causal relationship $X^c\\rightarrow Y$ iff $P(Y|X^c)$ is the same across different domains.\nIn the DGP of CMNIST+, (1) there exist causal relationships $X^c\\rightarrow Y^* \\rightarrow Y$, so $P(Y|X^c)$ is invariant across $E$. (2) There are correlations among $X^s$, $Y$ and $E$.\nTo let Fig.1 be in accordance with the DGP, we make the directed edges among $E$, $X^s$ and $X^c$ to be bidirected to signify correlations. We describe the DGP in Appendix C2.\n\n### Summary\nOur response covers the reviewer\u2019s concerns from (a) all the way to (g). We also updated the manuscript accordingly. We believe we resolved the concerns and hope the reviewer can kindly reevaluate our work.\n\n[1] Arjovsky et al. \"Invariant risk minimization.\" arXiv preprint arXiv:1907.02893 (2019).\n\n[2] Pearl and Bareinboim. \"External validity: From do-calculus to transportability across populations.\" Statistical Science (2014): 579-595.\n\n[3] Creager et al. \"Environment Inference for Invariant Learning.\" In ICML Workshop on Uncertainty and Robustness. 2020.\n", "title": "Response to (c) (results of EIIL on CMNIST+) and (e) and a kindly request to reevaluate our work"}, "_HtzMbph3ct": {"type": "rebuttal", "replyto": "Pz4iN87dQlg", "comment": "We thank the reviewer for detailed and actionable comments.\n\nTheorem 9 in [1] crucially relies on the linearity assumption of the modeling. It cannot generalize to the deep learning cases that our paper and many recent papers on IRM focus on. Our main points are: (1) IRM suffers from the non-overlapping issue of $F(X)$ from different domains under strong $\\Lambda$ spuriousness with deep models, and (2) IRM-CDM is a simple and effective fix to it. With the linear model assumption, Theorem 9 in [1] does not need overlapping $F(X)$ supports with different $E$. Linear models are global models in which constraints enforced in a local domain are effective globally on the entire space. So, each new domain lying in a linear general position, **although its raw features/feature representations/labels may not overlap with those from other domains**, removes one degree of freedom in the space of invariant solutions **globally for all the other domains**. But for DNNs with universal approximation property, as long as the data from two domains do not overlap, the model has enough capacity to learn a feature extractor s.t. $F(X)$ from different domains do not overlap, making IRM ineffective. This explains IRM\u2019s failure with deep models when different domains have very different $P(F(X)|E)$. In our fix, we enforce the overlapping of $F(X)$ by adding the CDM constraint, its efficacy is shown by results on CMNIST+.\nHere, we motivate CDM. For deep models, IRM constraint assumes/requires that $F(X)|E=0$ and $F(X)|E=1$ overlap. This is enforced via CDM which minimizes a certain divergence between $P(F(X)|E=0)$ and $P(F(X)|E=1)$. For example, a finite Jeffreys divergence between two distributions implies that their supports exactly overlap. Specifically, we add the CDM (not the unconditional one) constraint as the mismatch of $F(X)$ from different classes is not desired. Finally, we reformulate the CDM constraint as a regularizer in the optimization as: $\\underset{\\theta}{\\min}   \\underset{\\theta_D}{\\max} \\sum_{e} \n \\mathcal{L}^e_{IRM} + \\beta \\mathcal{L}_{CMD}$.\nTherefore, the CDM regularizer is a simple and effective fix for the non-overlapping issue of IRM with deep models. From our perspective, it is conceptually simpler than EIIL.\n\n[1] Arjovsky et al. \"Invariant risk minimization.\" arXiv preprint arXiv:1907.02893 (2019).", "title": "Response to theoretical issues in (d)"}, "juCzhj6zczm": {"type": "rebuttal", "replyto": "7kBz7On9zA4", "comment": "We really appreciate the reviewer's insightful and helpful suggestions!\n\n### Three colors in CMNIST+\n\n#### Q1. Can we create strong $\\Lambda$ spuriousness with just two colors during training and testing?\n\nA1. It is possible to create strong $\\Lambda$ spuriousness with just two colors for binary classification with two training domains. Let\u2019s say we use red (R) and green (G). To show this is possible, we use an example with the following setup: P(Y=1|E=1)=0.9, P(Y=1|E=2)=0.1. Let\u2019s say for $E=1$, we set $P(C=G|Y=1,E=1)=0.9$, $P(C=G|Y=0,E=1)=0.1$. Then, we can set $P(C=G|Y=1,E=2)=0.1$, $P(C=G|Y=0,E=2)=0.9$ for $E=2$. We can observe strong correlations between color, class label and domain label. Thus, we created strong $\\Lambda$ spuriousness with just two colors.\n\n#### Q2. Would IRM still fail in a two-color datasets with strong $\\Lambda$ spuriousness?\n\nA2. As our discussion on why IRM fails and why the proposed fix is effective does not depend on the number of colors in a dataset. Our concern with such datasets is that even if strong $\\Lambda$ spuriousness exists in training domains, it is a challenge to find a test domain that is quite different from both training domains. Following the example in A1., for the test domain $E=3$, if we set $P(Y=1|E=3)=0.5$, $P(C=G|Y=1,E=3)=0.5$ and $P(C=G|Y=0,E=3)=0.5$, it would be right in the middle of the two training domains. In this case, even if IRM fails, it can be difficult to show it with the test accuracy. A model only fits colors can reach 0.5 accuracy, which would show smaller differences between models fitting causal features and those fitting the spurious ones. This would make it more challenging to judge whether a model failed in an experiment.\n\nWe added these explanations to Appendix C2.\n\n### \u039b spuriousness\nThanks for pointing out the confusion! We rewrote the sentence as \u201cSpecifically, we consider the situation where strong spurious correlations among the spurious features, the class label and the domain label only hold for training data.\u201d Hope this makes sense to you.\n\n### Updated Manuscript\nThanks for your suggestion! We updated the manuscript according to the review! We hope it is helpful for you to better understand our work.\n", "title": "Three colors in CMNIST+, \u039b spuriousness and updated manuscript."}, "f9R1Yas6jVE": {"type": "rebuttal", "replyto": "-CseCzHzPB", "comment": "### Relationship between IRM and $Y \\perp E | F(X)$\n\nHere, we revise the reviewer\u2019s claim to make it clear. We can say under general conditions, the conditional independence $Y \\perp E |F(X)$ is a necessary condition for solutions of the original IRM optimization problem. In the original IRM paper [2], we can see IRM is defined as a two-stage optimization problem. We can see, any solution F(X) to the problem of IRM must satisfy $Y \\perp E |F(X)$. However, any F(X) satisfying $Y \\perp E |F(X)$ may not be a solution to the original IRM problem. For example, generally, $F(X) = E$ would not minimize the sum of the domain-specific risk $R^e$. However, this does not invalidate our work. **Note that under strong $\\Lambda$ spuriousness, there exist solutions to the IRM problem that still pick up spurious features. Consider the extreme case, in the training data, if $Y=E$, then $F(X)=E$ is a solution to the problem.**\n\n### IRM, strong $\\Lambda$ spuriousness and overlapping\n\nIRM [2] never assumes overlapping w.r.t. F(X) across different domains. Instead, the last sentence in the paragraph beneath Definition 3 (Page 5) shows that IRM cannot handle the case $F(X)=E$. This is because IRM can only guarantee to learn feature representations that elicit invariant predictors when there is overlapping in F(X). In short, IRM works under a possibly untenable assumption. Our work points this problem out and proposes a simple and effective fix to it. The meaning of our work is clear.\n\n### The \u201csimple alternative fix\u201d and the ELLI algorithm from [3]\n\nThe simple alternative fix in (c) is based on an incorrect assumption. The reviewer misunderstood the problem setting of OOD prediction. Here, the knowledge of which features are spurious/causal is not given. So, we cannot test whether there exists strong $\\Lambda$ spuriousness given the observational data, which should not be taken as prior knowledge. However, this recommended fix -- mixing and redividing the data as proposed must rely on the assumption that we, when constructing our datasets and designing the algorithms, have the prior knowledge that (1) there exists strong $\\Lambda$ spuriousness and the (2) color is a spurious correlation.\n\n**The reviewer can propose any method that may or may not work. However, this kind of review is out-of-scope. Even if the reviewer proposes a valid method, it does not invalidate our method. In addition, no evidence shows the simple alternative fix works.**\n\nThe workshop paper [3] does not show the method ELLI can solve the OOD prediction problem under strong $\\Lambda$ spuriousness. ELLI solves a minimax game. In the max step, it learns to assign a new environment label to each instance s.t. The IRMv1 loss is maximized. In the min step, it minimizes the IRMv1 loss. **Note that in the min step, it still relies on the IRMv1 loss to learn causal features. Once ${q}$ assigns the environments s.t. Strong $\\Lambda$ spuriousness exists, either by a bad initialization or a bad local minimum, then ELLI is expected to fail. ELLI relies on a trained reference model that fits specific spurious features, which our method and IRM do not require.**\n\nThe reviewer claims hypothetical results without evidence. We believe this is neither a professional nor a scientific way to review papers.\n\n### The data generating process\n\nIn fact, Table 1 is a complete definition of how the data is generated. If not, we hope the reviewer can specify which distribution is not defined.  The data generating process of CMNIST+ is very clear given Table 1 and the value of $\\rho$. We closely follow that of the original IRM paper except we modified the values the conditional distributions P(Y|E) and P(C|Y,E) take. \n\n### Can IRM work when the number of colors is larger than the number of environments?\n\n**Results in Figure 4 ($w_{plus}$ = 0.2) show that IRM can lead to results (~56%) that are significantly better than random guesses when the number of colors is larger than the number of environments and the $\\Lambda$ spuriousness is weak. Without defining P(Y|E) and P(C|Y,E) for each environment, the relationship between the number of colors and the number of domains/environments can not be determined.** For example, adding another environment which cannot weaken the strong $\\Lambda$ spuriousness is not expected to improve the performance of IRM.\n\n### Cite and compare with [1]\n\n**Can the reviewer read the reviewer guideline? It says authors are not expected to cite work published after 08/02. [1] is on arxiv on 08/04.**\n\n[1] Koyama, Masanori, and Shoichiro Yamaguchi. \"Out-of-distribution generalization with maximal invariant predictor.\" arXiv preprint arXiv:2008.01883 (2020).\n\n[2] Arjovsky, Martin, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. \"Invariant risk minimization.\" arXiv preprint arXiv:1907.02893 (2019).\n\n[3] Creager, Elliot, J\u00f6rn-Henrik Jacobsen, and Richard Zemel. \"Environment Inference for Invariant Learning.\" In ICML Workshop on Uncertainty and Robustness. 2020.\n", "title": "We thank the reviewer for the thoughtful suggestions and extensively detailed reviews."}, "MWwRJgaCRZI": {"type": "rebuttal", "replyto": "2J3rKotCOx", "comment": "### More benchmarks\n\nWe agree that adding more benchmarks can improve our confidence in IRM-CDM\u2019s effectiveness in solving the OOD prediction problem under strong $\\Lambda$ spuriousness. As it can be extremely challenging to confirm that there exists strong $\\Lambda$ spuriousness in a practical benchmark, we plan to add another semi-synthetic benchmark in the next version of this work.\n\n### \u039b spuriousness\n\nAs we introduced in the third paragraph of introduction, the strong \u039b spuriousness means the strong spurious correlation between $X^s$ (spurious features) and $Y$ (label) is via their common cause $E$ (domain variable). This means the causal effect $E$->$X^s$ and $E$->$Y$ are both strong. This is different from the scenario of CMNIST. In CMNIST, since the two training domains are similar to each other, the causal effect $E$ -> $X^s$ is weak. Therefore, the strong \u039b spuriousness does not exist in CMNIST. We will update the manuscript to make it clearer.\n\n### Three colors in CMNIST+\n\nThe main reason to use three colors is that it is not easy to create a proper benchmark for OOD prediction under strong $\\Lambda$ spuriousness with only two colors. As shown in Figure 3, we can see with two colors, we can only create domains on the line between R (red) and G (green). So, it would be either (1) the two training domains are very similar to each other (as it is in CMNIST) or (2) the test domain is similar to one of the training domains. In case (1), strong $\\Lambda$ spuriousness does not hold. In case (2), it is generally not ideal to benchmark OOD prediction since we expect invariant causal features should let the model generalize to unseen test domains which are not similar to the training ones.\n\n### Theoretical results (Table 2) regarding IRM\u2019s performance with label balancing\n\nTo read Table 2 regarding IRM\u2019s performance with label balancing, we first need to understand that the conditional independence of IRM ($Y \\perp E | F(X)$) can be satisfied by E, Concat(E,C) and S, so we should look at the three corresponding columns on the right half of Table 2. We know that the model resulting in *best training accuracy* would be learned among the three. As $\\rho$ increases, we should expect IRM with label balancing is expected to be more likely to pick up Concat(E,C) as its feature representation, which leads to test accuracy = 0.2. In practice, we can see as $\\rho$ increases, the test accuracy of IRM with label balancing becomes closer to 0.2. We will add a paragraph to clarify the connection between Table 2 and Figure 1.\n\n### Minor suggestions\n\nWe will update the manuscript with (1) a clarification on the relationship between IRM and $Y \\perp E | F(X)$. You can also find our answer in the first part of our response to reviewer 3. (2) We will move the footnote to another place and (3) make the meaning of $K_{irm}$ easier to understand.\n", "title": "We thank the reviewer for the helpful suggestions and thoughtful reviews"}, "LW-im5tzPuu": {"type": "review", "replyto": "q-qxdClTs0d", "review": "This paper attacks the problem of OOD learning from the angle of invariant causal feature learning. The key idea is to capture domain invariant causal features and use the extracted causality relation to convey domain-adaptive classification. In this work, domain invariant causal features are learned by IRM, which imposes the consistency constraint between causal features and class labels across different domains. The core idea is to address the existence of spuriousness correlation by introducing the MMD and KL divergence based conditional distribution matching constraint to the IRM learning process. The experimental study based on a crafted MNIST data set demonstrates the superior performances of the regularised IRM learning method in the domain invariant learning task. \n\nIn general, the paper introduces an in-depth discussion of the limitation of the IRM learning mechanism and points out the root cause of failure of IRM (spuriousness correlation). This is interesting and potentially impactful for practical OOD learning tasks. The paper is well-written and the proposed objective is novel to my knowledge. we tend to accept the paper. \n\nStill, our concerns are as follows:\n1. Though the results look promising on the toy data set, it would be better to have a real-world scenario as a testbed for the proposed method. Domain transfer is a popular application. How would this method perform in a domain transfer learning task? \n2. Following the first question, we would expect some discussion about the relation between the proposed method and other transfer learning methods, such as meta-learning methods. Could domain invariant casual feature learning be considered as a way of conducting meta-learning? \n3. A minor issue in Table.1: how many domain labels are there defined in the CMNIST data set? How are they defined? \n\n\n\n", "title": "This paper discusses an interesting problem of invariant causal feature learning applied in OOD learning scenarios. It proposes to use a divergence based regularization term to address the spurious correlation existing in IRM.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "WG0F978s1V": {"type": "review", "replyto": "q-qxdClTs0d", "review": "**Summary.** This paper advances generalizable machine learning via addressing a major limitation of invariant risk minimization (IRM). In particular, the author(s) identified and discussed the issue of strong $\\Lambda$ spurious, where spurious features and class labels are strongly correlated due to common cause, causing unprotected IRM to fail while trying to exclude such non-causal predictors. To avoid this. pitfall, the author(s) proposed to leverage conditional distribution matching (CDM) to regularize the representation, which effectively helps to alleviate this issue. Two empirical solutions, respectively non-adversarial MMD and adversarial KL matching, have been presented and validated. \n\n**Quality & Clarity.** Overall this paper is presented with clarity. The problem is well motivated and carefully discussed. What I found unsatisfactory is the proposed solution needs extra justifications, which is detailed in my weakness section below. \n\n**Originality & Significance.** The author(s) have identified a major weakness of IRM that I also find concerning: while developed from the notation of invariant representations, based on which invariant predictors are defined, IRM does not explicitly regularize the representation in its formulation. This view, provided fully developed, encapsulates sufficient novelties. On the significance side, while this submission indeed addresses a major concern of IRM demonstrated by artificial examples, the author(s) fail to present a concrete real-world example to showcase this concern is a thing that we should actually worry about.  \n\n**Main Weakness.**\nJustification of CDM needs to be strengthened. The author(s) have provided an argument that explains the extreme case, where non-causal features perfectly predict domain. The discussion needs to be substantially enriched. CDM has been proposed for dealing with the label shift in domain adaption, and it relies on assumptions that should be reconciled with those made by IRM. Please clarify. \n\nTheoretical results on pp 4. This is a totally misleading heading. What I have expected is some theoretical discussion, instead, the author(s) have provided a numerical table computed from \"theoretical computations\". I do not consider these as theoretical as they do not generalize beyond this particular example. \n\nInsufficient experimental validation. This is what kills this paper. There is only one experiment performed on a semi-synthetic testbed, which does not serve to evidence the practical utility of this proposal. \n\nThe domain adversarial neural net (DANN) model is highly relevant to the proposal made here and should be carefully discussed and compared. In fact, DANN also regularizes the representation to make it domain-agnostic. \n\n**Minor issues.**\nThe term out-of-distribution (OOD) is a bit misleading, as this phrase is usually associated with the task of anomaly detection, where novel samples that are very different from the training examples are identified.  I would suggest the author(s) replace OOD to avoid confusion. \n\nThe aspect ratio in Fig 1 is off and it. makes readers very uncomfortable. Please redo this figure. And it does not clearly depict what's different compared to the standard scenarios amendable to IRM.", "title": "Nice touch, but justifications need to be substantially enriched", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}