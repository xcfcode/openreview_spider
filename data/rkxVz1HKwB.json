{"paper": {"title": "Certifiably Robust Interpretation in Deep Learning", "authors": ["Alexander Levine", "Sahil Singla", "Soheil Feizi"], "authorids": ["alevine0@cs.umd.edu", "ssingla@cs.umd.edu", "sfeizi@cs.umd.edu"], "summary": "We develop an interpretation procedure for deep learning models which is certifiably robust to adversarial attack.", "abstract": "Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.", "keywords": ["deep learning interpretation", "robustness certificates", "adversarial examples"]}, "meta": {"decision": "Reject", "comment": "This paper discusses new methods to perform adversarial attacks on salience maps.\n\nIn its current form, this paper in its current form has unfortunately has not convinced several of the reviewers/commenters of the motivation behind proposing such a method. I tend to share the same opinion. I would encourage the authors to re-think the motivation of the work, and if there are indeed solid use cases to express them explicitly in the next version of the paper."}, "review": {"hvhITUvaMK": {"type": "rebuttal", "replyto": "LVUaRKvEg8", "comment": "This review, unfortunately, mischaracterizes the main contribution of our paper. We propose a provable *defense* against adversarial attacks on saliency maps: such attacks were already previously proposed by other authors (Ghorbani et al. 2019). The existence of these attacks provides the motivation for provable defenses, e.g. our work.", "title": "Author Response"}, "Hkxsru03FB": {"type": "review", "replyto": "rkxVz1HKwB", "review": "This paper introduces an extension of Cohen et al. (2019)\u2019s result that allows one to derive robustness certificates for interpretation methods, as well as a bound on the top-K overlap of saliency methods. These results motivate the introduction of Sparsified SmoothGrad and a relaxation of this method that has differentiable elements. These introduced approaches adapt previous methods so the derived bounds are applicable. The proposed methods are shown to perform as well as Quadratic SmoothGrad (Smilkov et al. 2017) in CIFAR-10 experiments.\n\nI\u2019m not familiar with the field so it is hard for me to judge how novel the presented results are or whether the used baselines are the proper ones. That being said, the paper presents an interesting idea and it is relatively easy to read (I really appreciate the fact that for every theorem there is an interpretation, in words, for it). The only thing that sometimes makes the paper hard to read is when it starts to refer to too many constants without remind the reader what they are about. I have two complaints/questions about the relevance of the introduced bounds though. Right now, to me, it seems that the derived theoretical guarantees are not that relevant, hopefully the questions below will help clarify that.\n\nIn page 6, before introducing the \u201cSparsified SmoothGrad and its Relaxations\u201d, it is said that q is set to 2^13 because otherwise the gap would be too large in images from ImageNet, for example, when comparing to traditional values of q. However, ImageNet is never revisited in the paper. I was expecting to see ImageNet results in the experimental section but they are not there (or maybe some correlation between the gap and performance -- robustness). More than that, the Quadratic SmoothGrad, which doesn\u2019t have any theoretical guarantee, seems to perform as well as the proposed methods. So where is the gap/theoretical result relevant? What are the settings in which having a method with the derived theoretical guarantees shine? What are the limitations of Quadratic SmoothGrad? Right now, it seems to me that the \u201cSparsified SmoothGrad and its Relaxations\u201d and its empirical analysis weaken the paper, because they take a big chunk of it when there is not enough evidence to claim them as an important contribution. Am I missing something? I gave this paper a relatively low score because I\u2019m not certain about the relevance of its results, but if my questions are satisfactory answered, I\u2019ll be happy to update my score.\n\n------\n\n\n>>> Update after rebuttal: I stand by my score after the rebuttal. \n\nUnfortunately I'm not an expert in this area and I don't feel confident in having a very strong opinion about this paper. That being said, enough presentation issues were raised that make me uneasy about raising my score. I do agree with some of the concerns raised by other reviewers. \n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 1}, "BkeotR3osr": {"type": "rebuttal", "replyto": "HkgezgVA5H", "comment": "We thank you for your comment. Your concerns here are equally applicable to the study of adversarial robustness in the classification case: in both instances, there is a desire to protect against adversarial attacks which may affect how a machine learning system makes decisions. The suggestion that users would \u201cignore the decision\u201d if a system returns an incorrect result assumes that the system is entirely redundant: that its output has no effect on the users\u2019 behavior. This is true in the classification case as well: if we a priori assume that the users know the correct classification before looking at the output, adversarial examples cannot possibly cause any harm. In addition to the medical examples laid out in the paper, gradient-based methods are also used for automated image segmentation and object localization: Subramanya, et al. (https://arxiv.org/abs/1812.02843) recently introduced an adversarial attack against GradCAM, a variation of gradient-based saliency maps which is tailored for object localization specifically.\nIn the classification case, the wide literature on adversarial robustness published in recent years indicates that  the community considers adversarial attacks to be an issue worthy of concern: attacks against interpretation are just as plausible from a security standpoint as (non-physical) attacks against classification.\n", "title": "Author Response"}, "H1e7_anior": {"type": "rebuttal", "replyto": "H1e5BxVC5r", "comment": " We thank you for your comment.\n\nQualitative Evaluation on ImageNet: We have added additional qualitative comparisons on ImageNet, in Figure 3 and Appendix G.\n\t\nGradients, SoftPlus and Transfer vs Whitebox Attacks: In order to use first-order methods to adversarially attack gradient-based interpretations, a network must have defined second derivatives with respect to the input image (because the saliency map itself consists of the first derivatives of the output with respect to the input image).  ReLU networks thus cannot be attacked in this way. Therefore, we use a proxy network with SoftPlus activations to determine the direction of the attack.\n\nFigure 2 Y Axis: This is the 60th percentile of the robustness certificate: 60 percent of images have robustness certificates at least this large.\n\nRank-based Certificates: it is clear that an $L_p$ norm based metric would be inappropriate for the purpose of certifying similarity between saliency maps: in most works using gradient-based saliency maps (e.g., Sundararajan et al.  (2017)), the top values are clipped for visualization purposes, so that the rest of the interpretation can be scaled to a reasonable color range without being dominated by a few large outlier pixels. This suggests that an $L_p$ norm approach may be meaningless, because an $L_p$ norm could be dominated by the behavior of outlier values. The fact that this clipping is accepted practice also indicates that it is the relative rank of the importance of features of an image, rather than the absolute ratios between salience measures, that is important for interpretation.\n", "title": "Author Response"}, "HkeO62hooB": {"type": "rebuttal", "replyto": "Hkxsru03FB", "comment": "We thank you for your feedback. We evaluate our robustness certificates on ImageNet samples in Figure 2. To address the concern about the gap between empirical and certified robustness, we show this gap on CIFAR samples in Appendix J. While the size of  perturbations with certified robustness is small compared to the empirical robustness on these samples, our main contribution is to demonstrate that a minor variation on the commonly-used SmoothGrad technique does in fact have a robustness guarantee: furthermore, this is the first robustness certificate for interpretation that can be evaluated at the ImageNet scale. This minor modification to SmoothGrad has little effect on the visual output (Figure 3).  Additionally, in testing the empirical attacks (Figure 4), we show that both quadratic SmoothGrad and our variant are empirically robust. Therefore, the variant (sparsified SmoothGrad) combines the visual quality and empirical robustness of Quadratic SmoothGrad with an additional theoretical guarantee of robustness.\n", "title": "Author Response"}, "Hyenpjhior": {"type": "rebuttal", "replyto": "r1epdz-RKr", "comment": "We respectfully disagree. We believe that you may have misunderstood the main point of the paper. You mention that:\n\n \u201cI believe right now just the basic gradient is sufficient to indicate the region of interest.\u201d\n\nThe central issue here is that basic gradient methods may NOT in fact indicate the region of interest in an image. A small adversarial noise can keep the label as is but change the basic gradient result significantly. This is the problem that we are addressing in this paper. \n\nAs you mention, gradient-based saliency maps represent only a local first order approximation to the true influence of each feature on the decision. This leads to two issues:\n\n* Low quality natural interpretations: as noted by Smilkov, et al. (2017) the gradient with respect to a particular pixel may \u201cfluctuate sharply at small scales\u201d and therefore be \u201cless meaningful than a local average of gradient values.\u201d This observation led to the development of SmoothGrad. To put this simply, a large gradient value over a (very) small range of input values of a feature represents in total a small influence on the class score by that feature. However, if the input image happens to be within this interval where the gradient is large, the feature will erroneously appear to be highly salient. In practice, this leads to simple gradient-based interpretations looking \u201cnoisy,\u201d as apparently random pixels appear to be highly salient.\n\n* Adversarial attacks on interpretation: as demonstrated by Ghorbani, et al. (2019), one can adversarially craft examples where the basic gradient interpretation is in fact very different from the true region of interest. This is a direct consequence of the saliency map being a \u201cfirst order approximation\u201d: it is therefore possible to make this approximation adversarially bad, by crafting a small perturbation to the input.\n \nAs detailed in the paper, saliency maps are used in a broad range of highly sensitive downstream applications, including in medical imaging and object localization. Because an adversarial attack has been proposed by Ghorbani et al. (2019) which can distort saliency maps, it is therefore a topic of interest to defend against this type of adversarial attack. \n", "title": "Author Response"}, "H1guSchjjS": {"type": "rebuttal", "replyto": "SJlg24cQcr", "comment": "We thank you for your constructive feedback. To address your comments:\n\n1 and 2. Note that we sparsify the saliency maps before smoothing: in other words, the final smoothed saliency map will be non-sparse, because pixels which are less salient overall may still occur in the top 10% in a minority of random samples. Empirically, we find that this sparsification prior to averaging has little effect on the final smoothed interpretation: in particular, the results are visually very similar to the quadratic SmoothGrad proposed by (Smilkov et al. 2017). This was shown in Figure 3 on an ImageNet sample, as well as on additional CIFAR samples in Appendix G. To address this comment, we have added additional ImageNet samples both in the body of the paper (Figure 3) and in the appendix (Appendix G).\n\n3: We have added empirical tests using additional values of the sparsification parameter to Figure 4.\n", "title": "Author Response"}, "r1epdz-RKr": {"type": "review", "replyto": "rkxVz1HKwB", "review": "This paper proposes a way to testify how much a SmoothGrad saliency can vary from the true saliency attesting to the adversarial robustness but with the goal of interpretation.\n\nAt the premise of this work I do not think the paper motivates the value of such a robustness certificate. Using the gradient (with SmoothGrad), while providing a reasonable interpretation of the model, is just a linear approximation of the true explanation of the prediction. So saying we have the correct approximation is not so useful. I also am not sure we need such a method. For example imagine a doctor is looking at a saliency map and we are sure that it is correct first order approximation because of some method. What were the negative cases where this would fail? How would this method improve that? I believe right now just the basic gradient is sufficient to indicate the region of interest.\n", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 4}, "SJlg24cQcr": {"type": "review", "replyto": "rkxVz1HKwB", "review": "The work addresses an important problem of robustness of interpretation methods against adversarial perturbations. The problem is well motivated as several gradient-based interpretations are sensitive to small adversarial perturbations. \n\nThe authors present a framework to compute the robustness certificate (more precisely, a lower bound to the actual robustness) of any general saliency map over an input example. They further propose variants of SmoothGrad interpretation method which are claimed to be more robust.    \n\nThe empirical validation of the underlying theory and use of the sparsified (and relaxed) SmoothGradient interpretation methods is unconvincing because of the following reasons:\n\n1. In the demonstrated experiment, the proposed alternative to SmoothGrad involves setting the lowest 90% of the saliency values to zero, and the top 10% (for sparsified SmoothGrad) or top 1% (in the case of relaxed sparsified SmoothGrad) to one. The problem with clamping most of the lower values to zero and the remainder (or most of the remainder) higher values to one is that it defeats the purpose of having a saliency map in the first place, which exist to characterize the relative importance of the input features. \n\n2. The paper claims that the proposed variant maintains the high visual quality of SmoothGrad, however, the claim is unsubstantiated. With the current setup, there is a clear trade-off between robustness and fidelity of interpretation, which the paper fails to acknowledge. In principle, one can always build extremely sparse or dense interpretation methods (close to all zeros or all ones), which would produce high robustness certificates but would be much less meaningful as they are not faithful to the underlying mechanism of prediction, and the characteristics of the input.\n\n3. The authors present empirical evidence on just one set of sparsification parameters and K. It would be more conclusive to evaluate the robustness of the proposed variations with different values of sparsification parameters, and K.\n", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 2}}}