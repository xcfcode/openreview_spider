{"paper": {"title": "Multiple-Attribute Text Rewriting", "authors": ["Guillaume Lample", "Sandeep Subramanian", "Eric Smith", "Ludovic Denoyer", "Marc'Aurelio Ranzato", "Y-Lan Boureau"], "authorids": ["glample@fb.com", "sandeep.subramanian.1@umontreal.ca", "ems@fb.com", "ludovic.denoyer@lip6.fr", "ranzato@fb.com", "ylan@fb.com"], "summary": "A system for rewriting text conditioned on multiple controllable attributes", "abstract": "The dominant approach to unsupervised \"style transfer'' in text is based on the idea of learning a latent representation, which is independent of the attributes specifying its \"style''. In this paper, we show that this condition is not necessary and is not always met in practice, even with domain adversarial training that explicitly aims at learning such disentangled representations. We thus propose a new model that controls several factors of variation in textual data where this condition on disentanglement is replaced with a simpler mechanism based on back-translation. Our method allows control over multiple attributes, like gender, sentiment, product type, etc., and a more fine-grained control on the trade-off between content preservation and change of style with a pooling operator in the latent space. Our experiments demonstrate that the fully entangled model produces better generations, even when tested on new and more challenging benchmarks comprising reviews with multiple sentences and multiple attributes.", "keywords": ["controllable text generation", "generative models", "conditional generative models", "style transfer"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper shows how techniques introduced in the context of unsupervised machine translation can be used to build a style transfer methods.\n\nPros:\n\n-  The approach is simple and questions assumptions made by previous style transfer methods (specifically, they show that we do not need to specifically enforce disentanglement).  \n\n-  The evaluation is thorough and shows benefits of the proposed method\n\n-  Multi-attribute style transfer is introduced and benchmarks are created\n\n-  Given the success of unsupervised NMT, it makes a lot of sense to see if it can be applied to the style transfer problem\n\nCons:\n\n- Technical novelty is limited \n\n- Some findings may be somewhat trivial (e.g., we already know that offline classifiers are stronger than the adversarials, e.g., see Elazar and Goldberg, EMNLP 2018).\n\n\n\n\n"}, "review": {"HklLDIyapm": {"type": "rebuttal", "replyto": "H1xk7NH9pX", "comment": "Thank you for your question. As AnonReviewer3 mentioned, simply copying input sentences wouldn\u2019t satisfy the auto-encoding part of equation (1), as noise has been added to sentences. However, it would indeed satisfy the back-translation loss.\n\nThe idea of denoising here is that by removing random words from a sentence, we hope to remove words that are required to infer the style.\nFor instance, if the input sentence is: \u201cthis place is awful\u201d\nand that the noised sentence becomes: \u201cthis place is <BLANK>\u201d,\nthe model will be trained to recover \u201cthis place is awful\u201d\nfrom: (\u201cthis place is <BLANK>\u201d, ATTRIBUTE=NEGATIVE)\n\nSince there might be a lot of occurrences of \u201cthis place is amazing\u201d in the dataset, the model will have to learn to consider the provided attribute in order to give a high probability to \u201cawful\u201d without penalizing the perplexity on the positive reviews.\n\nThe general argument is that the decoder needs to learn to use the attribute information whenever the input to the system is very noisy. This applies as well when inputs come from the back-translation process. Noisy inputs are produced in the back-translation process at the beginning of training when the model is insufficiently trained and does not generate well, and when generations are produced at high temperature. When using high softmax temperatures, the model tends to exhibit lower content preservation and higher attribute transfer since the generated samples are very noisy and it is therefore more difficult to recover the original input in the back-translation process while the decoder is forced to better leverage the attribute information.", "title": "Regarding the convergence of the model"}, "HygNdVy66m": {"type": "rebuttal", "replyto": "rylWfjH0hQ", "comment": "Thank you for your comment. We\u2019ve added a comparison with Hu et al., 2017 in the revised paper using the code you mentioned. We found that this model obtained a good accuracy / BLEU score, but with pretty high perplexity. We\u2019ve also added a reference to Yang et al in the related work section, thank you for pointing this out.", "title": "Results of the comparison"}, "r1eBLQ1pT7": {"type": "rebuttal", "replyto": "S1ll1KQ5hX", "comment": "\u201cI think the last and most critical question is what the expected style-transferred rewriting look like. What level or kind of \"content-preserving\" do we look for?\u201d - This is a great question, and a fundamental open research problem which, as far as we know, does not have a clear answer in existing literature. In our paper,  we view this line of research as looking for better ways to generate rewrites of text along certain directions, and exactly the \u201ckind\u201d of what content is being preserved would ideally be one of the \u201cknobs\u201d that a system can control. The phrase \u201cstyle transfer\u201d is useful to refer to previous work that have adopted it from the image domain, but its framing is a bit narrow for the scope of rewriting types our work addresses. We believe that the trade-off between attribute control and content preservation should depend on two factors 1) the eventual use case of such a system (and style transfer is one use case, but another one would be to obtain more \u201cinteresting\u201d and varied generations by augmenting a retrieval system with rewriting capabilities in a controllable way, and 2) the nature of attributes being controlled. Firstly, in contrast to previous work, we present means to control this inherent trade-off in the form of a latent-space pooling operator which can adapted to a particular use case. Secondly, the proposed method is fundamentally one that learns an unsupervised mapping between two or more domains of text, and the nature of the learned mapping will certainly depend on the nature of the domains. For example, it is often possible to map between the positive and negative domains by replacing a few words or small phrases and as a result, we can expect our models to preserve a lot of the input. By contrast, attributes such as one\u2019s age aren\u2019t as \u201clocal\u201d and might require rewriting more content to successfully be altered. In that case, the content that is being preserved might be the general structure of the text, its sentiment, etc. To make the trade-off clearer, we have added a figure to the manuscript showing how it varies across training (Fig. 1 in the appendix); we also include illustrations of rewrites at different trade-off levels in Table 13.\n\n\u201cTowards the end of Section 3, it says that \"without back-propagating through the back-translation generation process\". Can you elaborate on this and the reason behind this choice?\u201d - Back-propagating through the back-translation process would require computing gradients through a sequence of discrete actions since generations are sampled from the decoder. While this may be achieved via policy-gradient methods such as REINFORCE or other approximations like the Gumbel-softmax trick, these have been known to perform very poorly in high dimensional action spaces due to high variance of the gradient estimates. This approach also has the disadvantage of biasing the model towards the degenerate solution of copying the input while ignoring attribute information entirely to satisfy the cycle-consistency objective, since the gradients flow through the entire cycle, which is what we observed in practice.\n\n\u201cWhat does it mean by \"unknown words\" in \"... with 60k BPE codes, eliminating the presence of unknown words\" from Section 4?\u201d - We meant that by using BPE, we can operate without replacing infrequent words with an <unk> token -- we do not have unknown words because these are decomposed into subword units that belong to the BPE dictionary.\n\n\u201cwhat is the difference among the three \"Ours\" model?\u201d - These models differ in the choice of hyperparameters (pooling kernel width and back-translation temperature) to demonstrate our model\u2019s ability to control the content preservation vs attribute control trade-off. We have clarified this in the table caption.\n\n\u201cthe perplexity of \"Input Copy\" is very high compared with generated sentences.\u201d - This is true and we believe that this is a consequence of the fact that there is more diversity in the input reviews than in typical generations from ours and other systems. This lack of diversity is typical for models decoding with beam search, which leads to \"mode seeking behavior\" wherein the output generations contain fragments that occur most frequently in the training set. This results in the pre-trained LM assigning high likelihoods to these samples.\n\n\u201cwhat does the \"attention\" refer to?\u201d - The row in Table 7 that corresponds to \"-attention\" refers to a model that was trained without an attention mechanism in a vanilla sequence-to-sequence fashion, using the last hidden state of the encoder by concatenating it to the word embeddings at every time step of the decoder.\n\n\u201cIn the supplementary material, there are lambda_BT and lambda_AE. But there is only one lambda in the loss function (1).\u201d -Thank you for spotting this typo. We fixed this in the revised version of the paper.", "title": "Thank you for your review and raising interesting questions about this work. (part 2)"}, "Skxizm1aaQ": {"type": "rebuttal", "replyto": "S1ll1KQ5hX", "comment": "\u201cIs there any difference between the two discriminators/classifiers?\u201d - The discriminator and classifier have completely identical architectures - a 3 layer MLP with 128 dimensional hidden layers and LeakyReLU activations (now clarified in the model architecture paragraph in Section 3.3). We used two different terms to describe them since the classifier is fit post-hoc and doesn\u2019t adapt to the encoder representations in a min-max fashion while the discriminator does. Moreover, the classifier is fully trained on the final encoder representations, while the discriminator is \u201cchasing\u201d them without fully training after each and every update of the encoder representations. This is indeed a bit confusing, and we have clarified this in the paper. While a discriminator trained more thoroughly at each iteration might disentangle representations more, our goal was not to look at whether disentangled representations can result in better performance, but whether current training practices actually result in disentangled representations (see responses below as well).\n\n\u201cthere should be enough signal from the discriminator to adapt the encoder in order to learn a more disentangled representation.\u201d - This is a valid concern, but the experiments we ran suggest that this does not change the main observation. For instance, we also experimented with larger coefficients of adversarial training of 1.0 and 10.0 (as well as no adversarial training on the other end of the spectrum). While the attribute recovery accuracy drops a little at higher coefficients, it is still much higher than the discriminator accuracy during training. Also, models trained with high adversarial training coefficients have extremely high reconstruction and back-translation losses. Results are presented below, for better formatting please refer to the revised version of our paper.\n\n        Coef        Disc(acc)  Clf(acc)\n        0        &\t89.45% &  93.8%\n        0.001 &\t85.04% &  92.6%\n        0.01   &\t75.47% &  91.3%\n        0.03   &\t61.16% &  93.5%\n        0.1     &\t57.63% &  94.5%\n        1.0     &\t52.75% &  86.1%\n        10      &\t51.89% &  85.2%\n\n\u201cOn the other hand, this does not answer the question if a \"true\" disentangled representation would give better performance. The inferior performance from the adversarially learned models could be because of the \"entangled\" representations.\u201d - We agree completely. Our point is not that disentangled representations would not lead to good performance, but simply that disentanglement doesn't happen in practice with the kind of adversarially trained models typically used for this problem. We have made changes to the writing to make our stance clearer.\n\n\u201cRequest for ablation study on pooling and other architectural design choices.\u201d - In addition to the averaged attribute embeddings, we also explored using a separate embedding for each attribute combination in the cross-product of all possible attribute values. We found this to have similar performance to our averaging method. We decided against concatenating embeddings because we use the attribute embedding as the first input token to the decoder, and using a concatenation would mean dividing the embedding size for each attribute value by the number of attributes, to maintain to overall embedding size. This wouldn\u2019t scale as well to settings with many possible attributes. We settled on the attribute embedding averages because of its simplicity.\n\nWe have included a plot (Figure 1) that shows the evolution of attribute control (accuracy) and content preservation (BLEU) over the course of training as a function of the pooling kernel width. This demonstrates the latent space pooling operator\u2019s ability to trade off self-BLEU and accuracy - larger kernel widths favor attribute control while smaller ones favor content preservation.\n\n\u201cAs long as the \"back-translation\" gives expected result, it seems not necessary to have \"meaningful\" or hard \"content-preserving\" latent representations when the generator is powerful enough.\u201d\nWe observed that operating without a DAE objective didn\u2019t work since the model needs to be bootstrapped to be capable of producing outputs that are at least somewhat close to the original input before the back-translation process can take over. At the beginning of training, it is nearly impossible for the model to be able to recover the original input starting from a nearly random sequence of words. But it\u2019s indeed true that later on the back-translation loss is enough: in practice, we in fact removed the DAE objective by progressively decreasing lambda_AE from 1 to 0 over the first 300,000 iterations (c.f. Appendix section), even though we didn\u2019t observe a significant difference compared to simply fixing lambda_AE to 1.", "title": "Thank you for your review and raising interesting questions about this work. (part 1)"}, "rylhcb16TX": {"type": "rebuttal", "replyto": "H1lXb9okiX", "comment": "To make the architecture clearer, we updated the paper and added a paragraph, describing the architecture of the model in the \u201cImplementation\u201d section. That paragraph was previously in the appendix -- we hope inserting it into the main body makes the paper easier to follow. \n\nAs for our additions to the model, the methodology we used is similar to previous approaches in unsupervised machine translation, but with two key differences.\n\nFirst, our approach can handle multiple attributes, while previous approaches usually only consider two different domains (one for the positive reviews, and one for the negative reviews, for instance) and cannot be easily extended to multiple domains as they typically require one encoder and one decoder per domain. Our approach can handle multiple attributes at the same time, including categorical attributes (e.g. Table 9 in the Appendix).\n\nAlso, we introduced a pooling operator and we found it to be critical in our experiments. The problem we observed is that without it, the model has a tendency to converge to the \u201ccopy mode\u201d, where it simply copy words one by one, without taking the attribute input into consideration. We included a plot in the ablation study (Figure 1) that shows the evolution of the attribute transfer accuracy and the content preservation over training, for different pooling layer configurations. We can see that without the pooling operator, the model directly converges to the \u201ccopy mode\u201d, with a self-BLEU close to 90 after only a few epochs. A pooling operator with a window of size 8 not only alleviates this issue, but it also provides intermediate models during training with different trade-offs between content-preservation and attribute transfer.", "title": "Thank you for the review & comments"}, "BkekG-1TaQ": {"type": "rebuttal", "replyto": "B1la-jxRhQ", "comment": "Thank you for your review. We are glad to see that you liked the paper and it's contributions.", "title": "Thank you for the review!"}, "B1la-jxRhQ": {"type": "review", "replyto": "H1g2NhC5KQ", "review": "This paper presents a model for text rewriting for multiple attributes, for example gender and sentiment, or age and sentiment. The contributions and strengths of the paper are as follows. \n\n* Problem Definition\nAn important contribution is the new problem definition of multiple attributes for style transfer. While previous research has looked at single attributes for rewriting, \"sentiment\" for example, one could imagine controlling more than one attribute at a time. \n\n* Dataset Augmentation\nTo do the multiple attribute style transfer, they needed a dataset with multiple attributes. They augmented the Yelp review dataset from previous related paper to add gender and restaurant category. They also worked with microblog dataset labeled with gender, age group, and annoyed/relaxed. In addition to these attributes, they modified to dataset to include longer reviews and allow a larger vocabulary size. In all, this fuller dataset is more realistic than the previously release dataset.\n\n* Model\nThe model is basically a denoising autoencoder, a well-known, relatively simple model. However, instead of using an adversarial loss term as done in previous style transfer research, they use a back-translation term in the loss. A justification for this modeling choice is explained in detail, arguing that disentanglement (which is a target of adversarial loss) does not really happen and is not really needed. The results show that the new loss term results in improvements.\n\n* Human Evaluation\nIn addition to automatic evaluation for fluency (perplexity), content preservation (BLEU score), and attribute control (classification), they ask humans to judge the output for the three criteria. This seems standard for this type of task, but it is still a good contribution.\n\nOverall, this paper presents a simple approach to multi-attribute text rewriting. The positive contributions include a new task definition of controlling multiple attributes, an augmented dataset that is more appropriate for the new task, and a simple but effective model which produces improved results.", "title": "This paper presents a model for text rewriting for multiple attributes. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1ll1KQ5hX": {"type": "review", "replyto": "H1g2NhC5KQ", "review": "This work proposes a new model that controls several factors of variation in textual data where the condition on disentanglement is replaced with a simpler mechanism based on back-translation. It allows control over multiple attributes, and a more fine-grained control on the trade-off between content preservation and change of style with a pooling operator in the latent space.\n\nOne of the major arguments is it is unnecessary to have attribute-disentangled latent representations in order to have good style-transferring rewriting. In Table 2, the authors showed that \"a classifier that is separately trained on the resulting encoder representations has an easy time recovering the sentiment\" when the discriminator during training has been fooled. Is there any difference between the two discriminators/classifiers? If the post-fit classifier on top of the encoder representation can easily predict the correct sentiment, there should be enough signal from the discriminator to adapt the encoder in order to learn a more disentangled representation. On the other hand, this does not answer the question if a \"true\" disentangled representation would give better performance. The inferior performance from the adversarially learned models could be because of the \"entangled\" representations.\n\nAs the author pointed out, the technical contributions are the pooling operator and the support for multiple attributes since the loss function is the same as that in (Lample et. al 2018). These deserve more elaborated explanation and quantitative comparisons. After all, the title of this work is \"multiple-attribute text rewriting\". For example, the performance comparison between the proposed how averaged attribute  embeddings and simple concatenation, and the effect of the introduced trade-off using temporal max-pooling.\n\nHow important is the denoising autoencoder loss in the loss function (1)? From the training details in the supplementary material, it seems like the autoencoder loss is used as \"initialization\" to some degree. As pointed out by the authors, the main task is to get fluent, attribute-targeted, and content-preserving rewriting. As long as the \"back-translation\" gives expected result, it seems not necessary to have \"meaningful\" or hard \"content-preserving\" latent representations when the generator is powerful enough.\n\nI think the last and most critical question is what the expected style-transferred rewriting look like. What level or kind of \"content-preserving\" do we look for? In Table 4, it shows that the BLEU between the input and the referenced human rewriting is only 30.6 which suggest many contents have been modified besides the positive/negative attribute. This can also be seen from the transferred examples. In Table 8, one of the Male example: \"good food. my wife and i always enjoy coming here for dinner. i recommend india garden.\" and the Female transferred rewriting goes as \"good food. my husband and i always stop by here for lunch. i recommend the veggie burrito\". It's understandable that men and women prefer different types of food even though it is imagination without providing context. But the transfer from \"dinner\" to \"lunch\" is kind of questionable. Is it necessary to change the content which is irrelevant to the attributes?\n\n\nOther issues:\n- Towards the end of Section 3, it says that \"without back-propagating through the back-translation generation process\". Can you elaborate on this and the reason behind this choice?\n- What does it mean by \"unknown words\" in \"... with 60k BPE codes, eliminating the presence of unknown words\" from Section 4?\n- There is no comparison with (Zhang et. al. 2018), which is the \"most relevant work\".\n- In Table 4, what is the difference among the three \"Ours\" model?\n- In Table 4, the perplexity of \"Input Copy\" is very high compared with generated sentences.\n- In Table 7, what does the \"attention\" refer to?\n- In the supplementary material, there are lambda_BT and lambda_AE. But there is only one lambda in the loss function (1).\n- Please unify the citation style.", "title": "Good work but better presentation needed", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1lXb9okiX": {"type": "review", "replyto": "H1g2NhC5KQ", "review": "The paper proposes \"style transfer\" approaches for text rewriting that allow for controllable attributes. For example, given one piece of text (and the conditional attributes associated with the user who generated it, such as their age and gender), these attributes can be changed so as to generate equivalent text in a different style.\n\nThis is an interesting application, and somewhat different from \"style transfer\" approaches that I've seen elsewhere. That being said I'm not particularly expert in the use of such techniques for text data.\n\nThe architectural details provided in the paper are quite thin. Other than the starting point, which as I understand adapts machine translation techniques based on denoising autoencoders, the modifications used to apply the technique to the specific datasets used here were hard to follow: basically just a few sentences described at a high level. Maybe to somebody more familiar with these techniques will understand these modifications fully, but to me it was hard to follow whether something methodologically significant had been added to the model, or whether the technique was just a few straightforward modifications to an existing method to adapt it to the task. I'll defer to others for comments on this aspect.\n\nOther than that the example results shown are quite compelling (both qualitatively and quantitatively), and the experiments are fairly detailed.\n", "title": "Impressive experiments, but hard to determine how much is methodologically new here", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}