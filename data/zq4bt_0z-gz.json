{"paper": {"title": "Latent Programmer: Discrete Latent Codes for Program Synthesis", "authors": ["Joey Hong", "David Dohan", "Rishabh Singh", "Charles Sutton", "Manzil Zaheer"], "authorids": ["~Joey_Hong2", "~David_Dohan1", "~Rishabh_Singh1", "~Charles_Sutton1", "~Manzil_Zaheer1"], "summary": "", "abstract": "In many sequence learning tasks, such as program synthesis and document summarization, a key problem is searching over a large space of possible output sequences. We propose to learn representations of the outputs that is specifically meant for search: rich enough to specify the desired output but compact enough to make search more efficient. An appealing realization of such representation are discrete latent codes, as this naturally allows sophisticated combinatorial search strategies. The latent codes are learned using a self-supervised learning principle, in which first a discrete autoencoder is trained on the output sequences, and then the resulting latent codes are used as intermediate targets for the end-to-end sequence prediction task. Based on these insights, we introduce the Latent Programmer, a program synthesis method that first predicts a discrete latent codes from input/output examples, and then generates the program in the target language. We  evaluate the Latent Programmer on two domains: synthesis of string transformation programs, and generation of programs from natural language descriptions. We demonstrate that the discrete latent representation significantly improves synthesis accuracy.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper tackles program synthesis using a discrete latent code\napproach, enabling two-level beam search decoding. The approach is well\nmotivated, as program synthesis requires high-level choices that affect long\nsubsequences of the output, and discrete codes are amenable to heuristic search.\nEmpirical results show improvements over methods with no latent\nvariables and methods with continuous latent variables. However, the review process reveals that some of the claims made about the nature and necessity of the discrete latent codes is not sufficiently justified by the current analysis. With borderline assessments in a very competitive venue, *I cannot recommend acceptance*. I would like to encourage the authors to pursue this direction further, shedding more light on the nature of the latent representations learned as an interpretable planning mechanism.\n\nThe discussion was rich and surfaced a lot of concerns and issues with\nthe paper, that I strongly encourage the authors to take into account.  After\nthe author responses and internal discussion, many initial concerns\nwere settled, but some remain. The two main concerns raised are: (1) that the\npaper makes overly ambitious claims about high-level planning which is not\nbacked by an analysis of the latent codes themselves, and (2) that the\nimprovement may be due to increased generation diversity (which could be\npossible in continuous LV models too) rather than meaningful\nhigh-level planning. I believe that after clarifying such remaining loose ends, \nthis work would be of great interest to both the field of program synthesis as well as the discrete\nrepresentation learning community.\n"}, "review": {"NibCYzKxZbJ": {"type": "review", "replyto": "zq4bt_0z-gz", "review": "### Summary ###\nThe paper addresses the problem of program synthesis from examples, and also evaluates on program synthesis from natural language descriptions.\nThe paper proposes Latent Programmer, an approach that employs an adapted VQ-VAE to predict a sequence of latent codes, and then generates the output program conditioned on those codes.\nThe evaluation shows improved results over straightforward LSTM and Transformer baselines.\n\nOverall, I think that the approach is interesting, but the paper is very difficult to read and follow, and I am not sure that the evaluation is sufficiently extensive. I am thus voting for rejection at this time. \n\n### Strengths ###\n+ The proposed approach is a first application of VQ-VAE to program synthesis.\n+ The models outperforms LSTMs and Transformers\n\n### Weaknesses ###\n- The paper is very unclear and difficult to follow\n- It is unclear **why** the approach works\n- It is unclear whether this is a direct application of VQ-VAE to program synthesis, or is there any novel insight.\n- The evaluation is lacking comparison to additional baselines and datasets.\n\n**Clarity** - the paper is very difficult to read and follow. For example, the first few pages contain obscure phrases like \"discrete latent codes\", \"our sketches are comprised of a general latent vocabulary\", \"the semantics of the latent codes\", \"stop gradient operator\", \"sequence of tokens in latent space\", \"general vocabulary of discrete latent variables\". While I could understand each individual word, it was unclear what exactly do the authors refer to.\n\nThe related work section is thorough and compares previous approaches to the proposed approach, but at this point in the paper, the reader has no idea what these comparisons refer to.\nOnly by the end of page 4 I had a general idea in mind of what the proposed approach does, and still, I couldn't perfectly match the description to Figure 2 (Was $Z'$ defined anywhere? What *exactly* do the authors mean by quantization and how coarse is it? how are the program and the I/O examples encoded?). \n\nAdditionally, there are plenty of smaller things that make the paper even harder to follow. For example: the caption of Figure 2 is uninformative; Equation (1) and its preceding text could simply say \"nearest neighbor\"; in Equation 2 it isn't even clear what is $x$, and what kind of encoder is $ec$, both architecturally and in terms of representation? Are the two Transformers \"hierarchical\" or just \"two-level\" or \"two modules\" (Section 4.1, all these phrases are used to describe these two transformers); the equations in Equation 3 are written from right-to-left. Section 4.1 says that \"the plan is a sequence of tokens in latent space, denoted <TOK_1>, ..., <TOK_K>\". If these are \"tokens in **latent** space\", can we just call them \"a sequence of vectors\", instead of using the word \"token\" in an unusual way?\n\n**Why it works** - the paper does not give much intuition/explanation to **why** does the proposed approach work. The argument from the Abstract / Introduction that \"discrete latent codes can learn a useful representation for search\" is not very convincing, because eventually, the search is performed in the program space. So, the model does need to generate a sequence of tokens eventually. I think that maybe what the two steps of (1) generating the codes and then (2) generating the program given the codes do is provide more *diversity* of solutions, rather than better search.\n\n**VQ-VAE** - before reading the paper, I was not familiar with VQ-VAE. The background in Section 3 was not sufficient and not clear enough. I understand that explaining a new background concept is challenging, but I still think that Section 3 could be written more clearly *without* taking more space.\nFor example, by saying \"nearest neighbor\" instead of multiple lines and equations, and without citing \"van den Oord et al., 2017\" three times on the same half a page.\n\nAnother issue is that I am not sure whether this paper simply applies VQ-VAE on program synthesis, or is there a novel adaptation following insights about programs?\nFurther, is the focus the VQ-VAE really needed, or can a standard VAE work as well (or even an AE)?\n\n**Evaluation** - the evaluation presents impressive results, mainly compared to LSTMs and Transformers, which is an important comparison that is not always performed in all papers. However, it seems that there are not enough baselines to put the results in context. Additionally, the datasets could have been more standard: why generating a new dataset for string transformation? why taking such a small dataset for NL->python?\n\n* Baselines - aren't there any recent baselines for string transformation, other than RobustFill? There is no other neural or non-neural available model for program synthesis from examples?\nIn NL->code, I am sure that there are other baselines other than Wei et al., 2019. For example:\n\n1. Iyer et al., Mapping Language to Code in Programmatic Context, 2018\n2. Iyer et al., Learning Programmatic Idioms for Scalable Semantic Parsing, 2019\n3. Yin et al., \"TRANX ..\", 2018\n4. Shin et al., \"Program Synthesis and Semantic Parsing with Learned Code Idioms\", 2019\n\nand other semantic parsing papers.\n\n* Datasets - in the program synthesis task, the authors created a new dataset. Can the authors use existing datasets, where previous work already tuned their baselines on?\nIn the code generation task, aren't there any other datasets, with hopefully more than 11K examples?\n\n* Analysis - the authors do perform some analysis in Section 5.2, but it still feels that it is unclear why the model works. For example, can the VQ-VAE be simply VAE or AE? Are all the losses important? What I am looking for is a simpler architecture that achieves similar results using the same general idea.\n\n### Questions to Authors ###\n1. How can two Transformers have only 15M parameters? What would happen if RobustFill-Transformer used a full-sized Transformer-base?\n2. Can we have an additional baseline that improves diversity in RobustFill, to reject the hypothesis that the VQ-VAE simply increases the diversity of solutions? I am not sure how.\n3. The phrase \"self-supervised\" appears 5 times throughout the paper. Is it really self-supervised? The self-supervised part is that the encoding of the I/O examples is compared with the encoding of the program. Aren't the pair (program, specification), in fact, supervision?\n4. Can the approach work with a code generator that does not generate the code as a sequence of tokens, such as Maddison & Tarlow (ICML 2014), Yin & Neubig (ACL 2017), Brockschmidt et al. (ICLR 2019), or Alon et al (ICML 2020)? or is the proposed approach limited to only \"textual\" code generators?\n\n### Minor questions and comments ###\n* Related work, paragraph2: \"these works does not\" -> \"these works do not\"\n* It would help following if Figure 2 was in the same page that it is referenced\n* It would be easier to read the appendix and refer it if the authors could append the appendix after the references, in the same PDF, instead of attaching it as a separate zip.\n\n====== Post discussion comments ======\n\nAfter reading all reviews, responses, and discussions, I still do not see evidence that the model has learned a \"high-level plan\", which is the main claim of the paper.\n\nI agree that most deep learning models are not interpretable, but most papers do provide some qualitative/anecdotal/generality/strong empirical evidence to support their claims. In this paper, I do not see such strong evidence.\n\nMy main concerns:\n\n1. Is it possible that the 2-stage search simply increases the diversity of solutions? \nIf there is an empirical improvement, I would like to understand the simplest explanation (\"Occam's razor\"). If the main contribution is diversity, I would expect the authors to spell it out clearly.\n\nFurther, if the main contribution is diversity, maybe there are much simpler and general ways to achieve diversity (e.g., diversity inducing versions of beam search), that can be applied to different architectures (i.e., not coupled with VQ-VAE).\n\nI feel that my question \"Can we have an additional baseline that improves diversity in RobustFill, to reject the hypothesis that the VQ-VAE simply increases the diversity of solutions?\" was not answered by the authors.\n\n2. The 2-stage search is a general approach, but the paper did not convince me that it is useful to settings beyond the FlashFill task, and for models other than the textual approach where programs are generated as text.\n\nIf the authors claim that their approach allows \"high-level planning\", I would expect to see that it works across different models / tasks / datasets / settings.\n\nMinor: I do not agree with the authors that this is self-supervised. I think that the paper uses the term \"self-supervised\" incorrectly.\n\n", "title": "Interesting paper, but very unclear and evaluation is lacking", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "hmxE-zpZURs": {"type": "rebuttal", "replyto": "zq4bt_0z-gz", "comment": "We have updated our submission with a comparison to SketchAdapt [1], which is a recent work that also proposed a form of two-level search for program synthesis. Instead of using a learned latent vocabulary, SketchAdapt expands the program vocabulary with an additional HOLE token; the top-level search generates a program with holes, and the bottom-level search does enumerative synthesis of partial programs to fill in the HOLE token. The paper uses a slightly modified DSL that is more amenable to enumerative synthesis. We had trouble getting SketchAdapt to perform as well in our DSL implementation, so instead, we ran our Latent Programmer method on data generated by SketchAdapt's modified DSL (using code by the authors), and compared our results to the ones in Figure 3 of the paper. We were able to improve upon SketchAdapt even in this modified DSL:\n\nMethod: B=50, 100 accuracy\n\nSketchAdapt: 63, 64\n\nLatent Programmer: 64, 67\n\n[1] Nye et al. (2019) https://arxiv.org/pdf/1902.06349.pdf", "title": "Added Baseline to the Paper"}, "GIFzK-Qr4-M": {"type": "review", "replyto": "zq4bt_0z-gz", "review": "Edit: I have increased my score to 7.\n\n\nThis paper introduces a novel program synthesis system called the Latent Programmer, which uses discrete latent codes as a representational scheme to solve program synthesis problems in two domains: string transformations from examples and code generation from language descriptions.\n\nStrengths:\n\n-The paper is relatively clear.\n\n-The approach is novel.\n\n-The results seem to support the claim that this model outperforms baselines (although it would help to report the results of multiple runs with standard error).\n\n-The relative simplicity of the approach is a plus; it doesn't seem that it would be terribly difficult for a researcher to adopt this technique to a new problem.\n\nWeaknesses:\n\nI think the baselines/ablations could be more complete. For example, it seems that the gains over the RobustFill baselines could be due to any of 3 factors: 1) use of discrete representations 2) the use of an autoencoding loss, or 3) the ability to search through latent representations at test time.\n\nUnless I'm mistaken, compared to LP, the transformer RobustFill baseline differs in terms of both (1) and (2): RobustFill does not use discrete latent codes, and it does not use the autoencoding or latent prediction losses. As written, the paper seems to assume that (1) is the primary reason for the performance difference (\"[the transformer RobustFill model] can also be considered of an ablation of our LP model without latent codes]\"). However, I think these two factors need to be better disentangled, in order to determine which contributes most to the performance. Can a RobustFill model be trained with an additional auto-encoding loss, so that its loss function is more analogous to LP? Similarly, how might a continuous latent variable model, such as a VAE, perform on the string editing tasks?\n\nSimilarly, it seems there is evidence that (3) is an important factor: in Figure 5, when doing a beam search of size 10, but only searching in the decoder space and keeping the latents fixed (L=1), the performance seems identical to the transformer RobustFill baseline. LP seems to beat baselines with B=1. What are the results for B=100 and L=1?\n\nI think that disentangling these factors would really strengthen the paper, and could also be of large value to the neural program synthesis community.\n\nSummary:\n\nI think this is an interesting line of work with promising results. However, I do think that a baseline which uses an autoencoding loss but does not use discrete latent codes is an important ablation to perform. I therefore recommend a weak accept, and I'd be willing to raise my score if my concerns about baselines were addressed.", "title": "Promising neural program synthesis approach. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HtsiRzHZ1t": {"type": "rebuttal", "replyto": "xGgcT9U8mdI", "comment": "Thank you for the thoughtful reply. You raised some important concerns that we would like to address below.\n\n(1) \u201cThe RobustFill+VAE results show\u2026 two-level search is not the main contribution source.\u201d\n\nRespectfully, we believe that this does not follow. The main claim in our paper is that two-level search can specifically help when we use a learned discrete representation, because we can use combinatorial search techniques like beam search at the top level. The RobustFill+VAE ablation directly shows that a different, natural method for doing two-level search in continuous space (sampling at the top level) does not work as well as our proposed two-level search in discrete space. Thus, we would argue that these results strengthen the arguments in our paper. \n \n(2) \"So, the quantization (VQ) is what gives the highest gain, and not the two-level search?\"\n\nThis is a reasonable concern, but we are not sure how it is possible at a conceptual level to do VQ without two level search. If you do VQ, then a latent sequence must be generated at test time somehow. Then it seems to us that any method for doing so could be interpreted as two level search. (RobustFill+VAE is the other ablation, two level search without VQ, and that works worse). \n\n(3) What is the evidence that the two-level search helps?\n\nGreat question. We suggest that this evidence appears in Figure 5, where we vary the amount of search done at the top level vs the second level. In the L=1 row, we generate one latent sequence greedily, then do beam search of size 10 over programs. For L=3, we generate 3 latent sequences by beam search, and then for each of these, we generate 3 programs, yielding 9 programs in total. We see that L=3 outperforms L=1, which we take as direct evidence that the two-level search helps.\n\n(4) \u201cCan the authors take the exact same generated programs\u2026\u201d\n\nThe methodology from prior work is to randomly sample new programs from the DSL at each minibatch during training, and to sample new programs during evaluation. This is what Devlin et al (2017) do, as well as the other work that we cite (Nye et al. 2019, Balog et al. 2020). As these programs are sampled online, these authors do not make the sampled programs publicly available.\n\n(5) \"To me, it doesn't look like diminishing returns actually.\"\n\nThat's a good point, we agree. That said, we would say that: 1. given the slopes of the learning curves in Figure 3(a), it is not at all clear to us that training larger models would provide more convincing evidence about the difference between the methods, and 2. our largest models are ~20M parameters, which is a similar order of magnitude as recent work in the program synthesis (Balog et al. 2016, Devlin et al. 2017, Nye et al. 2019, Balog et al. 2020). Model sizes are different in program synthesis vs NLP.\n", "title": "Response to AnonReviewer2's Concerns"}, "WI4oqSPGloK": {"type": "rebuttal", "replyto": "NibCYzKxZbJ", "comment": "Thank you for your review. We would like to address the concerns you had below.\n\n(1) \u201c...paper is very difficult to read and follow.\u201d\n\nWe revised the paper following your comments regarding the clarity of our work. Namely, we moved the related work section to after our method is introduced, and fixed your comments about components of our work that are hard to follow. We hope this makes our approach easier to understand.\n\n(2) \u201c...does not give much intuition .. to why does the proposed approach work\u201d\n\nWe can try to give additional intuition for why we believe the two level search is helpful, and have updated the introduction of our revised work to make this more clear. To achieve a given task, often a programmer starts by specifying high-level pieces as a plan, and then filling in the details of each piece. For instance, a plan could be to \u201cextract the first name, then the last initial\", without working out full details about each step. We propose to use a sequence of tokens, called \"discrete latent codes\", to represent such plans. Instead of having a fixed dictionary of plans/codes, we let the model discover and learn what are the useful plans and how to convert a specification into latent codes. Being discrete, at inference time, we get advantage of employing combinatorial search techniques over a smaller space of possible high-level plans and then over programs conditioned on the code as a \u201cplan\u201d, in a two-level procedure.\n\nAlthough the model needs to generate a program eventually, we still argue that the top-level search can help to organize the lower-level search over programs. For example, the model could be confident that it needs to extract the last initial, but is less sure about whether it needs to extract a first name or not. By making a one-token change to the latent sequence, the search procedure can explore potentially many different alternative programs that do different things in the beginning. Whereas in traditional single-level search, the model would need to explore different multi-token prefixes of the alternatives. This is hard to achieve in limited budget beam search, which is notorious for producing a low diversity of solutions, as observed in NLP.\n\nAnother significant advantage is in the ability of our two-level search to leverage structure. For example, in long programs where similar blocks are repeated often, our two-level search can first capture this structure, which would greatly reduce how much the low-level search over programs would have to do. This is most evident in the fact that our method synthesizes long, complex programs much better than baselines that only search over programs. \n\n\n(3) \u201cCan the authors use existing dataset\u2026\u201d\n\nWe use existing datasets and training sets. \nIn the string editing domain, we follow the methodology of prior work of randomly generating programs from a fixed DSL (Devlin et al., 2017, Nye et al., 2019, Balog et al., 2020), so we are not aware of a publicly available, centralized dataset of string editing tasks. In the code generation task, we used the public dataset also used in Wei et al., 2019; we were also concerned with the size and quality, but ultimately chose this dataset because of its availability. \n\n(4) \u201cWhat I am looking for is a simpler architecture that achieves similar results using the same general idea...\u201d\n\nThis is an excellent point. In the revised paper, we added two additional ablative baselines that replaced the VQ-VAE component with a generic AE and a VAE. The results are in the revised paper, and summarized in a table below:\n\nMethod: B=1, 10, 100 accuracy\n\nRobustFill [Transformer]: 47, 51, 61\n\nLatent RobustFill [AE] : 47, 50 , 60\n\nLatent RobustFill [VAE]: 46 , 51, 62\n\nLatent Programmer: 51, 57, 68\n\nWe are also working on comparing our method to SketchAdapt [1], which first generates a program sketch with holes then fills the holes in via enumerative synthesis. We will add another comment if we are able to get results during the rebuttal phase. \n\n[1] Nye et al. (2019) https://arxiv.org/pdf/1902.06349.pdf\n", "title": "Response to AnonReviewer2 (1/2)"}, "OH6e2ENnwbV": {"type": "rebuttal", "replyto": "zq4bt_0z-gz", "comment": "We would like to thank the reviewers for their insightful feedback. Following the advice of several reviewers, we provided an illustrative motivation of our two-level search idea in the introduction, clarified some aspects of our method, improved the comparison to prior work, and added two additional ablative baselines to show the importance of having a discrete latent space and being able to search over it via beam search. For our two baselines we replaced the VQ-VAE component of the Latent Programmer with a generic AE and a VAE. In the former only one latent sequence can be decoded per task, and in the later latent sequences can be sampled but not searched over.  The results are in the revised paper, and are also summarized below:\n\nMethod: B=1, 10, 100 accuracy\n\nRobustFill [Transformer]: 47, 51, 61\n\nLatent RobustFill [AE] : 47, 50 , 60\n\nLatent RobustFill [VAE]: 46 , 51, 62\n\nLatent Programmer: 51, 57, 68\n\nWe are also working on evaluating our method in the same domain as SketchAdapt [1], which first generates a program sketch with holes then fills the holes in via enumerative synthesis. Among prior work in neural program synthesis, we think that the method proposed in that work is most similar to our two-level search. We will add another comment if we are able to get results during the rebuttal phase.\n\n[1] Nye et al. (2019) https://arxiv.org/pdf/1902.06349.pdf", "title": "Updates to the Paper "}, "1ojA9_eqxk-": {"type": "rebuttal", "replyto": "WI4oqSPGloK", "comment": "(5) \u201cWhat would happen if RobustFill-Transformer used a full-sized Transformer-base?\u201d\n\nWe wanted to keep the number of trainable parameters consistent with the original RobustFill model, so we chose our embedding and hidden dimensions accordingly. We performed additional experiments in Section 5.2 where the model size was increased, but unexpectedly saw diminishing returns, so we do not anticipate using a full-sized Transformer would greatly impact performance.\n\n(6) \u201cCan we have an additional baseline that improves diversity in Robustfill\u2026\u201d\n\nThis is non-obvious to do, but we imagine a solution can involve adding a diversity penalty in the beam search. However, we view this as an orthogonal research problem.\n\n[1] Vijayakumar et al. (2018) https://arxiv.org/pdf/1610.02424.pdf\n\n[2] Kool et al. (2019) https://arxiv.org/pdf/1903.06059.pdf \n\n(7) \u201cIs it really self-supervised?\u201d\n\nWe use the term \u201cself-supervised\u201d because we effectively have two supervised objectives, one that depends on the output of the other. We do not feel strongly about whether to call it self-supervised or supervised and would be willing to change it if strong opinions did exist.\n\n(8) \u201cCan the approach work with a code generator that does not generate the code as a sequence of tokens.\u201d\n\nChanging the code generation from an autoregressive sequence of tokens is an orthogonal complementary direction. Our goal was to compare two-level search in neural program synthesis against single-level search, and chose the generation framework of the baseline RobustFill to do so. A different decoder (i.e. Tree-LSTM) can be co-opted into the model architecture instead of a Transformer decoder.\n", "title": "Response to AnonReviewer2 (2/2)"}, "cKN7zUF6hug": {"type": "rebuttal", "replyto": "proGf20QMNf", "comment": "(7) \u201cBaseline models are inadequate.\u201d\n\nThe crux of our work is in showing that two-level search over latents and programs can outperform traditional search over just programs. We kept the search algorithm (beam search) constant across all baselines; many works, such as Balog et al., propose alternative methods of search than beam search that can easily be co-opted. \nHowever, we did add two additional ablative baselines that replaced the VQ-VAE component with a generic AE and a VAE to the revised version of the paper. We are also working on comparing our method to SketchAdapt [1], which first generates a program sketch with holes then fills the holes in via enumerative synthesis. We will post a new comment if we are able to get results during the rebuttal phase.\n\n[1] Nye et al. (2019) https://arxiv.org/pdf/1902.06349.pdf", "title": "Response to AnonReviewer1 (2/2)"}, "proGf20QMNf": {"type": "rebuttal", "replyto": "Kuuft1LDVNO", "comment": "Thank you for the detailed feedback. We would like to address some important concerns that you raised with the work.\n\n(1) \u201cthis paper heavily relies on Kaiser et al.\u201d \n\nIn our view, the novelty of our work lies not in the model architecture (we agree those differences are minor), but in how we use the latent codes to perform search. We propose a two-level search, one over a high-level, compact, and learned latent space and the other over the larger program space, in order to improve accuracy in program synthesis tasks. This involves learning a discrete latent code so that beam search can be used on both levels of search. To our knowledge, this is a novel strategy for learning to search.\n\nThus, the distinguishing feature of our work is in casting program synthesis as a two-level search problem; using a VQ-VAE (as Kaiser et al.) enables us to learn the first level. We agree that the mechanism that we use to learn latent codes relies heavily on Kaiser et al. However, we note that their work work leverages a VQ-VAE to learn a discrete latent state in order to reduce the decoding latency for text generation tasks, not as a way of better exploring the output space during search. \n\nWe tried to adequately cite the Kaiser et al. work (e.g. we cited it in the introduction) but perhaps we could have been more explicit in detailing the differences between their work and ours. We added a more detailed discussion in the related work section of our revised version, and clarified that the key contribution of our work is to propose two-level search for program synthesis. \n\n(2) \u201cthe decoder can just learn from the input X and disregard the VQ-VAE space...\u201d\n\nThanks for the reference. We were not aware of the term \"bypassing phenomenon\" but we also observed this phenomenon in early versions of our method, where the latent codes were ignored during decoding. To avoid this, we added a pre-training step where the true program is initially passed instead of the latent states so reasonable gradients can be passed through the latent states in the decoder. After making this change, we know that the decoder is not disregarding the latent space because it significantly outperforms Robustfill empirically (57% with latent vs 51% without). We explain this at the end of Section 3.2 and have also added the reference in the revised version. \n\n(3) \u201cA natural way of having a real discrete bottleneck is by reinforcement learning\u2026\u201d\n\nRL is definitely an appropriate framework, as program synthesis can (like many other applications) be cast as an MDP. We chose a supervised approach simply because there is little prior work in applying RL to program synthesis. We feel that successfully applying RL would be an independent research direction, as it would involve handling additional concerns such as computational overhead, reward sparsity, etc. We view RL as complementary to this work, and can be used in lieu of VQ-VAEs to train and search the latent space, and in finetuning the model. \n\n(4) \u201chow do you know the number of codes during test?\u201d\n\nWe included an EOS token that is used to end latent sequences during inference. Following standard beam search conventions, we included a brevity penalty to favor shorter sequences. Empirically, we noticed that the latent codes during inference are also observed to be proportional to the length of the synthesized programs, even though the length of the program is not known beforehand. It makes sense to us that the latent predictor would produce output sequences of similar length during inference as it saw during training.\n\n(5) \u201cThere is no quantitative analysis on the learned code.\u201d\n\nThis is an important point. Please see (2) of our response to AnonReviewer 3 where we discussed interpretability of the latent code.\n\n\n(6) \u201cWhy do we need diversity for program generation?\u201d\n\nWe are concerned with a system that outputs a list of programs, where all programs in the list can be evaluated (not necessarily just the top-1). In such a scenario, it is often advantageous for the programs in the list to be relatively diverse so that some program in the list is more statistically likely to be correct. This is why we additionally measure diversity, and also why our evaluation chooses the program with the maximum BLEU score. \n", "title": "Response to AnonReviewer1 (1/2)"}, "qtXF0dRW-9V": {"type": "rebuttal", "replyto": "QyeHzCorz0d", "comment": "Thank you for your review. We address the questions that you had below.\n\n(1) \u201cWhy \u2026 use BLEU as metric, rather than functional correctness?\u201d \n\nThis choice follows previous work (Wei et al 2019). We agree that in principle functional correctness is a better metric, and we use functional correctness metric for evaluating synthesized programs for the string editing task. The reason why we use BLEU for the Python code generation task is because the dataset consists of publicly scraped functions on Github. These functions are often not executable due to several reasons such as missing dependencies in the library they were obtained from, having complex input objects as arguments, or absence of test cases. Therefore, we report the BLEU score instead, following Wei et al.\n\n(2) \u201cDo you observe certain interpretability of latent codes?\u201d\n\nOne of the strengths of our work is that individual tokens in the discrete latent code can have arbitrary meaning, allowing the latent representation to be very rich and expressive. However, this is also a weakness, as we did not perform any grounding on the latent code to induce interpretability i.e. make individual tokens explicitly map to high-level API calls. The most we could do is provide a qualitative analysis via examples in the main paper and appendix. From the examples, we noticed that in programs with a repetitive structure, the predicted latent sequence would often have repeated tokens; however, in less-obviously structured programs, the latent sequence became difficult to interpret. This is an important point though and we have added a discussion to describe this more thoroughly in the revision in Section 5.2. \n\n(3) \u201cIs the length of latent codes \u2026 proportional to the length of synthesized programs?\u201d\n\nDuring training, we always make the ground truth latent code proportional to the length of the programs. This was done mostly for simplicity and could be improved in future work. During inference, however, the estimated latent codes have no restriction on length, as the latent predictor will continue to generate tokens in the latent sequence until an EOS token is generated.\n", "title": "Response to AnonReviewer3"}, "mWUgIp3OUmC": {"type": "rebuttal", "replyto": "GIFzK-Qr4-M", "comment": "Thank you for your review. We would like to address the points you raised below. \n\n(1) \"Three factors...\"\n\nThe reason that we chose to use discrete latent states (factor 1 in your review) is precisely because this makes it easier to search through latent factors at test time (3) using combinatorial search. So we believe that (1) and (3) are strongly coupled with each other. Searching over a continuous latent representation is much more challenging to do; though it has recently been done for control in robotics, it is unclear how the methods generalize to program synthesis [1]. \n\n(2) \"only searching in the decoder... seems identical to the transformer RobustFill baseline\"\n\nAs for why the L=1 case of our method performs similarly to the RobustFill baseline, our intuition is that if no search is performed over the latent representation, our proposed two-level search reduces to a single level, which is essentially similar to what the RobustFill baseline does. We imagine that the B=100 and L=1 base would also perform as well as RobustFill. \n\n(3) \"disentangling these factors would really strengthen the paper...\"\n\nThanks for the suggestion of disentangling these factors to clarify the advantages of our proposed approach. We performed additional experiments to disentangle the use of discrete latent states from the use of an autoencoder (or (1) and (2) in your review). As you suggested, we have added two additional ablative baselines that replace the VQ-VAE of the Latent Programmer with either a generic AE or a VAE. The results are in the revised paper, and are also summarized in below:\n\nMethod: B=1, 10, 100 accuracy\n\nRobustFill [Transformer]: 47, 51, 61\n\nLatent RobustFill [AE] : 47, 50 , 60\n\nLatent RobustFill [VAE]: 46 , 51, 62\n\nLatent Programmer: 51,  57, 68\n\nBoth those baselines have a similar autoencoding loss but perform similarly to the Robustfill baseline; this is because we cannot perform search over the latent representation.\n\n[1] Watter et al. (2015) http://papers.neurips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf", "title": "Response to AnonReviewer4"}, "Kuuft1LDVNO": {"type": "review", "replyto": "zq4bt_0z-gz", "review": "This paper proposes a VQ-VAE approach for program synthesis (generating a program from specifications, either input-output pairs or natural language description). Generally speaking, a VQ-VAE learns an autoregressive discrete latent code in addition to traditional Seq2Seq learning, and perform beam search on both latent code and output programs. \n\nExperimental results show that the model outperforms three baseline systems on two tasks. \n\n---\nI have major concerns on the nature of the VQ-VAE model. \n\n\n1) First of all, this paper heavily relies on Kaiser et al. (ICML'2018), except that the decoder of this paper is autoregressive and that this paper proposes beam search on the latent sequential discrete codes. \n\nHowever, this paper has very light citation on Kaiser et al. (2018). The authors should be more honest about previous work and make direct comparison on the difference. What is taken from previous work? What is an adaptation? What is an extension?  \n\nThe current writing shows that this paper has a heavy development on the model, when in fact, it's mostly taken from previous work. \n\n2) The author claims that VQ-VAE serves as a discrete bottleneck. However, I strongly disagree with this. \n\nThe decoder in this paper is well aware of the input by \"TransformerDecoder(Y', E)\" in Eq 4, where the E = TransformerEncoder(X). \n\nSo, the decoder can just learn from the input X and disregard the VQ-VAE space, despite a few semantic losses imposed on the latent code (latent prediction and end-to-end in Eq 5). Since the VQ-VAE latent space is in addition to a traditional Seq2Seq training, it cannot serve as a bottleneck/regularization.\n\nThis is known as the \"bypassing phenomenon\" in previous work:\n\nBahuleyan et al., Variational attention for sequence-to-sequence models, 2018. \n\nThe authors may want to explain why their VQ-VAE would not suffer from the bypassing phenomenon. \n\nNote: the bypassing phenomenon is actually different in  Kaiser et al. (2018). Their decoder is non-autoregressive, so their sequential discrete latent space can provide autoregressive information. But in this work, the decoder is autoregressive, which can simply learn from X directly. \n\n3) What's the real benefit of modeling the discrete latent codes by VQ-VAE? A natural way of having real discrete bottleneck is by reinforcement learning. There lacks comparison and discussion. \n\nNote: we all know RL systems are difficult to learn, but the auxiliary losses is Eq 5 can all applied to RL, too. You may also do pre-training or relaxations for RL. \n\n4) The latent codes are generated in an autoregressive fashion. During training, the number of latent codes is ceiling(T/(2^l)). But how do you know the number of codes during test? Did you include an EOS token for such autoregressive generation? If yes, how easy is it to learn the precise semantics of EOS without direct supervision signal?\n\n5) There is no quantitative analysis on the learned code. While there is an example, it is inadequate. We have no measure on how typical is the shown example.\n\n \n---\n\nI also have major concerns on experiments.\n\n6) The evaluation metrics are peculiar. For example, distance n-grams are used to evaluate diversity. We understand diversity is important for natural language generation, but why do we need diversity for program generation? \n\nThe BLEU is computed by the best BLEU score among the output beams, but increasing the beam size may not improve top-beam performance.\n\n7) Baseline models are inadequate. For the example-to-program generation, the authors only compared with Seq2Seq with LSTM or Transformer. There has been other efforts on search-based program synthesis, for example, Balog et al. (2017, 2020). I'd like to see a comparison, and what's the further improvement when they are combined (as claimed by this paper)?\n\nIn code generation from description, the authors only compared with two models in Wei et al., 2019 and two variants of Seq2Seq. But there could be more benchmarked datasets, like Hearthstone, spider, and other semantic parsing datasets in the old days. \n\n---\n\nMinor: \n\nThe exponential moving average is not proposed in Roy et al. (2018). It's proposed in Appendix A of the original VQ-VAE paper. \n\n---\n\nIn short, the discrete latent space beam search appears to be some interesting idea. But I have concerns on the soundness of this paper. \n\nIt is also noted that there's no code or output available. \n\n\n", "title": "nature of VQ-VAE?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "QyeHzCorz0d": {"type": "review", "replyto": "zq4bt_0z-gz", "review": "Summary: This paper proposes a two-level hierarchical program synthesizer, Latent Programmer, which first predicts a sequence of latent codes from given input-output examples, and then decodes the latent codes into a program.  The sequence of latent codes can be viewed as a high-level synthesis plan, guiding the subsequent low-level synthesis. Latent Programer significantly outperforms RobustFill on string manipulation tasks and achieves state-of-the-art results on Python code generation tasks. \n\nQuality: The paper presents a novel program synthesis idea and the evaluation is promising and convincing.\n\nClarity: The writing provides enough background and explains the main idea in a very clear manner.\n\nOriginality: The application of Vector Quantized Variational Autoencoder for a two-level hierarchical synthesis is quite novel. \n\nSignificance: This work shows that a promising hierarchical learning approach for program synthesis. Its effectiveness motivates many future explorations in this direction.\n  \n\nQuestions: \n\nQ1: Why Python Code generation tasks use BLEU as the metric, rather than functional correctness?\n\nQ2: Latent codes are motivated as a \"high-level plan\"? Do you observe certain interpretability of latent codes?\n\nQ3: Since the lengths of synthesized programs are different for different tasks, it might be good to have a task-specific length of latent codes. The authors do show that varying the length of latent codes could affect performance. Is the length of latent codes (always) proportional to the length of synthesized programs?\n", "title": "Interesting idea and convincing results", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}