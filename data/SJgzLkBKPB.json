{"paper": {"title": "Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution", "authors": ["Nikaash Puri", "Sukriti Verma", "Piyush Gupta", "Dhruv Kayastha", "Shripad Deshmukh", "Balaji Krishnamurthy", "Sameer Singh"], "authorids": ["nikpuri@adobe.com", "dce.sukriti@gmail.com", "piygupta@adobe.com", "dhruvkayastha@iitkgp.ac.in", "shripad@smail.iitm.ac.in", "kbalaji@adobe.com", "sameer@uci.edu"], "summary": "We propose a model-agnostic approach to explain the behaviour of black-box deep RL agents, trained to play Atari and board games, by highlighting relevant portions of the input state.", "abstract": "As deep reinforcement learning (RL) is applied to more tasks, there is a need to visualize and understand the behavior of learned agents. Saliency maps explain agent behavior by highlighting the features of the input state that are most relevant for the agent in taking an action. Existing perturbation-based approaches to compute saliency often highlight regions of the input that are not relevant to the action taken by the agent. Our proposed approach, SARFA (Specific and Relevant Feature Attribution), generates more focused saliency maps by balancing two aspects (specificity and relevance) that capture different desiderata of saliency. The first captures the impact of perturbation on the relative expected reward of the action to be explained. The second downweighs irrelevant features that alter the relative expected rewards of actions other than the action to be explained. We compare SARFA with existing approaches on agents trained to play board games (Chess and Go) and Atari games (Breakout, Pong and Space Invaders). We show through illustrative examples (Chess, Atari, Go), human studies (Chess), and automated evaluation methods (Chess) that SARFA generates saliency maps that are more interpretable for humans than existing approaches. For the code release and demo videos, see: https://nikaashpuri.github.io/sarfa-saliency/.", "keywords": ["Deep Reinforcement Learning", "Saliency maps", "Chess", "Go", "Atari", "Interpretable AI", "Explainable AI"]}, "meta": {"decision": "Accept (Poster)", "comment": "A new method of calculating saliency maps for deep networks trained through RL (for example to play games) is presented. The method is aimed at explaining why moves were taken by showing which salient features influenced the move, and seems to work well based on experiments with Chess, Go, and several Atari games. \n\nReviewer 2 had a number of questions related to the performance of the method under various conditions, and these were answered satisfactorily by the reviewers.\n\nThis is a solid paper with good reasoning and results, though perhaps not super novel, as the basic idea of explaining policies with saliency is not new. It should be accepted for poster presentation.\n"}, "review": {"K0df5XHrRTd": {"type": "rebuttal", "replyto": "7_AytLUgQXr", "comment": "Hey, thanks for the comment. To generate their maps, we directly used the code they had made available with their paper. We observed that for some frames in Atari their approach generates focused saliency maps, while for others, it generates noisy saliency maps. In contrast, SARFA generates focused saliency maps for most frames. We selected illustrative frames from the gameplay to show this distinction. These look different from the ones in their paper because they chose some other illustrative frames. The full videos for the gameplay showing SARFA Saliency are uploaded to our repository along with the full videos generated by Greydanus et al.", "title": "Thanks for the comment"}, "rJx9cWb3YS": {"type": "review", "replyto": "SJgzLkBKPB", "review": "This paper proposes an algorithm for explaining the move of the agents trained by reinforcement learning (RL) by generating a saliency map.\nThe authors proposed two desired properties for the saliency map, specificity and relevance.\nThe authors then pointed out that prior studies failed to capture one of the two properties.\nTo combine the two components into a single saliency map, the authors proposed using the harmonic mean.\n\nThe experimental results demonstrated that the proposed saliency map successfully focused only on important parts while the other method tend to highlight some irrelevant parts also.\nThe authors also did a great job for evaluating the goodness of the saliency maps, by preparing a human annotated chess puzzle dataset.\n\nI think the paper is well-written, and the basic idea look reasonable and promising.\nThe experimental evaluations are well designed and the results look convincing.\nSaliency map for RL is not yet mature, and I expect to see further improvements (especially, more theoretically principled ones) follow this study.\n\n\n### Updated after author response ###\nThe response from the authors seem to be reasonable to me. I therefore keep my score.", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 1}, "SkekPRl2sS": {"type": "rebuttal", "replyto": "Bkeeh2vEoB", "comment": "We have uploaded another revision to the paper, here are the salient changes from the last revision:\n\n- Added saliency maps for deep RL agents for Chess in Appendix C.\n\n- Perturbed non-salient parts of the board to observe the effect on the actions, as an approach to evaluate the quality of the saliency map (Section 3.5).\n\n- Added more discussion in Section 5.\n\n- Minor additions to the description of the properties in Section 2.", "title": "Additional revision"}, "BylD_6l3sS": {"type": "rebuttal", "replyto": "HJe916UjsH", "comment": "Thank you for your comments. We have made some additional changes in the latest revision.\n\n> the measure does particularly well for generating saliency maps for Stockfish\n> might be quite different \u2026 for deep neural network RL agents. \n> v) The saliency method might be particularly suited for Stockfish (whose action-value estimates might be strongly influenced by human saliency and chess theory). \n> See whether the method still produces good results for other chess agents\n\nAdded saliency maps for a deep learning agent for Chess to Appendix C.\n\n> iii) Show that the saliency map stays roughly constant for seemingly irrelevant perturbations. \n> I would still like to encourage the authors to try and come up with some perturbations that should have very predictable effects on the saliency\n\nAdded results in Section 3.5 \n", "title": "Additional changes"}, "BJeXh9ehsH": {"type": "rebuttal", "replyto": "SJg5oTWtsB", "comment": "Thank you for your comment. Based on our initial evaluation, it is very challenging to derive a scalable approach purely based on using specificity and relevance as the \u201caxioms\u201d, due to the need to evaluate exponentially many subsets of features, made much worse due to the fact that many such perturbations will be invalid or not meaningful. However, this direction is definitely useful, and we will explore this in future work.\n\nIn regards to whether specificity and relevance are sufficient, and whether there are additional desired properties for saliency, our extensive experiments with humans for puzzle-solving and on the chess saliency dataset show that our approach (specificity + relevance) performs significantly better than existing approaches. There may be other additional properties though, especially in light of the discussion of the limitations in Section 5, that we will explore in future work.", "title": "Additional desired properties"}, "Hyeu8gpssS": {"type": "rebuttal", "replyto": "Hyxvkl6uoH", "comment": "Agreed, both perturbation and gradient methods are useful for different use cases; we mention the difficulty in identifying meaningful perturbations in Section 5. \n", "title": "Perturbation and Gradient-Based methods"}, "Skx402YnYB": {"type": "review", "replyto": "SJgzLkBKPB", "review": "[score raised from weak reject to weak accept after rebuttal - on a more fine-grained scale, I would rate this paper now an accept (7), but not a strong accept (8), however since this year's scale is quite coarse, I'll stick with a score of 6]\n\nSummary\nThe paper proposes a new perturbation-based measure for computing input-saliency maps for deep RL agents. As reported in a large body of literature before, such saliency maps are supposed to help \u201cexplain\u201d why a deep RL system picked a certain action. The measure proposed in the paper aims at combining two aspects: specificity and relevance, which should ensure that the saliency map highlights inputs that are relevant for a particular action to be explained (specificity), and this particular action only (relevance). The paper shows illustrative examples of the proposed approach and two previously proposed alternatives on Chess, Go and three Atari games. Additionally the paper evaluates the method and the two previous alternatives on two interesting chess-tasks: chess-puzzles where human players were shown to be able to solve puzzles faster and with higher accuracy when given the proposed saliency map in addition, and evaluation against a curated dataset of human-expert saliency maps for 100 chess puzzles. \n\nContributions\ni) Novel, perturbation-based saliency measure composed of two parts. Main idea of specificity is (similar to Iyer et al. 2018) to use State-Action value function (Q-function) with a specific action, instead of State-Value function only. Main idea of relevance is to \u201cnormalize\u201d by taking into account change in Q-value for all other actions as well. Both parts are combined in a heuristic fashion.\n\nii) More objective/reproducible assessment of how saliency maps produced by different methods overlap with human judgement of saliency of pieces in chess. To this end: two experiments with human chess players/experts (puzzle solving, and expert-designed saliency maps).\n\nQuality, Clarity, Novelty, Impact\nThe paper is well written and most sections are easily understandable, though for some parts it might help if the reader is quite familiar with Chess/Go. The proposed saliency measure seems to address some shortcomings of previously proposed measures - my main criticism is that the construction of the measure seems rather ad-hoc and heuristic. It would have been great to formally define specificity and relevance (e.g. in an information-theoretic framework as Sparse Relevant Information) and then derive a suitable measure that is shown to satisfy/approximate the formal definitions. At least, there is one ablation study that justifies parts of the heuristic construction to some degree. \nThe proposed measure seems to do reasonably well on board-game domains, in particular chess. However it might also be the case that the measure does particularly well for generating saliency maps for Stockfish (which is the agent that happens to be evaluated in the chess domain), which might be quite different from previously reported methods that have been designed for deep neural network RL agents. The illustrative examples on Atari and Go do not allow for a statistically significant judgement of the quality of the proposed method.\n\nOn a conceptual level, a bigger issue (of many saliency methods in general, but the criticism also applies to the paper) is that the \u201cexplanations\u201d drawn from saliency maps are rarely evaluated rigorously. The paper makes a nice attempt by trying to establish some \u201cground-truth\u201d saliency in chess using humans to increase the degree of objectivity, which I greatly appreciate. However, it remains unclear whether explanations that happen to coincide with human notions of saliency really are of higher quality for assessing how an artificial system makes its decisions. The main goal of explainability/interpretability methods must be to come up with testable hypotheses that tell us something about how the artificial system makes its decisions in novel/unseen situations. The goal is not to explain a decision mechanistically after the fact (which is trivial, given a deterministic, differentiable feed-forward system), but to come up with non-trivial explanations that extrapolate/generalize. Specificity and Relevance are probably important ingredients of such explanations, but I think it\u2019s important to formalize them properly first. Currently I am in favor of suggesting a major revision of the work, but I am happy to reconsider my decision based on the rebuttal and other reviewers\u2019 assessment. Having said that, I do want to reiterate that I think it is great that the authors included some ablation studies and measures of \u201cusefulness\u201d of the saliency method.\n\nImprovements / Major comments\ni) Formally define specificity and relevance (e.g. as sparsity and compression?). Ideally derive a saliency measure based on these formal definitions.\n\nii) Show how saliency maps (of the same situation) change when producing explanations for different actions. I assume that currently the illustrative examples only show the action with the highest Q-value, what changes when using e.g. a less likely action?\n\niii) Show that the saliency map stays roughly constant for seemingly irrelevant perturbations. In particular, using the chess-dataset with expert annotations, apply various kinds of perturbations to non-salient pieces (e.g. removing them, swapping them for other pieces and potentially moving them in irrelevant ways) and see whether the AUC stays roughly constant.\n\niv) Apply non-relevant perturbations to the salient pieces. I.e. take the same puzzle/scene and move it across the board, does the saliency-map move in (roughly) the same way.\n\nv) The saliency method might be particularly suited for Stockfish (whose action-value estimates might be strongly influenced by human saliency and chess theory). See whether the method still produces good results for other chess agents (ideally trained without human heuristics or data). If this is hard to do for chess, think about a different application where this is easier.\n\nvi) Add a section that discusses current shortcomings and caveats with the method, and saliency maps for explainability in general.\n\nvii) (Experimental details) For each domain, please explain the perturbations that you used (removing pieces in Chess/Go, blurring pixels in Go, anything else e.g. replacing pieces?). In all experiments, was it always the action with the highest Q-value that was being explained?\n\n\n\nMinor comments\n\na) Table 1: (add error-bars) What is the variance across players? Are the results for the proposed method statistically significant?\n\nb) Chess saliency dataset. Are the expert saliency ratings binary? Why not have multiple degrees of saliency?\n\nc) Would the Greydanus et al. 2018 approach deliver similar results when using a threshold to cut off low-saliency inputs?\n\nd) Why is the saliency map in 3.4 binary (pieces are either salient or not)? How was the binarization threshold chosen? What would the non-binary saliency maps look like?\n\ne) Please provide all experimental details (in the appendix) that are necessary to reproduce the experiments. Referring to a code-repository is not a replacement for describing the methods in detail.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "S1xBnuYNir": {"type": "rebuttal", "replyto": "rJx9cWb3YS", "comment": "We would like to thank the reviewer for the positive feedback. We will work on further improving the quality of the paper over subsequent versions. ", "title": "Thank you for the positive feedback"}, "Byx0WCDViS": {"type": "rebuttal", "replyto": "Skx402YnYB", "comment": "We would like to thank the reviewer for their comments, many of the comments are very useful and will address them in the following revisions. \n\n> saliency maps are rarely evaluated rigorously. \n\nIndeed, evaluating saliency maps is a very challenging problem. Thank you for appreciating our attempt at evaluation; note that it is fairly comprehensive compared to most of the published approaches. We included the lack of generalization in Section 5.\n\n> The main goal of explainability/interpretability methods must be to come up with testable hypotheses that tell us something about how the artificial system makes its decisions in novel/unseen situations.\n\nOne of the goals of interpretability is indeed to describe the extrapolation/generalization of the agent, however, the goals of interpretability are multifold. Just knowing the reasons behind a single prediction/action can be quite useful, as we show. Note that this is aligned with the existing literature in \u201clocal explanations\u201d, that has been shown to be useful for interpreting, debugging, developing trust, and for the users learning the task. However, we agree with the limitations, and include it in Section 5.\n\n> iv) Apply non-relevant perturbations to the salient pieces. I.e. take the same puzzle/scene and move it across the board, does the saliency-map move in (roughly) the same way.\n\nUnfortunately, it is not easy to identify non-relevant perturbations, since moving the relevant pieces across the board, for example, may completely change the underlying strategy that is applicable, making different pieces salient or not salient.\n\n> vi) Add a section that discusses current shortcomings and caveats with the method, and saliency maps for explainability in general.\n\nWe have added such a section (Section 5).\n\n> vii) (Experimental details) For each domain, please explain the perturbations that you used (removing pieces in Chess/Go, blurring pixels in Go, anything else e.g. replacing pieces?). \n> e) Please provide all experimental details (in the appendix) that are necessary to reproduce the experiments. Referring to a code-repository is not a replacement for describing the methods in detail.\n\nWe have added such a section (Appendix A).\n\n> In all experiments, was it always the action with the highest Q-value that was being explained?\n\nYes, we only explain the actions with the highest Q-value, since greedy policies are often used at test time. However, we have also included examples of saliency maps for other actions, in Appendix B.\n\n> b) Chess saliency dataset. Are the expert saliency ratings binary? Why not have multiple degrees of saliency?\n\nYes, the saliency ratings are binary. Different degrees of saliency is harder to obtain. For instance, in a Chess position, it is much easier for experts to label whether pieces are important than to estimate (or agree on) the degree of importance. \n\n> c) Would the Greydanus et al. 2018 approach deliver similar results when using a threshold to cut off low-saliency inputs?\n\nWe feel that the \u201cthresholding\u201d should be part of the explanation technique; they should provide a sparse representation of which features are relevant (all of the baselines do already zero out irrelevant features). It is difficult to evaluate different thresholding amongst the features that they have identified as salient, since the threshold itself may be crucially dependent on the state or the value function, and it is not possible to search the different threshold values at each state. Nonetheless, we do provide an evaluation that is independent of the effect of the threshold (using ROC curves) but instead depends on the ranking of the features.\n\n> d) Why is the saliency map in 3.4 binary (pieces are either salient or not)? How was the binarization threshold chosen? What would the non-binary saliency maps look like?\n\nWe have updated the saliency maps in Section 3.4 to the original non-binary saliency maps output by our approach.\n", "title": "Response to the comments"}, "rJxZZaPEjS": {"type": "rebuttal", "replyto": "rJlbK2tLOB", "comment": "We have expanded the review of these approaches in the revised Section 4, including the connection of the axioms of attribution to perturbation-based techniques. More empirically speaking, Greydanus et al compared their approach against a gradient-based approach, and they noted in their paper that such saliency maps \u201ccan be difficult to interpret\u201c, \u201cchoose perturbations which lack physical meaning\u201d, and \u201cmay move it off from the manifold of realistic input images\u201d. In our paper, we show that our explanations are more useful than Greydanus et al. Note that the gradient-based techniques not only need white-box access to the agent, but even with white-box access, some agents may not be fully differentiable, such as Stockfish for chess.", "title": "Comparison to gradient-based approaches"}, "Bkeeh2vEoB": {"type": "rebuttal", "replyto": "SJgzLkBKPB", "comment": "Thank you for the helpful and constructive comments on our paper. In order to address some of the comments by the reviewers, we have uploaded a revision of the paper, addressing the following:\n\n- Added more background information for readers unfamiliar with the Chess notation as a footnote in the Introduction when we first introduce Figure 1.\n\n- Updated the saliency maps in Figure 7 and Section 3.4 to show the relative saliency (instead of the binarized map as in the original version).\n\n- Expanded the text on gradient-based approaches to saliency maps (non-RL) in Section 4 and their connection to our proposed work.\n\n- Added a discussion section describing the shortcomings of our approach, and of saliency maps  (Section 5).\n\n- Added more details about the experiment setup, in particular, the perturbation technique for each of the evaluations (Appendix A).\n\n- Added examples of the saliency maps for actions that were not the highest Q (in Appendix B), in particular, for the top three moves in a particular chess position. \n\nWe are currently carrying out a few more experiments for the next revision:\n- Evaluating and analyzing the saliency maps for deep RL agents for Chess.\n- Perturbing non-salient pieces for Chess, and evaluating how much the saliency map changes.\n", "title": "Revised version with additional content"}, "rJlbK2tLOB": {"type": "review", "replyto": "SJgzLkBKPB", "review": "Interpreting the policies of RL agents is an important consideration if we would like to actually deploy them and understand their behaviours. Prior works have applied saliency methods to DRL agents, but the authors note that there are two properties - specificity and relevance - that these methods do not take into account, and therefore result in misleading saliency maps. The authors define (and provide examples) of these properties, propose simple ways to calculate these (and like prior methods, relying on perturbations and therefore applicable to almost black box agents), and combine them neatly using the harmonic mean to provide a new way to calculate saliency maps for agents with discrete action spaces. While the improvements on Atari are hard to quantify, the results on chess and go are more interpretable and hence more convincing. The study using saliency maps to aid human chess players is rather neat, and again adds evidence towards the usefulness of this technique. Finally, the chess saliency dataset is an exciting contribution that can actually be used to quantify saliency methods for DRL agents. The proposed method is relatively simple, but is well-motivated and demonstratively (quantitatively, which is great work for general interpretability methods) better than prior methods, and in addition the authors introduce a new dataset + quantitative measure for saliency methods, so I would give this paper an accept.\n\nAlthough the authors motivate their choice of perturbation-based saliency methods as opposed to gradient-based methods, they should expand their review of the latter. As the technique the authors introduced is very general, it would be useful to know how it compares to the current state of research in terms of identifying properties that attribution methods should meet - a good example of this is integrated gradients (Sundararajan et al., 2017), which similarly identify \"sensitivity\" and \"implementation invariance\" as \"axioms\" that their method satisfies.", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 2}}}