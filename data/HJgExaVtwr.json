{"paper": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "summary": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "keywords": ["label noise", "semi-supervised learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes an algorithm for noisy labels by adopting an idea in the recent semi-supervised learning algorithm.\n\nAs two problems of training noisy labels and semi-supervised ones are closely related, it is not surprising to expect such results as pointed out by reviewers. However, reported thorough experimental results are strong and I think this paper can be useful for practitioners and following works. \n\nHence, I recommend acceptance."}, "review": {"NgsFwZ5fAxE": {"type": "rebuttal", "replyto": "keqS67sTCbi", "comment": "Hi Xinshao,\n\nThanks for your comments and sharing of your papers. We would like to note that the techniques used in DivideMix (i.e. warm-up, co-training, mixup) are commonly adopted in many previous works. Rather than being complex, DivideMix is a simple method that is easy to use.", "title": "Thanks but DivideMix is a simple method."}, "ryxKwK0jiH": {"type": "rebuttal", "replyto": "S1gKsV9EYS", "comment": "We thank Reviewer 2 for the insightful comments. We believe that our work can inspire new research ideas to explore the intersection between the areas of learning with noisy labels and semi-supervised learning.", "title": "Response to Reviewer #2"}, "HyxSBF0ssB": {"type": "rebuttal", "replyto": "HkeBKNcaKr", "comment": "We appreciate Reviewer 1 for the very helpful and constructive suggestions.  Following the suggestions, we have improved the paper to (1) emphasize the difference between the two types of loss corrections in Section 2.1, (2) include [1-2] into the related work section, and (3) add a training time analysis in Appendix D. Next we provide the computation time analysis.\n \nIn terms of inference time, our method does not introduce extra computation if a single model (e.g. theta1) is used for test, whereas the inference time is doubled if we use the ensemble of both models.\nIn terms of training time, Appendix D shows a detailed analysis. We first compare the total training time of DivideMix to several SOTA methods, using a single Nvidia V100 GPU. DivideMix (5.2 h) is slower than Co-teaching+ (4.3 h) but faster than P-correction (6.0 h) and Meta-Learning (8.6 h) which involve multiple training iterations. We also break down the computation time per-epoch for each operation in DivideMix, Co-Divide (Alg. 1, line 4-8) takes 17.2 seconds, MixMatch of data (Alg. 1, line 12-24) takes 16.0 seconds, whereas the model\u2019s forward-backward computation (Alg. 1, line 25-27) takes 12.5 seconds.", "title": "Response to Reviewer #1"}, "rJe2zKAosB": {"type": "rebuttal", "replyto": "SyxNO3Ne5S", "comment": "We appreciate Reviewer 3 for the recognition of this paper and the valuable comments. Next we response to the questions raised by the reviewer. \n \nQuestion 1: What is the value of \\tau in other experiments?\nResponse: As explained in Appendix B, we set the value of \\tau as 0.5 for other experiments.\n \nQuestion 2: I\u2019m also wondering if \\tau needs some decay during training, since deep NNs are gradually fitting noisy features during training.\nResponse: We find that the proposed DivideMix can prevent the network from fitting to label noise and keep higher loss for noisy samples. This is supported by Figure 3 in Appendix, which shows that the GMM improves in distinguishing clean and noisy samples as training proceeds. Therefore, we can keep a constant \\tau during training.\n \nQuestion 3: I\u2019m a bit concerned about efficiency. So how about the computation time?\nResponse: In terms of inference time, our method does not introduce extra computation if a single model (e.g. theta1) is used for test, whereas the inference time is doubled if we use the ensemble of both models.\nIn terms of training time, we have added a training time analysis in Appendix D. We first compare the total training time of DivideMix to several SOTA methods, using a single Nvidia V100 GPU. DivideMix (5.2 h) is slower than Co-teaching+ (4.3 h) but faster than P-correction (6.0 h) and Meta-Learning (8.6 h) which involve multiple training iterations. We also break down the computation time per-epoch for each operation in DivideMix, Co-Divide (Alg. 1, line 4-8) takes 17.2 seconds, MixMatch of data (Alg. 1, line 12-24) takes 16.0 seconds, whereas the model\u2019s forward-backward computation (Alg. 1, line 25-27) takes 12.5 seconds.\n \nQuestion 4: More details on experimental protocol may be needed: What kind of hyperparameter tuning was done? How many repeated runs? \nResponse: As explained in Section 4.1 and Appendix B, we keep most hyperparameters fixed across experiments and do light tuning on \\lambda_u (weight for unsupervised loss) on a per-experiment basis using a small validation set. We report results on single runs due to time concerns with the large number of experiments that we have conducted. Since all methods use the same training data, we believe that the results are fair for comparison. We thank the reviewer for this suggestion and would perform repeated runs in the future.", "title": "Response to Reviewer #3"}, "S1gKsV9EYS": {"type": "review", "replyto": "HJgExaVtwr", "review": "This paper proposed a method named DivideMix for learning with noisy labels, on top of the recent semi-supervised learning method MixMatch from Google. The idea is to model the per-sample loss distribution with a mixture model to dynamically DIVIDE the training data into (a labeled set with clean samples) and (an unlabeled set with noisy samples) and trains the model on both the labeled and unlabeled data in a semi-supervised manner.\n\nThe novelty is borderline. In the area of learning with noisy labels, it is known that SSL can work under this problem setting for several years, for example, the famous method \"virtual adversarial training\" from ICLR 2016 and a recent method \"smooth neighbors on teacher graphs\" from CVPR 2018. As a consequence, it is not surprising that the latest MixMatch can work as well, since MixMatch comes from mixup, virtual adversarial training and entropy minimization. This makes the novelty borderline. However, the significance may still be high according to the reported experimental results, and thus we may accept it to let more deep learning practitioners see the promising results.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 4}, "HkeBKNcaKr": {"type": "review", "replyto": "HJgExaVtwr", "review": "This paper proposes an algorithm that learns with noisy labels that achieves state-of-the-art results. Their algorithm tries to exploit the noisy samples by assigning a \u2018correct\u2019 label through MixMatch. It borrows the idea from both semi-supervised learning and learning with label noise. \n\nSuggestions:\n\n1. When the author talked about \u201ccorrect the loss function\u201d, there are in fact two types of corrections: the first type tries to correct the loss function by equally treating all the samples, e.g., classical Huber loss, or F-correction. The second type tries to either re-weight samples or separate clean and noisy samples explicitly, which also results in correcting the loss function. It would be more clear if Section 2.1 can emphasize the difference between the two. \n\n2. Also, there are other papers providing different but insightful ideas to label noise problem. The authors addressed the idea of using co-learning to avoid confirmation bias. However, it should be noted that this is not the only way to avoid confirmation bias, and their are other methods without using two networks [1-2], both providing some theoretical insights to the problem. It would be good to include them in the related work as well. \n\n3. I would like to see a comparison of running time other than the accuracy, understanding the efficiency of each algorithm is important from a practical perspective. \n\n[1] Learning with Bad Training Data via Iterative Trimmed Loss Minimization, Yanyao Shen, Sujay Sanghavi, ICML 2019.\n[2] Robust Learning from Untrusted Sources,Nikola Konstantinov, Christoph H. Lampert, ICML 2019\n\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "SyxNO3Ne5S": {"type": "review", "replyto": "HJgExaVtwr", "review": "This paper proposes a DivideMix framework for learning with noisy labels, where they first Co-Divide the training data into a labeled clean set and an unlabeled noisy set by modeling the per-sample loss distribution with GMM and using the small loss trick, then they exploit MixMatch to train the model on those labeled and unlabeled data in a semi-supervised manner. Experiments and comparisons with SOTA are provided, together with an ablation study.\n\nPros:\n-The paper bridges the area of learning with noisy labels with semi-supervised learning and proposes an interesting method to learn with noisy labels in a semi-supervised manner. The treatment proceeds from analyzing good unsupervised loss functions, improving the MixMatch and implementing thorough experiments.\n\n-The paper is clear and flows smoothly. The treatment is thorough, proceeding from designing algorithms, implementing experiments and an ablation study.\n\n-The impact of the method is a clear asset. Most existing label noise works focus on certain noise models, but this paper is more general and proposes an interesting method to learn with noisy labels in a semi-supervised manner, and the experimental results are promising.\n\n-The effort made on designing the confidence penalty for asymmetric noise is interesting.\n\nRemarks:\n-Sec 3.1: it seems that \\tau is an important hyperparameter for dividing the noisy data into labeled and unlabeled sets, but in Sec 4 I only see that \\tao is set to be 0.5 or 0.6 for CIFAR, what about other experiments? I\u2019m also wondering if \\tao needs some decay during training? For example, it may be 0.5 at early training epochs, but may be smaller at last, since deep NNs are gradually fitting noisy features during training.\n\n-Algorithm: the proposed method needs to train two networks simultaneously. During each epoch, it firstly divides the noisy data by modeling the per-sample loss distribution with GMM, and then do MixMatch with label co-refinement and co-guessing. I\u2019m a bit concerned about efficiency. So how about the computation time?\n\n-Experiments: more details on experimental protocol may be needed: what kind of hyperparameter tuning was done? How many repeated runs? It would be helpful to report the means and standard deviations based on repeated samplings.\n\nOverall take: this paper proposes a thorough treatment of learning with noisy labels in a semi-supervised manner, designing the algorithm and testing it empirically, which is an interesting and important contribution. My only concern is about the novelty since the small loss trick in label noise and the MixMatch approach in SSL are already explored by many recent studies, but to the best of my knowledge, this paper is the first to unify them to solve label noise problems.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "B1l5KLT-uS": {"type": "rebuttal", "replyto": "SJgK7RakdS", "comment": "Thanks for your comments! \nWe have indeed analyzed the divergence of the two networks using the networks' prediction discrepancy (on the same images) as an indicator. Our result shows that the two networks can stay sufficiently diverged because of multiple factors: different parameter initialization, different training data division, different mini-batch sequence, and different training targets. The iou of the labeled set is one of the causes, rather than a direct indicator of the network divergence. The iou would get smaller as training proceeds because both networks would get better at dividing clean and noisy samples.", "title": "Divergence analysis"}}}