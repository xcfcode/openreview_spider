{"paper": {"title": "Improved Architectures for Computer Go", "authors": ["Tristan Cazenave"], "authorids": ["Tristan.Cazenave@dauphine.fr"], "summary": "Improving training of deep networks for computer Go modifying the layers", "abstract": "AlphaGo trains policy networks with both supervised and reinforcement learning and makes different policy networks play millions of games so as to train a value network. The reinforcement learning part requires massive ammount of computation. We propose to train networks for computer Go so that given accuracy is reached with much less examples. We modify the architecture of the networks in order to train them faster and to have better accuracy in the end.", "keywords": ["Games", "Supervised Learning", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled."}, "review": {"HycPpS1Eg": {"type": "rebuttal", "replyto": "r13iPZkQg", "comment": "I added answers to all your comments in the revised paper.", "title": "answers"}, "rJHQpB14g": {"type": "rebuttal", "replyto": "ry6JoSAzg", "comment": "I added some more experiments to compare 13 and 20 layers networks.", "title": "new experiments"}, "r13iPZkQg": {"type": "review", "replyto": "Bk67W4Yxl", "review": "Dear author,\n\n1) Can you clarify whether the training and testing data you use is the same as any other papers (i.e. AlphaGo and DarkForest)? More specifically, is your best number of 48.485% directly comparable to the published 57% results for AlphaGo?\n2) Can you be more explicit about which architectures are new and which are previously published in your Figures and Tables? For example, it would be nice if figures 1-6 included citations to the papers where they were introduced (where applicable).\n3) How is your use of symmetries different from that of AlphaGo?\n4) What is the output of your network and what are the targets?The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.\n\nI found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:\n1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?\n2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?\n3) It is not at all surprising that ResNets do slightly better than vanilla CNNs on move prediction. I don't think this alone is enough for an ICLR paper. It would be good to see at least a comparison of several different variants of the network evaluated at actually playing Go, even if it's against other bots like GnuGo, Pachi, and Fuego.\n4) Are the differences between net_dark and your proposed networks after 20 iterations (Table 1) significant? \n\nPlease also see my original questions.", "title": "Pre-review questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkP7m8JEl": {"type": "review", "replyto": "Bk67W4Yxl", "review": "Dear author,\n\n1) Can you clarify whether the training and testing data you use is the same as any other papers (i.e. AlphaGo and DarkForest)? More specifically, is your best number of 48.485% directly comparable to the published 57% results for AlphaGo?\n2) Can you be more explicit about which architectures are new and which are previously published in your Figures and Tables? For example, it would be nice if figures 1-6 included citations to the papers where they were introduced (where applicable).\n3) How is your use of symmetries different from that of AlphaGo?\n4) What is the output of your network and what are the targets?The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.\n\nI found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:\n1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?\n2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?\n3) It is not at all surprising that ResNets do slightly better than vanilla CNNs on move prediction. I don't think this alone is enough for an ICLR paper. It would be good to see at least a comparison of several different variants of the network evaluated at actually playing Go, even if it's against other bots like GnuGo, Pachi, and Fuego.\n4) Are the differences between net_dark and your proposed networks after 20 iterations (Table 1) significant? \n\nPlease also see my original questions.", "title": "Pre-review questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ry6JoSAzg": {"type": "review", "replyto": "Bk67W4Yxl", "review": "In table 1, line 68 there is accuracy number for only net20, can you add the results for the other architectures as well? \nAlso in this comparison: \"It reached a 58.001% accuracy on the test set. It is greater than previous results reaching either 57.0% with 128 planes Silver et al. (2016), or 57.3% with 512 planes Tian & Zhu (2015).\", is the only difference the network architecture? do they use the same number of training images? The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.\n\nThe paper looks like a follow up paper of the author\u2019s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. \n\nThe paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?", "title": "Table 1 missing results", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H18RXc1El": {"type": "review", "replyto": "Bk67W4Yxl", "review": "In table 1, line 68 there is accuracy number for only net20, can you add the results for the other architectures as well? \nAlso in this comparison: \"It reached a 58.001% accuracy on the test set. It is greater than previous results reaching either 57.0% with 128 planes Silver et al. (2016), or 57.3% with 512 planes Tian & Zhu (2015).\", is the only difference the network architecture? do they use the same number of training images? The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.\n\nThe paper looks like a follow up paper of the author\u2019s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. \n\nThe paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?", "title": "Table 1 missing results", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}