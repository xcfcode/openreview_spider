{"paper": {"title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders", "authors": ["Pengyu Cheng", "Weituo Hao", "Siyang Yuan", "Shijing Si", "Lawrence Carin"], "authorids": ["~Pengyu_Cheng1", "~Weituo_Hao1", "~Siyang_Yuan1", "~Shijing_Si1", "~Lawrence_Carin2"], "summary": "A debiasing method for large-scale pretrained text encoders via contrastive learning.", "abstract": "Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space.", "keywords": ["Fairness", "Contrastive Learning", "Mutual Information", "Pretrained Text Encoders"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper presents a fair filter network to mitigating bias in sentence encoders by constructive learning. The approach reduces the bias in the embedding while preserves the semantic information of the original sentences. \n\nOverall, all the reviewers agree that the paper is interesting and the experiment is convincing. Especially the proposed approach is conceptually simple and effective. \n\nOne suggestion is that the model only considers fairness metric based on the similarity between sentence embedding; however, it would be better to investigate how the \"debiased embedding\" helps to reduce the bias in more advanced downstream NLP applications such as coreference resolution, in which researchers demonstrate that the bias in underlying representation causing bias in the downstream model predictions. "}, "review": {"XtqbsDdHe2_": {"type": "review", "replyto": "N6JECD-PI5w", "review": "This paper introduces a novel technique for debiasing pretrained contextual embedding models. Their approach trains a 2 layer fully-connected neural network which takes as input the output from the pretrained model and outputs a new, \"debiased\" representation. This model is trained by minimizing the InfoNCE between the representation produced of original sentence and the representation of that same sentence with some tokens replaced with differently-biased tokens (e.g. \"his\" -> \"hers\"). This paper also introduces a regularizer which minimizes the CLUB between the generated representation and a word embedding for a biased token. \n\nGenerally this paper is clearly written, addressing an interesting problem, presents some relevant supporting experiments, and seems original (I'm less familiar with other debiasing work, so I leave a better originality estimate up to the other reviewers).\n\nThe regularizer is described as a neural network, but that seems unnecessary. If it's parameterized as a neural network, and the weights are updated, then I think this is just part of the model and not really a regularizer. In addition, the motivation doesn't seem great -- it's not clear that we should be directly comparing (the mutual information of) a single word embedding and a contextual word embedding (which is built from the full sentence). All that said, empirically it seems to work well at debiasing the word embeddings, so it's a valuable contribution.\n\nI think the full list of debiasing dimensions isn't included -- maybe I missed it somewhere? That should definitely be included in the paper (not just as a citation), and if there isn't space it should be added to the appendix.\n\nThe three fine-tuning datasets could be improved, CoLA especially is known to have really high variance even just fine-tuning BERT multiple times with different random seeds. Since there are two other fine-tuning datasets there is sufficient evidence that the approach works well. As this is a fairly general approach, clearly written up, and seems to work well, I recommend it for acceptance. The experiments are a little light, and the regularization approach is a little unorthodox, and I would increase my score if there were further experiments (on other fine-tuning tasks and measuring other types of bias) and the regularization was better motivated.\n\nEdit: after reading the author response, my score remains unchanged. ", "title": "Clearly written, relatively simple, fairly effective", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Cu1xY4pek21": {"type": "rebuttal", "replyto": "ol-6lYPgjkc", "comment": "Thank you for your insightful comments. For your concerns:\n\n**Replaceable Words:** We agree that the current augmentation scheme under pre-defined replaceable words has some limitations. For undefined sensitive words from less common social topics, our model needs to add them manually case-by-case. However, the size of the word dictionary is finite, which makes it feasible to obtain a sufficient sensitive word list theoretically. The reviewer's suggestion is also practically insightful, that we may use words from different directions but not exactly the same meaning to do the augmentation (e.g. using \"her\" to replace \"boy\"). We will study this in future work.\n\n**Instantiation in Eq (5):** Our current objective in Eq (5). is strictly following the CLUB mutual information upper bound, which practically minimizes the correlation between word embeddings and debiased sentence embeddings in each data batch. The reviewer provided a good suggestion that we can choose negative samples of word embeddings based on the replaceable words instead of the word embeddings in the same batch. This negative sampling strategy may not be apparently explained by mutual information minimization, but it is a good empirical direction. We will conduct experiments for it in future work. \n\n**Additional Experiments:** We would like to try more complex networks' performance in the future to achieve a better trade-off between debiasing and downstream task performance.\n\n\n", "title": "Reviewer 4 Response"}, "brO_hT8kgtl": {"type": "rebuttal", "replyto": "tEknSDd6Qv", "comment": "We would like to thank you for your thoughtful comments. To address your concerns:\n\n**Regularizer:** Maximizing the log-likelihood between word sentence-level embeddings is to obtain an accurate mutual information estimator. Then with the estimator as an objective, we minimize the mutual information between word embedding and debiased sentence embedding. We agree that adding this regularizer causes accuracy drop on downstream tasks. But we think this small accuracy loss is acceptable compared with the gains of the debiasing performance.\n\n**Measurement:** We agree that if the encoder returns the same representation, the debiasing metric will reduce to zero. However, under this scenario, the representation will be meaningless, and the encoder performance on downstream tasks can be very bad. We think there is a trade-off between the fairness and the representativeness of the representations. From the results, our debiasing method can reduce the bias degree while holding the representativeness of embeddings on downstream tasks. \n\n**Data Source:** We follow the bias word lexicon from the previous work in [1], which is publicly available on their git repository. For gender related list, there are ten pairs of bias words for replacement. The pre-processing complexity for bias words replacement for one sentence is around O(m*n) where m is the size of the bias words list and n means the average number of words in the sentence. \n\n**Other Work:** Given the BERT architecture, the sentence embedding has attended on all words within it. Our method reduces the mutual information between sensitive word embedding and the sentence embedding. So our method does not only affect the those word embeddings that are present in the required bias lexicon, but also affects the sentence embedding as a whole. Also, plain average requires a strong assumption that the influence of bias words is symmetric, which is not always true. We are not sure if there exists simpler method, but would like to explore in the future.\n\n\n**Reference:**\n[1] Towards Debiasing Sentence Representations, ACL 2020", "title": "Reviewer 3 Response"}, "FcuUgtiSg4V": {"type": "rebuttal", "replyto": "XtqbsDdHe2_", "comment": "We are glad to see your positive comments. As for your concerns:\n\n**Regularizer:** We agree that the proposed regularizer can be also applied to the top layers of the encoder model, and even pre-trained with the model loss. Under the setup in our paper, we treat the fair filter as a post-hoc process, in which we do not need to re-train the text encoder as a \"black-box\" model. The post-hot process can further improve the stability of a system where the text encoder might serve as a modular part.\n\n**Word Embedding vs Sentence Embedding:** We agree that the word embedding and the contextual word embedding are not directly comparable. However, in our framework, we are not directly matching the word embedding to sentence embedding, but minimizing the mutual information between them. With our assumption that biased information of a sentence comes from sensitive words, reducing the information of sensitive words in the sentence embedding can alleviate the sentence-level bias degree.\n\n**Other Work:** We include other related work in Section 4.1. We agree that the performance for downstream tasks could be further improved, but achieving state-of-the-art results is not our main motivation here, given that debiasing may hurt the performance. We will apply our method to more fine-tuning tasks in our future work. ", "title": "Reviewer 2 Response"}, "8iRM2tiwNLh": {"type": "rebuttal", "replyto": "pOIfSPcnUY", "comment": "We appreciate your positive comments. \n\n**T-SNE:** From an intuition perspective, the gender related words should be close in the latent space. In other words, they should be equally distant to words like \u201cmath\u201d,\u201dscience\u201d, \u201cdance\u201d and \u201carts\u201d. As it\u2019s shown in the graph on the right, \u201che\u201d and \u201cshe\u201d, after being filtered out bias information, become closer than the graph on the left. We will include the detailed setup in the revision. \n\n**Evaluation Metric:** We agree that the significance level of the SEAT is one of the important critics for evaluating the bias degree. However, the p-value of the SEAT statistics depends on the permutation of the sentence embeddings to simulate the null distribution. And we found that the accurate estimation of the null distribution is challenging because the sentence sample space is too large. We will try to report the p-values of the SEAT statistics along with the effect sizes in the revision. As the reviewer pointed, the current evaluation of embedding bias (SEAT) does have room for improvement, which we think is also a valued research direction.", "title": "Reviewer 1 Response"}, "ol-6lYPgjkc": {"type": "review", "replyto": "N6JECD-PI5w", "review": "This paper studied a debiasing method to remove social bias in pretrained NLP models. The authors proposed to train a neural network which takes the sentence representations of a pretrained NLP model as input and outputs the unbiased representations. The neural network is trained by maximizing the mutual information between a sentence and its \u201ccounterpart sentence\u201d, which is automatically generated by replacing sensitive words by other values (e.g. replacing \u201che\u201d with \u201cshe\u201d). Moreover, the network can be further trained by minimizing the mutual information between the sentence representation and its sensitive word representation. The experiments show that the proposed method can effectively reduce bias while achieving better downstream task performance of the pretrained model.\n\nStrengths:\n- The studied research problem is important and would have a strong social impact. \n- The proposed methodology is well-motivated and generally reasonable.\n- The experimental results show strong performance\n\nWeakness:\n- The data augmentation process seems to require pairs of replaceable words to be available.\n\nThe research problem of mitigating social bias in NLP models is important to our community and can have strong social impact. Despite the popularity of pretrained NLP models, there are limited studies on fairness of BERT-like models. Therefore, I think this work is relatively novel on this specific topic. \n\nThe high-level intuitions of the proposed methodology is reasonable. I find both the applied losses quite intuitive. There are some minor limitations in the proposed method. For example, in the data augmentation process, it seems that replaceable words in different directions $r_j(\\cdot)$ need to be available. This is more strict than having a list of words for each direction in each topic: \u201cboy\u201d can be replaceable with \u201cgirl\u201d, but not necessarily \u201cshe\u201d or \u201cher\u201d. For some common topics, of course, it may not be difficult to find such resources. However, for less common social topics or topics with many values (e.g., countries, regions), it could be hard to construct such a lexicon. I think it would be better if the authors experiment their methods with less restrictive constraints on what are replaceable (e.g., allowing \u201cboy\u201d to be replaced by \u201cher\u201d) and see if the performances would be severely hurt. Alternatively, the authors can discuss how to automatically construct $r_j(\\cdot)$. \n\nI have a question about the instantiation of Eq (5). Since the authors already assume that they have replaceable words $r_j(\\cdot)$ available for any socially sensitive words, would it be better to minimize $q_{\\theta}(w_i^p|d_i)$ with $q_{\\theta}(r_j(w_i^p)|d_i)$ for all directions $j$\u2019s? Currently it seems like the negatives are basically other words in the same batch, which are not necessarily in the same direction. \n\nThe experiments conducted show pretty strong performance, where the proposed methods achieve better debiasing performances and better classification performances. Some additional experiments could provide more insights. For example, how about using more than one layer of fully-connected neural networks for the fair filter? \n\nGenerally, I recommend acceptance of this paper. \n", "title": "Reasonable method for an important research topic", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "tEknSDd6Qv": {"type": "review", "replyto": "N6JECD-PI5w", "review": "The paper proposes a method for debiasing pretrained sentence representations.\nThe existing representations are mapped to a new space and the mapping is trained to disregard specific bias words. Alternative versions of an input sentence are constructed by replacing specific bias words with other bias words. The representations mapping is then optimised to still produce a similar mapping in both cases. In addition, the representations are pushed away from the individual bias word representations.\nEvaluation is performed on SST-2, CoLA and QNLI, showing that the method is able to produce more similar representations for sentences containing bias words, while sacrificing a small amount of performance over regular BERT.\n\nThe method of optimizing different versions of the sentence to be similar is clear and intuitive. It also shows good empirical performance.\n\nHowever, the idea of maximizing the log-likelihood for the debiasing regularizer seems less motivated. Results also indicate that adding this component negatively affects classification accuracy.\n\nThe idea of measuring bias only through cosine similarity is questionable. Presumably a system that returns an identical representation for every sentence would score very highly on this evaluation? There should be some human evaluation involved or actually measuring cases where the bias word affects classification output on a downstream task.\n\nIt is unclear where the lexicon of bias words used in this work came from, is it available to everyone, how large it is and how much work would be required to construct it for other types or biases or other languages.\n\nIt is unclear whether the method only affects those word embeddings that are present in the required bias lexicon anyway. If so, then some simpler approaches could possible be just as effective, e.g. replacing all the gendered words with one gender equivalents or averaging over different versions of the same sentence containing different genders.", "title": "Could be a useful method; needs a stronger evaluation", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "pOIfSPcnUY": {"type": "review", "replyto": "N6JECD-PI5w", "review": "The paper builds on recent working attempts to debais sentence encoders by considering modified sentences. Sentences are selected that, for example, contain\u00a0gender cueing words, and then swap those words with a predetermined 'opposite' (i.e. man<->woman). The core novelty in the work is to train a lightweight modification the encoding of the sentence and its swap to (a) reduce the distance between the two embeddings, using a contrastive learning objective and (b) reduce the mutual information between cueing words and the new embeddings. Evaluated on a WHEAT style task, modified for sentences, the method performs significantly\u00a0better than existing recent work.\u00a0\n\nOverall I am extremely excited about the work: it is conceptually simple, the two pieces that are proposed are both evaluated, both seem to help, and it is in practice a lightweight\u00a0modification to a BERT style encoder.\n\nPositives:\n+ The method is conceptually simple and the modification of the existing embedding is cheap to compute.\n+ Works better than existing recent work, while maintaining equal or more of the original\u00a0model's downstream\u00a0accuracy\n+ Lightweight modification\u00a0of existing embedding system that does not require retraining of large transformer models.\n+ Conceptually simple method\n+ Overall extremely clean and self contained presentation\n\nNegatives:\n+ (nitpicking) The T-SNE experiment is unclear. I am unsure how it was constructed and why the graph on the right is better than the one on the left.\u00a0\n+ the evaluation measure (not a complaint specific to this work), is hard to interpret. Significance testing on differences between baseline and this approach would help (in particular, Table 4)\n\u00a0", "title": "Clean idea that works well with a good presentation ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}