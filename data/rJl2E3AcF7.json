{"paper": {"title": "Doubly Sparse: Sparse Mixture of Sparse Experts for Efficient Softmax Inference", "authors": ["Shun Liao", "Ting Chen", "Tian Lin", "Chong Wang", "Dengyong Zhou"], "authorids": ["sliao3@cs.toronto.edu", "tingchen@cs.ucla.edu", "tianlin@google.com", "dennyzhou@google.com", "chongw@google.com"], "summary": "We present doubly sparse softmax, the sparse mixture of sparse of sparse experts, to improve the efficiency for softmax inference through exploiting the two-level overlapping hierarchy. ", "abstract": "Computations for the softmax function in neural network models are expensive when the number of output classes is large. This can become a significant issue in both training and inference for such models. In this paper, we present Doubly Sparse Softmax (DS-Softmax), Sparse Mixture of Sparse of Sparse Experts, to improve the efficiency for softmax inference. During training, our method learns a two-level class hierarchy by dividing entire output class space into several partially overlapping experts. Each expert is responsible for a learned subset of the output class space and each output class only belongs to a small number of those experts. During inference, our method quickly locates the most probable expert to compute small-scale softmax. Our method is learning-based and requires no knowledge of the output class partition space a priori. We empirically evaluate our method on several real-world tasks and demonstrate that we can achieve significant computation reductions without loss of performance.", "keywords": ["hierarchical softmax", "model compression"]}, "meta": {"decision": "Reject", "comment": "This work proposes a new approximation method for softmax layers with large number of classes. The idea is to use a sparse two-layer mixture of experts. This approach successfully reduces the computation requires on the PTB and Wiki-2 datasets which have up to 32k classes. However, the reviewers argue that the work lacks relevant baselines such as D-softmax and adaptive-softmax. The authors argue that they focus on training and not inference and should do worse, but this should be substantiated in the paper by actual experimental results."}, "review": {"HklSWHq3p7": {"type": "rebuttal", "replyto": "rJl2E3AcF7", "comment": "We thank reviewers for their time and valuable comments. We have revised our article based on reviewers' suggestions. \nWe want to summarize the key points of this work as follows:\n\n* Our work focuses on speeding up softmax inference given large output dimension and achieved good empirical results on both synthetic and real dataset. For top-k language modeling task on Wiki-2, we can achieve more than 23x without any loss of performance.\n\n* Our method is novel in terms of constructing the two-level overlapping hierarchy of output classes. The hierarchy is captured through the mixture model and group lasso technique. The inference speedup is achieved by such a hierarchy. \n\n* The key difference between our work and existing methods is that our speedup is achieved by learning a new output embedding while most existing methods relied on approximating the trained/fixed embedding. This means our method is orthogonal with them in principle. One key advantage of our method is speedup without any loss while approximation based methods usually suffer the loss of performance. ", "title": "Summary of the revision and key points"}, "B1e8djk9Tm": {"type": "rebuttal", "replyto": "B1euHOqi37", "comment": "Dear Reviewer,\n\nThank you for your valuable comments. We have revised our writing in the revision, and will further improve its clarity. Please find our response as follows.\n\n- Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation. \n\nMitosis training can be considered as executing Algorithm 1 for multiple times with an increasing number of experts and inherited initialization from last round by changing W^e and W^g. Also, training with mitosis achieves similar performance as training without it shown in Appendix B, Figure (a). \n\n- How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set? \n\nThe hyper-parameters related to DS-softmax (such as lambda) are tuned according to the performance on a validation dataset. Also, as we mentioned in the paper, only one hyper-parameter (group lasso lambda) needs to be tuned. The heuristic we use to tune group lasso lambda is to increase lambda, starting from a small value, until it hurts the performance. Also threshold and balancing lambda variables are kept fixed as (0.01 and 10). \n\n- Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster? \n\nIn terms of baselines, SVD-softmax (NIPS\u201917) was chosen since it is a recent method that provides a significant inference speedup for softmax. Other alternatives, such as D-softmax and adaptive-softmax, focus on training instead of inference speedup. Furthermore, as claimed in their papers, they achieve limited speedup (around 5x) in language modeling, which is much worse than ours. With regards to Sparsely Gated MoE, it cannot speed up inference, since they select expert with full softmax.\n\nWe would like to emphasize that most existing methods for inference speedup focus on approximating trained softmax layer, which usually suffers a loss on performance. Our model allows the adaptive adjustment of the softmax layer, achieves speedup through capturing the two-level overlapped hierarchy during training, which is novel and does not suffer from the performance loss.\n", "title": "Our work focuses on inference speedup, and compares to the best approach (NIPS'17) we were aware of"}, "HJxF95J5aX": {"type": "rebuttal", "replyto": "rJxPUFHc3m", "comment": "Dear Reviewer:\n\nThank you for your valuable comments. We have addressed typos in the revision accordingly.  And please find our response as follows.\n\n-  Can you be more specific about the gains in training versus inference time?\n\nWe would like to emphasize that the our goal is to speed up the inference time for softmax, so we do not include any comparisons in terms of training time. According to our experiments, most speedup can be achieved in few epochs (given all other layers are pre-trained) so that the training time increase is not significant compared to the original one.\n\n- You motivate some of the work by the fact that the experts have overlapping outputs. Maybe in section 3.7 you can address how often that occurs as well? \n\nThanks for the suggestion. We demonstrate that ambiguous words are often overlapped between clusters as illustrated in Figure 3(b). We added one more Figure in Appendix B, Figure (b), to demonstrate the distribution of overlapping. \n\n- It wasn't clear how the sparsity percentage on page 3 was defined? \n\nSorry for the possible confusion. The sparsity in page 3 means the percentage of pruned words. We have added more clarifications in the revised version. \n\n- Can you motivate why you are not using perplexity in section 3.2?\n\nWe use top-k accuracy, instead of perplexity, because approximating top-k is required for most inference tasks in practice (see [1]). Perplexity captures the normalized log-likelihood of all possible words, while top-k accuracy is a better measure for inference speedup for top-k retrieval. For example, in some extreme cases, if a word only has a very small probability which makes it unpredictable at all (i.e. couldn\u2019t be retrieved by top-k for any reasonably small k), it could still have a huge impact in terms of perplexity, but has a much smaller impact on top-k accuracy, which seems more reasonable given the goal of top-k retrieval.\n\n[1] Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS), NIPS 2014", "title": "Thank you for your feedback!"}, "HJgvmMlBTQ": {"type": "rebuttal", "replyto": "SklHkeMohX", "comment": "Dear reviewer:\n\nWe appreciate your comments but it appears that there is some misunderstanding regarding our contribution in this work. \n\nOur work is for softmax inference speedup while Sparse-Gated MoE (MoE) was not designed to do so. It was designed to increase the model expressiveness. It cannot achieve speedup because each expert still contains full softmax space as we mentioned in the background section (page 2 line 21st) and method section (page 2 last 4th line). And since it is slower than the standard softmax by definition, we chose not to compare with it in the paper.\n\nOur algorithm addresses speed up in softmax inference. This is fundamentally different from Sparse-gated MoE. We divide the output space into multiple overlapped subsets. To find top-k predictions, we only search a few subsets. While in full softmax or MoE, the complexity is linear with output dimension. Therefore, we did not include a comparison with Sparsely-Gated MoE in our article and only compare with full softmax. \n\nJust for additional reference, we tested Sparsely-Gated MoE with different experts in PTB dataset; we compared the results to DS-Softmax. As expected, the Sparsely-Gated MoE does not achieve speedup in terms of softmax inference. \n\n______________________________________________\nMethod | Top 1 | Top 5 |Top 10| FLOPs| \nDS-8       | 0.257 | 0.448 | 0.530 | 2.84x |\nMoE-8    | 0.258 | 0.448 | 0.530 |  1x      |\nDS-16     | 0.258 | 0.450 | 0.529 | 5.13x |\nMoE-16  | 0.258 | 0.449 | 0.530 | 1x       |\nDS-32     | 0.259 | 0.449 | 0.529 | 9.43x |\nMoE-32  | 0.259 | 0.450 | 0.531 | 1x       |\nDS-64     | 0.258 | 0.450 | 0.529 |15.99x|\nMoE-64  | 0.260 | 0.451 | 0.531 | 1x       |\n______________________________________________\n\n* FLOPs means FLOPs reduction (i.e. baseline's FLOPs / target method's FLOPs).", "title": "Clarifications: Sparsely-Gated MoE (Shazeer et al. 2017) cannot speed-up softmax inference"}, "B1euHOqi37": {"type": "review", "replyto": "rJl2E3AcF7", "review": "The present paper proposes a fast approximation to the softmax computation when the number of classes is very large. This is typically a bottleneck in deep learning architectures. The approximation is a sparse two-layer mixture of experts.\n\nThe paper lacks rigor and the writing is of low quality, both in its clarity and its grammar. See a list of typos below.\n\nAn example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation. Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.\n\nAlgorithm 1 does not include mitosis, which may have an effect on the resulting approximation.\n\nHow are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?\n\nThe results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?\n\nThe column \"FLOPS\" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases. Also, a \"1x\" label seems to be missing in for the full softmax, so that the reference is clearly specified.\n\nAll in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.\n\nA brief list of typos:\n\n\"Sparse Mixture of Sparse of Sparse Experts\"\n\"if we only search right answer\"\n\"it might also like appear\"\n\"which is to design to choose the right\"\nsparsly\n\"will only consists partial\"\n\"with \u03b3 is a lasso threshold\"\n\"an arbitrarily distance function\"\n\"each 10 sub classes are belonged to one\"\n\"is also needed to tune to achieve\"", "title": "Good empirical results, but only one baseline and poor writing.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SklHkeMohX": {"type": "review", "replyto": "rJl2E3AcF7", "review": "The paper proposes doubly sparse, which is a sparse mixture of sparse experts and learns a two-level class hierarchy, for efficient softmax inference.\n\n[+] It reduces computational cost compared to full softmax.\n[+] Ablation study is done for group lasso, expert lasso and load balancing, which help understand the effect of different components of the proposed\n[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this. Besides, in evaluation, the paper only compares Doubly Sparse with full softmax. Why not compare with Sparsely-Gated MoE?\n\nOverall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.\n", "title": "Need to discuss more about how Doubly Sparse is superior to Sparsely-Gated MoE", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJxPUFHc3m": {"type": "review", "replyto": "rJl2E3AcF7", "review": "In this paper the authors introduce a new technique for softmax inference. In a multiclass setting, the idea is to take the output of a NN and turn it into a gating function to choose one expert. Then, given the expert, output a particular category. The first level of sparsity comes from the first expert. The second level of sparsity comes from every expert only outputting a limited set of output categories.\n\nThe paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. \"search right\" -> \"search for the right\", \"predict next word\" -> \"predict the next word\", ...) In section 3, can you be more specific about the gains in training versus inference time? I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well. You motivate some of the work by the fact that the experts have overlapping outputs. Maybe in section 3.7 you can address how often that occurs as well?\n\nNits:\n- it wasn't clear how the sparsity percentage on page 3 was defined?\n- can you motivate why you are not using perplexity in section 3.2?\n", "title": "New method for large scale softmax inference", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}