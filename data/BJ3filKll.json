{"paper": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"], "summary": "We show constructively that deep networks can learn to represent manifold data efficiently", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "keywords": ["Theory", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference."}, "review": {"rktNLYSUg": {"type": "rebuttal", "replyto": "SyHDxy8Vl", "comment": "We thank the reviewer for these additional comments.\n \n'It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks'\nWe agree. Our construction can be incorporated in deep networks in various ways, both in order to represent complex manifolds by using higher layers to combine several monotonic chains (as we have demonstrated for the swiss roll) or in producing hierarchical representations of the data (such as when an m-dimensional manifold lies in an l-dimensional linear subspace of the ambient space). We refer the reviewer to Appendix D in the revised version of our paper, which now includes further discussion of such deep architectures. \n \n'to what extent the proposed picture describes the totality of functions that are representable by the networks'\nOur approach suggests an efficient way to represent low-dimensional manifolds with neural nets. This can serve as part of representations of functions defined on manifolds. Our work does not suggest specific methods to represent such functions, although, as noted, Shaham et al. (2015) suggest one way to approach this problem.\n\nWe appreciate the minor comments of the reviewer and have incorporated them into our revision.\nRegarding the minor comment 'On page 5, mention how the orthogonal projection on S_k is realized in the network' please note that the orthogonal projection is not realized in the network. It is used at this point merely to derive a bound on the error.\u200b", "title": "Reply to AnonReviewer2"}, "BySNBFrUl": {"type": "rebuttal", "replyto": "BJoiKE_Nl", "comment": "We thank the reviewer for these constructive comments.\n\n'The experiments are all with a regression loss and a shallow network'\nIndeed monotonic chains can efficiently be handled with a shallow network. Our construction, however, can be incorporated in deep networks in various ways, both in order to represent complex manifolds by using higher layers to combine several monotonic chains (as we have demonstrated for the swiss roll) or in producing hierarchical representations of the data (such as when an m-dimensional manifold lies in an l-dimensional linear subspace of the ambient space). We refer the reviewer to Appendix D in the revision, which now includes further discussion of such deep architectures. \n \n'It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression'\nClassification loss is generally more permissive than a regression loss. Consequently, if we apply a classification loss to an architecture that can perform embedding in the lower levels and classification in the higher levels we will generally obtain a distorted embedding, in which each linear segment can deform (e.g. in the directions of class separation) -- such embeddings are often sufficient to achieve accurate classification. We now demonstrate this and provide further discussion in Appendix D of our revised manuscript.  \n \n'The theory sections could do with being more clearly written'\nWe appreciate the reviewers suggestions for improving the presentation of the theory, and will incorporate these in a revised version of the paper.", "title": "Reply to AnonReviewer3"}, "HkegVFHLe": {"type": "rebuttal", "replyto": "ryGb3w-rg", "comment": "We thank the reviewer for these constructive comments.\n \n(1) 'work on learning data representations from sets of local tangent planes'\nThank you for the references, which we have included in a revision of the paper. In particular, Zhang and Zha's analysis indicates that in the limit when the input points are sampled densely and lie near a piecewise linear manifold their embedding algorithm projects points that are off the manifold orthogonally onto the manifold. Our analysis, in contrast, indicates that a neural network with an efficient architecture will generally not project these points orthogonally. We note however that less efficient networks with significantly more units can also learn to project such points orthogonally onto the manifold. Our main objective in this paper is to show that networks can be significantly more efficient, although this comes at the price of increased error when the data is not fit exactly by the piecewise linear approximation.\n \n'how these old techniques compare to the deep network trained to produce the embedding of Figure 6'\nUnfortunately, it is not viable to experimentally compare the results of traditional manifold learning approaches and our constructions.  In our experiments, which serve to illustrate the types of constructions that can be learned by a deep network, we use supervision so that the network learns to map input points to a ground truth embedding.  Traditional manifold learning is unsupervised, and so must solve a more difficult problem.  In response to the reviewers comments we have applied LLE to the data used to produce Figure 6.  However, while a neural network can learn a good embedding with supervision, learning a construction similar to the one we derive theoretically, LLE is not able to produce a sufficiently good embedding using the somewhat limited data in our experiment, as it does not make use of supervision.  It learns an embedding that maps input faces to a straight line based on their azimuth, ignoring elevation.  However, we are grateful for the reviewer\u2019s suggestion, and feel that it will be interesting in future work to more comprehensively explore how the manifold embedding with deep networks is related to these now classical approaches.\n \n(2) 'data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space'\nSimilar to existing manifold learning techniques, our approach also assumes there are sufficiently many data points to allow identification of the manifold structure. We note that when the data is sparsely spread in a high dimensional space manifold embedding is probably not a preferred strategy. \n \n'network architectures that are not pure ReLU networks'\nCommon activation functions like softplus, sigmoid and tanh can be approximated accurately with piecewise linear functions that include just a few pieces. With such approximations our analysis can be readily shown to apply also to these activation functions.\n \n'most modern networks use a variant of batch normalization'\nOur analysis would not change significantly with batch normalization. Ioffe and Szegedy (2015) show that a network with batch normalization can represent any function that could be represented by a network without batch normalization, since added parameters allow a linear transformation to be learned that recovers the original network.\n \n(3) 'The error bound presented in Section 4 appears vacuous for any practical setting'\nOur error analysis represents a worst case analysis, illustrating what the error would be when the projection directions for every segment chosen in training align with the direction of the largest singular vector of the projection operator. Our experiments indicate that the average case is significantly more favorable. We hope to further analyze this case in future work. \n \n'only refer to fully supervised siamese network approaches'\nWe thank the reviewer for these references and have included them in our revision. We note that our construction can be used both in supervised and unsupervised settings, but for our experiments we only show results in supervised ones.\n \n'What loss do the authors use in their experiments?'\nIn our experiments we use the square loss || y - \\hat y ||^2 where for an input point x (in ambient space) y denotes the embedding coordinates produced by the network and \\hat y denotes the ground truth embedding coordinates. We also tested our method in a siamese setting where for two inputs xi xj we used the square loss (||yi - yj|| - ||\\hat yi - \\hat yj||)^2. These experiments yielded similar results.\n \nWe also somewhat disagree with the reviewer that our results are not surprising. It may seem natural that a neural network can embed a manifold, but we find it very surprising that this can be done so efficiently.  In contrast to these results, as we discuss in our paper, Shaham et al. (2015) have recently published a related construction that is much less efficient. \n", "title": "Reply to AnonReviewer4"}, "S13MPuXmg": {"type": "rebuttal", "replyto": "HJfrkIk7e", "comment": "Thank you for your helpful questions.  In response:\n\uf06e\tWe should say that the chains can be flattened by an isometry.\n\uf06e\tYes, we mean pieces of affine spaces.  We think the best way to clarify this is to refer to \u201csegments\u201d instead of \u201csubspaces.\u201d\n\uf06e\tThanks for pointing this out.  We meant to say: {\\cal C} \\cap H_{k+1}\\subseteq {\\cal C} \\cap H_{k}\n\uf06e\tWe think that the obvious points of comparison would be to networks with more or fewer units.  We can say that with a much less efficient construction, in which an independent basis is created for each segment separately, the relative error can be reduced to 0.  Such a construction (see Shaham et al.) would require more than m units for each additional segment, while our construction requires only one unit per additional segment.  A second possible comparison is to a network that represents the manifold in a single basis, but this would produce a very large relative error.\nWe will revise our paper to take account of these points.\n", "title": "Reply to questions"}, "HJfrkIk7e": {"type": "review", "replyto": "BJ3filKll", "review": "In page 3 one reads ``assume that these chains can be flattened...''. Flattened by what kind of operation? \nIn the definition from Page 3, should affine spaces not rather be pieces of affine spaces? \nIn the same definition, the assumption H_{k+1}\\subseteq H_{k} would imply that the hyperplanes are parallel. Are you sure this is what you have in mind? \nAt the end of Section 4 one reads about a mean relative error. Is there also a baseline? SUMMARY \nThis paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. \n\nPROS \nInteresting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. \n\nCONS \nThe paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). \n\nCOMMENTS \nIt would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. \nAlso, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. \n\nMINOR COMMENTS \n- Figure 1 could be referenced first in the text.  \n- ``Color coded'' where the color codes what? \n- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. \n- On page 5, mention how the orthogonal projection on S_k is realized in the network. \n- On page 6 ``divided into segments'' here `segments' is maybe not the best word. \n- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean? \n", "title": "terminology", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SyHDxy8Vl": {"type": "review", "replyto": "BJ3filKll", "review": "In page 3 one reads ``assume that these chains can be flattened...''. Flattened by what kind of operation? \nIn the definition from Page 3, should affine spaces not rather be pieces of affine spaces? \nIn the same definition, the assumption H_{k+1}\\subseteq H_{k} would imply that the hyperplanes are parallel. Are you sure this is what you have in mind? \nAt the end of Section 4 one reads about a mean relative error. Is there also a baseline? SUMMARY \nThis paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. \n\nPROS \nInteresting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. \n\nCONS \nThe paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). \n\nCOMMENTS \nIt would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. \nAlso, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. \n\nMINOR COMMENTS \n- Figure 1 could be referenced first in the text.  \n- ``Color coded'' where the color codes what? \n- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. \n- On page 5, mention how the orthogonal projection on S_k is realized in the network. \n- On page 6 ``divided into segments'' here `segments' is maybe not the best word. \n- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean? \n", "title": "terminology", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}