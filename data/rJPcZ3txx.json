{"paper": {"title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "authors": ["Jongsoo Park", "Sheng Li", "Wei Wen", "Ping Tak Peter Tang", "Hai Li", "Yiran Chen", "Pradeep Dubey"], "authorids": ["jongsoo.park@intel.com", "sheng.r.li@intel.com", "peter.tang@intel.com", "weiwen.web@gmail.com", "HAL66@pitt.edu", "yic52@pitt.edu", "pradeep.dubey@intel.com"], "summary": "Highly-performance sparse convolution outperforms dense with only 70% sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.\nThe number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. \nNevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.\n\nWe present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix\nmultiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1-7.3x convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.\n", "keywords": ["Deep learning", "Optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "While the core ideas explored in this paper are quite limited in algorithmic novelty (e.g., the direct sparse convolutions), the reviewers largely feel that the paper is well written, experiments are carefully done on multiple architectures and system issues are discussed in-depth. Given the interest in the ICLR community around performance characterization and acceleration of CNNs in particular, this paper offers an interesting perspective."}, "review": {"SkCpzH_4g": {"type": "rebuttal", "replyto": "SkAX_RbNl", "comment": "Please note that, in addition to our fast convolution algorithms, another important contribution is the performance model guiding the pruning process that allows pursuing desired speedup and model size reduction without falling into combinatorial number of choices offered by multiple layers. Our performance model can also guide other methods discussed in related work as shown by our application to dynamic network surgery (GDNS in Figure 4(a)).\n\nThanks for the suggestion on summarizing inference speedups and model size reductions of related work. A quick summary is shown below, which we will also consider including in our paper. It is important to note that our work achieves highest speedup without accuracy loss among all the techniques below. The speedups shown that are not our own measurements should be taken with a grain of salt because 1) many papers only provide relative speedups to a baseline whose efficiency is suboptimal (e.g. in some cases, the baseline is Caffe running on CPU, which is known to be suboptimal as it is tuned for GPU), and 2) what some papers report as \"speedup\" is actually FLOP reduction, not actual timing measurements. As we did in our paper, for more scientific comparison among different CNN speedup techniques, we recommend using dense matrix multiplication (GEMM) FLOP/s of the evaluated platform as the baseline, because many platforms readily have vendor-provided extensively-optimized GEMM implementations which can be a proxy of highly-optimized dense CNN implementation. This also aligns with a long-accepted common practice in the high-performance computing (HPC) community. We omit Denton et al. 2014, Jaderberg et al. 2014, and Lebedev et al. 2015 in the summary because they report improvements in a subset of conv and fc layers.\n \nAlexNet\nGESL (ours, 0% top-1 accuracy drop):                8.5x smaller model,                      2.5x speedup,                                      4.2x FLOP reduction\nDNS (0.5% top-1 accuracy drop):                    17.7x smaller model,                      1.0x speedup (not enough sparsity in conv layers), 2.8x FLOP reduction\nSSL (0% top-1 accuracy drop):                       1.01x smaller model (no sparsity in fc), 1.5x speedup,                                      1.3x FLOP reduction\nLebedev and Lempitsky (1.3% top-1 accuracy drop):   2.9x smaller model,                      3.0x speedup? (not sure if this is a real speedup or FLOP reduction)\nLiu et al. (1% top-1 accuracy drop):                1.04x smaller model (no sparsity in fc), 4.4x speedup (not sure if lowering overhead included)\nKim et al. (1.7% top-5 accuracy drop):              5.5x smaller model,                      1.8x speedup,                                      2.7x FLOP reduction\nTai et al. (0.4% top-5 accuracy drop):              5.0x smaller model,                      1.8x speedup,                                      5.3x FLOP reduction\n\nGoogLeNet\nGESL (0.2% top-1 accuracy drop):                    3.3x smaller model,                      2.0x speedups in conv and fc layers,               3.0x FLOP reduction\nDNS (our own evaluation, 2.5% top-1 accuracy drop): 1.5x smaller model,                      2.0x speedups in conv and fc layers,               2.6x FLOP reduction\nSSL (our own evaluation, 2% top-1 accuracy drop):   2.1x smaller model,                      speedup N/A yet,                                   2.3x FLOP reduction\nKim et al. (0.2% top-5 accuracy drop):              1.3x smaller model,                      1.2x speedup,                                      1.3x FLOP reduction\nIoannou et al. (0.4% top-1 accuracy drop):          1.7x smaller model,                      speedup N/A,                                       1.4x FLOP reduction\nTai et al. (0.4% top-5 accuracy drop):              2.8x smaller model,                      1.2x speedup,                                      2.9x FLOP reduction", "title": "Summary of related work"}, "B1B6WH_Ne": {"type": "rebuttal", "replyto": "HJTCvJz4e", "comment": "We respectfully ask the reviewer to reconsider the assessment that there is a lack of research contribution in the present paper. It is well accepted that deep learning is enabled by three similarly crucial components \u2013 algorithm, data, and performance. Each component benefited from a succession of original research efforts which are still active at present. These include the invention of back propagation (past) to new optimization algorithms for training (present), the creation of the ImageNet dataset (past) to the Re\u2019s \u2018\u2019data programming\u201d approach (present), the lowering method that transforms convolution to matrix products (past) to computer architecture for special-purpose hardware accelerator (present). \n\nThe present paper belongs to the performance research area of sparsification. While sparsification has been effective in memory footprint reduction, it has hitherto limited success in inference throughput enhancement. The greatly enhanced throughput reported here cannot be achieved solely by our direct sparse convolution technique (which is a new fast algorithm by itself). Preserving inference accuracy is an implicit requirement for all sparsification endeavor. Sparsification for performance enhancement therefore cannot be successful without understanding where to \u201cuse the sparsification budget\u201d for most effective performance gain. And a simplistic \u201cengineering\u201d trial-and-error approach is infeasible for the combinatorial number of choices offered by tens of layers and hundreds of channels. We successfully identified sparsification targets by combining a high-resolution performance model and a sparsity allocation methodology, both of which contain original research that are applicable to performance research in general.", "title": "Clarification on research contribution"}, "SJQqbr_Vg": {"type": "rebuttal", "replyto": "SJ6rokQEe", "comment": "We thank the reviewer\u2019s appreciation of our sparse convolution algorithms as well as guided pruning techniques and the comment that the performance-model-guided techniques can be used in future work. Indeed, there could be very interesting directions for future work on building new neural network architectures with our techniques. As shown in GoogLeNet designs, building new network architectures with reduced computing demand is important.  However, as shown in our paper, the actual performance improvement is not a simple linear function of reduced FLOP count; instead, the convolution kernel size, input and output channel dimensions, and characteristics of a target platform among others, determine the actual speed of a neural network  on the target platform. Our performance-model-guided approach can accurately project actual speedup of a neural network on specific target platforms (CPU/GPU/FPGA/ASIC). Consequently, design decisions on new network architecture can be custom made for a specific hardware platform in question. We will add an elaboration on this to our current paper.\n\nOur model is very transferable to other platforms including GPUs. For example, with a typical 90% sparsity, both Pascal Titian-X and P100 GPUs are projected to achieve similar ~3x speedups over their dense baselines. Moreover, our model also reveals that sparse convolution becomes memory bandwidth bound and thus provide diminishing speedups earlier on Titan-X than on P100. This is because Titan-X has a much higher flop to byte ratio than P100 equipped with a new high bandwidth memory technology, HBM2. These insights shed light on designing sparse convolution on GPUs.", "title": "Our performance model shows a nice direction for future work. Application to GPU"}}}