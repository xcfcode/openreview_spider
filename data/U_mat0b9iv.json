{"paper": {"title": "Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network", "authors": ["James Diffenderfer", "Bhavya Kailkhura"], "authorids": ["~James_Diffenderfer1", "~Bhavya_Kailkhura1"], "summary": "A new paradigm for learning compact yet accurate binary neural networks by pruning and quantizing randomly weighted full precision DNNs", "abstract": "Recently, Frankle & Carbin (2019) demonstrated that randomly-initialized dense networks contain subnetworks that once found can be trained to reach test accuracy comparable to the trained dense network. However, finding these high performing trainable subnetworks is expensive, requiring iterative process of training and pruning weights. In this paper, we propose (and prove) a stronger Multi-Prize Lottery Ticket Hypothesis:\n\nA sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3).\n\nThis provides a new paradigm for learning compact yet highly accurate binary neural networks simply by pruning and quantizing randomly weighted full precision neural networks. We also propose an algorithm for finding multi-prize tickets (MPTs) and test it by performing a series of experiments on CIFAR-10 and ImageNet datasets. Empirical results indicate that as models grow deeper and wider, multi-prize tickets start to reach similar (and sometimes even higher) test accuracy compared to their significantly larger and full-precision counterparts that have been weight-trained. Without ever updating the weight values, our MPTs-1/32 not only set new binary weight network state-of-the-art (SOTA) Top-1 accuracy -- 94.8% on CIFAR-10 and 74.03% on ImageNet -- but also outperform their full-precision counterparts by 1.78% and 0.76%, respectively. Further, our MPT-1/1 achieves SOTA Top-1 accuracy (91.9%) for binary neural networks on CIFAR-10. Code and pre-trained models are available at: https://github.com/chrundle/biprop.", "keywords": ["Binary Neural Networks", "Pruning", "Lottery Ticket Hypothesis"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors present a new theoretical framework that establishes that any network can be approximated by pruning a polynomially larger random binary networks, and also an algorithm for pruning binary nets. The results are important in the general context of the \"strong\" lottery ticket hypothesis, and are of both theoretical and practical interest. Although some of the ideas and technical contributions can be seen as a combination of prior tools and algorithms, the experimental findings are very novel.  Some further concerns of clarity and novelty were addressed by the authors."}, "review": {"RoK_5XkX1R7": {"type": "rebuttal", "replyto": "U_mat0b9iv", "comment": "We have now uploaded the camera ready version of our paper. A summary of the updates and additions for the camera ready version are provided below. Finally, we want to thank the reviewers and area chairs for their discussion during the review process and we are grateful for the acceptance of our work to ICLR 2021.\n\n* **Updates**: \n    * Updated results for Section 3.2 of paper including\n        *  MPTs-1/32 not only set new binary weight network state-of-the-art (SOTA) Top-1 accuracy -- 94.8% on CIFAR-10 and 74.03% on ImageNet -- but also outperform their full-precision counterparts by 1.78% and 0.76%, respectively \n        * MPT-1/1 achieves SOTA Top-1 accuracy (91.9%) for binary neural networks on CIFAR-10\n    * Public release of biprop code and some pre-trained models are available at: https://github.com/chrundle/biprop\n* **Additions**:\n    * Enabled training of BatchNorm layer parameters while using biprop which boosts performance of MPT networks. Results for these networks are provided in Section 3.2", "title": "Camera Ready Version"}, "hzs2S6hNoN": {"type": "review", "replyto": "U_mat0b9iv", "review": "This paper propose utilizing the existing \"lottery ticket\" result for constructing binary neural networks. This work has some novelty, in the sense that I haven't seen any other papers on untrained binary neural networks. The experimental results looks good.\n\nHowever, I have some concerns on this paper.\n1. This paper, at least the main text, is not self-contained. The writing needs significant improvement. The main contribution of the paper, section 2, is only one-page long. Neither the theory nor the algorithm are well explained in the main text. Moreover, the algorithm relies heavily on the edge-popup algorithm, which is not explained even in the supplementary material. The title is also somewhat too long.\n2. Though the proposed algorithm achieves excellent results in terms of parameter count and accuracy, I think the comparison is somewhat unfair. The subnetwork is sparse, and can be much slower on real hardware. Moreover, pruning from a larger network is known to achieve better result than training a smaller network from scratch, but the baselines does not utilize this.\n3. There lacks any discussion on the real time consumption of the proposed network.\n4. Time consumption of training should also be reported.\n\nPost rebuttal\n====\n\nThanks the authors for clarifying and revising the paper. The updated version does look much clearer to me, so I updated my ratings. \n\nI am still wondering the difference of biprop vs. a classical quantization-aware training for ternary networks. I did read the response to R3. From my understanding it seems that:\n1. biprop doesn't count 0 as a parameter, while TWN does;\n2. biprop prunes a larger network (WRN50), while TWN trains a network of the original size (ResNet-50);\nI am not sure if the superiority of biprop comes from these reasons, instead of LTH itself. biprop still looks more like a QAT algorithm than a LT-finding algorithm in the sence that\n1. it does not train the pruned network after finding the LT as the original LTH paper;\n2. it directly learns the binary weights.\nJust out of curiosity, but I think clarifying these concerns would make the paper stronger.", "title": "This paper has some good results, but the writing needs to be improved", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "qGJFRavU4A": {"type": "rebuttal", "replyto": "YqXEsYEnrl5", "comment": "We completely agree with the reviewer that we have not proved or showed \"biprop\" to be \"not requiring massive compute resources\".   \n\nThis was mentioned in the implication or future work section and we are sorry for not being precise when using this statement. We will make the change in the revised manuscript.  \n\nWe hope reviewer would agree that this was merely mentioned as an implication and not as the main contribution. Our contributions stand without this implication and the advantages of our approach w.r.t. existing approach, e.g., high accuracy, theoretical support, etc. ", "title": "Reviewer is correct"}, "nWTPCjaHmJc": {"type": "rebuttal", "replyto": "hBEuuyZOGmu", "comment": "We assume that R2 agrees with hypothesis-related and theoretical contributions of the paper that are independent of our algorithm related contribution, *biprop*, and he/she wants to understand advantages  provided by our algorithm. We note that *biprop* is one of many possibilities for identifying multi-prize tickets (MPTs) supported by the theory provided in our paper. Below is our response on the algorithmic contributions from two perspectives:\n\n1. **What our implementation of *biprop* has already achieved**: \n    * Without significant hyperparameter tuning and without commonly used tricks in binary neural networks (BNNs), MPTs identified using *biprop* match or outperform BNNs trained using weight-optimization based methods. Further, MPT-1/32 are current state-of-the-art (SOTA) for binary-weight networks. We also note that additional hyperparameter tuning and regularization tricks from the BNN literature are expected to further improve these gains based on their effectiveness for existing BNNs. This already emphasizes the significant practical gain and potential of MPTs identified using *biprop*. \n    * Another aspect that was not the main focus of the paper and R2 seems to be interested in is to find out the training time comparison of *biprop* with weight-optimization of BNNs and full precision (FP) neural networks (NN). To compare *biprop* with weight-optimization of BNNs, we first note that gradient-descent based BNN weight-training usually requires significantly more epochs than FP NN. For example the BinaryConnect, ProxQuant, and DSQ baselines for CIFAR-10 (in Section 3.2) require 300-400 epochs. In comparison, our MPT-1/32 networks on CIFAR-10 were identified using *biprop* for 250 epochs. Therefore, informally we could say that in terms of train time: *biprop* < weight-trained BNN; in terms of accuracy we can say: MPT > BNN. So you could say our approach is faster to train than SOTA binary-weight methods as well as achieves higher accuracy. The reason we have not emphasized this in the paper is due to the fact that this was not the focus of the paper. Further, a carefully designed experimental study would need to be conducted to reliably report the comparison. For both MPTs and FP weight-trained baselines, we used the same number of epochs as mentioned in the Appendix A.1.\n\n2. **What other \u201calgorithmic implication or innovations\u201d can be enabled to find MPTs**:\n    * Although we used a gradient-based approach in *biprop*, our general framework relying on pruning and quantization could potentially utilize gradient-free techniques. This would be significant to the binary neural network community as one of the pitfalls of gradient-based weight training is back-propagation through the sign acitvation function. However, this was not the focus of this paper but, instead, just one implication of our approach of pruning and quantizing to identify accurate BNNs.\n\nWe would like to emphasize again that our main contribution (and focus) was not to design a faster training algorithm but instead to theoretically and empirically verify our multi-prize lottery ticket hypothesis. In this regard, we hope the reviewer can agree that *biprop* was successful. Any further implications or benefits afforded by *biprop* or our general framework of pruning and binarization as a training mechanism for BNNs provide promising future avenues of research for the deep learning and binary neural network communities.", "title": "Faster training approach is not the focus of the paper"}, "OESVK7kYiR": {"type": "rebuttal", "replyto": "U_mat0b9iv", "comment": "We thank all the reviewers for such a constructive review! We are encouraged that they found our idea to be novel (R1, R2), innovative (R4), practically significant (R1), as well as theoretically (R1, R3) and empirically (R2, R3, R4) strong. We were pleased to find that R1 and R3 realized the theoretical, empirical, and practical significance of our work and suggested accepting the paper (in R3's case, pending a clarifying response). While R2 and R4 recommended rejection, their main points of contention lie in the brevity of Section 2 (which we have now revised and extended by nearly two pages) which resulted in their interpretation that our work simply applies the original lottery ticket hypothesis to binary neural networks. In fact, we have not only proposed an entirely new hypothesis but our hypothesis (1) is stronger than the original lottery ticket hypothesis, (2) theoretically proved in our work, and (3) was empirically verified by conducting extensive experiments on small- and large-scale data sets (and achieving strong results). Additionally, we provide an efficient algorithm for testing our hypothesis. It is for these reasons we believe that our work is a more significant contribution than was initially precieved from the first draft of our paper. We hope that our revisions will clarify these points and that R2 and R4 will also champion our paper for acceptance. \n\nWe are very grateful for each reviewer's comments and perspective and have used their feedback to improve our paper. We have worked through our manuscript and have made several changes to the main body of the paper that provide more detail and insight into our theory and the algorithm. Additionally, we have strengthened our empirical results. In particular, we have made the following changes:\n\n1. **Section 2.1 of the paper was revised and expanded to include the following**:\n    a) Clearer statement of the theoretical result\n    b) Proof sketch for the theoretical result\n    c) Brief discussion and implications of theoretical result\n2. **Section 2.2 of the paper was revised and expanded to include the following**:\n    a) Thorough discussion and motivation for our method for identifying multi-prize tickets (MPTs)\n    b) Detailed pseudocode for our algorithm used to empirically verify the Multi-Prize Lottery Ticket Hypothesis (biprop)\n    c) Motivation for our choice of gradient estimator and new insight on its computational efficiency\n3. **Updated empirical results in Section 3.2**:\n    a) Improved accuracy and reduced parameter count of MPT-1/1 on CIFAR-10 by tuning hyperparameters in algorithm (biprop)\n    b) Fixed a typo regarding BNN-1/1 SOTA on ImageNet (IR-Net): in the initial version we reported IR-Net to be achieveing 66.5% top-1 accuracy -- this was a typo and correct accuracy is 62.9% -- further reducing the gap between MPt-1/1 and SOTA.\n\nUpon reflection, we believe that these changes have in fact made the paper stronger. We hope that these revisions and additions address all of your questions and resolve each of your concerns.\n\n\nPlease see answers to individual reviewer below, for particular comments.\n\n--------\n\nBased on a brief response by R2, we have made the following additional revisions so Section 2.2 to improve clarity: a) Added line numbers to Algorithm 1 and referenced these numbers in discussion of Algorithm 1, b) mentioned use of minibatches by Algorithm 1. Additionally, we have removed the use of the terminology \"untrained\" throughout the paper. However, we note that our claims and contributions are not affected by removing this terminology.\n\n\n", "title": "Joint Response "}, "7GIiSKjJ1fC": {"type": "rebuttal", "replyto": "lNlT_ly4yr", "comment": "Following our rebuttal and discussion with rest of the reviewers, we hope that you will find your main concerns addressed, and you will also champion our paper for acceptance. If you have any remaining concerns or questions, please let us know before the discussion period ends so that we can address them.", "title": "Follow-up on rebuttal"}, "MHIxRQUYnYK": {"type": "rebuttal", "replyto": "Q9kdGneVlzw", "comment": "Thanks a lot for giving us the opportunity to clarify your doubts. \n\n> In Alg. 1, the only loop is the epochs. There are no loop over minibatches. Is biprop a batch algorithm?\n\nIn Algorithm 1, we have provided the pseudocode for \u201cbiprop\u201d framework. In our implementation, as we use minibatch variant of the algorithm, there is another loop over minibatches. The implementation details are already highlighted in the appendix with details on the batchsize used. If reviewer suggests, we can add the minibatch loop in the Algorithm in the revised version. We skipped it in this version for clarity -- to reduce the number of notations in the pseudocode. We also plan to make our sourcecode open upon publication.   \nPlease let us know if this is not clear and we will be happy to clarify any other specific concerns you might have. \n\n>I am also not sure about why does \"untrained\" matter. \n\nAs the term \u201cuntrained\u201d is causing confusion, we will remove this term from the revised paper. Note that, our claims and contributions are not affected by using this term. \n\n>From my understanding, finding the winning ticket behaves similarly as training, and the pruning scores are just \"trained\" like the weights.\n\nAs we have mentioned in Section 4: although we used gradient-based approaches in this paper to find MPTs, biprop is flexible to accommodate different class of algorithms that might avoid the pitfalls of gradient-based weight training.  Next, in contrast to weight-optimization that requires large model size and massive compute resources to achieve high performance, our hypothesis suggests that one can achieve similar performance without ever optimizing over weights of the large model.\nWe would like to emphasize again that we have proposed new paradigm to learn binary neural nets and have made notable progress on three fronts as mentioned in the previous response. \n\n>Is \"untrained\" only of theoretical interest, or does it have any practical implications, such as faster convergence of the \"training\" algorithm?\n\nAs mentioned above, the use of term \u201cuntrained\u201d does not affect the claims and contributions of the paper. For a detailed discussion on the implication of the framework, please see Section 4 where we have also provided specific directions to be explored to achieve these implications.\nAlso note that, both MPTs and FP weight-trained baselines, we used the same number of epochs as mentioned in the Appendix A.1. Theoretically, MPTs can be trained more efficiently than their FP counterparts. However, achieving this would require their implementation on specialized hardwares, e.g., FPGA, which is out of the scope of our paper.\n\nPlease let us know if you have any further questions. We are hopeful that reviewer will see the contributions and potential of our approach. \n", "title": "Algorithmic details on the batchsize is in the appendix "}, "TBBbtXvdlkk": {"type": "review", "replyto": "U_mat0b9iv", "review": "This work investigated a method of finding a subnetwork of redundant binary networks to gain an overall advantage over pruned or quantized networks.\n\nOne main concern of the reviewer is the similarity between the paper's approach---training a mask over a binary network---and the conventional ternary network.  Are the masked weights analogous to the 0 of the ternary networks, while the unmasked weights are in {-1, 1}? In that case, it's fairer to compare with ternary networks with 0 counted into the total params.\n\nFollowing this analogy, it is also misleading to claim that the network is \"untrained\", as to minimize the loss, any binarized weights can be updated to 0(masked), although indeed the weight update across 0 is forbidden. \n\nThe theory works of this paper are strong and prove that the expressive power of redundant binary(or ternary?) networks can match their denser counterpart.  Still, the question to clarify here is that whether \"subset (lottery ticket) + binary network\" equals to \"ternary network\". \n\nIn general, the paper is well written and the theoretical and experimental works support the authors' claim. The reviewer would recommend accepting the paper on the condition that the authors can address the comparison with the ternary network fairly.\n\n", "title": "lottery mask + binary = Ternary ?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "EQud3tNkyC8": {"type": "rebuttal", "replyto": "-KKxf_J3uLT", "comment": ">Moreover, the algorithm relies heavily on the edge-popup algorithm, which is not explained even in the supplementary material. \n\n\nWe understand why this confusion might have been caused and it might have appeared that we rely heavily on the edge-popup. First, we want to emphasize that the implementation of \u2018biprop\u2019 is motivated by a more general framework accommodates a large class of potential algorithms. In our experiments, we used a specific version that had some connections with \u2018edge-popup\u2019, but we want to highlight that the only common theme between our implementation of the pruning step and \u2018edge-popup\u2019 is that both of us use a common trick of optimizing over binary mask using real-valued score function-based approximation and truncation. This a well-established relaxation technique used in a range of application (see (Joshi & Boyd, 2009)).\nTo further see the difference, note that \u2018edge-popup\u2019 cannot accommodate \u2018sign\u2019 function to be present in the network. Having \u2018sign\u2019 function in MPT-1/32 requires making non-trivial changes such as solving a different optimization problem to approximate a FP NN with binary quantizer and the gain term (see Appendix C and also revised Section 2.2). We derive optimal binary quantizer and the gain term in a closed form. Further, due to binary nature of the activations in MPT-1/1, the gradient vanishes almost everywhere, which is undesirable for the standard back-propagation based score updates for pruning. To overcome this challenge, we use a gradient estimator in the backward pass. During the rebuttal period, we realized a novel interpretation of the gradient estimator using quadratic spline parameterization \u2013 this interpretation may be of an independent interest to the BNN community. Using this, we show that gradient estimator in (Liu et al., 2018a) is a special case of the discussed method. Note that, all of these issues do not appear in \u2018edge-popup\u2019. Furthermore, our implementation of the gradient estimator is 6x more memory efficient than the implementation provided in the code used by the authors of (Liu et al., 2018a).\n\nWe have highlighted this in the revised version of the paper and explained our algorithm in detail. \n\n\n>Moreover, pruning from a larger network is known to achieve better result than training a smaller network from scratch, but the baselines does not utilize this. \n\nNote that the baselines we use are weight-trained full precision NNs with the same architecture that is randomly initialized to search MPTs. In other words, we use the strongest baseline that is weight-trained, full precision, LARGE/original neural net and not a smaller network as pointed out by the reviewer. As we show in the paper that our approach is comparable to (or beat) even full precision neural networks trained using weight-optimization for all considered cases except MPT-1/1 on ImageNet. Thus, they match (or beat) any M-ary quantization including ternary (or pruned binary) neural networks trained using weight-optimization. This further emphasizes the impressiveness of our results. \n\nAlso please see our response to Reviewer 3 on the comparison with ternary NN. \n\n\n\n\n>There lacks any discussion on the real time consumption of the proposed network.Time consumption of training should also be reported.\n \nFor both MPTs and FP weight-train baselines, we used same number of epochs as mentioned in the Appendix A.1. Theoretically, MPTs can be trained more efficiently than their FP counterparts. However, achieving this and a fair train time comparison would require their implementation on the specialized hardware, e.g., FPGA and, this is out of the scope of our paper. Although minimizing training time was not our focus, we have mentioned in Section 4 (Algorithmic) addition ways to accelerate the training. Informally, we could say that MPTs require less time to train than both FP and quantized NNs. \n", "title": "Improved writing and additional results have made the paper stronger 2/3"}, "-KKxf_J3uLT": {"type": "rebuttal", "replyto": "hzs2S6hNoN", "comment": ">However, I have some concerns on this paper. This paper, at least the main text, is not self-contained. The writing needs significant improvement. \n\nIn the revised version of the paper, we have made the main text self-contained by bringing back details from the appendix to the main body. We would like to emphasize that most of these details were not missing from the paper but were pushed to the appendix to make the paper more readable (at the cost of self-containment). We are grateful to the reviewer for this suggestion as the revised reorganization has made the clarity of the paper significantly better without missing any major details. If there is any specific part of the paper that reviewer feels still need more details, we can make that change. \n\n\n>The main contribution of the paper, section 2, is only one-page long. \n\nWe would like to bring the attention of the reviewer to the fact that the challenge we faced was not that we did not have sufficient technical results but that we made significantly more technical contribution than we could fit in the main body of the paper. Note that related full-precision NN papers are either empirical (Frankle 2018 and Zhou 2019), algorithmic (Ramanujan 2020) or theoretical (Malach 2020). We have made notable progress on all three fronts: \n1. proposed generalization of (or much stronger) lottery ticket hypothesis -- highlights the extent of the DNN compression and a new paradigm to learn binary neural nets,\n2. developed high performing algorithms to find these winning tickets \u2013 our MPT-1/32 are current SOTA for BWNs and also beat their large and FP counterparts without any significant hyperparameter tuning or commonly used tricks for BNNs, and \n3. presenting first theoretical result that pruned BNNs are universal approximators -- proving that pruning a randomly initialized binary-weight DNN can approximate a real-valued target DNN.\n\nFurther, these advances are not a trivial extension of the related full-precision literature (explained later). \n\n>Though the proposed algorithm achieves excellent results in terms of parameter count and accuracy, I think the comparison is somewhat unfair. The subnetwork is sparse and can be much slower on real hardware. \n\nWe respectfully disagree with the comment that \u201cthe comparison is unfair as subnetwork is sparse and can be much slower on real hardware\u201d. This comment is not specific our approach but to any pruning-based approach. We think what reviewer is trying to say is that sparse NNs cannot achieve meaningful speedup on commodity hardware (e.g., GPU) built for dense matrix computations. This is indeed true and this is the precise reason that specialized accelerators are designed to exploit the sparsity. However, this does not refute our claims or make the comparison unfair in any way. The sparse model has fewer parameters and, theoretically, less computation costs and approaches such as [2,3] can be leveraged to achieve significant speed ups. However, the hardware implementation of MPTs is out of the scope of this paper and is mentioned as a worthwhile future direction in the revised manuscript.\n\n>Neither the theory nor the algorithm are well explained in the main text. \n\nWe are sorry for this and thank you for giving us an opportunity to fix this. \n1. In the initial version, we only provided an informal statement of our main theoretical result in the main body to improve the readability. In the revised version, we have provided more specific details on the result and also provided a proof sketch in the main body of the paper. \n2. In the initial version of the paper, we provided a general pseudocode for the \u2018biprop\u2019 framework accommodating a class of potential algorithms. In the revised version, we have instead provided a detailed algorithmic description and theoretical justification of the specific algorithm used in our experimental section. \n\n\n\n\nWe hope that these changes have made theory and algorithm clear. We would be happy to make any further changes suggested by the reviewer.\n", "title": "Improved writing and additional results have made the paper stronger 1/3"}, "yEZovBSjIG3": {"type": "rebuttal", "replyto": "EQud3tNkyC8", "comment": "> This paper propose utilizing the existing \"lottery ticket\" result for constructing binary neural networks.\n\nWe respectfully disagree with the reviewer on this and would like to clarify. Our work does not utilize the existing lottery ticket hypothesis (Frankle 2018) to construct/train BNNs. Our hypothesis significantly generalizes existing results, and our algorithms and theory are entirely different.\n\nWe have made original contributions on three fronts: \n1. Proposed generalization of (or much stronger) lottery ticket hypothesis -- highlights the extent of the DNN compression and a new paradigm to learn compact yet highly accurate binary neural nets,\n2. Developed high performing algorithms to find these winning tickets \u2013 our MPT-1/32 are current SOTA for BWNs and also beat their large and FP counterparts without any significant hyperparameter tuning or commonly used tricks for BNNs, and \n3. Presenting first theoretical result that pruned BNNs are universal approximators -- proving that pruning a randomly initialized binary-weight DNN can approximate a real-valued target DNN.\n\nHence, these advances are not a straightforward utilization of the related full-precision literature.\n\nWe would further like to point you to our joint response to all the reviewers to highlight novelty, revised results, and most importantly usefulness and implications of our approach on BNNs and DNNs in general. \n\nWe hope that the reviewer would agree that the comparison is fair, and our results are much stronger than initially perceived. We would be happy to clarify any further questions/concerns the reviewer might have. \n\n\n## Sources\n\n[1] Li, Fengfu, Bo Zhang, and Bin Liu. \"Ternary weight networks.\" arXiv preprint arXiv:1605.04711 (2016).\n[2] Wang, Ziheng. \"SparseRT: Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference.\" Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques. 2020.\n[3] Ardakani, Arash, Carlo Condo, and Warren J. Gross. \"Sparsely-connected neural networks: towards efficient vlsi implementation of deep neural networks.\" arXiv preprint arXiv:1611.01427 (2016).", "title": "Improved writing and additional results have made the paper stronger 3/3"}, "ihkSusQkRMC": {"type": "rebuttal", "replyto": "ttGCfrwhe02", "comment": "\n>The success essentials of their algorithm \u201cbiprop\u201d contains two-part: edge-popup and gradient estimator. Both of them are take-away from other works.\n>\n\nWe understand why this confusion might have been caused and it might have appeared that we rely heavily on the edge-popup and the gradient estimator. First, we want to emphasize that the proposed \u2018biprop\u2019 framework accommodates a large class of potential algorithms. In our experiments, we used a specific version that had some connections with \u2018edge-popup\u2019, but we want to highlight that the only common theme between our implementation of the pruning step and \u2018edge-popup\u2019 is that both of us use a common trick of optimizing over binary mask using real-valued score function-based approximation and truncation. This a well-established relaxation technique used in a range of application (see (Joshi & Boyd, 2009)). To further see the difference, note that \u2018edge-popup\u2019 cannot accommodate \u2018sign\u2019 function to be present in the network. Having \u2018sign\u2019 function in MPT-1/32 requires making non-trivial changes such as solving a different optimization problem to approximate a FP NN with binary quantizer and the gain term (see Appendix C and also revised Section 2.2). We derive optimal binary quantizer and the gain term in a closed form. Further, due to binary nature of the activations in MPT-1/1, the gradient vanishes almost everywhere, which is undesirable for the standard back-propagation based score updates for pruning. To overcome this challenge, we use a gradient estimator in the backward pass as pointed out by the reviewer. During the rebuttal period, we realized a novel interpretation of the gradient estimator using quadratic spline parameterization \u2013 this interpretation may be of an independent interest to the BNN community. Using this, we show that existing gradient estimator in (Liu et al., 2018a) is a special case of the discussed method. Furthermore, our implementation of the gradient estimator is 6x more memory efficient than the implementation provided in the code used by the authors of (Liu et al., 2018a). Note that all of these issues do not appear in \u2018edge-popup\u2019 and further our approach is not a trivial plug-and-play between \u2018edge-popup\u2019 and gradient estimation technique.\n\nWe have highlighted this in the revised version of the paper and explained our algorithm in detail. \n\n\n\n>Originality: Medium. They apply the Lottery Ticket Hypothesis to quantization/BNN. I regard this work as a new application of LTH to binary neural networks.\n>\n\nWe respectfully disagree with the reviewer on this. Our work is not trivially applying existing lottery ticket hypothesis (Frankle 2018) to BNNs. Our hypothesis significantly generalizes existing results, and our algorithms and theory are entirely different.\n\nWe have made original contributions on three fronts: \n1. Proposed generalization of (or much stronger) lottery ticket hypothesis -- highlights the extent of the DNN compression and a new paradigm to learn compact yet highly accurate binary neural nets,\n2. Developed high performing algorithms to find these winning tickets \u2013 our MPT-1/32 are current SOTA for BWNs and also beat their large and FP counterparts without any significant hyperparameter tuning or commonly used tricks for BNNs, and \n3. Presenting first theoretical result that pruned BNNs are universal approximators -- proving that pruning a randomly initialized binary-weight DNN can approximate a real-valued target DNN.\n\nFurther, these advances are not a trivial extension of the related full-precision literature. \n\n\n\n>Weak theory, unclear paper writing, difficult to follow\n>\nWe are sorry for this and thank you for giving us an opportunity to fix this. \n1. In the initial version, we only provided an informal statement of our main theoretical result in the main body to improve the readability. In the revised version, we have provided more specific details on the result and also provided a proof sketch in the main body of the paper. \n2. In the initial version of the paper, we provided a general pseudocode for the \u2018biprop\u2019 framework accommodating a class of potential algorithms. In the revised version, we have instead provided a detailed algorithmic description and theoretical justification of the specific algorithm used in our experimental section. \n\nWe hope that these changes have made theory and algorithm clear. We would be happy to make any further changes suggested by the reviewer.\n\n\nWe would further like to point you to our joint response to all the reviewers to highlight novelty, revised results, and most importantly usefulness and implications of our approach on BNNs and DNNs in general. \n\nWe hope that the reviewer would agree that the comparison is fair, and our results are much stronger than initially perceived. We would be happy to clarify any further questions/concerns the reviewer might have. \n", "title": "Improved writing and additional results have made the paper stronger 2/2"}, "ttGCfrwhe02": {"type": "rebuttal", "replyto": "lNlT_ly4yr", "comment": ">The main article spends little word to describe how to find the mask, and it is not a trivial way. \n\nWe are sorry for this and thank you for giving us an opportunity to fix this. \n\nIn the initial version of the paper, we provided a general pseudocode for the \u2018biprop\u2019 framework accommodating a class of potential algorithms. In the revised version, we have instead provided a detailed algorithmic description and theoretical justification of the specific algorithm used in our experimental section (see Section 2.2). Furthermore, we have moved key technical details for finding MPTs from the appendix to the main body. \n\nWe hope that these changes have made our algorithm clear. If there is any specific part of the paper that reviewer feels still need more details, we can make that change. \n\n\n> Besides, since many parts are moved into the appendix, the whole structure of the main body is kind of empty and shallow.\n \nIn the revised version of the paper, we have made the main text self-contained by bringing back details from the appendix to the main body. This modifications were primarily made to Section 2 in the paper. We would like to emphasize that most of these details were not missing from the paper but were pushed to the appendix to make the paper more readable (at the cost of self-containment). We are grateful to the reviewer for this suggestion as the revised reorganization has made the clarity of the paper significantly better without missing any major details. If there is any specific part of the paper that reviewer feels still need more details, we can make that change. \n\n>The experiments of generization are only done on the very small NN (e.g. Conv2/4/6/8).\n>\n\nWe are assuming the question is \"Why in Section 3.1 we do not consider large models?\" This is done for two main reasons \n1. Smaller models were faster to train and that gave us an opportunity to perform an in-depth analysis (e.g., varying depth, width, and pruning rate over multiple runs) so that we could obtain reliable empirical patterns \n2. For our approach, a less overparameterized region is more challenging (as discussed in our theoretical result). Furthermore, if our hypothesis holds for smaller models then it automatically extends to larger models. This is precisely what we showed in Section 3.2 where we considered larger models. \n\nTo summarize, experiments on smaller models in not a limitation but the strength of Section 3.1.\n\n\n\n>Many sentences in this paper are quite long and sometimes using nesting clauses, which makes the text to be obscure. Some summative and conclusive paragraphs are the simple repetition of previous \u201cclaims\u201d since the demonstrations are in the appendix. All of these make a lower clarity. \n\n\nWe have proof-read the whole paper again and tried our best to fix these issues. We would be grateful if reviewer could point us to any such places that we might have missed changing.\n\n\n\n\n## Sources\n\n[1] Li, Fengfu, Bo Zhang, and Bin Liu. \"Ternary weight networks.\" arXiv preprint arXiv:1605.04711 (2016).\n[2] Wang, Ziheng. \"SparseRT: Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference.\" Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques. 2020.\n[3] Ardakani, Arash, Carlo Condo, and Warren J. Gross. \"Sparsely-connected neural networks: towards efficient vlsi implementation of deep neural networks.\" arXiv preprint arXiv:1611.01427 (2016).", "title": "Improved writing and additional results have made the paper stronger 1/2"}, "F55M2tWwJgN": {"type": "rebuttal", "replyto": "TBBbtXvdlkk", "comment": ">Are the masked weights analogous to the 0 of the ternary networks, while the unmasked weights are in {-1, 1}? In that case, it's fairer to compare with ternary networks with 0 counted into the total params.\n\nZero is not counted into the total parameters to highlight the amount of sparsity in the obtained networks. If we count '0' towards the total parameter count, the number of total parameters for all networks will be the same and this will be unable to highlight the performance of the pruning. Reporting sparsity is a well-accepted practice/metric in the pruning literature -- in fact the goal of  pruning is precisely to increase the amount of sparsity (weight values equal to \u20180\u2019) without dropping the performance. The reason being that having sparser NNs may enable the application of sparse data structures for memory and compute efficiency. To clarify this, in the revised manuscript, we mention \u201cnon-zero\u201d parameters when reporting these numbers.\n\n>Following this analogy, it is also misleading to claim that the network is \"untrained\", as to minimize the loss, any binarized weights can be updated to 0(masked), although indeed the weight update across 0 is forbidden.\n>\n\nThe reason for using the term was to differentiate our approach from conventional approaches that optimize over the weights as we are only dropping/pruning connections from a randomly initialized/weighted binary neural network. The reviewer correctly pointed out that we indeed optimize over the mask (\u20180\u2019 values). To make our usage clear, we have clarified the term \u201cuntrained\u201d in the revised manuscript. However, we are open to completely removing this term if reviewer suggests so. \n\n>The theory works of this paper are strong ...redundant binary (or ternary?) networks can match their denser counterpart. Still, the question to clarify here is that whether \"subset (lottery ticket) + binary network\" equals to \"ternary network\".\n\nOur work proves the expressive power of pruning randomly initialized/weight binary neural networks. We would prefer using the term pruned randomly initialized binary neural networks to avoid confusing our approach with ternary neural network approaches in the literature [1]. Comparison with TNN is discussed next.\n\n>The reviewer would recommend accepting the paper on the condition that the authors can address the comparison with the ternary network fairly.\n\nThank you for asking and giving us an opportunity to answer this excellent question.\nNote that our approach does not fall under any existing framework including binary and ternary neural networks learned via weight-optimization. One could claim that our setting is more restrictive than even binary neural networks/BNN (let alone ternary neural networks) as we learn by pruning \u201crandomly weighted\u201d binary neural network. In other words, unlike existing BNN approaches, we do not optimize over weights and learn purely by pruning \u2013 pruning mask is the only optimizable parameter. As we show in the paper, our approach can match (or beat) even full precision neural networks trained using weight-optimization for all considered cases except MPT-1/1 on ImageNet. Thus, they match (or beat) any M-ary quantization including ternary neural networks.\n\nThe reviewer is correct in pointing out that weights in the original/large structure can be seen as taking ternary values as pruned weights can be consider having the value \u20180\u2019. But note the activations still take binary values not ternary in MPT-1/1. We differ from ternary weight networks (TWNs) [1] due to the following reasons: 1) In addition to unpruned weights in MPTs (i.e., {+1,-1}) not being trained, we allow a precise control over the sparsity, i.e., number of weights taking the value \u20180\u2019, and 2) As our goal is pruning, our solutions are significantly sparser than TWNs \u2013 TWNs achieve their best solutions with sparsity < 50% (usually 20%-40%), and our solutions are always with sparsity >50% (usually 60%-95%). This enables higher energy efficiency and more memory saving (sometimes even when compared to BNNs).\n\nInformally, we could say that (a) in terms of memory and compute requirements: MPT $\\leq$ BNN < TNN < FPN, and (b) in terms of performance/accuracy: MPT $\\geq$ FPN > TNN > BNN (for MPT-1/32).\n\nWe would further like to point you to our joint response to all the reviewers to highlight novelty, revised results, and most importantly usefulness and implications of our approach on BNNs and DNNs in general. We hope that the reviewer would agree that the comparison is fair, and our results are much stronger than initially perceived. We would be happy to clarify any further questions/concerns the reviewer might have.\n\n[1] Ternary weight networks. arXiv:1605.04711 (2016).\n[2] SparseRT: Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference.  https://arxiv.org/abs/2008.11849\n[3] Sparsely-connected neural networks: towards efficient vlsi implementation of deep neural networks. arXiv:1611.01427 (2016).", "title": "We have clarified the difference with ternary network"}, "uYVRWdwxT2d": {"type": "rebuttal", "replyto": "0hEFbcUJV6V", "comment": "\n>[c] The MPT 1/1 seems not to perform as well compared to trained binary activation network, which may reduce the quality of paper. It is good that the authors mention future work for MPT 1/1 network to attain state-of-art results of binary activation network. \n\n\nNote that the main contribution of this work is to provide a new paradigm for learning compact yet accurate binary neural networks by pruning and quantizing randomly weighted full precision DNNs. The only case where MPT-1/1 performance is not comparable to the performance of BNN SOTA is ImageNet dataset. \n\nNote we have quickly tuned hyperparameters for the MPT-1/1 on CIFAR-10. This has resulted in a boost in accuracy from 88.22\\% to 89.59\\% and a reduction in the parameter count from 3.68M to 1.61M. The best performing methods DSQ with VGG-small achieves 91.7\\% accuracy, however at the cost of (a) almost three-times more parameters, and (b) a more complex quantization scheme. We are confident that some of the tricks used in the BNN literature can be used in the MPT setup as well and will in turn make MPT-1/1 perform better than SOTA BNNs.\n\n\n>[d] Some of the references seem to lack information, e.g. Malach 2020, Orseau 2020, both of which do not have the venue or journal names.\n\nThank you for pointing this out. We have fixed this issue in the revised version.\n\n>[e] A question: for the binary weights subnetworks, what does it mean by 20 million parameters? The weights can only be -1 or 1, so the network has 20 million of -1 or 1, but they do not require any multiplication at the inference stage?\n\nThe reviewer's interpretation is correct \u2013 it would imply having have 20 million parameters (or weights/connections) that have values either -1 or +1. Note that due to the binarization, the heavy matrix multiplication operations (MAC) can be replaced with light-weight bitwise XNOR operations and Bitcount operations.\n\n\nWe would further like to point you to our joint response to all the reviewers to highlight novelty, revised results, and most importantly usefulness and implications of our approach on BNNs and DNNs in general. \n\nWe hope that the reviewer would agree that the comparison is fair, and our results are much stronger than initially perceived. We would be happy to clarify any further questions/concerns the reviewer might have.\n\n## Sources\n\n[1] Li, Fengfu, Bo Zhang, and Bin Liu. \"Ternary weight networks.\" arXiv preprint arXiv:1605.04711 (2016).\n[2] Wang, Ziheng. \"SparseRT: Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference.\" Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques. 2020.\n[3] Ardakani, Arash, Carlo Condo, and Warren J. Gross. \"Sparsely-connected neural networks: towards efficient vlsi implementation of deep neural networks.\" arXiv preprint arXiv:1611.01427 (2016).", "title": "We have addressed the minor concerns 2/2"}, "0hEFbcUJV6V": {"type": "rebuttal", "replyto": "kExE_sjVaEv", "comment": "Thank you for your positive review! \n\n>[a] It would be more convincing if the authors could further highlight the technical novelty of this work for the proof of Theorem 1. Compared with Malach 2020 \u201cProving the lottery ticket hypothesis: pruning is all you need\u201d, the authors can highlight what key differences are needed for proof of the binary network.\n\nTo the best of our knowledge ours is the first theoretical result proving that pruning a randomly initialized binary-weight DNN can approximate a real-valued target DNN. As it has been established that real-valued DNNs are universal approximators (Scarselli & Tsoi, 1998), our result carries the implication that pruned binary-weight DNNs are universal approximators as well. In relation to the first result establishing the existence of real-valued subnetworks in a randomly weighted DNN approximating a real-valued target DNN (Malach et al., 2020), the lower bound on the width established in Theorem 2 is better than their lower bound of $O\\left( \\ell^2 n^2 \\log(\\ell n / \\delta) /\\varepsilon^2 \\right)$. While the methodology of our proof is similar to (Malach et al., 2020), proving our result for binary-weight subnetworks is not a trivial extension of their technique and require different tools. The use of binary weights in the analysis results in more complex scenarios that have to be addressed carefully. In particular, the analysis in Lemma 1 requires cleverly splitting a complex scenario involving a multinomial distribution into two simpler cases involving binomial distributions. Then, Hoeffding's inequality is used to derive probabilistic bounds for in each case. The results are combined to yield the final probabilistic bound. This analysis is more complex than the analysis required for real-valued subnetworks as Hoeffding's inequality was not required to establish an analogous result for real-valued subnetworks (Malach et al., 2020). Additionally, the use of binary weights required us to identify an appropriate gain term for rescaling the binary weights in our analysis. Binary weights also result in potentially larger subnetworks being required to approximate a target network and we must carefully keep track of these requirements as we establish results for increasingly larger networks in Lemma 2, Lemma 3, and Theorem 2 in Appendix B. In the work on real-valued networks (Malach et al., 2020) the bound on the required number of parameters in the subnetwork is more straightforward. Furthermore, our analysis yields a tighter bound on the width of the binary network than the bound established in (Malach et al., 2020) for real-valued subnetworks. We hope that this insight into some of the complexity required when analyzing binary subnetworks in our paper illustrates how the analysis is more complex when working with binary subnetworks.\n\n>[b] For the comparison of full precision network and the MPT from this paper, it can be useful if the computational complexity is shown, where the complexity for MPT would be for the algorithm to find the mutli-prize tickets in an over-parameterized network.\n\nFor both MPTs and FP weight-trained baselines, we used the same number of epochs as mentioned in the Appendix A.1. Theoretically, MPTs can be trained more efficiently than their FP counterparts. However, achieving this would require their implementation on specialized hardwares, e.g., FPGA, and, this is out of the scope of our paper. \n", "title": "We have addressed the minor concerns 1/2"}, "TbIWbJzpfXy": {"type": "rebuttal", "replyto": "lNlT_ly4yr", "comment": "Q. Finally, I find that this paper has narrower page margins, which means each line can contain more characters. Besides, the header \u201cUnder review as a conference paper at \u2026\u201d is missing. These modifications of the submit template may volatile the conference rule and should be considered a cheating behavior.\n\nA. We  thank  the  reviewer  for  pointing  this  out.   \n\nThese  changes  were  not  intentional  but  caused  by mistakenly using the \u201cfullpage\u201d package with the ICLR 21 official latex template and that caused this change.   This  happened when transferring our  draft and used  packages (which included  the\u201cfullpage\u201d package) into the .tex file with the official ICLR 21 latex template.\n\nFurther  we  want  to  highlight  that  this  change  has  not  given  us  any  unfair  advantage  \u2013  for  your reference we have uploaded a revised version with the \u201cfullpage\u201d package removed showing that the same information exists in the version with and without this issue.\n\nFor these reasons, we hope that the reviewer will not take this issue into account in his/her revised score.\n\nWe will respond to your other concerns very soon. Thank you!", "title": "\u201cfullpage\u201d package with the ICLR latex template caused formatting issue "}, "lNlT_ly4yr": {"type": "review", "replyto": "U_mat0b9iv", "review": "The paper proposes an innovate method based on lottery ticket hypothesis to prune a BNN (parameters are only -1(0) and +1, it can be viewed as an extreme case of quantization) from a dense NN. It focuses on learning a mask to prune the NN instead of the traditional method (pruning on an already trained network). In addition, not only experiments but theortical proof are given and have a highly brief result.\n\nPro:\nThe way to find the mask iteratively is innovate and has a mathematical support.\nThe result of MPT is amazing because the untrained network can be pruned to a BNN with comparable accrancy of some trained SOTA NN on CIFAR dataset.\nThe experiments show it can be generized to deeper and wider network.\nIt has better accurancy than other BNN methods but network parameters still high.\nCon:\nThe main article spends little word to describe how to find the mask, and it is not a trivial way.\nThe experiments of generization are only done on the very small NN (e.g. Conv2/4/6/8).\n\nClarity: Very low. Pros: The authors try to express in a way that every step of logical connections in this paper can be clearly understood by readers. To reduce complexity, many parts are settled in the appendix. Cons: Many sentences in this paper are quite long and sometimes using nesting clauses, which makes the text to be obscure. Besides, since many parts are moved into the appendix, the whole structure of the main body is kind of empty and shallow. Some summative and conclusive paragraphs are the simple repetition of previous \u201cclaims\u201d since the demonstrations are in the appendix. All of these make a lower clarity. \nFinally, I find that this paper has narrower page margins, which means each line can contain more characters. Besides, the header \u201cUnder review as a conference paper at \u2026\u201d is missing. These modifications of the submit template may volatile the conference rule and should be considered a cheating behavior. So I give the \u201cvery low\u201d score on clarity.\n\nOriginality: Medium. Pros: They apply the Lottery Ticket Hypothesis to quantization/BNN. They give proof of their rationality. Cons: The success essentials of their algorithm \u201cbiprop\u201d contains two-part: edge-popup and gradient estimator. Both of them are take-away from other works. I regard this work as a new application of LTH to binary neural networks.\n", "title": "Good motivation and findings with seem-solid but actually weak theory. unclear paper writing, difficult to follow", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "kExE_sjVaEv": {"type": "review", "replyto": "U_mat0b9iv", "review": "The authors propose a stronger lottery ticket hypothesis in this paper \u2013 the multi-prize lottery ticket hypothesis. In particular, the new hypothesis seeks answer to the required amount of over-parameterization for a randomly initialized network to become able to compress to a sparse untrained binary subnetwork with on-par accuracy. The authors prove the existence of such subnetwork and show the bounds on over-parameterization. The paper proposes new methods to get the binary-weight tickets and the binary-activation tickets, where binary-weight tickets are subnetworks with weights as binary, and binary-activation tickets have the activation function in the forward propagation as binary. As binary networks can largely reduce the computational complexity for inference, this work has practical importance especially for applications with constraints for memory and power. The paper has many simulation results to support the theoretical guarantees, and the proposed approach on binary-weight networks has advantages over existing methods.\n\nThe paper has sufficient and novel contributions, both for the theoretical results on binary subnetworks and the empirical evaluations that reveal the efficiency of the algorithm on binary-weight subnetworks, so that I recommend this paper for publication. \n\nHere are some minor concerns.\n\n[a] It would be more convincing if the authors could further highlight the technical novelty of this work for the proof of Theorem 1. Compared with Malach 2020 \u201cProving the lottery ticket hypothesis: pruning is all you need\u201d, the authors can highlight what key differences are needed for proof of the binary network.\n\n[b] For the comparison of full precision network and the MPT from this paper, it can be useful if the computational complexity is shown, where the complexity for MPT would be for the algorithm to find the mutli-prize tickets in an over-parameterized network.\n\n[c] The MPT 1/1 seems not to perform as well compared to trained binary activation network, which may reduce the quality of paper. It is good that the authors mention future work for MPT 1/1 network to attain state-of-art results of binary activation network.\n\n[d] Some of the references seem to lack information, e.g. Malach 2020, Orseau 2020, both of which do not have the venue or journal names.\n\n[e] A question: for the binary weights subnetworks, what does it mean by 20 million parameters? The weights can only be -1 or 1, so the network has 20 million of -1 or 1, but they do not require any multiplication at the inference stage?\n", "title": "Review Paper 738", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}