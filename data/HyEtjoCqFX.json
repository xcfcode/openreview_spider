{"paper": {"title": "Soft Q-Learning with Mutual-Information Regularization", "authors": ["Jordi Grau-Moya", "Felix Leibfried", "Peter Vrancx"], "authorids": ["jordi@prowler.io", "felix@prowler.io", "peter@prowler.io"], "summary": "", "abstract": "We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize a prior action distribution for better performance and exploration. Entropy-based regularization has previously been shown to improve both exploration and robustness in challenging sequential decision-making tasks. It does so by encouraging policies to put probability mass on all actions. However, entropy regularization might be undesirable when actions have significantly different importance. In this paper, we propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutual-information. In particular, we express the RL problem as an inference problem where the prior probability distribution over actions is subject to optimization. We show that the prior optimization introduces a mutual-information regularizer in the RL objective. This regularizer encourages the policy to be close to a non-uniform distribution that assigns higher probability mass to more important actions. We empirically demonstrate that our method significantly improves over entropy regularization methods and unregularized methods.", "keywords": ["reinforcement learning", "regularization", "entropy", "mutual information"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a new RL algorithm (MIRL) in the control-as-inference framework that learns a state-independent action prior.  A connection is provided to mutual information regularization.  Compared to entropic regularization, this approach is expected to work better when actions have significantly different importance.    The algorithm is shown to beat baselines in 11 out of 19 Atari games.\n\nThe paper is well written.  The derivation is novel, and the resulting algorithm is interesting and has good empirical results.  A few concerns were raised in initial reviews, including certain questions about experiments and potential negative impacts of the use of nonuniform action priors in MIRL.  The author responses and the new version were quite helpful, and all reviewers agree the paper is an interesting contribution.\n\nIn a revised version, the authors are encouraged to\n  (1) include a discussion of when MIRL might fail, and\n  (2) improve the related work section to compare the proposed method to other entropy regularized RL (sometimes under a different name in the literature), for example the following recent works and the references therein:\n    https://arxiv.org/abs/1705.07798\n    http://proceedings.mlr.press/v70/asadi17a.html\n    http://papers.nips.cc/paper/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning\n    http://proceedings.mlr.press/v80/dai18c.html"}, "review": {"HJe0z7jRJE": {"type": "rebuttal", "replyto": "Byx_Yunv37", "comment": "We are thankful to the reviewer for noticing the improvements and raising the score. \n", "title": "reply"}, "Hkg8ZQo0k4": {"type": "rebuttal", "replyto": "SyejfjKhkN", "comment": "We thank the reviewer for appreciating the improvements of the paper. \n\n\nThe attached link indeed shows a different epsilon value for evaluation (and other hyperparameters) used in this particular DQN implementation. An epsilon value for evaluation that differs from 0.05 was used in some of the previous literature (e.g. distributed DQN in Bellemare et al. 2017, prioritized double DQN in Schaul et al. 2016). However, earlier DQN papers do report an epsilon value of 0.05 for evaluation (original DQN in Mnih et al. 2015, double DQN in van Hasselt et al. 2016, prioritized DQN in Schaul et al. 2016). While an epsilon value of 0.01 might improve evaluation results, we feel a value of 0.05 is not unreasonable since we compare all methods under the same evaluation procedure. Additionally, we chose the other hyperparameters following the original DQN paper (Mnih et. al. 2015).\n\n", "title": "reply"}, "ryea6zsRy4": {"type": "rebuttal", "replyto": "H1l9u2InyN", "comment": "We thank the reviewer for the feedback leading to improvements of the paper. \n\nIn the final version, we will add a couple of additional sentences clarifying why a limit on information rate might be beneficial at initial stages of learning. In short, in prior work, it has been shown that the rate-distortion framework improves generalization in a supervised learning setting (Leibfried and Braun 2016). The intuition is that limits in transmission rate prevent overfitting on the training set. Similarly, in our work for the RL setting, limits in transmission rate prevent the agents to bootstrap with a \u2018harsh\u2019 max-operator that would lead to overestimation and sample inefficiency, but instead use a softened version less prone to overestimation with an adaptive prior that additionally improves exploration. \n", "title": "reply"}, "rkeC9MsAkE": {"type": "rebuttal", "replyto": "SkeEX3Q6JV", "comment": "We thank the reviewer for raising the score and for the additional suggestions on analyzing potential limitations and drawbacks of our method. We will include a paragraph clarifying where our method might fail according to our pilot experiments, and perform additional experiments with a reward structure discouraging an infrequent action that is required to eventually succeed.", "title": "reply"}, "ryl__Ymjh7": {"type": "review", "replyto": "HyEtjoCqFX", "review": "** Summary: **\n\nThe authors use the reformulation of RL as inference and propose to learn the prior policy. The novelty lies in learning a state-independent prior (instead of a state-dependent one) that can help exploration in the presence of universally unnecessary actions. They derive an equivalence to regularizing the mutual information between states and actions.\n\n** Quality: **\nThe paper is mathematically detailed and correct.\n\n** Clarity: **\nThe paper is sufficiently easy to follow and explains all the necessary background.\n\n** Originality & Significance: **\nThe paper proposes a novel idea: Using a learned state-independent prior as opposed to using a learned state-dependent prior. While not a big change in terms of mathematical theory, this could lead to positive and interesting results empirically for exploration. Indeed they show promising results on Atari games: It is easy to see how Atari games could benefit as they have up to 18 different actions, many of which are redundant. \n\nMy two main points where I think the paper could improve are:\n- More experimental results, in particular, how strong are the negative effects of MIRL if we have actions that are important, but have a lower probability in the stationary action distribution?\n- A related work section comparing their approach to the many recent similar papers in Maximum Entropy RL", "title": "Interesting idea, more experimental results needed", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byx_Yunv37": {"type": "review", "replyto": "HyEtjoCqFX", "review": "The authors take the control-as-inference viewpoint and learn a state-independent prior (which is typically held fixed). They claim that this leads to better exploration when actions have different importance. They relate this objective to a mutual information constrained RL objective in a limiting case. They then propose a practical algorithm, MIRL and compare their algorithm against DQN and Soft Q-learning (SQL) on 19 Atari games and demonstrate improvements over both.\n\nGenerally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense. However, I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful. Furthermore, I had a number of questions about the experiments. If the authors can clarify their motivation and reasoning and strengthen the experiments, I'd be happy to raise my score.\n\nIn Sec 3.1, why is it sensible to optimize the prior? Can the authors give intuition for maximizing \\log p(R = 1) wrt to the prior? This is critical for justifying their approach. Currently, the authors provide a connection to MI, but don't explain why this matters. Does it justify the method? What insight are we supposed to take away from that? \n\nThe experiments could be strengthened by addressing the following:\n* What was epsilon during training? Why was epsilon = 0.05 in evaluation? This is quite high compared to previous work, and it makes sense that this would degrade MIRLs performance less than DQN and SQL.\n* What is the performance of SQL if we use \\rho as the action selector in \\epsilon-greedy. This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy.\n* Plotting beta over time\n* Comparing the action distributions for SQL and MIRL to understand the impact of the penalty. In general, a deeper analysis of the impact on the policy is important. \n* Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding? Does it?\n* How many seeds were run per game?\n* How and why were the 19 games selected from the full set?\n\nComments:\n\nThe abstract claims state-of-the-art performance, however, what is actually shown is that MIRL outperforms DQN and SQL.\n\nWith a fixed prior, the action prior can be absorbed into the reward (e.g., Levine 2018), so it is of no loss of generality to assume a uniform prior.\n\nCould state that the stationary distribution is assumed to exist and be unique.\n\nIn Sec 3.1, why is the prior state independent?\n\nIn Sec 3.1, p(R = 1|\\tau) is defined to be proportional to exp(\\beta \\sum_t r_t). Is this well-specified? How would we compute the normalizing constant since p(R = 0 | \\tau) is not defined?\n\nThroughout, I suggest that the authors not use the phrases \"closed form\" and \"analytic\" for expressions that are in terms of intractable quantities. \n\nIt should be noted that Sec 3.2 Optimal policy for a fixed prior \\rho follows from Levine 2018 and others by transforming the fixed prior into a reward bonus.\n\nIn Sec 3.2, the last statement does not appear to be necessary for the next subsection. Remove or clarify?\n\nI believe that the connection to MI can be simplified. Plugging in the optimal \\rho into Eq 3, we can see that Eq 3 simplifies to \\max_\\pi E_q[ \\sum_t \\gamma^t r_t] - (1 - gamma)/\\beta MI_p(s, a) where p(s, a) = d^\\pi(s) * \\pi(a | s) and d^\\pi is the discounted state visitation distribution. Thus Eq 3 can be thought of as a lower bound on the MI regularized objective.\n\nIn Sec 4, the authors state the main difference between their soft operator and the typical soft operator. What other differences are there? Is that the only one?\n\nSec 5 references the wrong Haarnoja reference in the first paragraph.\n\nIn Sec 5, alpha_beta = 3 * 10^5. Is that correct?\n\n=====\n11/26\nAt this time, the authors have not responded to the reviews. I have read the other reviews and comments, and I'm not inclined to change my score.\n\n====\n12/7\nThe authors have addressed most of my concerns, so I have raised my score. I'm still concerned that the exploration epsilon is quite different than existing work (e.g., https://github.com/google/dopamine/tree/master/baselines).", "title": "Interesting, but motivation and experiments need improvements", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJepxiJ507": {"type": "rebuttal", "replyto": "SJe43cJcR7", "comment": "Comments:\nThe abstract claims state-of-the-art performance, however, what is actually shown is that MIRL outperforms DQN and SQL.\n\n---------->[Attenuated wording] We have adjusted the formulation regarding the performance in the paper.  We outperform DQN and SQL, both recent and high-performing algorithms (though not the best algorithms on ATARI). Our normalized scores are also close to those reported in the recent state-of-the art RAINBOW paper, but we cannot make a direct comparison over different implementations and subsets of games.\n\n\n With a fixed prior, the action prior can be absorbed into the reward (e.g., Levine 2018), so it is of no loss of generality to assume a uniform prior.\n\n--------------->[Absorbing prior into reward] In case of a uniform prior that is unaffected in the course of training, this is possible. In our algorithm, the prior is adapted in the course of training. In this case, keeping the prior separate allows for overcoming the problem of non-stationarity in the reward function.\n\nCould state that the stationary distribution is assumed to exist and be unique.\n\n------------>[Unique stationary state distribution] We state now in the paper that the stationary distribution is assumed to exist and be unique.\n\n\nIn Sec 3.1, why is the prior state independent?\n---------->[State-independent prior] We base our formulation on the rate-distortion framework that generalizes entropy regularization by having optimal state independent priors. We provide some intuition for the one-step decision-making case in the background section.\n\n\nIn Sec 3.1, p(R = 1|\\tau) is defined to be proportional to exp(\\beta \\sum_t r_t). Is this well-specified? How would we compute the normalizing constant since p(R = 0 | \\tau) is not defined?\n\n----------->[Normalization constant] It is not required to compute the normalization constant explicitly since it would appear in Equation 5 as a constant that is unaffected by the optimization. More explicitly, the expectation of the log of the normalization constant of p(R=1|\\tau) w.r.t. q(\\tau) is just the log of the normalization constant of p(R=1|\\tau) without the expectation.\n\nThroughout, I suggest that the authors not use the phrases \"closed form\" and \"analytic\" for expressions that are in terms of intractable quantities.\n\n----------->[Wording] We modified the wording accordingly in the current version of the  paper.\n\nIt should be noted that Sec 3.2 Optimal policy for a fixed prior \\rho follows from Levine 2018 and others by transforming the fixed prior into a reward bonus.\n\nIn Sec 3.2, the last statement does not appear to be necessary for the next subsection. Remove or clarify?\n---------->[[Clarity] We added some clarifications to this section.\n\nI believe that the connection to MI can be simplified. Plugging in the optimal \\rho into Eq 3, we can see that Eq 3 simplifies to \\max_\\pi E_q[ \\sum_t \\gamma^t r_t] - (1 - gamma)/\\beta MI_p(s, a) where p(s, a) = d^\\pi(s) * \\pi(a | s) and d^\\pi is the discounted state visitation distribution. Thus Eq 3 can be thought of as a lower bound on the MI regularized objective.\n----------->[On simplified connection to MI] We moved the connection to Mutual information for the case of gamma -> 1 to the appendix, and adopted another way to show this connection similar to what the reviewer has proposed.\n\nIn Sec 4, the authors state the main difference between their soft operator and the typical soft operator. What other differences are there? Is that the only one?\n------------>The two main differences are an adaptive prior and adaptive beta.\n\nSec 5 references the wrong Haarnoja reference in the first paragraph. In Sec 5, alpha_beta = 3 * 10^5. Is that correct?\n----------->We corrected this typo. It should be 3*10^-5.\n", "title": "Continuation"}, "SJe43cJcR7": {"type": "rebuttal", "replyto": "Byx_Yunv37", "comment": "We are sorry for the delayed reply (the deadline was extended to the end of 26th November Anywhere on Earth time). We state the reviewers comments and denote with arrows ( --------->  ) our replies.\n\nThe authors take the control-as-inference viewpoint and learn a state-independent prior (which is typically held fixed). They claim that this leads to better exploration when actions have different importance. They relate this objective to a mutual information constrained RL objective in a limiting case. They then propose a practical algorithm, MIRL and compare their algorithm against DQN and Soft Q-learning (SQL) on 19 Atari games and demonstrate improvements over both.\n\nGenerally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense. However, I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful. Furthermore, I had a number of questions about the experiments. If the authors can clarify their motivation and reasoning and strengthen the experiments, I'd be happy to raise my score.\n\nIn Sec 3.1, why is it sensible to optimize the prior? Can the authors give intuition for maximizing \\log p(R = 1) wrt to the prior? This is critical for justifying their approach. Currently, the authors provide a connection to MI, but don't explain why this matters. Does it justify the method? What insight are we supposed to take away from that?\n\n-------------> [On prior optimization and mutual-information] We extended the paper with an explanation on mutual information and rate distortion theory, in order to help with an intuitive understanding of why this prior can help learning. We also added a related work section to note that other algorithms have considered optimizing the ELBO with respect to both variational and prior policy. However, these approaches do not use the marginal prior or have any connection to mutual information but instead optimise the policy while staying close to the previous policy. Additionally, we moved the connection to Mutual information for the case of gamma -> 1 to the appendix, and adopted another way to show this connection similar to what the reviewer has proposed.\n\n\n\nThe experiments could be strengthened by addressing the following:\n* What was epsilon during training? Why was epsilon = 0.05 in evaluation? This is quite high compared to previous work, and it makes sense that this would degrade MIRLs performance less than DQN and SQL.\n\n----------->[Epsilon in training and evaluation] Epsilon during training was decayed from 1.0 to 0.1 over the first 10^6 steps of the experiment. We used a fixed evaluation epsilon of 0.05. This procedure is standard in the literature for ATARI, as introduced by the DQN paper (see e.g. Mnih et al, 2015 ). We understand that in later DQN papers (e.g. Rainbow) different values for these hyperparameters have been used but we feel our choice is not unreasonable.\n\n\n\n* What is the performance of SQL if we use \\rho as the action selector in \\epsilon-greedy. This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy.\n\n----------->[On marginal exploration] We have run additional experiments combining SQL with marginal exploration. Using the marginal exploration helps SQL, but MIRL still achieves the best performance.\n\n* Plotting beta over time\n----------->[Plotting beta] We include the beta values evolving over time in the appendix. Additionally, we also include a more relevant term (beta x Qvalues).\n* Comparing the action distributions for SQL and MIRL to understand the impact of the penalty. In general, a deeper analysis of the impact on the policy is important.\n* Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding? Does it? * How many seeds were run per game?\n----------->[Policy and grid world] Responding the previous two questions: We have added additional experiments and plots to the paper in an effort to  provide more insight into the behavior of our method. These experiments include a simple grid world in which we expect MIRL to outperform SQL and a grid world in which we expect the prior to have negative effects (as suggested by another reviewer).\n* How and why were the 19 games selected from the full set?\n------------->[On other aspects] Due to computational constraints we were not able to run experiments on the full set of ATARI games. Therefore, we selected a subset of 20 random games, without prior experimentation on any of the games. We then evaluated our method using a single seed for every game. Data for experiments on 1 game were lost because of a cloud instance failure.  \n\n\n", "title": "Reply"}, "BJgtvCRFCQ": {"type": "rebuttal", "replyto": "B1lIeTW537", "comment": "We thank the reviewer for the comments.  Below we attempt to address each of the points raised by the reviewer.\n\nBackground and related work:\n\nWe have expanded the paper with a section highlighting the connection between the rate distortion framework and the mutual information constraint. We hope that this connection can help providing some intuitive insight into why our method can improve performance.\n\nWe have also added a related work section more clearly positioning our work with respect to existing algorithms (such as MPO and DistRL).\n\nExperiments:\n\nWe have included a new set of experiments on a small tabular domain.  While simple, we hope that this domain can provide more insight into the performance of the algorithm.\n\n\nDue to computational constraints we were not able to perform a complete search for optimal hyperparameter combinations in the Atari domain. Hyperparameter values were chosen by using values reported in the literature. Values for the new parameters introduced by MIRL were fixed by running a small number of exploratory experiments. Overall, we found the algorithm to be robust to changes in these values. All other hyperparameters were kept the same for all algorithms. \n\n\nWhile it is true that the prior does not converge in all of our ATARI experiments, we note that during the later stages of learning the plots do show a higher probability for subsets of actions. We have empirically observed that convergence of the prior can take a very long time, especially when the learner is still improving.  We expect that, given enough time, the probabilities of the marginal policy will eventually settle. Additionally, in these experiments we used a non-decaying learning rate for the marginal policy. This means that we can expect some oscillation due to tracking behaviour of our approximation, while the policy and state distribution still change.\n", "title": "Added Grid World experiments, Related Work section and better connection to Mutual Information"}, "rJl62p0F0X": {"type": "rebuttal", "replyto": "ryl__Ymjh7", "comment": "We thank the reviewer for the comments. \n\nWe have updated the manuscript with additional experiments in a grid-world domain aimed at answering the reviewer\u2019s concerns. The additional experiments are aimed at better understanding the behaviour of our mutual-information constraint. We demonstrate that our method clearly improves learning speed when there is a strong preference for a single action in the optimal policy.  We also examine an example in which the optimal policy crucially depends on an action with low probability in the marginal distribution. While MIRL does not improve performance in this case, it does not exhibit negative effects. We show that the learnt policy overcomes the prior when necessary for performance. \n\nAdditionally, we have added a related work section that positions and compares our work to the existing literature on inference-based RL and maximum entropy RL in particular.\n", "title": "Added results on Grid World and added Related Work section"}, "B1lIeTW537": {"type": "review", "replyto": "HyEtjoCqFX", "review": "This work introduces SoftQ with a learned, state-independent prior. One derivation of this objective follows standard approaches from an RL as inference to derive the ELBO objective.\n\nA more novel view derived here connects this objective with the rate-distortion problem to view the objective as an RL objective subject to a constraint on the mutual information between the state and action distribution.\n\nThey also outline a practical off-policy algorithm for optimizing this objective and compare it with Soft Q Learning (essentially, the same method but with a flat-prior) and DQN. They find that this results in small gains across most Atari games, with big gains for a few games.\n\nThis work is well-explained except in one-aspect. The rate-distortion view of the objective is not well-justified. In particular, why is it desirable in the context of RL to constrain this mutual information?\n\nEmpirical Deep RL performance is notoriously difficult to test (e.g. Henderson et al., 2017). The hyper-parameters are simply stated here, but no justification is given for how they are chosen / whether the baselines perform better under different choices. Given the gains compared with SoftQ are not that large, this information is important for understanding how much weight to place on the empirical result.\n\nThe fact that the prior does not converge in some environments (e.g. Seaquest) is noted, but it seems this bears further discussion.\n\nOverall it appears this work provides:\n- An algorithm for Soft Q learning with a learned independent prior\n- Moderate evidence for gains compared with a flat prior on Atari.\n- A connection with this approach and regularization by constraining the mutual information between state and action distributions.\n\nIt could be made a stronger piece of work by showing improvements in domains others than Atari, justifying the choice of regularization more. It would also benefit from positioning this work more clearly in relation to related approaches such as MPO (non-parametric state-dependent prior) and DistRL (state-dependent prior but shared across all games).", "title": "Simple approach that appears to work well", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Skgno2oEhX": {"type": "rebuttal", "replyto": "rJx6sNdCom", "comment": "Both our algorithm and MPO can be seen as optimizing the same  evidence lower bound (ELBO).  MPO proposes a general coordinate ascent type optimization in which the ELBO is updated in alternating steps, either with respect to the variational policy or the prior policy (while the other policy is kept fixed). Different design choices for the policies and optimization procedures give rise to different, but related algorithms. This approach is also common in variational inference based policy search and describes a large family of related policy search algorithms (see Deisendroth et al, 2013 for an overview.)\n\nOur algorithm follows recent soft Q-learning algorithms (e.g. Fox et al, 2016, Haarnoja et al. 2017). These algorithms consider the same ELBO, but omit the optimization with respect to the prior policy and only optimize the variational policy pi.  This can be seen as an entropy-regularized version of standard Q-learning algorithms.  When the prior is fixed to be a constant uninformative policy, this procedure reduces to max-entropy policy learning. The algorithm replaces the classic Bellman operator with a soft Bellman-operator to prevent deviations from a state-independent fixed prior policy. Several papers (e.g.Haarnoja et al 2017, Schulman et al 2017 ) have shown that these \u201csoftened\u201d algorithms offer advantages over their unsoftened counterparts, in terms of exploration, generalization and composability. Our approach further improves on soft Q-learning (as shown in our Atari experiments) by allowing for optimizing the prior (while still being state-independent). As shown in the paper, this results in a mutual information constraint (rather than a max entropy constraint) on the resulting policy.\n\nSo while we follow the same general scheme as soft Q-learning, we do update our prior policy as in the MPO algorithm. However, contrary to MPO, we do not consider the alternating, coordinate descent style optimization. Rather than executing a separate prior maximization step, we solve the ELBO for the optimal prior in the special case of state-independent priors.  We then directly estimate this optimal prior in our algorithm, instead of performing a gradient style update on the ELBO. While it is possible to consider the same class of state-independent priors with MPO, the way in which both algorithms optimize the ELBO will still be different. \n\nA modified MPO that uses a state-independent generative policy would converge to a solution that is penalized by an optimal marginal policy. However, since the parameter epsilon (that determines the deviation between the variational and the generative policy) is fixed and not scheduled in the course of training, the final solution is still constrained by the marginal policy which is sub-optimal because it is state-independent. This constraint would essentially limit the asymptotic performance of such a modified MPO. Of course, this could be alleviated by setting epsilon to a large value but this would correspond to an ordinary actor critic  approach without any regularization in the policy.\n\nIf the prior policy in our algorithm is replaced by a state-dependent prior, the optimal solution for such a prior is the variational policy (i.e. pi) itself. This essentially would eliminate the KL-constraint and reduce our algorithm to standard Q-learning. Q-learning is known to suffer from sample-inefficiency caused by the hard max-operator in the target (this leads to overestimated q-values). This is exactly the problem that was been addressed by soft Q-learning with entropy regularization. \n\nDeisenroth, M. P., Neumann, G., & Peters, J. (2013). A survey on policy search for robotics. Foundations and Trends\u00ae in Robotics, 2(1\u20132), 1-142.\n\nSchulman, J., Chen, X., & Abbeel, P. (2017). Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440.\n\nHaarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017). Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165.\n\nFox, Roy, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. UAI (2016).", "title": "Reply"}, "rkxW2SgCo7": {"type": "rebuttal", "replyto": "HJxlECDns7", "comment": "Thank you for your comment. \n\nFraming RL as an inference problem has been addressed before in the literature [1,2] and can be done in different ways.  The difference between the variational inference formulation in MPO and our variational inference formulation is the following:\n- The policy of the generative model in our case is state-independent (similar to [1]) with the optimal solution being the marginal distribution over actions ([1] does not consider an optimal marginal distribution though). In contrast, in MPO the generative policy is state-dependent and given by the previous-round behavioural policy. \n\nImportantly, our specific choice of state-dependent variational policy and state-independent generative policy directly leads to a mutual information regularizer. Note that the mutual information is not any expected KL, but a specific expected KL under the assumption of an optimal marginal policy (which is exactly what we model). MPO does not have the notion of an optimal marginal policy (in the sense of a state-independent marginal policy) and therefore the expected KL in MPO is not a mutual information.\n\nIn our experimental section we empirically validate that our mutual information regularized objective leads to improvements over soft-q learning (see [1]) where the generative policy is also state-independent but not subject to optimization (but instead given by a uniform distribution). \n\nWe will clarify this point in a revised version of the manuscript.\n\n[1] Levine, S. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review. arXiv 2018.\n[2] Neumann, G. Variational Inference for Policy Search in changing Situations. ICML 2011.", "title": "Differences between MPO and our approach"}}}