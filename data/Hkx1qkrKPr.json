{"paper": {"title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification", "authors": ["Yu Rong", "Wenbing Huang", "Tingyang Xu", "Junzhou Huang"], "authorids": ["yu.rong@hotmail.com", "hwenbing@126.com", "tingyangxu@tencent.com", "jzhuang@uta.edu"], "summary": "This paper proposes DropEdge, a novel and flexible technique to alleviate over-smoothing and overfitting issue in deep Graph Convolutional Networks.", "abstract": "Over-fitting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on~https://github.com/DropEdge/DropEdge.", "keywords": ["graph neural network", "over-smoothing", "over-fitting", "dropedge", "graph convolutional networks"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a very simple but thoroughly evaluated and investigated idea for improving generalization in GCNs. Though the reviews are mixed, and in the post-rebuttal discussion the two negative reviewers stuck to their ratings, the area chair feels that there are no strong grounds for rejection in the negative reviews. Accept."}, "review": {"Osk5RzdEIC": {"type": "rebuttal", "replyto": "NjBwV6BRhO", "comment": "Hi, Dongsheng,\n\nReally appreciate your interest and the questions your raised for our paper. Based on your comments, we recognize that there are indeed unclear and imprecise descriptions in our current version. But note that the main story of our paper still holds. We summarize our clarifications below.\n\n1)\tOur derivation is based on the work [1] where the over-smoothing is characterized in an asymptotical form, so we can only justify the increment of the RELAXED smoothing layer (which is an upper bound of the true smoothing layer) in Theorem 1. We have extra defined the relaxed smoothing layer in our paper and reflected the revisions in Theorem 1 and the proofs in the appendix.\n\n2)\tYour illustration on the conductance is right. Our original proof is by connecting the (relaxed) smoothing layer with the conductance. But, we later find that using conductance is unnecessary. Instead, the proof is better explained via the connection with the resistance (see Eq.(6) in the new appendix). We have rearranged the proof and rigorously shown that the resistance (thus the relaxed smoothing layer) will increase if sufficiently edges are dropped. \n\nThanks for your questions, which make our paper more rigorous and well-qualified. We hope our explanations help.\n\n[1] Kenta Oono, Taiji Suzuki: Graph Neural Networks Exponentially Lose Expressive Power for Node Classification\n", "title": "More Clarifications"}, "bhSBAXSwY": {"type": "rebuttal", "replyto": "TSFVeqQwYC", "comment": "Hi, Petar. Really appreciate your interest in our work. \n\nSorry for missing the part you mentioned in your paper. Yes, performing dropout on the attention coefficients should be a specific form of GAT with DropEdge, and the result for regular GAT was obtained without this dropout.  \n\nHaving said that, there are several different points inbetween. First, your version is indeed a post-conducted version of DropEdge, as you compute all attentions prior to attention dropout. Here, in our work, we first perform DropEdge and then use GAT, avoiding unnecessary computation of edge attentions. Besides, we further perform adjacency normalization following DropEdge, which, even simple, is able to make it much easier to converge during training. Without normalization, it will also intensify gradient vanish as the number of layers grows. \n\nMore or less, the attention dropout seems an ad-hoc trick in your paper, and the relation to over-smoothing is never explored. In our paper, however, we have formally presented the formulation of DropEdge and provided rigorous theoretical justification of its benefit in alleviating over-smoothing. We also carried out extensive experiments by imposing DropEdge on several popular backbones.\n\nWe are happy you raising this discussion, and have added the above connection in the final copy.\n", "title": "More clarifications"}, "BketVLLVsS": {"type": "rebuttal", "replyto": "ryl-3sBhcH", "comment": "We really thank the reviewer for the recognition of our contributions to the experimental evaluations and the theoretical justification. Here, we would like to provide more explanations to address the reviewer's concerns.\n\u00a0  \nQ1. The novelty of DropEdge.\n\nWe agree that our DropEdge is simple and is inspired by Dropout. Yet, when we put it in the context of graph learning, DropEdge is indeed a novel method that is able to alleviate over-smoothing, while Dropout cannot. DropEdge can be regarded as an extension of Dropout to graph edges, but this extension, in certain sense, is not straightforward\u2014people usually adapt the idea of Dropout in GNNs to drop the network activations with less thinking in dropping the network input. Interestingly, simply dropping edges by random is sufficient to deliver promising results, as verified by our paper experimentally and theoretically.\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\n\u00a0\nQ2. Choosing the dropout proportion.\n\nThe reviewer probably misunderstood the experimental setting. We did not fix the drop proportion $p$ to be 0.8 for the main of our experiments. Instead, as already presented in the second paragraph in Section 5.1, we use the validation set to determine the dropping rate for each benchmark in Tab.1; different datasets could have different dropping rates. In Section 5.2.1 (and Fig.3), we fix it to be 0.8 for a case study on evaluating how DropEdge can prevent from over-smoothing. \nTo further illustrate how the dropping proportion actually acts, we have conducted an example experiment for GCN-4 (the best GCN model for Cora in Tab. 2) by varying $p$ from 0 to 1 with an increasing step of 0.2. The results are\n$p$   |  1    | 0.8     | 0.6  | 0.4    |0.2      | 0\nGCN|0.624| 0.778|0.87 |0.869 |0.862 |0.855\nClearly, it generally improves the performance when $0<p<0.8$. The exceptional cases are $p=0.8\uff0c1$ when GCN degenerates (or closely degenerates) to MLP, which is reasonable due to the less expressive power.\u00a0Furthermore, the best performance is achieved when $0.4 \\leq p \\leq 0.6$; the selection of dropout proportion $p$ near 0.5 may also be a good choice.  \n\nQ3. Exploiting graph-specific properties when considering DropEdge.\n\nYes, this potentially promotes the performance. Given that our particular interest here is to keep the method simple and general, we are happy to explore more variants of sophisticated DropEdge in future work. \n\nQ4. The justification of why \u201cDropEdge can be considered as a data augmentation technique\u201d is valid.\n\nThanks for the comment and sorry for the unclear clarification in the current submission. We provide an intuitive understanding here. The key in GNNs is to aggregate neighbors' information for each node, which can be understood as a weighted sum of the neighbor features (the weights are associated with the edges). From the perspective of neighbor aggregation, DropEdge enables a random subset aggregation instead of the full aggregation during GNN training. Statistically, DropEdge only changes the expectation of the neighbor aggregation up to a multiplier $p$, if we drop edges with probability $p$. This multiplier will be actually removed after weights normalization, which is often the case in practice. Therefore, DropEdge does not change the expectation of  neighbor aggregation and is an unbiased data augmentation technique for GNN training. We have added the above specifications in the paper. \n\u00a0\nQ5. On the layer-independent DropEdge.\n\nThanks for the comment. Our original purpose of naming it like this is to reflect that DropEdge is conducted independently across layers. If this is a problem, we are willing to rename it as \u201clayer-wise DropEdge\u201d to remove the confusion. We agree that the analysis of \u201clayer-wise DropEdge\u201d is interesting and it is an important future work of theoretical aspects.\n\nQ6. Other comments.\n\nThanks for the valuable suggestions and we will re-organize Fig.1 accordingly. We will also fix the typos throughout our paper.\n", "title": "Response to Reviewer #4"}, "ByxtBPLNsr": {"type": "rebuttal", "replyto": "B1eC_r5atS", "comment": "We appreciate the reviewer for ordering the questions with numbers, which helps us to respond more conveniently.  \n\nQ1: DropEdge does change the graph properties for each epoch. But statistically, as discussed in our reply to Q4, Reviewer#4, DropEdge does not change the expectation of neighbor aggregation that plays a crucial role in characterizing input graphs. Hence, the statistics of graph properties are still preserved.\n\u00a0\nQ2: Drawing for our reply in Q1, DropEdge will not change the connectivity in expectation, even it may result in disconnected components occasionally in one epoch.\n\nQ3: The information measurement in Thm.1 refers to how much freedom we have to describe a point in a certain space. The dimensionality of the space is a natural and direct choice, thus we use dimension reduction to reflect information loss.\n\u00a0\nQ4: As we discussed in Section 4.3, The purpose of the graph sparsification and DropEdge are different. Graph sparsification aims to remove unnecessary edges of graphs, while keeping almost all information of the input graph, while DropEdge is an efficient approach to reduce the over-smoothing based on our theoretical analysis. Moreover, as mentioned in Q1, DropEdge preserves the statistic of graph properties, and involves no bias.\n\nQ5: According to our theoretical analysis, deeper GNN models suffer from more serious over-smoothing issues than that of shallower ones. It is thus not surprising that DropEdge can gain more improvements from more layers. The experimental results in Tab. 1 and Fig.2 validate our theoretical findings. \n\nQ6: The trend of Reddit dataset is still generally consistent with other datasets if we compare the results of 4/8/32 layers (the more layers the more improvements by applying DropEdge). The corner case happens when the depth is 16. If we check Table 7 in the appendix, there is a huge performance drop in GCN without DropEdge at 16 layers, making the improvement by DropEdge bigger than that of 32 layers.\n\nQ7: The motivation of FastGCN and ASGCN is to speed up GCN, and they can be considered as different efficient implementations of GCN.  We believe performing a comparison on GCN is sufficient without further consideration on FastGCN and ASGCN.  GAT is different from GCN, and we are willing to provide the results below:\n|                                   | Cora  | Citeseer |\n| GAT                          | 0.863 | 0.781     |\n| GAT w/ DropEdge | 0.881 | 0.792     |\nAs expected, DropEdge can still enhance its performance.\n\nMinor Q3: $C_l$ refers to the size of $l$-th hidden layer.\n", "title": "Response to Reviewer #2"}, "rJx638IVjB": {"type": "rebuttal", "replyto": "rJlykwUUcr", "comment": "We thank the reviewer for accepting the interestingness and the completeness of our paper. We present our responses below. \n\nQ1. About the significance of performance improvement. \n\nAs mentioned in Section 5.1, the benchmark datasets are well-studied and well-tuned in the graph learning field. Achieving a 1-2% increase can be regarded as a remarkable improvement.  For the baselines we consider in Tab.1, DropEdge generally improves them around (or bigger than) 1% under different depths, which is significant considering the challenge on these datasets. \n\nQ2. About the claim of our paper on deeper GNNs.\n\nThe reviewer possibly misunderstood our claim. Our paper is not showing deeper is better.  Instead, we are more interested in investigating why GCN failed with deep layers and how over-smoothing happened. We hence propose DropEdge, a simple but effective method that is capable of enhancing various kinds of GNNs regardless of the network depth. Our motivation of discussing and reporting the results of varying depth in Tab.1 is to study how much DropEdge can enhance deep GNNs.  The reviewer raised that  \"it looks like most of the time, 2-layers networks are already the best (or close to the best)\", which is true but only when all models (including the 2-layer ones) have applied DropEdge. \n\nQ3.  About the clarification of \"DropEdge either retards the convergence speed of over-smoothing or relieves the information loss caused by it\".\n\nThis sentence reflects two parts of Theorem 1. As the first part, it retards the convergence speed of over-smoothing and as the second part, it relieves the information loss caused by over-smoothing.\n", "title": "Response to Reviewer #3"}, "B1eC_r5atS": {"type": "review", "replyto": "Hkx1qkrKPr", "review": "The authors propose a simple and interesting strategy, DropEdge, to alleviate the over-fitting and over-smoothing in GCN. The logic is simple and clear and the paper is well-written. \n\nMajor concerns:\n1. After randomly enforce a certain rate of edges to be zero, how to preserve properties in the original complex network, such as degree power-law distribution, communities? If it was not necessary to preserve the properties, then what information should be preserved from the original graph.\n2. Randomly drop edges may result in disconnected components, how to handle disconnected components?\n3. Why do the authors use dimension difference as the measure to quantitatively evaluate information loss in Thm 1. More dimension reduction does not mean more information loss.\n4. As a follow-up concern for C1, graph sparsification makes more sense than DropEdge because it has clear information reserve targets while there is no target for the randomness in DropEdge.\n5. In Table 1 and Fig 2, why the improvements for more layers are bigger than those of the fewer layers?\n6. In Fig 2, why the trend of Reddit dataset is so different from others (the more layers the more improvements by applying DropEdge)? \n7. In Table 2, why there are the DropEdge versions for some methods while not for some other methods (e.g., FastGCN, ASGCN)? Why there is no result of GAT?\n\nMinor:\n1. Sec 3, \"notation\", \"\\mathbf{x}_n\" -> \"\\mathbf{x}_N\"\n2. Eq (1), \"\\mathbf{h}_n^{(l+1)}\" -> \"\\mathbf{x}_N^{(l+1)}\"\n3. What's C_l in the explaination under Eq(1)?", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 3}, "rJlykwUUcr": {"type": "review", "replyto": "Hkx1qkrKPr", "review": "This paper studied the problem of \"deep\" GCNs where the goal is to develop training methods that can make GCN becomes deeper while maintaining good test accuracy.  The authors proposed a new method called \"DropEdge\", where they randomly drop out the edges of the input graphs and demonstrate in experiments that this technique can indeed boost up the testing accuracy of deep GCN compared to other baselines. \n\nThis paper is clearly well-written and the authors conducted a comprehensive study on deep GCNs. I also like the discussion in sec 4.3 where the authors explicitly clarify what are the difference between DropEdge, Dropout and DropNode, as the other two are the methods that will pop up during reading this paper. The extensive experiment results also show that for deeper GCNs, DropEdge always win over other baselines (see Tab 1) despite most of them are marginal except the backbone being GraphSAGE on Citeseer. Can you explain why this is the case? Why other backbones seem to have similar performance  even with DropEdge (i.e. most of the accuracy increase are less than 3 %). \n\nQuestion:\n1.  When looking at Tab 1, it looks like most of the time, 2-layers networks are already the best (or close to the best) and are clearly better than 32 layers. Therefore, this makes me wonder: why do we need deeper networks at all if the shallow networks can already achieve a good (almost best) performance and it is also much similar and efficient in training? Can you please clarify why do we care to train a deeper network at all under this scenario? Are there any reasons that we would like to use deeper network as opposed to shallower networks?\n\n2. It is less clear to me regarding this sentence: \"DropEdge either retards the convergence speed of over-smoothing or relieves the information loss caused by it\" \n\nOverall, I think this paper presents an interesting study on making deeper GCNs comparable to shallow network performance, but since the the boosted performance doesn't really outperform most of the 2-layer networks,  I would like to hear the justification of why we need the deeper networks for this node classification task. ", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 2}, "ryl-3sBhcH": {"type": "review", "replyto": "Hkx1qkrKPr", "review": "The authors propose a simple but effective strategy that aims to alleviate not only overfitting, but also feature degradation (oversmoothing) in deep graph convolutional networks (GCNs).  Inspired by dropout in traditional MLPs and convnets, the authors clearly motivate their contribution in terms of alleviating both overfitting and oversmoothing, which are problems established both in previous literature as well as validated empirically by the authors. Ultimately, the authors provide solid empirical evidence that, while a bit heuristic, their method is effective at alleviating at least partially the issues of overfitting and oversmoothing.\n\nI vote weak-accept in light of convincing empirical results, some theoretical exploration of the method's properties, but limited novelty.\n\nPros:\nSimple, intuitive method\nDraws from existing literature relating to dropout-like methods\nLittle computational overhead\nSolid experimental justification\nSome theoretical support for the method\nCons:\nMethod is somewhat heuristic\nMitigates, rather than solves, the issue of oversmoothing\nLimited novelty (straightforward extension of dropout to graphs edges)\nUnclear why dropping edges is \"valid\" augmentation\n\nFollowup-questions/areas for improving score:\n\nIt would be nice to have a principled way of choosing the dropout proportion; 0.8 is chosen somewhat arbitrarily by the authors (presumably because it generally performed well). There is at least a nice interpretation of choosing 0.5 for the dropout proportion in regular dropout (maximum regularization).\n\nAs brought up in the comments, edges to drop out to the graph's properties is an interesting direction to explore. While the authors state that they would like to keep the method simple and general, the method is ultimately devised as an adaptation of dropout to graphs, so exploiting graph-specific properties seems reasonable and a potential avenue to further improving performance. \n\np2: \"First, DropEdge can be considered as a data augmentation technique\" Why are these augmentations valid; why should the output of the network be invariant to these augmentations? I would like to see some justification for why the proposed random modification of the graph structure is valid; intuitively, it seems like it might make the learning problem impossible in some cases.\n\nDeeper analysis of the (more interesting, I think) layer-independent regime would be nice. (As a side-note, the name \"layer-independent\" for this regime is a bit confusing, as the edges dropped out *do* depend on the layer here, whereas in the \"layer dependent\" regime, edges dropped out do *not* depend on the layer).\n\nComments:\nFigure 1 could probably be re-organized to better highlight the comparison between GCNs with and without DropEdge; consolidating the content into 2 figures instead of 4 might be more easily parsable. Adding figure-specific captions and defining the x axis would also be nice.\n\nUse \"reduce\" in place of \"retard\"\np2 \" With contending the scalability\" improve phrasing\np2 \"By recent,\" -> \"Recently,\"\np2 \"difficulty on\" -> \"difficulty in\"\np2 \" deep networks lying\" -> \"deep networks lies\"\np3 \"which is a generation of the conclusion\" improve phrasing\np3 \" disconnected between\" -> \"disconnected from\"\np4 \"adjacent matrix\" -> \"adjacency matrix\"\np4 \"severer \" -> \"more severe\"\np5 \"but has no help\" -> \"but is no help\"\np5 \"no touch to the adjacency matrix\" -> improve phrasing\n", "title": "Official Blind Review #4", "rating": "6: Weak Accept", "confidence": 1}, "rke283wbYS": {"type": "rebuttal", "replyto": "Hkx1qkrKPr", "comment": "Hi All, \n      We updated our code to support the semi-supervised setting of node classification. All semi-supervised classification results of Cora, Citeseer and Pubmed are available in the GitHub. \n      \n      Please check out them if you are interested in our work.\n      Link: https://github.com/DropEdge/DropEdge\n\nThanks!\nAuthors\n", "title": "The results of semi-supervised node classification."}, "r1lmckKyYr": {"type": "rebuttal", "replyto": "HyxukVbidr", "comment": "We appreciate your interests in our work. Your raising paper [1] brings us a different idea to measure over-smoothing and the approaches to relieve it. It is an interesting work. Here, we would like to take it as a chance to discuss the difference between [1] and our paper: \n\n(1)\tThe different understanding on over-smoothing. While [1] considers over-smoothing as the issue that node representations become identical and indistinguishable as network depth increases, this paper follows previous studies [2,3] that prefer to define it as the convergence of node representations to a stationary distribution (or a subspace as proved by [3]). The latter understanding admits that the convergent representation of different node could be different but it is only topology-aware and independent to the initial input. This is consistent to the random walk theory. Guided by this, we proposed DropEdge, a novel method to slow down the speed of convergence; as shown by Theorem 1, we have proved that dropping any edge (not just the inter-class ones) is able to retard the speed of over-smoothing or relieve the information loss caused by it.\n\n(2)\tDropEdge is simple yet effective. We agree that it could become more sophisticated if we apply certain heuristics other than randomness to determine which edge to be deleted (e.g. the inter-class edges). However, it will inevitably involve more complexity, and is impractical when the node labels are unknown (such as unsupervised learning). By contrary, DropEdge is efficient and applicable for broader cases. More importantly, as shown by our experiments, dropping edges by random is sufficient to enhance the performance of a variety of both shallow and deep GCNs.\n\n(3)\tDropEdge can prevent over-fitting as well; in this sense, it is more like Dropout. As already presented in our paper, DropEdge can be considered as a data augmentation technique. By DropEdge, we are actually generating different random deformed copies of the original graph; as such, we augment the randomness and the diversity of the input data, thus better capable of preventing over-fitting. \n\n[1] Deli Chen, Yankai Lin, Wei Li, Peng Li, JieZhou, Xu Sun: Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View.\n[2] Johannes Klicpera, Aleksandar Bojchevski, Stephan G\u00fcnnemann: Predict then Propagate: Graph Neural Networks meet Personalized PageRank.\n[3] Kenta Oono, Taiji Suzuki: Graph Neural Networks Exponentially Lose Expressive Power for Node Classification\n", "title": "Dropping Edges by random is simple yet effective, which can prevent overfitting as well."}, "ryxwiUP4uS": {"type": "rebuttal", "replyto": "BJgOg_9WOH", "comment": "Hi, Alex Williams. \n\nThanks for your interest in our work, and sorry for the delay response. \n\n(1)\tPlease note that the training-testing division of the datasets in this paper is different from that in the original GCN paper. As already mentioned in the last sentence of the first paragraph in Section 5, we follow the setting in FastGCN [1] and AS-GCN [2] to use full supervised data for the training (while the original GCN use the semi-supervised setting). This is why we obtained higher numbers for the same GCN model on Cora, Citeseer, and Pubmed. Our performance with DropEdge reaches 91.7% compared to 90.22% by GCN on Pubmed under the full supervision setting, which is still reasonable. The main purpose of this paper is to demonstrate the impact of DropEdge on promoting deep GCNs (see Table 1 and Table 7) without particular concern on what setting we prefer. \n\n(2)\tOur code is available in https://github.com/DropEdge/DropEdge . Feel free to check. If you have any question on running it, please tell us.\n\n[1] Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In Proceedings of the 6th International Conference on Learning Representations, 2018.\n[2] Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pp. 4558\u20134567, 2018.\n", "title": "The performance difference is due to the different experimental setting"}, "S1lB1PPVdr": {"type": "rebuttal", "replyto": "r1l1bxGEOr", "comment": "Hi William H Cohen,\n\nThanks for your interest in our work. Our code can be downloaded from\nhttps://github.com/DropEdge/DropEdge   \n", "title": "The link of our source code"}, "r1l2CcPKdS": {"type": "rebuttal", "replyto": "Syemfc3E_B", "comment": "\n(1)\tFirst of all, we believe that the ICLR open review here is a platform that only accepts constructive and valuable discussions on each submitted paper. If you respect this and are really interested in our paper, please remove your offensive words like \u201ccheat\u201d, \u201cindiscriminately change the settings\u201d, \u201cmisconduct\u201d, \u201ckeep fighting\u201d, \u201cthe results are too good to be true\u201d.\n\n(2)\tYour accusations on our experimental setting are rude and not reasonable by any means. \n\n(2.1) First, we have stated our settings clearly in the beginning of our experiments. We have no way to \u201ccheat\u201d anyone. The full supervised setting is originally introduced by FastGCN (not AS-GCN ). Our using the same setting as FastGCN here is due to our concern that both Cora and Citeseer are too small for benchmarking and it could incur bias if we keep using part of the labelled data. \n\n(2.2) Second, all compared methods in our experiments are conducted in the same setting. It is an apple-to-apple comparison. We have never contrasted our numbers against those semi-supervised methods, and of course we are never meant to. Your comment on saying our comparisons are unfair is unfair itself.\n\n(2.3) Different from the other three datasets, the Reddit dataset proposed by GraphSAGE is used under full supervision, which is consistent with our paper.\n\n(2.3) Finally, research is not just about competition. We believe that professional researchers in the community will respect a paper for its novelty, technicality and interestingness, not just because it can beat all methods under the machine-parsable setting. \n \n(3)\tStill, we are willing to provide the results under the semi-supervised setting on Cora, Citeseer and Pubmed. We obtained the following results on 2-layer GCN:\n\n| orginal | no-dropedge (ours) | dropedge (ours) |\n--------------------------------------------------------------------------------\nCora   | 81.5 | 81.1 | 82.8 |\nCiteseer | 70.3 | 70.8 | 72.3 |\nPubmed | 79.0 | 79.0 | 79.6 |\n\nThe results in the first column are from the original GCN paper, and those of the last two columns correspond to the GCNs w/o and w DropEdge, respectively. Note that the results w/o DropEdge are comparable to those in the GCN paper, demonstrating the reliability of our experiments; adding DropEdge consistently promotes the performance on all three datasets. We will add more results on other backbones if necessary.\n\nOverall, we sincerely hope you to show more respect to our work, and continue an encouraging discussion on the motivation, formulation and interestingness of our method. If you keep using offensive tone, we will refuse to respond to any question by you.\n", "title": "We only accept constructive and valuable discussions but not offensive and impolite comments."}, "S1lR897_dH": {"type": "rebuttal", "replyto": "ryxAKgmudB", "comment": "Hello Huaxin,\nThe link wrongly includes the last dot. Please remove it to reach the correct website. The correct link is \n\nhttps://github.com/DropEdge/DropEdge\n\nBest,\n", "title": "Please remove the last dot"}}}