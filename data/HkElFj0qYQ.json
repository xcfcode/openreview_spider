{"paper": {"title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "authors": ["Mehdi Jafarnia-Jahromi", "Tasmin Chowdhury", "Hsin-Tai Wu", "Sayandev Mukherjee"], "authorids": ["mjafarni@usc.edu", "chowdt1@unlv.nevada.edu", "hwu@docomoinnovations.com", "sayandev.mukherjee@huawei.com"], "summary": "Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.", "abstract": "Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.", "keywords": ["permutation phase defense", "adversarial attacks", "deep learning"]}, "meta": {"decision": "Reject", "comment": "This paper presents a new defense against adversarial examples using random permutations and a Fourier transform. The technique is clearly novel, and the paper is clearly written. \n\nHowever, as the reviewers and commenters pointed out, there is a significant degradation in natural accuracy, which does not seem to be easily recoverable. This degradation is due to the random permutation of the images, which effectively disallows the use of convolutions. \n\nFurthermore, Reviewer 1 points out that the baselines are insufficient, as the authors do not explore (a) learning the transformation, or (b) using expectation over transformation to attack the model. \n\nThis concern is further validated by the fact that Black-box attacks are often the best-performing, which is a sign of gradient masking. The authors try to address this by performing an attack against an ensemble of models, and against a substitute model attack. However, attacking an ensemble is not equivalent to optimizing the expectation, which would require sampling a new permutation at each step. \n\nThe paper thus requires significantly stronger baselines and attacks."}, "review": {"Ske-mUxFCX": {"type": "rebuttal", "replyto": "r1g-LRjic7", "comment": "Thanks for your detailed comments and sorry for the late reply. We had to perform some experiments to answer your questions.\n\n- The accuracy reported for CIFAR10 is for a simple 3 layer dense network. We agree that this is far from desirable, but we believe that if future work can reach higher accuracy on clean images in the permutation-phase domain with more advanced techniques, this automatically results in higher accuracy on adversarial examples.\n\n- The high accuracy is possibly due to the fact that the attacks do not try to modify the digit, rather try to destruct the pixels. In addition, we do not claim retaining this level of accuracy under any type of attack. The purpose of the distortion measure is to compare attacks based on the same ground. Indeed, based on the facts that you mentioned (average L2 distortion of 7.5), this shows that current attacks are not using the distortion budget efficiently.\n\n- As you mentioned, from figure 6 in the appendix, it can be seen that distortion level of 0.2 is quite large for human eye to correctly classify. The code will be released after the final decision to reproduce the results. I'm not sure if I understood your point on 10,000 different random noise. Do you mean expectation over transformation attack?\n\n- We have tested expectation over transformation (https://arxiv.org/pdf/1707.07397.pdf) by attacking a set of 30 PPD models and then test on ensemble of another 10 models. However, the results showed that no more effect than a single model.", "title": "Re: A couple of concerns"}, "BJezPa1FCX": {"type": "rebuttal", "replyto": "B1xMkk49am", "comment": "Thanks for your question.\nFFT should be placed second. The role of FFT is to build an image based on the relational positions of the permuted image (while the relational positions are encrypted using the permutation block). If FFT is placed first, the whole intuition behind the defense fails because adversary can also simply first take FFT and then perform the attack. Note that random permutation by itself is not secure (see Section 3.2 and the subsections).", "title": "Re: FFT after performing a permutation?"}, "B1eu-5JYR7": {"type": "rebuttal", "replyto": "HJlPpLn_2Q", "comment": "We thank the reviewer for the detailed feedback and address the comments below:\n\nReviewer comment: Poor clean accuracy makes the technique very impractical.\n\nOur response: The 48% accuracy on CIFAR-10 is for a simple 3 layer dense neural network and our goal was to show that even with such a simple network, SOTA robustness can be achieved. We believe that high accuracy combined with adversarial robustness is possible for CIFAR-10, and transfer learning shows promise in this direction. What we plan to do as future work is to replace the neural network in the PPD pipeline with a pre-trained model on massive datasets such as ImageNet and retrain the final layers to fit the permutation-phase domain.\n\nReviewer comment: Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs.\n\nOur response: Thanks for bringing this attack scenario to our attention. To test PPD against an adversary that tries to learn the transformation, we used Blackbox attack (https://arxiv.org/abs/1602.02697 ). In this attack, adversary probes an ensemble of PPD models as a black box by enough input-output pairs and trains a substitute model. The substitute model is then used to craft adversarial examples. Table 1 is updated with the Blackbox results.\n\nReviewer comment: The adversary may attack an ensemble of PPD models for different random permutations (i.e., expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.\n\nOur response: Per the reviewer's request, we tested PPD against expectation over transformation (EoT) (https://arxiv.org/abs/1707.07397) where the permutation is considered as the transformation. 30 PPD models (with different permutations) are used for EoT. The adversarial examples are then fed to an ensemble of 10 PPD models (with different permutations from the 30 models). Our experiments show that EoT can not decrease accuracy more than an adversary that attacks with a single model. One possible explanation is that EoT is mostly useful in the case that sampling a few transformations provides a good approximation of the expectation over transformation. For example, two scenarios that EoT is shown to be successful are: (1) synthesizing adversarial examples that are robust to camera viewpoint shift and (2) breaking a defense that randomly drops pixels of the image and replaces them with total variance minimization. In both of these two scenarios, sampling a few transformations gives a good idea of the expectation. However, in PPD, each transformation has its own fingerprint which is totally different from others.", "title": "Reply: Good idea but has limited use case"}, "r1eZktyYCX": {"type": "rebuttal", "replyto": "HJedRiS5hX", "comment": "We thank the reviewer for the positive feedback.\n\nReviewer comment: It would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys.\n\nOur response: Per the reviewer requested, we tested PPD against expectation over transformation (EoT) attack (https://arxiv.org/abs/1707.07397 ). EoT uses an ensemble of 30 PPD models to make adversarial examples. Our experiments showed that EoT could not degrade the performance more than an adversary that uses a single PPD model. One possible explanation is that each permutation yields a unique domain. In other words, information gained by other domains does not reveal much about the unknown domains.\n\nReviewer comment: It would be great to see this extended to convolutional networks.\n\nOur response: We have observed that PPD shows robustness even in the case that convolutional networks are used. However, the accuracy in the convolutional networks is slightly worse than dense networks. For example, convolutional nets achieved around 97% (rather than 98%) on MNIST dataset and 44% (rather than 48%) on CIFAR-10 dataset. One possible explanation for this observation is that the permutation block breaks the local properties of the images exploited by convolutional networks.", "title": "Reply: Intersting approach"}, "ByxUfwyKR7": {"type": "rebuttal", "replyto": "r1gIr-d0hX", "comment": "We thank the reviewer for the valuable feedback and address the comments in the following:\n\nReviewer comment: The test accuracy on Cifar10 seems to be quite low,  due to the permutation of the inputs. This makes me question  how favorable the trade-off between robustness vs performance is.\n\nOur response: Training on CIFAR-10 is a much more complicated task compared to MNIST and moving to the permutation-phase domain makes it even more difficult. We don't know at the moment what type of network structure and learning technique results in the best accuracy in the permutation-phase domain, but our experiments demonstrate that even a simple 3 layer dense neural network that achieves 48\\% accuracy on clean test images, provides SOTA robustness. We believe that using techniques such as transfer learning helps to improve accuracy on CIFAR-10 data set, but this idea requires more time and resources to evaluate and we leave it for future work.\n\nReviewer comment: The authors state \"We believe that better results on clean images automatically translate to better results on adversarial examples\". I am not sure if this is true. One counter argument is  that better results on clean images can be obtained by memorizing more structure of the data (see [1]). But if more memorizing (as opposed to generalization) happens, the classifier is more easily fooled (the decision boundary is more complicated and exploitable).\n\nOur response: By better results on clean images, we mean better results on clean \"test\" images. The paper referred by the reviewer states the ability of neural networks to memorize the entire training data set and reaching 100% training accuracy while achieving only random guess on testing data set. This is absolutely correct and in fact we have seen it even in the permutation-phase domain that training accuracy can reach as high as 100% while generalizing poorly on testing data set (although not reported in the paper).  However, our claim states that a PPD model that can reach high clean \"test\" accuracy, will not perform poorly on adversarial examples. This is simply because PPD breaks adversarial perturbation of pixel domain to random noise in the permutation-phase domain. Thus, the only goal left for future work is to increase clean test accuracy.", "title": "Reply: No Title"}, "r1gIr-d0hX": {"type": "review", "replyto": "HkElFj0qYQ", "review": "The Paper is written rather well and addresses relevant research questions.\nIn summary the authors propose a  simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). \n\nMy main points of critique are:\n\n1. The test accuracy on Cifar10 seems to be quite low,  due to the permutation of the inputs. This \nmakes me question  how favorable the trade-off between robustness vs performance is. \n\n2. The authors state \"We believe that better results on clean images automatically translate to better results on adversarial examples\"\n\nI am not sure if this is true.   One counter argument is  that better results on clean images can be obtained by memorizing more structure of the data (see [1]). But if more memorizing (as opposed to generalization) happens, the classifier is more easily fooled (the decision boundary is more complicated and exploitable).\n\n\n\n[1] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.", "title": " ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJedRiS5hX": {"type": "review", "replyto": "HkElFj0qYQ", "review": "This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. PPD relies on safekeeping of the key, specifically the seed used for permuting the image pixels. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. The method appears to be robust across attacks and distortion levels.\n\nThe idea is clearly presented and evaluated. \n\n*Details to Improve*\nIt would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys.\n\nIt would be great to see this extended to convolutional networks.\n", "title": "Interesting approach.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJlPpLn_2Q": {"type": "review", "replyto": "HkElFj0qYQ", "review": "This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. This setting has practical limitations, but is plausible in theory.\n\nWhile the defense technique is certainly novel and inspired, its use case seems limited to simple datasets such as MNIST. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. For this weakness, I recommend rejection but encourage the authors to continue exploring in this direction for a more suitable scheme that does not compromise clean accuracy.\n\nPros:\n- Novel defense technique against very challenging white-box attacks.\n- Sound threat model drawn from traditional security.\n- Clearly written.\n\nCons:\n- Poor clean accuracy makes the technique very impractical.\n- Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. Also, the adversary may attack an ensemble of PPD models for different random permutations (i.e. expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.", "title": "Good idea but has limited use case", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HyxIf9wC9Q": {"type": "rebuttal", "replyto": "rklxM4To9Q", "comment": "Thanks for your interesting question. So far, we have not been able to come up with an attack that can learn the permutation. We know that there are huge possible permutations (784! > 10^1500) for MNIST for example. This rules out the possibility of random guess, but does not preclude the possibility of leaking information. We don't know at the moment if this kind of attack is possible.", "title": "Reply"}, "ryxazwD0qQ": {"type": "rebuttal", "replyto": "Hyeb7Rhi57", "comment": "Encryption is not happening in the inference stage. It is happening in building the adversarial images. Maybe the word \"encryption\" has been misleading. Sorry for that. Let me explain in other words: \n\nI understand that when you are given an image for inference you do not know whether it is adversarial or clean and you just feed it to the classifier. But the question is can adversary really generate adversarial images? That's where we are stopping adversary not in the inference stage. Adversary requires two elements to generate adversarial examples: (a) input space, and (b) classifier. But how PPD stops adversary? You can think about it in two ways:\n\n1. If you consider the neural network as the classifier, PPD is hiding the input space.\n2. If you consider the whole pipeline as the classifier, PPD is hiding the classifier.\n\nI hope you find this explanation helpful.", "title": "Re: I feel that you did not really address my comments"}, "HklMPJPC5X": {"type": "rebuttal", "replyto": "SJgdGN2i97", "comment": "Maybe the following example helps to clarify partial vs complete obscurity:\n\nOne method proposed in https://openreview.net/forum?id=SyJ7ClWCb is total variance minimization. This approach randomly selects a \"small\" set of pixels, and reconstructs the simplest image that is consistent with the selected pixels. Why small set of pixels? Because the neural network wants to decide based on the reconstructed image and if a lot of pixels are changed, it will fail to make a good decision. This is what I mean by saying that important parts of the image (or in other words a large portion of the image) are preserved in total variance minimization.\n\nHowever, PPD completely obscures the input space by random permutation + pixel2phase. Why are we saying completely hides important pixels of the image? Consider an MNIST image of digit 2. Clearly, important pixels are the white pixels in the image. If we just use random permutation (which is a partial obscurity), we are changing positions of these white pixels. So, if adversary attacks white pixels in the unpermuted image of 2 (by converting them to grey pixels), it has successfully attacked a permuted image no matter what permutation is used, because grey pixels of unpermuted image remain gray pixels in the permuted domain as well. But adding pixel2phase block after the permutation block solves this issue. Fourier transform captures frequency of change in pixel values. So, instead of looking at what pixels are white in the permuted image of digit 2, it looks at the change in pixel values of neighbors (and this is hidden from adversary due to the random permutation). So severity of attack is mitigated in this way", "title": "An example for clarification"}, "rJlwltEs9X": {"type": "rebuttal", "replyto": "rJl20qQjq7", "comment": "Randomized perturbation of the input proposed in previous work does not completely change the input space. For example, the methods proposed in this paper https://openreview.net/forum?id=SyJ7ClWCb can be broken because they just change some parts of the input through randomization. In fact, they have to keep a large portion roughly unchanged to retain accuracy. However, PPD completely changes the input space. Note that the role of pixel2phase block is to build the input image of the neural network out of the relational positions of the pixels while the relational positions are hidden using the permutation.", "title": "PPD is not just obscurity, it completely changes the input space"}, "B1xKozVj9X": {"type": "rebuttal", "replyto": "H1eNFs7j97", "comment": "If I understood correctly, you mean: \n\"attacks that query the true model and construct a substitute model based on the query results without any permutation or pixel2phase block. Then, use the substitute model to craft adversarial examples.\"\nAnother comment also mentioned such a scenario based on this paper:  https://arxiv.org/abs/1602.02697. We agree that this is a valid scenario for adversarial attack. It will be tested and the results will be added to the revision.", "title": "Will be tested"}, "ryxP9XMicQ": {"type": "rebuttal", "replyto": "HylHzT-sqX", "comment": "Thanks for your comment. Input image by itself is not enough to craft adversarial images. Adversary requires both input image and a classifier to generate adversarial examples. However, hiding permutation seed stops adversary from using the true model (note that hiding permutation stops adversary's access to the gradient of loss function with respect to the input). So, adversary cannot push the image towards the decision boundaries of the classifier.\n\nAs an alternative, adversary has to use a substitute model to craft adversarial images. Assuming that the permutation is not revealed, adversary may use a similar model trained with a different permutation which is shown to be ineffective.", "title": "Assumption is valid"}, "BJeQJMZi97": {"type": "rebuttal", "replyto": "r1xuz7PZ5X", "comment": "Thanks for bringing this attack to our attention. We will test and add results to the revision.", "title": "Will be tested"}, "rkemB1Zs57": {"type": "rebuttal", "replyto": "SJxT0bPbcm", "comment": "-The blue curves in Figures 4 and 5 show the accuracy for the target model (the one with the known permutation). As seen in these figures, some attacks such as MIM and CW have decreased the accuracy of the target model to below 20% for MNIST. However, they have not been successful on models with hidden permutation (red dashed curves). This shows that robustness is actually achieved by unknown permutation and not gradient masking.\n\n- I'm not sure if I understood your second part of comment. Do you mean the following? \n\"Adversary trains ~1000 PPD models and craft adversarial examples for each of those models. Then, tests adversarial examples of each of those models against the unknown PPD model to see which PPD model out of the 1000 models was the most damaging to the unknown PPD model?\"\n\n", "title": "PPD robustness is because of unknown permutation"}, "Hkxk5Fgic7": {"type": "rebuttal", "replyto": "BJg9Hyvbqm", "comment": "The word \"black box\" is not used because PPD only requires to hide the random permutation seed. Everything else can be revealed. However, the term \"black box\" is usually meant for models that hide structure, parameters, training dataset, etc., and can only be queried.", "title": "Term \"black box\" usually describes models that hide everything"}, "rkgx0tkj97": {"type": "rebuttal", "replyto": "H1gfa0IZ5Q", "comment": "Thanks for pointing that out. It will be fixed in the revision.", "title": "Will be fixed in the revision"}}}