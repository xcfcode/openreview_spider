{"paper": {"title": "Words or Characters? Fine-grained Gating for Reading Comprehension", "authors": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W. Cohen", "Ruslan Salakhutdinov"], "authorids": ["zhiliny@cs.cmu.edu", "bdhingra@andrew.cmu.edu", "yey1@andrew.cmu.edu", "junjieh@cmu.edu", "wcohen@cs.cmu.edu", "rsalakhu@cs.cmu.edu"], "summary": "", "abstract": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test and Who Did What datasets. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task.", "keywords": ["Natural language processing", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The consensus amongst reviewers is that this paper's proposal for combining character level information with word-level information is sound, well presented, and well evaluated. The main negative sentiment was that the approach was perhaps a little increment, although the conceptual size of the increment was sufficient to warrant publication, and will be relevant to the interests of ICLR attendees working on the intersection of deep learning and NLP. After a cursory reading of the paper, I see no reason to disagree, and recommend acceptance."}, "review": {"H1EJQivul": {"type": "rebuttal", "replyto": "SkL7fPw_e", "comment": "Thanks for pointing this out, but I don't think the paper you mentioned is \"prior work\". Our paper was released on arxiv on Nov 6, while the paper you mentioned was released on arxiv on Nov 14. I would think these two are in parallel. Nevertheless, we would cite this paper according to your suggestion such that people reading our paper will also be aware of related parallel work.", "title": "reponse"}, "SkL7fPw_e": {"type": "rebuttal", "replyto": "B1hdzd5lg", "comment": "Sorry to pop up late, but I've been looking over ICLR accepted papers, and I noticed this one. Something very similar, by combining word and character information at the feature level using sigmoid gating, has been done before, see https://aclweb.org/anthology/C/C16/C16-1030.pdf\n\nIt would be good to see a citation to prior work in the final version of this paper. ", "title": "Prior work on this area"}, "SkwHLdkQg": {"type": "rebuttal", "replyto": "B1DsxsTzg", "comment": "We tried bidirectional RNNs in our preliminary experiments and did not see noticeable differences. Thus we use uni-directional character-level RNNs throughout the paper.", "title": "Char RNN"}, "Hyk9rO1Xe": {"type": "rebuttal", "replyto": "Hy-T0JRMx", "comment": "Deep feed forward neural network is a universal function approximator, so any representation can be represented as a feed forward network applied on the flat concatenation of all inputs, but there are difficulties in optimization and generalization. We believe the inductive bias we introduce is helpful, with the following reasons:\n1. One of our baseline models uses the concatenation of character-level representation, word-level representation, and the features as the input, followed by multiple layers of LSTMs. If we only consider the feedforward depth, this model is essentially a deep feedforward network applied on the flat concatenation, so this can serve as a baseline to show that simple concatenation followed by feedforward layers fails to learn efficient representation.\n2. The number of layers is tuned as a hyper-parameter on the development set, and further increasing the number of layers increases the number of parameters and leads to overfitting. In contrast, fine-grained gating actually reduces the number of parameters, compared to concatenation.\n3. Our comparison with baseline models covers the current state-of-the-art approaches to combine word and character representations, including concatenation and scalar weighting.", "title": "Clarification of baseline comparison"}, "Hy-T0JRMx": {"type": "review", "replyto": "B1hdzd5lg", "review": "Considering that combined representation is a nonlinear function of the features, lookup value and the char-level representations (essentially the flat concatenation), did you try a, say, deep feedforward neural network to implement that function? I think that would be a good baseline to compare against to show that the inductive bias introduced by fine-grained gating is indeed helpful compared to other means of implementing that function.I think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications). I like the fact that the authors use POS and NER in terms of an intermediate signal for the decision. Also they compare against a sufficient range of baselines to show the effectiveness of the proposed model.\n\nI am also convinced by the authors' answers to my question, I think there is sufficient evidence provided in the results to show the effectiveness of the inductive bias introduced by the fine-grained gating model.", "title": "Deep baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJ-dvmfEg": {"type": "review", "replyto": "B1hdzd5lg", "review": "Considering that combined representation is a nonlinear function of the features, lookup value and the char-level representations (essentially the flat concatenation), did you try a, say, deep feedforward neural network to implement that function? I think that would be a good baseline to compare against to show that the inductive bias introduced by fine-grained gating is indeed helpful compared to other means of implementing that function.I think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications). I like the fact that the authors use POS and NER in terms of an intermediate signal for the decision. Also they compare against a sufficient range of baselines to show the effectiveness of the proposed model.\n\nI am also convinced by the authors' answers to my question, I think there is sufficient evidence provided in the results to show the effectiveness of the inductive bias introduced by the fine-grained gating model.", "title": "Deep baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1DsxsTzg": {"type": "review", "replyto": "B1hdzd5lg", "review": "Hi, is there a particular reason for using a mono-directional RNN as character-level encoding? Have you tried with more expressive models i.e., a bidirectional RNN?SUMMARY.\n\nThe paper proposes a gating mechanism to combine word embeddings with character-level word representations.\nThe gating mechanism uses features associated to a word to decided which word representation is the most useful.\nThe fine-grain gating is applied as part of systems which seek to solve the task of cloze-style reading comprehension question answering, and Twitter hashtag prediction.\nFor the question answering task, a fine-grained reformulation of gated attention for combining document words and questions is proposed.\nIn both tasks the fine-grain gating helps to get better accuracy, outperforming state-of-the-art methods on the CBT dataset and performing on-par with state-of-the-art approach on the SQuAD dataset.\n\n\n----------\n\nOVERALL JUDGMENT\n\nThis paper proposes a clever fine-grained extension of a scalar gate for combining word representation.\nIt is clear and well written. It covers all the necessary prior work and compares the proposed method with previous similar models.\n\nI liked the ablation study that shows quite clearly the impact of individual contributions.\nAnd I also liked the fact that some (shallow) linguistic prior knowledge e.g., pos tags ner tags, frequency etc. has been used in a clever way. \nIt would be interesting to see if syntactic features can be helpful.\n\n", "title": "character-level encoding", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ByGtCUlVl": {"type": "review", "replyto": "B1hdzd5lg", "review": "Hi, is there a particular reason for using a mono-directional RNN as character-level encoding? Have you tried with more expressive models i.e., a bidirectional RNN?SUMMARY.\n\nThe paper proposes a gating mechanism to combine word embeddings with character-level word representations.\nThe gating mechanism uses features associated to a word to decided which word representation is the most useful.\nThe fine-grain gating is applied as part of systems which seek to solve the task of cloze-style reading comprehension question answering, and Twitter hashtag prediction.\nFor the question answering task, a fine-grained reformulation of gated attention for combining document words and questions is proposed.\nIn both tasks the fine-grain gating helps to get better accuracy, outperforming state-of-the-art methods on the CBT dataset and performing on-par with state-of-the-art approach on the SQuAD dataset.\n\n\n----------\n\nOVERALL JUDGMENT\n\nThis paper proposes a clever fine-grained extension of a scalar gate for combining word representation.\nIt is clear and well written. It covers all the necessary prior work and compares the proposed method with previous similar models.\n\nI liked the ablation study that shows quite clearly the impact of individual contributions.\nAnd I also liked the fact that some (shallow) linguistic prior knowledge e.g., pos tags ner tags, frequency etc. has been used in a clever way. \nIt would be interesting to see if syntactic features can be helpful.\n\n", "title": "character-level encoding", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}