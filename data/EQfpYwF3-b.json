{"paper": {"title": "Deep Learning meets Projective Clustering", "authors": ["Alaa Maalouf", "Harry Lang", "Daniela Rus", "Dan Feldman"], "authorids": ["~Alaa_Maalouf1", "~Harry_Lang1", "~Daniela_Rus1", "~Dan_Feldman1"], "summary": "We suggest a novel technique for compressing a fully connected layer (or an embedding layer).", "abstract": "A common approach for compressing Natural Language Processing (NLP) networks is to encode the embedding layer as a matrix $A\\in\\mathbb{R}^{n\\times d}$, compute its rank-$j$ approximation $A_j$ via SVD (Singular Value Decomposition), and then factor $A_j$ into a pair of matrices that correspond to smaller fully-connected layers to replace the original embedding layer. Geometrically, the rows of $A$ represent points in $\\mathbb{R}^d$, and the rows of $A_j$ represent their projections onto the $j$-dimensional subspace that minimizes the sum of squared distances (``errors'') to the points. \nIn practice, these rows of $A$ may be spread around $k>1$ subspaces, so factoring $A$ based on a single subspace may lead to large errors that turn into large drops in accuracy.\n\nInspired by \\emph{projective clustering} from computational geometry,  we suggest replacing this subspace by a set of $k$ subspaces, each of dimension $j$, that minimizes the sum of squared distances over every point (row in $A$) to its \\emph{closest} subspace. Based on this approach, we provide a novel architecture that replaces the original embedding layer by a set of $k$ small layers that operate in parallel and are then recombined with a single fully-connected layer. \n\nExtensive experimental results on the GLUE benchmark yield networks that are both more accurate and smaller compared to the standard matrix factorization (SVD). For example, we further compress DistilBERT by reducing the size of the embedding layer by $40\\%$ while incurring only a $0.5\\%$ average drop in accuracy over all nine GLUE tasks, compared to a $2.8\\%$ drop using the existing SVD approach.\nOn RoBERTa we achieve $43\\%$ compression of the embedding layer with less than a $0.8\\%$ average drop in accuracy as compared to a $3\\%$ drop previously.", "keywords": ["Compressing Deep Networks", "NLP", "Matrix Factorization", "SVD"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes to use projective clustering to compress the embedding layers of DNN. This is a novel interesting idea which can  impact the area of Knowledge distillation. There were some concerns about the empirical study which was addressed to some extent  by the authors during the rebuttal."}, "review": {"tsnw8rFFHb9": {"type": "review", "replyto": "EQfpYwF3-b", "review": "This work proposes a new approach, based on projective clustering, for compressing the embedding layers of DNNs for natural language modeling tasks. The authors show that the trade-off between compression and model accuracy can be improved by considering a set of k subspaces rather than just a single subspace. Methods for compressing DNNs is an active area of research and this paper presents a promising approach to do so as well as interesting results. \n\nRating: The paper presents interesting ideas for compressing embedding layers. However, since this is an empirical paper, I would expect a more comprehensive set of empirical results and a better comparison with other related methods. Overall, the paper seems not very mature in its current form, hence my rating is 'Ok but not good enough - rejection'.\n\nPros\n----\n* The proposed method is appealing due to its simplicity and the idea of considering multiple subspaces for embedding is plausible in the context of compressing embedding matrices of NLP models.\n\n* The results show improvements as compared to using just a single subspace. \n\n* The framework provides several ideas for future works. \n\nCons\n-----\n* Typically, the SVD takes the form A = UDV, where U and V are the left and right singular vectors and the diagonal entries of D are the singular values. From the discussion it is not clear whether you factor the singular values into U, or whether you simply ignore the singular values? Also, how do you enforce the orthogonality constraints on U and V during the fine tuning stage? Have you considered a simpler low-rank factorization A = EF in your experiments, where no orthogonality constraints on E and F are imposed?\n\n* It would be good to see the progression for k={2,3,4,5} in Figure 5 and 6. Further, the ensemble approach in Figure 6 hasn't been discussed in detail anywhere in the paper. It is not exactly clear to me how you are computing the ensemble.\n\n* It would be very helpful to see some Tables that shows the total number of weights, accuracy, k, j, etc., in order to better understand the performance.  \n\n* How do you determine k and j in practice? Are you using some heuristic or are you simply doing a grid search?\n\n* I would like to see how your method compares to ALBERT and whether a modified ALBERT (as you suggest in your future work section) is doing better.\n \n* I would be interesting to see if you approach is also useful for compressing a fully connected layer in different settings. This should be easy to test and could be reported in the Appendix. \n\nMinor comments:\n--------------\n* It is nice to see that you have many generalizations an extensions in mind, but this section appears very lengthy to me. \n\n* compression rater -> compression rates", "title": "Interesting work", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "8ZxHTiIGmUJ": {"type": "rebuttal", "replyto": "RcbG-gMSN45", "comment": "We have taken care of all your comments. Please let us know in case we missed anything.", "title": "Paper Revised"}, "ef1eWBnDtWu": {"type": "rebuttal", "replyto": "z5Fh_iUbnR9", "comment": "We appreciate the careful reading and thoughtful interchange, which aided us greatly in improving the clarity of our writing.", "title": "Thank you"}, "qCpNN6gp3-x": {"type": "rebuttal", "replyto": "3s5xP49jfPB", "comment": "We are glad you like the updated paper.  Thank you for your careful and attentive reading, and we look forward to seeing your updated rating after our edits based on your comments.", "title": "Thank you"}, "dyOldxP589g": {"type": "rebuttal", "replyto": "RcbG-gMSN45", "comment": "Dear Reviewer1\n\nWe are happy to update you that we have added many experiments including (but not only) :\n\n1.compressing fully connected layers using the suggested approach;  see section C.\n\n2. Improving the accuracy of a given model using the new architecture while maintaining the same number of parameters; see Table 2 \n\nWe also updated the discussion, conclusion, and future work sections as requested.\n\nFinally, we would like to thank you again for your detailed comments and helpful review.\n\n", "title": "Rebuttal Reply #2 "}, "ejVqcbJPf0A": {"type": "rebuttal", "replyto": "z5Fh_iUbnR9", "comment": "Dear Reviewer4,\n\nWe are happy to update that we tried one of our suggested extensions as requested, specifically speaking we tried the first suggestion of $\\ell^q$-error, with $q=1$.\n\nTo compute a local minimum for the new cost function,  we still use EM algorithm. The only difference is that the SVD computation of the optimal subspace for a cluster of points ($k=1$) is replaced by an approximation algorithm for computing the subspace that minimizes sum over non-squared distances.\n\nHowever, since the deterministic approximation algorithms for the new problem ($\\ell^1$ error) with $k=1$ take a time of $O(nd^4)$ (at least),  where $d=768$.  This change increased the running time of the algorithm from minutes to days (we need to run this approximation algorithm many times in the EM procedure.)\n\nFor that and due to time constraints, we conducted our experiments only on one network (RoBERTA) on $2$ tasks from the GLUE benchmark (MRPC and RTE).\n\nTable 3 compares the original approach of the paper to the $\\ell^1$ error, and it can be seen that mostly using the $\\ell^1$ error as initialization is better than the $\\ell^2$. However, for some reason (that needs further investigation), after fine-tuning for $2$ epochs both approaches reached almost the same accuracy, even more, the $\\ell^2$ approach achieved a better accuracy sometimes. We leave this for future research. \n\n------------------------------------------------------\n\nFinally, we would like to thank you again for appreciating our result, and for the very helpful suggestions to improve the paper.\n ", "title": "Rebuttal Reply #2 "}, "_T7DRNnxtoC": {"type": "rebuttal", "replyto": "tsnw8rFFHb9", "comment": "Dear Reviewer5,\n\nWe are happy to update the following:\n\n1. We have added the suggested table, you can see it in the new version (Table 1). Please, feel free to ask for any change/improvement.\n\n2. Experiments on Albert:\n\nWe tested if the suggested architecture (MESSI) can improve the accuracy of a pre-trained model while maintaining the same number of parameters.  So we used the model ALBERT and factored its embedding layer to the suggested MESSI architecture using our pipeline.  Here, we choose the values of $k$ and $j$ such that the original embedding layer size is maintained (up to a very small change).  The results are so promising. Table 2 in the paper shows that we improved the accuracy of the original model for most of the tasks.\n\n3. As previously updated, all other comments have been fixed, and experiments have been added.\n-------------------------------------------------\n\nFinally, we thank the reviewer for the comments that helped us further improve the quality of this paper. \n\n\n", "title": "Rebuttal Reply #2"}, "U3DFfgNw514": {"type": "rebuttal", "replyto": "RcbG-gMSN45", "comment": "We thank the reviewer for the suggestions to improve the writing, and for the detailed review.\n\nQ: the idea and the technique is not novel. See the related literature below\n\nA: Of course, projective clustering is a classic old technique. We gave citations to all of the reviewer's suggestions.\nNevertheless, as the title implies, our paper is the first that forges a link between projective clustering and Deep Learning.\nIt also answers the question of how to apply this classic technique to deep learning, by presenting a new network architecture.\nThe experimental results show that this new meeting of fields improves the results significantly in practice.\n\n --------------------------------------------------------------------------------------------\n\nabstract:\n\nQ: The abbreviations like NLP or SVD should be defined first, then used.\n\nA: Fixed.\n\nQ: The last sentence \u201cOpen Code for \u2026.\u201d Should not be mentioned in the abstract but in the code description section.\n\nA. Fixed.\n\nAs for the other comments about the abstract: \n\nAll the questions of the reviewers regarding the abstract are answered in the introduction. We try to add more hints but due to the space constraints, we can not answer all the questions already in the abstract.\n\n --------------------------------------------------------------------------------------------\n\nResults: \n\nQ It would be better to discuss the comparable results more thoroughly.\n\nA: Due to space limitations, such discussions appear in the appendix. More discussions will be added (before the rebuttal period ends) following the reviewer's request.\n\nQ: The conclusion is not provided explicitly.\n\nA: Fixed, thanks to the reviewer for pointing this out.\n\nQ:  Better not to start the section with numbered items right away. Better to have a starting sentence first. Appendix B\n\nA: Fixed.\n\n Q:  Titled results before fine-tuning and includes figures with no explanation. Provide proper description and discussion for each subfigure.\n\nA: Fixed .\n\n\n\n\n\n\n\n\n", "title": "Most of the writing issues are fixed as requested by the reviewer, and we are working on fixing the rest."}, "D9WE9Qidrz": {"type": "rebuttal", "replyto": "tsnw8rFFHb9", "comment": "First, we thank the reviewer for the detailed review and very helpful suggestions.\n\nQ: The paper presents interesting ideas for compressing embedding layers.\n\nA: We thank the reviewer for appreciating the ideas in our paper.\n\n --------------------------------------------------------------------------------------------\nQ. I would expect a more comprehensive set of empirical results \n \nA: Following this reviewer's request,  many experiments were added. Including \n\n    1. compressing fully connected layers using the suggested approach.\n\n    2. checking how another clustering method can fit in our pipeline. \n\n    3. more results on other $k$-value as requested.\n\nAlso, we are coding and running more experiments now -- hoping to finish before the deadline.\n\n --------------------------------------------------------------------------------------------\n\nQ:  Typically, the SVD takes the form A = UDV...from the discussion, it is not clear whether you factor the singular values into U, or whether you simply ignore the singular values?\n\nA: As in previous papers, we use the more general factorization (as e.g. in NNMF) A=EF which corresponds to a pair of layers. This means that we did not ignore the singular values, and they can be assigned to either the left or right matrix. See next question.\n\n --------------------------------------------------------------------------------------------\n\nQ: how do you enforce the orthogonality constraints on U and V during the fine-tuning stage? \n\nA: We did not.  This is a very good point that was added to the text. The orthogonalization is used to obtain a low rank approximation A~EF using SVD. From this point, we did not see an advantage to keep this property in the network.\n\n --------------------------------------------------------------------------------------------\n\nQ: Have you considered a simpler low-rank factorization A = EF in your experiments, where no orthogonality constraints on E and F are imposed?\n\nA: This is exactly what we do after computing SVD, as explained in the previous answers. In fact, any non-orthogonal base to the span of the reduced matrix will do.  It is an interesting idea to try other non-orthogonal basis, as done e.g. in Dictionary learning.\n\n --------------------------------------------------------------------------------------------\n\nQ:  It would be good to see the progression for k={2,3,4,5} in Figure 5 and 6.\n\nA: Added. We thank the reviewer for this good suggestion.\n\n --------------------------------------------------------------------------------------------\n\nQ:  The ensemble approach in Figure 6 hasn't been discussed in detail anywhere in the paper. \n\nA:  Added. Due to space constraints, the graphs and explanations can be found in the appendix. \n\n --------------------------------------------------------------------------------------------\n\n\nQ:  It would be very helpful to see some Tables that show the total number of weights, accuracy, k, j, etc., in order to better understand the performance.\n\nA: We thank the reviewer for the suggestion, and expect to finish this task before the end of the rebuttal. \n\n --------------------------------------------------------------------------------------------\n\n\nQ: How do you determine k and j in practice? Are you using some heuristic or are you simply doing a grid search?\n\nA:  For a given compression rate $x$, we try multiple values of $k$ via binary search on $k$.,For every such $k$ value we compute the implied value  $j = (1-x)dn/(n+kd)$, and then check the compression result on those values.\n\n --------------------------------------------------------------------------------------------\n\nQ: Apply your technique on ALBERT.\n\nA: We are implementing and running these experiments and expect to finish before the end of the rebuttal.\n\n --------------------------------------------------------------------------------------------\n\nQ: Compress a fully connected layer in different settings using your approach.\n\nA: Added. See Appendix C for  LENET_300_100 and vgg19 models. The results are strong and better than the competitors as in the other experiments. \n\n --------------------------------------------------------------------------------------------\n\nQ: It is nice to see that you have many generalizations and extensions in mind, but this section appears very lengthy to me.\n\nA: Fixed. We were indeed excited by our results and have many future ideas. However, most of this section was moved to the appendix.\n\n --------------------------------------------------------------------------------------------\n\nQ: compression rater -> compression rates\n\nA: Fixed. We thank the reviewer for the careful reading.\n", "title": "We thank the reviewer for the suggestions -- many experiments were added, and we are working on adding more."}, "n3vaTO5uxfE": {"type": "rebuttal", "replyto": "z5Fh_iUbnR9", "comment": "First, we thank the reviewer for appreciating our result, for the high scoring, and for the very helpful suggestion.\n\nQ. I would be really interested in seeing the comparison of projective clustering vs simpler clustering methods such as k-means\n\nA: Added. This was a very good idea that we believe that significantly improved the paper. See graphs in section E.\n\n The new graphs show that projective clustering can guarantee a better initialization, i.e., better accuracy before fine-tuning. This implies a fewer number of epochs.\n\n--------------------------------------------------------------------------------------------------------\n\nQ: Is projective clustering the only way to find clusters in multiple subspaces?\n\nA: It is a very good question and the answer depends on the definition of \"clusters in multiple subspaces\", i.e., the generative model. Projective Clustering aims to compute subspaces that maximize the likelihood, assuming that every point was generated by adding some noise to a point on a single subspace. As the reviewer suggests, we may assume soft clustering, e.g., that every point is a linear combination of points on a pair of subspaces. Such versions may be defined e.g. via Dictionary Learning. We expect that this paper will inspire many such generalizations in future works via our suggested network architecture or its variants.\n\n--------------------------------------------------------------------------------------------------------------------\n\nQ.The authors discussed extensions such as using error and distance, but no experiments were performed for the extensions. Some experiment results will be better to establish the flexibility of the framework of projective clustering in model compression tasks.\n\nA. We are doing our best to add such experiments, we believe we can provide some before the rebuttal period.\n\n", "title": "We added more experiments thanks to the reviewer's suggestion, and we are working on more."}, "RcbG-gMSN45": {"type": "review", "replyto": "EQfpYwF3-b", "review": "This paper extends the idea of using subspace clustering to compress the neural nets by considering multiple subspaces and projecting each point to its closest subspace. The paper needs more investigation on the related works. Basically, the idea and the technique is not novel. See the related literature below:\n[1] Trittenbach, Holger, and Klemens B\u00f6hm. \"One-Class Active Learning for Outlier Detection with Multiple Subspaces.\" Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2019.\n[2] Liu, Risheng, et al. \"Fixed-rank representation for unsupervised visual learning.\" 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2012.\n[3] Xu, Dong, et al. \"Concurrent subspaces analysis.\" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). Vol. 2. IEEE, 2005.\n[4] Feng, Jianzhou, et al. \"Learning dictionary via subspace segmentation for sparse representation.\" 2011 18th IEEE International Conference on Image Processing. IEEE, 2011.\n---------------------------------------------\nPros:\n\u2022\tSmoothly readable. \n\u2022\tThe contribution section is described thoroughly and properly. \n\u2022\tProviding the codes for reproducing results.\n---------------------------------------------\nCons:\nAbstract:\n\u2022\tThe abbreviations like NLP or SVD should be defined first, then used. \n\u2022\tAssuming that the reader already has corresponding field knowledge about systems such as GLUE, DistilBERT, or RoBERTa and mentioning them in the abstract may be bold.\n\u2022\tDetails of the methods such as the use of Aj matrix or k>1 subspace should not be mentioned in the abstract but rather in the contribution or introduction section accordingly.\n\u2022\tThe last sentence \u201cOpen Code for \u2026.\u201d Should not be mentioned in the abstract but in the code description section. \n\u2022\tThe figures 1-3 in the paper look not well organized, which makes the proposed simple idea to be extremely complex.\nResults:\n\u2022\tIt would be better to discuss the comparable results more thoroughly. \n\u2022\tModel compression literature should be reviewed and the typical methods should be compared with in the experiments.\nDiscussion and Conclusion:\n\u2022\tOnly discussion of the results is provided in this section and the conclusion is not provided explicitly. \n\n\nFuture Work:\n\u2022\tBetter not to start the section with numbered items right away. Better to have a starting sentence first. \nAppendix B\n\u2022\tTitled results before fine-tuning and includes figures with no explanation. Provide proper description and discussion for each subfigure. \n", "title": "Notitle", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "z5Fh_iUbnR9": {"type": "review", "replyto": "EQfpYwF3-b", "review": "Summary:\nThis paper applies projective clustering to the embedding layer of deep networks with large model sizes such as RoBERTa. The idea of finding more than one subspaces to factorize the embedding weight matrix has nice intuition and insights. I vote for accepting.\n\nStrengths:\n1. The paper has convincing evidence showing the reduction in percent of accuracy drop when applying projective clustering to the embedding weight vectors.\n2. The paper has illustration figures that clearly show the intuition of the approach as well as how the compression is achieved.\n\nWeaknesses:\n1. It would be better if more baselines can be included in the experiment comparisons. In particular, Since Step 2-3 of the proposed MESSI pipeline (page 4) is partitioning of all the input neurons and computing SVD for each partition, I would be really interested in seeing the comparison of projective clustering vs simpler clustering methods such as k-means to partition the input neurons, in the evaluation.\n2. The authors discussed extensions such as using $L_1$ error and $L_1$ distance, but no experiments were performed for the extensions. Some experiment results will be better to establish the flexibility of the framework of projective clustering in model compression tasks.\n\nQuestions during rebuttal period: \n1. Please provide some results regarding the weaknesses above, especially the result of more baseline methods.\n2. Is projective clustering the only way to find clusters in multiple subspaces? What are some alternatives? For example, in subspace clustering, all the data points can be projected to the same subspace and form clusters; we may run subspace clustering for multiple times to get clustering results in different subspaces.", "title": "Nice application of projective clustering to model compression", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}