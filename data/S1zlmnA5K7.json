{"paper": {"title": "Where Off-Policy Deep Reinforcement Learning Fails", "authors": ["Scott Fujimoto", "David Meger", "Doina Precup"], "authorids": ["scott.fujimoto@mail.mcgill.ca", "david.meger@mcgill.ca", "dprecup@cs.mcgill.ca"], "summary": "We describe conditions where off-policy deep reinforcements algorithms fail and present a solution.", "abstract": "This work examines batch reinforcement learning--the task of maximally exploiting a given batch of off-policy data, without further data collection. We demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are only capable of learning with data correlated to their current policy, making them ineffective for most off-policy applications. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space to force the agent towards behaving on-policy with respect to a subset of the given data. We extend this notion to deep reinforcement learning, and to the best of our knowledge, present the first continuous control deep reinforcement learning algorithm which can learn effectively from uncorrelated off-policy data.", "keywords": ["reinforcement learning", "off-policy", "imitation", "batch reinforcement learning"]}, "meta": {"decision": "Reject", "comment": "The paper proposes batch-constrained approach to batch RL, where the policy is optimized under the constrain that at a state only actions appearing in the training data are allowed.  An extension to continuous cases is given.\n\nWhile the paper has some interesting idea and the problem of dealing with extrapolation in RL is important, the approach appears somewhat ad hoc and the contributions limited.\n\nFor example, the constraint is based on whether (s,a) is in B, but this condition can be quite delicate in a stochastic problem (seeing a in s *once* may still allow large extrapolation error if that only observed transition is not representative).  Section 4.1 gives some nice insights for the special finite MDP case, but those results are a little weak (requiring strong assumption that may not hold in practice) --- an example being the requirement that s' be included in data if (s,a) is in data and P(s'|s,a)>0 [beginning of section 4.1].\n\nIn contrast, there are other more robust and principled ways, such as counterfactual risk minimization (CRM) for contextual bandits (http://www.jmlr.org/papers/v16/swaminathan15a.html).  For MDPs, the Bayesian version of DQN (the cited Azizzadenesheli et al., as well as Lipton et al. at AAAI'18) can be used to constrain the learned policy as well, with a simple modification of using the CRM idea for bandits.  Would these algorithms be reasonable baselines?"}, "review": {"HJe89GvsyE": {"type": "rebuttal", "replyto": "r1eYNjE9k4", "comment": "It is true that specific/adversarial counter-examples exist for most forms of function approximation. However, in this work we examine environments where deep Q-learning algorithms have already been shown to perform well on. We show that given the same dataset, deep off-policy algorithms can perform very differently depending on how close their policy is to the policy which generated the dataset. This is an important insight because we show that off-policy algorithms, which would otherwise perform well, can now fail. Furthermore, we demonstrate how this issue can be corrected and introduce a practical algorithm.", "title": "Re: Relationship with previous work?"}, "BkeBv0rmC7": {"type": "rebuttal", "replyto": "Hygun0mzAQ", "comment": "Hi again, glad the response was helpful!\n\n(1) The inverse weighting happens by taking the mean in the KL divergence term (line 150) rather than the sum. (2) Good catch! Yes, the clipping only occurs during inference, the VAE training itself is not modified. We have updated the supplementary to reflect this. ", "title": "Re: Minor Clarification"}, "BJlW8CJGCm": {"type": "rebuttal", "replyto": "r1eG-1RlCQ", "comment": "Hi, thanks for your interest in the paper, and for catching a few typos! When the paper is de-anonymized we will include a GitHub repository in the paper. Until then, we have included the core algorithm code in an anonymous pastebin ( https://pastebin.com/UTaiR5ZZ ), which should speed up your implementation.\n\n> In the algorithm description (Algorithm 1 in the paper), could you explain how the perturbation model is updated in more detail. Equation 10 is equally confusing. I'm trying to understand whether the actions should come from a sampled mini-batch or if they should come from the VAE decoder when (1) passing them to the perturbation model to compute perturbations and (2) when actually perturbing the actions before passing them to the critic network.\n\nThe actions can come from anywhere. Either the mini-batch, randomly generated, or from the VAE. During the target update and during test time, the perturbations are applied to actions generated by the VAE, so it makes the most sense to train them on the same distribution. \n\nEquation 10 describes the deterministic policy gradient algorithm (DPG). The perturbation model is updated so that the perturbed actions maximize the critic. Normally with DPG, there is a policy pi(s) which is trained to maximize Q(s,pi(s)). In our method, instead of pi(s) which can output any action, we have xi(s,a) + a, where xi(s,a) can only output in a limited range. \n\n> Could you also give me some more intuition on why a perturbation model is trained? It seems as though we could just leave the perturbation model out and just do Q-learning using the VAE, critic, and value networks. \n\nYou could, but as discussed in the paper, using a perturbation model gives an increase in flexibility in the action selection. For example, if the VAE outputs 10 similar actions, allowing perturbations gives more diversity in actions that could be possibly selected. Another case is where the entire action space has been adequately covered, and any action could be viably generated by the VAE. To see the highest valued action would likely require sampling from the VAE many times. Allowing perturbations avoids this issue. \n\n> Finally, equation (11) implies that the value function is trained on a mini-batch of next states instead of current states. Is there a reason why the value function loss isn't just over the current states (s) instead of the next states (s')?\n\nIn principle it doesn't matter, but the code does in fact train over s, as shown in Algorithm 1. Thanks for catching this, along with the other typos. We have uploaded a corrected version. ", "title": "Re: Questions on implementation details"}, "BJeevRYjpX": {"type": "rebuttal", "replyto": "S1zlmnA5K7", "comment": "We would like to take this opportunity to thank each reviewer again. We found that the quality of the reviews was high and the reviewers made insightful commentary, each with different flavors with respect to the paper. As this paper introduces the first analysis into the batch setting with deep function approximation, it makes sense that there would be small issues in clarity, and as displayed by our lengthy related works, there are many possible interpretations to where, and how, our work is insightful or significant. We have carefully responded to each reviewer and have made many small updates throughout the paper to improve the clarity and dispel any confusion about the task as well as our approach.\n \nAs mentioned in our response to Reviewer 2, almost all significant issues with clarity were from Section 4.2 which we have re-written to better justify the design choices made when approximating the batch-constraint in a continuous setting. We have expanded the introduction on extrapolation error to be more explicit on its origin. Additionally, although no reviewer took issue with the original title, we have taken the opportunity to modify the title to be more informative towards the contents of the paper. We believe these changes address the reviewers\u2019 concerns and greatly improve the quality of the paper. We will continue to edit the paper before the deadline and our happy to respond to further comments, questions or concerns. \n", "title": "General Response and Overview"}, "Hyglr0YiTm": {"type": "rebuttal", "replyto": "HJeQ-p0F2Q", "comment": "We would like to thank the reviewer for their time, feedback and thoughts. A concern presented by the reviewer was limited technical contribution. We would like to re-emphasize our contribution towards the introduction and analysis of extrapolation error in off-policy learning. Our paper provides important insight into the working of deep reinforcement learning with finite amounts of data, or the purely \u201cexploitative\u201d setting, as well as imitation learning with noisy demonstrations. \n  \n> It makes intuitive sense that the proposed approach works well as long as we only encounter state-action pairs that are closed to one of the state-action pairs in the batch.  However, I do not expect that this is always the case.  The proposed method is to simply choose the closest action in the batch.  Then why does the proposed approach perform well?\n \nIn regions with no data at all, there is no possible mechanism for recovery because the agent, and any possible agent, will not have trained in this region. Taking the action of the closest state-action pair is an oversimplification of our method, which likely stems from the lack of clarity in our original version of Section 4.2, which has been re-written. Our BCQ algorithm produces deep network policies that can be evaluated across the entire state space and considers both the similarity of the action to the batch as well as the expected value of the action. \n\nThat being said, it is important to take actions which are \u201cclose\u201d in the Bellman update to minimize the extrapolation error in the value estimate. Otherwise, as shown in Section 3.2, there can be deterioration in performance even in regions of certainty. That is, a non-batch-constrained off-policy reinforcement learning algorithm may fail if exposed to any uncertain regions during training. Our algorithm performs well by reducing the error into the system. Informally, our value estimates are more accurate. \n \nIn experiments where BCQ may take actions leading it to unseen states, such as in the experiments with an expert behavioral policy without exploration, we find that there is sufficient generalization to regions with less data to still perform well, while stabilizing the value function. \n\nFor future work, an interesting extension of the algorithm would be to bias it towards regions of certainty, through an optimism-under-certainty heuristic, the polar-opposite to many exploration algorithms. This occurs implicitly in our algorithm as mimicking previously taken actions is more likely to lead to regions of certainty, but could be enforced more strongly. \n \n> A key assumption in the discrete case is that whole episodes are in the batch.  This is rather restricting, because in many applications, it is infeasible to collect a whole episode, and parts of many episodes are collected from many agents.\n \nThe data doesn't need to be collected in episodic fashion, rather, that there is sufficient coverage. Collecting data in episodes is one way to ensure this, but not specifically required. This is a weaker assumption than assumptions necessary for standard Q-learning, as we no longer require visitation over all possible state-action pairs. ", "title": "Response to Reviewer 3"}, "Bye92aKsTQ": {"type": "rebuttal", "replyto": "rylyPcQ9h7", "comment": "We would like to thank the reviewer for thorough review and constructive feedback. The issues with clarity largely stemmed from Section 4.2, which we agree with the reviewer was not as clear as it could be. This section has been re-written and will hopefully satisfy the reviewer. We have removed superfluous details and simplified the presentation of Section 4.2. We believe these changes better streamlines the introduction of the (unchanged) algorithm, and better justifies some of the algorithmic choices. Other small adjustments to notation and clarity have been made throughout the paper, with regards to both your comments as well as the other reviewers.\n \nFurther Responses to Comments:\n \n> Page 6: \u201cTheorem 1 implies with access to only a subset of state-action pairs in the MDP, the value function\u2026 This suggests batch-constrained policies are a necessary tool for combating extrapolation bias.\u201d This might be true, but it does not follow from the Theorem 1 as it only applies to the batch Bellman operator and not the standard one used in most methods.\u201d\n \nThe claim we intended to make was not that batch-constrained policies are necessary, but rather suggest that they are likely, or potentially, necessary. We have clarified this in the paper.\n\n> Figure 4: where is \u201cTrue value\u201d curve on the plots?\n\nInitially we left out the true value curve to allow for a larger figure, putting more emphasis on the results. We have re-added the true value curve.\n", "title": "Response to Reviewer 2"}, "H1lXqTYiaQ": {"type": "rebuttal", "replyto": "SJxX5LHp3X", "comment": "We would like to thank the reviewer for their helpful comments and positive feedback. We have added an experimental result to the supplementary material to distinguish ourselves further from imitation learning algorithms and made several clarifying statements and adjustments based on your recommendations.\n \nOne con listed was missing comparisons against other state of the art imitation learning algorithms which are robust to noisy demonstrations. However, to the best of our knowledge, we are not aware of any which satisfy the batch setting, where no further data is collected, while also setting no requirements on data being labelled expert vs. non-expert. One algorithm which does satisfy these conditions is [1], but only operates in with discrete actions, making it weak baseline in a continuous control benchmark, where independent discretization would be required. If there was a particular algorithm you had in mind when writing the review, we would be happy to include it in the final paper.\n \nWe also note the line between off-policy and robust imitation is fairly thin. For example, in the tabular setting, our approach can learn from the set of data that includes all state-action pairs, similarly to off-policy learning. All state-action pairs, of course, also includes expert actions and could be considered a robust imitation learning algorithm as well. An expert behavioral policy is necessary for the data collection process to be sufficiently interesting, as a purely randomly policy doesn\u2019t cover enough of the state space for it to be possible to learn meaningful behavior. To further demonstrate the effectiveness of our algorithm as an off-policy algorithm, we included results with a purely random behavioral policy on a pendulum and reacher task in the supplementary material B, where the state space can be sufficiently covered by taking random actions. \n \nFurther Responses to Questions/Comments: \n\n> Could one obtain a similar effect to BCRL by simply initializing the value estimates pessimistically?\n \nEssentially yes, especially in the tabular setting, however, this would slow learning as it may take many updates to \u201cwash away\u201d initial negative bias. Furthermore, in a function approximation setting, maintaining an optimistic or pessimistic initialization over many timesteps is impractical and often implausible. Finally, for a fixed, non-batch-constrained policy, this also gives biased estimates. Introducing the notion of batch-constrained gives some understanding to when the policy would be biased vs. when it wouldn't.\n \n> Sec 4.1: Since B is a set of (s, a, s', r) tuples, what does it mean for a state s' to be \"in B\"? Similar question for state-action tuples (s, a).\n \ns' in B is shorthand for (s, a, s', r) in B for some s, a, r. We have added a clarifying sentence in the background.\n \n> As you note in the appendix, the construction in Sec 4.1 is essentially creating a new MDP that contains only the transitions that occur in the training data. I'd suggest stating as much in the main paper for intuition.\u201d\n \nAt your recommendation we have added this to the main paper.\n \n> * Sec 4.2 / 5: The perturbation constraint \\Phi is set to 0.05 in the experiments. Since the actions in these control problems are vectors, what does a scalar constraint correspond to? How is the constraint enforced during learning?\n \nThis correspond to \\Phi * I * tanh() following the final layer. We have added a clarifying sentence in the supplementary.\n\nReferences: \n[1] Gao, Yang, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. \"Reinforcement learning from imperfect demonstrations.\" arXiv preprint arXiv:1802.05313 (2018).", "title": "Response to Reviewer 1"}, "SJxX5LHp3X": {"type": "review", "replyto": "S1zlmnA5K7", "review": "Summary:\nProposes BCRL for learning from a fixed collection of off-policy experience (I'll call this the \"training data\"). BCRL attempts to avoid backing up values from states that are not present in the training data, on the assumption that the current estimates of these values are likely to be inaccurate. In the continuous state-action case, this is accomplished by training a generative model to propose, given a state `s`, an action `a` such that a transition similar to `(s, a)` is in the training data. A secondary policy is then trained to perturb the proposed action within a constrained region to maximize value. BCRL outperforms DDPG and DQN when learning from fixed data, but BCRL is slightly worse than behavior cloning at learning to reproduce an expert policy that does not take exploration actions.\n\nReview:\nThe overall approach is sound. The problem of extrapolation is intuitively obvious, but not something I had thought about before. I think typically exploration would correct the problem since states with over-estimated values would become more likely to be reached, giving an opportunity to get a better estimate. \n\nThe learning setting is closer to imitation learning than to what I would call RL, since the BCRL approach essentially avoids extrapolation error by ignoring the parts of the problem that are not represented in the training data. The well-known problem with behavior cloning is compounding errors once the agent strays into areas of the state space that are far from the training data. To me \"off-policy RL\" implies that the goal is to learn a complete policy from off-policy data. I think the \"competitors\" to which BCRL should be compared are imitation learning algorithms address noisy demonstrations, and not so much off-policy RL algorithms. It would also be interesting to see the generalization performance of BCRL outside of its training data.\n\nThe BCRL idea might be applicable in a conventional RL setting as well, since the initial stages of learning could be subject to a similar extrapolation error until there has been enough exploration. A comparison to something like TRPO in this setting would be interesting.\n\nThe paper is well-written with good coverage of related literature. There are a few points where the technical content is imprecise, which I note below. \n\nComments / Questions:\n* Could one obtain a similar effect to BCRL by simply initializing the value estimates pessimistically?\n* Sec 4.1: Since B is a set of (s, a, s', r) tuples, what does it mean for a state s' to be \"in B\"? Similar question for state-action tuples (s, a).\n* As you note in the appendix, the construction in Sec 4.1 is essentially creating a new MDP that contains only the transitions that occur in the training data. I'd suggest stating as much in the main paper for intuition.\n* Sec 4.2 / 5: The perturbation constraint \\Phi is set to 0.05 in the experiments. Since the actions in these control problems are vectors, what does a scalar constraint correspond to? How is the constraint enforced during learning?\n* What are the distance functions D_S and D_A?\n\nPros:\n* A good approach to applying RL methods in the \"imitation-like\" setting. I've seen similar things attempted before, but this method makes more sense. \n\nCons:\n* The learning setting is more like \"fuzzy\" behavior cloning from noisy data than off-policy RL. Experimental comparison against more-sophisticated imitation learning approaches is missing.", "title": "Solid approach to applying RL algorithms to batch imitation learning from noisy demonstrations", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rylyPcQ9h7": {"type": "review", "replyto": "S1zlmnA5K7", "review": "Authors consider a problem of off-policy reinforcement learning in a setting explicitly constrained to a fixed batch of transitions. \nThe argument is that popular RL methods underperform significantly in this setting because they fail to account for extrapolation error caused by inevitably sparse sampling of the possible action-state space.\nTo address this problem, authors introduce the notion of batch-constrained RL which studies policies and associated value functions only on the state-space covered by the available training data.\nFor practical applications a deep RL method is introduced which enables generalisation to the unseen states and actions by the means of function approximation.\n\nI find the problem studied in the paper very important. It is indeed strongly connected to the idea of imitation learning which has been studied previously, but I like the explicit point from which authors see the problem.\nThe experimental results seem quite appealing to justify use of the proposed approach.\n\nHowever, on the clarity side the paper should be improved before publication.\n\nThe interplay between action generating VAE G_w(s) and \\pi is unclear to me.\nFirst, what does it mean that G(s) is trained to minimise the distance D_A?\n\nIf G(s) is a VAE, then it is trained to minimise the corresponding variational lower bound, how is minimisation of the distance over actions is incorporated here? And what exactly is this distance?\nSimilarly, what does \u201cD_S will be defined by the implicit distance induced by the function approximation\u201d exactly mean?\n\nOther comments / questions:\n\nPage 6: \u201cTheorem 1 implies with access to only\na subset of state-action pairs in the MDP, the value function\u2026 This suggests batch-constrained policies\nare a necessary tool for combating extrapolation bias.\u201d\nThis might be true, but it does not follow from the Theorem 1 as it only applies to the batch Bellman operator and not the standard one used in most methods.\n\nCorollary 1 and 2: What is Q^* here?\n\nPage 7, first sentence: should there be if A_s, e != \\emptyset? \n\nEpsion-Batch-constrained policy iteration: would the beam search actually maximize Q function? This needs to be proven or at least discussed.\n\nI don\u2019t see how is epsilon used in the iteration scheme.  This needs to be clarified.\n\nEquation 11: the subscript of the max operator looks weird, should there be just a_i?\n\nFigure 4: where is \u201cTrue value\u201d curve on the plots?\n\nThe notation \\pi(s, a; \\Phi) used throughout the paper is confusing and can be interpreted as a joint distribution over states and actions.\n\nAs I said, currently the paper does not appear to be easy to follow to me and even if it does contain important ideas, I believe they must be communicated in a clearer way.\nI am eager to revise my evaluation if authors make substantial effort to improve the paper.", "title": "Interesting ideas, but clarity must be improved", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJeQ-p0F2Q": {"type": "review", "replyto": "S1zlmnA5K7", "review": "This paper studies extrapolation error in off-policy batch reinforcement learning (RL), where the extrapolation error refers to the overestimation of the value for the state-action pairs that are not in the training data.\n\nThe authors propose batch-constrained RL, where the policy is optimize under the constraint that, at each state, only those actions that have been taken in that state in the training data are allowed.  This is then extended to continuous space, where it allows only the state-action pairs that are close to a state-action pair in the training data.  When there is no such action for a given state, the action that is closet to a feasible action at that state is selected.\n\nIt makes intuitive sense that the proposed approach works well as long as we only encounter state-action pairs that are closed to one of the state-action pairs in the batch.  However, I do not expect that this is always the case.  The proposed method is to simply choose the closest action in the batch.  Then why does the proposed approach perform well?  Is it because the experiments are performed under rather deterministic settings?  How often are no state-action pairs found in the neighbor?  Is there any mechanism for recovering from \"not in the batch\"?\n\nThe paper would be much stronger if it study this challenge of \"not in the batch\" more in depth.  Technical contributions in the present paper are rather limited.\n\nA key assumption in the discrete case is that whole episodes are in the batch.  This is rather restricting, because in many applications, it is infeasible to collect a whole episode, and parts of many episodes are collected from many agents.  Although this assumption is stated, it would be nice to emphasize by also stating that the theorems do not hold when this assumption does not hold.  The assumption becomes less important for continuous case, because of approximation.  It might be interesting to study the performance of the proposed approach when the assumption does not hold in the continuous case.\n", "title": "How well and why it works at states that are far from any states in the batch?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkgTBMocc7": {"type": "rebuttal", "replyto": "r1lpP08c5Q", "comment": "Thank you for your interest in our paper! From what is stated at a theoretical level, Lemma 1 and Theorem 1 are not broken by non-determinism.\n \nThe Bellman operator assumes access to the underlying MDP as it includes an expectation over the next state. The batch Bellman operator, which we introduce, is an extension which masks out any unseen state-action pairs by setting their value to infinity. As a result, the batch Bellman operator can still access the true expectation, even with only a sample of the possible transitions in the batch, allowing Theorem 1 and Lemma 1 to hold.\n \nThat being said, I believe the point you are making has to do with the more practical scenario, say with a batch-constrained tabular Q-learning-- what happens if we only have a sample of the possible transitions, without access to the true expectation? Our approach makes the same practical assumption as other off-policy algorithms such as Q-learning and KBRL, which is that the samples you do have for (s,a) are representative of the true MDP. In this case, as you have stated, there is no guaranteed convergence to the true Q^pi(s,a), for any off-policy algorithm, unless the samples you have are indeed representative, e.g. the environment is deterministic or there is infinite data. And of course, we demonstrate the effectiveness of our approach in more complex settings in Section 5.\n", "title": "Re: What about MDP's where the batch doesn't show all the next states you get to after an action?"}, "rJxRRCmcqX": {"type": "rebuttal", "replyto": "Skxqda6YqX", "comment": "Hi, thanks for your question. Indeed, the value is usually dependent on the kernel function, but this weighting is normalized over all examples of the corresponding action (equation 5). With only one example of a1 the kernel term will be reduced to 1.", "title": "Re: question on example of extrapolation error in simple example"}}}