{"paper": {"title": "Multiagent Reinforcement Learning in Games with an Iterated Dominance Solution", "authors": ["Yoram Bachrach", "Tor Lattimore", "Marta Garnelo", "Julien Perolat", "David Balduzzi", "Thomas Anthony", "Satinder Singh", "Thore Graepel"], "authorids": ["yorambac@gmail.com", "lattimore@google.com", "garnelo@google.com", "perolat@google.com", "dbalduzzi@google.com", "twa@google.com", "baveja@google.com", "thore@google.com"], "summary": "For games that are solvable by iterated elimination of dominated strategies, we prove that simple standard reinforcement learning algorithms converge to the iterated dominance solution.", "abstract": "Multiagent reinforcement learning (MARL) attempts to optimize policies of intelligent agents interacting in the same environment. However, it may fail to converge to a Nash equilibrium in some games.  We study independent MARL under the more demanding solution concept of iterated elimination of strictly dominated strategies.  In dominance solvable games, if players iteratively eliminate strictly dominated strategies until no further strategies can be eliminated, we obtain a single strategy profile. We show that convergence to the iterated dominance solution is guaranteed for several reinforcement learning algorithms (for multiple independent learners). We illustrate an application of our results by studying mechanism design for principal-agent problems, where a principal wishes to incentivize agents to exert costly effort in a joint project when it can only observe whether the project succeeded, but not whether agents actually exerted effort. We show that MARL converges to the desired outcome if the rewards are designed so that exerting effort is the iterated dominance solution, but fails if it is merely a Nash equilibrium.", "keywords": ["multiagent", "reinforcement learning", "iterated dominance", "mechanism design", "Nash equilibrium"]}, "meta": {"decision": "Reject", "comment": "The paper proofs that reinforcement learning (using two different algorithms) converge to iterative dominance solutions for a class of multi-player games (dominance solvable games). \n\nThere was a lively discussion around the paper. However, two of the reviewers remain unconvinced of the novelty of the approach,  pointing to [1] and [2], with [1] only pertaining to supermodular games. The exact contribution over such existing results is currently not addressed in the manuscript.  There were also concerns about the scaling and applicability of the results, as dominance solvable games are limited. \n\n[1] http://www.parisschoolofeconomics.eu/docs/guesnerie-roger/milgromroberts90.pdf\n[2] Friedman, James W., and Claudio Mezzetti. \"Learning in games by random sampling.\" Journal of Economic Theory 98.1 (2001): 55-84."}, "review": {"BkloUdRjjS": {"type": "rebuttal", "replyto": "S1gushOisH", "comment": "Milgrom and Roberts's  Theorem 8 statement says that:\n\"Let {x(t)} be an adaptive dynamic process and let x = inf(S) andX = sup (S). Then for every *supermodular* game T...\"\ni.e. the theorem relates only for supermodular games. \n\nGiven this theorem and a *supermodular game*, one could indeed run through the iterated elimination sequence. However, the theorem only applies for supermodular games (which is again a restricted class of game).\n\nYou've asked where specifically in the proof of their Theorem 8 Milgrom and Roberts use the fact that the game is supermodular. Their proof of theorem 8 applies their Theorem 5 (or rather, their Lemma 1 which is the key element of Theorem 5). In the proof of Theorem 8, in the last transition (top of page 1270), the authors explicitly state the last transition follows from Lemma 1.\n\nTheir Lemma 1 again relates only to supermodular games: note that the Lemma is proved using Theorem 1 and Theorem 2, which require a supermodular function f and a lattice (the definition of a supermodular game requires having these). Note that x \\wedge y and x \\vee y in the definitions of the paper relate to the supermum and infimum of a lattice. Their Lemma 1 leverages the fact that the smallest and largest best response are defined, which requires invoking their Theorem 1 and their Theorem 2, which again relate only to *supermodular* games (the definition of a supermodular game requires having such a lattice structure and supermodular function). \n\nIn short, Theorem 8 applies only to supermodular games and not general games - the condition A6 is used along their Lemma 1 which relates to supermodular games. Indeed, for the *restricted class* of supermodular games Milgrom and Roberts results are sufficient to show that many learning dynamics converge to the iterated elimination sequence. However, our results hold for *general games* that are dominance solvable, which is a much larger class of games (though our proof is only for specific learning dynamics of REINFORCE and IW-MCPI). ", "title": "Theorem 8 relates only to supermodular games."}, "H1x1FlvssB": {"type": "rebuttal", "replyto": "SygfZk8oiH", "comment": "Thanks for reading our response and further discussion!\n\nThe paper you cite focuses on supermodular games, which is a restricted class of games. \n\nAs the authors themselves note (in the beginning of section 3, page 1268):\n\n\"Recently, Fudenberg and Kreps (1988) have investigated limiting behavior in a class of learning models for general extensive form games. ...\nThey conclude that learning may, even in the long-run, yield a larger set of strategies than is identified by Nash equilibrium.\nShapley and Fudenberg-Kreps establish the rather negative conclusion that Nash equilibrium play is not the only possible outcome of learning in general  games. For *supermodular* games, however, sharper and more positive results are possible...\"\n\nIn other words, their additional results relate only to this class of games. Supermodular games are games where the strategies of each player have a lattice structure such that the incentives of one agent to choose a higher strategy (in their lattice) increases as other players switch to higher strategies (on their lattices). See more details here:\nhttps://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-254-game-theory-with-engineering-applications-spring-2010/lecture-notes/MIT6_254S10_lec07.pdf\n\nSupermodular games are an interesting class as they are dominance solvable if and only if they have a unique Nash equilibrium (this is clearly not the case for general games, e.g. Rock-Paper-Scissors has a unique Nash but is clearly not dominance solvable). However, this is a *restricted* class of games (see discussion in the bottom of page 2 here: https://pdfs.semanticscholar.org/e889/fcddbe6bb87ee9a0de6a93ef25fa17f366ee.pdf )\n\nOur results hold for *general games* that are dominance solvable, but only to the specific learning rules we've examined (REINFORCE, IW-MCPI). In contrast, the results you discuss hold for a much wider class of learning rules, but only for the restricted class of supermodular games. \n\nAgain, we hope this helps address your concerns about our results being subsumed by earlier work.   ", "title": "This additional paper relates only to supermodular games, not any arbitrary game. Our results are for general games."}, "r1emt9Q5jr": {"type": "rebuttal", "replyto": "H1luZksD9r", "comment": "Thank you for the helpful comments. \n\nFirst, regarding novelty: we point out in response to Reviewer 1 that the previous results they noted relate to Fictitious Play and Replicator Dynamics, which are fundamentally different from the RL algorithms we consider. Please see the full comments in our response to Reviewer 1. As we point out, our paper investigates very basic RL methods that are the foundation of many popular RL agents, and we are not aware of any existing work that shows convergence of these methods to the iterated dominance solution. We added a section on \u201cComparison of Our Results and Existing Results on the Convergence of Other Algorithms to the Iterated Dominance Solution\u201d. \n\nRegarding scaling to real-world applications: we wholeheartedly agree this is an important issue! Indeed, there are known polynomial algorithms for computing strict iterated dominance solutions (see, e.g. Conitzer and Sandholm, Complexity of iterated dominance, 2005, who also note that things are trickier for *weak* iterated dominance). However, these are based on a normal-form representation of a game. Games with multiple timesteps (extensive form) can be translated into normal form, but with the size growing very quickly in the number of timesteps. This means that for practical applications, we might only be able to show convergence for restricted classes of games / environments. We are adding a discussion of this in the paper, and expanding the discussion on mechanism design, where we may want to *design* a game so as to guarantee it has an iterated dominance solution. \n\nWe will of course fix the typos you noted and extend the captions in the Appendix - much appreciated! \n", "title": "See comments to reviewer 1 regarding novelty. We will discuss scaling to real world applications."}, "SyxVGcXqiH": {"type": "rebuttal", "replyto": "H1x2QH4-cB", "comment": "Thank you for your helpful comments. \n\nWe are condensing the the sections on definitions and preliminaries to get to the point more quickly. \nWe are also fixing the typos and the notation inconsistencies you noted - much appreciated! \n", "title": "We will condense the definitions and preliminaries sections, and fix the typos. Thanks!"}, "ByeQsYX9iS": {"type": "rebuttal", "replyto": "HJeqKrkBcS", "comment": "Thank you for the comments. \n\nFirst, regarding item 3, as we wrote in the original submission (see first paragraph of Section 3 on page 4), our results hold for *both* the \u201cround-robin\u201d setting where one agent learns at a time (which we call the serial mode) and for the case where all agents learn simultaneously (which we call the parallel mode). As we discuss there, our analysis is more elaborate, as it covers the parallel mode as well. In other words, we agree the more realistic setting is the one where all agents learn simultaneously, and our results hold for this more realistic case as well. We\u2019ll emphasize this earlier in the paper. \n\nSecond, as you suggest in item 2 regarding convergence rates, we have added a discussion of the convergence rate in an appendix (see \u201cConvergence Rates for IW-MCPI\"), which we briefly discuss here. \n\nOur convergence result was an asymptotic one, showing that eventually IW-MCPI almost surely converges to an iterated elimination solution. Note that even in a single bandit settings, the number of samples required to discern that one action x yields a better reward than another action y with high probability 1-\\delta depends on the difference in rewards r_x - r_y. Similarly, in our case the rate of convergence depends on the game\u2019s payoffs, with the key factor being the degree to which dominated actions are suboptimal, as captured by the g in the proof our Theorem on IW-MCPI convergence. \n\nIn our proof g denotes to the gap in rewards between a dominated action i and any dominating action j (see Theorem 3.4, and note rewards are normalized to be in [0,1]). Denote the total number of actions across all the players as S = \\sum_i |S_i| (and note this bounds the number of elimination steps). Denote by epsilon the minimal gap between the dominated action and the other actions across all the elimination steps (i.e. in all elimination steps, the difference between the reward of the eliminated action and the reward of other actions is at least epsilon). Then the required number of steps so that IW-MCPI reaches the iterated elimination solution with high probability is: \nO( S / \\epsilon^(2 / (1-p))) \nwhere p is the decay rate of our exploration.\nFor instance, setting p=1/2 we get a convergence time of:\n O(S / epsilon^4). \n\nFinally, regarding 1, as we wrote in the paper, we acknowledge that many games are not dominance solvable and are not covered by our results. We thus emphasized the implications of our work to mechanism design settings, where one can, under some costs or restrictions, design the game as to make it dominance solvable. We will expand the discussion to emphasize the implications of this. \n\nWe hope this addresses your concerns regarding the paper. \n", "title": "We have added an analysis of convergence rates. Also, we DO cover the case of simultaneous updates."}, "rylLgtQ5oS": {"type": "rebuttal", "replyto": "SyxMP-S9tB", "comment": "Thank you for the comments, and for pointing out this related work. \n\nOur paper deals with Monte-Carlo Policy Improvement and REINFORCE (a policy gradient method). In general, convergence results on one type of algorithm may not apply to other types of algorithms. In other words, though different RL algorithms may sometimes converge to the same outcome (if they converge at all), this is certainly not an automatic guarantee. \n\nThe results in Fudenberg and Levine [1] relate to Fictitious Play (FP) and the Replicator Dynamics (RD); as they mention, their results are actually due to Nachbar 1990, see Evolutionary selection dynamics in games: Convergence and limit properties. The work by Bowling [2] mentions Q learning, but merely says that in some cases multi-agent Q learning can find Nash equilibria, such as in fully collaborative games where all agents had identical rewards. It then points out that *other* naive learning methods (i.e. not Q-learning) converge to iterated dominance solutions, and point to the publication by Fudenberg and Levine. In short, these existing results apply to Fictitious Play and Replicator Dynamics, rather than the algorithms we considered, which are different (and again, all of these are different from Q learning), so our results are not covered by this existing work. \n\nFurther, we note that the algorithms covered by this existing work (FP and RD) originate from evolutionary game theory, and were designed to compute equilibria. Thus, they are *fundamentally different* from the RL algorithms we consider. There exist good algorithms for computing iterated dominance solutions (see, e.g. Conitzer and Sandholm, Complexity of iterated dominance, 2005), but our motivation is not to compute the iterated dominance solution, but rather to study how commonly used RL algorithms behave in dominance solvable games. We focused on policy gradient methods as these lie at the heart of many popular agents, and on policy iteration as it is among the simplest and most basic methods. \n\nFP does not rely on computing gradients, but rather on repeatedly finding the best response to the empirical distribution of actions taken by the opponents so far. Similarly, RD remains a different dynamical system from policy gradient methods; RD is a *regret minimizing* approach which is known to have significant differences with policy gradient methods (see, e.g. Neural Replicator Dynamics, Omidshafiei et al. and Cycles in Adversarial Regularized Learning, Mertikopoulos, Papadimitriou and Piliouras). We are not aware of any results on the RL methods we study in this paper. \n\nIn short, the results you mention deal with very different algorithms from those we consider (which are the ones relating to frequently used RL agents). We thus believe our results are novel and valuable to the RL community. \n\nBased on your comments, we are adding a section on \u201cComparison of Our Results and Existing Results on the Convergence of Other Algorithms to the Iterated Dominance Solution\u201d, discussing how our results differ from the papers you discussed (and other work). \n\nFinally, you pointed out that you expect any \u201creasonable\u201d learning rule would converge on this iterated dominance solution. To emphasize why the analysis is tricky, we have added an appendix discussing convergence issues with REINFORCE, which we briefly discuss here (see the new appendix \u201cConvergence Issues for REINFORCE with More Than Two Actions\u201d for full details). \n\nConsider the case of player 1 having 3 actions, where the reward from these actions depends on the action choice of player 2, with the rewards being r^b1 = (0, 0.5, 1) (respectively for the player 1\u2019s actions) when player 2 takes the first action or being r^b2 = (0, 0.5, 0.1) when player 2 takes the other action. In other words, in this case the first action is always dominated for the player 1, but depending on player 2\u2019s action, the optimal action for player 1 may be either its second or third action. \n\nPerforming the REINFORCE update for player 1 always reduces the logit of the first action. However, for some action logits, the update on r^a increases the logit of action 3 and decreases that of action 2, where *the decrease in the logit of action 2 is larger than the decrease in the logit of the dominated action 1*. Similarly, the update on r^b increases the logit of action 2 and decreases that of action 3, where *the decrease in the logit of action 3 is larger than the decrease in the logit of the dominated action 1*.\n\nWhen player 2 constantly switches between the two actions, we oscillate between the first and second updates. This shows that the REINFORCE update does *not* guarantee that the logits of the dominated action decreases relative to that of the non-dominated actions, highlighting why the analysis is tricky.   \n\nWe hope the discussion above addresses your concerns regarding the novelty and technical contribution of the paper. \n", "title": "The work you mention relates to Fictitious Play and Replicator Dynamics which are very different from the RL algorithms we consider. Our results are not subsumed by existing work."}, "SyxMP-S9tB": {"type": "review", "replyto": "ryl1r1BYDS", "review": "This work studies learning under independent MARL, and shows theoretically and experimentally that two independent MARL algorithms converge for games that can be solved by iterated dominance.\n\nThis work is clear and well-written, but I do not understand what the contribution of this work is to the literature. The fact that standard MARL learning rules (e.g. independent Q learning) converge in games with iterated dominance solutions is a very well-known result in Learning in Games (see [1], [2]). The authors examined slightly different learning rules (REINFORCE and MCPI), but I would expect that almost any reasonable learning rule would converge in iterated-dominance-solvable games; if anything, it would be surprising if this were *not* the case. The applications of the convergence result result to \"noisy effort\" games is pretty standard and the results expected based on the theory.\n\nQuestion to the authors:\n- How does this work differ from the known results about convergence of naive learners in iterated-dominance-solvable games?\n\n[1] Michael Bowling, \"Convergence Problems of General-Sum Multiagent Reinforcement Learning\", Sec. 5.2\n[2] Fudenberg & Levine, 1999", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 3}, "H1x2QH4-cB": {"type": "review", "replyto": "ryl1r1BYDS", "review": "The main idea of this paper is to solve multi-agent reinforcement learning problem in dominance solvable games. The paper reviewed general multi-agent reinforcement learning and general norm-form game in game theory. The authors aim to recover multi-agent policies through independent MARL in norm-form dominance-solvable games. The paper states that one of solution concepts of dominance-solvable games is iterated dominance solution, which is different from Nash Equilibrium and may be more suitable under certain scenarios. Furthermore, the paper considers two common RL methods for control and learning policy: REINFORCE and Monte-Carlo policy iteration. The main contribution of the paper is to prove that both REINFORCE in binary action case and Monte-Carlo algorithms find the agents\u2019 policies converging to the iterated dominance solution. The interesting aspect of this paper is that iterated dominance solution based reward scheme can guarantee convergence to the desired agents policies at a cheaper cost in practical principal-agent problems. In appendix, the paper extended its conclusion to Markov games and three possible action cases. To the current status of the paper, I have a few concerns below.\n\n1.\tIt takes too much space for preliminary work and basic concepts, in Sec 1.1 (preliminary) and Sec 2 (MA-RL and Dominance-Solvable Games).\n2.\tThe notations are inconsistent and unnecessarily complicated. For example, for agent i \u201cits possible actions are the strategies in S_i\u201d (section 2); any action \u201ca \\in S_i\u201d (section 2,1); for agent i \u201cfor all s_i \\in S_l\u201d (Algorithm 1 line 2). It can be consistent to use the same notation to describe the same term. Moreover, \u201ca score per action, x_1, \u2026, x_{m_i}\u201d and \u201ceach agent starts with initial logits for x_1, \u2026, x_n\u201d. Formally, the corner mark in the same location should represent the uniform meaning. \n3.\tTypos: lemma 3.1 proof \u201cg = \u2026 (r_{s_h}-r_{s_h})\u201d should be \u201cg = \u2026 (r_{s_h}-r_{s_l})\u201d; above section 3.2 \u201cour proof of Theorem 3.1\u201d, should be \u201cLemma 3.1\u201d.\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 4}, "HJeqKrkBcS": {"type": "review", "replyto": "ryl1r1BYDS", "review": "This paper studies independent multi-agent reinforcement learning (MARL) in dominance solvable games. The main contribution of this paper is that the authors have proved the convergence to the iterated dominance solution for two RL algorithms: REINFORCE (Section 3.1, binary action case only) and Importance Weighted Monte-Carlo Policy Improvement (IW-MCPI, Section 3.2). Empirical analysis for principal-agent games is demonstrated in Section 4.\n\nThe paper is interesting in general, however, I do not think this paper has quite met the (very high) standard of ICLR, due to the following limitations:\n\n1) As the authors have mentioned, the dominance solvable games are quite limited.\n\n2) This paper only has *convergence* results, but does not have *convergence rate* results. In other words, the authors have not proved how fast the agents converge to the iterated dominance solution. Might the authors establish a convergence rate result such as a regret bound?\n\n3) This paper assumes an unrealistic setting in which when one agent learns, the strategies (policies) of all the other agents are fixed. In other words, the agents learn in a round-robin fashion, rather than learn simultaneously. I do not think this setting is realistic in most practical problems.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}, "H1luZksD9r": {"type": "review", "replyto": "ryl1r1BYDS", "review": "This paper studies reinforcement learning algorithms in a specific subset of multi-agent environments that are 'dominance solvable'. This means that, given an initial set of strategies in the game, if we iteratively remove 'dominated strategies' (those whose utility is strictly less than another strategy independent of the strategies used by other agents), then only one strategy remains for each player. The remaining strategy is called the iterated dominance solution. The paper proves the convergence of certain RL algorithms (REINFORCE in the 2-action case, and importance weighted monte-carlo policy iteration in the multi-action case) for normal-form games. The paper demonstrates the utility of this via mechanism design: in a principal-agent problem where one can design the rewarding scheme given by a 'principal agent' to various (RL) sub-agents, rewarding schemes motivated by iterated dominance guarantees the best solution for the principal agent, whereas schemes motivated by Nash equilibria do not. \n\nThe paper is quite well-written and understandable. To my knowledge, the idea is novel and has not yet been explored in the RL literature (UPDATE: based on Reviewer #1's review, this may not be the case. I'll wait to hear the author response to this). I did not check the proofs thoroughly. However, the experiments in the principal-agent problem make sense, and it's interesting to see that iterated dominance reward schemes results in good performance for the principal agent. I appreciate that, while the main results in the paper are limited to normal-form games (which are quite restricted), there are empirical results in the appendix showing the extension to Markov games with multiple timesteps, suggesting that the applicability of iterated dominance reward schemes extend beyond the simple two-action case, where no temporally extended decisions need to be made. Even so, the Markov game considered is fairly simplistic. \n\nMy personal curiosity about this paper revolves around scaling to real-world applications. This is not really discussed in the paper; the conclusion talks about directions for future work, for example expanding the number of RL algorithms where convergence can be proven, or producing complexity bounds for convergence. What I want to know is: what sorts of games can we compute the iterated dominance reward schemes for? How can this be applied when the space of policies becomes too large to be enumerated (and thus determining whether a policy is strictly dominated becomes impossible)? I don't expect this paper to solve these issues, but it would be nice to have a discussion of them. \n\nOverall, I'd say this paper is interesting to the multi-agent RL community and I could imagine others building off of this work, so I err on the side of acceptance. \n\n\nSmall fixes:\n- Our proof of Theorem 3.1 -> Theorem 3.2\n- I'd recommend extending the captions of figures 6-8 and 9-11 in the Appendix. \n- Close bracket in Section 6.3 title", "title": "Official Blind Review #4", "rating": "6: Weak Accept", "confidence": 2}}}