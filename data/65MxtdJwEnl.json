{"paper": {"title": "Neural CDEs for Long Time Series via the Log-ODE Method", "authors": ["James Morrill", "Patrick Kidger", "Cristopher Salvi", "James Foster", "Terry Lyons"], "authorids": ["morrill@maths.ox.ac.uk", "~Patrick_Kidger1", "salvi@maths.ox.ac.uk", "foster@maths.ox.ac.uk", "tlyons@maths.ox.ac.uk"], "summary": "We process very long (17k) time series by using a neural CDE with a numerical solver that (a) steps over multiple data points at once, (b) may be interpreted as a binning technique, (c) represents a length/channel tradeoff.", "abstract": "Neural Controlled Differential Equations (Neural CDEs) are the continuous-time analogue of an RNN, just as Neural ODEs are analogous to ResNets. However just like RNNs, training Neural CDEs can be difficult for long time series. Here, we propose to apply a technique drawn from stochastic analysis, namely the log-ODE method. Instead of using the original input sequence, our procedure summarises the information over local time intervals via the log-signature map, and uses the resulting shorter stream of log-signatures as the new input. This represents a length/channel trade-off. In doing so we demonstrate efficacy on problems of length up to 17k observations and observe significant training speed-ups, improvements in model performance, and reduced memory requirements compared to the existing algorithm.", "keywords": ["CDE", "neural differential equation", "time series", "long time series", "log-ODE"]}, "meta": {"decision": "Reject", "comment": "This paper presents a method for improving the learning of neural controlled differential equation (CDE) models. Neural CDE models provide a number of advantages over neural ODE models in terms of their ability to incorporate continuous-time observations. The primary strength of this paper is that it proposes a mathematically rigorous approach to enable neural CDE models to be learned more efficiently from long time series by converting the CDE to an ODE via the log-ODE method. The results are promising in that the method is able to simultaneously improve accuracy, reduce running time and reduce memory required during learning. \n\nThe paper has two main weaknesses. First, the authors claim that due to the problems they are solving (time series with up to 17,000 steps), there are no viable baselines outside of the family of methods that they are proposing. As was noted in the reviews, it would be advisable to consider even very basic baselines for these experiments in addition to current benchmark results. For example, the EigenWorms data set was used in the time series classification benchmark described in Bagnall et al. and there are benchmark results available that appear to outperform those shown in Table 2 (see mean test accuracy results reported here: http://www.timeseriesclassification.com/results/AllAccuracies.zip). The authors are also encouraged to consider even coarse RNN approximations such as partitioning the time series into tractable blocks for learning. It is not clear that the data sets actually have long-range dependencies despite being long. \n\nThe second weakness is that the representation that underlies the log-ODE method (the log-signature transform) has been used in previous work in conjunction with discrete-time RNNs. It can be viewed as a preprocessing method in a sense, as was noted by a reviewer. However, it is much more fundamentally integrated with methods for solving CDE's than its prior application to RNNs indicates. \n\nOverall,  support for the paper did not rise to the bar required for acceptance, but we encourage the authors to revise and re-submit the work to a future venue. \n"}, "review": {"Gqp3DXjacpS": {"type": "review", "replyto": "65MxtdJwEnl", "review": "Summarizing the paper claims\n------------------------------------------\nThe paper introduces an approach that allows training Neural Controlled Differential Equations (CDEs) for long time series. In contrast to the Neural ODE that is determined to its initial condition, Neural CDE produces a trajectory dependent on time-varying data. The authors propose to use log-signature as input instead of the original time series. Log-signature can be understood as a lossy representation for time series, which has a much smaller length and varies slower over the same time interval. Hence, larger steps may be used in the numerical solver, and that yields to the training speed-up.\n\nIn a little more detail, the log-signature is constructed as a sequence of statistics computed using the original time series. It is characterized by the depth N, where N states for the maximal length of paths used to calculate statistics. By applying log-signature transform, the solution of the CDE may be approximated by the solution of the ODE.\n\nStrong points\n-------------------\n- The paper is clearly written.\n- The experiments are conducted for four real-wold problems (worms classification, predicting a person's heart rate/respiratory rate/oxygen saturation). \n- The authors provide an ablation study for the (log-signature length) / (number of channels in time series) trade-off,  investigate an influence of log-signature depth and solver's number of steps to the performance. \n- The paper states the limitations of the proposed method.\n\nWeak points\n-----------------\nThe paper provides an overview of many RNN based models used for long time series; however, it doesn't compare with any of them. That would be interesting to see how good is a proposed method comparing to the RNN based one in terms of test accuracy/training time/memory usage.\n\nRecommendation (accept or reject)\n------------------------------------------------\nI recommend accepting the paper. The paper provides a detailed theoretical formulation and demonstrates a significant training time speed-up for various real-world problems.\n\nUpdate: The authors thoroughly addressed all the questions, the experiments demonstrate an improvement, the theory coincides with experiments. From my perspective, that would be useful for the neural ODE community to know more about the proposed log-signature-based technique. I increase the score. \n\nQuestions\n--------------\n- Which type of ODE solvers has been used for the experiments? Does the solver's order influence the neural CDE performance?\n- What order is the value of  $\\beta(v, N)$ in the experiments?\n- In section 4.1, it is written that the naive subsampling achieves speed-up without performance improvements. Could you provide time and accuracy for these experiments?\n-  What architecture is used for $\\hat{f}$ neural network? How the choice of architecture affects performance?\n- The standard deviation in Table 1 has quite large values. Haven't you tried to tune training hyperparameters to reduce it?\n- What is the stopping criterion for the training? I'm curious why in Table 1, for the same number of solver's steps, the training time for $NCDE_2$ is longer than for  $NCDE_3$? (It seems that for the same number of training epochs, the time should be shorter because we do less preprocessing computations for log-transformation) \n\n", "title": "The training time of neural CDEs is reduced by replacing the original input time series with corresponding log-signature.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "b1e5JLCcY-d": {"type": "review", "replyto": "65MxtdJwEnl", "review": "**Summary.** The authors describe how to apply a log signature to temporal datasets. This operation reduces dimensionality along the time axis at the price of adding some dimensionality to the spatial dimension. Then they train a neural controlled differential equation (Neural CDE) on the transformed dataset and show that their model learns more quickly and achieves better test generalization. They report results on two real-world datasets (EigenWorms and the TSR vitals dataset).\n\n**Strong points.** This paper is technically sound and the method shows clear improvement over \u201cno preprocessing.\u201d\n\n**Weak points.** The authors are proposing a simple method of reparameterizing time series data so that information is transferred from the time dimension to the space dimension. Strangely, the title \u201cNeural CDEs for Long Time Series via the Log-ODE Method\u201d implies that they are proposing a new model. They are not. Rather, they are proposing a data preprocessing technique that they apply to a dataset _before_ they train an ML model on it. It\u2019s also worth noting that this feature engineering technique is already being used on time series data (eg Liao, 2019, \u201cLearning Stochastic\u2026\u201d). The only difference is that here the authors are using a continuous-time analogue of an RNN. But since this feature engineering trick is applied to the dataset independently of what model is used, I\u2019m not sure what new scientific insights are to be gained. This work seems very similar to Liao et al (2019); compare Figure 1 in the two papers, for example. To the authors: am I missing a critical new contribution? In what ways is the \u201clog signature\u201d feature engineering trick significantly different in the context of CDEs compared to RNNs?\n\nAnother issue with this paper is that it presents the theory in an unnecessarily mathematical manner. Some math is useful. But whenever the authors introduce terms that an ML audience is not familiar with, they should offer a sentence of intuition regarding what that term means. From my notes on this paper, here are phrases that I found confusing and were not accompanied by any explanation or intuition: \u201cRiemann\u2013Stieltjes integral,\u201d \u201cn-fold iterated integral,\u201d \u201ctree-like equivalence,\u201d \u201cHolder continuous paths,\u201d \u201cMobius function,\u201d \u201cLie brackets whose foliage is a Lindon word,\u201d and \u201cMagnus expansion.\u201d Again, it\u2019s ok to introduce these concepts, but 1) you should give some reasonable intuition for what they are and why they are relevant and 2) they should be directly relevant to the main contributions of the paper.\n\nFor other reviewers/readers who want an intuitive introduction to log signatures, I thought that [this README](https://github.com/kormilitzin/the-signature-method-in-machine-learning) did a very nice job. To the authors: are there ways that we can simplify the theory section so that it remains technically correct while also being readable and accessible? Are there specific reasons that you chose to introduce the \"log signature\u201d operation in the way you did?\n\nThe experiments are technically sound and the results are presented well. However, it is surprising that the authors did not compare to the RNN/RNN model that they mentioned in the second paragraph of related work. That would seem to be the natural baseline; using the non-preprocessed dataset is a rather trivial baseline, as the performance of recurrent models degrades quite seriously across such long sequences.\n\n**Recommendation.** 4: Ok but not good enough - rejection\n\n**Reasoning.** The Log Signature method has already been shown to be a viable preprocessing technique for time series data. The main contribution of this paper is to show that it also works with CDEs. Since the log signature method is model-independent, it does not seem surprising that this is the case. The experimental results are technically sound but would be improved by adding a stronger baseline. It would make more sense to compare to the RNN/RNN model described in the Related Work section, or to another method of data preprocessing that reduces dimensionality along the time axis.\n\n**To improve the paper.** When using a mathematical concept that will be new to an ML audience, give a one-sentence intuition for what it does and how it is relevant. Try and give a simple, intuitive example of how one might apply the log signature to a short, example time series. Make sure the reader is able to quickly grasp how a log signature of, say, order 2, works in practice. You might consider transitioning to a deeper theoretical treatment after doing so, or possibly refer the reader to a relevant tutorial such as (Chevyrev and Kormilitzina, 2016).\n\nOne way to improve the experimental results would be to compare to a stronger baseline such as RNN/RNN. If you don't want to compare to RNN/RNN, you could even just compare to a simple tensor reshape operation that folds the time dimension into the space dimension (eg., given a dataset with axes [\"num_examples\",\"time\",\"space] = [a,b * c,d], reshape to [a,b,c * d]). There are probably more clever and effective approaches than that. But the point is that, instead of comparing your dataset preprocessing method to \"no dataset preprocessing at all\", try comparing it to \"another dataset preprocessing method\" that does something similar. In this case, that method would involve transferring some of the dataset dimensionality from time to space).", "title": "Review of the \"CDEs & the Log-ODE Method\" for ICLR 2021", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "_TyR-_W1cMj": {"type": "rebuttal", "replyto": "8WfgwtPHPWl", "comment": "Thank you for your thoughtful response.\n\n**Model independence**  \nThis sounds like a small matter of phrasing. Instead of \"The use of log-signatures is not model-independent\", would the reviewer accept \"The *benefit* of log-signatures is not model-independent\"?\n\nWe have now adjusted the paper to try and make this distinction clear. (Certainly we do not wish to be misleading.)\n\nWhen used as pre-processing for e.g. an RNN model, then the endpoints of the log-signature are fixed. In contrast, we offer a model for which (in the notation of the paper) one can calculate $Z_b$ given $Z_a$ and $X|_{[a, b]}$ *for any choice of $a, b$.* This is a property of being a true ODE model, and not simple pre-processing.\n\nIt is important to note that the alternative suggested title suggested is not accurate. General use of log-signatures is *not* the \"log-ODE method\". This terms refers specifically to their application, in the manner discussed in the paper, in conjunction with a differential equation.\n\n**Use of theory**  \nIt is very common for sections of papers to be optional. How often does one skip the details of experiments when reading a paper? It is not that the section \"has no bearing on the rest of the paper\", it is that it *enhances* the rest of the paper without being a dependency.\n\nIn any case, this is a difference in presentational style. We hope that we can agree to disagree, and respectfully ask that the reviewer not seek to prevent publication on these grounds.\n\n**Comparison to reshaping**  \nIt is not that it does not fit on the GPU -- but the number of parameters is really very different. Using the EigenWorms dataset as an example, reshaping the channels according to the smallest step size considered (a factor of 8) increases the parameter count from 34629 to 186725; a multiplier of 539%. Even if subsampling is additionally combined with reshaping, so as to only double the number of channels, then the parameter count is increased to 56357; a multiplier of 163%.\n\nWhen writing the paper, we (the authors) had a discussion on possible baselines, and concluded that there really were very few sensible options available.", "title": "Response"}, "w79Veac2Xyw": {"type": "review", "replyto": "65MxtdJwEnl", "review": "### **Summary and Contributions of Paper**\nThis paper proposes a new method for computing Neural CDEs via the signature transform, which transforms a path integral into log signatures, i.e. a collection of iterated integrals. Then standard ODE tools are applied to each piecewise log signature.\n\n### **Strengths**\n- The writing quality is rigorous.\n- The approach seems motivated and based on a clever mathematical trick via the signature transform.\n- Experiments are convincing and sound.\n- Appendix provides proof of the approximation properties of the clipped-term signature transform (which originally requires infinite basis for exact approximation)\n\n### **Weaknesses**\n- The signature transform seems somewhat esoteric and nonstandard for readers without specific knowledge in this field. It would be very good if the authors could give more intuitive/pictorial views of this transform (I needed to read online surveys multiple times to understand the intuition behind this). For instance, I read this in detail: https://arxiv.org/pdf/1905.08494.pdf (NeurIPS 2019), which provides a much cleaner explanation of the signature transform, but also demonstrates that an entire paper is needed to simply explain the method. \n- While the authors claim that there is an ease of implementation via pre-existing tools, the larger bottleneck seems to be actually understanding the method itself (which seems to also be a function of how the paper treats this material). While I have no doubt that this work would be great for a very mathematically minded community, I am unsure of its merits for the ICLR conference community. I think the authors should provide more high level overview of the signature transform, and keep the strict math in the appendix.\n\nI am not an expert on these types of methods, so my confidence will not be as high, but I believe that this paper contributes via its insight with the signature transform, and thus my rating is marginally above the acceptance threshold.\n\nIf the authors could perhaps make the work more approachable, I would be happy to raise my score.\n", "title": "Seems novel and motivated, but should be more approachable.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Iq9esF5FE4": {"type": "rebuttal", "replyto": "b1e5JLCcY-d", "comment": "Thank you for your review.\n\n**Novelty**  \nThe use of log-signatures is not model-independent. Log-signatures specifically extract information describing how the path drives a controlled differential equation. This has certainly been a motivation for its use in previous work (most notably Liao et al. 2019, who consider RNNs), but it is only with the advent of neural CDE models that this can be taken to its logical conclusion, and actually synergised with a true differential equation model.\n\nIn doing so one actually obtains an implementation of the log-ODE method -- rather than just an inspired choice of pre-processing. *This preserves the existing differential equation structure.*\n\nMoreover, previous application of similar ideas have been introduced as general-purpose techniques. Unfortunately, this does not always pan out in practice: performance improvements are often moderate at best, if at all. What previous work has failed to identify, and which we emphasise here, is the importance of application to long time series in particular. Such problems necessarily involve sacrifices which the log-ODE method is well-placed to capitalise upon.\n\nThus the difference to previous work is we believe significant; we appreciate that this is not completely clear and have added additional commentary to rectify this.\n\n **Use of theory**  \nWe have aimed to simplify the theory even further; in particular removing the comparisons to non-ML literature. In detail:\n\n- The term \"Riemann--Stieltjes integrals\" has been removed. The meaning of the integral is left unambiguous as the path is differentiable;\n- The term \"n-fold iterated integral\" has been removed as being just terminology;\n- The term \"tree-like equivalance\" has been replaced with a similar statement on translation invariance;\n- Holder continuity has been replaced with Lipschitz continuity (which is weaker but better-known);\n- The definition of $\\beta(v, n)$ (which uses the Mobius function) has been moved to the appendix;\n- The discussion on Lie brackets has been removed, as a more minor point;\n- The comparison to the Magnus expansion has been removed.\n\nWe completely recognise the difficulty of these concepts if the theory is unfamiliar. It is precisely because of this that the start of Section 2 (\"Theory\") begins with *\"We begin with motivating theory, though we note that this section is not essential for using the method. Readers more interested in practical applications should feel free to skip to section 3\",* and the rest of the paper is careful to be independent of this section. We think this represents a best possible presentation when there is a certain minimum theoretical sophistication that is required, and have now added an additional sentence to help emphasise this.\n\nBeyond even this, we have now used the additional page to introduce further intuition behind these ideas, in particular with respect to the geometry of the input, and the relationship to CDEs. See Section 2.1.\n\n **Comparison to an RNN Baseline**  \nOn a serious practical note, RNN-based models do not fit in the memory of the GPU resources we have available. This is one of the main advantages of using differential equation models in the first place, for which adjoint backpropagation is available. (As per the first paragraph of Section 4.)\n\nIndeed one reason we did not compare to an RNN baseline is because they are not ODE (CDE) models. Such methods offer continuous time modelling, adjoint backpropagation, and the ability to easily handle irregular and partially observed time series -- for which this paper represents a demonstration that another kind of difficult data, namely long time series, can also be added to that list. We anticipate that this model will have most utility when one or more of these properties is desired.\n\nWe considered reshaping the data as the reviewer suggested, however:\n- Doing so results in incomparable models: the models on reshaped data have dramatically more parameters, due to the cubic scaling of Neural CDE parameter counts. (See section 6.3 of the original Neural CDE paper.) Avoiding this kind of explosion is precisely the purpose of log-signatures as summary statistics.\n- Moreover this typically only makes things worse. The interpolated paths still have roughly the same complexity as before halving the number of data samples -- but now there are two of them. Unlike RNNs, CDEs decouple the number of integration steps from the length of the time series. (A property generally in their favour when considering smoothly-varying data.)\n\nWe have added additional commentary to the experimental section to justify our choice.\n\n**Summary**  \nWe hope we have addressed every concern that the reviewer has raised. We would be very happy to have further discussion if there are any other obstacles to raising the review score.", "title": "Response"}, "oSoKyXkkRTq": {"type": "rebuttal", "replyto": "w79Veac2Xyw", "comment": "Thank you for your review.\n\n**On approachability**    \nWe have worked to simplify the presentation so as to make the work more approachable.\n\nMost importantly, we have used the additional space to introduce further intuition of these ideas. In particular we have:\n\n- added an introduction to the signature and log-signature section, giving a high-level overview before giving their mathematical definition;\n- added pictorial descriptions giving a geometric insight into the (log-)signature transform;\n- demonstrated with a Taylor expansion how signature terms naturally arise in the solution of CDEs\n\nAdditionally:\n\n- We have replaced the definition of the signature transform with the simpler-to-understand definition previously used in the \"Deep Signature Transforms\" (NeurIPS 2019) paper, that the reviewer highlights;\n- The definition of $\\beta(d, n)$ (the size of the logsignature) has been moved to the appendix;\n- Where possible, mathematical terms (in particular \"Riemann--Stieltjes integrals\"; \"n-fold iterated integral\";  \"Lie brackets\"; \"Magnus expansion\") have been removed, when the discussion could be rephrased without them;\n- The term \"tree-like equivalance\" has been replaced with a similar statement on translation invariance;\n- Holder continuity has been replaced with Lipschitz continuity (which is weaker but better-known).\n\nWe completely recognise the difficulty of these concepts if the theory is unfamiliar. It is precisely because of this that the start of Section 2 (\"Theory\") begins with *\"We begin with motivating theory, though we note that this section is not essential for using the method. Readers more interested in practical applications should feel free to skip to section 3\",* and the rest of the paper is careful to be independent of this section. We think this represents the best possible compromise between readability and technicality, and have now added an additional sentence to help emphasise this.\n\n**On choice of venue**  \nWe have submitted to ICLR because our paper is about improving an already-existing machine learning technique.\n\n**Summary**  \nWe hope that these improvements are sufficient to address your concerns for an improved score. Please let us know if you have any additional questions or feedback, and we will be happy to address these as well.", "title": "Response"}, "0SUi0vCy5Hh": {"type": "rebuttal", "replyto": "Gqp3DXjacpS", "comment": "Thank you for your review.\n\n **Comparison to an RNN Baseline**  \nRNN-based models do not fit in the memory of the GPU resources we have available. This is one of the main advantages of using differential equation models in the first place, for which adjoint backpropagation is available. (As per the first paragraph of Section 4.) Indeed, our focus is on ODE (CDE) modelling, for continuous time modelling, adjoint backpropagation, or the ability to easily handle irregular and partially observed time series. (In this case, adjoint backpropagation.)\n\nWe have added additional commentary to the experimental section to help make this clear.\n\n **Responses to Questions**  \n\n- The solver used is a fourth-order Runge-Kutta with 3/8 rule (\"rk4\"). This is specified in the appendix. We did also investigate Euler and midpoint solvers, but found that they gave typically worse performance.\n- The order of $\\beta(v, N)$ is to the best of our knowledge an open question! We have added a remark to this effect.\n- Naive subsampling is already included. This corresponds to an NCDE depth 1 model with step > 1 (top rows of results), since this is just the NCDE updated at t=t_0, t=t_0+step, t=t_0+2step, ... . We have added additional commentary to make this clear.\n- The architecture of $\\widehat{f}$ is specified in the appendix, as a feedforward neural network with number of hidden layers in {1, 2, 3} and ReLU activation functions. The sizes of the network were optimised as hyperparameters. \n- On that note, we did indeed perform hyperparameter searching. The standard deviations reported are we then got.\n- Thank you for catching this - there was indeed an early stopping criterion if the training loss failed to improve over 60 epochs, which explains this difference. You are correct in assuming over the same number of training epochs the training time should be longer. We have added an additional comment explaining this.\n\n **Summary**  \nWe hope this addresses all of the reviewer's concerns. If the reviewer has any further questions by which our paper and their score may be improved, then we would be happy to address these as well.", "title": "Response"}}}