{"paper": {"title": "Unsupervised Learning Using Generative Adversarial Training And Clustering", "authors": ["Vittal Premachandran", "Alan L. Yuille"], "authorids": ["vittalp@jhu.edu", "ayuille1@jhu.edu"], "summary": "", "abstract": "In this paper, we propose an unsupervised learning approach that makes use of two components; a deep hierarchical feature extractor, and a more traditional clustering algorithm. We train the feature extractor in a purely unsupervised manner using generative adversarial training and, in the process, study the strengths of learning using a generative model as an adversary. We also show that adversarial training as done in Generative Adversarial Networks (GANs) is not sufficient to automatically group data into categorical clusters. Instead, we use a more traditional grouping algorithm, k-means\t clustering, to cluster the features learned using adversarial training. We experiment on three well-known datasets, CIFAR-10, CIFAR-100 and STL-10. The experiments show that the proposed approach performs similarly to supervised learning approaches, and, might even be better in situations with small amounts of labeled training data and large amounts of unlabeled data.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of significant novelty, potential problems with GAN training) and unanimously recommend rejection. I do not see a reason to overturn their recommendation."}, "review": {"rkFhgYG7g": {"type": "review", "replyto": "SJ8BZTjeg", "review": "What is the key novelty of this paper given the InfoGAN paper? In section 3.2, the paper seems to propose a novel \u201chybrid\u201d approach. However, there seems not much technically novel based on the descriptions in the paper.\n \nThe number of semantic categories does not necessarily equal to \u201ck\u201d. In this sense, there is no guarantee that the performance is optimal when we set the number of semantic categories equal to \u201ck\u201d. Any comments on this?\n\nIn Figure 2, the samples in each row look quite similar. It\u2019s questionable if the model is well-trained.\nThe papers investigates the task of unsupervised learning with deep features via k-means clustering. The entire pipeline can be decomposed into two steps: (1) unsupervised feature learning based on GAN framework and (2) k-means clustering using learned deep network features. Following the GAN framework and its extension InfoGAN, the first step is to train a pair of discriminator network and generator network from scratch using min-max objective. Then, it applies k-means clustering on the top layer features from discriminator network. For evaluation, the proposed unsupervised feature learning approach is compared against traditional hand-crafted features such as HOG and supervised method on three benchmark datasets. Normalized Mutual Information (NMI) and Adjusted RAND Index (ARI) have been used as the evaluation metrics for experimental comparison. Although the proposed method may be potentially useful in practice (if refined further), I find the method lacks novelty, and the experimental results are not significant enough.\n", "title": "questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SynBgHuNx": {"type": "review", "replyto": "SJ8BZTjeg", "review": "What is the key novelty of this paper given the InfoGAN paper? In section 3.2, the paper seems to propose a novel \u201chybrid\u201d approach. However, there seems not much technically novel based on the descriptions in the paper.\n \nThe number of semantic categories does not necessarily equal to \u201ck\u201d. In this sense, there is no guarantee that the performance is optimal when we set the number of semantic categories equal to \u201ck\u201d. Any comments on this?\n\nIn Figure 2, the samples in each row look quite similar. It\u2019s questionable if the model is well-trained.\nThe papers investigates the task of unsupervised learning with deep features via k-means clustering. The entire pipeline can be decomposed into two steps: (1) unsupervised feature learning based on GAN framework and (2) k-means clustering using learned deep network features. Following the GAN framework and its extension InfoGAN, the first step is to train a pair of discriminator network and generator network from scratch using min-max objective. Then, it applies k-means clustering on the top layer features from discriminator network. For evaluation, the proposed unsupervised feature learning approach is compared against traditional hand-crafted features such as HOG and supervised method on three benchmark datasets. Normalized Mutual Information (NMI) and Adjusted RAND Index (ARI) have been used as the evaluation metrics for experimental comparison. Although the proposed method may be potentially useful in practice (if refined further), I find the method lacks novelty, and the experimental results are not significant enough.\n", "title": "questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkO-NACzl": {"type": "review", "replyto": "SJ8BZTjeg", "review": "Hello,\n\nA couple of questions:\n\n1. Looks like in Figure 2 every \"class\" consists essentially of a single image and its slight variations? Doesn't this mean GAN training failed? Do all your GANs produce samples of this quality? \n\n2. Could you please report classification accuracies? Otherwise it is impossible to compare your method to most other works.\n\n3. Why have you not compared to existing well-performing unsupervised learning methods? For example, feature learning with k-means (Coates and Ng 2011), sparse coding methods such as Hierarchical Matching Pursuit (Bo et al., 2012 and 2013), Exemplar-CNN (Dosovitskiy et al. 2014) .\n\n4. Why do you not show results with visual features on STL-10?\n\n5. Supervisedly learned filters in Figure 3 looks unusual to me, they are not normally that smooth. Have you optimized the hyperparameters? What is the resulting accuracy?\n\n6. Could you relate your method to Categorical GAN https://arxiv.org/pdf/1511.06390.pdf ?\n\nThanks!The paper proposes an approach to unsupervised learning based on generative adversarial networks (GANs) and clustering. The general topic of unsupervised learning is important, and the proposed approach makes some sense, but experimental evaluation is very weak and does not allow to judge if the proposed method is competitive with existing alternatives. Therefore the paper cannot be published in its current form. \n\nMore detailed remarks (many of these are copies of my pre-review questions the authors have not responded to):\n\n1) Realted work overview looks incomplete. There has been work on combining clustering with deep learning, for example [1] or [2] look very related. A long list of potentially related papers can be found here: https://amundtveit.com/2016/12/02/deep-learning-for-clustering/ . From the GAN side, for example [3] looks related. I would like the authors to comment on relation of their approach to existing work, if possible compare with existing approaches, and if not possible - explain why.\n\n[1] Xie et al., \"Unsupervised Deep Embedding for Clustering Analysis\", ICML 2016 http://jmlr.org/proceedings/papers/v48/xieb16.pdf\n[2] Yang et al., \"Joint Unsupervised Learning of Deep Representations and Image Clusters\", CVPR 2016 http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Joint_Unsupervised_Learning_CVPR_2016_paper.pdf\n[3] J.T. Springenberg, \"Unsupervised and semi-supervised learning with categorical generative adversarial networks\", ICLR 2016, https://arxiv.org/pdf/1511.06390v2.pdf\n\n2) The authors do not report classification accuracies, which makes it very difficult to compare their results with existing work. Classification accuracies should be reported. They may not be a perfect measure of feature quality, but reporting them in addition to ARI and NMI would not hurt.\n\n3) The authors have not compared their approach to existing unsupervised feature learning approaches, for example feature learning with k-means (Coates and Ng 2011), sparse coding methods such as Hierarchical Matching Pursuit (Bo et al., 2012 and 2013), Exemplar-CNN (Dosovitskiy et al. 2014)\n\n4) Looks like in Figure 2 every \"class\" consists essentially of a single image and its slight variations? Doesn't this mean GAN training failed? Do all your GANs produce samples of this quality? \n\n5) Why do you not show results with visual features on STL-10?\n\n6) Supervisedly learned filters in Figure 3 looks unusual to me, they are normally not that smooth. Have you optimized the hyperparameters? What is the resulting accuracy?\n", "title": "pre-review questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1v-2iWNx": {"type": "review", "replyto": "SJ8BZTjeg", "review": "Hello,\n\nA couple of questions:\n\n1. Looks like in Figure 2 every \"class\" consists essentially of a single image and its slight variations? Doesn't this mean GAN training failed? Do all your GANs produce samples of this quality? \n\n2. Could you please report classification accuracies? Otherwise it is impossible to compare your method to most other works.\n\n3. Why have you not compared to existing well-performing unsupervised learning methods? For example, feature learning with k-means (Coates and Ng 2011), sparse coding methods such as Hierarchical Matching Pursuit (Bo et al., 2012 and 2013), Exemplar-CNN (Dosovitskiy et al. 2014) .\n\n4. Why do you not show results with visual features on STL-10?\n\n5. Supervisedly learned filters in Figure 3 looks unusual to me, they are not normally that smooth. Have you optimized the hyperparameters? What is the resulting accuracy?\n\n6. Could you relate your method to Categorical GAN https://arxiv.org/pdf/1511.06390.pdf ?\n\nThanks!The paper proposes an approach to unsupervised learning based on generative adversarial networks (GANs) and clustering. The general topic of unsupervised learning is important, and the proposed approach makes some sense, but experimental evaluation is very weak and does not allow to judge if the proposed method is competitive with existing alternatives. Therefore the paper cannot be published in its current form. \n\nMore detailed remarks (many of these are copies of my pre-review questions the authors have not responded to):\n\n1) Realted work overview looks incomplete. There has been work on combining clustering with deep learning, for example [1] or [2] look very related. A long list of potentially related papers can be found here: https://amundtveit.com/2016/12/02/deep-learning-for-clustering/ . From the GAN side, for example [3] looks related. I would like the authors to comment on relation of their approach to existing work, if possible compare with existing approaches, and if not possible - explain why.\n\n[1] Xie et al., \"Unsupervised Deep Embedding for Clustering Analysis\", ICML 2016 http://jmlr.org/proceedings/papers/v48/xieb16.pdf\n[2] Yang et al., \"Joint Unsupervised Learning of Deep Representations and Image Clusters\", CVPR 2016 http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Joint_Unsupervised_Learning_CVPR_2016_paper.pdf\n[3] J.T. Springenberg, \"Unsupervised and semi-supervised learning with categorical generative adversarial networks\", ICLR 2016, https://arxiv.org/pdf/1511.06390v2.pdf\n\n2) The authors do not report classification accuracies, which makes it very difficult to compare their results with existing work. Classification accuracies should be reported. They may not be a perfect measure of feature quality, but reporting them in addition to ARI and NMI would not hurt.\n\n3) The authors have not compared their approach to existing unsupervised feature learning approaches, for example feature learning with k-means (Coates and Ng 2011), sparse coding methods such as Hierarchical Matching Pursuit (Bo et al., 2012 and 2013), Exemplar-CNN (Dosovitskiy et al. 2014)\n\n4) Looks like in Figure 2 every \"class\" consists essentially of a single image and its slight variations? Doesn't this mean GAN training failed? Do all your GANs produce samples of this quality? \n\n5) Why do you not show results with visual features on STL-10?\n\n6) Supervisedly learned filters in Figure 3 looks unusual to me, they are normally not that smooth. Have you optimized the hyperparameters? What is the resulting accuracy?\n", "title": "pre-review questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}