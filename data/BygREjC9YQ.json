{"paper": {"title": "A unified theory of adaptive stochastic gradient descent as Bayesian filtering", "authors": ["Laurence Aitchison"], "authorids": ["laurence.aitchison@gmail.com"], "summary": "We formulated SGD as a Bayesian filtering problem, and show that this gives rise to RMSprop, Adam, AdamW, NAG and other features of state-of-the-art adaptive methods", "abstract": "We formulate stochastic gradient descent (SGD) as a novel factorised Bayesian filtering problem, in which each parameter is inferred separately, conditioned on the corresopnding backpropagated gradient.  Inference in this setting naturally gives rise to BRMSprop and BAdam: Bayesian variants of RMSprop and Adam.  Remarkably, the Bayesian approach recovers many features of state-of-the-art adaptive SGD methods, including amongst others root-mean-square normalization, Nesterov acceleration and AdamW.  As such, the Bayesian approach provides one explanation for the empirical effectiveness of state-of-the-art adaptive SGD algorithms.  Empirically comparing BRMSprop and BAdam with naive RMSprop and Adam on MNIST, we find that Bayesian methods have the potential to considerably reduce test loss and classification error.", "keywords": ["SGD", "Bayesian", "RMSprop", "Adam"]}, "meta": {"decision": "Reject", "comment": "The aim of this paper is to interpret various optimizers such as RMSprop, Adam, and NAG, as approximate Kalman filtering of the optimal parameters. These algorithms are derived as inference procedures in various dynamical systems. The main empirical result is the algorithms achieve slightly better test accuracy on MNIST compared to an unregularized network trained with Adam or RMSprop.\n\nThis was a controversial paper, and each of the reviewers had a significant back-and-forth with the authors. The controversy reflects that this is a pretty interesting and relevant topic: a proper Bayesian framework could provide significant guidance for developing better optimizers and regularizers. Unfortunately, I don't think this paper delivers on its promise of a unifying Bayesian framework for these various methods, and I don't think it's quite ready for publication at ICLR.\n\nThere was some controversy about relationships to various recently published papers giving Bayesian interpretations of optimizers. The authors believe the added value of this submission is that it recovers features such as momentum and root-mean-square normalization. This would be a very interesting contribution beyond those works. But R2 and R3 feel like these particular features were derived using fairly ad-hoc assumptions or approximations almost designed to obtain existing algorithms, and from reading the paper I have to say I agree with the reviewers.\n\nThere was a lot of back-and-forth about the correctness of various theoretical claims. But overall, my impression is that the theoretical arguments in this paper exceed the bar for a primarily practical/empirical paper, but aren't rigorous enough for the paper to stand purely based on the theoretical contributions. \n\nUnfortunately, the empirical part of the paper is rather lacking. The only experiment reported is on MNIST, and the only result is improved test error. The baseline gets below 99% test accuracy, below the level achieved by the original LeNet, suggesting the baseline may be somehow broken. Simply measuring test error doesn't really get at the benefits of Bayesian approaches, as it doesn't distinguish it from the many other regularizers that have been proposed. Since the proposed method is nearly identical to things like Adam or NAG, I don't see any reason it can't be evaluated on more challenging problems (as reviewers have asked for). \n\nOverall, while I find the ideas promising, I think the paper needs considerable work before it is ready for publication at ICLR.\n"}, "review": {"Byl_YTnU0X": {"type": "rebuttal", "replyto": "SyxpYrZLCm", "comment": "Thanks for your comments.\n\nI have removed the comment about \"dummy\" w_i\n\nQ1) I have clarified in the text that w*_i is not the optimal weight wrt the data we actually have.  Instead, it is the optimal weight wrt the true underlying, unknown data distribution. As such, this Eq. (1) is analogous to maximum likelihood with infinite data.  Thus, you only get a delta-posterior if you have infinite data.  In contrast, in the usual finite data setting, w*_i is unknown, even if mu_{-i} is fixed, so we obtain a sensible non-delta posterior.\n\nIndeed, the two sections are intended to be quite distinct.  Broadly, Sec. 2 reasons about the generative process for the gradients, whereas Sec. 3 describes how we might perform inference under this model.  In particular, Sec. 2 gives a justification for the dynamical model presented in Eq. (11) (now Eq. 12).  It does this by noting that mu_{-i} introduces not only randomness, but also dynamics, because mu_{-i} changes slowly over time as it is being optimized (Fig. 1A).  However, mu_{-i} is a highly complicated, high-dimensional distribution, we cannot reason about it directly.  Instead, we define a surrogate model (Fig. 1B/C) that captures the structure of the model in 1A, but is highly simplified, such that we can reason about it.  I have changed the boundary between these sections slightly to emphasise that Sec. 3 describes inference in the generative model obtained in Sec. 2.\n\nFurther comments:\nI have clarified that we consider the Bayesian setting, where the data (that are indeed given) are assumed to have be generated from the specified generative model.  In this context, maximizing P(data| w) is straightforward, because we have such a model describing the probability of the data in terms of w, whereas maximizing P(data| w*) is much harder, because we don't have such a likelihood.\n\nQ2) I have spelled out a bit more completely the logic here, adding an extra display equation.  While I agree that your approach would simplify matters, in practice, we need to keep conditioning on Lambda_i, because we're trying to follow the usual Laplace approximations to the likelihood, and they use a variety of different approximations for \\Lambda_i.  We also need to condition on w*_i, because we're going to end up trying to infer w*_i from the gradient, so we need a model of the form P(g_i| w*_i) (or similar).\n\nQ3) I have moved back to using Lambda_{like,i}.\n\nI've clarified that the gradient is proportional to the negative Hessian, and noted the link to Fisher Information type results in the text.  However, the formal link is a bit technical, and it remains unclear under what conditions this link is formally correct.  The real goal here, though is to be able to use any approximation to the negative Hessian used in past work directly, rather than having to rederive it.\n\n", "title": "Response"}, "SklIUQINC7": {"type": "rebuttal", "replyto": "HJgLe01m0Q", "comment": "Thanks for your comments.\n\n(1) You\u2019re correct.  To optimize a VI objective, we can use any arbitrary optimiser.  Adam may work really well in practice.  But if we simply use Adam to optimize a variational objective, that doesn\u2019t tell us why Adam might be a good idea in the first place.  \n\nThere\u2019s a bunch of theory around the qualities of standard natural gradient (with no momentum), and around Bayesian filtering.  Thus, Khan, Zhang and Olliver do give a good justification for the rules that emerge directly from standard natural gradient (or the equivalent Bayesian filtering approach).  In contrast, there is to my knowledge, no theory around \u201cnatural momentum\u201d (though it does seem like an important research direction).\n\n(2) We have fixed this, and propagated it through into later expressions.\n\n(3) I have changed this to an approximation, and noted in the text that we get equality when the expectation of mu_{like,i} is independent of Lambda_i.\n\n(4) In general, this is a good idea, as it helps to clarify that the approximations we're making here are those used in past work. In Eq 1-9, we're now explicitly linking to the Laplace approximation / EKF.  In particular, Eq (2) is the Laplace/EKF like-approximation (i.e. second-order Taylor series expansion of the likelihood), and we now note this explicitly.  Further, in Eq. 9 we explicitly link the result back to the original Laplace approximation.  Please let me know if there's anywhere I should further clarify the link.\n\nI should also note that the resulting algorithm is closely linked to these natural-gradient VI style approaches.  As you noted initially, we recover them through Olliver (2017) in the limit as eta -> 0.\n\n(5) Thanks: this is a good point.  It\u2019s only self-consistent with respect to the steady-state expressions, which I wasn\u2019t pointing out here.  I\u2019ve given a considerably more thorough derivation, showing where the scaling comes from, though it becomes a good-bit more involved (e.g. requiring us to make a distinction between big-O and big-Theta, as in Knuth (1976) \u201cBig omicron and big omega and big theta.\u201d).\n\n(6) You're right that, in essence, this can be understood as an approximation.  There's a couple of points to make.  First, an approximation is absolutely necessary here, as we cannot write down and/or integrate over the distribution over all the other parameters, mu_i.  Second, in some ways, I wouldn't go so far as to call it an approximation.  In particular, for an approximation, we should have written down the exact dynamics of mu_i, and simplified them.  Here, we have given up on this exercise entirely, and instead written down a model directly over z.  This is what we mean when we say \"In practice, we use a simplified model as reasoning about all possible trajectories of \u03bc_{\u2212i}(t) is intractable\".", "title": "Response"}, "SkxmoLr53Q": {"type": "review", "replyto": "BygREjC9YQ", "review": "Paper summary: The authors analyze stochastic gradient descent through the lens of Bayesian filtering. In doing so they (approximately) recover several common adaptive gradient optimization schemes. The paper focuses on a theoretical construction of this framework and offers a limited empirical study.\n\nDetailed comments:\n\nI thought that the paper presented some interesting ideas but amongst the many things discussed there is very little which is empirically gratified. While the Bayesian filtering framework is interesting in that it recovers slight variations of existing algorithms, and also caters for some recent practical tricks, I do not feel that it substantially improves our theoretical understanding of these methods.\n\n1) I found the notation difficult to follow in the introduction and parts of section 2. I have highlighted several places explicitly below. I found paragraphs 2 and 3 of the introduction particularly challenging.\n\n2) I found the introduction of Bayesian filtering challenging to follow. For example, which form of the likelihood is assumed for the Taylor expansion? How/why is $\\mu_{like}$ identified using the gradient? Linking to Kalman filtering made things easier to follow.\n\n3) I think that the related work, and possibly a chunk of section 2, should include a discussion of Noisy Natural Gradient [1]. While the derivation differs, the motivation and final form of the updates seem to have a large overlap but this work is not cited.\n\n4) Start of 2.1: \"z will have on element representing a single parameter\", after which z is treated as a vector. I believe this sentence is present to distinguish RMSProp from Adam when momentum is added but I found it confusing at first.\n\n5) I found the comparisons between BRMSProp-vs-RMSProp and BAdam-vs-Adam fairly unconvincing. The assumptions are not clearly demonstrated to have little practical significance and Figure 2. does not seem to support the claim that these methods are strongly related. Is it possible to demonstrate empirically that these algorithms have equivalent behaviour under some limiting factors? And if not, is there a good reason for this that still justifies the comparison? I would appreciate some clarifications on these points.\n\n6) I am not sure what you mean by \"We now assume that the data is strong enough to reduce the uncertainty in the momentum below its levels under the prior\". I believe that I am following the mathematical arguments correctly but I find this phrasing misleading. Furthermore, this section uses e.g. ppth and wpth to refer to coordinates, I think it would be clearer to simply write Sigma_{pp}, etc.\n\n7) Section 4.2 is lacking justification in my opinion (am I missing something?). I think that this section needs to have the derivation clearly laid out (in the appendix would be fine). Furthermore, the NAWD algorithm is not explored empirically, or analyzed theoretically at all. I would argue that more evidence is needed that this is a reasonable thing to do before it is meaningful to include it in the final print of this paper. In general, sections 4.4 - 4.7 feel a little out-of-place and thrown together. I think there are interesting comments here which are certainly worth including but their presentation should be rethought and some empirical investigation would be valuable.\n\nMinor comments:\n\n- In introduction, how exactly does $w'_i$ differ from $w_i$?\n- In introduction, after para 2, the notation in the equation is confusing, e.g. overloading w_i(t) and w_i(mu_{-i}(t)).\n- In introduction,  para 3, \"must depend on other parameters\" - this seems like an obvious statement but it is presented as being crucial\n- Should \"Related Work\" start at 1 or 2?\n- (VERY MINOR) In section 2.2 and 2.3, \"christen\" seems like an add choice of word. Perhaps just \"call\"?\n- Equations 10 and 11 introduce an independence assumption on the dimensions of the parameter vector. I think this should be explicitly stated.\n- Section 7.2 heading typo: MOMEMTUM\n\nClarity: I found the paper challenging to follow in places due to choices of notation (and a weak background in Kalman filtering and related techniques).\n\nSignificance: I do not feel that this work offers a strong case for significance. The empirical evaluation is very limited. The theoretical framework introduces is interesting but is not justified particularly well in the paper and does not directly offer explanations for many of the observations noted in this paper and elsewhere.\n\nOriginality: To my knowledge, the ideas presented in the paper are original and hint at potentially interesting viewpoints of optimization.\n\nReferences:\n\n[1] Zhang et al. \"Noisy Natural Gradient as Variational Inference\" https://arxiv.org/pdf/1712.02390.pdf", "title": "I do not find the results of the paper particularly convincing though I would not rule out Bayesian filtering as a framework for analyzing adaptive methods", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hyg6_TTMCQ": {"type": "rebuttal", "replyto": "rJgJQhiZ07", "comment": "Thanks for your comments.\n\n(1) I have updated section 2 considerably, also in response to comments from AnonReviewer1.  We now define w*_i only through the argmax.  However, w*_i is still a random variable, because it depends on on the current settings of the other weights, mu_{-I}, which are treated as random variables.  Please let me know if this needs further clarification.\n\nUnit-swapping symmetries is where we permute the hidden units, and the weights and biases such that the output of the network is unchanged.  For instance, see Sussmann (1992) \"Uniqueness of the weights for minimal feedforward nets with a given input-output map.\"\n\n\\mathcal{D} referred to the actual data.  Here, we mean to use d, which represents a single minibatch, randomly chosen from the true underlying data distribution.\n\n(5) Many state-of-the-art methods (RMSprop, Adam, AdamW) were initially derived without a strong theoretical motivation.  The primary contribution of this work paper is to indicate that it may be possible to give a principled motivation for these methods.  As such, perhaps we're already seeing good performance in e.g. Adam because Adam has a principled basis.  As such, the empirical evaluations are intended merely to raise the possibility that we might get further performance improvements by explicitly considering this principled approach.\n\n(7) I have clarified this point in Section 7.\n\nIn Section 7, we draw links to previously suggested, empirically successful methods.  Given that the original papers have already shown the empirical effectiveness of these methods, it is unclear what our simulations might add, over and above this original work.  In order to maintain this strong link to past empirical work, I have deleted the more speculative parts of this section, 7.5-7.7.\n\nThat said, I agree that it would be great to try more challenging tasks: and I'll see whether I have a chance to do this before the deadline.", "title": "Response"}, "SylcBr6zRX": {"type": "rebuttal", "replyto": "rkxXskaMCQ", "comment": "Thanks again for your comments.\n\nI updated Section 2 in response to your previous comment, so some of these points may be redundant.  Nonetheless, there are still some important points to be considered here.\n\nThis has now gone (in response to your next comment), and we define w*_i purely using the argmax.  Please let me know if I can clarify further.\n\nWe no longer condition on the value of \u00b5_{\u2212i} anywhere.  I agree that doing so would lead to very confusing notation.  That said, \u00b5_{\u2212i} is still random variable controlled by the optimisation of all the other parameters.  It is not possible to write down this process explicitly, because not only are the dynamics of \u00b5_{\u2212i} highly complex, but they also depend strongly on e.g. the optimiser.  We deal with this in the transition from Fig. 1A to Fig. 1B, by writing down a simplified model over w*_i directly, rather than trying to integrate over the \u00b5_{\u2212i} random variables.\n\nAgreed about the Gaussian-like form.  However, this is the approximation taken in past work (e.g. Zhang 2017; Khan 2017; 2018), and our goal here is not to improve upon these approximations, but to place them in a dynamical framework.  I have attempted to draw closer links between this past work and the current derivation, but please let me know if I can make these stronger.\n\nThe comment about Lambda_i has gone.  Broadly, as we are currently trying to tie the derivation in Sec. 2 to past approximations (e.g. Zhang 2017; Khan 2017; 2018), we will choose Lambda_i based on those approximations (e.g. using the Fisher Information).", "title": "Response"}, "SygJL2hGAX": {"type": "rebuttal", "replyto": "HklQLC5WCX", "comment": "Thanks for your comments.  I have updated Section 2, which hopefully clarifies it considerably.\n\nIt is important to note that w*_i is both a mode, AND a random variable, because it depends on the current estimate of the other parameters, mu_{-i}, which we treat as a random variable.  I updated the text to emphasise this point, including by deleting Eq. (1), leaving only the mode-based definition.\n\n- We have done this as far as possible (e.g. by using Lambda_i = lambda_i).\n- We have rewritten the text to avoid conditioning on random variables, except where absolutely necessary (i.e. Eq. 1), and for Eq. (1), we have repeatedly emphasised the consequences of conditioning on a random variable.\n- We have removed \\mu_i from the conditioning.\n- I have given more specific definitions of mu_{like, i} and Lambda_i as Taylor series coefficients, simultaneously with the original Taylor series expansion.  It is difficult to any more specific definitions of mu_{like, i} and Lambda_i without assuming a specific form for the data, which, as you pointed out earlier, is highly undesirable.  I have ensured that Lambda_i is included in the conditioning in Eq. (7)/Eq. (8).\n- I have removed the discussion of the variance in Eq. (6), and instead noted that typical approximations of the original likelihood (new Eq. 2) are necessarily quite severe (e.g. using the Fisher Information).  As such, instead of trying to obtain new approximations, we instead, try to leverage these older approximations, by obtaining a model for the gradients which is equivalent to any given approximation to the original likelihood (new Eq. 2).  In particular, our goal is that the likelihood, conditioning on the gradient, should be equivalent to whatever approximate second-order Taylor series approximation to the original likelihood in new Eq. (2) one chooses to make.\n\nIt would be interesting to investigate whether this derivation could be used improve or better understand typical approximations for Lambda_i (or the variance of the gradient).  However, in practice I suspect you are forced to use quite severe approximations, that are difficult to justify without being able to refer to past work that has used those exact approximations, and as such, it is easier to try to use approximations from past work directly.", "title": "Response"}, "HkxH12v-RX": {"type": "rebuttal", "replyto": "rJxLfaOlCQ", "comment": "Thanks for your comments.\n\nEq (1) and Eq (2) are actually equivalent: I agree, it would be crazy to use w*_i for two different things.  I think of Eq (1) as the \"primary\" definition of w*_i.  The confusion comes in with Eq (2).  In particular, Eq. (2) is *not* the mode of the usual likelihood, conditioned on all the data.  Instead, the expectation is taken over data, d, drawn from the underlying, unknown, true data distribution (which is why we need to write it as an expectation).  As such, Eq. (2) in effect does maximum likelihood with infinite data, which picks out the true value for the parameter, w*_i.  I have altered the text to clarify this point.\n\nThere was a slight problem with the notation here: we have changed P(w*_i | w_{\u2212i}=\u00b5_{\u2212i}, D) to P(w*_i | \u00b5_{\u2212i}, D).  That said, w*_i as defined by Eq. (1) is a standard random variable, so it can be marginalised and conditioned, just like any other.  In particular, we can marginalise over \u00b5_{\u2212i}, using,\nP(w*_i | D) = \\int d\u00b5_{\u2212i} P(w*_i | \u00b5_{\u2212i}, D) P(\u00b5_{\u2212i}| D),\nwhile it is difficult to write down the distribution over \u00b5_{\u2212i}, it could be sampled. In fact, this is just what we do in transitioning from Fig. 1A to Fig. 1B: the integral over \u00b5_{\u2212i} is difficult, so instead we directly write down simplified dynamics over w*_i.\n\nComputing one of these distributions is order(N), where N is the number of parameters, so it should be tractable.  However, computing all the distributions is order(N^2), which is intractable for all but the smallest networks.\n\nNote that while Eq. (3) takes a \"Gaussian-like\" form, d doesn't appear in the correct place on the RHS.  Instead, this is a Gaussian over a parameter, w_i.  What's going on here is that we're only interested in the dependence on w_i, so we have put all the dependence on the minibatch into const, mu_{like,i} and Lambda_i.  As these are arbitrary functions of the data, we can, in fact have arbitrary data.  More broadly here, we're treating the likelihood as function of the parameters (e.g. http://www.inference.org.uk/mackay/Bayes_FAQ.html#likelihood).\n\nmu_{like,i} is the mode for one minibatch.  w*_i is expected mode, given infinite data.  Because mu_{like,i} depends on the minibatch, it will exhibit some variability across minibatches, mu_{like, i}, but it will have mean w*_i.  I have updated this section, to note that we can also get the variability of mu_{like, i} by considering the Fisher Information, which gives us the variance of g.  Lambda_i can be batch-dependent (though the derivation simplifies considerably, if we assume that it is fixed, and I did this initially).  For instance, consider classification with noisy labels (e.g. from mechanical turk), where some labels are known to be high-quality, and others are low-quality.  The model should use higher Lambda_i for high-quality, and thus more informative labels.\n\nI think the same basic approach could be used in many domains: importing some aspects of the approximations into the inference problem, such that we can use Bayes theorem to reason under the approximations.  However, I'm not sure this would simplify the presentation much.", "title": "Response"}, "S1epgrIt2Q": {"type": "review", "replyto": "BygREjC9YQ", "review": "In this work, the authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior.  In Ollivier, 2017, a framework is proposed to connect Bayesian filtering and natural gradient.  On the other hand,  in Khan et al., 2018. an approach is proposed to connect natural gradient and adaptive gradient methods.  The main contributions of this work are (1)  introducing a dynamical prior and (2) recovering RMSProp and Adam as special cases. \n\nHowever, the proposed dynamical prior is very similar to the fading memory technique used in Ollivier, 2017. (see Proposition 3 of Ollivier, 2017) \nFurthermore, the authors argue that this work recovers a root-mean-square form while Khan et al., 2018 recovers a different sum-square form. Unfortunately, the authors have to use a series of unnatural approximations to recover the root-mean-square form. In fact, as mentioned in Khan, 2017b  this proposed method without these approximations is also a mean-square form. (also see Eq (2.28-2.29) of Ollivier, 2017)\n\nSince the authors mainly follow Ollivier, 2017 and make unnatural approximations,  the work has a limited impact.  To get a higher rating, the authors should clearly give justifications and insights of these approximations.\n\nDetailed comments:\n(1) On Page 1,  \"The typical approach to Bayesian filtering, where we infer a distribution, ... jointly, forces us to use extremely strong, factorised approximations, and it is legitimate to worry that these strong approximations might meaningfully disrupt the ability of Bayesian filtering to give close-to-optimal updates.   ... we instead consider ... that incorporates factorisation into the problem setting, and therefore requires fewer approximations downstream. \"\nThe proposed method is equivalent to jointly perform Kalman filtering with full-covariance with an additional diagonal-approximation step. This additional step might also meaningfully disrupt the ability of Bayesian filtering. Furthermore, such approximation ignores the off-diagonal terms in the low-rank approximation at Eq (8). \n\nMinor: You should use \\approx at Eq (8) since a rank-1 approximation is used.  \n\n(2) On page 2, \"It has been noted that under specific circumstances, natural gradient is approximate Bayesian filtering (Ollivier, 2017), allowing us to link Bayesian filtering to the rich literature on natural gradients.  However, this only occurs when the dynamical prior in the Bayesian filtering problem has a specific form: the parameters being fixed over time (i.e.  arguably an online data, rather than a true Bayesian filtering setting).\" \nThe authors should comment the difference between the dynamical prior and the fading memory technique (see Proposition 3 of Ollivier, 2017) where at page 14 of Ollivier, 2017, Ollivier mentions that \"this is equivalent ... or to the addition of an artificial process noise ... in the model\".  I think Ollivier's idea is very similar to the dynamical prior used at Eq (1) of this submission.  Furthermore, the second-order Taylor expansion with a Fisher information-based estimation of Hessian (see the equation below Eq(1) of this submission) is exactly the same as Ollivier's Extended Kalman filter (see  Eq 2.25 at Lemma 9  and Lemma 10 of Ollivier, 2017).  The authors should cite Ollivier, 2017.\n\nMinor: Eq (6) should be E_p [ - \\nabla_z^2 \\log p(d|z) ] = E_p  [ e e^T ], where \"-\", the negative sign is missing. Please see the definition of the Fisher information matrix.\n \n(3) On page 2, \"While there have been attempts to use natural gradients to recover the Adam or RMSprop root-mean-square form for the gradient normalizer, in practice a different sum-square form emerges (Khan & Lin, 2017; Khan et al., 2018). In contrast, we show that to recover the Adam or RMSprop form for the gradient normalizer.\" \nKhan et al., 2018 is a mean-square form for variational inference due to the entropy term of the variational distribution. (see Sec 3 and 5 of  Khan et al., 2018 and Khan, 2017b )\nUnfortunately, the \"root-mean-square form\" does not appear naturally in this submission. In practice, the proposed update is also a mean-square form  (see Eq (2.28-2.29) of Ollivier, 2017 and Khan, 2017b) without a series of unnatural approximations used in this submission.\nTo justify these assumptions, the authors should explain when \"the steady state posterior variance\" (see sec 2.21) and  \"a self-consistent solution\" (see sec 7.1) achieve.  As far as I know, \\sigma^2_t = \\sigma^2_{t+1} in sec 2.2.1 only holds in the limit case when t-> \\inifity.  Why does the equality hold at each time step t? The authors should give a justification or an intuition about these approximations since this paper is a theory paper. Please also see my next point.\n\n(4) Section 7.1 is also confusing.\nIn sec 7.1, the authors assume that A \\in O(\\eta). However, A=\\eta^2/(2\\sigma^2) in sec 2.2 and A_{1,1} =  ( \\eta_w^2+\\eta^2 )/ (2\\sigma^2) at Eq (14). In both cases, A can be \\in O(\\eta^2). This is very *critical* since the authors argue that O(\\eta^3) can be neglected in sec 7.1.  The authors use this point to show that Adam is a special case. \nIf A \\in O(\\eta^2), we know that \"A \\Sigma_{post}\" \\in O(\\eta^3) should be neglected. At the last equation on page 10,  the authors do not neglect \"A \\Sigma_{post}\". Why?  The authors should clarify this point to avoid doing *selective* neglection.  Again, the impact of this paper should be inspiring new adaptive methods.\nThe authors also mention that the second-order term in A is neglected in sec 7.2. Any justification? \n\n\nReferences\n[1] Ollivier, Yann. \"Online Natural Gradient as a Kalman Filter.\" arXiv preprint arXiv:1703.00209 (2017).\n[2] Khan, Mohammad Emtiyaz, and Wu Lin. \"Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models.\" arXiv preprint arXiv:1703.04265 (2017).\n[3] Khan, Mohammad Emtiyaz, et al. \"Vprop: Variational Inference using RMSprop.\" arXiv preprint arXiv:1712.01038 (2017b).\n[4] Khan, Mohammad Emtiyaz, et al. \"Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam\" (2018)\n\n", "title": "Unnatural approximations", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Syx3r6dp6Q": {"type": "rebuttal", "replyto": "HJl-3AvnaX", "comment": "(1) Indeed, the fading-memory technique can be viewed as \"an artificial process noise proportional to [the posterior covariance]\".  We use a different process noise that is more natural (in the sense that the process noise is fixed, rather than depending on inferences made under the model).  Further, this different process noise gives a very different, behaviour of the update rules from the mean-square normaliser that requires this form for the process noise.  I should also note that all of our approximations are very standard and well-understood in the Physics and Engineering literature, to the extent that they are arguably high school, rather than undergraduate level (e.g. steady state, neglecting terms such as A Phi A proportional to dt^2).\n\n(2) This is obvious, because for a well-calibrated model, the posterior is well-calibrated, and thus the true parameter can be understood as being drawn from the posterior.  Further, in (2), we take the expectation under the true data distribution --- in effect, giving us infinite data.  Of course, in the infinite data regime, we again recover the underlying parameters.\n\nTo confirm the validity of the approximations in this section, we show that the second-order approximation to the likelihood is the same under the model conditioned on the gradient, and under the original model.\n\n(3) It is well known that an autoregressive process of any order can be written as an AR(1) process by defining an expanded state space.  See Example 1 in lecture notes: http://faculty.washington.edu/ezivot/econ584/notes/statespacemodels.pdf\nIn that link, they go so far as regarding this equivalence as \"obvious\".\n\n(4) Could you be more specific?  What do you get?  If you're worried about the denominator, that term is basically,\n\nSigma e e^T Sigma / (1 + e^T Sigma e)\n\nTaking a first-order Taylor series expansion of the denominator, we obtain,\n\nSigma e e^T Sigma (1 - e^T Sigma e)\n\nBut as Sigma ~ eta, and we neglect terms that scale with eta^3, we have, this reduces to just Sigma e e^T Sigma\n\n(5) This is a standard approximation made in almost all work on dynamical systems in engineering and physics.  Specifically, it is valid because Phi~1, and\nfor RMSprop:\nA~eta^2\nso A Phi A ~ eta^4 is dominated by A Phi ~ eta^2, and can be neglected\nand for Adam:\nA~eta\nso A Phi A~ eta^2 is dominated by A Phi ~ eta, and can be neglected.  Note that, phrased in this way, this constitutes taking the \"leading order\" terms in an expression, and is again a standard, well-understood approximation.\n\n(6) Indeed, there is a difference at small t, and this may be at the root of our improved performance.", "title": "Response"}, "rkg-ctln6Q": {"type": "rebuttal", "replyto": "H1gRViDo6Q", "comment": "(1) Indeed, the EKF defines an approximate linearised likelihood, as a Taylor series expansion of the underlying nonlinear model.  The underlying non-linear model cannot, and should not depend on inferences made under the model.  Only the linearised model depends on the inferred mean for the latent variable, and then, only through the location at which you perform the linearisation.\n\nWe entirely agree that the approximate, linearised likelihood does indeed depend on inferences made under the model, and that this a perfectly natural, sensible thing to do.  This is not the question.  The question is whether the dynamical prior (in particular the process noise) depends on inferences made under the model.  Olliver 2017 set the process noise proportional to the posterior uncertainty (covariance), and it is this that can't be written down within the EKF framework.\n\n(2) I would kindly request that you read the new section, entitled \"Factorisation implies a rich dynamical prior\", which provides the principled justification that you request.\n\nAgain, it isn't really an approximation, but a new way of setting up the problem.  In particular, we note that the updates in most algorithms for neural network optimization are based on just the gradient for that parameter.  As such, if we define a factorised model for each parameter separately, we can take the \"data\" to be not the underlying input-output pairs, but the backpropagated gradient for that parameter.  This generative model naturally gives rise to the emergence of a new latent variable, w_i^*, the optimal value for the ith parameter, conditioned on the current estimate of all the other parameters.  If we choose to work with this parameter, then dynamics emerge automatically: w_i^* must change over time, because it depends on our current estimate of all the other parameters, which are changing as they are optimized.\n\nThe exact same exercise can trivially be extended to non-diagonal approximations using the derivations in our paper (\"Bayesian (Kalman) filtering as adaptive SGD\"), and we are considering the Kronecker factored approach in future work.  However, we believe that the we have already made considerable contributions: an entirely novel connection between high-dimensional correlations and temporal changes in a principled Bayesian model, that recovers state-of-the-art adaptive methods including RMSprop, Adam, AdamW and NAG.", "title": "Response"}, "S1eKdWDiam": {"type": "rebuttal", "replyto": "HygmvPLo6Q", "comment": "To reiterate.  The EKF is an inference technique.  The underlying prior (just like all other prior distributions in the Bayesian literature) does not depend on inferences under that model.  For instance, on the Wikipedia page for the EKF (https://en.wikipedia.org/wiki/Extended_Kalman_filter), under \"formulation\", they define the prior, which makes reference to nonlinear functions, but not to inferences made under the model.\n\nOf course, when performing inference, (\"Discrete-time predict and update equations\" on Wikipedia), they do make reference to the inferred means and uncertainties.  To perform inference (and only to perform inference), they linearise the nonlinear functions, around the current inferred mean.  To reiterate, there is --- and has to be --- an underlying nonlinear model that does not depend on the inferences.  Only the surrogate, approximate linearised model depends on the inferences, and then, only through the location at which you perform the Taylor expansion.\n\nI agree Olliver 2017 obtain Kalman Filter updates that are equivalent to natural gradient at every step.  However, this requires them to use process noise, Q, proportional to the filtering covariance, P, i.e. they use Q = Q(P).  It is not possible, to my knowledge, to define a nonlinear EKF style model as in (https://en.wikipedia.org/wiki/Extended_Kalman_filter), under \"formulation\" which displays the same behaviour i.e. Q(P) = Q(z), and, again to reiterate, Olliver 2017 do not claim as such.\n\nThat said, for the main thrust of our argument, the artificiality or otherwise for the Olliver 2017 process noise is a side point.  Olliver 2017 work hard to ensure that their approach is equivalent to natural gradient, but that means they can't recover momentum, and don't recover the root-mean-square form for the gradient normaliser.  In contrast, we give a principled justification for a different Kalman filtering approach (with a different choice for the process noise) that is able to recover momentum, and the root-mean-square gradient normaliser.", "title": "Response"}, "rJggX1UoTX": {"type": "rebuttal", "replyto": "BklHJvro6Q", "comment": "(1) Presumably, you mean AR(2)?  Vanilla gradient descent has AR(1) like dynamics, as the next position depends on the previous one, whereas momentum-based methods can be formulated as depending on the past two positions, where we estimate the velocity using their difference.\n\nRegarding [1], we can always take an inference problem, convert it to an optimisation problem using VI, and apply momentum to perform the optimisation.  This is always possible, and is always a valid approach.  But this is using, rather than recovering momentum.  In contrast, in our work, we are interested in how momentum might emerge as the solution to a principled inference problem.  \n\nAn alternative viewpoint is to think about your reference (Olliver 2017).  As long as they're doing natural gradient, the link to Bayesian filtering and hence to Bayesian inference is retained. As soon as they stop doing natural gradient (e.g. by adding momentum) the link to Bayesian filtering is broken, and they are simply using some optimisation strategy (though perhaps a very effective one) to optimize a loss function (albeit one that happens to be motivated by Bayesian considerations).\n\n(2) Under the standard set-up for an EKF, the generative model does not depend on inferences made under that model.  For instance, one might sample the latent variable z from the following process,\nz(t+1) ~ N((I - A) z(t), Q),\nand the observed data, g, from another Gaussian,\ng(t+1) ~ N(f(z(t)), I)\nNote that as we have an arbitrary function, f(z(t)), here, we are in the EKF setting that you describe.\n\nTo reiterate, this is the true, underlying generative model, and it never depends on inferences made under the model.  To perform extended Kalman filtering, we can define an approximate model as a linearisation of f around the current mean.  However, that is categorically not what is going on in Olliver (2017).  There they set the process noise, Q, equal to their filtering uncertainty.  Olliver (2017) do not claim, and it is not, to my knowledge possible to set up a valid generative model (i.e. one for which the process noise, Q, is a function of z) under which this emerges in an EKF-like fashion.\n\nPlease let me know if there are any other issues I can clarify.  I await with interest your opinion on our new derivations that treat the back propagated gradient as the data in a Bayesian inference problem!", "title": "Response"}, "BJgJEO4iTm": {"type": "rebuttal", "replyto": "S1epgrIt2Q", "comment": "First, it should be noted that neither Khan et al. (2018) nor Zhang et al. (2017) recover the root-mean-square-gradient form for the normalizer.  To quote from Khan et al. (2018):\n\"Using ... an additional modification in the VON update, we can make the VON update very similar to RMSprop. Our modification involves taking the square-root over s_{t+1} in (7)\"\nAnd to quote from Zhang et al. (2017):\n\"These update rules are similar in spirit to methods such as Adam, but with the addition of adaptive weight noise. We note that these update rules also differ from Adam in some details: (1) Adam keeps exponential moving averages of the gradients, which is equivalent to momentum, and (2) Adam applies the square root to the entries of f in the denominator. We define noisy Adam by adding momentum term to be consistent with Adam. We regard difference (2) inessential. The choice of squaring or divison may affect optimization performance, but they don't change the fixed points, i.e. they are fitting the same functional form of the variational posterior using the same variational objective.\"\nBoth of these approches use natural gradient VI, and they both encounter the same problem: that natural gradient gives you a mean-square-gradient, rather than a root-mean-square gradient form for the normalizer.  Khan et al. (2018) deal with this by reaching in and replacing the mean-square with a root-mean-square normalizer (without a principled justification based on approximate inference), and Zhang et al. (2017) regard the difference between a root-mean-square and a mean-square gradient normalizer as \"inessential\".\nMoreover, neither approach gives rise to momentum.\n\nOlliver (2017) works hard to ensure that their filtering technique is equivalent to natural gradient VI, and therefore, while they don't examine it, they are also likely to be unable to recover the root-mean-square normalizer.  More problematic is the (as you quoted) \"the addition of an artificial process noise\".  However, it is important to complete the quote, \"the addition of an artificial process noise Q_t proportional to [the posterior covariance] P_{t-1}\"  There are two things that they could potentially be referring to as \"artificial\" here:\n1.) The introduction of any process noise into the generative process.\n2.) The introduction of a generative process that depends on inferences under that process.\nNotably, 2 is very, very artificial: I have never seen a Bayesian generative model which depends on inferences under that model.  Our approach does not use this process noise, and as such, if nothing else, our generative process is more meaningful, in the very basic sense that it doesn't depend on the posterior.\n\nDetailed comments:\n\n(1) There is no such equivalence, because, as we make clear in a new section, entitled \"Factorisation implies a rich dynamical prior\", these two methods don't even perform inference over the same random variable. The approach that you suggest towards factorising the problem fails immediately because P(w_i| D) (i.e. the distribution over one parameter conditioned on all the data), is meaninglessly broad distribution, because the posterior distribution is highly symmetric (e.g. unit-swapping symmetries).  Further, under this model, there is no principled motivation for the introduction of dynamics.\n\nInstead, we note that the updates in most algorithms for neural network optimization are based on just the gradient for that parameter.  As such, if we define a factorised model for each parameter separately, we can take the \"data\" to be not the underlying input-output pairs, but the backpropagated gradient for that parameter.  This generative model naturally gives rise to the emergence of a new latent variable, w_i^*, the optimal value for the ith parameter, conditioned on the current estimate of all the other parameters.  If we choose to work with this parameter, then dynamics emerge automatically: w_i^* must change over time, because it depends on our current estimate of all the other parameters, which are changing as they are optimized.\n\nIn essence, this approach converts the intractable high-dimensional correlations in the full posterior into tractable low-dimensional temporal dynamics.  Notably, dynamics do not and cannot emerge from a straightforward factorised approximation to the original full posterior.  I'd be grateful for any further assistance to clarify the presentation.\n\nMinor: We have rewritten this section, pushing the discussion of Fisher Information into the Appendix, and defining e e^T such that it equals \\mLambda.  This is possible because the Hessian is always rank 1 (as we now clarify).\n", "title": "Response (1/2)"}, "S1lCMu4iTX": {"type": "rebuttal", "replyto": "S1epgrIt2Q", "comment": "(2) We have included a paragraph in the introduction on the difference with Olliver's (2017) fading memory technique.  To reiterate, they use a highly unnatural generative process under which the process noise depends on inferences under that generative model (in particular, the posterior covariance).  In contrast, the process noise under our model is \"meaningful\" in the sense that it does not depend on inferences under the model.\n\nMinor: we have delete this equation.\n\n(3) Steady-state is indeed only reached as t -> infinity, and we have added a note to this effect.  At the very least, we don't expect to obtain a mean-square form for the normalizer, because that would require us to follow Olliver (2017) in using process noise proportional to our uncertainty, which we categorically do not do.\n\n(4) The first point is that we agree, the impact of this paper should be inspiring new adaptive methods.  And we do just that, with BRMSprop and BAdam.  Importantly, these approximations are never used in simulations/updates for BRMSprop and BAdam.  Instead, they are only used to think about the similarities and differences between our Bayesian approaches (BRMSprop and BAdam) and classical methods such as RMSprop or Adam.  As such, in some sense, the stronger the approximations we need to make BAdam close to Adam, the more scope we have for developing improved adaptive methods!\n\nIn response to your specific questions.\nHere, we are considering the limit of \\eta -> 0, so assuming A scales with \\eta is weaker than assuming A scales with \\eta^2.  While A for RMSprop indeed scales with \\eta^2, A for Adam scales with \\eta (the top-right element is -eta). As A in general scales with \\eta, the terms you refer to, A S_post, scale with \\eta^2, and cannot be neglected in general.  However, for the case of RMSprop, A indeed scales with eta^2, and so we can neglect these terms, and indeed we do just that in the original draft (the approximate variance updates that we solve for in steady state do not include any weight-decay terms).  We have clarified this point in the supplementary, including both the RMSprop and Adam cases.\n\nAs a technical issue, the usual definition of big-O notation considers the limit as \\eta -> \\infty, whereas the relevant limit in our case is \\eta -> 0.  I have therefore replaced the \\in big-O notation, with the physics-inspired \\sim (for \"scales with\").", "title": "Response (2/2)"}, "ryeqhwEjpX": {"type": "rebuttal", "replyto": "BkeCieGqn7", "comment": "Assessment: We have hopefully clarified the approximations (esp. in Adam) and the introduction of a separate inference problem per variable (see the new sectinon \"Factorisation implies a rich dynamical prior\".  It should be noted that the methods we propose --- BRMSprop and BAdam --- don't use these approximations.  Instead, the approximations are only necessary to understand the similarities and differences betweeen BRMSprop and BAdam and the corresponding classical methods: RMSprop and Adam.  Finally, I agree that assessing the effectiveness of the Bayesian model for the optimal weight would make interesting future work, and we envisage including it in a broader investigation of the statistics of optimal weights under optimization.\n\nIntro: we have deleted the reference to Bayesian optimization, and rewritten this section in order to clarify it considerably (see the section \"Factorisation implies a rich dynamical prior\").  In this section, we note that we aren't making a factorised approximation in the usual sense, where you write down an inference problem over all N parameters jointly, and use a factorised posterior.  Instead, we change the inference problem itself, by writing down N inference problems, one for each parameter, where the data for each individual problem is the backpropagated gradient.  This bakes factorisation into the problem setting. \n\nWhen we look at the generative process for the gradients, we find that the potential for a rich dynamical prior has emerged automatically, because the gradients for one parameter depend on all the others, and those other parameters change slowly as they are also being optimized. Critically, this converts intractable high-dimensional correlations in the original problem into tractable low-dimensional dynamics.  In contrast, these dynamics do not emerge in a traditional approach where you simply approximate the high-dimensional posterior.\n\nResults: this is an interesting point, which bears further discussion.  I agree that the obvious approach here would be to use the Hessian (i.e. the second derivative of the actual likelihood).  However, using Hessian directly has two problems.  First, the Hessian isn't necessarily positive definite, and if such a likelihood is combined with a sufficiently weak prior under our approximations, it can result in a meaningless posterior (e.g. a Gaussian with a non-positive semidefinite covariance matrix).  Second, the Hessian (i.e. the full matrix of second derivatives) is difficult and expensive to compute in modern autodiff software such as PyTorch.  Because of these two disadvantages, we chose to work with a closely related quantity: the Fisher Information.  This is the Hessian for the expected log-likelihood of data drawn from the model, and it therefore represents an approximation to the actual Hessian.  However, in the case of classification, we expect the approximation to be reasonable, because it is based on the same input data, with output labels sampled from the model.  And as the classification error gets smaller, the model outputs and the true classification become more similar, and thus the approximation becomes becomes increasingly good.  Importantly, the Fisher Information resolves the two issues we had above: it is always positive definite, and it can be computed by taking the covariance of gradients, which are easy to compute in standard autodiff frameworks.  This is a standard approach (e.g. Zhang et al. 2017).", "title": "Response (1/2)"}, "BkeX5DNjTQ": {"type": "rebuttal", "replyto": "BkeCieGqn7", "comment": "Experiments: usually, when we set the learning rate to be too small, it is impossible to move the parameters far enough from their initializations, and so we obtain very poor performance.  One of the interesting things about our framework is that as eta goes to zero, it converges to Bayesian inference without the dynamical prior, which, in effect, gives an adaptive learning rate that goes as the sum-of-square gradients.  In practice, we still expect to need to tune the learning rate to find the minimum of the curves in Fig. 2 (now Fig. 3).  As regards toy experiments, we expect the benefits of our approach to become more evident in complex models with strong posterior correlations, so the simplest relevant toy experiment is linear regression with highly correlated inputs --- which isn't all that simple.  Regarding convergence, tried a range of numbers of epochs, and found that the displayed curves are pretty stable at 50 epochs, but that if anything, the difference between the Bayesian and classical methods actually increased for larger number of epochs.\n\nWe have discussed the ADF-style approach (Ghosh et al.), and have noted that the Kalman filtering approach is well-understood. Finally, it should be noted that Khan et al. (2018) does not recover the root-mean-square-gradient form for the normalizer.  To quote from Khan et al. (2018):\n\"Using ... an additional modification in the VON update, we can make the VON update very similar to RMSprop. Our modification involves taking the square-root over s_{t+1} in (7)\"\nBoth they and Zhang et al. (2017) of these approches use natural gradient VI, and they both encounter the same problem: that natural gradient gives you a mean-square-gradient, rather than a root-mean-square gradient form for the normalizer.  Khan et al. (2018) deal with this by reaching in and replacing the mean-square with a root-mean-square normalizer (without a principled justification based on approximate inference), and Zhang et al. (2017) regard the difference between a root-mean-square and a mean-square gradient normalizer as \"inessential\".", "title": "Response (2/2)"}, "r1lwDgEi6Q": {"type": "rebuttal", "replyto": "SkxmoLr53Q", "comment": "7) We have added an Appendix section spelling out how inference in this filtering model implements NAG.  The empirical evaluations in Fig. 2 include NAWD as part of the full BRMSprop and BAdam algorithms.  We agree that it would be great to do an empirical investigation of all the features discussed in this paper, but this becomes an empirical analysis of a pretty large swathe of techniques for adaptive stochastic gradient descent, which is out of scope for the present paper.\n\nMinor comments:\n- w' has been removed.\n- While we have rewritten this section, we have retained the notation.  w is the underlying unknown random variable, whereas mu is our current mean estimate of that variable. Hopefully this distinction is clearer in the new version.  We are also using the recommended DL book notation for random variables.\n- I agree, it is obvious.  But it is also crucial: it is the key for why we need to introduce a rich dynamical prior.  Again, hopefully this discussion has been improved in the new version.\n- Related work has been incorporated into the introduction.  I've also explicitly introduced an Introduction section, so that next section is 2.\n- I have replaced christen\n- The independence assumption comes right at the start when we write down a factorised generative model (as we explain in the new section \"Factorisation implies a rich dynamical prior\"  in Eq. 10 in the original submission, we're doing filtering in a 1D inference problem.\n- Fixed", "title": "Response (2/2)"}, "ryxGTLVjam": {"type": "rebuttal", "replyto": "SkxmoLr53Q", "comment": "To my knowledge, this is the first work that is able to reconcile Bayesian inference with adaptive SGD methods such as RMSprop or Adam.  (Please see the response to reviewer 3, or the new introduction for details).  To do this, we introduce an entirely new approach to relating optimization and Bayesian inference, where we define a Bayesian generative model for the backpropagated gradients.  This model implicitly converts intractable high-dimensional correlations in the original posterior into tractable low-dimensional temporal changes.  Empirically, this approach gives very promising initial results.\n\n1) I have replaced paragraphs 2 and 3 with a section entitled \"Factorisation implies a rich dynamical prior\", which takes a far more in-depth look at how writing down a factorised generative model forces us to include a rich dynamical prior.  Also see the response to reviewer 3.\n\n2) I have replaced the title of this section with Bayesian (Kalman) filtering as adaptive SGD to emphasise the link to Kalman filtering.  Hopefully the new introduction has made this clearer.  Otherwise, the approach is relatively standard (e.g. Zhang et al. 2017).\n\n3) We have included a discussion of [1] in the introduction.  In short, they state: \"These update rules are similar in spirit to methods such as Adam, but with the addition of adaptive weight noise. We note that these update rules also differ from Adam in some details: (1) Adam keeps exponential moving averages of the gradients, which is equivalent to momentum, and (2) Adam applies the square root to the entries of f in the denominator. We define noisy Adam by adding momentum term to be consistent with Adam. We regard difference (2) inessential. The choice of squaring or divison may affect optimization performance, but they don't change the fixed points, i.e. they are fitting the same functional form of the variational posterior using the same variational objective.\"  Thus, they cannot be considered as recovering Adam from natural gradient VI, because natural gradient VI doesn't give momentum or the root-mean-square form for the normalizer.\n\n4) I've flipped the sentence around so hopefully it's a bit less confusing.  It now reads:\n\"For Adam, z will have two elements representing a parameter and the associated momentum, whereas for RMSprop it will have only one element representing a single parameter.\"\nI have also added a new equation defining z for RMSprop and Adam here, to clarify the shape of z in the subsequent derivation.\n\n5) I was also surprised to see such substantive performance improvements with BRMSprop and BAdam, as compared with RMSprop and Adam: this is the benefit of obtaining updates as inference under a well-motivated prior.  The justification for the comparison comes from the sections \"Recovering RMSprop from BRMSprop\" and \"Recovering Adam from BAdam\".  The correspondence is close for RMSprop (becoming exact as t increases, if g^2 remains constant), and somewhat less so for Adam.  But our goal wasn't to match RMSprop and Adam exactly.  Our goal was to improve upon RMSprop and Adam by capturing their essential dynamical assumptions under a well-motivated Bayesian prior.  As such, there's a sense in which we don't actually want RMSprop and BRMSprop to be exactly equivalent, because if they were equivalent, we wouldn't be able to get inspiration for new adaptive methods, and we couldn't get improved performance.\n\nOtherwise, in Fig. 2, we attempted to make these comparisons as fair as possible otherwise (same network, same initialization, same momentum for Adam and BAdam, sweeping out all sensible learning rates for both methods).  As such, it is also possible to take Fig. 2 (now Fig. 3) as showing considerably improved performance over standard adaptive baseline methods.\n\n6) In the supplementary section entitled \"Setting the momentum decay\", we show that under the prior, the uncertainty in momentum is eta_p/2.  Here, we make the assumption that data is reasonably strong, and so our uncertainties are well below those under the prior, i.e. Sigma_pp << eta_p/2.  If this were not true, the network would be unlikely to be able to learn anything.  As such, we might expect BAdam and Adam to differ in the case where the data is weak, and it would be interesting to establish which approach has better empirical performance in that case.\n\nI have replaced references to the ppth element with the \"lower-right\" element of the matrix equation, and explicitly stated the component equation that I'm referring to.", "title": "Response (1/2)"}, "Byenmc08a7": {"type": "rebuttal", "replyto": "Hyg16Apot7", "comment": "Given you're an author on this paper, I was wondering whether you could answer a question about it?\n\nOn the ArXiv version (https://arxiv.org/pdf/1712.02390.pdf) I found the following quote:\n\"These update rules are similar in spirit to methods such as Adam, but with the addition of adaptive weight noise. We note that these update rules also differ from Adam in some details: (1) Adam keeps exponential moving averages of the gradients, which is equivalent to momentum, and (2) Adam applies the square root to the entries of f in the denominator. We define noisy Adam by adding momentum term to be consistent with Adam. We regard difference (2) inessential. The choice of squaring or divison may affect optimization performance, but they don\u2019t change the fixed points, i.e. they are fitting the same functional form of the variational posterior using the same variational objective.\"\n\n1) Is this an accurate description of the differences between your method and Adam (just to check I haven't taken something out of context)?\n\n2) Just to check: you don't obtain momentum via a principled approximate inference algorithm, but instead add it in at the end?  \n\n3) Just to check: you obtain a mean-squared-gradient, rather as opposed to the root-mean-squared-gradient normaliser in Adam?  \n\n4) Overall, given that the essential insights of Adam are the combination of momentum and the root-mean-squared-gradient normaliser, in what sense can your approach be said to \"recover\" Adam as opposed to being \"similar in spirit\", as in the paper?", "title": "Question about this reference"}, "BkeCieGqn7": {"type": "review", "replyto": "BygREjC9YQ", "review": "* Description\n\nThe paper considers the following random process on the parameters z (modeled as Gaussians):\n- shrink z towards zero and add Gaussian i.i.d. noise to it.\n- update the parameters to the posterior w.r.t. a batch, where the likelihood is approximated as a diagonal multivariate normal distribution.\nThis results in a Kalman filter like updates. There have been related methods proposed performing Bayesian learning in the form of assumed density filtering, considered as separate learning algorithms. At the same time methods such as RMSprop and Adam were previously derived from completely different considerations. The work can derive these methods in the Bayesian framework with certain additional assumptions / simplifications. It allows to naturally explain tracking the gradient statistics as uncertainties and the normalization of the gradient in the existing methods as the update of the mean parameters in the Kalman filter taking into account these uncertainties. \nThe experiments on MNIST show that derived more Bayesian variants of RMSprop and Adam can improve generalization in terms of test likelihood and test error. \n\n* Assessment\n\nThe provided derivation of Bayes like learning algorithms is relatively simple and could be very useful in practice and in further improvement of the learning methods. The approximations used are not completely clear. The clarification of the idea of a separate optimization problem per variable is necessary. The provided experiments, if there is nothing subtle, are clearly done and would be sufficient.\nThere are some open questions such as: does the method in fact learn useful variances of the parameters, i.e. really performs an approximate Bayesian learning? Overall if find it a promising novel research direction of high practical relevance.\n\n* Clarity\n\nIntro:\nWhy is the unnumbered equation on page 1 is called a \u201cBayesian optimization problem\u201d? There is so many sings called Bayesian that one cannot be sure what it means. In the context of the paper it should be a Bayesian learning problem, but I do not see a posterior distribution over the parameters. Overall, I did not get the point of the discussion in the introduction and Figure 1 altogether. Everything it says to me is that global minimize coordinates are dependent through the objective. I do not see what the unnumbered equation on page 1 has to do with Bayesian inference and how the correlation of parameters in the posterior distribution is related to the dependencies in the minimizer. Could authors please seriously consider clarifying this section?\nIn what follows the paper keeps a factorize approximation to the posterior of parameters of a NN in the form of a Gaussian distribution per coordinate. It thus does not in any way avoid making this restrictive assumption.\n\nResults:\nSorry, I am not familiar with the background behind (6). Which value of z is assumed in the conditional expectation, is it conditioning on \u201cz = \\mu_{prior}\u201d? How come the approximation to the variance of the data likelihood does not depend on the data? If we make this approximation, how much it is still relevant to the Bayesian learning?\n\nWhat are the overheads of the proposed methods? I expect they scale as easily to large problems as SGD?\n\n* Experiments\n\nFrom Figure 2 it seems that BRMSprop and BAdam can achieve relatively good results for large range of eta in 10^-5 to 10^-2 and it seems from the trend that even smaller eta would work. Does it mean they do not need in fact tuning of the learning rate? \nThe experiment uses 50 epochs, do the compared methods reach the convergence? Could the authors consider an experiment running best setting of parameters per method with twice as many epochs?\nSome artificial toy experiments could be of interest. For example, consider a classification problem with a 1D Gaussian data distribution in each class and the logistic regression model with 2 parameters. Does the method approximate the posterior distribution?\n\n* Related work\n\nThe approach to Bayesian learning taken in the paper needs to be better discussed. I think it is from the family of methods known as \u201cassumed density filtering\u201d, occurring in:\nGhosh et al. \u201cAssumed Density Filtering Methods for Scalable Learning of Bayesian Neural Networks\u201d\nwith earlier works well described in \nMinka T. \u201cExpectation propagation for approximate Bayesian inference\u201d. \nIn particular equation (5) of the submission is well known.\nThe work  Khan et al. 2018 \u201cFast and scalable Bayesian deep learning by weight-perturbation in Adam\u201d also derives Bayesian learning algorithms in the forms closely similar to RMSprop and Adam and interprets the running statistics as uncertainties. However it takes the variational Bayesian learning approach, which means the reverse KL divergence is used somewhere. Could the authors discuss conceptual similarities and differences to this work?\n", "title": "Promising novel research, high practical relevance", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}