{"paper": {"title": "Imagining the Latent Space of a Variational Auto-Encoders", "authors": ["Zezhen Zeng", "Jonathon Hare", "Adam Pr\u00fcgel-Bennett"], "authorids": ["zz8n17@ecs.soton.ac.uk", "jsh2@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk"], "summary": "To understand the information stored in the latent space, we train a GAN-style decoder constrained to produce images that the VAE encoder will map to the same region of latent space.", "abstract": "  Variational Auto-Encoders (VAEs) are designed to capture compressible information about a dataset.  As a consequence the information stored in the latent space is seldom sufficient to reconstruct a particular image.  To help understand the type of information stored in the latent space we train a GAN-style decoder constrained to produce images that the VAE encoder will map to the same region of latent space. This allows us to ''imagine'' the information captured in the latent space.  We argue that this is necessary to make a VAE into a truly generative model.  We use our GAN to visualise the latent space of a standard VAE and of a $\\beta$-VAE.", "keywords": ["VAE", "GAN"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a new method for improving generative properties of VAE model.  The reviewers unanimously agree that this paper is not ready to be published, particularly being concerned about the unclear objective and potentially misleading claims of the paper. Multiple reviewers pointed out about incorrect claims and statements without theoretical or empirical justification. The reviewers also mention that the paper does not provide new insights about VAE model as MDL interpretation of VAE it is not new."}, "review": {"Syg8gRa6tr": {"type": "review", "replyto": "BJe4PyrFvB", "review": "The paper proposes a new method for improving generative properties of VAE model. The idea is to train VAE in two stages: at first, train the vanilla VAE, then at the second stage freeze the encoder part and train the decoder part as a GAN generator with an additional regularizer which encourages cycle consistency in the latent space. Also the authors claim that other VAE-GAN hybrids which try to improve VAE model are \u201cmisguided\u201d and poor samples and reconstructions of VAE are the consequence of minimum description length problem. \n\nConcerns:\n1) The main concern about this paper is the inaccuracy and very general statements without theoretical or empirical justification. For example, the authors claim that \u201cthe whole point of VAEs is to capture only compressible information and discard information specific to any particular image\u201d. What is the definition of \u201conly compressible information\u201d or \u201cinformation specific to any particular image\u201d? Is there an experiment which can support this statement? Other examples of such general statements: \u201cstrength of a VAE is that it builds a model of the dataset that does not over-fit\u201d, \u201cthe latent code does not contain enough information to do the reconstruction\u201d, \u201cVAEs are not broken and \u201cfixing\u201d them is actually likely to break them\u201d. \n2) Poor experiment comparisons with other baselines. The authors compare their method only with the vanilla VAE which is clearly insufficient. The authors claim that other VAE-GAN hybrids break the \u201cstrength of the VAE\u201d. Could you please provide examples of problems and provide experiments where VAE-GAN baselines will be worse than VAE or the proposed method?\n3) The paper structure is very confusing. The main part and experiments part are mixed. Therefore, it is hard to follow the text. The experiment setup is not clearly stated. For example, it is unclear for which dataset luminance-normalized Laplacian was computed. \n\nOverall, the paper proposes a new method and  gives an alternative view on the VAE model. However, statements from the paper are very pretentious and are not rigorously proven. Also there are not empirical comparisons with other VAE-GAN hybrids. Therefore, I would suggest rejecting the current version.\n\n--------------------------------------------------------\nUpdate after author rebuttal\n\nThank you for your thoughtful response. However, I still think that the paper does not give new insights about VAE model and has poor experiment justifications of their statements. Considering MDL interpretation of VAE it is not new (see [1]). Therefore, the contribution of this paper is very limited. \n\nAfter reviewing the other reviews and the author rebuttal, I do not change my original score.\n\n[1] Xi Chen et al.,  Variational  Lossy  Autoencoder, 2016\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 4}, "Skg_RY4tdB": {"type": "review", "replyto": "BJe4PyrFvB", "review": "This paper proposes to augment the VAE objective with two additional terms: (1) a GAN-like objective that ensures that the generated samples are not distinguishable from real samples, and (2) an additional regularizer to make sure that the latent variable that generated the image can be reconstructed from the VAE encoder. \n\nOverall the paper is written very well and was a pleasure to read. In particular I appreciated the differences between the present work and the many VAE/GAN hybrids in section B of the Appendix. However, after reading the paper a couple of times, it was still not clear to me what the \"main point\" of the paper is. To be more specific:\n\n- From the perspective of obtaining good unconditional samples from a VAE-like model, the authors do not compare their approach against like methods like IntroVAE (https://arxiv.org/pdf/1807.06358.pdf) which also adds an adversarial objective to the VAE. Qualitatively at least, it seems apparent that the generated samples here are worse in quality that the IntroVAE work.\n\n- From the perspective of learning more faithful reconstructions, it seems clear from Figure 5(a) that a regular beta-VAE does better. And to me it is not clear why we would want faithful reconstructions?\n\n- The main novelty of the proposed method is in adding an extra term to the generator loss controlled by lambda (equations 1 and 4). In the appendix the authors experiment with varying the lambda parameter and find that generally having lambda > 0 produces \"better\" metrics, though the choice of these metrics are somewhat questionable. It would be great to see the generated samples as \\lambda is varied (to me this is more interesting than seeing the generations as \\beta is varied).\n\nFurther, I take several issues with the authors' point of view regarding the current state-of-affairs in VAEs:\n\n1. \"However, one of their perceived problems is their reconstruction performance.\" \n\nI somewhat disagree with this characterization. Sure, there has been much work on modifying the VAE objective such that the latent variable is not ignored (i.e. posterior collapse), but the point of these works is not to get \"better reconstruction performance\". \n\n2. \"However, having a model that does not over-fit the dataset can be useful, but in this case the decoder of a standard\nVAE should not be regarded as a generative model\u2014that is not its purpose. If we wish to generate\nrealistic looking images we need to imagine the information discarded by the encoder\"\n\nI am not sure I understand this characterization. The decoder is by definition a generative model. Depending on the decoder/encoder capacities (e.g. PixelCNN decoder vs DeConvNet decoder), different types of information will be encoded in the latent space.\n\n3. \"The job of the decoder in a variational autoencoder is to reconstruct the image only using information\nthat can be compressed. Image specific information is ignored. For example, information about the\nprecise shape of an object is probably not compressible. As a result the decoder tends to hedge its\nbets and has a blurry outline.\"\n\nAgain, all of this is dependent on how the encoder/decoder is parameterized. I do not agree that \"information about the precise shape of an object is probably not compressible\". For example see https://hal.archives-ouvertes.fr/hal-01676326/document\n\n4. \"VAEs are often taken to be a pauper\u2019s GAN. That is, a method for generating samples that is easier\nto train than a GAN, but gives slightly worse results.\"\n\nMy view on VAEs is that they are a way of training latent variable models with likelihood training. One potential application of this is to generate samples, but that is not the only (nor the primary) application. \n\n\nFinally, I hope I am not coming across as nitpicking or overly combative, but I am genuinely confused as to the problem that this paper is addressing. I look forward to discussing further among other reviews and the authors during the rebuttal period.\n\n[Response to author rebuttal]\n\nThank you very much for your thoughtful response. However, I must say that I just fundamentally disagree with motivations and some of the statements made in the paper. In particular, there seems to be some conflation (I could be misunderstanding) of \"good reconstruction\" vs \"good generation\". For example:\n\n- \"Our point is that it does not generate a realistic image by design (in contrast to a GAN which is designed to generate a realistic image).\" \n\nI am not sure I understand. Would you say an autoregressive model (e.g. PixelCNN) does not generate realistic images by design? (Clearly, they do!). Would you say that autoregressive language models (e.g. GPT2) do not generative realistic language by design? The argument seems to be that likelihood-based training of generative models does not, by design, encourage realistic-looking images (indeed it is true that good likelihood does not *necessarily* imply good generative models). However, there is massive empirical evidence that likelihood training does result in good generation.\n\nAfter reviewing the other reviews and the author rebuttals, I am maintaining my original score.\n\n\n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 2}, "ByeO-pZ7jr": {"type": "rebuttal", "replyto": "Skg_RY4tdB", "comment": "We are glad that the paper was a pleasure to read.  We feel it was taken\nin the spirit it was written as a thought provoking reflection on VAEs.\nWe are sad that in the end the reviewer did not see the point.  Clearly,\nthis was a failure of our paper, but we feel that one of the problems\nwas that the main novelty was taken to be LSR-GAN, which for us was just\na visualisation tool.  The main novelty was the interpretation of the\nVAE and drawing conclusions from that interpretation.\n\nTo clarify why LSR-GAN is useful.  Although VAEs give reasonable\nreconstructions on training and test data, for many data sets they have\npoor performance in generating images from randomly sampled parts of the\nlatent space.  Clearly this is because the VAE decoder has not been\ntrained in these regions.  However, these are regions of great interest\n(the latent representation of VAEs are there real strength and we would\nlike to these to be meaningful away from training images).  LSR-GAN give\nus much better images when we sample from the latent space (as evidenced\nby FID scores for example).  Due to page limits we had to relegate\nexamples of these to Appendix E, but these are remarkably more realistic\n(to be fair surreal) than we would get from using the VAE decoder.\n\n*** 1.\n\nWe don't quite see what you are disagreeing about.  We called this a\n\"perceived problem\" because, as we strongly argued, VAEs are not\ndesigned for reconstruction.  Clearly a lot of work on VAEs is focused\non the latent representation they learn.  Nevertheless, some of the\nliterature has focused on improving performance.  We have no desire to\nbelittle work, for example, on preventing latent variable collapse, but\nwe also think it is worth stating that this could also be viewed as\nautomatic dimensionality selection mechanism, which is a desirable\nproperty rather than a problem that needs to be fixed.\n\n*** 2.\n\nWe are trying to present a new perspective and in doing so we wanted to\nmake a strong (provocative?) statement.  We accept that the decoder is a\ngenerative model in that it generates something.  Our point is that it\ndoes not generate a realistic image by design (in contrast to a GAN\nwhich is designed to generate a realistic image).  Undoubtedly different\ninformation will be stored in the latent space depending on the decoder,\nbut \"incompressible\" information (e.g. information that appears only in\none training example) won't be stored for any decoder.\n\n*** 3.\n\nWe tried to give a concrete example (inserting the word probably because\nwe are speculating).  Our point was to explain why you would expect some\nblurriness.  A precise description of an image would seem incompressible\n(otherwise VAEs should achieve zero reconstruction error which in our\nexperience is quite far from what we find).\n\n*** 4.\n\nWe totally agree!  We concede that it would be useful to make this\nexplicit.  Our whole paper is to point out that a VAE shouldn't be\nviewed as a pauper's GAN.  By design the decoder is not meant to produce\ngood reconstructions.\n\nWe have clearly failed to communicate the purpose of this paper.  We\nbelieve the MDL interpretation of VAEs provide many novel insights.  We\ncannot claim the MDL interpretation is new, but the insights are not\nwell known.  They include the observation that the decoded images should\nbe poor quality, that latent variable collapse is desirable, that latent\nspaces need to imagined because they won't capture all the information\nin an image and even that the correct form of the reconstruction term is\n$N \\log(\\sigma) + const$ (i.e. the entropy of $N$ independent variables from a\ndistribution $N(0,\\sigma^2)$).  These are not revolutionary, but at the same\ntime they are not well known and we feel they are important.", "title": "Response to Reviewer #3"}, "BJlR2h-Qjr": {"type": "rebuttal", "replyto": "Bkxzxw96Kr", "comment": "We strongly defend our claim about the form of the reconstruction error.\nAlthough this was made more as a side-remark we believe that it is very\nimportant to calculate the reconstruction error as we stated.\n\nThe reconstruction error is actually an expected negative log\nprobability, which by definition is the entropy of the error.  Under the\nstandard assumption that the errors are zero mean normal variables this\nterm should be the entropy of a Gaussian/normal distribution which is\nindeed equal to $\\log(\\sigma^2)/2+const$ as we assert.  The exponential\nof the negative entropy for a zero mean Gaussian is not a Gaussian\ndistribution (or a distribution at all) so we don't accept your\nstatement that this has to be wrong---it follow algebraically by putting\nin the definition of $\\sigma^2$ into equation (2).  If you want to put\nin an unbiased statistical estimator for the variance it just changes\nthe constant term by a very small amount.  Given we don't know $\\sigma$\na-priori using an empirical estimate seems the only alternative (given\nthe number of errors we also expect that the empirical estimator (biased\nor unbiased) will be a very good approximation.\n\nWe accept that our term is not common (we have not seen it used\nanywhere), but we are entirely confident that it is correct!\n\nThe assertion that you can use any value of sigma you choose, we find\ndifficult to understand.  If instead of using $\\sigma^2=1/2$ we used\n$\\sigma^2=1/20$ this would be equivalent to changing the relative\nproportion of the KL term by a factor of 10.  In the \"true\" VAE this\nshould be the a true probability of making an error.  Given the errors\nhave a variance, we should use that variance.  When you do this you will\nget the expression we derived.\n\nThe reconstruction error will only overflow if the mean squared error\nfor all pixels, colour channels and the whole mini-batch is zero.  We\nhave never experienced this!  The modification is very easy to\nimplement.  All you need to do is replace MSE in the loss function by\nN*log(MSE) where N is the number of colour channels times the number of\npixels times the size of the minibatch.  In our experience this is\nequally easy to learn and will give you better reconstructions (although\nthis could depend on the scaling you use in your implementation).\n\nAlthough in the MDL interpretation it is clear that the negative\nlog-probability term is just an entropy which you wish to minimise, it\nis also the case in the original paper that you must minimise the\nlog-probability of the errors.  Arbitrarily choosing $\\sigma^2=1/2$ no\nlonger makes this a probability of anything meaningful.\n\n*** Comments on experiments and related work\n\nThere are no experimental comparisons because that was not the point of\nthe paper.  The paper was about understanding VAEs.  LSR-GAN was\nintroduced as a tool to visualise the information stored in the latent\nspace of a traditional VAE.  Other VAE-GAN hybrids are not intended to\ndo this so a direct comparison makes no sense.\n\n*** Other Sections\n\nThe whole point of the paper is to explain what at VAE does using the\nMDL framework.  This, in our opinion, provides some new insights.\nWithout Section 3 there is really no content.\n\n*** Additional Feedback\n\nOur comment \"VAEs are often taken as a pauper's GAN\" was to succinctly\ncapture two aspect of VAE (a) they are easier to train than GANs and\n(b) they produce blurrier outputs than GANs.  However, the whole paper\nis written to point out that VAEs produce blurry reconstructions by\ndesign.  It is not the real point of VAEs.  VAEs produce rich latent\nembeddings of images.  Our intention was very far from insulting VAEs\n(clearly we would not have written the paper if we felt VAEs were\nuninteresting).  We can, of course, rephrase this, we had deliberately\nchosen the phrase to capture one view of VAEs, but the point of the\npaper is to show that this view is superficial and misses the point.", "title": "Response to Reviewer #2"}, "HylIK2ZQjr": {"type": "rebuttal", "replyto": "Syg8gRa6tr", "comment": "*** Concern 1)\n\nWe believe the statements we have made are justified both theoretically\nand empirically.  We take the statements highlighted\n\n\"the whole point of VAEs is to capture only compressible information and\ndiscard information specific to any particular image\"\n\nThis is a crucial argument we believe is justified by the minimum\ndescription length (MDL) interpretation of the VAE loss function.  The\nloss function consists of an expected negative log-probability or the\nentropy of the errors.  By Shannon's theorem this is also the minimum\ncode length needed to communicate the error.  The KL term is a relative\nentropy that has an information theoretic interpretation of the code\nlength needed to communicate a random variable with distribution $q(z|x)$\nusing a coding with an underlying distribution of all code words of\n$p(z)$.  Therefore we can see the VAE loss function as the cost of\ncommunicating the original image by sending a code $z$ that the decoder\ndecodes as $\\hat{x}$ and an additional cost of repairing the image by\ncommunicating the errors.\n\nAs a consequence, if a single image contains a butterfly in a corner, it\nmakes no sense for this to be encoded in the latent space as this would\nbe at least as costly as communicating the error.  If many images\ncontained butterflies then you can reduce the overall code length by\ncoding this into the latent space.  In contrast, an autoencoder would\nattempt to encode all information in the latent space.  This is what we\nmean by \"only compressible information\" and \"information specific to a\nparticular image\".  We believe these are theoretically justifiable\nstatements by the MDL interpretation.\n\n\"strength of a VAE is that it builds a model of the dataset that does\nnot over-fit\"\"\n\nThis follows theoretically from the above argument, but it leads to an\ninteresting prediction.  A normal autoencoder (or a beta-VAE with\nbeta<1) will provide better reconstructions of the training images (we\ndidn't explicitly show this, but this is quite well known).  However, it\nwon't produce better reconstructions on unseen testing images because the\nextra-information encoded by the autoencoder is image specific\n(over-fitting the training set).  The graphs in Figure 5 confirm this\nprediction empirically showing for a beta-VAE the reconstruction error\non images from the CIFAR-10 test set are the same for $\\beta\\leq1$ and\nonly become worse when $\\beta>1$.\n\n\"VAEs are not broken...\"\n\nOur point is that VAEs are designed to balance the two terms in their\nloss function which will lead to poor reconstructions.  Fixing this\nwould mean that image-specific information was encoded in the latent\nspace which is precisely what VAEs are designed to avoid.\n\n*** Concern 2)\n\nAs we explained this paper is about different view of VAEs.  LSR-GAN was\nnot designed to produce better images than other GAN-VAE hybrids its\npurpose is to show (imagine) what is encoded in the latent space of a\nVAE.  Perhaps it was a mistake to introduce LSR-GAN as it has clearly\nbeen a distraction, but its purpose was to make a point that the latent\nspace of a VAE has to be imagined as it does not encode all the\ninformation in an image.\n\n*** Concern 3)\n\nClearly we have to accept that the paper has failed to communicate what\nwe set out to communicate.  However, as we pointed out this is different\nto most papers.  Our new method is a visualisation tool to understand\nVAEs rather than yet another attempt to generate high quality images.\n\n*** General Comment\n\nWe feel this is a bit harsh.  A lot of the argumentation seems to have\nbeen missed leading to an impression that our statements were rhetorical\nrather than based on a sound theoretical basis.  We chose a tone to\nhighlight that we are challenging common held beliefs, but we can clearly\nchange this if has caused confusion.", "title": "Response to Reviewer #1"}, "B1eCVhZXjS": {"type": "rebuttal", "replyto": "BJe4PyrFvB", "comment": "We would like to thank the reviewers for the time spent on reviewing the\npaper.  The purpose of the paper is to take a step back and ask the\nbigger question \"What is the purpose of a VAE?\" (why add the KL term).\nWe believe understanding this is important when trying to improve their\nperformance.  It is not our intention to propose a new network that\nbeats the current state-of-the-art on some metrics.  We propose LSR-GAN\nas a tool for visualising what is encoded in a standard VAE (as we argue\nthis involves imaging details that won't be encoded).  As other VAE-GAN\nhybrids were not designed for this purpose a direct comparison seems\nredundant.", "title": "To All Reviewers"}, "Bkxzxw96Kr": {"type": "review", "replyto": "BJe4PyrFvB", "review": "Summary:\nThis paper proposes a hybrid VAE-GAN model, called the latent space renderer-GAN (LSR-GAN), with the goal to \u201cimagine\u201d the latent space of a VAE, and to improve the decoding and sampling quality of a VAE. First, a VAE-like model is trained, after which the encoder weights are frozen, and the decoder is trained as the generator of a GAN (together with an auxiliary discriminator). The generator loss also contains a reconstruction-like term in the latent space, described by the negative log density of the encoding distribution of the original latent conditioned on the output of the generator: -log q(z|g(z)). \n\nDecision: reject\nThis paper contains incorrect claims. Leaving those mistakes aside, the experiments don\u2019t lead to new insights and no comparison against other VAE-GAN hybrids is made. \n\nSupporting arguments for decision:\nIn the introduction the authors state \u201cThis is a consequence of the well known evidence lower bound or ELBO objective function consisting of a negative log-probability of generating the original image from the latent representation (this is often implemented as a mean squared error between the image and the reconstruction, although as we argue in Appendix A this term should be proportional to the logarithm of the mean squared error)...\u201d. This statement is surprising and I don\u2019t see how it can be correct. If -log p(x|z) is something like the log of the mean squared error, then the density p(x|z) should be a squared error function, which is not even a valid distribution.\n\nTo be more precise, according to the authors, if one takes a Gaussian p(x|z), the reconstruction error should be modeled with  \n\n-log p(x|z) = log 1/N sum (x_i - \\hat x_i)^2  + const \t\t\t\t\t(1)\n\nwhere \\hat x_i is a function of z. Note the logarithm on the right-hand side here. In appendix A, the authors observe that most other implementations instead optimize \n\n-log p(x|z) = sum_{i=1}^N (x_i -\\hat x_i)^2/(2\\sigma^2) + N/2 log(2 pi sigma^2) \t\t(2)\n\nWhere often sigma is set to \u00bd (so that the last term on the lhs of (2) drops out when gradients are taken). The authors claim the latter is incorrect because sigma should be equal to the (biased) empirical variance. They then insert sigma=empirical variance into (2), take derivatives while ignoring the dependence of sigma on \\hat x_i, and then arrive at something like eq. 1. They also argue that those implementations that use sigma=\u00bd are actually optimizing a beta-VAE because of this \u201cincorrect\u201d prefactor of the reconstruction error. \nI\u2019m confident that the above claims are not correct. \nAs an example, using a Gaussian decoder distribution requires parameterizing the mean and variance of that Gaussian distribution as a function of z. Here, setting sigma = \u00bd (as is commonly done in other literature) is valid, contrary to the above claim. One is always allowed to just set the variance to a constant and ignore its dependence on z. This just leaves you with a less flexible distribution to model p(x|z).\n\nThe authors appear to use (1) as the reconstruction error term in the ELBO during VAE optimization, and then play with different prefactors of the KL term similar to beta-VAEs. The combination of the reconstruction error (1) and the prefactor no longer makes this a VAE, but more like a regularized auto-encoder with an unconventional reconstruction error that should not be interpreted as coming from the negative of a log density. Due to the logarithm in front of the mean squared error, the loss function is less sensitive to large errors in reconstruction. It is surprising that the reconstruction error does not cause overflow as log (0) --> - infinity. \n\nComments on experiments and related work: \n- Experiments only show images of reconstructions of a VAE and LSR-GAN, a mean squared error plot of reconstruction errors and an accuracy plot of a classifier evaluated on the reconstructions produced by both models. In terms of MSE the LSR-GAN actually performs worse than their VAE baseline. The authors also train a classifier on cifar-10 and measure its accuracy using the ground truth labels and the reconstructed images of a VAE and LSR-GAN. Here the LSR-GAN performs marginally better than the VAE, but the overall accuracy is very poor.\n- No experimental comparison is made against other VAE-GAN hybrids. Related work on hybrid VAE-GANS is discussed in the appendix, not in the main paper.\n\nOther sections:\n- Section 3 contains a very long interpretation of the minimum description length (MDL). It is unclear what the goal of this section is, as it does not lead to any results, nor does it help bring the point across as to why the proposed model is good at imagining the latent space of a VAE.\n\n\nAdditional feedback to improve the paper (not part of decision assessment):\nThe additional loss term of the generator is very similar to what is used in reweighted wake sleep, although this is not mentioned. It could be worth making a connection here. \nIn the conclusion the authors state \u201cVAEs are often taken to be a pauper\u2019s GAN\u2019. This is not a very scientific statement and can be perceived as insulting for various reasons. Please rephrase this.\n", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 3}}}