{"paper": {"title": "Learning Python Code Suggestion with a Sparse Pointer Network", "authors": ["Avishkar Bhoopchand", "Tim Rockt\u00e4schel", "Earl Barr", "Sebastian Riedel"], "authorids": ["avishkar.bhoopchand.15@ucl.ac.uk", "t.rocktaschel@cs.ucl.ac.uk", "e.barr@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "summary": "We augment a neural language model with a pointer network for code suggestion that is specialized to referring to predefined groups of identifiers", "abstract": "To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.\n The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc)."}, "review": {"SysuqayUe": {"type": "rebuttal", "replyto": "B1iOZNEVe", "comment": "Thank you for your comments. Note that the attention mechanism is not hardcoded, it is learned from data. What is hardcoded, however, is the heuristic to restrict the attention to context representations at positions of the previous n identifiers instead of all context representations of the previous n tokens. Which one of these identifiers is chosen and whether the plain neural language model or the pointer network is used for predicting the next token, is a soft decision that is learned (please also see our response to AnnoReviewer1). Also note that in principle there is no restriction on what information from the context is used as we are using a neural language model that can store information in its memory (as opposed to ngram models where the context is limited to the previous n-1 tokens). In theory this could be information from any previous timesteps (so it is as general as it can be), but in practice it is hard for the model to learn such long-range dependencies. That\u2019s why we use the pointer network to provide a way to refer back to identifiers that have been defined many tokens in the past. We agree that it would be great to see a comparison on py150, but please note that this corpus appeared very few days before the ICLR deadline, so we have to defer a comparison to future work. \n", "title": "Re: attention mechanism hardcoded?"}, "BJQGcak8x": {"type": "rebuttal", "replyto": "HJ7hhhfVl", "comment": "Thank you for your review and feedback. We have now updated the paper to include the link to the Python corpus. Subsequent to your comment, we ran the baseline models with a batch size of 30 and noted that they achieved worse validation and test set perplexities. We have therefore chosen to continue reporting the better results of the baseline models with a batch size of 75 and have made this clear in the paper. ", "title": "Re: Review"}, "B180t6JUg": {"type": "rebuttal", "replyto": "SJ8ttDXNx", "comment": "Thank you for taking the time to review our paper!", "title": "Re: no title"}, "B1iOZNEVe": {"type": "rebuttal", "replyto": "S1SnOnTQg", "comment": "The claim that this approach is more flexible than the papers mentioned above does not really make sense. If anything, the attention mechanism used here is somewhat hardcoded. That is, the prediction of an identifier can only depend on other identifiers. \n\nThis is a more restricted requirement than prior work mentioned above which discovers the elements upon which a prediction should be conditioned, so it is more general. In terms of assumptions it only assumes a parser for the language, which all programming languages have.\n\nI would also be happy to see a comparison on the py150.php dataset in terms of precision. As is, the numbers in this paper do not look to be competitive.", "title": "attention mechanism hardcoded?"}, "BJHDOOM4g": {"type": "rebuttal", "replyto": "BJgR3jbEg", "comment": "Thank you for your review and comments! I just want to clarify that the attention mechanism is learned. What is fixed is the heuristic to only attend over the previous N outputs of an RNN at positions of identifiers instead of all kinds of previous tokens. As you pointed out, this heuristic of tracking previously used identifiers in scope has been used before. What we believe is novel is the following. For the previous N identifiers, the model is learning to first choose between a normal neural language or copying one of the N previously seen identifiers. When copying, the choice for one of these identifiers is modeled via a learnable attention mechanism. Also note that for this attention mechanism, the output representations at positions of identifiers is used, not the representation of the identifier in the input vocabulary. That is, the output representation at a position of an identifier can contain contextual information. We will update the paper to make this clearer.", "title": "The attention mechanism is learned"}, "S1SnOnTQg": {"type": "rebuttal", "replyto": "ryV_9Rrmx", "comment": "Thank you for the comments and suggestions and for pointing us to indeed related recent work. We will certainly clarify equation 10 and add a reference to Allamanis et al (ICML\u201916). \n\nProbabilistic Model for Code with Decision Trees by Raychev et alia is closed-access and was published on the 3rd of November. Realistically, we had no way of knowing about it prior to the submission of our own work to ICLR on the 4th of November.  Like our work, their work also provides a Python corpus; this is all to the good: The community now has two Python corpora to investigate and evaluate systems on. Raychev et alia report higher accuracy of predicting identifiers, but do so trained against raw, as opposed to normalized, tokens, making direct comparison with our results difficult. They have also not yet published the tooling underlying their work, again impeding direct comparison.\n\nWith regards to Learning Programs from Noisy Data by Raychev et alia (POPL\u201916): they learn long-range dependencies by learning if-else programs, an encoding of decision trees, over ASTs and evaluate on the task of completing field or API names following a dot. In contrast, our unrestricted suggestions via the soft-attention mechanism over a history of previous tokens (or identifiers in the case of the Sparse Pointer Networks) is in principle a more flexible approach to code suggestion, as the model can learn from code usage which parts of the previous context are most relevant for suggesting the next token. More importantly, we make very few assumptions about the underlying programming language.\n", "title": "Re: Prior work and suggestions"}, "ryV_9Rrmx": {"type": "rebuttal", "replyto": "r1kQkVFgl", "comment": "Some of the claimed contributions the paper makes already exist in prior work. For instance:\n\n\u2192 There is already a Python data set available: http://www.srl.inf.ethz.ch/py150.php. It contains 150k files collected from GitHub and is used in the OOPSLA\u201916 paper below.\n\n\u2192 The paper misses critical related work which has higher precision of predicting Python code than reported here (the n-gram baseline is several years old):\n\nProbabilistic Model for Code with Decision Trees, OOPSLA\u201916\nhttp://dl.acm.org/citation.cfm?doid=2983990.2984041\n\nThat work achieves 51% accuracy for predicting identifiers (see Table 3), significantly higher than the 30% reported in this submission. In fact, the model of OOPSLA\u201916 (as well as those of POPL\u201916 and ICML\u201916 by Raychev et.al, Bielik et.al.), already learns long-distance relationships and need not be hardcoded as done via the M_t matrix in this paper.\n\n\u2192 There is also a missing citation to a paper that already uses attention networks for code (summarization in this case):\n\nA Convolutional Attention Network for Extreme Summarization of Source Code, ICML\u201916\nhttps://arxiv.org/abs/1602.03001\n\n\nFew suggestions/questions for improving the paper:\n\u2192 It would be useful if the authors could provide a more detailed breakdown of the model performance for different types of program tokens (e.g., constraints, methods, identifiers, control structures, etc.) as provided in prior work.\n\n\u2192 What is the motivation of learning over tokens instead of Abstract Syntax Trees? One would assume that the purely syntactic information provided by the tokenization is less suitable as it includes many irrelevant tokens (e.g., semicolons, function brackets, commas, indentation).\n\n\u2192 It would be interesting to include more evaluation of what the network is able to learn. In particular, one would expect that it would learn when to predict and identifier (in which case the identifier attention should be high) and when to predict some other program element. By inspecting the example program in the appendix (Fig. 4.) it however seems that only in 1 out of 4 cases when a reused identifier is expected the weight for the attention is high. However this is likely not representative of the full dataset. This might also be one of the reason why the results in top 1 prediction are in fact worse compared to LSTM with attention mechanism. \n\n\u2192 Minor: it would be useful to clarify what equation 10 does. In particular the size of vector s_t is never defined as well as the index j. From what I understand the size of s_t is the size of the identifier vector vocabulary and the equation 10, initializes values of those elements found in vector m_t to corresponding values from \\alpha and the remaining to -C.", "title": "Prior Work & Suggestions"}, "SkSu-rB7e": {"type": "rebuttal", "replyto": "B1C8dofXe", "comment": "Thank you for your question! We don\u2019t have a strong intuition as to why that is the case. However, top 1 accuracy of predicting the next Python token is only a proxy to the actual usefulness of the suggestion model, as in a single context there might be various appropriate tokens that could continue the code. Since the main task of a code suggestion system is to give these options to the programmer to chose from, we believe that top 5 accuracy might be a more sensible metric in terms of actual code suggestion usefulness.", "title": "Re: Table 2"}, "B1C8dofXe": {"type": "review", "replyto": "r1kQkVFgl", "review": "Thanks for the paper, it was mostly clear and well-motivated. I just wonder why it seems to be doing worse with the sparse pointer network on accuracy (Table 2)? Do you have some intuition for that?This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope. ", "title": "Table 2", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJgR3jbEg": {"type": "review", "replyto": "r1kQkVFgl", "review": "Thanks for the paper, it was mostly clear and well-motivated. I just wonder why it seems to be doing worse with the sparse pointer network on accuracy (Table 2)? Do you have some intuition for that?This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope. ", "title": "Table 2", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJ9IMIk7l": {"type": "rebuttal", "replyto": "BJZPZKizg", "comment": "Thank you for your comment and question! The use of a smaller batch size for the Sparse Pointer Network is the result of GPU memory and time constraints. We agree that this may have a small effect on the perplexity and are currently working on updating our results using the same batch size for all models. We were also unable to test the Sparse Pointer Network with a history length of 50 due to computational constraints but wanted a stronger baseline than an attention model that only has access to the past 20 output representations. We found it encouraging that the Sparse Pointer Network is on par or better than the attention model with a history of 50.", "title": "Re: Clarification re: batch sizes and history length "}, "BJZPZKizg": {"type": "review", "replyto": "r1kQkVFgl", "review": "I was initially confused regarding the filtered view / sparse pointer concept but Figure 1 made the definition clear. I feel this is most likely due to me considering \"identifier\" as a more general concept than the identifiers used in code modeling / IDEs.\n\nI am curious why the batch size jumps between the sparse pointer network (batch = 30) and the attention model (batch = 75) however. At least for word level language modeling, batch size can have an impact, usually with larger resulting in a worse perplexity.\n\nI'm also curious if you tried the sparse pointer network with a history 50 identifier representations, as you did with the attention network. Was there a reason you didn't? An equivalent history window would allow a more direct comparison for the sparse pointer network, especially given the attention model with a history of 50 is beating or near equal to the sparse pointer on some metrics.\n\nI look forward to seeing the Python corpus. It's likely to result in some very interesting continued work in this domain!This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. Code suggestion seems an area where attention and/or pointers truly show an advantage in capturing long term dependencies.\n\nThe sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board. Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes. Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.\n\nThe construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO). Given that code suggestion seems an interesting area for future long term dependency work, it may be promising as an avenue for future task exploration.\n\nOverall this paper and the dataset are likely an interesting contribution even though there are a few potential issues.", "title": "Clarification re: batch sizes and history length", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJ7hhhfVl": {"type": "review", "replyto": "r1kQkVFgl", "review": "I was initially confused regarding the filtered view / sparse pointer concept but Figure 1 made the definition clear. I feel this is most likely due to me considering \"identifier\" as a more general concept than the identifiers used in code modeling / IDEs.\n\nI am curious why the batch size jumps between the sparse pointer network (batch = 30) and the attention model (batch = 75) however. At least for word level language modeling, batch size can have an impact, usually with larger resulting in a worse perplexity.\n\nI'm also curious if you tried the sparse pointer network with a history 50 identifier representations, as you did with the attention network. Was there a reason you didn't? An equivalent history window would allow a more direct comparison for the sparse pointer network, especially given the attention model with a history of 50 is beating or near equal to the sparse pointer on some metrics.\n\nI look forward to seeing the Python corpus. It's likely to result in some very interesting continued work in this domain!This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. Code suggestion seems an area where attention and/or pointers truly show an advantage in capturing long term dependencies.\n\nThe sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board. Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes. Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.\n\nThe construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO). Given that code suggestion seems an interesting area for future long term dependency work, it may be promising as an avenue for future task exploration.\n\nOverall this paper and the dataset are likely an interesting contribution even though there are a few potential issues.", "title": "Clarification re: batch sizes and history length", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}