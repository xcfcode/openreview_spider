{"paper": {"title": "Which Model to Transfer? Finding the Needle in the Growing Haystack", "authors": ["Cedric Renggli", "Andr\u00e9 Susano Pinto", "Luka Rimanic", "Joan Puigcerver", "Carlos Riquelme Ruiz", "Ce Zhang", "Mario Lucic"], "authorids": ["~Cedric_Renggli1", "~Andr\u00e9_Susano_Pinto1", "luka.rimanic@inf.ethz.ch", "~Joan_Puigcerver1", "~Carlos_Riquelme_Ruiz1", "~Ce_Zhang1", "~Mario_Lucic1"], "summary": "", "abstract": "Transfer learning has been recently popularized as a data-efficient alternative to training models from scratch, in particular in vision and NLP where it provides a remarkably solid baseline. The emergence of rich model repositories, such as TensorFlow Hub, enables the practitioners and researchers to unleash the potential of these models across a wide range of downstream tasks. As these repositories keep growing exponentially, efficiently selecting a good model for the task at hand becomes paramount. We provide a formalization of this problem through a familiar notion of regret and introduce the predominant strategies, namely task-agnostic (e.g. picking the highest scoring ImageNet model) and task-aware search strategies (such as linear or kNN evaluation). We conduct a large-scale empirical study and show that both task-agnostic and task-aware methods can yield high regret. We then propose a simple and computationally efficient hybrid search strategy which outperforms the existing approaches. We highlight the practical benefits of the proposed solution on a set of 19 diverse vision tasks.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper studies efficient strategies for selection of pre-trained models for a downstream task. The main concerns consistently raised by the reviewers were limited methodological novelty, insufficient experimental analysis, unclear findings, and positioning of the paper with respect to related work that was ignored in the initial version. After the author response, R4 raised the score to borderline accept (still indicating the paper is weak without proper comparisons with other methods), whereas all other reviewers remained negative. The paper does have merits, as the methods are simple, and the problem is very practical (and somewhat understudied). However, the AC agrees with the majority that the paper is not ready for ICLR. The novelty is limited and the paper would benefit from more experiments, such as comparisons with simple baselines like early stopping as indicated by R1 and R3, and other methods such as Task2vec which address the same problem. The authors are encouraged to revise the paper according to the reviewers comments and submit it to another top conference."}, "review": {"9E8ZWMJsska": {"type": "review", "replyto": "TmUfsLjI-1", "review": "### Summary\n\nThe paper evaluates three procedures for selecting models for transfer learning. The choices are task-agnostic selection, linear training, and the hybrid approach. They empirically show that the hybrid algorithm works the best on few-shot learning on images.\n\n### Feedback\n\n* The paper is a straightforward paper and easily understandable. The message is practical, but not very surprising. Unfortunately, it is only on image data; it would have been great if the authors had used an example from NLP too.\n* The hybrid approach is super-simple, which is nice. The results in Figure 6 confirm that how its ensemble nature helps. Although it does not necessarily outperform the linear algorithm in Figure 6. The ensembling approach also does not seems to be the optimal solution. The authors could study the generalization performance of the hybrid algorithm to provide further insights.\n* An empirical run-time analysis is missing.\n* While the authors indicate that all models perform comparatively poorly on the structured tasks, they do not provide specific insights about the root cause of this.\n* Overall, the idea is simple and practical, but the methodological contributions of this paper is rather limited.\n\n--------\n### Post-Response Update\nUnfortunately, the authors' response is not satisfactory on multiple issues. Thus, I reduce my rating by one point.", "title": "A Hybrid (Ensemble) Approach for Pretrained Model Search", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r7BM1yDl2rC": {"type": "rebuttal", "replyto": "yCFbNhLJmpG", "comment": "We thank the reviewer for his comments. However, we disagree with the proposed suggestions as they are either orthogonal to our method, or have been shown as inefficient, by other related work.\n\n\n\n*   LEEP: This method assumes a pre-trained classification head as part of the pre-trained model, which works for fully/semi-supervised. However, some models from the VTAB benchmark do not fulfill this requirement! (e.g., feature representations originating from a GAN or VAE which are part of the models in the pool)\n*   Batch Norm requires one to adjust the parameters to the new data distribution. Reporting the behaviour of using restricted finetune compute to estimate unrestricted finetune compute accuracy is possible, however that is clearly as (if not more) sensitive of hyperparameter selection. Instead, we focused on doing an extensive study of methods which are known to be less sensitive to hyperparameter search to identify the best models, which are then used with larger finetune compute.\n*   Whilst one could use \u201cMINE\u201d, it is known that estimating the MI suffers considerably from a bias - variance tradeoff and no method bypasses those issues (see \u201cPoole, B., Ozair, S., Van Den Oord, A., Alemi, A., & Tucker, G. (2019, May). On Variational Bounds of Mutual Information. In ICML.\u201d). The suggested approach \u201cMINE\u201d falls into this category and additionally has a large computational requirement as it requires to train a neural network which makes it unusable as a \u201ccheap\u201d proxy task.\n*   We restricted ourselves to the 1K samples setting from VTAB on purpose, as it is known that this yields the largest gain in using transfer learning compared to training from scratch. The impact of the size of the model pool is directly visible via the novel definition of regret which we consider as a major contribution compared to other performance proxies.", "title": "Discussion"}, "prEXPl82hel": {"type": "review", "replyto": "TmUfsLjI-1", "review": "Paper summary: This paper looks at the problem of efficiently choosing pre-trained models as initialization for downstream target tasks. It compares 3 strategies, a task-agnostic one which uses imagenet accuracies, a task-aware one which uses the acccuracy of linear classifiers on fixed representations, and a hybrid one which combines the two.\n\nPros:\n+ The evaluation is fairly thorough. I especially like the fact that the authors consider the different axes along which pre-trained models differ (model capacity, generalist/experts etc.)\n+ The pool of downstream datasets is large.\n+ The suggested strategy is simple and easy to implement. \n+ The problem is significant in practice since almost all practical applications of neural networks have this prroblem, and the gains seem large. I wish there was more work on this problem.\n\nCons:\n- The biggest issue is that this paper ignores several important papers publiished before on this problem. Especially of note is the Task2Vec approach, which computes model and task embeddings. I would like comparisons both in terms of accuracy/regret as well as computational cost:\nAlessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, Pietro Perona; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 6430-6439\n\nOther papers that are also relevant and should be cited and comparisons discussed:\nBishwaranjan Bhattacharjee, John R. Kender, Matthew Hill, Parijat Dube, Siyu Huo, Michael R. Glass, Brian Belgodere, Sharath Pankanti, Noel Codella, Patrick Watson; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020, pp. 760-761\n\nAmir R. Zamir, Alexander Sax, William Shen, Leonidas J. Guibas, Jitendra Malik, Silvio Savarese; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3712-3722\n\n- The approach is not particularly novel. There is also no novel technical insight that explains the results.\n\n- The use of the JFT dataset hampers reproducibility since the dataset is not public. I'd like to see results with JFT excluded.\n\nFor acceptance, I would definitely want to see the first of these convincingly addressed.\n[Updated rating]", "title": "Good baselines for model selection, but paper ignores many prior papers on this problem", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "26uo1ZWTvt": {"type": "rebuttal", "replyto": "9E8ZWMJsska", "comment": "We thank the reviewer for the thoughtful comments and are grateful to see that the simple yet consistent proposed solution is appreciated. \n\n_[Unfortunately, it is only on image data; it would have been great if the authors had used an example from NLP too.]_\n\nWe agree that extending this work to include NLP tasks is an important followup project. At this stage, fine-tune strategies on its own are less explored and unclear in NLP compared to vision tasks (e.g., pre-trained models are typically extended by more than one linear classification layer, or the representations are not always taken from the last pre-logit layer).\n\n_[Although it does not necessarily outperform the linear algorithm in Figure 6.]_\n\nWe see Figure 6 (which is now Figure 7 in the revised version) as a confirmation of some of our claims, rather than a weakness:\n\n\n\n*   Since hybrid with budget B is based on linear on budget B-1, it is **expected that the graphs are fairly similar when experts are the best models** (which is the case in restricted pools)\n*   It shows the **necessity of including the task-agnostic method** - on ALL pool we clearly see that linear is not able to choose the best model\n*   We want to emphasize that Figure 6 cares only about winners, which differs from the rest of the paper where we look at the notion of regret\n\n_[The ensembling approach also does not seems to be the optimal solution. The authors could study the generalization performance of the hybrid algorithm to provide further insights.]_\n\nThe generalization property of the hybrid strategy using a budget of B, by definition, comes directly from the generalization of the task-aware strategy for B-1 and the first pick of the task-agnostic strategy. Giving any generalization bounds for training or fine-tuning deep neural networks is already a hard and currently open research problem, which we do not aim at solving here. Instead, we showcase empirical failures of each strategy which, interestingly, do not overlap much, enabling consistent improvements using the proposed hybrid strategy.\n\n_[An empirical run-time analysis is missing.]_\n\nEfficient implementation of all methods, which is crucial for a fair comparison, is out of the scope of this work. At this stage, understanding the limitations of using task-aware (especially a linear classifier) or task-agnostic search methods is the founding block. Run-time analysis, extension towards related work that provides cheaper estimators (such as LEEP (Nguyen et al., 2020),  NCA (Tran  et  al.,  2019) or H-Score (Bao et al., 2019)), in order to speed up the entire search process, are all valid future projects that our work opens.", "title": "Response to AnonReviewer2"}, "dii8uDOz4YU": {"type": "rebuttal", "replyto": "prEXPl82hel", "comment": "We thank the reviewer for the thoughtful comments and pointing out the potentially relevant related work. We have updated the main body of the paper, in particular the \"Background\" section and the \"Other related work\" section to improve the positioning of our work, and added Figure 2 for further clarification. That being said, we believe that our contributions are relevant to the research community and the practitioners. \n\n[Related methods] \n\n\n\n*   Task2Vec (and Model2Vec) (Achille et. al.):\n    *   We highlight multiple aspects when comparing \u201cmeta-learned task-aware\u201d strategies such as Model2Vec to our choice for search strategies (also described in the revised paper)\n    *   [Computational comparison] Assuming access to M models, our proxy tasks certainly require more compute time than Model2Vec, O(M) compared to O(1) for the search part. However, **the computational requirement for Model2Vec is shifted to the meta-learn algorithm** (which needs to be rerun from scratch whenever new pre-trained models are available - a **different category of model search strategies than what we examine**), which has an asymptotic complexity of O(M) for Model2Vec.\n    *   [Hybrid helps Model2Vec] Figure 3 in the Task2Vec paper shows that the **generalist model outperforms the selected expert in 15 out of 50 cases**, sometimes by a large margin - **hybrid strategy should improve their proposed method significantly**, formulating a very interesting future research problem.\n*   P2L (Bhattacharjee et. al.):\n    *   Estimates the impact of transferring the learned representations from a source dataset (unclear how one could distinguish multiple models trained on the same dataset) by incorporating the dataset size and the divergence between the upstream and downstream dataset.\n    *   This is **orthogonal** to the goal of **selecting a pre-trained model without having knowledge of the meta-data used to train the model in the first hand.**\n*   Taskonomy (Zamir et. al.):\n    *   **We can use any model**, whereas Taskonomy is restricted to models trained on the same input with different labels.\n    *   **We examine fine-tuning**, whereas for both methods the encoder part which is transferred is not fine-tuned.\n    *   While Task2Vec and Model2Vec are directly applicable by ranking models based on their similarity in the embedding space, it's unclear how to apply the model search on new tasks using Taskonomy without semantic relations between the upstream and downstream tasks and without training a new network from scratch.\n\n_[The approach is not particularly novel. There is also no novel technical insight that explains the results.]_\n\nWe respectfully disagree with the lack of technical insights gained in this work:\n\n\n\n*   The failure of task-aware and task-agnostic methods seems to be non-overlapping, depending on the model pool inspected. This is seen through the success of our hybrid approach, a fact that is very surprising\n*   **The hybrid approach is universal**, e.g., Task2Vec would also benefit from including it\n\n_[The use of the JFT dataset hampers reproducibility since the dataset is not public. I'd like to see results with JFT excluded.]_\n\nThe pool ImageNetAccuracies in the appendix does not contain any proprietary model, and it confirms the improvement of the hybrid strategy over the linear proxy task.", "title": "Response to AnonReviewer4"}, "M3PIhWATVbj": {"type": "rebuttal", "replyto": "sw1CohP7e4o", "comment": "We thank the reviewer for the thoughtful comments.\n\n_[The major weakness of this paper is that it seems there is no consistent strategy to out-perform all other methods in every task.]_\n\nWe respectfully disagree: Hybrid strategy is **consistent across all pools** as seen in Figure 6 in the revised draft (previously Figure 5) and Figure 17 in the revised supplementary (previously Figure 16).\n\n_[Even if for the advanced strategy hybrid proposed in the latter part of this paper, the optimal pick for this hybrid method is almost identical to the linear evaluation in task-aware strategy in ResNet-50 and expert model pools.]_\n\nThis is correct and expected, since a hybrid strategy with B models receives B-1 top models from linear evaluation. The challenge is in fact to return the experts when needed (which is something other methods are usually not capable of), whilst not missing the generalist model when they perform best.\n\n_[So my biggest concern for this paper is that we don't have a take-home message, other than showing the \"No-Free Lunch Theorem\" in model selection.]_\n\nWe believe that there are several take-home messages:\n\n\n\n*   This is the **first work** that **formulates** the model-search question with a notion of **regret**, on **real-world scenarios** that are most common in practice for **the end user** - choosing the best model with minimal computational demands\n*   We carefully examine cases in which either Task-agnostic or Task-aware methods fail. It is expected that in general they perform well, however, one would like to understand how strong the failures are. In particular, we observe that **failures do not overlap too much**, which is why we believe that this large-scale study sheds a new light on these failures\n*   Finally, we propose a **simple**, **computationally feasible** search strategy - **hybrid** - that captures these failures under one umbrella, consistently outperforming other methods due to the above mentioned property of failures not overlapping often\n\n_[An interesting direction might be, how we can design a really fast approximation of fine-tuning, so that we can evaluate a model's fitness only by a few iterations (within a very short amount of training time), instead of performing full fine-tuning on the target task.]_\n\nWe believe that this is out of the scope of our work since understanding the correct method for early stopping is an interesting and difficult problem on its own. There are several reasons for not including such a study in our paper:\n\n\n\n*   The complexity coming with such an approach which goes way beyond training a linear layer. Both proxy tasks that we analyzed **require only a forward pass** (inference) through the pre-trained networks in order to get the representations\n*   Training a large network (or fine-tuning it) robustly with early stopping for mischosen hyperparameters or initialization parameters is an open research problem\n*   Fine-tuning requires additional knowledge about the architecture (e.g., knowing when the batch norm layers are used) of the networks, which a typical user would not be able to grasp for all the pre-trained embeddings. For our proxy task, **no such knowledge is required** until fine-tuning the winning models", "title": "Response to AnonReviewer3"}, "C4_UBOR7jWz": {"type": "rebuttal", "replyto": "quU7eawAk_", "comment": "We thank the reviewer for the thoughtful comments and pointing out the potentially relevant related work. We have updated the main body of the paper, in particular the \"Background\" section and the \"Other related work\" section to improve the positioning of our work, and added Figure 2 for further clarification. That being said, we believe that our contributions are relevant to the research community and the practitioners. \n\n[Contributions]\n**Hybrid approach** is not the main and only contribution, but a byproduct of a careful large scale analysis of current available model search strategies.\n\n[Not limited to classification]\nThe proposed method is **not limited to models pre-trained with classification heads, even though we focus only on classification tasks.** Note that VAE and BIGGAN-based models are included (Table 3 of the Appendix). \n\n[Related methods] \nWe thank the reviewer for helping us place the work into the more broad research context! While these methods are indeed related in the general sense, they are **not directly comparable to our method:**\n\nDuality diagram similarity (DDS) and DEPARA:\n - **We can use any model**, whereas Taskonomy is restricted to models trained on the same input with different labels.\n - **We examine fine-tuning**, whereas for both methods the encoder part which is transferred is not fine-tuned.\n - The methods aim at computing the results of the Taskonomy dataset faster by only working on a subset of the dataset (the same data for all models).\n - Adapting DEPARA and DDS to learn some dependency between tasks or models similarly to Task2Vec or Model2Vec would make it a \u201cmeta-learned task-aware\u201d strategy, on which we elaborate in the revisited paper.\n - While Task2Vec and Model2Vec are directly applicable by ranking models based on their similarity in the embedding space, it's unclear how to apply the model search on new tasks using DDS or DEPARA (or taskonomy in general) without semantic relations between the upstream and downstream tasks and without training a new network from scratch. \n\nLEEP:\n - By including the VTAB models, **we have access to models pre-trained with different loss functions, fully unsupervised to semi-supervised and strongly supervised**, whilst LEEP is only applicable to models pre-trained for classification tasks\n - LEEP provides theoretical guarantees only for fixed features, even though fine-tuning outperforms the linear classifier in their work.\n - Linear correlation is not necessarily transitive, which is why LEEP cannot be used directly to derive the relationship between fine-tuning and linear classifier. Hence, we **go one step further than LEEP** in understanding this behaviour.\n\n[Other suggestions]\nEarly stopping: \n - Training a network robustly with early stopping is an open research problem\n - Fine-tuning requires additional knowledge about the architecture (e.g., knowing when the batch norm layers are used). For our proxy task, **no such knowledge is required**.\n\nImpact of the dimension: \n - Already shown in Appendix for Pool **_DIM2048_**. The results for this model pool are **consistent** with the pool **_RESNET-50_** in the main body of the paper and were hence relegated to supplementary material. \n\nMutual Information (MI) based approaches:\n - MI could be useful, but there is no clear understanding on how to correctly estimate the MI and its relation to transferability.\n\nWe are hopeful that, together with the thoroughly revised Section 2, we addressed the major concerns.", "title": "Response to AnonReviewer1"}, "sw1CohP7e4o": {"type": "review", "replyto": "TmUfsLjI-1", "review": "[Summary] This paper presents a large-scale study on model-selection strategies for transfer learning, by performing task-agnostic and task-aware strategies on a large number of models evaluated on a diverse range of tasks. \n\n[Strength] The problem setting is novel and interesting. The proposed quantitative measurement of the quality of selected models, named \"regret\" is well designed.\n\n[Weakness] The major weakness of this paper is that it seems there is no consistent strategy to out-perform all other methods in every task. Intuitively, task-aware strategies should be better than task-agnostic strategies, but they perform similarly (almost equally) in all model pools, which is quite surprising.\n\nEven if for the advanced strategy hybrid proposed in the latter part of this paper, the optimal pick for this hybrid method is almost identical to the linear evaluation in task-aware strategy in ResNet-50 and expert model pools.\n\nSo my biggest concern for this paper is that we don't have a take-home message, other than showing the \"No-Free Lunch Theorem\" in model selection. So, I hope the authors could re-emphasize what we really learn from this large-scale study.\n\nAn interesting direction might be, how we can design a really fast approximation of fine-tuning, so that we can evaluate a model's fitness only by a few iterations (within a very short amount of training time), instead of performing full fine-tuning on the target task.\n\nConsidering this limited effective information from this paper, I think it's not suitable for publishing.\n\n  ", "title": "Unclear findings", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "quU7eawAk_": {"type": "review", "replyto": "TmUfsLjI-1", "review": "This paper presents a large scale empirical study on pretrained model selection for transfer learning and show that a hybrid approach that combines task-agnostic and task-aware methods outperforms the existing approaches on VTAB benchmark. The paper is well written and easy to follow. Experiments using 46 pretrained models and 19 downstream tasks show the effectiveness of the hybrid strategy in selecting the right model for transfer learning with low computational complexity. \n\nOverall, I vote for rejecting the paper as the paper has very limited novelty and experiments are not convincing. In particular, I fail to find the major contributions of the paper except the empirical study on VTAB dataset. While papers related to empirical study are interesting and worth of acceptance, this paper does not provide any major insight that could be useful for the future research on transfer learning. Furthermore, many experiments and comparisons are missing which should be included in the paper for a better understanding of the empirical study.\n\nHow is the proposed hybrid ranking strategy comparable to the model selection approaches presented in Duality Diagram Similarity: a generic framework for initialization selection in task transfer learning, ECCV 2020; DEPARA: Deep Attribution Graph for Deep Knowledge Transferability, CVPR 2020. These papers should be clearly discussed with possible comparisons in the experiments to show the advantage of the hybrid approach.\n\nComparison with many simple baselines are missing in the paper. E.g., How does the hybrid strategy comparable to fine-tuning with early stopping. Can we select pre-trained models by finetuning for only few epochs? How does the number of epoch affect the final performance while comparing to the hybrid strategy? \n\nHow is the current method comparable to Leep: A new measure to evaluate transferability of learned representations? Experiments and analysis should be included in the experiments to verify the effectiveness of the hybrid strategy.\n\nHow does the size of representation/feature affect the final performance? Does the conclusion still hold with different size of features? How does amount of data in the target task affects the performance of ranking? What is the effect of number of pretrained models on the ranking?\n\nMutual information between the features and discrete labels of the downstream task can be used to rank different models for transfer learning. How does the proposed hybrid strategy related to mutual information based ranking strategy? Experimental comparison should be included in the paper to verify this.\n\nDoes the ranking strategy and analysis presented in the paper limited to only classification models? In particular, can models trained using self supervised learning where there is no classification head, be used as pre-trained models in the current approach? How does the analysis change while considering self-supervised models which are now-a-days quite popular in representation learning? What about considering discriminators of generative models, e.g., VAE or BigGAN in the transfer learning analysis? More experiments and analysis should be performed in the experiments.\n", "title": "Reject due to limited novelty and lack of convincing experiments", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}