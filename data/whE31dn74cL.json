{"paper": {"title": "A Temporal Kernel Approach for Deep Learning with Continuous-time Information", "authors": ["Da Xu", "Chuanwei Ruan", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "authorids": ["~Da_Xu2", "~Chuanwei_Ruan1", "~Evren_Korpeoglu1", "~Sushant_Kumar1", "~Kannan_Achan1"], "summary": "We propose a temporal kernel learning approach based on random features and reparameterization to characterize the continuous-time information in deep learning models.", "abstract": "Sequential deep learning models such as RNN, causal CNN and attention mechanism do not readily consume continuous-time information. Discretizing the temporal data, as we show, causes inconsistency even for simple continuous-time processes. Current approaches often handle time in a heuristic manner to be consistent with the existing deep learning architectures and implementations. In this paper, we provide a principled way to characterize continuous-time systems using deep learning tools. Notably, the proposed approach applies to all the major deep learning architectures and requires little modifications to the implementation. The critical insight is to represent the continuous-time system by composing neural networks with a temporal kernel, where we gain our intuition from the recent advancements in understanding deep learning with Gaussian process and neural tangent kernel. To represent the temporal kernel, we introduce the random feature approach and convert the kernel learning problem to spectral density estimation under reparameterization. We further prove the convergence and consistency results even when the temporal kernel is non-stationary, and the spectral density is misspecified. The simulations and real-data experiments demonstrate the empirical effectiveness of our temporal kernel approach in a broad range of settings.", "keywords": ["Kernel Learning", "Continuous-time System", "Spectral Distribution", "Random Feature", "Reparameterization", "Learning Theory"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents a novel approach for integrating time into deep neural network models based on the Gaussian process limit view of a neural network model. Specifically, the approach augments an a-temporal neural network designed to process a single time point with a temporal kernel that relates data points across time. The composition of the a-temporal neural network kernel with with the temporal kernel is accomplished efficiently using a random features representation of the temporal kernel. The authors propose to represent the temporal kernel via its spectral decomposition, which makes the approach quite flexible. Learning leverages re-parameterization. While random features have been used to approximate temporal kernels in prior work [1], the approach in this paper is significantly more general in that it can be composed with any a-temporal deep architecture and the authors show results for RNNs, CNNs, and attention-based models. The predictive performance of the approach also appears to be consistently better than baselines and it works particularly well on the challenging case of irregularly sampled data.\n\nIn terms of weaknesses, the reviewers had a number of questions about the paper. The authors updated the paper to include some more recent models including ODE-RNNs. This material is currently presented in the appendices and needs to be moved into the main paper. Several of the reviewers also had technical questions questions that are in fact addressed in the manuscript; however, the authors are relying heavily on the appendices to present many important details and the paper is currently over 30 pages long. The frequent references to the appendix for additional details makes the paper a challenging read. The authors have already done some work to address clarity by adding a new figure, but should prioritize moving additional key details into the main body of the paper to improve readability. \n\n[1] http://auai.org/uai2015/proceedings/papers/41.pdf"}, "review": {"ZKTMBwzvLav": {"type": "review", "replyto": "whE31dn74cL", "review": "##### Post-rebuttal update\n\nI've read the rebuttal and updated my score.\n\n---------------\n\nThis paper proposes a deep learning model for incorporating temporal information by composing the NN-GP kernel and a temporal stationary kernel through a product. The temporal stationary kernel is represented using its spectral density, which is parameterized by an invertible neural network model. This kernel will be learned from the training data to characterize its temporal dynamics.\n\n##### Originality & Significance\nThe modeling approach taken in this paper is original to my best knowledge. Although it is well-known that second-order stationary processes has a SDE correspondence, it is rare that this property is connected to NN as GPs and this work finds an application where such ideas can be potentially useful. However, I find it difficult to say anything about significance of this idea since it is not very clearly described. I encourage the authors to make a substantial revision to clarify the issues listed below.\n\n##### Clarity\nThe clarity is low. Although the motivation and the high-level idea is clear, I find it very difficult to understand the actual approach taken by this work. There is no description of the actual algorithm and I can see many algorithmic and computational issues left without discussion:\n* How is prediction made at a specific (t, x)? Do you use a GP predictive mean conditioned on the training points?\n* If the prediction is made by GPs, how do you solve the scalability issue? When the training set is large, do you take a sparse approach? The temporal kernel is defined through a random feature representation, do you take advantage of it for fast computation?\n* or you just take a weight-space approach and compose the features (take pairwise product of the features of k_T and \\Sigma to form the new features)?\n* Is NN-GP or NTK kernels used to compute the kernels? How do you compute them? Do you use a Monte-Carlo estimate or the closed-form (computed through a recursion)? \nI will be happy to raise the score if these questions are properly addressed.\n\n##### Strengths\n* The modeling approach is novel.\n* The proposed method consistently outperforms other baselines in handling irregular continuous-time data.\n\n##### Weaknesses\n* The method used is not clearly described.\n* The non-stationary extension to Bochner's theorem is a known result.\n* Although the performance is shown to outperform other NN-based approaches in experiments, there might be scalability issues to apply the approach to larger-scale problems with long sequences (assume a non-weight-space approach).\n\n##### Minor\nP16: A.4.1: \"We the Fourier transformation transformation on both sides\"?\n", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "LksBrZh5pD1": {"type": "review", "replyto": "whE31dn74cL", "review": "This article proposes a methodology to *adapt* NNs to continuous-time data though the use of a (temporal) reproducing kernel. I enjoyed reading the paper, the message is clear, illustrative and the connection with other existing works is to the point. Although I am unfortunately unable to assess the theoretical novelty of the paper (I am unaware of the details of the state of the art in the subject) the contribution of the paper relates to the study of a kernel, given by an ODE, attached to the input of the NN. This kernel is also represented using Fourier feature expansions. \n\nThough the paper heavily relies on well-known concepts  (standard NN, GPs, Fourier features), I see that is has a contribution. \n\nI suggest the following amendments:\n-for some readers, the general proposed architecture might be confusing. Perhaps a diagrams (similar to that in Fig A.1) would be useful in the first pages of the paper. How does the kernel turn the continuous-time data into NN-ready?\n-much useful material is relegated to the appendix, if key results, scope and more are only in the appendix, they might not receive the deserved attention.\n-please better clarify how different your work is from the existing literature: NTK, deep kernel learning, neural ODEs, etc\n  \n ", "title": "Well written, connected with the current literature, appropriate experimental validation", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Lsjkk5joWYZ": {"type": "rebuttal", "replyto": "IC8nQwTunHp", "comment": "We want to thank the reviewer for the time and effort on providing valuable feedback to our paper. We apologize for the confusions caused by typos and unclear writing. We have corrected them in this version of the paper, and increased the font size in the figures. We have modified our paper according to the comments and requests by the reviewers, and all the changes we made are marked in \\color{red} in this version of the paper. \n\nWe agree with the reviewer that adding the neural ODE and temporal point process methods as baselines would make our arguments more solid. Given the limited time we have in the rebuttal stage, we manage to include RNN-ODE and RRN-TPP (for temporal point process) for all three cases in the time series prediction task. We choose RNN as the base architecture because the published implementations only support RNN, and extending their work to TCN and attention could be the topic of another paper. The added results are provided in Table A.1 and Figure A.2. RNN-ODE has a comparable (but slightly inferior) performance; however, the ODE solution is highly unstable, and we observe a large variance. The instability issue has also been observed and discussed by the authors of the original neural ODE paper [1].  \nThe RNN-TPP approach, on the other hand, does not improve the vanilla RNN baseline by much. We suspect that:\n\n1. The model assumption of TPP is more realistic for event prediction, i.e. predicting which event (from a limited pool of events) to occur next as a classification task, rather than the standard time-series prediction;\n\n2. The performance of TPP relies heavily on the intensity function that is very sensitive to the model specification. We have yet found a satisfactory solution from the existing literature on how to choose the intensity function.\n\nBased on the evidence, we believe that our approach also outperforms (and is more stable than) the RNN-ODE and RNN-TPP in the time-series prediction task. Unfortunately, we find it very difficult to tune the RNN-ODE and RNN-TPP model to reach a comparable performance in the recommendation task, because their model (training) architectures are somewhat restrictive. It is challenging to adapt them to a domain-specific setting. Therefore, we do not report their results for the temporal recommendation task. \n\nAlso, we make the following revisions to the original paper according to the comments and requests from the other reviewers:\n\n1. We add the dedicated algorithm description and diagram in Algorithm 1 and Figure 2. They may help the readers better understand the computation (forward and backward pass) flow of the proposed method.\n\n2. We provide a more detailed comparison between our approach and the related work in Section 4.\n\nWe again express our gratitude to the reviewer, and we are looking forward to future discussions. \n\n[1]. Duvenaud, D. K. https://www.youtube.com/watch?v=YZ-_E7A3V2w\n", "title": "We thank the reviewer for the careful reading and providing valuable feedback"}, "m_S6cffD5Es": {"type": "rebuttal", "replyto": "9YipEv-204i", "comment": "We thank the reviewer for the time and effort on providing valuable feedback to our paper. We apologize for the confusions caused by typos and unclear writing. We have corrected them in this version of the paper. As for the first two comments, we provide our thoughts as follow.\n\nTo Comment 1: \"What is the purpose of Claim 1? From the supplementary it just shows that f(t) and f[i] are not equal, but they may be very close and does not have a huge impact on the result. There are lots of approximations in other parts of the model.\"\n\nWe think of Claim 1 more as a motivating example, rather than the main result of our paper. We agree that the discrepancy between the spectral density of f(t) and its equally-spaced discretization f[i] might be small, which ultimately depends on the regularity of the underlying continuous-time system. Here, we hope that Claim 1 can serve as a rigorous motivation for modelling the spectral density distribution: even for such a simple setting and the discretization is equally-spaced, the spectral density is changed nonetheless. As a consequence, one can only expect larger perturbations for a more complex continuous-time system under irregular sampling. \n\n\nTo Comment 2: \"Why f(t) needs to be ODE ?\"\n\nWe agree that there may be alternative options, but we find ODE the most natural characterization of a continuous-time dynamic system, and it is extensively applied for solving signal processing problems. Therefore, we hope that by exploring from the ODE perspective, our work can benefit more audience from both the deep learning and signal processing domain.\n\n\nAlso, we make the following revisions to the original paper according to the comments and requests from the other reviewers, which we summarize below. All the modified parts are marked in \\emph{RED} in this version of the paper. \n\n1. We add the dedicated algorithm description and diagram in Algorithm 1 and Figure 2. They help the readers better understand the computation (forward and backward pass) flow of the proposed method.\n\n2. We provide a more detailed comparison between our approach and the related work in Section 4.\n\n3. In the experiments, we consider the RNN-ODE and RNN-TPP (temporal point process) as additional baseline methods for the time series prediction task. We are only able to experiment with the neural ODE and temporal point process models under RNN, because the published implementations are restricted to the RNN setting. Extending their methods to other architectures could be the topic of another paper. The results are provided in Table A.1 and Figure A.2. RNN-ODE has a comparable (but slightly inferior) performance; however, the ODE solution is highly unstable, and we observe a large variance. The RNN-TPP approach, on the other hand, does not improve the vanilla RNN baseline by much. \n\nWe again express our gratitude to the reviewer, and we are looking forward to future discussions. ", "title": "We thank the reviewer for the careful reading and providing valuable feedback"}, "0IEeBqvnAL": {"type": "rebuttal", "replyto": "LksBrZh5pD1", "comment": "We want to thank the reviewer for the time and effort on providing valuable feedback to our paper, particularly for suggesting the amendments. We believe they help bring a better version of our paper. \nWe have modified our paper according to the comments and requests by the reviewers, and all the changes we made are marked in \\color{red} in this version of the paper.\n\nIn response to the comments:\n\nTo Comment 1. \"For some readers, the general proposed architecture might be confusing.\"\n\nWe thank the reviewer for pointing out the potential confusion in terms of the architecture and computation workflow. We add a detailed diagram in Figure 2, together with the algorithm description for the forward and backward pass in Algorithm 1. We believe they help clarify the workflow of our approach.\n\nTo Comment 2. \"How does the kernel turn the continuous-time data into NN-ready?\"\n\nThe key for converting the continuous-time data to NN ready is to use the random feature representation, which can be thought of as a time encoding functional that takes the continuous-time data as input, and produce a vector representation. We provide a detailed illustrate on this conversion in Figure 2. \n\nTo Comment 3. \"Some useful material is relegated to the appendix.\"\n\nWe agree that some of the material should be moved to the main paper. In particular, we add the scope and limitation to Section 6, and the related-work comparisons to Section 5. As for the numerical results, since other reviewers have requested for additional experiments, we decide not to change the original layout and presentation in the rebuttal stage to avoid confusions. We will move more results to the main paper in the final version.\n\nTo Comment 4. \"Please better clarify how different your work is from the existing literature: NTK, deep kernel learning, neural ODEs, etc.\"\n\nOur approach characterizes the continuous-time ODE system via the lens of kernel. It complements the existing neural ODE methods which are often restricted to specific architectures, rely on ODE solvers and lack theoretical understandings. \nThe existing work on deep kernel learning mostly assumes a given kernel family. We propose a novel deep kernel learning approach by parameterizing the spectral distribution under random feature representation, and demonstrate its effectiveness with both theoretical arguments and numerical results.  \nFinally, since the idea of composing NN with a temporal kernel under its random feature representation is proposed in this paper for the first time, the derivations of the associated NTK is novel in its own regard.\n\nWe again express our gratitude to the reviewer, and we are looking forward to future discussions.\n", "title": "We thank the reviewer for the careful reading and providing valuable feedback"}, "6wT2Q-vIPSD": {"type": "rebuttal", "replyto": "ZKTMBwzvLav", "comment": "We want to thank the reviewer for the time and effort on providing valuable feedback to our manuscript. We believe the comments help us to bring a better version of this paper. We have made substantial modifications to the original paper, and all the changes are marked in RED in this version of the paper. \n\nWe first apologize for the typos and unclear writing. We have corrected them in the paper. \nAs for the clarity issues of the algorithm and computation, we add a detailed diagram to illustrate the architecture in Figure 2, and provide the algorithmic description for the forward and backward computation in Algorithm 1. They help clarify the questions from the reviewer, which we also summarize as below.\n\nQuestion 1. \"How is the prediction made at a specific (t, x)? Do you use a GP predictive mean conditioned on the training points?\"\n\nThe detailed answer is provided in Algorithm 1. In short, we first sample from the auxiliary distribution and use the invertible neural network to parameterize the random samples. We then construct the random features as in eq. (8) as a time-aware encoding (which is exactly the random feature representation of the temporal kernel), and combine the encoding with the selected hidden layer of the neural network. Our strategy for the forward passing (prediction) is similar to that of the variational autoencoder[1], which saves us from the computation complexity of using GP to make predictions.\n\nQuestion 2. \"If the prediction is made by GPs, how do you solve the scalability issue? When the training set is large, do you take a sparse approach? The temporal kernel is defined through a random feature representation, do you take advantage of it for fast computation?\"\n\nFollowing our answer to the previous question, and we hope the reviewer could take a glance at the new Figure 2 and Algorithm 1, since we are not using GP to make predictions, the computation complexity is almost the same as the original neural network. The computing time comparisons are actually provided in Figure A.5 (we understand it is not mandatory for the reviewers to check the appendix), so we mention it here that the training and inference computation time are almost the same for CNN, RNN and attention mechanism using our approach. The trick for speeding up the computation is precisely by taking advantage of the random feature representation, as pointed out by the reviewer, where we use the reparameterization to construct the representation under the INN efficiently. \n\nQuestion 3. \"Do you just take a weight-space approach and compose the features\"\n\nAll the operations are conducted in the feature (weight) space, i.e. when doing training and inference, instead of computing the kernel compositions, we conduct the corresponding compositions in their feature spaces. \n\nQuestion 4: \"Is NN-GP or NTK kernels used to compute the kernels? How do you compute them? Do you use a Monte-Carlo estimate or the closed-form?\"\n\nBoth the NN-GP and NTK kernels can be computed, though we do not explicitly rely on their outcome in the training and inference, by Monte-Carlo estimate via sampling from the auxiliary distribution. The output from the RF-INN module in Figure 2 can now be used to construct the random feature representation, from which we can recover the NN-GP kernel (via Claim 1) and NTK kernel (via Proposition A.1). We agree that it could be an exciting topic to study the properties of the obtained NN-GP and NTK kernel, which we leave to future work. \n\nWe also wish to justify the weakness commented by the reviewer.\n\nComment 1: \"The method used is not clearly described.\"\n\nWe provide the detailed descriptions of our approach in the new Figure 2 and Algorithm 1.\n\nComment 2: \"The non-stationary extension to Bochner's theorem is a known result.\"\n\nWe have discussed in the appendix that we refer to the Yaglom Theorem [2] and the seminal work of [3] to construct the random feature representation of non-stationary kernels. However, our stochastic convergence result in Proposition 1 is novel to the best of our knowledge. We agree with the reviewer that our claim about the novelty in terms of the non-stationary kernel is not precise, and we will modify it to focus on the theoretical contributions.\n\nComment 3: \"There might be scalability issues to apply the approach to larger-scale problems with long sequences\"\n\nAs we have clarified in the previous response, the scalability issue is not a concern for our approach. As we show in the new Figure 2, the computation complexity with respect to sequence length is at the same order compared with the vanilla RNN (for example), since we conduct the sampling and reparameterization with the invertible neural network at the beginning of each batch. \n\n[1]. Kingma D P, Welling M. Auto-encoding variational bayes, 2013\n\n[2]. Yaglom A M. Correlation Theory of Stationary and Related Random Functions, 1987\n\n[3]. Remes S, Heinonen M, Kaski S. Non-stationary spectral kernels, 2017\n", "title": "We thank the reviewer for the careful reading and providing valuable feedback"}, "9YipEv-204i": {"type": "review", "replyto": "whE31dn74cL", "review": "The ms introduces a time component in the traditional NN setup, where the hidden layers change dynamically according to time. The idea is to borrow strength from the newly introduced time dimension to improve prediction performance.\n\nOne of the key idea is to treat each hidden layer in NN as a Gaussian process, which is represented  as \u201cneural network kernel\u201d. Functions drawn from this Gaussian process at different time points are assumed to follow a continuous-time system, which is actually a ODE. For some reason it is difficult to use the continuous-time system to compute the temporal kernel directly in the time domain. The ms proposes to convert the functions to the frequency domain, which leads to a nice property such that we can compute a temporal kernel (Eq. (2)) in frequency/spectral domain. Hidden layer of NN at different time points can be seen as a large Gaussian process, whose kernel could be composed by the aforementioned NN kernel and temporal kernel (Eq. (5)). This decoupling is the key of this ms. \n\nRegarding the computation of the kernels, NN kernel can be computed by extracting the features of the hidden layer. The temporal kernel can be computed by sampling the spectral distribution, which is called the random feature representation (Eq. (7)). However, it is not clear how to specify the spectral distribution. In all examples, Normal distribution is used, which is OK but may not be able to capture the complexity of ODE. \n\nThe ms applies the proposed method to real dataset and achieve better performance than baseline methods in prediction tasks involving irregular time points setup.\n\nIn general I feel this is a nice paper. The idea of learning NN and time dynamics at the same time seems to be useful in many applications. The ms cleverly decouples the learning of NN kernel and temporal kernel in two independent modules, which can maximumly utilise current implementations. \n\nHowever, due to my limited knowledge in signal processing, I am not able to dig into the mathematical details and make strong recommendations (especially Claim 2). \n\nSome minor comments\n1. What is the purpose of Claim 1? From the supplementary it just shows that f(t) and f[i] are not equal, but they may be very close and does not have a huge impact of the result. There are lots of approximations in other parts of the model.\n2. Why f(t) needs to be ODE ?\n3. Page 3, section 3, line 2, in the formula of f(iw), the second derivation term seems to be missing\n4. Page 4, 5 lines after Eq. (3), a_2(x)!=0 => a_1(x)!=0\n5. Page 5, Eq. 7, cos(tw_n) => cos(tw_m)\n", "title": "Clever treatment to decouple NN and temporal dynamics", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "IC8nQwTunHp": {"type": "review", "replyto": "whE31dn74cL", "review": "This paper proposed a general deep learning method with temporal-kernel for continuous time series modeling.\n\nThe proposed method is technically sound and solid. The decomposition of the neural and temporal kernel brings together the kernel methods and deep learning, which delivers a general and fundamental solution to time series, especially the irregularly sampled ones or those with missing values.\nIn brief, this work may demonstrate a promising way of handling such problems and inspires and encourages other research in this direction.\n\nThe writing is thorough and clear. Though I did not check all proofs and the supplementary, the descriptions and arguments in the paper are properly delivered.\n\nThe proposed model consistently outperforms RNN, TCN, and attention baselines on a variety of datasets. The settings of the case 2/3 are reasonable.\nBesides, it is interesting to see that the speed is still comparable to the baselines.\n\nHowever, in my opinion, more baseline comparisons need to be added. I did not quite buy the claim that the proposed method is not compared with recurrent neural ODE-type models and point process models because of its more generalization and flexibility.\n\nMinor typos:\nIn page 3: infitnitesimal -> infinitesimal; covaraince -> covariance\nThe font size in figures may be increased for better readability.\n", "title": "Review from R3", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}