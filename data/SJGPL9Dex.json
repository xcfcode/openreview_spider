{"paper": {"title": "Understanding Trainable Sparse Coding with Matrix Factorization", "authors": ["Thomas Moreau", "Joan Bruna"], "authorids": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"], "summary": "We analyse the mechanisms which permit to accelerate sparse coding resolution using the problem structure, as it is the case in LISTA.", "abstract": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \\cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.\n\nIn this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "keywords": ["Theory", "Deep learning", "Optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "The work is fairly unique in that it provides a theoretical explanation for an empirical phenomenon in the world of sparse coding. The reviewers were overall favourable, although some reviewers thought parts of the paper were unclear or had confusion about the relationship to LISTA. I suspect the analysis here could also shed light on other problems."}, "review": {"ry8kYRXHl": {"type": "rebuttal", "replyto": "ByDo3EJSl", "comment": "Thank you for your review and for pointing out these typos.\nThe goal of Section 2.3.2 is to state that our theoretical analysis does not take into account the structure of the input distribution. Our theory is based on uniform bounds, for the whole input space, but the upper bounds can be improved by considering a restricted input space, i.e. using hypothesis on the inputs.\nThe training using SGD and back propagation is de facto using the input distribution and does not optimize uniformly. Thus, the network parameters are linked to a factorization using (15). The implications of using this technique for different distributions are not studied in this paper, as we focused on the effect of the dictionary structure, but should be studied in a follow up paper.", "title": "Response"}, "BkjqhqY4l": {"type": "rebuttal", "replyto": "Hyw6iO_Nx", "comment": "Thank you for your feedback.\n\nIn Section 3.3, the MNIST experiment is conducted with a dictionary learned a priori, using classical dictionary learning and sparse coding techniques. The idea is to evaluate the capabilities of the models to approximate sparse codes with unconstrained  dictionaries, with potentially high coherence. We also clarified this point in the most recent manuscript version.\n\nRegarding the title of the article, we agree that it could be confusing. There is no set name for these type of architectures besides the seminal LISTA network. We have updated the title to \u201cUnderstanding Trainable Sparse Coding with Matrix Factorization\u201d.", "title": "Response"}, "S1LF9TUVg": {"type": "rebuttal", "replyto": "Hy8glzMNg", "comment": "Thank you for your review and the time spent on our paper.\n\nWe would like to clarify our main contribution (and we will make sure the introduction states it even more clearly): it is not to provide an alternative algorithm/model to LISTA, but rather to provide a theoretical framework that explains when and why Lista works better than non-adaptive methods.\n\nIn that respect, Facnet is a re-parametrization of LISTA that directly maps to our mathematical analysis (the factorization with matrices A and S). Despite being a model that uses half of the effective parameter size, our experiments show that it nearly matches the performance of LISTA in all regimes, and it provides evidence that our factorization is sufficient and necessary for LISTA to succeed. The only practical interest of Facnet is that it uses less parameters than LISTA.\n\nBut LISTA will always have at least the same performance as the parameters of FacNet can be mapped directly to parameters for LISTA without loss of performances (the converse is not true). FacNet is thus a tool that permits to show numerically the link between our analysis and LISTA.\n\nI hope that clarifies your concerns. Please let us know if there are still aspects that require clarification.\n\nBest,\n\nJoan", "title": "Response"}, "SkMQB6GXg": {"type": "rebuttal", "replyto": "BJQ1IY1Xg", "comment": "Thank you for your question. FacNet shares the same network structure as LISTA, but with additional constraints on the parameters. Thus, LISTA is a generalization of FacNet, as the parameter space is larger for LISTA than for FacNet. This is coherent with the results observed in numerical experiments as LISTA outperforms FacNet, up to some optimization errors. I will update the paragraph presenting Factorization Network (end of p.6) to clarify this point.", "title": "Empirical performance of FacNet"}, "BJQ1IY1Xg": {"type": "review", "replyto": "SJGPL9Dex", "review": "The plots suggest that the proposed FacNet is almost always outperformed by LISTA and L-FISTA. This is surprising since the paper seem to suggest that LISTA is in some sense a specialization of FacNet. Any comment about this?This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010). A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LISTA (via Theorem 2.2. and Corollary 2.3).\n\nFacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms FacNet, up to some optimization errors. It is not clear what is the advantage of using FacNet instead of LISTA.\n\nOverall, the paper lacks clarity in several parts. It would be good to state beforehand what the main contribution is. As stated in the clarification question/answer below, this paper would benefit from a more clear explanation about the connection of FacNet with LISTA. \n\nMinor comments/typos:\n- p. 6: \"memory taps\" -> tapes?\n- sec 3.2: \"a gap appears has the number of iterations increases\" -> as?\n- sec. 4: \"numerical experiments of 3\" -> of sec 3", "title": "Empirical performance of FacNet", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "Hy8glzMNg": {"type": "review", "replyto": "SJGPL9Dex", "review": "The plots suggest that the proposed FacNet is almost always outperformed by LISTA and L-FISTA. This is surprising since the paper seem to suggest that LISTA is in some sense a specialization of FacNet. Any comment about this?This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010). A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LISTA (via Theorem 2.2. and Corollary 2.3).\n\nFacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms FacNet, up to some optimization errors. It is not clear what is the advantage of using FacNet instead of LISTA.\n\nOverall, the paper lacks clarity in several parts. It would be good to state beforehand what the main contribution is. As stated in the clarification question/answer below, this paper would benefit from a more clear explanation about the connection of FacNet with LISTA. \n\nMinor comments/typos:\n- p. 6: \"memory taps\" -> tapes?\n- sec 3.2: \"a gap appears has the number of iterations increases\" -> as?\n- sec. 4: \"numerical experiments of 3\" -> of sec 3", "title": "Empirical performance of FacNet", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}