{"paper": {"title": "Meta-Learning with Network Pruning for Overfitting Reduction", "authors": ["Hongduan Tian", "Bo Liu", "Xiao-Tong Yuan", "Qingshan Liu"], "authorids": ["hongduan_tian@nuist.edu.cn", "kfliubo@gmail.com", "xtyuan1980@gmail.com", "qsliu@nuist.edu.cn"], "summary": "", "abstract": "Meta-Learning has achieved great success in few-shot learning. However, the existing meta-learning models have been evidenced to overfit on meta-training tasks when using deeper and wider convolutional neural networks. This means that we cannot improve the meta-generalization performance by merely deepening or widening the networks. To remedy such a deficiency of meta-overfitting, we propose in this paper a sparsity constrained meta-learning approach to learn from meta-training tasks a subnetwork from which first-order optimization methods can quickly converge towards the optimal network in meta-testing tasks. Our theoretical analysis shows the benefit of sparsity for improving the generalization gap of the learned meta-initialization network. We have implemented our approach on top of the widely applied Reptile algorithm assembled with varying network pruning routines including Dense-Sparse-Dense (DSD) and Iterative Hard Thresholding (IHT). Extensive experimental results on benchmark datasets with different over-parameterized deep networks demonstrate that our method can not only effectively ease meta-overfitting but also in many cases improve the meta-generalization performance when applied to few-shot classification tasks.", "keywords": ["Meta-Learning", "Few-shot Learning", "Network Pruning", "Generalization Analysis"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a regularization scheme for reducing meta-overfitting. After the rebuttal period, the reviewers all still had concerns about the significance of the paper's contributions and the thoroughness of the empirical study. As such, this paper isn't ready for publication at ICLR. See the reviewer's comments for detailed feedback on how to improve the paper. "}, "review": {"SkxXjkpqKr": {"type": "review", "replyto": "B1gcblSKwB", "review": "\nIn this paper, the authors propose a new method to alleviate the effect of overfitting in the meta-learning scenario. The method is based on network pruning. Empirical results demonstrate the effectiveness of the proposed method.\n\nPros:\n+ The problem is very important in the meta-learning field. The model is intuitive and seems useful. In addition, the generalization bound further gives enough insights for the model.\n\nCons:\n- The proposed method is simple and lacks technical contributions. Adding sparse regularizers is a common practice to alleviate over-fitting in the machine learning field. In addition, the retraining process increases the time complexity of the proposed model (i.e., we need to train three times to get the powerful model). \n\n- In the experiment parts, it will be more interesting if the authors can do the experiments without pre-training. Since in traditional meta-learning settings (e.g., Reptile and MAML), pre-training process does not be introduced. Thus, it might be more convincing if the authors can training the mask and initialization together.\n\n\nPost rebuttal:\n\nI have read other reviewers and the authors' responses. I still think the contribution is not enough to be accepted. I do not change my mind.", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "HkgZbj0g5r": {"type": "review", "replyto": "B1gcblSKwB", "review": "The paper proposes to use the sparse network to mitigate the task-overfitting. The algorithm is based on an iterative training over the pruning phase and retraining phase. A theoretical analysis of the sparsity effects for the Reptile-based first-order meta-learning is provided which indicates if the sparsity level k satisfies k^2 << parameter dimension p, the sparsity can help with the generalization. The paper studies the Reptile + DSD pruning and Reptile + IHT pruning on two standard few-shot classification benchmark.\n\nThe paper is in general well-written. My major comments/questions are the following;\n\n1. As the author pointed in the experiment section, there is a mis-match between the experiments and the theory. The output of Algorithm 1 is not sparse though during the iterative training the initialization of the retraining phase is sparse. But the theorem says the generalization is achieved because of sparsity, which requires k^2 << p. The ablation study seems even put more doubts on what the theorem suggests, which basically shows sparsity alone harms the general performance.\n\n2. Is there a typo in Eq (1)? Should the outside loss be evaluated on the query set rather than the support set? If so, does this typo influences the prove of Theorem 1 based on Shalev-Shwartz et.al. 2009?\n\n3. In Figure 1, first of all, what experiments does this figure corresponds to? In Figure 1 (b), the gap between training and testing for both pruning methods are quite large which seems doesn\u2019t solve the overfitting very much? The test traces are intertwined, so it is not clear that the test accuracy get really improved. Using a consistent color for the Figure 1 (a) and Figure 1 (b) can make it much easier to read.\n\n4. The paper needs to discuss about the computational-complexity. It seems each iteration in Algorithm 1 involves meta-training a sparse network and a dense network. And the algorithm needs the number of iteration t > 1. Is there any difficulty in scaling?\n\n5. In the experiments section, for Omniglot almost all results are overlapping with confidence interval. Maybe it should not mark some numbers with bold font. The results in Mini-imagenet show the improvement by proposed methods. Does the effectiveness related to the total number of image classes in dataset?\n\nSome other comments:\n\n1. As the author mentioned, there are two types of overfitting: the meta-level overfitting and task-level overfitting. Why the proposed methods deal with meta-level overfitting rather than task-level overfitting?\n\n2. How does the random mask generated in Algorithm 1?\n\n3. In experiments, can the training accuracy be also provided? \n\nIn general, this paper study an interesting problem in meta-learning and the paper is written in a clear way. The major problems are a mis-match between the theorem and the methods and the experimental results are not very strong. I will give a borderline rating.\n\n############\n\nThanks for the authors' feedback and I have read them. I am still not convinced about the proposed method about the effectiveness and the trade-off in computation. I hold the concerns about the rebuttal \"this typo can be easily fixed with almost no impact on the technical proofs of the theoretical results\".  Therefore I maintain the current rating as borderline leaning to rejection.\n", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 2}, "Syg1bRkLYH": {"type": "review", "replyto": "B1gcblSKwB", "review": "\n\n============= Post Rebuttal Assessment =============\n\nI appreciate the authors thorough response: I believe the writing has improved considerably and the presentation is more convincing.\n\nAll in all, I think it's a good paper but not completely ready for publication based on the following final assessment.\n\nThe theoretical contribution of the paper is not entirely relevant to the proposed method as also mentioned by other reviewers. The empirical aspect of the work is incremental; by combining two prior works. In this situation, I would happily suggest acceptance if 1) the experimental setup is very thorough and 2) the results are consistent and conclusive 3) the improvements are significant.\n\nRegarding these, the paper's most interesting result is 5-way 5-shot classification with IHT/DSD + Reptile. I think that makes the paper borderline since the results are not state of the art and the experiments are not thoroughly done, while on the other hand, it does show clear improvement for this case. For such a borderline scenario, and since I have to choose between weak reject and weak accept, I would lean towards weak reject as I believe the experimental setup can be significantly improved by following some of the items below for the next version:\n\n- A thorough ablation study on all the components of IHT and DSD\n\n- A study of the effect of the hyperparameters of IHT/DSD\n\n- Additional datasets/architectures to show consistent improvement for various cases.\n\n- ResNet results are not reliable in the current form. I understand that it would need a sizable computational buget to perform a proper comparison. However, that means either the results should be taken away or done appropriately.\n\nTo reiterate, the reason I am asking for the above experiments is that the paper comes with incremental novelty as the main point, in my opinion. Thus it has to be backed up by extensive experiments to be confidently considered for publication.\n\ndget to perform a proper comparison. However, that means either the results should be taken away or done appropriately.\n\nTo reiterate, the reason I am asking for the above experiments is that the paper comes with incremental novelty as the main point, in my opinion. Thus it has to be backed up by extensive experiments to be confidently considered for publication.\n\n\n============= Summary =============\n \nThe paper addresses the issue of overfitting the meta-parameters of optimization-based meta learning techniques. It uses an iterative training-pruning-retraining setup to learn the meta-parameters. The pruning step in each iteration is to limit the capacity of the meta-learner and thereby alleviate overfitting. Two pruning methods called DSD and IHT are employed on top of a first-order meta learning technique called Reptile. The combination is tested on 4 few-shot classification scenarios of two datasets; omniglot and miniImageNet. Results suggests improved accuracy on top of the Reptile meta-learning algorithm on miniImageNet.\n \n \n============= Strengths and Weaknesses =============\n\n+ overfitting the meta learner due to the small number of samples (shots) per task and the large number of meta-parameters and/or base-learner parameters is an important problem in meta learning which is the focus of this work.\n+ the results suggest improvements over the Reptile baseline on miniImageNet.\n \n \nGeneral concerns:\n- abstract: \u201cthe existing meta-learning models have been evidenced to overfit on meta-training tasks when using deeper and wider convolutional neural networks.\u201d several methods such as CAVIA and MetaOptNet (among many others) address this issue by limiting the capacity of the learner.\n- what is the formal definition of meta-generalization and meta-overfitting? This definition will be helpful for understanding the paper\u2019s arguments, for instance, why \u201creducing meta-overfitting\u201d and \u201cimproving meta-generalization\u201d are two different matters (as suggested in the last part of the abstract).\n- Figure 1.b: the green bars (proposed method) don\u2019t seem to improve the generalization (testing accuracy). I can only speculate that the figure is to demonstrate that (in the rightmost plot) the difference between train and test accuracy for green bars is less than it is for the red bars. However, one should note that it seems to be due to the training accuracy being lower, which can be achieved, in the extreme case, by a random classifier (zero gap). So, it\u2019s not necessarily useful when testing accuracy is not improved.\n- eq (1) and eq (2): the left-most loss L should be on D^{query}\n- page 4: \u201cIn view of the \u201clottery ticket hypothesis\u201d (Frankle & Carbin, 2018), the model in equation 2 can be interpreted as a first-order meta-learner for estimating a subnetwork\u201c -> eq (2) in the current form still requires a 2nd-order derivative and I cannot see how (Frankle & Carbin, 2018) helps make it first-order optimization.\n- the usefulness of the provided theory is in question since the final method does not enforce the sparsity in practice. That is, the output of the final meta-parameters are *dense*.\n- page 7 mentions \u201ctop k_l entries\u201d: what does \u201ctop\u201d mean here? k_l dimensions of \\theta with the highest absolute value, maybe?\n- why do the two separate steps of only training the pruned subnetwork using Reptile and then retrain the whole network (with the updated subnetwork) again using Reptile? One can instead only train Reptile on the whole \\theta^{(i)}_M (with L0 norm constrained using the M projection) in a single Reptile step. Doing the former should be justified over the latter since its twice as expensive. Also it should be shown empirically (as an ablation study) that the former works better. \n- reference list: arXiv versions of many papers are cited, it\u2019s good to cite the peer-reviewed version of papers when applicable. For instance, IHT pruning method does not seem to be peer-reviewed which is a caveat for the current work.\n- 10 pages seem excessive for the content of the paper. For instance, the experiments section can be shortened extensively and the theories can be put into the appendix.\n \n \nExperimental concerns:\n- Has the standard miniImageNet split been used? This split is especially important to be respected since CAVIA\u2019s accuracy is taken from the original paper. \n- The reported number for CAVIA in table 2 is not the best number the original paper achieve. The best number is 51.82 \u00b1 0.65% for one-shot, and 65.85 \u00b1 0.55% for 5-shot. \n- There are quite a few new hyperparameters (3 iteration numbers for pretraining, pruning/finetuning, and retraining, and then the sparsity level k_l). It\u2019s important to mention how the hyper-parameters are chosen, especially since they are different for different setups. \n- there seems to be no meta validation set for Omniglot.\n- For a fair comparison, the network size of the baseline as well as the learning rate should be searched with the same budget as the hyperparameter search done for the proposed method.  \n- ResNet experiment is especially concerning since 1) there is an even higher-level of hyperparameter tuning for ResNet: first conv layer is not pruned, different pruning rates for different residual blocks are used. 2) the baseline is only tried with one setting for the capacity. It is evident in Table 2 that there is a sweet spot for the capacity of the ConvNet baseline, there is no reason this does not apply to ResNet.\n- Ablation study:  \u201cFor the purpose of fair comparison, only the retraining phase is removed and other settings are the same as proposed in Section 5.1\u201d: this is not fair. I believe a fair comparison would be to repeat the hyperparameter search for the ablation studies with the same budget as the full version of the proposed method.\n- Table 2 only compares with CAVIA while many other meta learning methods could be relevant here such as MetOptNet which also limits the number of base learner\u2019s learnable parameters thereby helping with overfitting. Also, CAVIA has results on ResNet useful for the last experiment.\n \n \n \n============= Final Decision =============\n\nWhile the paper addresses an important problem and reports improvements, there are many concerns with it including the writing, method, and experimental setup.  My \u201cweak reject\u201d rating, however, is mainly based on the experimental concerns especially regarding the hyperparameters and the baselines.\n \n \n============= Minor Points =============\n\n- code is not provided. I think it\u2019s in general very helpful to release the codes for reproducibility and replicability of the experiments, especially in the case of an incremental study.\n- what meta learning task has been used for Figure 1? \n- There are different ways of including batchnorm in a meta learning setup. For instance, Reptile used two different strategies. How is batchnorm implemented for this paper\u2019s experiments? Particularly, indicate if you are using transductive or non-transductive version of batchnorm at test time.\n \nThe text as well as the math notations require polishing on various occasions including the following (non-exhaustive) list:\n- abstract: \u201csparsity constrained\u201d -> sparsity-constrained\n- abstract: \u201ccan not only\u201d -> rephrase so that it\u2019s not read as \u201ccannot only\u201d\n- abstract and intro: \u201cease meta-overfitting\u201d can be interpreted as facilitating overfitting -> maybe better to say \u201calleviate meta-overfitting\u201d\n- intro: \u201cmemorize the experience from previous meta-tasks for future meta-task learning with very few samples\u201d \u2192 memorize the experience from previous tasks for a future task with very few samples\n- intro: \u201cAs can be observed from Figure 1(a) that\u201d -> It can be observed from Figure 1(a) that\n- intro: \u201csparsity benefits considerably to the\u201d \u2192 sparsity benefits considerably the\n- intro: \u201calong with two specifications to two widely used networking pruning methods\u201d: rephrase\n- related works: \u201cmost closely\u201d -> closest\n- page 4: \u201cultra goal -> maybe ultimate goal?\n- page 4: \\theta is not defined\n- page 4: eq (2): i -> m or m -> i\n- page 4: the definition of an unordered set (S) coming as a sample of the product of m \\Tau spaces is not precise. Better to say each T_i \\in \\Tau.\n- page 4: J_l is not defined,\n- sec 3.3: the minimization are done over \\theta, it\u2019s better to put that under \u201cmin\u201d and the conditions should be clearly indicated by \u201cwith or s.t. or similar\u201d\n- page 7: zero-one -> binary\n- page 7: \u201c as the restriction of \\theta^{(t)} over the mask M^{(t)}\u201d -> rephrase\n- page 7: \u201cit deems to reduce\u201d -> rephrase\n- page 8: \u201cthis is consist\u201d -> consistent\n \n- it\u2019s also good to settle on a single term for each concept to make it an easier read, for instance:\n- \u201cmeta-training tasks\u201d and \u201ctraining tasks\u201d have been used interchangeably. I think \u201ctraining tasks\u201d and \u201cmata training dataset\u201d are better choices since a \u201cmeta-training\u201d task can refer to the whole task of meta learning.\n- Meta-overfitting and meta-level overfitting\n- meta-learner, meta-estimator, model\n- meta-level training accuracy, meta-training accuracy\n- meta training, meta learning\n- base learner, learner, model \u2192 I think it\u2019s better to emphasize on learner being \u201cbase learner\u201d, avoid model as it is not clear what it refers to.\n \n \n \n============= Points of improvements =============\nThe paper would significantly improve if 1) the text is polished and the equations revised. 2) hyperparameter optimization is done carefully and thoroughly described for both the proposed method as well as the baselines\n\n\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 2}, "ryeANU6ioB": {"type": "rebuttal", "replyto": "B1gcblSKwB", "comment": "We thank all reviewers for their constructive comments. Per your suggestions, we have carefully edited our submission. We hope that the given concerns have been addressed satisfactorily in the revised manuscript. Below we provide a summary of changes.\n1.\tAt the end of Section 4.1.2, we add a short discussion on the generalization performance of the dense output from the retraining phase. \n2.\tMore details about the setting of hyper-parameters are provided in the experiment section.\n3.\tWe have included more results of CAVIA on MiniImageNet for a more complete comparison.\n4.\tOther changes are minor clarifications that are detailed in our responses to the reviewers.\n5.\tThe code is released along with the revised paper at https://drive.google.com/open?id=1VOY1sCA1j5G1LE2AbDrPoZM-1ZwwVOHA and the corresponding datasets can be available here at https://drive.google.com/open?id=17Cftpney_up0u5SCPb5IuskFplrcAuri\n", "title": "Paper Revision"}, "ryeI-9XdjS": {"type": "rebuttal", "replyto": "Syg1bRkLYH", "comment": "Thank you for the insightful review. We hope the main concerns can be addressed by the following clarification.\n\nResponse to general concerns:\n\n1.\tWe would like to highlight that in contrast to CAVIA and MetaOptNet which handle overfitting by limiting the capacity of the inner-task learner, our method explores another direction of limiting the capacity of the inter-task meta-learner to improve generalization performance across tasks. \n\n2.\tThe meta-generalization performance is measured by the population risk of meta-learner, while meta-overfitting is quantified by the gap between the population and empirical risks. We will update the paper to clarify these concepts.  \n\n3.\tWe address your concern about the curves in Figure 1(b). The main purpose of this figure is to demonstrate the power of our method for overfitting reduction. Concerning the loss in training accuracy of our method, we remark that such a trade-off between training accuracy loss and overfitting reduction is usually the case in sparse learning. In order to remedy this issue, we further propose to use a re-training phase to improve the overall training and testing accuracy. \n\n4.\tYes, the left-most loss in Equation 2 should be evaluated on the query set. This typo can be easily fixed with almost no impact on the technical proofs of our theoretical results.\n\n5.\tSorry for the confusion. We will remove the misleading term of \u201cfirst-order\u201d in the revised paper.\n\n6.\tWe would like to address your concern about the usefulness of our sparse generalization theory. First, the ablation study results in Figure 3 affirmatively confirmed our theoretical prediction by showing that the sparse meta-initialization network did reduce the gap between training and testing accuracy. Second, to further justify the improved generalization accuracy with dense re-training, we comment that since the obtained sparse network generalizes well, it is expected to serve as a good initialization for future re-training via SGD. Then roughly speaking, according to the SGD stability theory in [Hardt et al., 2016] the output dense network will also generalize well (with high probability) if the re-training phase converges fast enough. We will update the paper to have this point clarified. \n\n[Hardt et al., 2016] Hardt M, Recht B, Singer Y. Train faster, generalize better: Stability of stochastic gradient descent, ICML, 2016: 1225-1234.\n\n7.\tYes, the parameters are ranked according to their absolute values.\n\n8.\tThe reason to fine tune the subnetwork during the iteration is because according to our numerical experience with IHT network pruning, sufficient steps of subnetwork fine tuning tend to substantially improve the stability and convergence behavior of the method. \n\n9.\tWe appreciate your suggestions about reducing paper length and citing peer-reviewed references. We'll do our best to address them in the revised paper.", "title": "Reply to Official Blind Review #1 Part(1/2)"}, "Syl0XcQdsH": {"type": "rebuttal", "replyto": "Syg1bRkLYH", "comment": "Response to experimental concerns:\n\n1.\tYes, the standard MiniImageNet split was used in our work to ensure fairness of comparison.\n\n2.\tWe note the best accuracy results by CAVIA (51.82\u00b10.65% for one-shot, and 65.85\u00b10.55% for 5-shot) on MiniImageNet were reported in the 512-channel case. These results, however, are still inferior to our results obtained even with 64 channels (51.91\u00b10.45%for one-shot, and 67.23\u00b10.65% for 5-shot). We will update Table 2 to have the best available results for CAVIA included for a more complete comparison. \n\n3.\tDue to the limited computational resource, we cannot afford tuning the optimal hyper-parameters via grid search. Alternatively, we have tried a small number of hyper-parameter configurations based on our numerical experience and chose the one with the optimal validation performance. \n\n4.\tIn our experiment on Omniglot, we followed the implementation of Reptile to split the 1623 character classes into 1200 training classes (including 100 validation classes for MAML and zero validation class for Reptile) and 423 test classes. \n\n5.\tWe would like to clarify that in Table 2, we do have listed a set of results under varying network channels for our method and the baselines as well. We expect the performances might be further improved with more extensive hyper-parameter tuning under additional computation budge.  \n\n6.\tSince the primal goal of this paper is to demonstrate the benefit of network pruning in reducing the inter-task overfitting, we focus ourselves on the variant of our method based on Reptile and CNN. We did not extensively tune our model with respect to other backbone network architectures, although the related results in Table 2 have already shown some promise of our method implemented with ResNet-18. \n\n7.\tPer your this suggestion, we have additionally applied the identical rule of hyper-parameter selection to the ablation study. The numerical results indicated that the current choice is still optimal among the considered parameter configurations. \n\n8.\tRegarding the comparison with other relevant methods, we remark that since our method was implemented using Reptile as the backbone meta-learner, we focus on the comparison to Reptile to show the benefit of meta-learning with network pruning. Please keep in mind that our principle of limiting the inter-task network capacity is orthogonal to that of the inner-task overfitting reduction methods such as CAVIA and MetaOptNet. We believe it is possible to develop more sophisticated techniques to achieve the best of both worlds in future study. We will add more discussions on the comparison to these state-of-the-art results in the revised paper. \n\nResponse to minor concerns:\n\nWe will fix all the minor issues in the revised paper along with which the code will be released.", "title": "Reply to Official Blind Review #1 Part(2/2)"}, "rJlzd_7_sS": {"type": "rebuttal", "replyto": "SkxXjkpqKr", "comment": "Thank you for the insightful review. We hope the main concerns can be addressed by the following clarification. \n\n1.\tThe degree of novelty of our method is higher than you suggest. We contribute several new insights on generalization theory and algorithm at the intersection of optimization-based meta-learning and non-convex sparse learning which to our knowledge has not been studied in prior work. Particularly, the sparse generalization theory established in our paper is applicable to non-convex deep models and thus contrasts itself from the existing vast body of results which are mostly about parameter estimation and support recovery consistency for convex problems. \nConcerning the increased computational complexity for achieving the practically improved generalization performance, we believe such a trade-off is reasonable and common in statistical learning with sparsity.\n\n2.\tSince we use deep neural nets as the base learner, the pre-training phase should be beneficial for generating a good initialization for further processing with network pruning. Our offline numerical results did suggest that sufficient pre-training always leads to better generalization performance than simply doing meta-training and pruning from scratch.", "title": "Reply to Official Blind Review #3"}, "S1xjWdmOor": {"type": "rebuttal", "replyto": "HkgZbj0g5r", "comment": "Thank you for the insightful review. We hope the main concerns can be addressed by the following clarification. \n\n1.\tWe would like to clarify that the ablation study actually well supports our generalization theory.\n\n- The results presented in Theorem 1&2 basically show that the meta-generalization gap bounds have polynomial dependence on the sparsity level rather than the size of the meta-initialization network. The ablation study results in Figure 3 affirmatively confirm this theoretical prediction by showing that the sparse meta-initialization network did reduce the gap between training and testing accuracy. Therefore, network pruning is beneficial, both in theory and practice, for making the meta-learner less overfitting. \n\n- To further justify the improved generalization accuracy with re-training, we comment that since the obtained sparse meta-initialization network generalizes well, it is expected to serve as a good initialization for future re-training via SGD. Then roughly speaking, according to the SGD stability theory in [Hardt et al., 2016] the output dense network will also generalize well (in high probability) if the re-training phase converges fast enough. We will update the paper by explicitly remarking this point in the related discussion. \n\n[Hardt et al., 2016] Hardt M, Recht B, Singer Y. Train faster, generalize better: Stability of stochastic gradient descent, ICML, 2016: 1225-1234.\n\n2. Yes, the outside risk should be evaluated on the query set. This typo can be easily fixed with almost no impact on the technical proofs of the theoretical results.\n\n3. Sorry for the confusion. The curves in Figure 1 correspond to the experiments on MiniImageNet. Although still being relatively large, the generalization gap of the sparse network is obviously reduced in comparison to the dense network. Per your suggestion, we will update the figure layout for better visualization. \n\n4. Concerning computational complexity, the generalization performance gain of our method is achieved at the price of increased computational cost mainly due to the additional (iterative) network pruning and re-training steps. We believe such a trade-off is reasonable and common in iterative hard thresholding methods for statistical learning with sparsity. \n\n5. We agree that the total number of classes should play a role in the performance gap between the considered datasets. We also think that another important factor is the regularity of image. Different from the natural scene images in MiniImageNet, the character images in Omniglot have relatively simpler foreground and background, and thus are easier to be classified.\n\n6. Regarding the motivation to deal with inter-task overfitting rather than the inner-task overfitting, since the optimization-based meta-learning is designed to learn fast from small amount of data in future tasks with the help of meta-learner, we conjecture that the former would have higher impact on the overall generalization performance than the latter. It is absolutely an interesting future work to jointly handle the inter-task and inner-task overfitting, e.g., through simultaneously limiting the capacity of the context parameters for inner-task update as suggested by CAVIA and MetaOptNet. \n\n7. The binary mask is generalized based on a hard thresholding operation to preserve the desired portion (say 50%) of parameters with top absolute values in each layer. \n\n8. The training accuracy results on the considered data sets are available for check at: https://drive.google.com/open?id=1OtaajWwga_0dTWx8CM8oObT3S7DJiG_f\n", "title": "Reply to Official Blind Review #4"}}}