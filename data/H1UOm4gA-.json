{"paper": {"title": "Interactive Grounded Language Acquisition and Generalization in a 2D World", "authors": ["Haonan Yu", "Haichao Zhang", "Wei Xu"], "authorids": ["haonanyu@baidu.com", "zhanghaichao@baidu.com", "wei.xu@baidu.com"], "summary": "Training an agent in a 2D virtual world for grounded language acquisition and generalization.", "abstract": "We build a virtual agent for learning language in a 2D maze-like world. The agent sees images of the surrounding environment, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher\u2019s language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the world, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably interpolates and extrapolates to interpret sentences that contain new word combinations or new words missing from training sentences. The new words are transferred from the answers of language prediction. Such a language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms five comparison methods for interpreting zero-shot sentences. In addition, we demonstrate human-interpretable intermediate outputs of the model in the appendix.", "keywords": ["grounded language learning and generalization", "zero-shot language learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This manuscript was reviewed by 3 expert reviewers and their evaluation is generally positive. The authors have responded to the questions asked and the reviewers are satisfied with the responses. Although the 2D environments are underwhelming (compared to 3D environments such as SUNCG, Doom, Thor, etc), one thing that distinguishes this paper from other concurrent submissions on the similar topics is the demonstration that \"words learned only from a VQA-style supervision condition can be successfully interpreted in an instruction-following setting.\" "}, "review": {"B1JBH18gf": {"type": "review", "replyto": "H1UOm4gA-", "review": "This paper introduces a new task that combines elements of instruction following\nand visual question answering: agents must accomplish particular tasks in an\ninteractive environment while providing one-word answers to questions about\nfeatures of the environment. To solve this task, the paper also presents a new\nmodel architecture that effectively computes a low-rank attention over both\npositions and feature indices in the input image. It uses this attention as a\ncommon bottleneck for downstream predictors that select actions and answers to\nquestions. The paper's main claim is that this model architecture enables strong\ngeneralization: it allows the model to succeed at the instruction following task\neven when given words it has only seen in QA contexts, and vice-versa.\nExperiments show that on the navigation task, the proposed approach outperforms\na variety of baselines under both a normal data condition and one requiring\nstrong generalization.\n\nOn the whole, I think this paper does paper does a good job of motivating the\nproposed modeling decisions. The approach is likely to be useful for other\nresearchers working on related problems. I have a few questions about the\nevaluation, but most of my comments are about presentation.\n\nEVALUATION\n\nIs it really the case that no results are presented for the QA task, or am I\nmisreading one of the charts here? Given that this paper spends a lot of time\nmotivating the QA task as part of the training scenario, I was surprised not to\nsee it evaluated. \n\nAdditionally, when I first read the paper I thought that the ZS1 experiments\nfeatured no QA training at all. However, your response to one of the sibling\ncomments suggests that it's still a \"mixed\" training setting where the sampled\nQA and NAV instances happen to cover the full space. This should be made more\nclear in the paper. It would be nice to know (1) how the various models perform\nat QA in both ZS1 and ZS2 settings, and (2) what the actual performance is NAV\nalone (even if the results are terrible).\n\nMODEL PRESENTATION\n\nI found section 2 difficult to read: in particular, the overloading of \\Phi\nwith different subscripts for different output types, the general fact that\ne.g. x and \\Phi_x are used interchangeably, and the large number of different\nvariables. My best suggestions are to drop the \\Phis altogether and consider\nusing text subscripts rather than coming up with a new name for every variable,\nbut there are probably other things that will also help.\n\nOTHER NOTES\n\n- This paper needs serious proofreading---just in the first few pages the errors\n  I noticed were \"in 2D environment\" (in the title!), \"such capability\", \"this\n  characteristics\", \"such language generalization problem\", \"the agent need to\",\n  \"some early pioneering system\", \"commands is\". I gave up on keeping track at\n  this point but there are many more.\n\n- \\phi in Fig 2 should be explained by the caption.\n\n- Here's another good paper to cite for the end of 2.2.1:\n  https://arxiv.org/pdf/1707.00683.pdf.\n\n- The mechanism in 2.2.4 feels a little like\n  http://aclweb.org/anthology/D17-1015\n\n- I don't think the content on pages 12, 13, and 14 adds much to the\n  paper---consider moving these to an appendix.", "title": "Review", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1KF2Z5xf": {"type": "review", "replyto": "H1UOm4gA-", "review": "[Overview]\nIn this paper, the authors proposed a unified model for combining vision, language, and action. It is aimed at controlling an agent in a virtual environment to move to a specified location in a 2D map, and answer user's questions as well. To address this problem, the authors proposed an explicit grounding way to connect the words in a sentence and spatial regions in the images. Specifically, By this way, the model could exploit the outputs of concept detection module to perform the actions and question answering as well jointly. In the experiments, the authors compared with several previous attention methods to show the effectiveness of the proposed concept detection module and demonstrated its superiority on several configurations, including in-domain and out-of-domain cases.\n\n[Strengths]\n\n1. I think this paper proposed interesting tasks to combine the vision, language, and actions. As we know, in a realistic environment, all three components are necessary to complete a complex tasks which need the interactions with the physical environments. The authors should release the dataset to prompt the research in this area.\n\n2. The authors proposed a simple method to ground the language on visual input. Specifically, the authors grounded each word in a sentence to all locations of the visual map, and then perform a simple concept detection upon it. Then, the model used this intermediate representation to guide the navigation of agent in the 2D map and visual question answering as well.\n\n3. From the experiments, it is shown that the proposed model outperforms several baseline methods in both normal tasks and out-of-domain ones. According to the visualizations, the interpreter could generate meaningful attention map given a textual query.\n\n[Weakness]\n\n1. The definition of explicit grounding is a bit misleading. Though the grounding or attention is performed for each word at each location of the visual map. It is a still kind of soft-attention, except that is performed for each word in a sentence. As far as I know, this has been done in several previous works, such as: (a). Hierarchical question-image co-attention for visual question answering (https://scholar.google.com/scholar?oi=bibs&cluster=15146345852176060026&btnI=1&hl=en). Lu et al. NIPS 2016. (b). Graph-Structured Representations for Visual Question Answering. Teney et al. arXiv 2016. At most recent, we have seen some more explicit way for visual grounding like: (c). Bottom-up and top-down attention for image captioning and VQA (https://arxiv.org/abs/1707.07998). Anderson et al. arXiv 2017.\n\n2. Since the model is aimed at grounding the language on the vision based on interactions, it is worth to show how well the final model could ground the text words to each of the visual objects. Say, show the affinity matrix between the words and the objects to indicate the correlations.\n\n[Summary]\n\nI think this is a good paper which integrates vision, language, and actions in a virtual environment. I would foresee more and more works will be devoted to this area, considering its close connection to our daily life. To address this problem, the authors proposed a simple model to ground words on visual signals, which prove to outperform previous methods, such as CA, SAN, etc. According to the visualization, the model could attend the right region of the image for finishing a navigation and QA task. As I said, the authors should rephrase the definition of explicit grounding, to make it clearly distinguished with the previous work I listed above. Also, the authors should definitely show the grounding attention results of words and visual signal jointly, i.e., showing them together in one figure instead of separately in Figure 9 and Figure 10.\n", "title": "Review from AnonReviewer3", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJLqaM7bz": {"type": "review", "replyto": "H1UOm4gA-", "review": "The paper introduces XWORLD, a 2D virtual environment with which an agent can constantly interact via navigation commands and question answering tasks. Agents working in this setting therefore, learn the language of the \"teacher\" and efficiently ground words to their respective concepts in the environment. The work also propose a neat model motivated by the environment and outperform various baselines. \n\nFurther, the paper evaluates the language acquisition aspect via two zero-shot learning tasks -- ZS1) A setting consisting of previously seen concepts in unseen configurations ZS2) Contains new words that did not appear in the training phase. \n\nThe robustness to navigation commands in Section 4.5 is very forced and incorrect -- randomly inserting unseen words at crucial points might lead to totally different original navigation commands right? As the paper says, a difference of one word can lead to completely different goals and so, the noise robustness experiments seem to test for the biases learned by the agent in some sense (which is not desirable). Is there any justification for why this method of injecting noise was chosen ? Is it possible to use hard negatives as noisy / trick commands and evaluate against them for robustness ?  \n\nOverall, I think the paper proposes an interesting environment and task that is of interest to the community in general. The modes and its evaluation are relevant and intuitions can be made use for evaluating other similar tasks (in 3D, say). ", "title": "interesting contribution", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJGGiTcQM": {"type": "rebuttal", "replyto": "H1UOm4gA-", "comment": "For AC, reviewers, and others: to check the revisions, please compare the original version (modified: 27 Oct 2017, 13:14) and the latest version (modified: 02 Jan 2018, 10:46). Most changes were made according to the reviewers' comments. Some minor changes were made to improve the presentation. ", "title": "Versions"}, "Skq3gyaWf": {"type": "rebuttal", "replyto": "B1JBH18gf", "comment": "Thanks for your comments! They really help a lot.\n\nFirst, thanks for suggesting adding the results for QA. Originally we intended to use QA as an auxiliary task to help train NAV. We didn't think of adding results for it (although we indeed had some records showing how well different methods perform in QA during the training). In the revised paper, we have included the QA classification accuracies in the normal, ZS1 and ZS2 settings (Figure 6 c, Figure 7 c and f). We believe that this addition actually demonstrates the generalization ability of our model even better (not only in NAV but also in QA). Because now we also evaluate QA in the test, we modify all the related paragraphs across the paper to emphasize this addition.\n\nWe believe that the original text already clarifies (section 4.4 when defining ZS1) that ZS1 is about excluding word pairs from both NAV commands and QA questions, but not about training NAV alone. Note that training both NAV and QA together does not necessarily imply that the sampled NAV and QA instances cover the full space. For ZS1, a subspace of sentences (containing certain word pairs) is not covered. For ZS2, a different subspace of sentences (containing certain new words) is not covered. In other words, our zero-shot setting is not achieved by turning off either NAV or QA, but instead is by excluding certain sentence patterns from the training (for both NAV and QA).\n\nAs requested, we also added the performance of training NAV alone without QA in the normal language setting (Figure 6). This ablation is called NAVA in the revised experiment section. An analysis of this ablation was also added (section 4.3).\n\nThanks for suggesting citing [de Vries et al 2017] and [Kitaev and Klein 2017]. We find that they are indeed closely related to our work. We have cited and discussed them at the end of section 2.2.1 (-> 2.2.2) and section 2.2.4 (-> 2.2.5), respectively.\n\nWe have simplified the notations in section 2 to keep the presentation concise as suggested. We moved the content of pages 12, 13, and 14 to Appendix A. We went through a careful round of proofreading of the revised paper. While we are still trying to get others into the proofreading process, we have uploaded the second version of the paper to facilitate possible discussions on the OpenView.\n", "title": "Evaluation of QA in ZS1 and ZS2 added; NAV alone added"}, "SJnXZJpWz": {"type": "rebuttal", "replyto": "r1a6A4PJz", "comment": "Hi, we have updated our paper and added the details about our RL approach in Appendix E. Also, as R2 requested, we added the experiment results for training NAV alone. Please take a look at the revision if interested. Thank you!", "title": "RL details added in the revised paper"}, "HkQl1ypZf": {"type": "rebuttal", "replyto": "HJLqaM7bz", "comment": "Thanks for your comments.\n\nThe experiment of robustness is aimed at testing the agent in a\nscenario out of our control, such as executing navigation commands\nelicited by human after the training is done. In such case, we simply\nassume that the evaluator does not have any knowledge of the training\nprocess. A natural-language sentence elicited by the human evaluator\nmight convey a meaning that is similar or same to a sentence generated\nby our grammar, however, it might not be that well-formed (e.g.,\ncontaining extra irrelevant words). One simple way of simulating this\nscenario (incompletely) is to insert noisy word embeddings into the original sentence.\n\nThis preliminary experiment serves to provide some numbers to let the\nreaders have a rough idea about how well the agent will perform in an\nuncontrollable setting. However, because of its minor significance and\na possible misunderstanding, we have removed this section (4.5) from\nthe original paper.\n", "title": "Robustness was for a test in an uncontrollable setting; removed in case of possible confusion"}, "ryVnJ16Wz": {"type": "rebuttal", "replyto": "B1KF2Z5xf", "comment": "Thanks for your comments!\n\nWe agree with the reviewer that our original definition of explicit\ngrounding had some ambiguity. Thus we added several paragraphs to\nelaborate on this. Because then the original section 2.2.1 became so long that we divided it into two (2.2.1 and 2.2.2). Specifically, we rephrased section 2.2.1 (-> 2.2.2) by giving a detailed definition about what it means for a framework to have an explicit grounding strategy. We also discussed the similarities and differences of our grounding with the related work pointed out by the reviewer at the end of section 2.2.1 (-> 2.2.2).\n\nIn summary, our explicit grounding requires two extra properties on top\nof the soft attention mechanism:\n\n1) the grounding (image attention) of a sentence is computed based on\nthe grounding results of the individual words in that sentence (i.e., compositionality);\n\n2) in the framework, there are no other types of language-vision\nfusions besides this kind of groundings by 1)\n\nOne benefit of such an explicit grounding is that Eq 2 achieves a\nlanguage \"bottleneck\" for downstream predictors (as Reviewer 2 pointed\nout in the comment). This bottleneck is used for both NAV and QA. It\nimplies an \"independence of path\" property because given the image all\nthat matters for the NAV and QA tasks is the attention $x$ (Eq 2). It\nguarantees, via the model architecture, that the agent will perform\ncompletely in the same way on the same image even given different\nsentences as long as their $x$ are the same. Also because $x$ is\nexplicit, the roles played by the individual words of $s$ in\ngenerating $x$ are interpretable. This is in contrast to Eq 1 where\nthe roles of individual words are unclear. The interpretability\nprovides a possibility of establishing a link between language\ngrounding and prediction. We argue that these are crucial reasons that\naccount for our strong generalization in both ZS1 and ZS2 settings.\n\nWe have modified the original Figure 10 so that the image attention is\nvisualized jointly with the word attention. More examples are shown\nnow after the modification. Because of a limited space, we moved this\npart to Appendix A and divided it into six figures (Figure 10 - Figure 15).\n", "title": "Explicit grounding clarified"}, "rkCvlKvkf": {"type": "rebuttal", "replyto": "r1a6A4PJz", "comment": "1. The formula of the module A was not contained in the paper,\nprimarily due to the cut down of pages. It is just a feedforward\nsub-network that approximates the value function and generates the action\ndistribution, given the representation q (Fig. 2). However, as you\nhave asked, we now think it might be a good idea to add it back in the\npaper.\n\n2. Continuing on your first question, our RL method is simply\ncombining AC and Experience Replay (ER). ER is mainly used to\nstabilize AC while maintaining sample efficient. This might result in\nsome conflict between off-policy and on-policy, since the experiences\nsampled from the replay buffer were not generated by the current\npolicy. However, we find that this works well in practice (perhaps\nbecause of the small replay buffer). Similar work was also proposed\nrecently:\n\n\tSample Efficient Actor-Critic With Experience Replay, Wang et al, ICLR 2017.\n\nwhich is more sophisticated compared to ours.\n\nMore specifically, for every minibatch sampled from the replay buffer,\nwe have the following gradient:\n\n-\\sum_{k=0}^K(\\nabla_{\\theta}\\log\\pi_{\\theta}(a_k|x_k)+\\nabla_{\\theta}v_{\\theta}(x_k))(r+\\gamma\n v_{\\theta'}(x_k')-v_{\\theta}(x_k))\n\nwhere k is the sample index in the batch, (x_k, a_k, x_k') is the\nsampled transition, v is the value function to learn, \\theta is the\ncurrent parameters, \\theta' is the target parameters that have update\ndelay as in ER, and \\gamma is the discount factor. This gradient\nmaximizes the expected reward while minimizes the TD error.\n\n3. We did try only training NAV without QA. However, it only worked to\nsome extent for small-size maps like 3x3 or 5x5, with a much smaller\namount of object classes. It was difficult to converge on the current\nsetting of 7x7 maps with 119 object classes. Thus QA is important for\nthe learning. It is not uncommon to see some auxiliary tasks used for\na better convergence in RL problems, for example, language prediction\nand some other cost functions were used in parallel with RL:\n\n\tGrounded language learning in a simulated 3d world, Hermann et al,\n\tarxiv 1706.06551, 2017\n\nNote that QA only helps understanding of questions. The NAV commands\nand action control still need to be learned from RL. Questions and\ncommands are disjoint sets of sentences. The only common part is some\nlocal word or phrase patterns. More importantly, QA offers the\nopportunity to assess the transferring ability of the model across\ntasks, denoted as ZS2 in the paper.\n\nWe believe that such task of jointly learning language and vision is\nchallenging. Even for children, it is very likely that they learn from\na mixture of signals of the environment instead from a single task.\n", "title": "Thanks for your comments! They are really good questions and helpful."}}}