{"paper": {"title": "Towards a better understanding of Vector Quantized Autoencoders", "authors": ["Aurko Roy", "Ashish Vaswani", "Niki Parmar", "Arvind Neelakantan"], "authorids": ["aurkor@google.com", "avaswani@google.com", "nikip@google.com", "aneelakantan@google.com"], "summary": "Understand the VQ-VAE discrete autoencoder systematically using EM and use it to design non-autogressive translation model matching a strong autoregressive baseline.", "abstract": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models,  however, despite several recent improvements, the training of discrete latent variable models has remained  challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete autoencoder with EM and combining it with sequence  level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.\n", "keywords": ["machine translation", "vector quantized autoencoders", "non-autoregressive", "NMT"]}, "meta": {"decision": "Reject", "comment": "Strengths:\n\n- well-written \n- strong results for non-autoregressive NMT\n- a novel soft EM version of VQ-VAE\n\nWeaknesses:\n\n-  as pointed out by reviewers, the improvements are mostly not due to the VQ-VAE modification rather due to orthogonal (and not interesting) changes e.g., knowledge distillation. If there is a genuine contribution of VQ-VAE, it is small and required extensive parameter selection\n\n-  the explanations provided in the paper do not match the empirical results\n\nTwo reviewers criticize the experiments / experimental section: rigour / their discussion.  Overall, there is nothing wrong with the method but the experiments are not showing that the modification is particularly beneficial.  Given these results and also given that the method is not particularly novel (switching from EM to Soft EM in VQ-VAE), it is hard for me to argue for accepting the paper."}, "review": {"rkxyFEXDyE": {"type": "rebuttal", "replyto": "H1xKgVEHkN", "comment": "We thank the reviewer for reading our updated manuscript and for their feedback. We acknowledge that we missed this sentence in the introduction, since we focused on updating the experimental section and the writing therein as per the comments of R3. We will definitely go over the manuscript carefully and make the presentation more clear. Do you have any more specific aspects of the presentation that you would like to be improved ?", "title": "Thanks for your attention"}, "ByxiE4XPJ4": {"type": "rebuttal", "replyto": "ryl0iUEU14", "comment": "We thank the reviewer for carefully reading our updated manuscript and for raising their score. We acknowledge that it may be difficult for the reader to grasp the difference in implementation of VQ-VAE from Kaiser et al. We have now separated out the two VQ-VAE results in Table 1 in the Experiments section. It will be reflected in the final draft if the paper is accepted.\n", "title": "Thanks for your attention"}, "r1lCtgTx27": {"type": "review", "replyto": "HkGGfhC5Y7", "review": "This paper introduces a new way of interpreting the VQ-VAE, \nand proposes a new training algorithm based on the soft EM clustering. \n\nI think the technical aspect of this paper is written concisely. \nIntroducing the interpretation as hard EM seems natural for me, and the extension\nto the soft EM training is sound reasonable. \nMathematical complication is limited, this is also a plus for many non-expert readers. \n\nI'm feeling difficulties in understanding the experimental part.\nTo be honest, I think the experimental section is highly unorganized, not a quality for ICLR submission. \nI'm just wondering why this happens, given clean and organized technical sections...\n\nFirst, I'm confusing what is the main competent in the Table 1. \nIn the last paragraph of the page 6, it reads; \n\"Our implementation of VQ-VAE achieves a significantly better BLEU score and faster decoding speed compared to (10).\"\nHowever, Ref. (10) is not mentioned in the Table 1. Which BLEU is the score of Ref. (10)? \n\nSecond, terms \"VQ-VAE\", (soft?)\"EM\" and \"our {model, approach}\" are used in a confusing manner. \nFor example, in Table 1, below the row \"Our Results\", there are:\n- VQ-VAE\n- VQ-VAE with EM\n- VQ-VAE + distillation\n- VQ-VAE with EM + distillation\n\nThe \"VQ-VAE\" is not the proposed model, correct? \nMy understanding is that the proposal is a VQ-VAE solved via soft EM, which corresponds to \"VQ-VAE with EM\". \n\nThird, a paragraph \"Robustness of EM to Hyperparameters\" is mis-leading. \nThe figure 3 does not show the robustness against a hyperparameter. \nIt shows the BLEU against the number of \"samples\" (in fact, there is no explanation about what the \"samples\" means). \nI think hyperparameters are model constants such as the learning rate of the SGD, alpha-beta params for Adam, dimension of hidden units, number of layers, etc. The number of samples are not considered as a model hyperparameter; it's a dataset property. \nThe figure 5 shows the reconstructed images of the original VQ-VAE and the proposed VQ-VAE with EM. \nHowever, there is no explanation which hyperparameter is tested to assess \"the robustness to hyperparameters\". \n\nFourth, there is no experimental report on the image reconstructions (with CIFAR and SVHN) in the main manuscript. \nIn fact, there is a short paragraph that mentions about the SVHN results, \nbut it only refers to the appendix. \nI think appendix is basically used for additional results or proofs, that are not essential for the main message of the paper. \nHowever, performance in the image reconstruction is one of the main claims written in the abstract, the intro, etc. \nSo, the authors should include the image reconstruction results in the main body of the paper. \nOtherwise, claims about the image reconstructions should be removed from the abstract, etc. \n\n\n+ Insightful understanding of the VQ-VAE as hard EM clustering\n+ Natural and reasonable extension to soft-EM based training of the VQ-VAE\n-- Unorganized experiment section. This simply ruins the quality of the technical part. \n\n\n## after feedback\n\nSome of my concerns are addressed the feedback. \nConsidering the interesting technical parts, I raise the score upward, to the positive side. ", "title": "Experimental section", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1gBDs-Xk4": {"type": "rebuttal", "replyto": "Byearq9zpQ", "comment": "R3, we believe have addressed your concerns and clarified some of your points. Do you have an updated impression of our paper? Thanks for your consideration.", "title": "Updated paper"}, "BJxg8jZ71V": {"type": "rebuttal", "replyto": "ryxRaw5zpm", "comment": "R1, we believe have addressed your concerns and clarified some of your points. Do you have an updated impression of our paper? Thanks for your consideration.", "title": "Updated paper"}, "SyeQNj-714": {"type": "rebuttal", "replyto": "BygRDlrs6Q", "comment": "R4, we believe have addressed your concerns and clarified some of your points. Do you have an updated impression of our paper? Thanks for your consideration.", "title": "Updated paper "}, "H1xD-S0dh7": {"type": "review", "replyto": "HkGGfhC5Y7", "review": "General:\nThe paper presents an alternative view on the training procedure for the VQ-VAE. The authors have noticed that there is a close connection between the original training algorithm and the well-known EM algorithm. Then, they proposed to use the soft EM algorithm. In the experiments the authors showed that the soft EM allows to obtain significantly better results than the standard learning procedure on both image and text datasets.\n\nIn general, the paper shows a neat link between the well-known EM algorithm and the learning method for the VQ-VAE. I like the manner the idea is presented. Additionally, the results are convincing. I believe that the paper will be interesting for the ICLR audience.\n\nPros:\n+ The connection between the EM algorithms and the training procedure for the VQ-VAE is neat.\n+ The paper is very well written, all concepts are clear and properly outlined.\n+ The experiments are properly performed and all results are convincing.\n\nCons:\n- The paper is rather incremental, however, still interesting.\n- The quality of Figure 1, 2 and 3 (especially Figure 3) is unacceptable.\n- There is a typo in Table 6 (row 5: V-VAE \u2192 VQ-VAE).\n- I miss two references in the related work on training with discrete variables: REBAR (Tucker et al., 2017) and RELAX (Grathwohl et al., 2018).\n- The paper style is not compliant with the ICLR style.\n\n--REVISION--\nI would like to thank authors for their effort to improve quality of images. In my opinion the paper is nice and I sustain my initial score.", "title": " Training procedure for VQ-VAE is equivalent to the EM algorithm", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BygRDlrs6Q": {"type": "rebuttal", "replyto": "rkeQ2ZXspm", "comment": "We thank the reviewer for a careful reading of our paper and for their thoughtful review. Below we address the specific points raised by the reviewer:\n\n>>>\nThe first contribution of the paper is that it shows a simple VQ-VAE to work well on the EN-DE NMT task, in contrast to the results by Kaiser et al. (2018)...\n<<<\n\nThe main difference between the setup of Kaiser et al (2018) and the current work is the point \"Attention to Source Sentence Encoder\" in the Analysis section. The discrete latents in Kaiser et al (2018) are a function ae(x, y) where x is the input sequence and y is the target sequence. The dependence on x is in the form of attention layers. This makes it a much more complex function to learn and the authors of that work report that VQ indeed did not work for them, and so they had to resort to Product Quantization (referred to as DVQ in their work) with multiple codebooks to get a good result. We found that the attention to source sequence x to be an unnecessary complication, and so our latents are just a function ae(y), where y is the target sequence.\n\nWe do not attribute this to tuning the code-book size, we apologize for the misunderstanding. The robustness of EM is in the case when the latents ae(x, y) are also a function of x, see Figure 5 in the Appendix (\" Comparison of VQ-VAE (green curve) vs EM with different number of samples (yellow and blue curves) on the WMT\u201914 English-German translation dataset with a codebook size of 2 14, with the encoder of the discrete autoencoder attending to the output of the encoder of the source sentence as in Kaiser et al. (2018).\") The optimization problem is much harder in this case and we see that the VQ runs collapse while various versions of EM (with different number of samples) still give a good result.  The EM version does depend on the number of samples, but is much more stable compared to VQ even when the latents are a function of x. \n\nWe apologize if the \"Attention to source sentence encoder\" was not adequately clear: we had a statement to the effect of \"Also, removing this attention step results in more stable training particularly for large code-book sizes, see e.g., Figure 3.\", but it unfortunately seems to have got lost in a revision..\n\n>>>\nThe last claimed contribution (using denoising techniques) is hidden in the appendix...\n<<<\n\nDenoising autoencoders as used by Lample et al., were used in the context of learning better initial representations for unsupervised MT. We found that applying it to the context of discrete autoencoders like VQ-VAE can give some improvements\n in larger datasets like En-Fr. For En-De denoising VQ-VAE did not give us any improvement over VQ-VAE. We do not claim we invented denoising autoencoders, we write: \n\n\"On the larger English-French dataset, we show that denoising discrete autoencoders gives us a significant improvement (1.0 BLEU) on top of our non-autoregressive baseline (see Section D)\"\n\n>>>\nI'd like to see some of the results in the paper published eventually...\n<<<\n\nWe hope our first paragraph addresses the question of why VQ-VAE did not work in Kaiser et al. without product quantization, but worked in our case. We have made this more explicit in the latest version. Also note that all our VQ-VAE runs for MT do not have the attention to source sequence x, except Figure 5 where we explicitly mention this.\n\n>>>\n- the strong performance of the VQ-VAE baseline remains unexplained, and the claimed explanation contradicts empirical results.\n<<<\n\nWe hope that the previous paragraphs and the new draft addresses this concern. \n\n>>>\n- the new EM algorithm gives relatively small improvements, with hyperparameters that were likely selected based on test set scores .\n<<<\n\nThe hyperparameters were selected on WMT'13 and the results are reported on WMT'14. EM gives small improvements with knowledge distillation, because the optimization problem is much easier in this case. When the optimization problem is harder we see more gains from EM: \n\n1) In the setting when the latents are informed by the source sequence x, EM is much more stable than VQ-VAE (Figure 5) \n2) In the case when knowledge distillation is not used it gives a gain of +1.0 BLEU \n3) When the hidden dimension is smaller (256 or 384) instead of 512, we see gains of +1.3 BLEU and +0.6 BLEU respectively.\n\n>>>\n- most of the empirical gain is attributable to knowledge distillation, which is not a novel contribution\n<<<\n\nThat is a valid point, and we did indeed find knowledge distillation to be very important for good performance for NMT in addition to removing the attention to source sequence x.  \n\n\n", "title": "Reply to Reviewer 4"}, "HkeSbl7j6X": {"type": "review", "replyto": "HkGGfhC5Y7", "review": "This paper discusses VQ-VAE for learning discrete latent variables, and its application to NMT with a non-autoregressive decoder to reduce latency (obtained by producing a number of latent variables that is much smaller than the number of target words, and then producing all target words in parallel conditioned on the latent variables and the source text). The authors show the connection between the existing EMA technique for learning the discrete latent states and hard EM, and introduce a Monte-Carlo EM algorithm as a new learning technique. They show strong empirical results on EN-DE NMT with a latent Transformer (Kaiser et al. (2018)).\n\nThe paper is clearly written (excepting the overloaded appendix), and the individual parts of the paper are interesting, including the link between VQ-VAE training and hard EM, the Monte-Carlo EM, and strong empirical results. I'm less convinced that the paper as a whole delivers on what it promises/claims.\n\nThe first contribution of the paper is that it shows a simple VQ-VAE to work well on the EN-DE NMT task, in contrast to the results by Kaiser et al. (2018). The paper attributes this to tuning of the code-book, but the results (table 3) seem to contradict this, with a code-book size of 2^16 even slightly better than the 2^12 that is used subsequently. The reason for the performance difference to Kaiser et al. (2018) remains opaque. While interesting, the empirical effectiveness of Monte-Carlo EM is a bit disappointing, achieving +0.3 BLEU over the best configuration for EN-DE (after extensive hyperparameter tuning, seen in table 4), and -0.1 BLEU on EN-FR. Monte-Carlo EM also seems very sensitive to hyperparameters, namely the sample size (tables 4,5), contradicting the later claim that EM is robust to hyperparameters. The last claimed contribution (using denoising techniques) is hidden in the appendix, an application of an existing technique, and not compared to knowledge distillation (another existing technique).\n\nI'd like to see some of the results in the paper published eventually. However, the claims need to better match the empirical evidence, and for a paper that has \"better understanding\" in the title, I'd like to gain a better understanding of the differences to Kaiser et al. (2018) that make VQ-VAE fail for them, but not in the present case.\n\n+ clearly written paper\n+ interesting, novel EM algorithm for VQ-VAE\n+ strong empirical results on non-autoregressive NMT\n\n- the strong performance of the VQ-VAE baseline remains unexplained, and the claimed explanation contradicts empirical results.\n- the new EM algorithm gives relatively small improvements, with hyperparameters that were likely selected based on test set scores .\n- most of the empirical gain is attributable to knowledge distillation, which is not a novel contribution", "title": "interesting parts, but needs more rigour", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byearq9zpQ": {"type": "rebuttal", "replyto": "r1lCtgTx27", "comment": "We thank the reviewer for reading our paper. Below we address specific points raised by the reviewer:\n\n>>>\nI'm feeling difficulties in understanding the experimental part.\nTo be honest, I think the experimental section is highly unorganized, not a quality for ICLR submission. \nI'm just wondering why this happens, given clean and organized technical sections...\n>>>\n\nWe have made an effort to clean up the experimental section part in the updated draft. We would appreciate specific comments to help us make the experimental section more readable and organized.\n\n>>>\nFirst, I'm confusing what is the main competent in the Table 1. \nIn the last paragraph of the page 6, it reads; \n\"Our implementation of VQ-VAE achieves a significantly better BLEU score and faster decoding speed compared to (10).\"\nHowever, Ref. (10) is not mentioned in the Table 1. Which BLEU is the score of Ref. (10)? \n>>>\n\nThis should be fixed in the updated version.\n\n>>>\nSecond, terms \"VQ-VAE\", (soft?)\"EM\" and \"our {model, approach}\" are used in a confusing manner. \nFor example, in Table 1, below the row \"Our Results\", there are:\n- VQ-VAE\n- VQ-VAE with EM\n- VQ-VAE + distillation\n- VQ-VAE with EM + distillation\n\nThe \"VQ-VAE\" is not the proposed model, correct? \nMy understanding is that the proposal is a VQ-VAE solved via soft EM, which corresponds to \"VQ-VAE with EM\". \n<<<\n\nYes VQ-VAE is not the proposed model, although we report it in \"Our Results\" because the implementation is different from Kaiser et al in two crucial aspects 1) No attention to source sequences for the discrete latents 2) Product Quantization (PQ) which the authors of Kaiser et al call DVQ is not being used. Hence we also report it in \"Our Results\".\n\n>>>\nThird, a paragraph \"Robustness of EM to Hyperparameters\" is mis-leading. \nThe figure 3 does not show the robustness against a hyperparameter. \nIt shows the BLEU against the number of \"samples\" (in fact, there is no explanation about what the \"samples\" means). \nI think hyperparameters are model constants such as the learning rate of the SGD, alpha-beta params for Adam, dimension of hidden units, number of layers, etc. The number of samples are not considered as a model hyperparameter; it's a dataset property. \n>>>\n\nThe number of samples used for EM training of VQ-VAE is a hyperparameter, how is it a property of the dataset? You are free to choose any number of samples regardless of the dataset.\n\n>>>\nThe figure 5 shows the reconstructed images of the original VQ-VAE and the proposed VQ-VAE with EM. \nHowever, there is no explanation which hyperparameter is tested to assess \"the robustness to hyperparameters\". \n<<<\n\nOur apologies, this should be robustness to initialization of the codebook. VQ-VAE/K-means is much more sensitive to a good initialization as compared to EM.\n\n>>>\nFourth, there is no experimental report on the image reconstructions (with CIFAR and SVHN) in the main manuscript. \nIn fact, there is a short paragraph that mentions about the SVHN results, \nbut it only refers to the appendix. \nI think appendix is basically used for additional results or proofs, that are not essential for the main message of the paper. \n\nHowever, performance in the image reconstruction is one of the main claims written in the abstract, the intro, etc. \nSo, the authors should include the image reconstruction results in the main body of the paper. \nOtherwise, claims about the image reconstructions should be removed from the abstract, etc. \n>>>\n\nWe have removed all image references from the main section and now only report it in the Appendix. We hope this helps improving the quality and clarity of the main paper.", "title": "Reply to Reviewer 3"}, "ryxRaw5zpm": {"type": "rebuttal", "replyto": "SylKiwqf6m", "comment": "Continued from above:\n\n>>>\n- There is no justification of using *causal* self-attention...\n<<<\n\nAttention to the source embeddings is a natural and justified way to inform the discrete latents (see e.g., [2]). Also, the attention to source sequences for generating the discrete latents from the targets is not causal. The only causal attention layers are for encoding the inputs and in the autoregressive decoder from the latents. \n\n>>>\n- As for the experimental evaluation results: it seems that distillation...\n<<<\n\nIn page 7 of the current draft (and page 6 of the original submission), we say \"Additionally, we see a large improvement in the performance of the model by using sequence-level distillation (12), as has been observed previously in non-autoregressive models (6; 16).\" We have also added a sentence to this effect in the conclusion in the updated draft.\n\n>>>\n- What is the significance of the observed differences in BLEU scores? ...\n<<<\n\nWe point the reviewer to [1, 2, 3, 4] which are the current state-of-the-art literature on non-autoregressive machine translation. None of these works report average or std devs on several runs, instead they select the best hyperparameter from a validation set and report the result of this model on a held out test set (which is a perfectly valid thing to do).\n\n>>>\n- It seems that the tuning of the number of discrete latent codes...\n<<<\n\nThe optimal hyperparameters are selected on the validation set (WMT'13) while the reported results are on the held out WMT'14 test set. This is standard practice in the NMT literature. We have made this more explicit in the latest draft.\n\n>>>\n- It seems that all curves in figure 3 collapse from about 45 BLEU...\n<<<\n\nWe have made this figure larger so that it is easier to read. The figure is intended to show the robustness of the EM runs vs the VQ-VAE runs: the collapsed curve is  a VQ-VAE run with bad initialization, while the other superimposed curves are different EM runs of the same configuration with various values of the number of samples. \n\n[1] https://openreview.net/forum?id=B1l8BtlCb\n[2] http://proceedings.mlr.press/v80/kaiser18a/kaiser18a.pdf\n[3] https://openreview.net/forum?id=r1gGpjActQ\n[4] https://arxiv.org/abs/1802.06901\n", "title": "Reply to Reviewer 1 continued"}, "SylKiwqf6m": {"type": "rebuttal", "replyto": "BJeUvaXU2Q", "comment": "We thank the reviewer for taking the time to read our paper. Below we address the specific points raised by the reviewer:\n\n>>>\nOverall the technical writing in the paper is sloppy....\n<<<\n\nIn this work, we improve upon VQ-VAE to learn shorter latent representations of a target sentence in order to speed up MT, rather than to train a generative model. We achieve considerable speedup in decoding state of the art NMT models without much loss in BLEU (a universally accepted metric for translation quality), which has powerful implications for real world, production level MT systems. While evaluating the improvements of our training for generative modeling is interesting, our focus is on using VQ-VAE for a practical task. \n\nMoreover, we have now added a paragraph on the generative process (Page 3). We hope that this will clarify some of the content. We welcome the reviewer to share what they think is \"sloppy\" and \"imprecise\", and what would help us further improve the content of the paper.\n\n>>>\nThe technical presentation of the work by the authors starts only at page 5...\n<<<\n\nOur goal is to use the autoencoder from VQ-VAE as a tool to compress the target sentence for fast decoding. We therefore chose to focus on the part of the algorithm, describing it's connection to hard-EM and our improvements on it using EM. We would appreciate concrete suggestions to improve the content.\n\n>>>\nQuantitative experimental evaluation is limited to a machine translation task...\n<<<\n\nThe main focus of our work is to design a better non-autoregressive machine translation model and which is an area of active research (see for e.g., [1, 2, 3, 4]). None of those works evaluate their proposed method on datasets other than machine translation because the goal of their work is non-autoregressive MT. We do not care about generative modeling of images with VQ-VAE because plenty of other models do it much better (for e.g., a GAN/VAE/PixelCNN++). \n\nThe keywords of our paper states: \"machine translation, vector quantized autoencoders, non-autoregressive, NMT\", while the TL;DR of our submission is \"Understand the VQ-VAE discrete autoencoder systematically using EM and use it to design non-autogressive translation model matching a strong autoregressive baseline.\" \n\n>>>\n- The related work section (4) provides a rather limited overview of relevant related work...\n<<<\n\nAgain, the main aim of our work is to speed up the decoding for real world Neural Machine Translation (NMT) systems, which is an active area of research (see e.g., [1, 2, 3, 4]). We have focussed on generative models that are practically relevant to non-autoregressive NMT and because of page limitations we have not been able to include every paper on generative modeling. If we have missed relevant references we would appreciate if the reviewer would let us know what they are.\n \n[1] https://openreview.net/forum?id=B1l8BtlCb\n[2] http://proceedings.mlr.press/v80/kaiser18a/kaiser18a.pdf\n[3] https://openreview.net/forum?id=r1gGpjActQ\n[4] https://arxiv.org/abs/1802.06901", "title": "Reply to Reviewer 1"}, "Byg4DS-Zp7": {"type": "rebuttal", "replyto": "H1xD-S0dh7", "comment": "We thank the reviewer for taking the time to read our paper and for the useful comments to help improve our presentation! We have increased the resolution of the images by moving some of them to the appendix, and hope that fixes the visibility issue for the figures. We have also fixed the typo - thanks for pointing it out! We have added the two references pointed out and have also fixed the bibliography style to be the ICLR style. Please let us know if we can improve anything else.\n", "title": "Reply to reviewer 2"}, "BJeUvaXU2Q": {"type": "review", "replyto": "HkGGfhC5Y7", "review": "Summary: \n\nThis paper presents a new training algorithm for vector-quantized autoencoders (VQVAE), a discrete latent variable model akin to continuous variational autoencoders.\nThe authors propose a soft-EM training algorithm for this model, that replaces hard assignment of latent codes to datapoints with a weighted soft-assignment.\n\nOverall the technical writing in the paper is sloppy, and the presentation of the generative model takes the form of an algorithmic description of the training algorithm, rather than being a clear definition of the generative model itself.\n\nThe technical presentation of the work by the authors starts only at page 5 (taking less than a full page), after several pages of imprecise presentation of previous and related work. The paper could be significantly improved by making this preceding material more concise and rigorous. \n\nQuantitative experimental evaluation is limited to a machine translation task, which is rather uncommon in the literature on generative latent variable models. I would expect evaluation in terms of held-out data log-likelihood (ie bits-per-dimension) used in probabilistic generative models, and possibly also using measures from the GAN literature such as inception scores. Datasets that are common include CIFAR-10 and resized variants of the imagenet dataset. \t \n\n\nSpecific comments:\n\n- Please adhere to the ICLR template bibliography style, which is far more readable than the style that you used. \n\n- Figure 1 does not seem to be referenced in the text. \n\n- The last paragraph of section 2.1 is unclear. It mentions a sampling a sequence of latent codes. The notion of sequentiality has not been mentioned before, and it is not clear what it refers to in the context of the model defined so far up to that point. \n\n- The technical notation is very sloppy. \n* In numerous places the paper refers to the joint distribution P(x1,\u2026,x_n, z1, \u2026, zn) without defining that the distribution factorizes across the samples (xi,zi), and without specifying the forms of p(zi) and p(xi|zi). \n* This makes that claims such as \u201ccomputing the expectation in the M step (Equation 11) is computationally infeasible\u201d are not verifiable. \n\n- Please be clear about how much is gained by replacing the exact M-step with a the one based on the samples from the posterior computed in the E-step. \n\n- What is the reason to decode the weighted average of the embedding vectors, rather than decoding all of them, and updating the decoder in a weighted manner?\n\n- reference 14 for Variational autoencoders is incorrect, please use the following citation instead: \n@InProceedings{kingma14iclr,\n  Title                    = {Auto-Encoding Variational {B}ayes},\n  Author                   = {D. Kingma and M. Welling},\n  Booktitle                = {{ICLR}},\n  Year                     = {2014}\n}\n\n- The related work section (4) provides a rather limited overview of relevant related work. \nHalf of it is dedicated to recent advances in machine translation, which does not bear a direct connection to the technical material presented in section 3.\n\n- There is no justification of using *causal* self-attention on the source embedding, is this a typo?\n\n- As for the experimental evaluation results: it seems that distillation is a much more critical factor to achieve good performance than the proposed EM training of the VQ-VAE model. Unfortunately, this fact goes unmentioned when discussing the experimental results. \n\n- What is the significance of the observed differences in BLEU scores? Please report average performance and standard deviations over several runs with randomized parameter initialization and batch scheduling. \n\n- It seems that the tuning of the number of discrete latent codes (table 2 in appendix) and other hyper-parameters (table 3 in appendix) was done on the test set, which is also used to compare to related work. A separate validation set should be used for hyper parameter tuning in machine learning experiments.\n\n- It seems that all curves in figure 3 collapse from about 45 BLEU to values around 17 BLEU, why is this? The figure is hard to read since poor quality, and curves that are superposed. \n", "title": "A soft-EM training algorithm for vector-quantized autoencoders", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}