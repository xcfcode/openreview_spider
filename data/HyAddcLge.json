{"paper": {"title": "Revisiting Distributed Synchronous SGD", "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"], "summary": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "keywords": ["Optimization", "Deep learning", "Applications"]}, "meta": {"decision": "Reject", "comment": "Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that \"backup workers\" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware."}, "review": {"S1zYdhqSx": {"type": "rebuttal", "replyto": "rJW29ruSg", "comment": "We thank the reviewer for the review and suggestions, and address the concerns raised in detail below.\n\n1. The reviewer suggests that stragglers are not an issue with dedicated machines and proper implementation of communications, which is counter to our experiences. The issue of stragglers is exacerbated with larger number of machines, posing a challenge to scalability -- perhaps the reviewer has been using a relatively smaller number of machines? Our experiments were all conducted on Google's internal cluster with containers (https://research.google.com/pubs/pub43438.html). We also point out that the straggler phenomenon has been observed by others (Dean & Barroso (2013)) in production clusters. As discussed in both our paper and Dean & Barroso (2013), stragglers may occur from multiple reasons, including failing hardware, software bugs, contention of resources, pre-emption of jobs. Communication bottlenecks which the reviewer referred to are only one possible cause.\n\nFurthermore, not everyone has access to dedicated machines and clusters. In our cluster, containers share underlying physical hardware with imperfect isolation that shows up in the tail when pushing higher utilization. Even with elastic cloud resources such as EC2, one has to share network resources with other jobs and processes. Pre-emption of machines are also possible with spot instances. Our solution is a simple, elegant solution that attempts to address the *effect* of stragglers, rather than having multiple technically challenging approach to solve each cause.\n\nThe reviewer's suggestion of an \"MPI_Reduce\" (or AllReduce) is interesting and could help to alleviate some communication bottlenecks (which we emphasize again is not the only cause of stragglers) in synchronous training. We do also point out that a similar approach for asynchronous training could be technically harder to implement and analyze.\n\n\n3. It is true that our solution only addresses straggling workers but not parameter servers. In practice, however, one typically has an order of magnitude more workers than parameter servers. Hence, we have found that that straggling workers are a greater problem than straggling parameter servers. The issue of slow parameter servers is something we would leave for future work, should it become a significant bottleneck in practice.\n\n\n4. The reviewer is correct that hyperparameters need to be tuned with each batch size. (For more discussions, we refer the reviewer to the below comments / discussion with Ioannis Mitliagkas on tuning.) In general, we tried our best to tune the hyperparameters for asynchronous training; in contrast we spent less effort tuning the synchronous training, but nevertheless found it to produce better results for the reported models and datasets. Hence, we feel confident that our experiments demonstrate that synchronous training of deep models can outperform asynchronous training.\n\nWe agree with the reviewer that our results could be strengthened by averaging over multiple runs. Unfortunately, doing so is rather expensive -- 10 runs of the Inception experiments could cost ~150,000 GPU hours. We would like to reassure the reviewer that we did not cherry-pick results, i.e. we reported results from a random run. We also found results to be pretty consistent as we were fine-tuning our implementation. In addition, other real users within the company, training similar models for their work, have also observed similar results.", "title": "Thank you for the review"}, "HJzncBqVg": {"type": "rebuttal", "replyto": "HJ9ud6I4e", "comment": "We thank the reviewer for the comments and suggestion.\n\nIndeed, our solution was designed with the specific distribution of learners' efficiencies in mind, and may not necessarily be the ideal solution for other distributions. However, we contend that this particular distribution is the one that matters in practice, since it is what we empirically observe in real production workloads (see Figures 3, 4), and it is what makes a fully synchronous (no backups) solution difficult to use in practice. Similar tail distributions have been observed by Dean & Barroso (2013) in other production systems with similar latency-related problems. Our contribution lies in demonstrating the feasibility of our solution in a synchronous machine learning system.\n\nFurthermore, we believe that distributions that are likely to cause problems for our solution, e.g. extremely high variability in learners' efficiencies, are also problematic for Async-Opt and naive synchronous optimization (with no backups), due to increased staleness and straggler effects. That said, we again emphasize that these are not distributions we observe in practice, and thus of lesser concern to us.", "title": "Thank you for the review"}, "S11lFB54x": {"type": "rebuttal", "replyto": "Hy8b0jINl", "comment": "We thank the reviewer for the comments and suggestions.\n\nWe would like to point out that our solution does not require extra budget. Indeed, given a fixed number of machines N, we can reserve a small number b, so that running with b out of N workers as backups would be faster than a fully synchronous solution with N workers. This is the premise of discussion in Section 3.1 on straggler effects.\n\nWhile we agree that the proposed solution looks straightforward in retrospect, we would argue that our contributions lie in motivating and demonstrating the feasibility of such a solution in a real-world production machine learning system. It is counter-intuitive and not immediately obvious that discarding the work of a few workers can in fact improve overall running time, since this reduces throughput relative to Async-SGD, and increases number of iterations to convergence relative to synchronous training (see Figure 5).\n\nThe reviewer's suggestion for controlling Async-SGD's staleness is an interesting one, and provides a different trade-off between accuracy and throughput, similar to the \"softsync\" approach of Zhang et al. (2015b). Our main goal in this paper, however, is to demonstrate that a synchronous solution is competitive with, and can outperform asynchronous solutions.\n\nWe also contend that asynchronous approaches do not completely resolve the staleness issues highlighted in Section 2.1, making analysis and understanding more difficult. On the other hand, synchronous solutions are straightforward to analyze, and more predictable behavior that aids debugging and improves reproducibility.", "title": "Thank you for the review"}, "rkaxR_44e": {"type": "rebuttal", "replyto": "HybKKdENx", "comment": "Thanks! I enjoyed reading the paper! ", "title": "thanks"}, "HybKKdENx": {"type": "rebuttal", "replyto": "S1rLQ_4Ng", "comment": "Our proposed solution (sync with backups) is indeed faster than sync without backups. My earlier comment about \"equivalence\" was in terms of the algorithm's output and convergence -- running 50+3 sync with backups gives the same algorithmic convergence than running 50 sync without backups, but has a faster per-iteration time. Apologies if the earlier statement was unclear.\n\nYour question about proportional or marginal gains does not have a straightforward answer, unfortunately, as it depends on the percentage of backup workers used. For example, if we had used 90+10 instead of 100 workers (see Figure 3, 4), we get a gain of ~30%, going from 2.6s to <1.8s, which is more than proportional. However, dropping more stragglers from 90+10 to 80+10 provides only marginal gains. Thus, our largest gains are when dropping only the final few stragglers.\n\nWe could also consider the problems in terms of additional resources instead of dropping stragglers. Although we do not have the results available, one can expect that using 99 sync-without-backups has almost the same running time as 100 sync-without-backup. If we were to use 99+1 sync-with-backups instead, Figures 3,4 suggest we would achieve a >1% improvement in per-iteration time.\n\nFor the Inception experiments in Section 4.3, we do get faster speeds and convergence for async even after 53 machines. This can be seen from the fact that using 106 machines for async is faster than using 53 machines for async.", "title": "Sync-with-backups faster, but same output, as sync-without-backups"}, "S1rLQ_4Ng": {"type": "rebuttal", "replyto": "H1XINIVNx", "comment": "Thanks for your response. It is impressive that you have a solution to address the variability in worker completion times. My understanding is that this variability increases with #workers.\n\nMy question was how does this compare with simply using sync. You mention in your response above that \"using 50+3 workers for sync-with-backup is equivalent to using 50 workers for sync-without-backup\". However, as you describe in the paper just using 50 workers for sync without backup can have significant straggler slowdown arising from contention and synchronization with the server. So, you should be going faster than sync without backup for the extra 6% overhead of the backup workers to make sense. Is the speedup proportional to the resource costs of the extra workers you are using or marginal? \n\nAn additional minor point is that if the dataset achieves its peak speedup with say 50 workers (i.e. beyond 50 workers the increase in speedup is negligible), running async with 53 workers can actually hurt its performance (This seems like your baseline in 8(a)). The extra 3 workers in the async case will cause extra noise leading to divergence for the async case; and will cause unnecessary straggler/synchronization costs for the sync case. Hence, I was wondering how will the speedup look like for an appropriately chosen N (sync) with N+b (your method). \n\n\n\n", "title": "comparison with sync"}, "H1XINIVNx": {"type": "rebuttal", "replyto": "SJuUIa7Ng", "comment": "Figure 3 and 4 show that it takes extremely long to collect the final few gradients, and as such allow us to achieve high efficiency. For example, using 90 of 100 machines has a median epoch of < 1.8s, but using 100 of 100 machines has a media of about 2.6s.\n\nWe are a little confused about the comment on Figure 8a, which does show sync (with backup) results. These results are equivalent to running a fully synchronous training (without backups) -- for example, using 50+3 workers for sync-with-backup is equivalent to using 50 workers for sync-without-backup.", "title": "Efficiency"}, "SJuUIa7Ng": {"type": "rebuttal", "replyto": "HyAddcLge", "comment": "Is this technique efficient? If you are using 5-10% extra resources (backup workers), do you get at least 5-10% speedup over sync SGD? Figure 8(a) does not have any sync results. It is well understood that async may sometimes never converge to the correct results.", "title": "efficiency"}, "H1lppI_Qe": {"type": "rebuttal", "replyto": "By2MMx_7e", "comment": "Yes, every worker has access to the same entire dataset, which is a typical setting for distributed TensorFlow jobs.", "title": "All workers access all data."}, "Sy7Y6IdQe": {"type": "rebuttal", "replyto": "SJuqCK7mx", "comment": "We thank Ioannis for his suggestions and feedback.\n\nAs discussed in our private communications, the learning rate of 0.045 for Inception was tuned for asynchronous training and thus the best known configuration to our team. It is of course impossible to discount the possibility that there are better configurations that we have not tried, but this is an issue faced by all researchers and practitioners. Nevertheless, we have tried our best to obtain the best learning rates for asynchronous training.\n\nIn light of the work uncovering the relation between asynchrony and momentum, we are currently running some experiments with Inception with lower momentum, but have yet to find a setting of lower momentum that outperforms our current results.", "title": "Configurations tuned for asynchronous training"}, "By2MMx_7e": {"type": "review", "replyto": "HyAddcLge", "review": "Do you assume that each of the workers have the same full replica of the dataset?The paper claim that, when supported by a number of backup workers, synchronized-SGD \nactually works better than async-SGD. The paper first analyze the problem of staled updates\nin async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the \nauthors shows the effectiveness of the proposed method in applications to Inception Net\nand PixelCNN.\n\nThe idea is very simple, but in practice it can be quite useful in industry settings where \nadding some backup workders is not a big problem in cost. Nevertheless, I think the \nproposed solution is quite straightforward to come up with when we assume that \neach worker contains the full dataset and we have budge to add more workers. So, \nunder this setting, it seems quite natural to have a better performance with the additional \nbackup workers that avoid the staggering worker problem. And, with this assumtion I'm not \nsure if the proposed solution is solving difficult enough problem with novel enough idea. \n\nIn the experiments, for fair comparison, I think the Async-SGD should also have a mechanism \nto cut off updates of too much staledness just as the proposed method ignores all the remaining \nupdates after having N updates. For example, one can measure the average time spent to \nobtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD \nso that Async-SGD does not perform so poorly.", "title": "question", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hy8b0jINl": {"type": "review", "replyto": "HyAddcLge", "review": "Do you assume that each of the workers have the same full replica of the dataset?The paper claim that, when supported by a number of backup workers, synchronized-SGD \nactually works better than async-SGD. The paper first analyze the problem of staled updates\nin async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the \nauthors shows the effectiveness of the proposed method in applications to Inception Net\nand PixelCNN.\n\nThe idea is very simple, but in practice it can be quite useful in industry settings where \nadding some backup workders is not a big problem in cost. Nevertheless, I think the \nproposed solution is quite straightforward to come up with when we assume that \neach worker contains the full dataset and we have budge to add more workers. So, \nunder this setting, it seems quite natural to have a better performance with the additional \nbackup workers that avoid the staggering worker problem. And, with this assumtion I'm not \nsure if the proposed solution is solving difficult enough problem with novel enough idea. \n\nIn the experiments, for fair comparison, I think the Async-SGD should also have a mechanism \nto cut off updates of too much staledness just as the proposed method ignores all the remaining \nupdates after having N updates. For example, one can measure the average time spent to \nobtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD \nso that Async-SGD does not perform so poorly.", "title": "question", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJpPDf8Xg": {"type": "rebuttal", "replyto": "HkAXr3kme", "comment": "While there is an assumption that there are less than b stragglers, we would like to point out the following:\n1. Typically we set b to be about 5% of the total number of machines, and empirically we observe that the number of slow workers is less than this threshold (see Section 3.1). Thus, we believe that this is a weak assumption that almost always holds in practice.\n2. Furthermore, regardless of the true number of stragglers, our approach always shaves off the difference between the time taken to collect n gradient and (n-b) gradients, where the most significant savings occur.\n3. In the off-chance that there are more than b stragglers within an iteration, the correctness of the algorithm is not compromised.", "title": "Weak assumption"}, "SJuqCK7mx": {"type": "rebuttal", "replyto": "HyAddcLge", "comment": "The proposed technique of over-provisioning a few workers to improve the hardware efficiency of synchronous training gives a very welcome boost, as evidenced beautifully by Figure 6.\n\nHowever, as I have discussed in private communication with the authors, the comparison with asynchronous methods leaves a few questions regarding tuning.\u00a0\n\nLearning rate:\u00a0\n\nThe authors carefully describe the process of tuning the synchronous implementation. However no tuning is reported for the asynchronous implementation: the value is set to 0.045 for all configurations. Figure 7(a) shows that tuning the learning rate can cause a difference of almost a whole percentage point in test precision.\n1. What other values of LR have the authors tried for asynchronous training?\n2. Is 0.045 the best one for all configurations they report?\n3. Is there any chance that the 0.5% difference in precision between sync and async is due to insufficient LR tuning (as suggested by Figure 7(a))?\u00a0\n\n\nMomentum:\n\nOur results (https://arxiv.org/abs/1605.09774), that we discussed with the authors in August, suggest that asynchrony interacts with momentum, and that it is important for performance to tune your momentum when running asynchronously. The standard value 0.9 used throughout the paper is likely too much for asynchronous training.\u00a0\n\nThe authors have now kindly suggested that they might be able to test a few configurations with momentum less than 0.9. I think those results, if added, would be a great addition to this very good paper and contribute an extremely useful point to our understanding of this space.\n", "title": "Some discussion on tuning"}, "HkAXr3kme": {"type": "review", "replyto": "HyAddcLge", "review": "Do you assume that the number of slow learners are less than b?\n\nThis paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. \n\nMy main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:\n\n- provide more experiments to show the performance with different efficiency distributions of learners.\n- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.", "title": "assumption", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJ9ud6I4e": {"type": "review", "replyto": "HyAddcLge", "review": "Do you assume that the number of slow learners are less than b?\n\nThis paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. \n\nMy main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:\n\n- provide more experiments to show the performance with different efficiency distributions of learners.\n- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.", "title": "assumption", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}