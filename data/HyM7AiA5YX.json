{"paper": {"title": "Complement Objective Training", "authors": ["Hao-Yun Chen", "Pei-Hsin Wang", "Chun-Hao Liu", "Shih-Chieh Chang", "Jia-Yu Pan", "Yu-Ting Chen", "Wei Wei", "Da-Cheng Juan"], "authorids": ["haoyunchen@gapp.nthu.edu.tw", "peihsin@gapp.nthu.edu.tw", "newgod1992@gapp.nthu.edu.tw", "scchang@cs.nthu.edu.tw", "jypan@google.com", "yutingchen@google.com", "wewei@google.com", "dacheng@google.com"], "summary": "We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks.", "abstract": "Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.\n", "keywords": ["optimization", "entropy", "image recognition", "natural language understanding", "adversarial attacks", "deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample. The authors describe this as \"maximizing the complement entropy.\" Rather than adding the cross-entropy objective and the (negative) complement entropy term (since the complement entropy should be maximized while the cross-entropy is minimized), this paper proposes an alternating optimization framework in which first a step is taken to reduce the cross-entropy, then a step is taken to maximize the complement entropy. Extensive experiments on image classification (CIFAR-10, CIFAR-100, SVHN, Tiny Imagenet, and Imagenet), neural machine translation (IWSLT 2015 English-Vietnamese task), and small-vocabulary isolated-word recognition (Google Commands), show that the proposed two-objective approach outperforms training only to minimize cross-entropy. Experiments on CIFAR-10 also show that models trained in this framework have somewhat better resistance to single-step adversarial attacks. Concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion. The primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem."}, "review": {"rJe3xlA2yN": {"type": "rebuttal", "replyto": "BkxPea7hyV", "comment": "You are totally right. We did negate the complement entropy term (and added it to the primary objective) for maximizing complement entropy. We are sorry about the confusion and we will update the final manuscript to make this more clear: minimizing cross-entropy and maximizing complement entropy (e.g., in Algorithm 1).", "title": "Response to Area Chair1"}, "r1eubJAnJN": {"type": "rebuttal", "replyto": "HygCehX3yN", "comment": "Thanks for the comment. We summed the cross-entropy with the normalized complement entropy (Eq.3), and the corresponding advantages were discussed in Section 3.1.", "title": "Response to Area Chair1"}, "r1lArwq9Am": {"type": "rebuttal", "replyto": "rkg-IxqSAQ", "comment": "Thank you for the ideas. Yes, we indeed directly added the two objectives together in our experiments. We agree that introducing two additional weights to merge the primary and complement objectives is a good idea, and with proper tuning, this approach may further improve the model's performance and reduce the training time. We aimed to design a methodology with fewer hyper-parameters, so we didn't explore this direction, and our current proposed method works in many scenarios, as shown in our experiments.  With these promising results, we will continue to explore the approach of merging the two objectives, and build connections between these two approaches, in our immediate future work.\n\nRegarding reporting the increase in training time, we have added the information of training time in section 2.2 (on the top of page 4).\n", "title": "Follow-up"}, "Syx-XwqcCQ": {"type": "rebuttal", "replyto": "BkxI3CtBCQ", "comment": "Thanks for your clarification. Based on all of the experiment results we have so far, such as loss gap values, we are only able to claim that models trained by COT generalize better (i.e., better performance on separate test sets).  While achieving better performance on separate test sets is a good indicator that COT does not produce models that overfit, further experiments and theoretical investigations on whether COT can be a rigorous option to guard against overfitting is left as a future work.", "title": "Follow-up"}, "H1lJy06u07": {"type": "rebuttal", "replyto": "HyM7AiA5YX", "comment": "We thank all reviewers and the anonymous for the constructive comments. We have updated the manuscript in Abstract, Section 2, Section 3.4, Conclusion and Appendix A to address your feedback and concerns. Here we provide a summary of these updates:\n\n(1) For AnonReviewer3\u2019s main suggestion of forming adversarial attacks using \u201cboth\u201d gradients from both primary and complement objectives, we have designed and conducted the additional FGSM (single-step) white-box experiments. The experiments set adversarial perturbations to be generated based on the sum of the primary gradient and the complement gradient (i.e., the gradient calculated from complement objective), while the results indicate that COT is more robust to single-step adversarial attacks under standard settings [1].\n\n(2) To provide more precise claim, we update the original claim \u201crobustness to adversarial attacks\u201d into \u201crobustness to single-step adversarial attacks\u201d according to (1). Additionally, more details of the original transfer attack experiments are provided in the manuscript.\n\n(3) We have added a description about the increase of training time and corrected typos pointed out by the reviewers in the manuscript.\n\n[1] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. \u201cExplaining and harnessing adversarial examples\u201d. In ICLR\u201915.", "title": "Manuscript Updated"}, "S1ebZ7sRam": {"type": "rebuttal", "replyto": "rJeeh8J5pm", "comment": "Thank you for your comments. We understand that the adversarial attack techniques used here may not be state-of-the-art methods; however, we want to emphasize that the primary goal of this paper is to improve model's accuracy, although experimental results do show that robustness is also one of the benefits of the models trained by COT. \n\nWe agree with the reviewer that transfer adversarial attack is different from the classic settings of adversarial attacks. To verify our method under standard adversarial attacks, we have conducted additional experiments on white-box attack, and provided the results below; the experimental results confirmed that COT is indeed more robust to this type of attacks, and therefore we believe the main conclusion that COT is more robust (compared to baselines) to adversarial attack still holds. We will add these results of the white-box attack into the final version of the paper. Additionally, we will rename the current experiments to \u201ctransfer attacks\u201d to avoid confusions. The definition of the transfer attacks can be found in several recent publications [1, 2, 3].\n\nFor the white-box attacks, we conducted the experiments as also suggested by AnonReviewer3. The update is to set adversarial perturbations to be Epsilon * Sign (Primary gradient + Complement gradient). Results indicate that COT is more robust to this type of white-box attacks under standard settings.\n\nTest errors on Cifar10 under FGSM white-box adversarial attacks\n===========================================================\n\t\t\t\t              Baseline\t                        COT\nResNet-110 \t\t\t      62.23%                 \t\t52.72%\nPreAct ResNet-18               65.60%                 \t\t56.17%\nResNeXt-29 (2\u00d764d)           70.24%                 \t\t61.55%\nWideResNet-28-10\t       59.39%                 \t\t55.53%\nDenseNet-BC-121               65.97%                 \t\t55.99%\n===========================================================\n \nThe reviewer also suggested to try out several recent methods on white-box and black-box attacks. We do agree with the reviewer that it's a great idea. However, since the main focus of the current paper is to improve accuracy, and the manuscript is already close to the page limit, we feel it's better to study this problem in a separate paper. As a matter of fact, we are planning on a follow-up work with the focus on the robustness of the models trained with COT. \n \n[1] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow. \u201cTransferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples.\u201d Arxiv, 2016\n\n[2] Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song. \u201cDelving into Transferable Adversarial Examples and Black-box Attacks.\u201d In International Conference on Learning Representation, 2017.\n\n[3] Wieland Brendel, Jonas Rauber, Matthias Bethge. \u201cDecision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.\u201d In International Conference on Learning Representation, 2018.", "title": "Thank you for the comments"}, "HJlWte4WR7": {"type": "rebuttal", "replyto": "BkeB3kugCQ", "comment": "Thank you for the clarifications on the recent research trending in adversarial attacks as well as your great suggestions on making the claim precise. We will adopt your suggestion and make it clear in the paper that the proposed training objectives make the models more robust to single-step adversarial attacks instead of claiming general robustness. We will use this new statement consistently across our updated version of the paper. ", "title": "Follow-up"}, "S1eR7XqFaQ": {"type": "rebuttal", "replyto": "r1gc8uIw27", "comment": "\nWe sincerely thank the reviewer for the useful and detailed comments. Below we provide explanations for each of your comments or questions.  \n\n\n(Q1) End of page 1: \"the model behavior for classes other than the ground truth stays unharnessed and not well-defined\". The probabilities should still sum up to 1, so if the ground truth one is maximized,  the others are actually implicitly minimized. No?\n\n(A1) Your understanding is totally correct. We have changed the original text to a more clear statement:\n\n\u201cTherefore, for classes other than the ground truth, the model behavior is not explicitly optimized --- their predicted probabilities are indirectly minimized when \u0177_ig is maximized since the probabilities sum up to 1.\u201d\n\nWe want to thank the reviewer again for crystalizing the manuscript.\n\n\n(Q2) Page 3, sec 2.1: \"optimizing on the complement entropy drives \u0177_ij to 1/(K \u2212 1)\". I believe that it drives each term \u0177_ij /(1 \u2212 \u0177_ig ) to be equal to 1/(K-1). Therefore, it drives \u0177_ij to (1 \u2212 \u0177_ig)/(K-1) for j!=g.\n\nThis indeed flattens the \u0177_ij for j!=g, but the effect on \u0177_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, \u0177_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? \n\nFor example, with 4 classes, we look at the predicted probabilities of a given sample of class 1:\nSuppose after step 1 of Algo 1, the predicted probabilities are:  0.5 0.3 0.1 0.1 \nAfter step 2:  0.1 0.3 0.3 0.3\nThen step 1: 0.5 0.3 0.1 0.1\nThen step 2: 0.1 0.3 0.3 0.3\nAnd so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments?\n\n(A2) Thanks for the detailed comment. As the reviewer pointed out, \u201cdrives \u0177_ij to 1/(K \u2212 1)\u201d was indeed a typo and should be corrected to \u201cdrive \u0177_ij /(1 \u2212 \u0177_ig) to 1/(K-1)\u201d. We have modified the manuscript correspondingly. Indeed, maximizing complement entropy in Eq(2) only drives \u201c\u0177_ij /(1 \u2212 \u0177_ig) to 1/(K-1)\u201d, and therefore in the example provided above, the predicted probabilities after step 2 can be \u201c0.1 0.3 0.3 0.3\u201d or \u201c0.5, (1 - 0.5)/3, (1 - 0.5)/3, (1 - 0.5)/3\u201d, or other values so long as the incorrect classes (\u0177_ij's) receive similar predicted probabilities. According to our observations from the experiments, the probabilities tend to converge to \u201c0.5, (1 - 0.5)/3, (1 - 0.5)/3, (1 - 0.5)/3\u201d. Experiments show that the algorithm does not have trouble converging; the algorithm converges smoothly in all the experiments we have conducted. Again, we thank the reviewer for the insightful comment; studying the theory of COT convergence is an intriguing topic and we leave it as a future work.\n\n\n(Q3) Sec 3.1: \"additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance\": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective?\n\n(A3) Hyper-parameters in this statement indeed refer to the learning rate, and we have modified the statement in the manuscript to avoid confusion; the modified statement is provided below:\n\n\u201ctherefore, additional efforts for tuning learning rates might be required for optimizers to achieve the best performance.\u201d\n\nRegarding the second question about tuning learning rates, we have conducted several experiments with different learning rates specifically tuned for each objective. The experimental results show that using the same learning rate for both primary and complement objectives leads to the best performance when Eq(3) is used as the complement objective.\n\n\n(Q4) Sec 3.2: The additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper.\n\n(A4) Yes, one additional backpropagation is required in each iteration when applying COT. On average, the total training time is about 1.6 times longer compared to the baselines. Thanks for the suggestion, and we have included this in the latest manuscript (section 2.2).", "title": "Response to AnonReviewer3 [1/2]"}, "S1eWWmcKaQ": {"type": "rebuttal", "replyto": "r1gc8uIw27", "comment": "\n(Q5) Sec 3.4: As the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the  relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective?\n\n(A5) Thanks for the comment. We should have made clear that \u201cblack box\u201d [1] (rather than \u201cwhite box\u201d) adversarial attacks are considered in the manuscript. Specifically, we follow the common practice of generating adversarial examples using both FGSM and I-FGSM methods with the gradients from a baseline model; this way, the model trained by COT is actually a \u201cblack box\u201d to these attacks. We have modified the manuscript to clarify this part. Also, thanks for the great suggestion of forming adversarial attacks using \u201cboth\u201d gradients (from both primary & complement objectives). We are designing and conducting experiments at the moment and will share results when ready.\n\n\nFor the part of secondary comments and typos, we appreciate your thorough reading again and have corrected all these typos according to your suggestions. Meanwhile, in the following, we also provided explanations to your secondary comments.\n\n\n(Q1) Page 3, sec 2.1: \"...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities...\", using maximizes instead of optimizes would be clearer.\n\n(A1) Thanks for the suggestion. We have reworded the manuscript to \u201cmaximizes.\u201d\n\n\n(Q2) In the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, \u0177_ig appears. Shouldn't C take all \\hat_y as an argument in this case?\n\n(A2) Since the probabilities sum up to one, \u0177_ig can be inferred from y^hat_Cbar. Also, for us, it seems more direct and clear to show that complement entropy is calculated from y^hat_Cbar when C takes y^hat_Cbar as the only argument. Therefore, we incline to keep the orignal formulation. If the reviewer has strong preference, please kindly let us know and we are happy to make changes accordingly.\n\n\n(Q3) Algorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1}\n(resp. theta'_t) which is the previous parameter. Is there a reason for this choice?\n\n(A3) Thanks for the comment. Originally, we want to notify readers that there are two backprops within one iteration. We agree that those symbols are confusing and therefore we have modified the manuscript with those symbols removed.\n\n\n(Q4) Sec 3.2  Figure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)?\n\n(A4) Thanks for pointing this out. This is a typo and we have already corrected it in the manuscript: median -> mean.\n\n\n(Q5) Sec 3.2, Table 3 and 4: why is it the validation error that is reported and not the test error?\n\n(A5) Thanks for the detailed comment. For a fair comparison, we report the error in the exact same way as the open-sourced repo from the ResNet authors:\nhttps://github.com/KaimingHe/deep-residual-networks.\n\n\n(Q6) Sec 3.3: \"Neural machine translation (NMT) has populated the use of neural sequence models\": populated has not the intended meaning.\n\n(A6) We thank the reviewer for pointing out this typo. We have already corrected it in our manuscript: populated -> popularized\n\n\n(Q7) \"Studying on COT and adversarial attacks..\" --> could be better formulated\n\n(A7) Thanks for the comment again. We have modified the manuscript as follows: \"Studying on the relationship between COT and adversarial attacks\u2026\u201d\n\n\n[1] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz. \u201cMixup: Beyond Empirical Risk Minimization.\u201d In International Conference on Learning Representation, 2018.", "title": "Response to AnonReviewer3 [2/2]"}, "rJg8OW9FT7": {"type": "rebuttal", "replyto": "Syx8_6U63X", "comment": "\n(Q1) One small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm (Algorithm 1) with multi-objective optimization. In particular, I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics, and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved.\n\n(A1) We sincerely thank the reviewer for the helpful and constructive suggestion about associating COT with multi-objective optimization. This is really a brilliant idea. As a straight-line future work, we will survey multi-objective optimization techniques, and explore the direction of formulating COT into a multi-objective optimization problem.", "title": "Response to AnonReviewer2"}, "HJlOtAKta7": {"type": "rebuttal", "replyto": "B1lv2Bdph7", "comment": "\n(Q4) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1?\n\n(A4) We are very grateful for this novel idea, and we have conducted several preliminary experiments to explore this idea. Below are the comparisons between (a) the original COT method, and (b) the approach of combining the two objectives into one single objective. The experimental results show that the original COT method works better in almost all cases, and we conjecture that these two methods converge to different local minima. This idea is worth exploring, and we leave it as a straight-line future work. \n\nTest error of the state-of-the-art architectures on Cifar10 \n===========================================================\n\t\t\t\t                Combining into one objective\t  COT\nResNet-110 \t\t\t        7.42%                 \t\t                  6.84%\nPreAct ResNet-18                 4.92%                 \t\t                  4.86%\nResNeXt-29 (2\u00d764d)             4.79%                 \t\t                  4.55%\nWideResNet-28-10\t\t4.00%                 \t\t                  4.30%\nDenseNet-BC-121           \t4.64%                 \t\t                  4.62%\n===========================================================\n\nTest error of the state-of-the-art architectures on Cifar100\n===========================================================\n\t\t\t\t                 Combining into one objective\t  COT\nResNet-110 \t\t\t         28.80%                 \t\t                  27.90%\nPreAct ResNet-18                  25.30%                 \t\t                  24.73%\nResNeXt-29 (2\u00d764d)              23.20%                 \t\t                  21.90%\nWideResNet-28-10\t\t 21.96%                 \t\t                  20.99%\nDenseNet-BC-121           \t 22.17%                 \t\t                  20.54%\n===========================================================\n\n\n(Q5) How does alternating between two objectives change the training time? Do the authors use backpropagation?\n\n(A5) Yes, we do use backpropagation. One additional backpropagation is required in each iteration when applying COT, and therefore the overall training time is about 1.6 times longer according to our experiments.\n\n\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. \u201cDeep Residual Learning for Image Recognition.\u201d In IEEE Conference on Computer Vision and Pattern Recognition, 2016.\n[2] Sergey Zagoruyko, Nikos Komodakis. \u201cWide Residual Networks\n.\u201d In British Machine Vision Conference, 2016.\n[3] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger, David Lopez-Paz. \u201cDensely Connected Convolutional Networks\n.\u201d In IEEE Conference on Computer Vision and Pattern Recognition, 2017.", "title": "Response to AnonReviewer1 [2/2]"}, "r1ejg15YaQ": {"type": "rebuttal", "replyto": "B1lv2Bdph7", "comment": "We would like to thank the reviewer for all the insightful feedbacks. Below we provide the explanations for each question or comment raised by the reviewer:\n\n\n(Q1) How is this idea related to regularization? If we increase the regularization parameter, we can attain sparse parameter vectors. \n\n(A1) Conventionally, regularization techniques (e.g., Ridge or Lasso) are applied on the parameter space. We want to point out that all the results reported in the manuscript, for both baselines and models trained by COT, have already used L2-norm regularization on the parameter space, exactly as specified in the original papers  (e.g., ResNet [1], WideResNet [2], and DenseNet [3]). In other words, COT is applied on top of the existent of those regularization techniques.\n\nIf your questions haven\u2019t been addressed satisfactorily, please kindly let us know and we will be happy to discuss further.\n\n\n(Q2) Would this method also complement from overfitting?\n\n(A2) Thank you for the comment. We would like to further clarify what you meant by saying \u201ccomplement from overfitting.\u201d Our interpretation of the question is: whether COT could be used to fight against overfitting. Overfitting means a model fails to generalize, and in our paper we have reported the generalized performance of models trained by COT on the test data, which confirms models trained by COT generalize better. In addition, we also calculate the loss gap \"(testing loss - training loss)\" and report the results in the following table, where a smaller gap indicates that a model generalizes better. Experimental results confirm that models trained by COT seem to generalize better due to the smaller gap between training and testing loss.\n\n \"(Testing loss - training loss)\u201d from the state-of-the-art architectures on Cifar10 \n==================================================\n\t\t\t\t                Baseline\t        COT\nResNet-110 \t\t\t        0.36                 0.33\nPreAct ResNet-18                 0.28                 0.26\nResNeXt-29 (2\u00d764d)             0.20                 0.19\nWideResNet-28-10\t\t0.23                 0.21\nDenseNet-BC-121           \t0.22                 0.22\n=================================================\n\n\n(Q3) In the numerical experiments, the comparison is carried out against a \"baseline\" method. Do the authors use regularization with these baseline methods? I believe the comparison will be fair if the regularization option is turned on for the baseline methods.\n\n(A3) Yes, the regularization (e.g., L2 Norm) techniques are used in all of the baseline methods, as specified in their original papers (e.g., ResNet [1], WideResNet [2], and DenseNet [3]). We agree with the reviewer that \u201cthe comparison will be fair if the regularization option is turned on for the baseline methods,\u201d and that is exactly we did in our paper: all the hyper-parameters, regularization and other training techniques are configured in the same way as in the original papers. For the details of experimental setup, please refer to the Section 3.2 in our manuscript.", "title": "Response to AnonReviewer1 [1/2] "}, "Syx8_6U63X": {"type": "review", "replyto": "HyM7AiA5YX", "review": "This paper considers augmenting the cross-entropy objective with \"complement\" objective maximization, which aims at neutralizing the predicted probabilities of classes other than the ground truth one. The main idea is to help the ground truth label stands out more easily by smoothing out potential peaks in non-ground-truth labels. The wide application of the cross-entropy objective makes this approach applicable to many different machine/deep learning applications varying from computer vision to NLP. \n\nThe paper is well-written, with a clear explanation for the motivation of introducing the complement entropy objective and several good visualization of its empirical effects (e.g., Figures 1 and 2). The numerical experiments also incorporate a wide spectrum of applications and network structures as well as dataset sizes, and the performance improvement is quite impressive and consistent. In particular, the adversarial attacks example looks very interesting.\n\nOne small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm (Algorithm 1) with multi-objective optimization. In particular, I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics, and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved.", "title": "Simple and sensible heuristic with impressive improvement", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1lv2Bdph7": {"type": "review", "replyto": "HyM7AiA5YX", "review": "In this manuscript, the authors propose a secondary objective for softmax minimization. This complementary objective is based on evaluating the information gathered from the incorrect classes. Considering these two objectives leads to a new training approach. The manuscript ends with a collection of tests on a variety of problems.\n\nThis is an interesting point of view but the manuscript lacks discussion on several important questions:\n\n1) How is this idea related to regularization? If we increase the regularization parameter, we can attain sparse parameter vectors. \n2) Would this method also complement from overfitting?\n3) In the numerical experiments, the comparison is carried out against a \"baseline\" method. Do the authors use regularization with these baseline methods? I believe the comparison will be fair  if the regularization option is turned on for the baseline methods.\n4) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1?\n5) How does alternating between two objectives change the training time? Do the authors use backpropagation?", "title": "Nice idea but leaves several questions not answered", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1gc8uIw27": {"type": "review", "replyto": "HyM7AiA5YX", "review": "\n========\nSummary\n========\n\nThe paper deals with the training of neural networks for classification or sequence generation tasks, using a cross-entropy loss. Minimizing the cross-entropy means maximizing the predicted probabilities of the ground-truth classes (averaged over the samples). The authors introduce a \"complementary entropy\" loss with the goal of minimizing the predicted probabilities of the complementary (incorrect) classes. To do that, they use the average of sample-wise entropy over the complement classes. By maximizing this entropy, the predicted complementary probabilities are encouraged to be equal and therefore, the authors claim that it neutralizes them as the number of classes grows large. The proposed training procedure, named COT, consists of alternating between the optimization of the two losses.\n\nThe procedure is tested on image classification tasks with different datasets (CIFAR-10, CIFAR-100, Street View House Numbers, Tiny ImageNet and ImageNet), machine translation (training using IWSLT dataset, validation and test using TED tst2012/2013 datasets), and speech recognition (Gooogle Commands dataset). In the experiments, COT outperforms state-of-the-art models for each task/dataset.\n\nAdversarial attacks are also considered for the classification of images of CIFAR-10: using the Fast Gradient Sign and Basic Iterative Fast Gradient Sign methods on different models, adversarial examples specifically designed for each model, are generated. Then results of these models are compared to COT on these examples. The authors admit\nthat the results are biased since the adversarial attacks only target part of the COT objective, hence more accurate comparisons should be done in future work.\n\n===========================\n Main comments and questions\n===========================\n\nEnd of page 1: \"the model behavior for classes other than the ground  truth stays unharnessed and not well-defined\". The probabilities  should still sum up to 1, so if the ground truth one is maximized,  the others are actually implicitly minimized. No?\n\nPage 3, sec 2.1: \"optimizing on the complement entropy drives \u0177_ij to 1/(K \u2212 1)\". I believe that it drives each term \u0177_ij /(1 \u2212 \u0177_ig ) to be equal to 1/(K-1). Therefore, it drives \u0177_ij to (1 \u2212 \u0177_ig)/(K-1) for j!=g.\n\nThis indeed flattens the \u0177_ij for j!=g, but the effect on \u0177_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, \u0177_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? \n\nFor example, with 4 classes, we look at the predicted probabilities of a given sample of class 1:\nSuppose after step 1 of Algo 1, the predicted probabilities are:  0.5 0.3 0.1 0.1 \nAfter step 2:  0.1 0.3 0.3 0.3\nThen step 1: 0.5 0.3 0.1 0.1\nThen step 2: 0.1 0.3 0.3 0.3\nAnd so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments?\n\nSec 3.1:\n\"additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance\": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective?\n\nSec 3.2:\nThe additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper.\n\nSec 3.4:\nAs the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the  relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective?\n\n===========================\nSecondary comments and typos\n===========================\n\nPage 3, sec 2.1: \"...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities...\", using maximizes instead of optimizes would be clearer.\n\nIn the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, \u0177_ig appears. Shouldn't C take all \\hat_y as an argument in this case?\n\nAlgorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1}\n(resp. theta'_t) which is the previous parameter. Is there a reason for this choice?\n\nSec 3:\n\"We perform extensive experiments to evaluate COT on the tasks\" --> COT on tasks\n\n\"compare it with the baseline algorithms that achieve state-of-the-art in the respective domain.\" --> domainS\n\n\"to evaluate the model\u2019s robustness trained by COT when attacked\" needs reformulation.\n\n\"we select a state- of-the-art model that has the open-source implementation\" --> an open-source implementation\n\nSec 3.2:\nFigure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)?\n\nTable 3 and 4: why is it the validation error that is reported and not the test error?\n\nSec 3.3:\n\"Neural machine translation (NMT) has populated the use of neural sequence models\": populated has not the intended meaning.\n\n\"We apply the same pre-processing steps as shown in the model\" --> in the paper?\n\nSec 3.4:\n\"We believe that the models trained using COT are generalized better\" --> \"..using COT generalize better\"\n\n\"using both FGSM and I-FGSM method\" --> methodS\n\n\"The baseline models are the same as Section 3.2.\" --> as in Section 3.2.\n\n\"the number of iteration is set at 10.\" --> to 10\n\n\"using complement objective may help defend adversarial attacks.\" --> defend against\n\n\"Studying on COT and adversarial attacks..\" --> could be better formulated\n\nReferences: there are some inconsistencies (e.g.: initials versus first name)\n\n\nPros\n====\n- Paper is clear and well-written\n- It seems to me that it is a new original idea\n- Wide applicability\n- Extensive convincing experimental results\n\nCons\n====\n- No theoretical guarantee that the procedure should converge\n- The training time may be twice longer (to clarify)\n- The adversarial section, as it is,  does not seem relevant for me\n\n", "title": "Interesting new idea, good experimental results, some points to clarify.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}