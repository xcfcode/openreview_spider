{"paper": {"title": "Pix2Scene: Learning Implicit 3D Representations from Images", "authors": ["Sai Rajeswar", "Fahim Mannan", "Florian Golemo", "David Vazquez", "Derek Nowrouzezahrai", "Aaron Courville"], "authorids": ["rajsai24@gmail.com", "fmannan@gmail.com", "florian.golemo@inria.fr", "dvazquez@cvc.uab.es", "dereknow@gmail.com", "aaron.courville@gmail.com"], "summary": "pix2scene: a deep generative based approach for implicitly modelling the geometrical properties of a 3D scene from images", "abstract": "Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics. We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images. Our method learns the depth and orientation of scene points visible in images. Our model can then predict the structure of a scene from various, previously unseen view points. It relies on a bi-directional adversarial learning mechanism to generate scene representations from a latent code, inferring the 3D representation of the underlying scene geometry. We showcase a novel differentiable renderer to train the 3D model in an end-to-end fashion, using only images. We demonstrate the generative ability of our model qualitatively on both a custom dataset and on ShapeNet. Finally, we evaluate the effectiveness of the learned 3D scene representation in supporting a 3D spatial reasoning.", "keywords": ["Representation learning", "generative model", "adversarial learning", "implicit 3D generation", "scene generation"]}, "meta": {"decision": "Reject", "comment": "This paper proposes an approach for learning to generate 3D views, using a surfel-based representation, trained entirely from 2D images.  After the discussion phase, reviewers rate the paper close to the acceptance threshold.\n\nAnonReviewer3, who initially stated \"My second concern is the results are all on synthetic data, and most shapes are very simple\", remains concerned after the rebuttal, stating \"all results are on synthetic, simple scenes. In particular, these synthetic scenes don't have lighting, material, and texture variations, making them considerably easier than any types of real images.\"\n\nThe AC agrees with the concerns raised by AnonReviewer3, and believes that more extensive experimentation, either on more complex synthetic scenes or on real images, is needed to back the claims of the paper.  Particularly relevant is the criticism that \"While the paper is called \u2018pix2scene\u2019, it\u2019s really about \u2018pix2object\u2019 or \u2018pix2shape\u2019.\"\n"}, "review": {"r1gDLpRpkE": {"type": "rebuttal", "replyto": "HygY_sl9yE", "comment": "Dear Reviewer 3, thank you again for your time and comments.\n\nWe want to clarify what we interpret to be a fundamental misunderstanding regarding our contributions and those of prior art.\n\nWorks like those of Yan et al., Eslami et al.  train on _multiple images_ of the same scene  and Kanazawa et al. and others use various forms of weak supervision. We, on the other hand, only present a _single view_ per unique scene configuration in an unsupervised manner.\n\nDuring training, we present our model with multiple views of similar (but never identical!) scenes. Even for scenes with simple objects, object placement and orientation is always different (i.e., randomly rotated and translated) per image. This is a fundamental difference, and significantly changes the problem landscape and difficulty.\n\nMost prior works you referred to consider multiple views of each individual scene. We, on the other, learn to infer 3D structure from single samples of unique 3D scenes. We do so in a _completely unsupervised_ fashion.\n\nFrom this, we are able to then synthesize, e.g., many novel views of a fixed scene configuration. We also demonstrate the ability to (smoothly) explore the space of possible scene configurations. We also introduce mental rotation task as a benchmark to evaluate 3D understanding based models irrespective of the underlying 3D representation used.\n\nWe are happy to further discuss and contextualize our contribution with respect to these works you\u2019ve raised across your replies during our review discussion, including in the case of techniques where reproducibility would be very challenging. Given our clarification, we feel the case for differentiation from prior art, as well as practicality and novelty, become clear.\n\nMoving forward we plan on scaling our method to datasets (like ImageNet) where, for every 3D scene, only a single image is available. Texture variation is one of the key directions of future development needed to make this jump, we feel.\n\nAs the other two reviewers agree, we feel that the challenge of this particular problem (single-view, many scenes; purely unsupervised) and our results are promising and exciting for the community. Moreover, we hope the direction we present will open up avenues of future work towards robust, unsupervised scene understanding and reconstruction.\n\nRefs:\n\nEslami et al, Neural scene representation and rendering. Science, 2018.\nKanazawa 2018 - Learning Category-Specific Mesh Reconstruction\nYan et al, perspective transformer nets , 2016\n", "title": "Clarification regarding the concerns"}, "rye_zVPdyN": {"type": "rebuttal", "replyto": "H1lqQQBRRQ", "comment": "Dear Reviewer 3, \n\nWe were wondering if you had any time to consider our response (the one below this post, titled \"Details about Kanazawa (2018)\"). If we forgot to address any of your concerns, please let us know.\n\nWe would like to emphasize that the scope of this paper is the scene geometry reconstruction. Dealing with other scene properties like material and lights are part of our ongoing work. Our current model handles arbitrary geometric complexity as we demonstrated in the paper going from simple shape primitives to complex scenes (such as the ones in Figure 6 and 16).\n", "title": "Encouraging further discussion"}, "rygmHzvtnm": {"type": "review", "replyto": "BJeem3C9F7", "review": "The paper deals with creating 3D representations or depth maps from 2D image data using adversarial training methods. \nThe flow makes the paper readable.\n\nOne main concern is that most of the experiments seem to have results as visual inspections of figures provided. It is really hard to judge the correctness or how well the algorithms do.\n\nIt would be useful to provide references of equation 1 if used from previous text.\n\nIn the experiments, it is usually not clear how many training images were used, how many test. How different were the objects used in the training data vs test? Were all the test objects novel? How useful were the GAN techniques? Which part of the GAN did the most work i.e. the usefulness and accuracy of the different parts of the net? Even in 4.2, though it mentions use of 6 object types for both training and testing, using the figures is hard to estimate how well the model does compared to a reference baseline.\n\nIn 4.4.1, the discussion on how much improvement there is due to use of unlabeled images is missing? Do they even help? It is not quite clear from table 1. How many unlabeled images were used? How many iterations in total are used of the unlabeled ones (given there is 1 in 100 update of labeled ones). \n\nMissing reference: http://www.cs.cornell.edu/~asaxena/reconstruction3d/saxena_make3d_learning3dstructure.pdf\n", "title": "learning 3D or depth images from 2D images", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "Skg8XI2b14": {"type": "rebuttal", "replyto": "H1lpQ8o1JN", "comment": "Dear Reviewer 2,\n\nAwesome, thank you so much!\n\nAnd thanks for the correction. When we prepare the camera-ready version, we'll remove the extra bracket. And as for the depth map difference: yeah, good idea. How'd you imagine that - e.g. have an additional row of depth map images on the right where the value of each pixel = ground truth - reconstructions? If so, no problem at all. We just have to wait until after the review period because we currently can't edit the paper.\n\nAnd could we kindly ask: did you change the rating somewhere in the reviewer console? Because for us, your original rating still stands at \"3\".\n\nCheers and good Sunday\n", "title": "Thank you and yes, will try to update"}, "rJgcCZE1k4": {"type": "rebuttal", "replyto": "H1lqQQBRRQ", "comment": "Dear Reviewer 3,\n\nThanks for your time again. We\u2019d like to respond to your remark about other work. We don\u2019t think the comparison to Kanazawa (2018) is fair. They don\u2019t use ground truth mesh representation, but they have a very strong shape feedback from (a) the ground truth silhouette of the objects (b) the symmetry assumption (c) ground truth semantic object-specific keypoints. In more detail: They only experiment with birds and for each bird, they manually annotated the silhouette. Together with the mirror-symmetry assumption along one center plane, this covers most of the body\u2019s geometry. But in addition to this, the training images were also annotated with 14 keypoints like the beak, legs, etc. and during training they minimize the distance between the 3D model keypoints and the annotations. These three factors in combination create a strong 3D ground truth signal.\n\nWhile this work is indeed impressive, we feel it\u2019s inaccurate to call it \u201cunsupervised\u201d. The goal of our method is to infer structure purely from images. Without any kind of segmentation or manual annotation, purely unsupervised. There is only one other work that does something similar in Rezende (2016). But, as we outlined in the revised paper, (a) they only demonstrate this on perfectly illuminated single objects, whereas we do multiple objects with varying light conditions and (b) since they didn\u2019t release any code, it\u2019s hard for us and the community to reproduce their work.\n\nYes, our scenes are simple, but to our best knowledge, we are the first to actually do this reconstruction from single unrelated images, not multiple views of identical scenes and without any auxiliary loss or training data.\n\n\nRefs:\n\n(Kanazawa 2018) - Learning Category-Specific Mesh Reconstruction\nfrom Image Collections, https://arxiv.org/pdf/1803.07549.pdf\n\n(Rezende 2016) - Unsupervised Learning of 3D Structure from Images, http://papers.nips.cc/paper/6600-unsupervised-learning-of-3d-structure-from-images.pdf \n", "title": "Details about Kanazawa (2018)"}, "HkxuniHCRX": {"type": "rebuttal", "replyto": "rylarlqaAX", "comment": "Dear Reviewer 1,\n\nThanks, that's really lovely to hear. \n\nAs for the human evaluation: oh yeah, that's a clear oversight and should be in the appendix. To answer your questions:\n\nWe posted the questionnaire to our lab-wide mailing list, where 41 participants followed the call. The questionnaire had 1 calibration question where, if answered incorrectly, we pointed out the correct answers. For all successive answers, we did not give any participant the correct answers and each participant had to answer all 20 questions to complete the quiz. \n\nWe also asked participants for their age range, gender, education, and for comments. While many commented that the questions were hard, nobody gave us a clear reason to discard their response. All participants were at least high school graduates currently pursuing a Bachelor's degree. The majority of submissions (78%) were male, whereas the others were female or unspecified. Most of our participants (73.2%) were between 18 and 29 years old, the others between 30 and 39. The resulting test scores are normally distributed according to the Shapiro-Wilk test (p<0.05) and significantly different from random choice according to 1-sample Student's t test (p<0.01).\n\nP.S.: If you are curious, you can take the anonymized test as well: https://goo.gl/forms/QzVxjh9XOzhlhiIr2", "title": "Addendum for human evaluation"}, "H1g9CKHqCQ": {"type": "rebuttal", "replyto": "BJeem3C9F7", "comment": "Dear reviewers,\n\nThanks again for all comments and suggestions. We've significantly rewritten major parts of the paper according to your input. We think your feedback has improved the quality of the paper. We specifically clarified the message of our paper, our contributions, and our methods.\n\nThanks again for your time and for giving the new revision another look.", "title": "New Revision Added"}, "B1l2AzuX0m": {"type": "rebuttal", "replyto": "ByxFX5Yi3X", "comment": "\n== RE: More realistic scenes and colors.\n\nMost previous approaches have focused on the reconstruction or generation of single objects with no background. In this work, we consider scenes formed by one or more objects in a simple room. We have added new results of more complicated scenes where the number of objects, as well as their shape, vary. Please see our new qualitative results ( https://ibb.co/cbL6AL ) and quantitative results ( https://ibb.co/nj73qL , right side of the table). Additional scene reconstructions can be found here: https://ibb.co/hjpBc0 . We included these new results in the paper, which we\u2019ll update soon. We use diffuse materials with uniform reflectance for all our experiments. Learning the reflectance and color/texture properties (in addition to the surface depth and orientation) is significantly more challenging, but we are currently working towards that.\nWe'll keep more realistic settings involving textures, shadows, and realistic lighting for future work.\n\n\n== RE: Missing citations in 3D-IQTT and human performance.\n\nFor the 3D-IQTT we\u2019ve carried out a human evaluation on over 40 students in the department, the results of which were included in the paper and can be seen at https://ibb.co/nhHUS0 . The results show that although our model performs better than the baselines, we are still lagging behind the human level. We also cited and discussed Shepard and Metzler's work.\n\n\n== RE: Move related work to the supplementary material.\n\nWe completely rewrote the introduction to more clearly introduce the problem, challenges, assumptions, and the proposed method. We also moved unnecessary related work to the dedicated section.\n\n\n== RE: Comparison to \u201cUnsupervised Learning of 3D Structure from Images\u201d [Rezende et al. 2016] and \u201cPerspective Transformer Nets\u201d [Xinchen et al. 2016]\n\nPerspective Transformer Nets can be considered weakly supervised. They render 24 images per object, which are all observed by the networks during training. In most contexts, one needs a 3D object in order to obtain 24 images of an object. By contrast, we learn on a single image per individual scene, i.e. no scene configuration is ever seen twice. Moreover, the metric used in this paper was intersection over union which works well for voxel-based representation. It's not clear how this evaluation tool can be extended for surfels. Figure 9 in our paper demonstrated our capacity to new, unseen viewing angles. In order to evaluate our model\u2019s reconstruction of a scene from different novel viewpoints, we added MSE evaluations on the depth map and Hausdorff distance evaluation on the reconstructed surfels from different camera angles. Please see our modified table ( https://ibb.co/nj73qL ). In principle, we could compare our approach with the method of Rezende, but the information provided in the paper is not sufficient to accurately reconstruct their model.\n", "title": "Reply to Reviewer 3's \"Interesting inverse graphics model\" (part 2 of 2)."}, "HJxxaf_70Q": {"type": "rebuttal", "replyto": "ByxFX5Yi3X", "comment": "Thanks for your time and continued feedback\n\nTL;DR: We reworked the intro from the ground, drawing a much clearer distinction between our method and related methods. We also added the missing citation and conducted some human evaluations for our mental rotation task.\n\n\n== RE: The motivation of the paper is unclear.\n\nWe have re-written the introduction to more clearly reflect the extent of our contributions. This revision will be released shortly. \n\n\n== RE: Advantages of Implicit representation over-explicit. \n\nImplicit and explicit 3D representations aren\u2019t strictly better or worse but have significant advantages and disadvantages.  \nAn explicit representation stores all rendering-relevant information about all entities in a given 3D space. The main benefit of this method is that it\u2019s easily transferable, in the sense that an explicit model can directly be loaded into a 3D modeling software and viewed from all angles without any inconsistencies. In the worst possible case, one would store all the properties of each voxel in 3D space, like opaqueness, illumination, color, reflectance, etc. For example, in a space of 512x512x512 points with 10 values per point which are float32-encoded, this would amount to 5GB of data for a single scene. The vast majority of these points aren\u2019t relevant to most viewpoints, like the inside of objects. Therefore the common workaround is to use a sparse representation, like meshes. These do however come with their own drawbacks, like the question of how to discretize complex objects. This makes mesh representation difficult to deal with in neural networks. The current state of the art relies on deforming a pre-existing mesh.\nOn the other hand, the implicit approach represents the 3D scene in a high-dimensional latent variable. The main drawback of this is the lack of interpretability: this vector on its own is meaningless and needs to be decoded into a viewpoint-dependent representation that can be rendered. Increasing a single value in this vector could, for example, illuminate an object more or morph a chair in the scene into a table, or both. The big benefit of this is scalability. This produces a compressed representation that fits the complexity of a given scene. The only remaining issue is the viewpoint-dependent consistency, i.e. making sure that when the scene is viewpoint-dependently decoded into a renderable model, the scene\u2019s content is the same from all angles.\nWe believe that neural networks can approximate this consistency-enforcing function and we think that in the long term when going to arbitrarily complex scenes, the advantages of this implicit representation outweigh the downsides.\n\nTo summarize, in the explicit approach dealing with sparsity and discrete representations remains a major challenge for deep learning, and the cubic scaling of dense approaches like voxel-based representations makes them impractical for large scenes. On the other hand, our implicit representation generates plausible 3D models that show viewpoint extrapolation. This can directly be observed in our video ( https://bit.ly/2zADuqG ) for example in the rotating chair video, where we only fed the model a single chair image, and then move the camera to generate new viewing angles. \n\n\n== RE: Compare surfels with voxels, meshes and point clouds==\n\nBoth meshes and voxels don\u2019t scale well to scenes with multiple objects - voxels due to their poor scaling in complexity and meshes due to difficulties representing them with neural networks, as mentioned above. Point clouds don\u2019t provide any surfaces. Therefore no shadows, no reflectance, and no shading are possible. In terms of evaluation, most contemporary works use supervised training for their voxel/mesh-reconstruction where we use unsupervised/self-supervised training. A direct comparison with these would be unfair. The only other method we found with unsupervised reconstructions was [Rezende et al. 2016] and there is no source code available for their method. \nWe did include a new benchmark in the paper, the 3D-IQTT, that should allow for more methods to compare their reconstruction accuracy independently of the underlying method.\n\n[Rezende et al. 2016]  Unsupervised Learning of 3D Structure from Images.\n", "title": "Reply to Reviewer 3's \"Interesting inverse graphics model\" (part 1 of 2)."}, "BJxwzGBJ0X": {"type": "rebuttal", "replyto": "rygmHzvtnm", "comment": "Dear reviewer, thank you for your time and effort. \nTL;DR: We added more quantitative results. Our paper already includes examples generalizing to viewpoints that weren\u2019t part of the training data, but we included additional samples. And we added significantly more details about the methods.\n\nHere are our responses in more detail:\n\n\n== RE: Most evaluations are qualitative.\n\nThere is no standard protocol to evaluate 3D reconstruction and generation. Most of the state-of-the-art methods (fully unsupervised methods learned on single images) just show qualitative results in their papers. \nWe did quantitatively evaluate the surfel reconstruction against the ground truth via Hausdorff distance (HD) as described in Appendix B and the reconstructions of our model via mean squared error (MSE) on the depth map. We achieved near-zero MSE and reasonably lower HD (when reconstructed from the same view). We included a table showing the difference in these values for different, unseen camera views: https://ibb.co/nj73qL. On top of these metrics, we also created the 3D IQ test task (3D-IQTT) which is exclusively quantitative. We compared our method with two CNN baselines and we now also included human evaluation. The CNN baselines demonstrate that the task can only be solved with an understanding of the 3D geometry. A preview of the updated comparison table can be found here: ( https://ibb.co/nhHUS0) .\n\n\n== RE: Add reference for the rendering equation.\n\nSorry for the oversight. We have added the reference for the rendering equation: it is an approximation of Kajiya\u2019s rendering equation [Kajiya 1986].\n\n[Kajiya 1986] \u201cThe Rendering Equation\u201d\n\n\n== RE: More details on experimental setup.\n\nWe have added more details on the experimental setup (camera, lights, and material properties used) in the appendix. All our images are of resolutions 128x128. \nExcept for the 3D-IQTT, we didn\u2019t store a fixed dataset but rather created the dataset on the fly. For example in the existing Figure 4, during the data generation process the rotation, translation, and object categories were randomized. The probability of seeing the same configuration from two different views is near zero.\n\n\n== RE: Which parts of the GAN are more important.\n\nOur Pix2Scene architecture is a bidirectional adversarial model. It consists of an encoder, decoder, renderer, and discriminator. The encoder translates the input image into a latent representation. The decoder transforms a similar latent vector, sampled from noise, into our surfel representation, which is converted into a 2D image by the renderer. The discriminator\u2019s purpose is to make sure the output images become the same distribution as the input images, and ascertain that the encoded latent representation corresponds to the latent input to the decoder. See our existing Figure 3 for an overview. The decoder-rendering part is important for generating new viewpoints for a given latent code and the encoder-decoder pipeline allows us to infer the 3D structure of a 2D image. Without the encoder, the model would be purely generative.\n\n\n== RE: Novelty of the generated images.\n\nGAN-based models usually suffer from mode-collapse. We demonstrated in Figure 8 that our model overcame this issue and was able to interpolate between two given scenes. We\u2019ve added another figure to further emphasize the interpolation capabilities of our model.\n\n\n== RE: 3D-IQTT semisupervised learning.\n\nThanks for this feedback. We agree that this section wasn\u2019t sufficiently clear. We\u2019ve rewritten a part of this section and added the details on the interleaved training. It's similar to algorithm 2 from [Kingma et al. 2014], except instead of a randomized minibatch, we train a few iterations of unsupervised data followed by a few iterations of supervised data. We also extended Table 1 to include an entirely unsupervised case as well as human performance on the same task. A preview of the table can be found here: https://ibb.co/nhHUS0. In all cases, the model was trained with an unsupervised dataset of 100,000 lines of data, where each line contained the reference image, the 3 possible answers, but no information on which one was the correct answer.\n\n[Kingma et al. 2014] \"Semi-supervised Learning with Deep Generative Models\", 2014. \n\n\n== RE: Missing reference Make-3D.\n\nSorry for the oversight. We will add the reference in our introduction.\n", "title": "Reply to Reviewer 2"}, "SJgja6-aTQ": {"type": "rebuttal", "replyto": "HJejoMW9hX", "comment": "\n== RE: How camera and class conditioning is done.\n\nFor the view conditioning, we used conditional batch-normalization which transforms a 3-dimensional vector (representing the camera coordinates) into affine batch-normalization parameters. For the class conditioning, we used the standard conditional GAN technique: We encoded the class labels as one-hot vectors and concatenated this vector to the inputs of the decoder, encoder, and discriminator. We added all of this to the paper.\n\n\n== RE: Reconstruction error.\n\nWe added the exact formula for the reconstruction loss to the paper. \nIt is a weighted sum of two terms: (a) the L2 loss of the input image given to the encoder and the rendered output on the decoder, and (b) the L2 loss of the noise given to the decoder and the inferred latent code of the rendered decoder output. This loss is similar to the bi-directional L2-loss used in [Li et al. 2017] and [Huang et al. 2018].\n\n[Huang et al. 2018] \u201cMultimodal Unsupervised Image-to-Image Translation\u201d, 2018\n[Li et al. 2017] \u201cALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching\u201d, 2017\n\n\n== RE: 3D-IQTT loss function and training algorithm.\n\nIn these experiments, we removed the assumption of knowing the camera position. Our model had to learn to represent the scene geometry in a part of the latent vector (z_scene) and the camera position in another part (z_view).\n\nThe loss term for the supervised part of the training enforced this: it rewards the z_scene of the correct answer to be close to the z_scene of the reference and it pushes z_scene of the reference and z_scene of wrong answers apart. We also minimized mutual information between z_scene and z_view in order to enforce distinct source of information captured by the latent dimensions. We also improved the explanation of this method in the paper.\n\nWe also added experiments where we did not add any supervised samples. In this case, z_scene and z_view get entangled and the task becomes significantly harder. Both CNN baselines were trained on the supervised data; therefore when comparing them in the unsupervised condition they perform according to the random initialization while our model was able to at least leverage the unsupervised data. Please see our updated Table-1 (https://ibb.co/nhHUS0 )\n\nWe also added the interleaved training algorithm for the semi-supervised task to the paper. It's similar to algorithm 2 from [Kingma et al. 2014], except instead of a randomized minibatch, we train a few iterations of unsupervised data followed by a few iterations of supervised data.\n\n[Kingma et al. 2014] \"Semi-supervised Learning with Deep Generative Models\", 2014.", "title": "Reply to Reviewer 1's \"Nice model but some details missing\" (part 2 of 2)."}, "B1epOpZ6TQ": {"type": "rebuttal", "replyto": "HJejoMW9hX", "comment": "Thanks so much for your time.\nTL;DR: We\u2019re completely rewriting the intro to focus on our actual contribution and not the long-term plan. We\u2019re also working towards removing all the assumptions like camera position and light position knowledge. And we\u2019ve added a lot more details about the 3D-IQTT methods.\n\n== RE: Providing clarification on the claims.\n\nWe agree that the extent of the contributions claimed in the introduction was unclear. We set out to present our long-term goal, to \u201clearn the 3D structure of the real world just from single images\u201d; however, in this paper, we have just made the first steps towards the goal and that was not apparent from the original introduction. We have reworked our claims accordingly in our new paper version (will be uploaded in the next few days) to reflect that our method \u201creceives a monochrome image as input, estimates a depth map, and then shades this depth map using perfect knowledge of lighting and camera pose, which reconstructs the input\u201d - as you suggested.\n\nOur model makes several assumptions: (a) the camera pose is known, (b) the material properties are constant, (c) the light positions are known, and (d) the world is piece-wise smooth. In order to achieve our long-term goal, we have to eventually get rid of these. Therefore we have made some first steps to address each one:\n\n- (Camera pose is known): In the 3D-IQTT experiments, we estimated the camera position in our latent representation while we kept the camera looking at the center of the object. We used the estimated camera parameters in the generator for the rendering process.\n- (Material properties are constant): We used diffuse materials with uniform reflectance for all our experiments. The reflectance values were chosen arbitrarily but kept fixed for input-output pairs. In other words, we use fixed material which can be chromatic (reflects different wavelengths by different amount) or monochromatic (reflects all wavelengths the same amount). This is not the same as using \"monochromatic image\", it is just that material is constant and doesn't need to be inferred.  We've added the details to the appendix. \nLearning the reflectance and color/texture properties (in addition to the surface depth and orientation) is significantly more challenging, but we are currently working towards that.\n- (Lighting assumptions):  In our work presented in the last version of the paper, we used multiple point light sources that were placed randomly on the surface of a spherical sector around the scene and colored randomly. For each pair of rendered input image and model-reconstructed output image, these light conditions were identical. We added more details about how we handled the lighting to the appendix. \n- (The world is piece-wise smooth): This might not be perfectly accurate, but it\u2019s a common assumption in 3D reconstruction. In an extreme case like when capturing cactus spikes, this might not work, but for example, when we were reconstructing a chair with a thin stretcher (see our video, https://bit.ly/2zADuqG), the reconstruction worked well.\n\n\nWe agree that we are currently mainly recovering a depth map, but the surfel representation was picked with our long-term goal in mind, since gives us several advantages: (a) surfel representation allow us to represent only the visible surface of a complicated scene instead of explicitly representing the complete scene. Given an image we can infer its implicit 3D representation and then recreate novel surfel representations of the underlying scene from unobserved viewpoints. Moreover this representation fits well with current convolutional architectures (b) with our existing normal estimation and additional material estimation this allows for realistic shading.\n", "title": "Reply to Reviewer 1's \"Nice model but some details missing\" (part 1 of 2)."}, "Bkl_mZ_iTQ": {"type": "rebuttal", "replyto": "ByxFX5Yi3X", "comment": "Dear AnonReviewer3,\n\nThank you so much for your review. We appreciate the time you put into this and your feedback.\n\nBefore we respond in full to all of your points, we have a quick question:\nYou mentioned, \"It\u2019d be good to compare with these [voxel/point clouds/primitives] scene representations\". We'd love to implement this but we're having some issues finding suitable code.  Do you know of any code for methods that implicitly (i.e. without supervised training) learns object reconstruction using meshes/voxels/point clouds?\n\nThanks,\nthe Pix2Scene team\n", "title": "Quick question regarding comparison to voxel/point clouds/primitives"}, "ByxFX5Yi3X": {"type": "review", "replyto": "BJeem3C9F7", "review": "This paper explored explaining scenes with surfels in a neural recognition model. The authors demonstrated results on image reconstruction, synthesis, and mental shape rotation. \n\nThe paper has many strengths. The model is clearly presented, the implementation is neat, the results on synthetic images are good. In particular, the results on the mental rotation task are interesting and new; I feel we should include more studies like these for scene and object representation learning. \n\nA few concerns remain. First, the motivation of the paper is unclear. The main advantage of the proposed representation, according to the intro, is its `implicitness\u2019, which enables viewpoint extrapolation. I\u2019d like to see more explanation on why \u2018explicit\u2019 representations don\u2019t support that. A lot of the intro is currently talking about related work, which can be moved to later sections or to the supp material.\n\nThe paper then moves on to discuss surfels. While it\u2019s new combine surfels with deep nets, I\u2019m not sure how much benefits it brings over voxels, point clouds, or primitives. It\u2019d be good to compare with these scene representations. \n\nMy second concern is the results are all on synthetic data, and most shapes are very simple. While the paper is called \u2018pix2scene\u2019, it\u2019s really about \u2018pix2object\u2019 or \u2018pix2shape\u2019. I\u2019d like to see results on more realistic scenes, where the number of objects as well as their shape and material varies.\n\nFor the mental rotation task, the authors should cite and discuss the classic work from Shepard and Metzler and include human performance for calibration.\n\nI\u2019m on the border for this paper. Happy to adjust my rating based on the discussion and revision.\n", "title": "Interesting inverse graphics model. Motivation and experiments are lacking.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJejoMW9hX": {"type": "review", "replyto": "BJeem3C9F7", "review": "This paper introduces a method to create a 3D scene model given a 2D image and a camera pose. The method is: (1) an \"encoder\" network maps the image to some latent code vector, (2) a \"decoder\" network uses the code and the camera pose to create a depthmap, (3) surface normals are computed from the depthmap, and (4) these outputs are fed to a differentiable renderer which reconstructs the input image. At training time, a discriminator provides feedback to (and simultaneously trains on) the latent code and the reconstructions. The model is self-supervised by the reconstruction error and the GAN setup. Experiments show compelling results in 3D scene generation for simple monochromatic synthetic scenes composed of an empty room corner and floating ShapeNet shapes. \n\nThis is a nice problem, and if the approach ever works in the real world, it will be useful. On synthetic environments, the results are impressive.\n\nThe paper seems to claim more ground than it actually covers. The abstract says \"Our method learns the depth and orientation of scene points visible in images\", but really only the depth is learned, and the \"orientation\" is an automatically-computed surface normal, which is a free byproduct of any depth estimate. The \"surfel\" description includes a reflectance vector, but this is never estimated or further described in the paper, so my guess is that it is simply treated as a scalar (which equals 1). Taking this reflectance issue together with the orientation issue, the model is not really estimating surfels at all, but rather just a depthmap, which makes the method seem considerably less novel. Furthermore, the differentiable rendering (eq. 1) appears to assume that all light sources are known exactly -- this is not a trivial assumption, and yet it is never mentioned in the paper. The text suggests that only an image is required to run the model, but Figure 3 shows that the networks are conditioned on the camera pose -- exact knowledge of the camera pose is difficult to obtain precisely in real settings, so this again is not an assumption to ignore. \n\nTo rewrite the paper more plainly, one might say that it receives a monochrome image as input, estimates a depthmap, and then shades this depthmap using perfect knowledge of lighting and camera pose, which reconstructs the input. This may sound less appealing, but it also seems more accurate.\n\nThe paper is also missing some details of the method and evaluation, which I hope can be cleared up easily.\n- What is happening with the light source? This is critical in the shading equation (eq. 1), and yet no information is given on it -- we need the color and the position of every light in the scene. \n- How is the camera pose represented? Section 3.3.3 says conditional normalization is used, but what exactly is fed to the network that estimates these conditional normalization parameters? \n- What is the exact form of the reconstruction error? An equation would be great.\n- How is the class-conditioning done in 4.2?\n- In Eq. 4, the first usage of D_\\theta should use only the object part of the vectors, and the second usage should use only the geometric part, right? Maybe this can be cleared up with a second D_subscript.\n- I do not understand the \"interleaved\" training setup in 4.4.1. Please explain that more. \n- It is not clear to me why the task in 4.4.2 needs any supervised training at all, if the classification is just done by computing L2 distances in the latent space. What happens with \"0 sampled labels\"?\n\nOverall, I like the paper, and I can imagine others in my group liking it. I hope it gets in, assuming the technical details get cleaned up and the language gets softer.", "title": "Nice model but some details missing", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}