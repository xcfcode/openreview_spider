{"paper": {"title": "Attentive Recurrent Comparators", "authors": ["Pranav Shyam", "Ambedkar Dukkipati"], "authorids": ["pranavm.cs13@rvce.edu.in", "ad@csa.iisc.ernet.in"], "summary": "Attention and Recurrence can be as good as Convolution in some cases. Bigger returns when we combine all three.", "abstract": "Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\\%).", "keywords": ["Deep learning", "Computer vision"]}, "meta": {"decision": "Reject", "comment": "This paper shows some strong performance numbers, but I agree with the reviewers that it requires more analysis of where those gains come from. The model is very simple, which is a positive, but more studies such as ablation studies and other examples would help a lot."}, "review": {"rJ0vnCeXg": {"type": "review", "replyto": "BJjn-Yixl", "review": "The results for ARC in the current version of the paper do not outperform some baselines.  It seems from the author comment that new results which do outperform will be updated soon.  Could you also provide insight and interpretation (e.g. qualitative examples) into where and how the proposed method outperforms the baselines?This paper introduces an attention-based recurrent network that learns to compare images by attending iteratively back and forth between a pair of images. Experiments show state-of-the-art results on Omniglot, though a large part of the performance gain comes from when extracted convolutional features are used as input.\n\nThe paper is significantly improved from the original submission and reflects changes based on pre-review questions. However, while there was an attempt made to include more qualitative results e.g. Fig. 2, it is still relatively weak and could benefit from more examples and analysis. Also, why is the attention in Fig. 2 always attending over the full character?  Although it is zooming in, shouldn\u2019t it attend to relevant parts of the character?  Attending to the full character on a solid background seems a trivial solution where it is then unclear where the large performance gains are coming from.\n\nWhile the paper is much more polished now, it is still lacking in details in some respects, e.g. details of the convolutional feature extractor used that gives large performance gain.", "title": "discussion of results", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HJWrxQM4x": {"type": "review", "replyto": "BJjn-Yixl", "review": "The results for ARC in the current version of the paper do not outperform some baselines.  It seems from the author comment that new results which do outperform will be updated soon.  Could you also provide insight and interpretation (e.g. qualitative examples) into where and how the proposed method outperforms the baselines?This paper introduces an attention-based recurrent network that learns to compare images by attending iteratively back and forth between a pair of images. Experiments show state-of-the-art results on Omniglot, though a large part of the performance gain comes from when extracted convolutional features are used as input.\n\nThe paper is significantly improved from the original submission and reflects changes based on pre-review questions. However, while there was an attempt made to include more qualitative results e.g. Fig. 2, it is still relatively weak and could benefit from more examples and analysis. Also, why is the attention in Fig. 2 always attending over the full character?  Although it is zooming in, shouldn\u2019t it attend to relevant parts of the character?  Attending to the full character on a solid background seems a trivial solution where it is then unclear where the large performance gains are coming from.\n\nWhile the paper is much more polished now, it is still lacking in details in some respects, e.g. details of the convolutional feature extractor used that gives large performance gain.", "title": "discussion of results", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJdEJGyml": {"type": "review", "replyto": "BJjn-Yixl", "review": "In your paper, you write that you are missing results that will be reported.\n\nDo you have a newer version of your paper? In the current version your model doesn't perform as well as other methods. Could you report your \"full context arc\" results? Adding other more qualitative contributions or explaining why comparison is that not important would be nice too. This paper presents an attention based recurrent approach to one-shot learning. It reports quite strong experimental results (surpassing human performance/HBPL) on the Omniglot dataset, which is somewhat surprising because it seems to make use of very standard neural network machinery. The authors also note that other have helped verify the results (did Soumith Chintala reproduce the results?) and do provide source code.\n\nAfter reading this paper, I'm left a little perplexed as to where the big performance improvements are coming from as it seems to share a lot of the same components of previous work. If the author's could report result from a broader suite of experiments like in previous work (e.g matching networks), it would much more convincing. An ablation study would also help with understanding why this model does so well.", "title": "missing results", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "rJu4Ftb4x": {"type": "review", "replyto": "BJjn-Yixl", "review": "In your paper, you write that you are missing results that will be reported.\n\nDo you have a newer version of your paper? In the current version your model doesn't perform as well as other methods. Could you report your \"full context arc\" results? Adding other more qualitative contributions or explaining why comparison is that not important would be nice too. This paper presents an attention based recurrent approach to one-shot learning. It reports quite strong experimental results (surpassing human performance/HBPL) on the Omniglot dataset, which is somewhat surprising because it seems to make use of very standard neural network machinery. The authors also note that other have helped verify the results (did Soumith Chintala reproduce the results?) and do provide source code.\n\nAfter reading this paper, I'm left a little perplexed as to where the big performance improvements are coming from as it seems to share a lot of the same components of previous work. If the author's could report result from a broader suite of experiments like in previous work (e.g matching networks), it would much more convincing. An ablation study would also help with understanding why this model does so well.", "title": "missing results", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}