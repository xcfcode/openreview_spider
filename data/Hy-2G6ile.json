{"paper": {"title": "Gated Multimodal Units for Information Fusion", "authors": ["John Arevalo", "Thamar Solorio", "Manuel Montes-y-G\u00f3mez", "Fabio A. Gonz\u00e1lez"], "authorids": ["jearevaloo@unal.edu.co", "solorio@cs.uh.edu", "smmontesg@inaoep.mx", "fagonzalezo@unal.edu.co"], "summary": "Gated Multimodal Units: a novel unit that learns to combine multiple modalities using multiplicative gates", "abstract": "This paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved the macro f-score performance of single-modality approaches and outperformed other fusion strategies, including mixture of experts models. Along with this work, the MM-IMDb dataset is released which, to the best of our knowledge, is the largest publicly available multimodal dataset for genre prediction on movies.", "keywords": ["Multi-modal learning", "Applications", "Supervised Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The authors propose a Gated Muiltimodal Unit to combine multi-modal information (visual and textual). They also collect a large dataset of movie summers and posters. Overall, the reviewers were quite positive, while AR4 points to related models and feels that the contribution in the current version is too weak for ICLR. The AC read the paper and the authors responses but tends to agree with AR4. The authors are encouraged to strengthen their work and resubmit to a future conference."}, "review": {"SydM2AsLe": {"type": "rebuttal", "replyto": "S1ibdAIUl", "comment": "We have released the source code in the https://github.com/johnarevalo/gmu-mmimdb repository.", "title": "Source code released"}, "S1ibdAIUl": {"type": "rebuttal", "replyto": "Hy-2G6ile", "comment": "We have added a new version which includes the mixture of experts evaluation. Since this is a multilabel scenario, we implement tied and untied gates for the outputs. We also evaluate Logistic regression and MaxoutMLP as experts. \n\nOverall, the MoE models did not work very well in comparison with other fusion strategies. We believe this has to do with the amount of training samples. It is known that MoE models require large datasets because the data is fractionated over different experts.", "title": "New revision submitted"}, "SJ234iC4g": {"type": "rebuttal", "replyto": "Sy4GlaZ4g", "comment": "Thanks for your review. Our comments are embedded below.\n\n\".. the paper should test the algorithm in other applications...\".\n\nWe agree the model should be tested in more applications. However, as stated in another submission related to multimodal learning (https://openreview.net/forum?id=rJJ3YU5ge) and to the best of our knowledge, there are not standard multimodal datasets, most of the open multimodal datasets task involves the mapping from one modality to another (e.g. image captioning) rather than finding a way to combining them to improve the performance in the particular task. Indeed, this was the main motivation to build a publicly available dataset. We\u2019ll evaluate another preliminary dataset we have been collecting composed of books with their covers, however we would appreciate any suggestion for other standard dataset to be included as well.\n\n\"Another concern lies in how to evaluate the performance of information fusion. The abstract claims \"The model improves the macro f-score performance of single-modality models by 30% and 4% with respect to visual and textual information respectively\", however, such an improvement is off the key. If two modals are complementary to each other, the fusion results will always be higher. The key fact is how much better than baselines the proposed GMU is.\"\n\nWe agree that the main comparison it is not with single but with multimodal approaches, and thus the abstract should emphasize it. Unfortunately, the phrase you quote was part of one of the first versions of our paper and we missed to update it with our last findings (we\u2019ll update it in the next revision). We compared the model with two early fusion models  (concatenate and linear sum) which have proven to be a good way to combine multimodal features. We also compared with a simpler late fusion strategy (avg_probs). GMU obtained better results in all metric reported for this multilabel task.\n\n\"...I would also expect other techniques, including fine-tuning, dropout, distillation may help too.  It would be nice if the author could compare these techniques.\"\n\nWe initially avoided fine-tuning VGG and the word vectors because it could easily overfit our dataset (~16K training samples Vs. 138M(VGG), 12M (word vectors) of parameters). We included dropout as regularization strategy, but it is not clear for us how can be used as fusion strategy. We'll try fine-tuning both representations and look at distillation as a late fusion strategy.\n \n\"I also hope this paper could talk in more details the connection with mixture-of-expert (MoE) model. Both models are based on the nonlinear gated functions, while both method may suffer from local minimum for optimization on small datasets. I would like more in-depth discussion in their similarity and difference.\"\n\nWe\u2019ll add a more detailed discussion of MoE Vs GMU. We\u2019ll also train the MoE with the best model of each modality, and discuss the results.\n\n\"To gain more attention for GMU, I would encourage the author to open-source their code and try more datasets.\"\n\nSo far, we\u2019ve focused on releasing the dataset, but we also plan to release the code as soon as possible. We want to refactor it so that it could be easy to use and to reproduce these results.", "title": "Comments on review"}, "H1sAY9ANl": {"type": "rebuttal", "replyto": "r1zs_jEVe", "comment": "Thanks for your review. We will focus on including other gated models for comparison and including a more elaborated comparison between GMU and other multiplicative architectures. We also thank you for suggesting our work to the workshop, we would be glad to share it on this event.", "title": "Comment on review"}, "r1fHw504e": {"type": "rebuttal", "replyto": "HJnquEuEe", "comment": "Thanks for your review. These are our comments:\n\n1. We proposed untied weights for the multi-modal case, i.e.  there is a different linear transformation for each gate (rendered latex: http://i.imgur.com/zzyq27I.gif):\nh_i = tanh(W_i x_i )\\\\\nz_i = \\sigma ( W_{z_i} [x_1,x_2, ... , x_k] )\\\\\nh = \\sum_i z_i h_i\n\nSince we have not addressed the multi-modal scenario, this formulation is opened to experimental evaluation.\n\n2. Following the gated units in recurrent networks (e.g GRUs, LSTMs), we fixed this tanh activation function. However, this is also a choice to be made when building the neural network architecture.\n\n3. An MLP with two hidden units is also able to solve the task. However, it is not clear how the model is dealing with the added noise, you can see the plot for the same synthetic dataset used in Figure 7 using the MLP with 2 hidden units in http://i.imgur.com/Bjsdlbl.png\n\n4. We agree. We\u2019ll extend the discussion and experimentation for the comparison with other gated models.\n", "title": "Comments on review"}, "ry0fHtWVl": {"type": "rebuttal", "replyto": "S1lFzFT7e", "comment": "We include a plot to visualize the behavior of the GMU with respect to the input features.", "title": "Add visualization"}, "S1lFzFT7e": {"type": "rebuttal", "replyto": "Hy-2G6ile", "comment": "Following your comments, we've added a new revision which includes:\n  - More details on parameter exploration and training procedure.\n  - Updates on multimodal baseline and synthetic results.\n  - Histogram for image sizes.\n  - Link to the dataset webpage.\n  - Plot depicting the influence of each modality in the label assignment.\n\nThanks again for your feedback.", "title": "New revision submitted"}, "H1tNrkSml": {"type": "rebuttal", "replyto": "S1cPuGzQe", "comment": "Thanks for your questions and feedback.\n\nIn the first report we only ran one experiment in which the logistic regression classifier achieved 64% of accuracy while the GMU obtained 100%. Following your comment, we've run 1000 synthetic experiments with different random seeds, and the GMU outperformed the logistic regression classifier in 370, while obtaining equal results in the remainder ones. Our goal in these simulations was to show that the model was able to learn a latent variable that determines which modality carries the useful information for the classification, rather than de-noising the input.\n\nWe've tried both PCA and t-sne transformation to plot samples in a 2D space as a visualization strategy, but unfortunately we didn't find such visualizations concluding enough. We argue this visualization is challenging due to the complexity added from the multi-label setup and their correlations between genres.\n\nDuring the training process, we noticed a similar behavior of the GMU in comparison with other units such as convolution filters or fully connected units. Overall, batch normalization considerably helped in terms of training time and convergence, resulting in less sensitivity to hyperparameters such as initialization ranges or learning rate. Also, dropout and max-norm regularization strategies helped to increase the performance at test time.\n\nOur work is closely related to the mixture of experts (MoE) approach [1]. However, the common usage of MoE is focused on combining predictors to address a supervised learning problem [2]. The GMU is proposed as a new component in the representation learning scheme, making it independent from the final task (e.g. classification, regression, unsupervised learning, etc) provided that the defined cost function be differentiable.\n\nFollowing your questions, we\u2019ll do the next changes to the paper:\n  - report the new simulations results.\n  - extend related work section with the MoE discussion.\n  - Include GMU training behavior as explained above.\n  - Similarly to the text length, we'll include histograms for height and width of images.\n\n[1] Jacobs, Robert A., et al. \"Adaptive mixtures of local experts.\" Neural computation 3.1 (1991): 79-87.\n[2] Yuksel, Seniha Esen, Joseph N. Wilson, and Paul D. Gader. \"Twenty years of mixture of experts.\" IEEE transactions on neural networks and learning systems 23.8 (2012): 1177-1193.", "title": "Comments on questions"}, "S1cPuGzQe": {"type": "review", "replyto": "Hy-2G6ile", "review": "I appreciated the synthetic data experiments as a clear example showing the capabilities of GMUs.  However, I would have liked to see more detail.  You say the (non-GMU) \"logistic classifier was not\" able to solve the task.  I would have expected it to do OK, but with less reliability due to the noise.  I would also like more intuition about how the gate learned to distinguish the relevant input versus the noise.\n\nI was very happy to see Table 4 with example outputs, but since the main technical contribution of the paper is the gated units, it would be nice to see the gating parameters projected back into text and image space.  What features of each modality indicate it is amore reliable indicator?\n\nMultiplicative operations in deep learning can sometimes be hard to train.  Could you comment on your experiences here?\n\nCould you say more about related work in multiplicative operations in deep learning?\n\nYou give some statistics on the text data for the movies.  Can you say something about the image data?  Perhaps even just the size in pixels?\nThe paper introduces Gated Multimodal Units GMUs, which use multiplicative weights to select the degree to which a hidden unit will consider different modalities in determining its activation.  The paper also introduces a new dataset, \"Multimodal IMDb,\" consisting of over 25k movie summaries, with their posters, and labeled genres.\n\nGMUs are related to \"mixture of experts\" in that different examples will be classified by different parts of the model, (but rather than routing/gating entire examples, individual hidden units are gated separately).  They are related to attention models in that different parts of the input are weighted differently; there the emphasis is on gating modalities of input.\n\nThe dataset is a very nice contribution, and there are many experiments varying text representation and single-modality vs two-modality.  What the paper is lacking is a careful discussion, experimentation and analysis in comparison to other multiplicative gate models---which is the core intellectual contribution of the paper.  For example, I could imagine that a mixture of experts or attention models or other gated models might perform very well, and at the very least provide interesting scientific comparative analysis.  I encourage the authors to continue the work, and submit a revised paper when ready.\n\nAs is, I consider the paper to be a good workshop paper, but not ready for a major conference.", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1zs_jEVe": {"type": "review", "replyto": "Hy-2G6ile", "review": "I appreciated the synthetic data experiments as a clear example showing the capabilities of GMUs.  However, I would have liked to see more detail.  You say the (non-GMU) \"logistic classifier was not\" able to solve the task.  I would have expected it to do OK, but with less reliability due to the noise.  I would also like more intuition about how the gate learned to distinguish the relevant input versus the noise.\n\nI was very happy to see Table 4 with example outputs, but since the main technical contribution of the paper is the gated units, it would be nice to see the gating parameters projected back into text and image space.  What features of each modality indicate it is amore reliable indicator?\n\nMultiplicative operations in deep learning can sometimes be hard to train.  Could you comment on your experiences here?\n\nCould you say more about related work in multiplicative operations in deep learning?\n\nYou give some statistics on the text data for the movies.  Can you say something about the image data?  Perhaps even just the size in pixels?\nThe paper introduces Gated Multimodal Units GMUs, which use multiplicative weights to select the degree to which a hidden unit will consider different modalities in determining its activation.  The paper also introduces a new dataset, \"Multimodal IMDb,\" consisting of over 25k movie summaries, with their posters, and labeled genres.\n\nGMUs are related to \"mixture of experts\" in that different examples will be classified by different parts of the model, (but rather than routing/gating entire examples, individual hidden units are gated separately).  They are related to attention models in that different parts of the input are weighted differently; there the emphasis is on gating modalities of input.\n\nThe dataset is a very nice contribution, and there are many experiments varying text representation and single-modality vs two-modality.  What the paper is lacking is a careful discussion, experimentation and analysis in comparison to other multiplicative gate models---which is the core intellectual contribution of the paper.  For example, I could imagine that a mixture of experts or attention models or other gated models might perform very well, and at the very least provide interesting scientific comparative analysis.  I encourage the authors to continue the work, and submit a revised paper when ready.\n\nAs is, I consider the paper to be a good workshop paper, but not ready for a major conference.", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkqEFuzMl": {"type": "rebuttal", "replyto": "H1EE-JGGl", "comment": "Thanks for your comments,\n\n1) It is an interesting idea of only fine-tuning the last layer. We'll evaluate it and report the best baseline.\n\n2) Indeed, we aimed to illustrate it on the Section 5.1 \"Evaluation over synthetic data\". We'll improve this section so that it is clearer.\n\nThanks again for your feedback.", "title": "Comments on questions"}, "H1EE-JGGl": {"type": "review", "replyto": "Hy-2G6ile", "review": "Thanks for the interesting paper. I have two questions on the experiments and implementation:\n\n1) As for the baseline of the concatenation method,  the paper implemented by retraining the network. What about first retraining separately and then concatenating them and fine tuning the last layer? That may reduce the overfitting and get better results. \n\n2) Could illustrate the effects of GMU using some toy data? I think information like decision boundaries and optimization curves will be useful for the users to understand this new model.This paper proposed The Gated Multimodal Unit (GMU) model for information fusion. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. The paper collected a large genre dataset from IMDB and showed that GMU gets good performance.\n\nThe proposed approach seems quite interesting, and the audience may expect it can be used in general scenarios beyond movie genre prediction. So it is quite straightforward that the paper should test the algorithm in other applications, which was not done yet. That is the biggest shortcoming of this paper in my opinions.  \n\nAnother concern lies in how to evaluate the performance of information fusion. The abstract claims \"The model improves the macro f-score performance of single-modality models by 30% and 4% with respect to visual and textual information respectively\", however, such an improvement is off the key. If two modals are complementary to each other, the fusion results will always be higher. The key fact is how much better than baselines the proposed GMU is. There is a long list of techniques for fusions, so it is difficult to conduct an impressive comparison on only one real dataset. I think GMU did a nice work on movie dataset, but I would also expect other techniques, including fine-tuning, dropout, distillation may help too.  It would be nice if the author could compare these techniques. \n\nI also hope this paper could talk in more details the connection with mixture-of-expert (MoE) model. Both models are based on the nonlinear gated functions, while both method may suffer from local minimum for optimization on small datasets. I would like more in-depth discussion in their similarity and difference.\n\nTo gain more attention for GMU, I would encourage the author to open-source their code and try more datasets.", "title": "several questions on experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Sy4GlaZ4g": {"type": "review", "replyto": "Hy-2G6ile", "review": "Thanks for the interesting paper. I have two questions on the experiments and implementation:\n\n1) As for the baseline of the concatenation method,  the paper implemented by retraining the network. What about first retraining separately and then concatenating them and fine tuning the last layer? That may reduce the overfitting and get better results. \n\n2) Could illustrate the effects of GMU using some toy data? I think information like decision boundaries and optimization curves will be useful for the users to understand this new model.This paper proposed The Gated Multimodal Unit (GMU) model for information fusion. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. The paper collected a large genre dataset from IMDB and showed that GMU gets good performance.\n\nThe proposed approach seems quite interesting, and the audience may expect it can be used in general scenarios beyond movie genre prediction. So it is quite straightforward that the paper should test the algorithm in other applications, which was not done yet. That is the biggest shortcoming of this paper in my opinions.  \n\nAnother concern lies in how to evaluate the performance of information fusion. The abstract claims \"The model improves the macro f-score performance of single-modality models by 30% and 4% with respect to visual and textual information respectively\", however, such an improvement is off the key. If two modals are complementary to each other, the fusion results will always be higher. The key fact is how much better than baselines the proposed GMU is. There is a long list of techniques for fusions, so it is difficult to conduct an impressive comparison on only one real dataset. I think GMU did a nice work on movie dataset, but I would also expect other techniques, including fine-tuning, dropout, distillation may help too.  It would be nice if the author could compare these techniques. \n\nI also hope this paper could talk in more details the connection with mixture-of-expert (MoE) model. Both models are based on the nonlinear gated functions, while both method may suffer from local minimum for optimization on small datasets. I would like more in-depth discussion in their similarity and difference.\n\nTo gain more attention for GMU, I would encourage the author to open-source their code and try more datasets.", "title": "several questions on experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}