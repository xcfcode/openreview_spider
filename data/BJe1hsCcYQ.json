{"paper": {"title": "Lorentzian Distance Learning", "authors": ["Marc T Law", "Jake Snell", "Richard S Zemel"], "authorids": ["law@cs.toronto.edu", "jsnell@cs.toronto.edu", "zemel@cs.toronto.edu"], "summary": "A distance learning approach to learn hyperbolic representations", "abstract": "This paper introduces an approach to learn representations based on the Lorentzian distance in hyperbolic geometry. Hyperbolic geometry is especially suited to hierarchically-structured datasets, which are prevalent in the real world. Current hyperbolic representation learning methods compare examples with the Poincar\\'e distance metric. They formulate the problem as minimizing the distance of each node in a hierarchy with its descendants while maximizing its distance with other nodes. This formulation produces node representations close to the centroid of their descendants. We exploit the fact that the centroid w.r.t the squared Lorentzian distance can be written in closed-form. We show that the Euclidean norm of such a centroid decreases as the curvature of the hyperbolic space decreases. This property makes it appropriate to represent hierarchies where parent nodes minimize the distances to their descendants and have smaller Euclidean norm than their children. Our approach obtains state-of-the-art results in retrieval and classification tasks on different datasets. ", "keywords": ["distance learning", "metric learning", "hyperbolic geometry", "hierarchy tree"]}, "meta": {"decision": "Reject", "comment": "Dear authors,\n\nThe reviewers all appreciated the treatment of the topic and the quality of the writing. It is rare for all reviewers to agree on this, congratulations.\n\nHowever, all reviewers also felt that the paper could have gone further in its analysis. In particular, they noticed that quite a few points were either mentioned in recent papers or lacked an experimental validation.\n\nGiven the reviews, I strongly encourage the authors to expand on their findings and submit the improved work to a future conference."}, "review": {"Bkx5FsmP1N": {"type": "rebuttal", "replyto": "Bkx8735NJV", "comment": "We thank the reviewer for taking time to check our statement about the differentiability of Eq. (3). \nFor simplicity, we will write the proof in .tex code since we cannot update the pdf file.\n\nLet us note $\\textbf{c} = (x, c_2)$ and $\\textbf{d} = (d_1, d_2)$ the considered points in Eq. (3) when the dimensionality is 2 (as suggested by the reviewer). We show in the following that Eq. (3) is not differentiable when $\\textbf{c} = \\textbf{d}$. To this end, we consider that our variable is $x$ (i.e. the first element of \\textbf{c}). \nWe also note $h = x - d_1$. \nNote that we could equivalently consider that our variables are the other elements of $\\textbf{c}$ or $\\textbf{d}$. \nSince we assume that $\\textbf{c} = \\textbf{d}$, we have $c_2 = d_2$ and we study the behavior of $x$ when it tends to $d_1$ or equivalently when $h$ tends to 0.\n\nWhen $c_2 = d_2$, Eq. (3) can be written as:\n\\begin{equation}\narcosh(f(h)) = log( f(h) + \\sqrt{f^2(h) - 1})\n\\end{equation}\nwhere \n\\begin{equation}\nf(h) = 1 + 2 \\frac{h^2}{(1 - (d_1 + h)^2 - d_2^2)(1 - d_1^2 - d_2^2)} = 1 + 2 \\frac{h^2}{\\alpha}\n\\end{equation}\nwhere we note $\\alpha = (1 - (d_1 + h)^2 - d_2^2)(1 - d_1^2 - d_2^2) > 0$ which is positive since the points $\\textbf{c}$ and $\\textbf{d}$ are constrained to be in the interior of the unit disk.\n\nFor all $h$, $arcosh(f(h))$ is then nonnegative.\n \nWe also have $f(0) = 1$ and $arcosh(f(0)) = 0$.\n\nWe now study two one-sided limits. The first one is the right-sided limit:\n\\begin{equation}\nlim_{h \\to 0^+} \\frac{arcosh(f(h)) - arcosh(f(0))}{h} = lim_{h \\to 0^+} \\frac{arcosh(f(h))}{h} = lim_{h \\to 0^+} (arcosh(f(h)))'\n\\end{equation}\nwhich is nonnegative since $arcosh(f(h))$ is nonnegative.\nThe second one is the left-sided limit:\n\\begin{equation}\nlim_{h \\to 0^-} \\frac{arcosh(f(h)) - arcosh(f(0))}{h}  = lim_{h \\to 0^-} \\frac{arcosh(f(h))}{h} = lim_{h \\to 0^-} (arcosh(f(h)))'\n\\end{equation}\nwhich is nonpositive.\n\nWe show in the following that these two limits are different, which implies that the function is not differentiable when $\\textbf{c} = \\textbf{d}$.\n\nWe then need to show that at least one of these limits is nonzero to show they are different.\nWe study the first one and show that it is nonzero.\n\n\n\n\n\nThe derivative of arcosh wrt $z > 1$ is:\n\\begin{equation}\narcosh'(z) = \\frac{1}{\\sqrt{z^2 - 1}}\n\\end{equation}\n\nThe derivative of $f$ wrt $h$ is:\n\\begin{equation}\nf'(h) =  \\frac{4h (d_1 h + d_2^2 + d_1^2 - 1)}{(d_2^2 + d_1^2 - 1)(h^2 + 2 d_1 h + d_2^2 + d_1^2 - 1)^2}\n\\end{equation}\n\nThe derivative of $arcosh(f)$ wrt $h$ is then $f'(h) arcosh'(f(h))$ which can be written:\n\\begin{equation}\n\\frac{4h (d_1 h + d_2^2 + d_1^2 - 1)}{(d_2^2 + d_1^2 - 1)(h^2 + 2 d_1 h + d_2^2 + d_1^2 - 1)^2 \\sqrt{(1 + 2 \\frac{h^2}{(1 - (d_1 + h)^2 - d_2^2)(1 - d_1^2 - d_2^2)})^2-1}}\n\\end{equation}\nwhich is equal to:\n\\begin{equation}\n\\frac{4h(d_1 h + d_2^2 + d_1^2 - 1)}{(d_2^2 + d_1^2 - 1)(h^2 + 2 d_1 h + d_2^2 + d_1^2 - 1)^2 \\sqrt{\\left(2 \\frac{h^2}{(1 - (d_1 + h)^2 - d_2^2)(1 - d_1^2 - d_2^2)} \\right) \\left(2 \\frac{h^2}{(1 - (d_1 + h)^2 - d_2^2)(1 - d_1^2 - d_2^2)} + 2\\right)}}\n\\end{equation}\nwhich is equal to:\n\\begin{equation}\n\\frac{2h(d_1 h + d_2^2 + d_1^2 - 1)}{(h^2 + 2 d_1 h + d_2^2 + d_1^2 - 1) \\sqrt{ h^2 \\left( h^2 + (1 - (d_1 + h)^2 - d_2^2)(1 - d_1^2 - d_2^2)\\right)}}\n\\end{equation}\nFrom the above equation, one can see that the right sided limit $L$ is:\n\\begin{equation}\n    L := lim_{h \\to 0^+} (arcosh(f(h)))' = lim_{h \\to 0^+} \\frac{2(d_1 h + d_2^2 + d_1^2 - 1)}{(h^2 + 2 d_1 h + d_2^2 + d_1^2 - 1) \\sqrt{ \\left( h^2 + (1 - (d_1 + h)^2 - d_2^2)(1 - d_1^2 - d_2^2)\\right)}}\n\\end{equation}\nwhich is equal to:\n\\begin{equation}\nL = \\frac{2}{1 - d_2^2 - d_1^2}\n\\end{equation}\nwhich is nonzero by definition of the domain of $\\textbf{d} = (d_1, d_2)$.\nFor instance, when $\\textbf{d} = 0$, we have $L = 2$.\n\nA similar proof can show that the left-sided limit is:\n\\begin{equation}\n    lim_{h \\to 0^-} (arcosh(f(h)))' = -L\n\\end{equation}\n\nThe fact that these two one-sided limits are different shows that Eq. (3) is not differentiable when $\\textbf{c} = \\textbf{d}$.\n\nWe would also like to emphasize that L increases as the l2 norm of \\textbf{d}$ increases.", "title": "Reply to differentiability  "}, "HJlQ5vP4kV": {"type": "rebuttal", "replyto": "HkeimBHmJE", "comment": "The Poincar\u00e9 metric is not differentiable on P^2 (or P^d in general for any value of d) when the compared points are equal. \nThe formulation of the Poincar\u00e9 metric on P^d is given in Eq. (3): when c = d (the optimal case that we might want to obtain for some pairs), Eq. (3) is equal to arcosh(1). However, due to its definition, arcosh(x) = log( x + sqrt( x^2 - 1)) is not differentiable at x = 1 (due to the sqrt term).\nWe agree that arcosh is differentiable on (1, +inf). Moreover, the points have to be reprojected onto the interior of the unit disk if the optimized variables are in P^2.", "title": "About the differentiability of the Poincar\u00e9 metric on P^2"}, "Hylsv6FiT7": {"type": "rebuttal", "replyto": "BJe1hsCcYQ", "comment": "We thank the reviewers, we clarify some points in the individual responses.\n\nWe have updated the paper with some requested additions. \nThe references that we use in our rebuttals are:\n\n[A] Ungar, Analytic Hyperbolic Geometry in N dimensions\n[B] Ungar, Barycentric Calculus in Euclidean and Hyperbolic Geometry, 2010\n[C] Gulcehre et al., Hyperbolic attention networks. Arxiv 2018\n[D] Nickel and Kiela, Learning continuous hierarchies in the lorentz model of hyperbolic geometry, ICML 2018\n[E] Ganea et al., Hyperbolic neural networks. NIPS 2018\n[F] De Sa et al., Representation tradeoffs for hyperbolic embeddings, 2018\n", "title": "We have updated the paper."}, "rklLNTKs6X": {"type": "rebuttal", "replyto": "Byx1QW7N2X", "comment": "- \u201cTheorem 3.4 (centroid computation with constraints) shows that minimizing the energy function $\\sum_{i} d_L^2 (x_i, a)$, when a is constrained to a discrete set, is equivalent to minimizing $d_L(c,a)$, where $c$ is given by Lemma 3.2. This is interesting as compared to the previous two theorems, but it is not clear whether/how this equivalence is used in the proposed embedding.\u201d\n\nWe would like to emphasize why this theorem is important for some important contribution of the paper. \nWe show that the distances with respect to a set of points can be reduced to calculating the distance to the centroid in the case of the Lorentzian distance. Interpreting distances to a set of points can then be done by interpreting their centroid. \nFrom this observation, we study the property of the centroid which can be written in closed form. In particular, we show that the Euclidean norm of the centroid decreases as the curvature of the space decreases, representing trees then becomes easier. \n\n- \u201cThese contributions are useful but incremental. Notably, (1) needs more experimental evidence (e.g. a toy example) to show the numerical instability of the other methods, and to show the learning curves of the proposed re-parametrization against the Riemannian stochastic gradient descent, which are not given in the paper.\u201d\n\nAs mentioned above, our goal is not to prove that our algorithm is more stable than existing approaches although we explain that the Poincar\u00e9 distance metric is not differentiable everywhere on the domain and its gradient tends to infinity when distances are infinitesimal. Both our approach and the Riemannian stochastic gradient descent are stable, although we can use momentum-based optimizers.\n\nAs already mentioned, the main advantage of our approach is that we are able to interpret the behavior of distances with sets of similar points.\n", "title": "Response to AnonReviewer2"}, "B1epRhtspX": {"type": "rebuttal", "replyto": "rkeqJ1V6hX", "comment": "- \u201cUsing Weierstrass allows to use standard optimization methods without leaving the manifold. However, the optimization is still performed on a Riemannian manifold.\u201d\n\nWe understand the concern that a Riemannian optimizer would probably be more appropriate since our representations lie on a manifold. \nHowever, from Eq. (9), our distance function can also be seen as the sum of a simple bilinear form between real vectors and another term promoting some similarity of their Euclidean norms. This formulation is then very similar to optimizing (squared) Euclidean distances.\n\n- \u201cComputing the centroid in closed form is interesting but isn't really exploited in the paper.\u201c\n\nWe agree that we do not explicitly use the closed-form solution of the centroid in our experiments. However, our last theorem explains that minimizing the distances to a set of points is equivalent to minimizing the distance to its centroid. \nOur study of the centroid is important to understand the behavior of our distance function with the considered set of similarity constraints (based on hierarchical relationships).\n\n- \u201cThe only use of the centroid seems then to justify the regularization method, i.e., that parents should have a smaller norm than their children. However, this insight alone seems not particularly novel, as the same insight can be derived for standard hyperbolic method and has, for instance, been discussed in Nickel & Kiela (2017, 2018), Ganea et al (2018), De Sa (2018).\u201d\n\nAlthough the fact that the representation of the common ancestor should have lower Euclidean norm is mentioned in these papers, it is never proven that it has lower Euclidean norm. The closest example that mentions a minimizer of an expectation of (squared) hyperbolic distances is De Sa [F]. However, they do not exploit a closed-form of the centroid and have to use a gradient-based method to minimize an optimization problem based on it (see [F], Section 4.2). \nWe show that the Euclidean norm of the centroid of a set of point can be controlled with the curvature of the hyperbolic space. We experimentally show its impact in Table 2. Retrieval performance (Mean Rank and MAP) in Table 1 shows how close to its descendents a common ancestor is. The Poincar\u00e9 metric is defined for a fixed curvature of -1 and cannot have smaller curvature given its formulation exploiting arcosh. \n\nFig. 1 of our submission shows an example where the centroid of the Poincar\u00e9 metric does not have a smaller Euclidean distance than the set of points. \nBy manipulating the curvature of the space, the centroid of the Lorentzian norm can produce centroids with smaller Euclidean norm (as we demonstrate that they depend on each other).\nWe can also plot the centroid of the squared Poincar\u00e9 distance, which shows that the corresponding centroid does not have a smaller Euclidean norm either.\n\n- \u201cp.3: Projection onto the Poincar\u00e9 ball/manifold is only necessary when the exponential map isn't used. Nickel & Kiela (2018), Ganea et al (2018) therefore don't have this problem.\u201d\n\nThat is exactly what we explain in p.3, although Ganea et al. [E] also reproject their embeddings onto the Poincar\u00e9 ball at each iteration (as explained in the \u201cNumerical errors\u201d paragraph of Section 4 of [E]). \nNickel & Kiela (2018) propose to work in the hyperboloid space to avoid this reprojection as we explain in p.3.\n\n- \u201cp.7: Since MR and MAP are ranking measures, and the ranking of distances between H^d and the L^2 distance should be identical, it is not clear why the experiments show significant differences for these methods when \\beta=1\u201d\n\nAlthough the order of distances is the same between H^d and the L^2 (since they only differ by an arcosh activation function), these two distance functions are not equivalent. \nThe arcosh has a logarithmic form and then tends to penalize differences between small distances more than differences between large distances. \nThis generates a difference during training that is for instance similar to the difference obtained by training a linear loss vs a quadratic loss. The quadratic loss tends to penalize outliers more than a linear loss. \nThe fact that these distance functions are not equivalent explains the difference of the results.\n\n- \u201cp.7: Embeddings in the Poincar\u00e9 ball and the Hyperboloid are both compatible with the regularization method in eq.14. It would be interesting to also see results for these methods with regularization.\u201d\n\nWe agree but the point of that regularizer was to show that the global structure of the tree could be easily recovered by using such constraints without having a significant impact on the retrieval performances (i.e. Mean Rank and Mean Average Precision).\nWe have added for instance in the updated version a study of the impact of such regularization on classification performance in Table 3. Removing that regularization consistently leads to (slightly) better classification scores.", "title": "Response to AnonReviewer3"}, "HyxZY3FiaQ": {"type": "rebuttal", "replyto": "SJe6XoC02m", "comment": "-\u201cEq 11, bears lots of similarity to the Einstein gyro-midpoint\u201d\n \nAs mentioned in our introduction, there exist various hyperbolic geometries with corresponding distances: the Poincar\u00e9 distance, the Lorentzian distance and the gyrodistance [A,B]. We have investigated their relationships. \nTo the best of our knowledge, the centroid of the Poincar\u00e9 distance metric has no closed form solution. We provide in our submission a closed-form solution for the squared Lorentzian distance for any negative curvature of the space. \nThe main motivation to introduce the centroid is our last theorem which shows that comparing distances wrt the squared Lorentzian distance with a set of points is equivalent to comparing distances with their centroid. By studying the properties of the centroid, we then have an idea of the behavior of distances with the corresponding set of points.\n\nGulcehre et al. [C] optimize the Poincar\u00e9 distance but exploit as representative the Einstein gyrocentroid, also known as the Einstein midpoint when the set has only 2 points. Unlike the 2 previous centroids, that point is in general not an optimizer of an expectation over gyrodistances to a set of points. Nonetheless, when the set contains only 2 points, it is the minimizer of the Einstein addition of the gyrodistances between it and the two points of the set by using the gyrotriangle inequality. However, the Einstein addition is not commutative, the expectation properties then do not generalize for a set of more than 2 points although the gyrocentroid does preserve left gyrotranslation.\n \nConceptually, the main difference is that the Lorentzian centroid can be seen as a minimizer of some expectation over distances, the Lorentzian centroid is then ideal to represent the common ancestor of a set of tree nodes. On the other hand, the gyrocentroid can be seen as a point which preserves left gyrotranslation (see Remark 5.7 of [A]).  The motivation of exploiting the point that preserves left gyrotranslation to be compared wrt the Poincar\u00e9 metric (which corresponds to another distance) is not clear to us, at least for our task.\n\nEach of the distances have different centroids which are illustrated in the updated Fig 1.\n \n- \"Experiments are limited to small scale datasets.\"\n \nWe report quantitative results on the same datasets as [D] and [E].\nMore exactly, [E] consider two tasks but they mention for the first task: \"for the sentence entailment classification task, we do not see a clear advantage of hyperbolic MLR compared to its Euclidean variant.\" We then compared our method in the task where they see an improvement when using hyperbolic representations. Our approach outperforms theirs.\n\n- \"The paper claims that Riemannian optimization is not necessary for this model\"\n\nWe do not sell the fact that we do not use Riemannian optimization as a contribution. Other approaches such as [D] can also do that (although their function is not differentiable on the whole domain and gradients tend to infinity for pairs of points with infinitesimal distance). We only say that, given the formulation of our distance in Eq. (9), using standard SGD is sufficient to outperform current hyperbolic approaches.\nWe have not tried a Riemannian optimizer since the performance of a standard SGD already works well. We plan to do that in the future.\n   \n- \"The novelty of the approach is limited compared to [1].\u201d\n\nThe main contribution of the paper is not only a closed-form solution for the centroid. We exploit our last theorem that explains that distances with a set of points can be reduced to the study of the distance wrt the centroid. We then analyze some of its properties and explain why they are appropriate to represent trees.\n\n- \"Have you tried learning beta?\"\n \nFollowing the review, we have learned beta by learning a variable constrained to be positive by using a softplus activation function.\nHere are results on some datasets, they are comparable with those reported when beta is 0.01:\nACM: MR: 1.03 - MAP: 98.4 - SROC: 53.4\nEuroVOC: MR: 1.06 - MAP: 96.5 - SROC: 33.8\nWordnet verbs: MR: 1.10 - MAP: 94.7- SROC: 26.1\nThe learned beta has values in the interval [10^(-6),10^(-4)] \n \n- \u201cOn Eurovoc the results are worse than d_P in H^d\u201d\n\nOnly the SROC score is worse, we obtain better or comparable retrieval scores (i.e. MR and MAP) on this dataset. The SROC score can be improved by increasing the regularization parameter lambda, but we only reported scores for one value of lambda.\n\n- \u201cCan you reduce the number of data points in Fig 2\u201d\n\nAs requested, we have added Fig. 5 that only plots the names of the kernel method researchers mentioned in Table 4. Most of them are represented close to the same radius, which validates our study of the Lorentzian distance for small values of beta.\nWe have also added Fig. 3 to illustrate the Lorentzian distance to 1 point as a function of beta.", "title": "Response to AnonReviewer1"}, "SJe6XoC02m": {"type": "review", "replyto": "BJe1hsCcYQ", "review": "Summary\n\nLearning embeddings of graphs in hyperbolic space have become popular and yielded promising results. A core reason for that is learning hierarchical representations of the graphs is easier in hyperbolic space due to the curvature and the geometrical properties of the hyperbolic space. Similar to [1, 2], this paper uses Lorentzian model of the hyperbolic space in order to learn embeddings of the graph. The main difference of the proposed approach in this paper is that  they come up with a closed-form solution such that each node representation close to the centroid of their descendants. A curious property of the equation for the centroids proposed to learn the embeddings of each node also encodes information related to the specificity in the Euclidean norm of the centroid. Also this paper introduces two additional hyperparameters. Beta hyperparameter is selected to control the curvature of the space. Depending on the task the optimal curvature can be tuned to be a different value. This also ties closely with the de-Sitter spaces. Authors provide results on different graph embedding benchmark tasks. The paper claims that, an advantage of the proposed approach is that the embedding of the model can be tuned with regular SGD without needing to use Riemannian optimization techniques.\n\nQuestions\n\nHave you tried learning beta instead of selecting as a hyperparameter?\nThe paper claims that Riemannian optimization is not necessary for this model, but have you tried optimizing the model with the Riemannian optimization methods?\nEquation 11, bears lots of similarity to the Einstein gyro-midpoint method proposed by Abraham Ungar which is also used by [2]. Have you investigated the relationship between the two formulations?\nOn Eurovoc dataset the results of the proposed method is worse than the d_P in H^d. Do you have a justification of why that happens?\n\n\nPros\nThe paper delivers some interesting theoretical findings about the embeddings learned in hyperbolic space, e.g. a closed for equation in the\nThe paper is written well. The goal and motivations are clear.\n\n\nCons\nExperiments are only limited to small scale-traditional graph datasets. It would be more interesting to see how those embeddings would perform on large-scale datasets such as to learn knowledge-base embeddings or for recommendation systems.\n\nAlthough the idea is interesting. Learning graph embeddings have already been explored in [1]. The main contribution of this paper is thus mainly focuses on the close-form equation for the centroid and the curvature hyperparameter. These changes provide significant improvements on the results but still the novelty of the approach is in that sense limited compared to [1].\n\n\nMinor comment:\n\nIt is really difficult to understand what is in Figure 2 and 3. Can you reduce the number of data points and just emphasize a few nodes in the graph that shows a clear hierarchy.\n\n[1] Nickel, Maximilian, and Douwe Kiela. \"Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry.\" arXiv preprint arXiv:1806.03417 (2018).\n[2] Gulcehre, Caglar, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia et al. \"Hyperbolic Attention Networks.\" arXiv preprint arXiv:1805.09786 (2018).", "title": "Review for Lorentzian Distance Learning", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkeqJ1V6hX": {"type": "review", "replyto": "BJe1hsCcYQ", "review": "The paper proposes a new approach to compute hyperbolic embeddings based on the squared Lorentzian distance. This choice of distance function is motivated by the observation that the ranking of these distances is equivalent to the ranking of the true hyperbolic distance (e.g., on the hyperboloid). For this reason, the paper proposes to use this distance function in combination with ranking losses as proposed by Nickel & Kiela (2017), as it might be easier to optimize. Moreover, the paper proposes to use Weierstrass coordinates as a representation for points on the hyperboloid.\n\nHyperbolic embeddings are a promising new research area that fits well into ICLR. Overall, the paper is written well and good to understand. It introduces interesting ideas that are promising to advance hyperbolic embeddings. However, in the current version of the paper, these ideas are not fully developed or their impact is unclear.\n\nFor instance, using Weierstrass coordinates as a representations seems to make sense, as it allows to use standard optimization methods without leaving the manifold. However, it is important to note that the optimization is still performed on a Riemannian manifold. For that reason, following the Riemannian gradient along geodesics would still require the exponential map. Moreover, optimization methods like Adam or SVRG are still not directly applicable. Therefore, it seems that the practical benefit of this representation is limited.\n\nSimilarly, being able to compute the centroid efficiently in closed form is indeed an interesting aspect of the proposed approach. Moreover, the paper explicitly connects the centroid to the least common ancestor of children in a\ntree, what could be very useful to derive new embedding methods. Unfortunately, this is advantage isn't really exploited in the paper. The approach taken in the paper simply uses the loss function of Nickel & Kiela (2017) and this loss doesn't make use of centroids, as the paper notes itself. The only use of the centroid seems then to justify the regularization method, i.e., that parents should have a smaller norm than their children. However, this insight alone seems not particularly novel, as the same insight can be derived for standard hyperbolic method and has, for instance, been discussed in Nickel & Kiela (2017, 2018), Ganea et al (2018), De Sa (2018). Using the centroid to derive new hyperbolic embeddings could be interesting, but, unfortunately, is currently not done in the paper.\n\nFurther comments\n- p.3: Projection back onto the Poincar\u00e9 ball/manifold is only necessary when\n  the exponential map isn't used. The methods of Nickel & Kiela (2018), Ganea et al (2018) therefore don't have this problem.\n- p.7: Since MR and MAP are ranking measures, and the ranking of distances between H^d and the L^2 distance should be identical, it is not clear to me why the experiments show significant differences for these methods when \\beta=1\n- p.7: Embeddings in the Poincar\u00e9 ball and the Hyperboloid are both compatible with the regularization method in eq.14 (using their respective norms). It would be interesting to also see results for these methods with regularization.", "title": "Review - Lorentzian Distance Learning", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byx1QW7N2X": {"type": "review", "replyto": "BJe1hsCcYQ", "review": "This paper proposed an unsupervised embedding method for hierarchical or graph datasets. The embedding space is a hyperbolic space as in several recent works such as (Nickel & Kiela, 2017). The author(s) showed that using the proposed embedding, the optimization has better numerical stability and better performance.\n\nI am convinced of the correctness and the experiment results, and I appreciate that the paper is well written with interesting interpretations such as the demonstration of the centroid. However, the novelty of this contribution is limited and may not meet the publication standard of ICLR.  I suggest that the authors enhance the results and resubmit this work in a future venue.\n\nTheoretically, there are three theorems in section 3:\n\nTheorem 3.1 shows the coordinate transformation from the proposed parametrization to hyperboloid then to Poincare ball preserves the monotonicity of the Euclidean norm. This is straightforward by writing down the two transformations.\n\nLemma 3.2 and theorem 3.3 state the centroid in closed expression based on the Lorentzian distance, taking advantage that the  Lorentzian distance is in a bi-linear form (no need to take the arcosh()  therefore the analysis are much more simplified) These results are quite striaghtforward from the expression of the energy function.\n\nTheorem 3.4  (centroid computation with constraints) shows that minimizing the energy function\n$\\sum_{i} d_L^2 (x_i, a)$, when a is constrained to a discrete set, is equivalent to minimizing $d_L(c,a)$, where $c$ is given by Lemma 3.2.\nThis is interesting as compared to the previous two theorems, but it is not clear whether/how this equivalence is used in the proposed embedding.\n\nTechnically, there are three novel contributions,\n\n1. The proposed unconstrained reparametrization of the hyperboloid model does not require to project the embedding points onto the hyperbolic manifold in each update.\n\n2. The cost is based on the Lorentzian Distance, that is a monotonic transformation of the Riemannian distance (without taking the arccosh function). Therefore the similarity (a heat kernel applied on the modified distance function) is measured differently than the other works. Informally one can think it as t-SNE v.s. SNE which use different similarity measures in the target embedding.\n\n3. The authors discussed empirically the different choice of beta, which was typically chosen as beta=1 in previous works, showing that tunning the beta hyperparameter can give better embeddings.\n\nThese contributions are useful but incremental. Notably, (1) needs more experimental evidence (e.g. a toy example) to show the numerical instability of the other methods, and to show the learning curves of the proposed re-parametrization against the Riemannian stochastic gradient descent, which are not given in the paper.\n\nBy listing these theoretical and technical contributions, overall I find that most of these contributions are incremental and not significant enough for ICLR.", "title": "An incremental work on hyperbolic embedding with Lorentzian Distance", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}