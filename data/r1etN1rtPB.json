{"paper": {"title": "Implementation Matters in Deep RL: A Case Study on PPO and TRPO", "authors": ["Logan Engstrom", "Andrew Ilyas", "Shibani Santurkar", "Dimitris Tsipras", "Firdaus Janoos", "Larry Rudolph", "Aleksander Madry"], "authorids": ["ailyas@mit.edu", "engstrom@mit.edu", "shibani@mit.edu", "tsipras@mit.edu", "firdaus.janoos@twosigma.com", "rudolph@csail.mit.edu", "madry@mit.edu"], "summary": "", "abstract": "We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of \"code-level optimizations:\" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains in deep reinforcement learning.\n", "keywords": ["deep policy gradient methods", "deep reinforcement learning", "trpo", "ppo"]}, "meta": {"decision": "Accept (Talk)", "comment": "This paper provides a careful and well-executed evaluation of the code-level details of two leading policy search algorithms, which are typically considered implementation details and therefore often unstated or brushed aside in papers. These are revealed to have major implications for the performance of both algorithms.\n\nThe reviewers are all in agreement that this paper has important reproducibility and evaluation implications for the field, and adds substantially to our body of knowledge on policy gradient algorithms. I therefore recommend it be accepted.\n\nHowever, a serious limitation is that only 3 random seeds were used to get average performance in the first, key experiment. Experiments are expensive, but that result is not meaningful without more runs, and arguably could be misleading rather than informative. The authors should increase the number of runs as much as possible, at least to 10 but ideally more."}, "review": {"v2HLSZXW9Q": {"type": "rebuttal", "replyto": "r1etN1rtPB", "comment": "We have now uploaded the camera-ready. While preparing to upload, we confirmed a bug in our code (precisely, in computing KL-divergences), and thus reran all of our experiments to ensure their validity. The vast majority of the results went unchanged, with the exception of the explicit plots of KL divergence for the PPO-NoClip algorithm, which have thus been removed (along with the respective 2-3 sentences in the text.) We also made a few improvements to the paper for clarity. A full summary is below:\n- Added numbers from OpenAI baselines to compare to\n- More thorough explanation of gridding procedure and hyperparamters used\n- Increased sample size for both the ablation study (5 agents per config for a total of 320 agents trained) as well as the rewards tables (now > 80 agents per cell, with 95% bootstrapped confidence intervals)\n- Fix typos/editing for clarity", "title": "Camera ready and revision changes"}, "H1xMvclFtr": {"type": "review", "replyto": "r1etN1rtPB", "review": "Summary\n\nThis paper calls to attention the importance of specifying all performance altering implementation details that are current inherent in the state-of-the-art deep policy gradient community. Specifically, this paper builds very closely on the work started by  Henderson et al. 2017, building a conversation around the importance of more rigorous and careful scientific study of published algorithms. This paper identifies many \"code-level optimizations\" that account for the differences between the popular TRPO and PPO deep policy gradient algorithms. The paper then subselects four of these optimizations and carefully investigates their impact on the final performance of each algorithm. The clear conclusion from the paper is that the touted algorithmic improvement of PPO over TRPO has negligible effect on performance, and any previously reported differences are due only to what were considered unimportant implementation details.\n\nReview\n\nThis paper investigates the claims made by Schulman et al. 2017 carefully, by investigating the impact of PPO's clipping mechanism on maintaining a valid trust-region; the central claim made by PPO's originating paper. The empirical results suggest that PPO is not sufficient for maintaining a valid trust-region, however the \"code-level optimizations\" that differ between the TRPO implementation the PPO implementation are sufficient. The ablation study of the four optimizations studied by the paper shows dramatic and clear results suggesting that annealing stepsizes and normalize rewards make very strong differences in learning performance; much more effect than demonstrated by the differences between TRPO and PPO's core algorithmic contribution as demonstrated in Figure 2 and even more strongly in Figure 3. I find the work included in this paper to be novel and a valuable contribution to the field.\n\nFor the above reasons, I recommend to accept this paper for publication at ICLR. In the following paragraphs I will discuss why I only recommend a weak accept instead of a strong accept.\n\nMy primary concern with the empirical study is the use of only three random seeds. As demonstrated in Henderson et al. 2017 (which is heavily cited in this paper), using such a small number of random seeds can have very misleading results. Although the effects appear very strong in the empirical studies in this paper, the effects likewise appear strong in Henderson et al.'s Figure 6 where 10 random seeds were split into two groups for the same algorithm. For this paper to make such strong claims about the negligence of the careful scientific study on TRPO and PPO, it would be best if this paper included far more random seeds in its investigation.\n\nMy second concern is with the discussion and conclusions drawn from Tables 1 and 2. It appears that the inclusion of clipping plays a strong role in the variance of each algorithm on every domain except Hopper. Specifically, the algorithms that include clipping appear to be much lower variance than the algorithms including clipping. Admittedly using only 3 seeds means that investigating the variance appropriately is near impossible (see the above paragraph), however variance should be considered and discussed in a conversation about the effects of the core contribution of PPO. If clipping leads to more consistent results across runs, even if those results are a little worse, it is still a valid and important contribution.\n\nThe paper cites Henderson et al. 2017 in several places. I would point out (perhaps in the introduction) that this paper builds on work already done in Henderson et al. 2017. Specifically, Henderson et al. 2017 investigates the effects of using different codebases for TRPO and shows that these different codebases result in dramatically different performance. The similarity to the investigation in this paper to too close to be unreported. However, I find that the investigation in this paper is much more complete and insightful than that of Henderson et al. 2017 (this paper has a more narrow focus), thus contributes significantly and meaningfully to this ongoing conversation.\n\nAdditional Comments (do not affect score)\n\nIt might be worthwhile to move the related work section to the beginning of the paper, either merged with the introduction or immediately after. This section is of critical importance to understanding the scope of this paper and for understanding why you are studying what you study. In fact, there is already a bit of duplication between the related works and introduction sections, so the paper could likely gain some additional real-estate by combining these.\n\nI disagree with the terminology \"code-level optimizations\" and I find that it is misleading. This caused a bit of confusion on my first pass reading the paper, as I originally was expected the code differences to be more akin to using Tensorflow vs PyTorch or switching hash table functions, etc. Instead the changes focused on in this paper are changes to the problem specification and algorithm implementation. These are not simply implementation details as \"code-level optimizations\" suggests, but are rather details that necessarily must be included in peer-reviewed works. I don't have a suggested name to switch to, but felt strongly enough to mention it.", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 3}, "HJeISRZAKS": {"type": "review", "replyto": "r1etN1rtPB", "review": "This paper investigates the impact of implementation \"details\", with existing implementations of TRPO and PPO as examples. The main takeaway is that the performance gains observed in PPO (compared to TRPO) are actually caused by differences in implementation, and not by the differences between the two learning algorithms. In particular, adding to the TRPO code the same implementation changes as in PPO makes TRPO on par with (and possibly even better than) PPO. The clipping objective of PPO is also found to have no significant impact on its performance. This calls for more careful comparisons between algorithms (by minimizing implementation changes and more in-depth ablation studies) than has typically been done until now in the RL research community.\n\nAlthough this paper is pretty straightforward and does not bring meaningful algorithmic improvements, I still believe it should be accepted as reproducibility and evaluation are a major issue in RL, and people need to be aware of these kinds of implementation differences that can affect the reported results.\n\nMy only important concern is that I could not find a link to the code, which I believe is a must for such a paper focusing on implementation. Could the authors please confirm that they will release their code?\n\nOther small remarks:\n\t- Fig. 1 is hard to read, I think more synthetic results could have easily conveyed more clearly the intended message\n\t- When referring to Fig. 2 and 3 please specify \"left\", \"middle\" or \"right\"\n\t- Fig. 2's caption should describe the plots in left to right order (also what does \"maximum versus mean KL\" mean?)\n\t- Fig. 3's caption lists mean KL twice on its first line\n\t- \"The trust region for PPO-NoClip bounds KL to a lesser degree\": this is confusing as it sounds like it is \"less bounded\" while it is actually \"more bounded\" (as said in Fig. 3's caption)\n\t- It would help comparing Fig. 2 and Fig. 3 if they both used the same y axis range\n\t- Typo: \"enforcing\" => enforces\n\nUpdate after author feedback: increasing score to \"Accept\" thanks to the release of the code", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 2}, "rylcHmbnoS": {"type": "rebuttal", "replyto": "r1etN1rtPB", "comment": "Dear Reviewers, \n\nWe finished our cleaning and documentation of the codebase before the rebuttal deadline! As such, we have revised our paper to include a link to the (anonymized) codebase, which contains extendable, modular, and commented implementations of PPO and TRPO, with precise control over all the code-level optimizations. We also provide utilities for reproducing the results of this work.\n\nThe code can be found here: https://github.com/implementation-matters/code-for-paper.\n\nWe will continue to improve the code over time to make it even easier to use, but we believe it is definitely in a good enough state (and is sufficiently important) to be added to the paper now.", "title": "Code Release"}, "BkeUPN-2oS": {"type": "rebuttal", "replyto": "SylZgZgFor", "comment": "We are glad to hear that our response addressed your concerns!\n\nSince you brought up the issue of code explicitly in your review, we wanted to point out our latest revision and general comment, where we posted code for reproducing our results, with toggles for all of the relevant code-level optimizations discussed in our work.", "title": "Re: Re: Response"}, "HJxMBiZ9sS": {"type": "rebuttal", "replyto": "HJgUz6HFor", "comment": "We have moved all the optimizations listed to the main text, added a table summarizing the different algorithms, and have written clarifications in the sections describing each new algorithm we present. Thank you for your feedback, and let us know if there is any more clarification needed!", "title": "Updated revision"}, "Byeyga6OjH": {"type": "rebuttal", "replyto": "HJeISRZAKS", "comment": "Thank you for your comments on our paper. With regards to the reviewer\u2019s main concern, we completely agree and are definitely planning to release code for this work along with the final version. We have been working on making the code more readable, modular, and easy to run, and will include a GitHub link with the final version of the paper. (We are almost done with cleaning up the codebase, if we are done before the revision deadline we will upload an anonymous copy and link it, but either way a link will appear in the final version.)\n\nWe address the other minor comments below:\n- We experimented with many different plot styles and visualization techniques for Figure 1 and converged on this version due to readability and its ability to express the relatively intricate data collected. However, we have updated the caption of Figure 1 to better describe the plot style (as it is somewhat unconventional), hopefully alleviating this concern.\n- We have updated both the captions of Figure 2 and 3 to fix the noted issues, and also added (left), (middle), and (right) to our references to Figures 2 and 3\n- We have fixed various typos/confusing wordings, including those found by the reviewer. Thank you for pointing them out!\n", "title": "Response"}, "rJeMhnp_sr": {"type": "rebuttal", "replyto": "BJeHgxi6tr", "comment": "Thank you for your review and comments on our paper. We have made sure to more precisely define code-level optimizations (algorithmic changes that are predominantly found in codebases and not presented as core parts of their respective RL algorithms, see our reply to R1 for more detail), PPO-M (PPO but without any of the code-level optimizations mentioned in Sec 2 or the Appendix), and PPO-NoClip (PPO minus the clipping, including all the optimizations). We have also copied the optimizations listed in Section 2 to the appendix, so that the appendix contains a complete list of the optimizations.\n", "title": "Response"}, "HkeJq2ausH": {"type": "rebuttal", "replyto": "H1xMvclFtr", "comment": "Thank you for your detailed review and comments, which we have taken into account in our (now uploaded) revision of the manuscript. We address each point raised in the original review below:\n\nThree random seeds: For the ablation study, we only used three random seeds for computational reasons (as every random seed requires running all of the hyperparameter configurations). However, it appears as though the reviewer is (rightfully) more concerned with Tables 1 and 2---both of these, however, actually used 10 random seeds rather than 3. We have added this to their captions to clarify this issue. We will also run a few more random seeds and update the means and variances accordingly when the results become available (we cannot guarantee this before the rebuttal deadline, however.)\n\nDiscussing variances in Table 2: We agree that a discussion of a possible variance reduction induced by clipping would improve the paper. In order to make sure that the apparent reduction in variance is not spurious we will be sure to run more random agents\u2014if the trend remains, we will certainly include a discussion of how clipping might serve to reduce the variance in PPO rather than its \u201cconventionally perceived\u201d purpose of ensuring monotonic reward increase. \n\nDiscussion of Henderson et al: We agree with the reviewer that our work builds on that of Henderson et al, and certainly did not intend to imply otherwise (hence the extensive citation noted by the reviewer). We have taken the reviewer\u2019s advice and moved the related work to Section 2, and added the suggested mention of building on Henderson et al there.\n\n\u201cCode-level optimizations\u201d: We initially chose to use the term \u201ccode-level optimization\u201d to indicate that these were algorithmic optimizations that are for the most part found only in the code of RL algorithms. However, we appreciate the reviewer\u2019s point that the term may cause confusion. To avoid this confusion and for lack of a better term, we have, in our revision, added a footnote which indicates precisely what we mean by \u201ccode-level optimization.\u201d We would be happy to amend this footnote or change the term if our fix has not alleviated the issue.\n", "title": "Response"}, "Hyx12SsVsB": {"type": "rebuttal", "replyto": "HJexEd0ZoB", "comment": "Thanks for the kind comment! We actually studied all algorithms with normalized advantage in place (in a sense we treated normalization as part of the advantage estimation process, rather than as part of the RL algorithm itself). Studying the effect of normalization would be interesting future investigation! (note that even TRPO/other RL algorithms use normalized advantage as well.)", "title": "Thanks!"}, "BJeHgxi6tr": {"type": "review", "replyto": "r1etN1rtPB", "review": "\n# Summary\nThe papers studies the effects of code-level optimization on the performance of TRPO and PPO. Details, usually considered as implementation-level particularities, are shown to be of crucial importance for the algorithms' performance.\n\n# Decision\nThe paper makes an important point, it is written clearly, and the body of evidence is convincing. Therefore, I recommend this paper for publication.\n\n# Suggestions\nMake it more clear what is meant by code-level optimizations.\n    - In Sec. 2, there is a link to Appendix A.2 for a \"full list\", but the list in A.2 does not contain all points from Sec. 2.\n    - For PPO-M, it is said \"implements only the core of the algorithm\". What exactly does that mean?\n    - PPO-NoClip is defined as \"PPO without clipping\". Does it mean that it includes all other tricks apart from clipping? Please, be explicit in such places.\n", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 3}}}