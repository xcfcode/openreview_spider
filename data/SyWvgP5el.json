{"paper": {"title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "authors": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Balaraman Ravindran", "Sergey Levine"], "authorids": ["aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "ravi@cse.iitm.ac.in", "svlevine@eecs.berkeley.edu"], "summary": "An ensemble optimization approach to help transfer neural network policies from simulated domains to real-world target domains.", "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.", "keywords": ["Reinforcement Learning", "Applications"]}, "meta": {"decision": "Accept (Poster)", "comment": "The approach here looks at learning policies that are robust over a parameterized class of MDPs (in the sense the probability that the policy doesn't perform well is small over this class. The idea is fairly straightforward, but the algorithm seems novel and the results show that the approach does seem to provide substantial benefit. The reviewers were all in agreement that the paper is worth accepting.\n \n Pros:\n + Nice application of robust (really stochastic, since these are chance constraints) optimization to policy search\n + Compelling demonstration of the improved range of good performance over methods like vanilla TRPO\n \n Cons:\n - The question of how to parameterize a class of MDPs for real-world scenarios is still somewhat unclear\n - The description of the method as optimizing CVaR seems incorrect, since they appear to be using an actual chance constraint, whereas CVaR is essentially a convex relaxation ... this may be related to the work in (Tamar, 2015), but needs to be better explained if so."}, "review": {"r1RW1DKEe": {"type": "rebuttal", "replyto": "r1tEPyWEg", "comment": "Thank you for reviewing our paper and for the valuable feedback.\n\nFollowing your suggestions, we have added details about the need for a baseline (appendix A.6) and results when using the REINFORCE algorithm for BatchPolOpt step (appendix A.7). Our experiments suggest that a baseline is important for policy optimization to work in practice, especially for the CVaR case.", "title": "incorporated your suggestions"}, "Hk6fFxA7g": {"type": "rebuttal", "replyto": "SJKBzagXe", "comment": "Thank you for reviewing our paper and for the useful pointer.\n\n\nQ1: We have added a citation to Wang et al. in the paper and briefly discussed the connections. We elaborate on the differences below:\n-- Wang et al. use a hand-engineered policy class similar to SIMBICON and optimize a small set of parameters using CMA-ES. EPOpt is a general policy gradient meta-algorithm for directly optimizing expressive policies.\n-- Wang et al. observed cautious policies with degraded performance, whereas we do not observe degradation in performance due to acquiring a robust policy. This is likely due to our use of a more expressive function approximator to represent the policy. We believe that this is an interesting empirical result that will be of interest to the deep RL community.\n-- Our end goal is to transfer to an unknown MDP whereas Wang et al. seek to optimize average performance on the ensemble. \n-- We show effectiveness of model adaptation to enable transfer in the absence of domain knowledge to pick informative source distributions. Wang et al. do not study adaptation.\n-- Our experiments show that the sub-sampling step (epsilon<1) we employ produces significantly more robust policies.\n-- We analyze transfer with unmodeled effects, which is important for transferring policies from simulations to real world.\n\n\nQ2: We have added results for different epsilon settings to the paper (appendix A.5). We have also added details about optimizing policies when using epsilon<1 to Section 3.1. There wasn\u2019t a marked difference between different epsilon settings when only one physical parameter was varied (Section 4.1). However, the difference was clearly visible when multiple parameters were varied (Section 4.2). We observed that as epsilon is decreased, the variance in performance decreases, with a small decrease in average performance.", "title": "added results for different epsilon settings and discussed relationship to Wang et al. '10"}, "ry7sDeAme": {"type": "rebuttal", "replyto": "SyHzcqJ7l", "comment": "Thank you for reviewing our paper and for the interesting questions!\n\nWe use the same parametrization for the baseline as used in Duan et al. \u201816 and we have edited Section 3.1 to emphasize this. The parameters of the baseline are estimated using only the subset of trajectories with returns less than epsilon percentile of the return distribution.\n\nBatchPolOpt simply calls one step of an underlying policy gradient subroutine, which in our case was TRPO. We have edited Section 3.1 to clarify this. We emphasize that our method is not specific to TRPO and can be used with any batch policy optimization subroutine. We chose TRPO primarily because there are readily available open-source implementations, and the algorithm has been shown to work on a range of problems. The particular choice of policy gradient method is largely orthogonal to the main contribution of this paper.\nFollowing your comments, we ran experiments using REINFORCE with the same baseline parametrization as before. Our results indicate that REINFORCE is unable to produce robust policies, and has difficulty in finding a hopping gait even for a single MDP (i.e. without an ensemble). This mirrors the findings in Duan et al \u201816.\n\nWe have added a discussion of sample complexity to appendix A.2. The results in Figure 1 used 150 iterations with 240 trajectory samples per iteration, which is comparable to Schulman et al. \u201815 and Duan et al. \u201816. We tried a few different settings for epsilon, and finally chose 0.1 since it produced satisfactory results both qualitatively and quantitatively. We have summarized results for different epsilon settings in appendix A.5.\n\nThough Mordatch et al. \u201815 also use an ensemble of models, they do not train parametrized policies. Instead, they optimize individual open loop trajectories in simulation and use PD control to track those trajectories. While this is related to our approach in that ensembles are used, it is not a policy search method, and therefore cannot be directly compared to our method in regard to its ability to find robust policies.\nOur understanding is that Bayesian GP-TD is primarily used for policy evaluation [4,5], and hence is not directly connected to the main contribution of our paper. To our knowledge, GP based methods have not been applied successfully to high dimensional continuous locomotion tasks like those we study in this paper. \nWe note here that we show comparisons to reasonable alternatives like training a policy on a single maximum likelihood model (which is the most common practice in model-based control), and training a policy which optimizes for the average return over a model distribution (EPOpt with epsilon=1).\n\nEPOpt considers a distribution over models to learn the robust policy directly. Boosting on the other hand combines a set of weak learners (e.g instance level policies) to produce a strong learner (robust policy).\n\n[1] Duan et al., Benchmarking Deep Reinforcement Learning for Continuous Control, ICML 2016.\n[2] Schulman et al., Trust Region Policy Optimization, ICML 2015.\n[3] Mordatch et al., Ensemble-CIO: Full-body dynamic motion planning that transfers to physical humanoids, IROS 2015.\n[4] Engel et al., Reinforcement learning with Gaussian processes, ICML 2005.\n[5] Engel et al., GPTD Tutorial slides, ICML 2007.", "title": "answers"}, "SJKBzagXe": {"type": "review", "replyto": "SyWvgP5el", "review": "1. The EPOpt algorithm in some abstract sense reminds me of:\n\nWang, J. M., Fleet, D. J., Hertzmann, A. Optimizing Walking Controllers for Uncertain Inputs and Environments. ACM Transactions on Graphics 29, 4 (Proceedings of SIGGRAPH 2010). \n\nCan you comment on the relationship? \n\n2. It would be interesting to see how the illustrated performance (particularly in Figure 1, right) changes as a function of \\epsilon, e.g., as epsilon is increased (or decreased) from 0.1 default. Has this been tried? Can the differences in performance be characterized?\n\n \n\n\nPaper addresses systematic discrepancies between simulated and real-world policy control domains. Proposed method contains two ideas: 1) training on an ensemble of models in an adversarial fashion to learn policies that are robust to errors and 2) adaptation of the source domain ensemble using data from a (real-world) target domain. \n\n> Significance\n\nPaper addresses and important and significant problem. The approach taken in addressing it is also interesting \n\n> Clarity\n\nPaper is well written, but does require domain knowledge to understand. \n\nMy main concerns were well addressed by the rebuttal and corresponding revisions to the paper. ", "title": "Pre-review questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJwiMAWVe": {"type": "review", "replyto": "SyWvgP5el", "review": "1. The EPOpt algorithm in some abstract sense reminds me of:\n\nWang, J. M., Fleet, D. J., Hertzmann, A. Optimizing Walking Controllers for Uncertain Inputs and Environments. ACM Transactions on Graphics 29, 4 (Proceedings of SIGGRAPH 2010). \n\nCan you comment on the relationship? \n\n2. It would be interesting to see how the illustrated performance (particularly in Figure 1, right) changes as a function of \\epsilon, e.g., as epsilon is increased (or decreased) from 0.1 default. Has this been tried? Can the differences in performance be characterized?\n\n \n\n\nPaper addresses systematic discrepancies between simulated and real-world policy control domains. Proposed method contains two ideas: 1) training on an ensemble of models in an adversarial fashion to learn policies that are robust to errors and 2) adaptation of the source domain ensemble using data from a (real-world) target domain. \n\n> Significance\n\nPaper addresses and important and significant problem. The approach taken in addressing it is also interesting \n\n> Clarity\n\nPaper is well written, but does require domain knowledge to understand. \n\nMy main concerns were well addressed by the rebuttal and corresponding revisions to the paper. ", "title": "Pre-review questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}