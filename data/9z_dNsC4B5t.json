{"paper": {"title": "MetaNorm: Learning to Normalize Few-Shot Batches Across Domains", "authors": ["Yingjun Du", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "authorids": ["~Yingjun_Du1", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "summary": "We propose MetaNorm, a simple yet effective meta-learning normalization approach that learns adaptive statistics for few-shot classification and domain generalization", "abstract": "Batch normalization plays a crucial role when training deep neural networks. However, batch statistics become unstable with small batch sizes and are unreliable in the presence of distribution shifts. We propose MetaNorm, a simple yet effective meta-learning normalization. It tackles the aforementioned issues in a unified way by leveraging the meta-learning setting and learns to infer adaptive statistics for batch normalization. MetaNorm is generic, flexible and model-agnostic, making it a simple plug-and-play module that is seamlessly embedded into existing meta-learning approaches. It can be efficiently implemented by lightweight hypernetworks with low computational cost. We verify its effectiveness by extensive evaluation on representative tasks suffering from the small batch and domain shift problems: few-shot learning and domain generalization. We further introduce an even more challenging setting: few-shot domain generalization. Results demonstrate that MetaNorm consistently achieves better, or at least competitive, accuracy compared to existing batch normalization methods.  ", "keywords": ["Meta-learning", "batch normalization", "few-shot domain generalization"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes an lightweight method for cross-domain few-shot learning, using a meta-learning approach to predict batch normalization statistics.\nAfter the extensive paper revisions and discussion, the reviewers all agreed that this paper is above the bar for acceptance, assuming that the authors will include results for both the standard and expanded target set size in the final version of the paper. The authors are strongly encouraged to include these results in the camera-ready version of the paper."}, "review": {"-zo3h8VN94F": {"type": "rebuttal", "replyto": "eKU4UBJ2w2", "comment": "Thank you. We will include results for standard and expanded target set sizes, as suggested, in the final version.", "title": "Fourth response to AnonReviewer2"}, "lhjbbxa5DgP": {"type": "rebuttal", "replyto": "nBP-wloKY-W", "comment": "**Small things:**\n\n* Thank you. When taking the error bar into account in Table 3, other methods become more competitive and MetaNorm maintains the best average rank with the highest accuracy on eight of the thirteen datasets. We have adapted the text describing the experiment accordingly.\n\n*  We clarify that we reproduce the PyTorch code provided by CNAPS with TensorFlow for Meta-Dataset. All our (re-)implemented TensorFlow code will be released.\n\n**The larger issue:**\n\n* We thank *AnonReviewer2* for sharing the primary reservation. We respectfully disagree on MetaNorm requiring an unreasonable large target set. Table 18 shows MetaNorm performs well with the standard query set size of 75 (15 per category). It is slightly better than TaskNorm and comparable with TBN. MetaNorm achieves its best performance with a query size of 125 (25 per category), only slightly larger than the standard size of 75. We do not consider this increase unreasonable, nor impractical. We do agree it deserves more attention and we have added a discussion in the subsection \"Impact of Target Set Size\", including your compelling suggestion to leverage image synthesis techniques.\n\n*  We believe our Meta-Dataset comparison in Table 16 is 'apples to apples', as different methods are evaluated on similar conditions. We have also conducted the experiments on miniImageNet with the standard query set size using MAML and VERSA. The results are shown in Table 18 of the appendix. MetaNorm achieves comparable performance to non-transductive approaches, e.g., TaskNorm, under similar conditions, and the performance is very close to that of TBN. We conclude the performance gains are predominantly from the proposed meta-learning batch normalization mechanism, rather than simply a larger query set. We would like to mention that TBN *always* needs to use the entire query set, even during the meta-test stage, while MetaNorm does not rely on the query set anymore at meta-test. A fact that we believe to be a considerable advantage.\n\n We thank the reviewer.", "title": "Third response to AnonReviewer2 "}, "cGmSGQiy050": {"type": "rebuttal", "replyto": "i8ecbg65vjJ", "comment": "We are indebted to *AnonReviewer2*  for pushing the quality and precision of our manuscript forward. \n\n* The reviewer is right. We have taken the error bar into account in Table 2, 13, 14 and 15, re-bolded the data in the tables, and explained accordingly. We also softened our conclusion in Table 2: ``MetaNorm is a consistent top-performer, regardless of the meta-learning algorithm.'' And in the corresponding paragraph: \"MetaNorm achieves comparable performance to transductive batch normalization, especially under the 5-way-1-shot setting, which is challenging since only a few examples are available to generate statistics.\" \n\n* Our MetaNorm only increases the query size during meta-training, allowing our model to learn the ability to infer the statistics of the test task using only a few samples. In the meta-test stage, we use the query size consistent with the previous methods, which makes the comparison fair to our opinion.  We observe from Figure 2 that TBN does not seem to be affected by the target size, under both the 5-way, 1-shot, and 5 way, 5-shot settings. Due to the time limit, we are unable to complete all experiments with non-transductive methods at the same large query size during the rebuttal phase, we certainly would like to add these experiments in the final version. We ask for your understanding and trust.\n\n* We have added the implementation details of Meta-Dataset in appendix D.2. We use the same large target size for our MetaNorm Meta-Dataset experiments. \n\n* Indeed, we evaluate 200 validation tasks, 600 test tasks following exactly the same setting of [VERSA](https://github.com/Gordonjo/versa). We apologize for this mistake and we have fixed it. \n\n*  The results of VERSA with TBN are reproduced by us using the author-provided code. \nCompared with VERSA, we have the extra KL-term in our final loss, which needs more iterations to converge. For the comparison, we also have increased the iterations for VERSA. We found the performance of VERSA does not increase after convergence, even with more iterations. We believe a comparison based on the convergence, instead of the same number of iterations, to be fair as well.\n\n* We will add training convergence comparisons on Omniglot and Meta-Dataset in the final version. We clarify that the other methods in Figure 4 use the query size from their original implementations ([TaskNorm](https://github.com/cambridge-mlg/cnaps) and [ProtoNets](https://github.com/abdulfatir/prototypical-networks-tensorflow)).\n\n* An error bar was dropped while copying, we have fixed it.  Thank you. \n\nAll identified typos have been fixed. We did another pass over the manuscript leading to a few more repairs. Thanks again. \n", "title": "Second response to AnonReviewer2 "}, "QzDQ0ISSZZ9": {"type": "rebuttal", "replyto": "IVYcb3BAOWR", "comment": "We thank *AnonReviewer4* for constructive comments and clear guidance. \n\n**Regarding presentation, too many implementation details and descriptions of the experiments are missing from the main paper (and in certain cases, missing altogether). Regarding datasets and implementation, it is ok to provide non essential details in the appendix (especially considering the large number of datasets considered). However, no information is provided at all in the main paper, which makes it very difficult to understand the setting of the experiments.**\n\nWe regret our experimental description has confused you, and hope it is still mendable with the extra page. We have expanded the experimental details in the main paper and in the appendix, which we detail per item below.\n\n**For example, in Table 1 and Figure 2, it is not known on which datasets experiments are run, and it is never mentioned in the main text (for table 1) what FSL method is employed.**\n\nIn Table 1 and the subsection on *Effect of KL Term* we clarify our method runs on miniImageNet for few-shot learning and PACS for domain generalization. In Figure 2 we report on the same two sets. We have updated the caption and the subsection on *Impact of Target Set Size*. Thank you.\n\n**In addition, Table 2 experiments comparing MetaNorm to different approaches sorely lacks description. The approach is compared to 9 different algorithms, none of which are given a description or reference to learn more about the method.**\n\nThe reviewer is right, we apologize. We have added the reference of each algorithm. In the subsection on *Sensitivity to Domains* we stated the baseline as \"The baseline normalization uses the statistics from the source domains for the batch normalization of the target domain\".\n\n**Finally, experiments in Figure 2 would be a lot more interesting if compared to standard methods. It would be interesting to see how the proposed strategy allows to be more sample efficient and reach stronger performance than transductive batch norm in situations where sample size is the smallest. As this is one of the cited main limitations of TBN, this experiment is highly important.**\n\nWe have added the TBN by [Bronskill et al. (2020)](https://arxiv.org/pdf/2003.03284.pdf) in Figure 2 and add to the discussion in  *Impact of Target Set Size*: \"The experimental results show that TBN has limited effect on the target size both on the 5-way, 1-shot and 5 way, 5-shot tasks\". To avoid clutter, we only show the results of TBN with VERSA. The same trend holds also for MAML and ProtoNets. \n\n**With regards to related work, I would suggest to move the section after the introduction, where it provides much better context to facilitate method comprehension, in particular regarding the description of the TBN strategy.**\n\n We moved the related work section after the introduction.\n\n**Authors should also comment on how their work, and in particular FSL domain generalisation setting, relates to cross domain few-shot learning works, e.g. Tseng et al ICLR 2020, Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation It is currently presented as a completely new approach to FSL, and appears to ignore past cross domain FSL works. Please provide additional context regarding such works.**\n\nWe add in Section  *MetaNorm for Few-Shot Domain Generalization*: \"Others have considered the related task of cross-domain few-shot learning, e.g. [Tseng et al. (2020)](https://arxiv.org/abs/2001.08735) and [Guo et al. (2020)](https://arxiv.org/abs/1912.07200). Different from their settings, our few-shot domain generalization is more challenging as the support and query set are from *different* domains in the meta-test stage and the target domain is also unseen throughout the training stage\".\n\n**RECOMMENDATION**\n\n1. We have reorganized the section *Experiments Results*. \n\n2. We moved the related work section after the introduction.\n\n3. We have provided a discussion about cross-domain few-shot learning in the section of *related work*. \n\n4. We have provided the suggested comparison to standard TBN in Figure 2.\n\n**Additional suggestions**\n\n5. We have added an overview figure of the proposed method in Figure 3 of the appendix.\n\n6. There is no consistent way to sort the methods because they are implemented under three different models. We have corrected the bolding in Table 2 and Table 14. \n\n7. We have provided a list of contributions at the end of the introduction.\n\n8. We have added some sentences about KL divergence and hypernetworks in our methodology.\n\n9. We have updated the related work section to include a discussion on conditional batch normalization.", "title": "Response to AnonReviewer4"}, "RAh_ugPFHcX": {"type": "rebuttal", "replyto": "0HKa-K3gSo9", "comment": "We thank *AnonReviewer2* for the thorough review and detailed constructive comments.\n\n**(1) There is no mention of the MetaNorm training efficiency (which is generally understood to be the primary goal of batch normalization, see [1,2]). How does training efficiency of MetaNorm compare to other methods?**\n\nThe reviewer is right. We plot the training loss versus training iterations with the ProtoNets algorithm (the codebase we use here is based on [Fatir's](https://github.com/abdulfatir/prototypical-networks-tensorflow) re-implementation.) as shown in Figure 3 in the appendix. MetaNorm achieves the fastest training convergence, demonstrating the benefit of leveraging meta-learning by MetaNorm for batch normalization. We have added the \"Training Loss versus iteration\" in the appendix. Thank you.\n\n**(2) Reproducibility: The paper is currently missing details that are required to reproduce the results and ensure fair comparison with other methods including:**\n\n* We clarify our codebases in the appendix. For MAML we use the code provided by [Finn](https://github.com/cbfinn/maml). For ProtoNets, we use the code provided by [Fatir](https://github.com/abdulfatir/prototypical-networks-tensorflow). For VERSA,  we use the code provided by [Gordon](https://github.com/Gordonjo/versa). For TaskNorm,  we reproduce it ourselves based on the above three models and [TaskNorm](https://github.com/cambridge-mlg/cnaps). All models are implemented with TensorFlow. Our code will be publicly released.\n    \n* Different models have different training settings. The details of the experimental setting have been added in the appendix as well. \n\n* For a fair comparison, we always choose 125 query samples for the few-shot scenario and 128 target samples for the domain generalization. \n\n**(3) There are many omissions in the paper:**\n\nThank you for your detailed guidance. Much appreciated.\n\n* We have added after equation 3 that \"m\" is a random variable that represents the distribution of activation. \n\n* We have rewritten the KL expressions.\n\n* We have added the total loss in the revised version of the paper. The weight of the KL term of the different models is different. We have explained them in the appendix of the experimental setting. The references and descriptions of the various methods have been added in the revised version.\n\n* Indeed, the $\\pm$ indicates the 95\\% con\ufb01dence interval over tasks in the few-shot learning. We have added this description in the updated paper.\n\n* We make bold according to the highest value without considering the following error bar. We have added this description in the revised version to avoid confusion. It is worth noting that, we take the error bar into account when computing the average ranking.\n\n* We choose the units of the hidden layers by cross-validation. We implemented the hypernetworks with $64$, $128$, $256$, $512$ units. We found $128$ can get the best performance. We have added this experiment in the appendix. We use elu as non-linearities, and the output layer does not use any non-linearities. We also have added the structure of inference networks in the appendix.\n\n* Yes, we did an ablation study on various configurations. We found that shared inference functions are better than those not shared across channels. This would be because the way of sharing parameters by different channels can reduce network parameters and increase computational efficiency. We have added this in the appendix.\n\n**(4) There are factual errors in the paper:**\n\n * We have corrected it. Thank you.\n\n*  We have modified this sentence as \"its performance is not always performing better than transductive batch normalization\".\n\n**(5) There are several missing attributions and some exact text taken from other papers that is not quoted or cited:**\n\nWe apologize for hasty and imprecise writing. We fixed it in the revised version.\n\n* We have clarified what numbers in Tables 2, 11, 13, and 14 are provided by [Bronskill et al. (2020)](https://arxiv.org/pdf/2003.03284.pdf) and which ones have been computed by us. \n\n* MetaNorm is based on the different codebases of each model and uses different hyper-parameters. \n\n* The indicated sentences have been re-phrased in the updated version. Thank you.\n\n**(6)  In Figure 2, the title says \"Impact of target set size.\". However, for the left and middle plot, the horizontal axis is labeled as |S|, which is the support set size. Please resolve the ambiguity. Minor Comments:**\n\nAll repaired. Thank you.\n\n\n\n\n\n", "title": "Response to AnonReviewer2"}, "YKJFfp8E4mN": {"type": "rebuttal", "replyto": "9z_dNsC4B5t", "comment": "**We thank all AnonReviewers for their insightful reviews, sharp comments and supportive suggestions. Here, we provide a summary of the updates made in the new version, as suggested by the reviewers.**\n\n## Main manuscript\nThe following updates have been incorporated by using the extra page for the main manuscript:\n* We have provided an explicit list of contributions at the end of the introduction.\n* We have expanded related work, by adding references and discussion related to *batch normalization for domain adaptation and domain generalization*, and a new subsection about *conditional batch normalization*. We also move the related work after the introduction. \n* We have added our intuition why MetaNorm generates proper statistics for new domains, and we provide a new comparison for domain generalization using ground truth statistics in the subsections *MetaNorm for Domain Generalization*, *Sensitivity to Domains*, and Table 4.\n* We have expanded the experiment with varying query size in Figure 2 with standard TBN.\n* We have added a detailed description and reference of each algorithm in *Experimental Results*.\n * We have clarified what experimental results in Table 2 are provided by [Bronskill et al. (2020)](https://arxiv.org/pdf/2003.03284.pdf), and which ones are computed by us.\n* We have added a discussion about cross-domain few-shot learning in *MetaNorm for Few-Shot Domain Generalization*.\n* We have clarified in the *Methodology*, the KL divergence and hypernetworks, the total loss,  the description of $m$ in equation 3, and the parameters of the KL expressions in equations 4, 8, 10.\n\n## Appendix\nThe following updates have been inserted in the Appendix:\n* We have provided an overview figure of the implementation of our MetaNorm for few-shot learning in Figure 3. \n* We have added the details and codebases about the experimental setting for MAML, ProtoNets, and VERSA in Appendix D.\n* We have added the detailed descriptions about the structure of $f_{\\mu}^l(\\cdot)$ and $f_{\\mu}^l(\\cdot)$ in Table 9, 10.\n * We have clarified what experimental results in Tables 14, 15, and 16 are provided by [Bronskill et al. (2020)](https://arxiv.org/pdf/2003.03284.pdf), and which ones are computed by us.\n*  We have added the convergence analysis and plot the training loss versus iterations for the ProtoNets algorithm in Appendix H and Figure 4.\n* We have implemented experiments on the effect of different numbers of units in the hidden layer on performance and added this experiment in Table 17. \n\n\n", "title": "Summary of Changes"}, "9GfXpR3vIv5": {"type": "rebuttal", "replyto": "KqMgD1x2Y9", "comment": "We thank *AnonReviewer1*  for the insightful comments and the acknowledgment of the originality and significance of this work.\n\n**Perhaps missing some additional details that would help the reader better understand why and how the method works. [...]\nFirst, why do we expect an amortized procedure to produce reasonable normalization statistics when tested out of distribution, e.g., on a new domain? It would be interesting to report, during testing.\nSecond, a similar question can be asked about the fully \"in distribution\" setting, i.e., where batch normalization was invented. If one were to apply the proposed method in a standard supervised learning setting (e.g., minimize KL between inferred statistics on each training point and the ground truth statistics computed on the training batch), would the inferred test time statistics come close to either the average statistics computed through training or perhaps even the ground truth test statistics? Is it any worse than standard batch norm? Could it perhaps even be better?**\n\nThank you for sharing the insight. We assume we can generate reasonable normalization statistics by using only one sample from the new domain, because, intuitively, a single sample already carries much of its domain information. This ability is indeed acquired by meta-learning. We have done the suggested experiment using standard batch normalization. In the training stage, we compute the ground truth statistics using all the test data on the meta-test domain $\\mathcal{D}^t$ instead of using the inferred statistics $p(m|\\mathcal{D}^{s}\\backslash \\mathbf{a}_i)$. Results in Table 4 reveal MetaNorm is still better on most domains and on average. This is reasonable because ground truth statistics from the test data do not necessarily reflect the true data distribution. The experimental results demonstrate MetaNorm can generate reasonable normalization statistics from only one sample in its domain. We have added the discussion to the *Sensitivity to Domains* subsection. \n\n**Most of my concerns about clarity, which are small, relate to the experiments suggested above, i.e., better exposition of how and why the method works. Providing intuition throughout the paper regarding this point would strengthen the paper.**\n\nWe have added the intuition about \"how and why the method works\", and provided a new comparison for domain generalization by using ground truth statistics in the revised version of the paper.\n\n**There also seem to be a few instances of \"meta\" vs \"non meta\" terms being misused, e.g., \"training\" vs \"metatraining\" when describing domain generalization.**\n\nThe description of domain generalization has been fixed.\n\n**As far as I am aware, this work presents a novel method, though I am not an expert regarding the relevant prior work. The citations to batch normalization as used in domain adaptation seem appropriate. A few other papers in a similar vein perhaps should also be cited.**\n\nWe have expanded the related work section, and discuss the suggested references in *related work*. Thank you. \n\n\n\n\n\n\n", "title": "Response to AnonReviewer1"}, "qwqe7X0ZMjx": {"type": "rebuttal", "replyto": "oacCnNvvbE9", "comment": "We thank *AnonReviewer3* for the honest assessment.\n\n**The implementation of the hypernetworks is effectively MLP with one hidden layers of 128 units. Has the author conduct ablation on hypernetwork on various configuration?**\n\nIndeed, we implemented the hypernetworks with $64$, $128$, $256$, $512$ units. We choose the units of the hidden layers by cross-validation. We found $128$ obtains the best performance. We have added this ablation in the appendix. Thank you.\n\n", "title": "Response to AnonReviewer3"}, "0HKa-K3gSo9": {"type": "review", "replyto": "9z_dNsC4B5t", "review": "This paper describes a new method for normalizing few-shot learning episodes. The authors point out that the statistics of an episode are unreliable when the size of the episode is small or when the data distribution changes from episode to episode. To remedy this, the authors propose a method called \u2018MetaNorm\u2019 which uses a meta-learning approach to infer the means and variances to be used in the batch normalization layers that are employed in the feature extractor component. In particular, they meta-learn the parameters for a set of hypernetworks in an amortized fashion that learn to generate the means and variances of the batch normalization layers conditioned on the contents of the episode. The paper focuses entirely on the few-shot image classification scenario where MetaNorm is evaluated in various settings including standard few-shot classification and domain generalization (including a novel few-shot domain generalization setting).\n\n**Pros:**\nFundamentally, the concept behind MetaNorm is innovative and promising. The idea of using meta-learned hypernetworks to generate the means and variances of the normalization layers is good. The paper also makes a good case for the effectiveness of a KL term that is added to the classifer loss function that helps the hypernetworks learn a good set of parameters. The range of experiments is sufficient.\n\n**Cons:**\nThe paper is poorly executed and missing too much information to ascertain that the method outperforms competitive approaches.\n\nSpecific concerns:  \n(1) There is no mention of the MetaNorm training efficiency (which is generally understood to be the primary goal of batch normalization, see [1,2]). How does training efficiency of MetaNorm compare to other methods? And if training efficiency is not a goal, MetaNorm should be compared to methods whose goal is to adapt a classification system to a variety of data distributions (e.g. [3,4,5,6, etc.]).\n\n(2) Reproducibility: The paper is currently missing details that are required to reproduce the results and ensure fair comparison with other methods including:\n- Which existing codebases were used (if any)?\n- What hyper-parameters and training settings are used? (e.g. learning rates, number of training iterations, the use of early stopping, the number of test episodes, weight decay, etc.)\n- In Section 4, \u201cImpact of Target Set Size\u201d indicates that the number of samples in the query set is a key parameter for MetaNorm and that 125 for the few-shot scenario and 128 for the domain generalization scenario are the best values to use. Are these the values used in the experiments? If not, what values are used?\n\n(3) There are many omissions in the paper:\n- The variable \u2018m\u2019 as used in equations 3, 4, 8, 9, 10 is not defined.\n- The KL expressions in equations 4, 8, and 10 should be written as a proper minimization expression where the parameters that are minimized are explicitly stated.\n- The total loss function for MetaNorm is never stated. One could assume that it is a sum between the regular loss function for the particular few shot learning problem and the KL term, but it should be explicitly stated. Also, is there a weight on the KL term in the loss function? If so, how is the optimal weight determined?\n- Various methods in the tables are not defined or referenced including RN, TaskNorm-I, TaskNorm-L, \u2018class\u2019, and \u2018example\u2019.\n- How are the confidence intervals on the results in the table calculated? Are they 95% confidence intervals, or something else?\n- In the tables, how do you decide which entry to \u2018bold\u2019 given the confidence intervals? In table 2, TBN (and sometimes TaskNorm) are often within error bars of MetaNorm but are not bolded. Same for tables 11 and 13, 14.\n- It would be good to have more information on the hypernetworks that generate the normalization parameters. In Section 2, it says: \u201cThey are three layer networks with one hidden layer of 128 units and the input size depends on the feature dimensions of the convolutional layers.\u201d How big is the other hidden layer and output size of the network? What sort of non-linearities are used, etc.?\n- In section 2 it says: \u201cNote that the inference functions $f_\\mu^l()$ and $f_\\sigma^l()$ are shared by different channels in the same layer and we will learn L pairs of those functions if we have L convolutional layers in the meta-learning model\u201d. What is the justification for this parameterization (say versus having unique functions per channel)? Was an ablation study that compares various configurations done?\n\n(4) There are factual errors in the paper:\n- In Section 1 and Section 3, the authors state that Prototypical networks uses transductive batch normalization. This is not the case (refer to https://github.com/jakesnell/prototypical-networks for the version of Prototypical networks authored by Jake Snell). Please correct.\n- In section 1 it states that TaskNorm \u201cremains under-performing compared to transductive batch normalization\u201d. While the results listed in Table 2 support this statement, the results in Table 3 refute this statement. Please clarify.\n\n(5) There are several missing attributions and some exact text taken from other papers that is not quoted or cited: \n- In Table 2, the competitive results for MAML and ProtoNets seem to be extracted from [2], yet these are not attributed. Did you reproduce them? Were the MetaNorm numbers generated with a similar code base and hyper-parameters?\n- In Tables 11, 13, and 14, the competitive results also seem to be extracted from [2] and were not attributed. Were they reproduced, or just stated? Were the MetaNorm numbers generated with a similar code base and hyper-parameters?\n- The following sentences seem to be directly copied from [2]. If so, there should be quotation marks and attribution:\n\u201cBatch normalization relies on the implicit assumption that the dataset comprises i.i.d. samples from some underlying distribution.\u201d\n\u201cThe challenge constructs few-shot learning tasks by drawing from the following distribution. First, one of the datasets is sampled uniformly; second, the \u201cway\u201d and \u201cshot\u201d are sampled randomly according to a fixed procedure; third, the classes and context / target instances are sampled.\u201d\n\u201cIn the meta-test phase, the identity of the original dataset is not revealed and the tasks must be treated independently (i.e. no information can be transferred between them). The meta-training set comprises a disjoint and dissimilar set of classes from those used for meta-test.\u201d\n\n(6) In Figure 2, the title says \u201cImpact of target set size.\u201d. However, for the left and middle plot, the horizontal axis is labeled as |S|, which is the support set size. Please resolve the ambiguity.\nMinor Comments:\n- In section 2, MetaNorm for Few-Shot Classification: The sentence \u201cThe $p(m|Q)$ can be estimated by directly calculating statistics using the query set, which however performs inferior to inference by optimization.\u201d is grammatically incorrect.\n- In section 2, MetaNorm for Few-Shot Classification: should \u201cmultiple layer perception networks\u201d be \u201cmulti-layer perceptron networks\u201d? Similarly, in section 2 it says: \u201c\u2026which are realized as multi-layer perceptions and we call hypernetworks.\u201d \u2018perceptions\u2019 should be \u2018perceptrons\u2019.\n- Sometimes the nomenclature support set / query set is used, and other places it refers to the same as context set / target set. Please use consistent nomenclature.\n- In section 2, it says \u201c\u2026which are realized as multi-layer perceptions and we call hypernetworks\u201d. And later in section 2, it says: \u201cThey are parameterized by feed-forward multiple layer perception networks, which we call hypernetworks\u201d.  Note that hypernetworks are an established concept, you should cite [7].\n- Section 1 should clarify in the batch normalization exposition that the methods described in the paper apply only to normalizing 2D convolutional layers (i.e. does not apply to fully connected layers as they do not have feature maps).\n\n**References:**  \n[1] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448\u2013456, 2015.  \n[2] John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard Turner. Tasknorm: Rethinking batch normalization for meta-learning. In International Conference on Machine Learning. 2020.  \n[3] Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. \"Learning multiple visual domains with residual adapters.\" Advances in Neural Information Processing Systems. 2017.  \n[4] Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. \"Efficient parametrization of multi-domain deep neural networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.  \n[5] Requeima, James, et al. \"Fast and flexible multi-task classification using conditional neural adaptive processes.\" Advances in Neural Information Processing Systems. 2019.  \n[6] Tseng, Hung-Yu, et al. \"Cross-domain few-shot classification via learned feature-wise transformation.\" arXiv preprint arXiv:2001.08735 (2020).  \n[7] Ha, David, Andrew Dai, and Quoc V. Le. \"Hypernetworks.\" arXiv preprint arXiv:1609.09106 (2016).\n", "title": "Review of \"MetaNorm: Learning to Normalize Few-Shot Batches Across Domains\"", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "KqMgD1x2Y9": {"type": "review", "replyto": "9z_dNsC4B5t", "review": "This paper proposes to replace batch normalization statistics, which are typically computed as the batch moments during training or a fixed training average during testing, with the outputs of learned neural networks. These networks are trained to minimize the KL divergence between their output and the expected or desired batch statistics. In this way, the statistics computation is amortized and can hopefully generalize in the face of small batches and distribution shift.\n\nPros:\n\n+ The paper is generally well written and easy to follow.\n+ There are extensive experiments demonstrating the empirical effectiveness of the proposed approach.\n\nCons:\n\n- Perhaps missing some additional details that would help the reader better understand why and how the method works.\n\nCurrently, I recommend acceptance because I believe the pros rather handily outweigh the cons. I provide more details below.\n\n\nQuality\n---\n\nThe paper is well written and the experiments are thorough and well executed. Related to the con listed above, a few additional experiments could be useful for gaining a deeper understanding of the method.\n\nFirst, why do we expect an amortized procedure to produce reasonable normalization statistics when tested \"out of distribution\", e.g., on a new domain? It would be interesting to report, during testing, are the inferred statistics actually close to the ground truth statistics on the test domain? Or otherwise, are they interpretable in some other fashion? Any other insight that the authors could supply regarding this general question would also be appreciated.\n\nSecond, a similar question can be asked about the fully \"in distribution\" setting, i.e., where batch normalization was invented. If one were to apply the proposed method in a standard supervised learning setting (e.g., minimize KL between inferred statistics on each training point and the ground truth statistics computed on the training batch), would the inferred test time statistics come close to either the average statistics computed through training or perhaps even the ground truth test statistics? Is it any worse than standard batch norm? Could it perhaps even be better?\n\nClarity\n---\n\nMost of my concerns about clarity, which are small, relate to the experiments suggested above, i.e., better exposition of how and why the method works. Providing intuition throughout the paper regarding this point would strengthen the paper.\n\nThe few shot domain generalization setting is not too difficult to understand if the reader is familiar with both the few shot learning and domain generalization settings individually -- it is probably difficult to parse otherwise, but that is perhaps hard to rectify given the space constraints.\n\nThere also seem to be a few instances of \"meta\" vs \"non meta\" terms being misused, e.g., \"training\" vs \"meta-training\" when describing domain generalization.\n\nOriginality\n---\n\nAs far as I am aware, this work presents a novel method, though I am not an expert regarding the relevant prior work. The citations to batch normalization as used in domain adaptation seem appropriate. A few other papers in a similar vein perhaps should also be cited:\n\nhttps://arxiv.org/abs/1603.04779\nhttps://arxiv.org/abs/2002.04019\nhttps://arxiv.org/abs/2006.10963\nhttps://arxiv.org/abs/2006.16971\n\nSo although this general line of work seems to be attracting a decent amount of attention, this work is still novel in that it amortizes the inference process and presents extensive empirical results.\n\nSignificance\n---\n\nThis work seems significant to researchers interested in meta-learning, domain generalization, and problems involving distribution shift in general. The proposed method is also relatively simple, allowing for easier adoption and further testing.", "title": "Official Blind Review #1", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "oacCnNvvbE9": {"type": "review", "replyto": "9z_dNsC4B5t", "review": "Summary: The paper proposes\u00a0an effective meta-learning normalization, named MetaNorm, to infer adaptive statistics for batch normalization by minimizing the KL divergence. The module is lightweight. The proposed method is evaluated on few-shot learning, domain generalization, and few-shot domain generalization. \n\nJustification of rating:\u00a0 Overall, the proposed approach is logical and sound. The formulation and methodology seem to be correct. As I am not personally working on the specific topics, I might not be able to discover major issues in this work.\n\nStrengths:\n+ The proposed MetaNorm leverage meta-learning approach with KL divergence for learning to learn normalization statistics from data. \n+ The proposed approach is model agnostic and can be easily embedded into meta-learning approaches.\n+ Evaluation demonstrates it is able to learn with few examples, as well as handle variations in domain for domain generalization problem. The experimental results show consistent improvement over compared baselines (Table 2-5)\n+ The paper provide sufficient information for researcher to reproduce the results. Code will be released in the future.\n\nMinor comments:\n- The implementation of the hypernetworks is effectively MLP with one hidden layers of 128 units. Has the author conduct ablation on hypernetwork on various configuration?", "title": "This work address batch normalization by introduce a KL term for learning to learn statistic", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "IVYcb3BAOWR": {"type": "review", "replyto": "9z_dNsC4B5t", "review": "The authors propose a method for cross domain few-shot classification that learns to generate domain specific data statistics from very few training examples for domain independent batch normalisation.  \nThey propose to train small auxiliary networks that generate data statistics for normalisation. Networks are trained within a meta-learning framework using a KL divergence loss, which enforces estimated statistics on small support/training examples to match statistics from query sets where more data is available. \n\n\nSTRENGTHS\n\nThe paper is well written and motivated. The proposed approach is, for the most part, easy to follow and understand. The approach benefits from its simplicity and versatility (e.g. it is not tied to a specific FSL method) and promising performance is obtained.\n\nThe authors provide a very large set of experiments in multiple scenarios, and definitely demonstrates a strong effort in evaluating their approach. \n\n\nWEAKNESSES\n\nUnfortunately, despite the fact that a significant amount of time was dedicated to evaluate the method, the experimental section needs substantial modifications. This is mainly due to the presentation of the experiments, as well as some key missing comparisons.   \nRegarding presentation, too many implementation details and descriptions of the experiments are missing from the main paper (and in certain cases, missing altogether). Regarding datasets and implementation, it is ok to provide non essential details in the appendix (especially considering the large number of datasets considered). However, no information is provided at all in the main paper, which makes it very difficult to understand the setting of the experiments. For example, in Table 1 and Figure 2, it is not known on which datasets experiments are run, and it is never mentioned in the main text (for table 1) what FSL method is employed.   \nIn addition, Table 2 experiments comparing MetaNorm to different approaches sorely lacks description. The approach is compared to 9 different algorithms, none of which are given a description or reference to learn more about the method. It is therefore impossible, besides guessing, to know what the model is being compared to. This issue is also noticeable in Table 3-5, in particular with a baseline in Table 4 that is never described.   \nFinally, experiments in Figure 2 would be a lot more interesting if compared to standard methods. It would be interesting to see how the proposed strategy allows to be more sample efficient and reach stronger performance than transductive batch norm in situations where sample size is the smallest. As this is one of the cited main limitations of TBN, this experiment is highly important2- With regards to related work, I would suggest to move the section after the introduction, where it provides much better context to facilitate method comprehension, in particular regarding the description of the TBN strategy. \n\nAuthors should also comment on how their work, and in particular FSL domain generalisation setting, relates to cross domain few-shot learning works, \ne.g. Tseng et al ICLR 2020, Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation\nIt is currently presented as a completely new approach to FSL, and appears to ignore past cross domain FSL works. Please provide additional context regarding such works.\n\n\nRECOMMENDATION\n\nIn summary, the authors propose an interesting, simple strategy for more robust BN that can be of interest to the community. I would strongly recommend that the authors make the following modifications to strengthen their paper:\n\n1-\tReorganise and expand on the experimental evaluation to provide necessary details and make the main paper self-contained, even if it requires moving an experiment to the supplementary materials.  \n2-\tMove the related works section after the introduction, to provide additional context before delving into the method  \n3-\tRelate the proposed work to past work on cross-domain FSL and potentially tone down claims that few-shot cross domain learning is a completely new problem investigated here.  \n4-\tPlease provide a comparison to standard TBN in Figure 2\n\n\nAdditional suggestions\n\n5-\tIf possible, please provide an overview figure of the proposed method.  \n6-\tIn result tables, please sort methods according to their overall performance, and correct bolding in table 2 (protonet 5-way-5 shot MetaNorm is not best performing method) and highlight setting where methods have very similar performance (as in Table 14) for clarity.   \n7-\tProvide a clear list of contributions at the end of the introduction section  \n8-\tWhile KL divergence and hypernetworks are well known terms, it would make the paper more accessible to add a sentence (and equation in the case of the KL divergence) or two describing the terms. In particular, hypernetworks are generally used to characterise weight generators for entire architectures and might lead to confusion.  \n9-\tIt could be nice to provide more attention to how this work relates to conditional batch norm works, and whether they can be complementary.  \n", "title": "Interesting approach, some improvements to the paper (experiments) needed", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}