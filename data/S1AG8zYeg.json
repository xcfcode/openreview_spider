{"paper": {"title": "Sentence Ordering using Recurrent Neural Networks", "authors": ["Lajanugen Logeswaran", "Honglak Lee", "Dragomir Radev"], "authorids": ["llajan@umich.edu", "honglak@eecs.umich.edu", "radev@umich.edu"], "summary": "We consider the problem of organizing a given collection of sentences into a coherent order.", "abstract": "Modeling the structure of coherent texts is a task of great importance in NLP. The task of organizing a given set of sentences into a coherent order has been\ncommonly used to build and evaluate models that understand such structure. In this work we propose an end-to-end neural approach based on the recently proposed\nset to sequence mapping framework to address the sentence ordering problem. Our model achieves state-of-the-art performance in the order discrimination task\non two datasets widely used in the literature. We also consider a new interesting task of ordering abstracts from conference papers and research proposals and\ndemonstrate strong performance against recent methods. Visualizing the sentence representations learned by the model shows that the model has captured high\nlevel logical structure in these paragraphs. The model also learns rich semantic sentence representations by learning to order texts, performing comparably to\nrecent unsupervised representation learning methods in the sentence similarity and paraphrase detection tasks.", "keywords": ["Natural language processing", "Deep learning", "Applications"]}, "meta": {"decision": "Reject", "comment": "Sentence ordering is central to a number of NLP tasks (e.g., summarization), and this paper introduces a very sensible application of existing techniques, experimentations is also very solid. However, the technical contributions seems quite limited.\n \n Positive:\n \n -- An important NLP problem (I would disagree here with Reviewer1), a good match between the problem and the methods\n -- Solid experiments \n -- A well-written paper\n \n Negative:\n \n -- A straightforward application of existing methods\n \n Unfortunately, the latter, I believe, is a problem. Though application papers are welcome, I believe they should go beyond direct applications and provide extra insights to the representation learning community."}, "review": {"H1a-zDK8e": {"type": "rebuttal", "replyto": "S1AG8zYeg", "comment": "We thank all reviewers for their constructive comments.\n\nR1, R3: Difference between our work and Vinyals et al. (2015)\n\nWhile our work is built upon Vinyals et al. (2015), we would like to point out the following key differences in our model:\n \n- Their intention was to show that the input, output orderings can matter in set/sequence mapping tasks, which they demonstrate using small scale experiments. In contrast, we build on these ideas to address the challenging problem of modeling logical and hierarchical structure in text.\n\n- Different from the original method, the following two techniques proved to be beneficial in the sentence representation learning experiment, which yielded an improvement of 25% (Pearson score) in the semantic relatedness task. These ideas are also more generally applicable within the set to sequence framework beyond our task domain.\n\n1. In addition to the sentences in the current paragraph, we also added contrastive sentences (i.e., sentences from other paragraphs) to the memory in the decoder. This forces the model to not only identify the correct sentence from the current paragraph, but also to distinguish and ignore sentences that are irrelevant to the context. \n\n2. We observed that the bilinear scoring function (eqn 10) produces much better representations compared to the MLP scoring function (eqn 9) that was suggested in Vinyals et al. (2015). We hypothesize that this difference in performance is due to the generative vs discriminative approach in predicting the next sentence as we discuss in section 3.1.\n\nWe have revised the paper to include an ablative study of the contributions of the above techniques. Please see section C.3 of the supplementary material.\n\n\nR1: Significance of the sentence ordering problem\n\nWe emphasize the importance of the sentence ordering problem below.\n\n- A sentence ordering component is essential in tasks that involve presenting a coherent summary of facts. In applications such as extractive multi-document summarization and retrieval-based question answering, sentences are fetched from multiple sources and an ordering component becomes necessary to coherently organize these sentences [1,2]. Our proposed method can be used for such applications, which will be interesting future work.\n\n- Perhaps more importantly, by learning to perform sentence ordering we are able to model text coherence. Automatic methods for evaluating human/machine generated text have great importance. Concrete applications include Automated essay scoring [3] and Text generation [4,5]. Discourse structure and coherence quality are key dimensions along which essays are scored in the ETS TOEFL and GRE analytical writing tasks [6]. A text generation system has to pay attention to choices such as presentation order of facts, order of entities in a clause, number of possibilities for pronoun resolution, etc. that influence coherence [7]. A model of coherence aids the better design of these systems.\n\n- Context prediction approaches have shown to yield useful representations for words and sentences in prior work. In this context, we demonstrate that the objective of the ordering task also helps learn useful sentence representations with cheaper computational cost compared to recent methods. \n\n\nR2: Questions regarding technical details\n\nPlease find below the responses to your questions about the technical details.\n\n- c and h are the usual cell state and hidden state (also called cell output) of an LSTM cell. Please refer to part D of the supplementary material for the exact equations. Intuitively, the cell state captures history information and the cell output extracts information from the cell state that is useful for (predicting) the output for the current time step.\n\n- W is a matrix of weights. As we discuss in the paper the first scoring function is a single hidden layer feed-forward net that takes s, h as inputs. This has the parametric form (omitting bias terms): f(s,h) = W\u2019tanh(W [s; h])). Assuming the dimensionality of s, h to be D and hidden layer size to be H, the weight matrix W has size Hx2D and W\u2019 has size 1xH. In the bilinear scoring function W would be a DxD matrix.\nIf you could let us know what other notations you found confusing, we would be happy to clarify.\n\n- The hidden state of the decoder is initialized with the final hidden state of the encoder as we describe in page 5 (second line). This is similar to how sequence to sequence models operate.\n\n\nReferences\n\n[1] Christensen, J., Mausam, S. S., Soderland, S., & Etzioni, O. (2013). Towards Coherent Multi-Document Summarization. In HLT-NAACL.\n[2] Barzilay, R., Elhadad, N., & McKeown, K. R. (2001). Sentence ordering in multidocument summarization. In Proceedings of the first international conference on Human language technology research (pp. 1-7). Association for Computational Linguistics.\n[3] Miltsakaki, E., & Kukich, K. (2004). Evaluation of text coherence for electronic essay scoring systems. Natural Language Engineering, 10(1), 25-55.\n[4] Park, C. C., & Kim, G. (2015). Expressing an image stream with a sequence of natural sentences. In Advances in Neural Information Processing Systems.\n[5] Kiddon, C., Zettlemoyer, L., & Choi, Y. (2016). Globally coherent text generation with neural checklist models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n[6] Automated scoring of writing quality. URL https://www.ets.org/research/topics/\nas_nlp/writing_quality. Accessed: 2016-11-1.\n[7] Kibble, R., & Power, R. (2004). Optimizing referential coherence in text generation. Computational Linguistics, 30(4), 401-416.", "title": "Rebuttal"}, "HJeV5iEQx": {"type": "rebuttal", "replyto": "r1hxXdxml", "comment": "Thank you for your comments and questions.\n\nYes, you are right about the encoder and decoder. I don\u2019t see how we can merge the encoder and decoder in this manner. h_{t, dec} represents the state in the decoder where the previous (t-1) tokens have been received as input and the t th token is to be predicted. But in the encoder h_{t, enc} does not have such an interpretation. Furthermore, the number of read cycles in the encoder is a constant and does not depend on the document length. Perhaps I did not understand your suggestion. It would be helpful if you could elaborate a bit more. \n\nFor the order discrimination and sentence ordering experiments the single hidden layer feed-forward net was used. For the experiments in section 4.4 the bilinear function was used. The bilinear function produced much better semantic representations of sentences, as measured by the performance in the sentence similarity and paraphrase detection tasks. As we discuss in section 3.1, this difference arises from the discriminative and generative approaches in predicting the next sentence.\n\nNo, the RNN Decoder corresponds to the setting where the sentence encoder + decoder are trained from scratch. We apologize for the confusion. We identified a bug in the decoding procedure for the RNN Decoder and updated the performance numbers for this model. The new result invalidates our previous conclusion that the difference comes from better sentence representations as the model performance is much better now. In light of this update, the performance difference between the two models primarily comes from the better decoder initialization for the proposed model.", "title": "Model Clarifications"}, "rkcs5i47e": {"type": "rebuttal", "replyto": "HkaIlURzx", "comment": "Thank you for pointing out this paper. We have updated the supplementary material with a comparison (section C2).", "title": "Comparison with document-context language models"}, "BJU_qj4mx": {"type": "rebuttal", "replyto": "B1wKXYRfx", "comment": "Thank you for your comments and questions.\n\nI agree with your point that the accuracy measure conflates the two sources of errors you mentioned. However, we have attempted to do a fair comparison by using an identical decoding procedure across all methods.\n\nWe performed further experiments based on your suggestion. We have updated the supplementary material to include these experiments and a discussion (section C1). As you mentioned, there is a modest gap in classifying pairwise rankings. However, the gaps are  significant in the more realistic task of choosing the best order from a given large set of candidate orderings.", "title": "Accuracy and greedy decoding"}, "r1hxXdxml": {"type": "review", "replyto": "S1AG8zYeg", "review": "It seems that the only information from the encoder used in the decoder is the final hidden state, which is used as the initial state in the decoder. Is this correct? Would it make sense to \"merge\" the encoder and decoder, by directly using h_{t, enc} in Eq. 7 instead of h_{t, dec}?\n\nWhat scoring function (defined in section 3.1) is used in the experiments? (I believe it is the bilinear function, but cannot find it in the text). Is there a big difference between the two scoring functions?\n\nIn section 4.3.4, the authors mention that \"when the sentence encoder is initialized with a good set of parameters [...] this compensates for the lack of an encoder and leads to better performance.\" Does this setting corresponds to the \"RNN Decoder\" line in Table 3? Does this mean that the difference of performance between \"RNN Decoder\" and \"Proposed model\" is due to better representations?The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs).\n\nThe first processing step of the model is to encode each sentence using a word level LSTM recurrent network. An order invariant encoder (based on Vinyals et al. (2015)) is then used to obtain a representation of the set of sentences. The input at each time step of this encoder is a \"bag-of-sentences\", over which attention probabilities are computed. The last hidden representation of the encoder is then used to initialize the decoder. This decoder is a pointer network, and is used to predict the order of the input sentences.\n\nThe model introduced in this paper is then compared to standard method for sentence ordering, as well as a model with the decoder only. The encoder-decoder approach outperforms the other methods on the task of ordering a given set of sentences. The authors also show that the model learns relatively good sentence representations (e.g. compared to Skip-thought vectors).\n\nThis paper is relatively well written, and easy to follow. The model described in the paper does not really introduce anything new, but is a natural combination of existing techniques to solve the task of sentence ordering. In particular, it uses an order-invariant encoder to get a representation of the set of sentences, and a pointer network to predict the ordering. While I am not entirely convinced by the task of sentence ordering, this approach seems promising to learn sentence representations.\n\nOverall, this is a pretty solid and well executed paper.\n\npros:\n - sound model for sentence ordering\n - strong experimental results\ncons:\n - might be a bit incremental\n - usefulness of sentence ordering", "title": "Intuition for encoder / decoder", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1BC_3WEe": {"type": "review", "replyto": "S1AG8zYeg", "review": "It seems that the only information from the encoder used in the decoder is the final hidden state, which is used as the initial state in the decoder. Is this correct? Would it make sense to \"merge\" the encoder and decoder, by directly using h_{t, enc} in Eq. 7 instead of h_{t, dec}?\n\nWhat scoring function (defined in section 3.1) is used in the experiments? (I believe it is the bilinear function, but cannot find it in the text). Is there a big difference between the two scoring functions?\n\nIn section 4.3.4, the authors mention that \"when the sentence encoder is initialized with a good set of parameters [...] this compensates for the lack of an encoder and leads to better performance.\" Does this setting corresponds to the \"RNN Decoder\" line in Table 3? Does this mean that the difference of performance between \"RNN Decoder\" and \"Proposed model\" is due to better representations?The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs).\n\nThe first processing step of the model is to encode each sentence using a word level LSTM recurrent network. An order invariant encoder (based on Vinyals et al. (2015)) is then used to obtain a representation of the set of sentences. The input at each time step of this encoder is a \"bag-of-sentences\", over which attention probabilities are computed. The last hidden representation of the encoder is then used to initialize the decoder. This decoder is a pointer network, and is used to predict the order of the input sentences.\n\nThe model introduced in this paper is then compared to standard method for sentence ordering, as well as a model with the decoder only. The encoder-decoder approach outperforms the other methods on the task of ordering a given set of sentences. The authors also show that the model learns relatively good sentence representations (e.g. compared to Skip-thought vectors).\n\nThis paper is relatively well written, and easy to follow. The model described in the paper does not really introduce anything new, but is a natural combination of existing techniques to solve the task of sentence ordering. In particular, it uses an order-invariant encoder to get a representation of the set of sentences, and a pointer network to predict the ordering. While I am not entirely convinced by the task of sentence ordering, this approach seems promising to learn sentence representations.\n\nOverall, this is a pretty solid and well executed paper.\n\npros:\n - sound model for sentence ordering\n - strong experimental results\ncons:\n - might be a bit incremental\n - usefulness of sentence ordering", "title": "Intuition for encoder / decoder", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1wKXYRfx": {"type": "review", "replyto": "S1AG8zYeg", "review": "The definition of \"accuracy\" in Table 3 is different from that of \"accuracy\" in Table 2 in that the former is computed over each sentence's absolute predicted position while the later is computed over pairwise ranking between two alternative orderings. The potential downside of accuracy as defined in Table 3 is that it conflates two different sources of errors: (1) whether the model can correctly rank one ordering of paragraph over an alternative ordering (which corresponds to the accuracy in Table 2) and (2) whether the model can generate the sequence of ideal ordering in a greedy manner (through beam search). Given that most other baseline models are not designed to perform well for greedy sequential generation (i.e., (2)), it seems natural that they will perform poorly when the accuracy and Kendall's tau are computed over the ordering generated by each baseline method. I think this may be also why there's surprisingly big performance gap in Table 3, while the differences are much more modest in Table 2. Is it possible to report Table 2 style accuracy in Table 3 as well, by creating an identical set of pairwise ranking tasks across different methods?\nThis paper presents an empirical study on sentence ordering using RNN variants.\n\nI\u2019m not sure how strong novelty this paper brings in terms of technical contributions. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015) that has been developed for set-to-sequence type tasks. Sentence ordering is a good fit for set-to-sequence formulation, as the input sentences are rather a set than a sequence. Thus, the main contribution of this work can be viewed as a new application of an existing model rather than a new model development.\n\nThat said, the empirical evaluation of the paper is very solid and the authors have updated the supplementary material to strengthen the evaluation even further in response to the QAs.\n\n", "title": "accuracy", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkhA8DmVg": {"type": "review", "replyto": "S1AG8zYeg", "review": "The definition of \"accuracy\" in Table 3 is different from that of \"accuracy\" in Table 2 in that the former is computed over each sentence's absolute predicted position while the later is computed over pairwise ranking between two alternative orderings. The potential downside of accuracy as defined in Table 3 is that it conflates two different sources of errors: (1) whether the model can correctly rank one ordering of paragraph over an alternative ordering (which corresponds to the accuracy in Table 2) and (2) whether the model can generate the sequence of ideal ordering in a greedy manner (through beam search). Given that most other baseline models are not designed to perform well for greedy sequential generation (i.e., (2)), it seems natural that they will perform poorly when the accuracy and Kendall's tau are computed over the ordering generated by each baseline method. I think this may be also why there's surprisingly big performance gap in Table 3, while the differences are much more modest in Table 2. Is it possible to report Table 2 style accuracy in Table 3 as well, by creating an identical set of pairwise ranking tasks across different methods?\nThis paper presents an empirical study on sentence ordering using RNN variants.\n\nI\u2019m not sure how strong novelty this paper brings in terms of technical contributions. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015) that has been developed for set-to-sequence type tasks. Sentence ordering is a good fit for set-to-sequence formulation, as the input sentences are rather a set than a sequence. Thus, the main contribution of this work can be viewed as a new application of an existing model rather than a new model development.\n\nThat said, the empirical evaluation of the paper is very solid and the authors have updated the supplementary material to strengthen the evaluation even further in response to the QAs.\n\n", "title": "accuracy", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkaIlURzx": {"type": "review", "replyto": "S1AG8zYeg", "review": "For the sentence ordering task, I am wondering whether you can compare with the document-context language models proposed in the following work: https://arxiv.org/pdf/1511.03962v4.pdfThis paper extends the \"order matters\" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering.\n\nI think the problem is interesting and the results are promising. However, there are some problems about technical details:\n\n- Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states? Refer to (Vinyals et al. 2015a)?\n- Some notations in this paper are confusing. For example, what is the form of W in the feed-forward scoring function? Does it have the same form as the W in the bilinear score function?\n- What is the connection between the encoder and decoder in the proposed model? How to combine them together? I read something relevant from the caption of Figure 1, but it is still not clear to me.\n", "title": "Comparison", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Skba7frNx": {"type": "review", "replyto": "S1AG8zYeg", "review": "For the sentence ordering task, I am wondering whether you can compare with the document-context language models proposed in the following work: https://arxiv.org/pdf/1511.03962v4.pdfThis paper extends the \"order matters\" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering.\n\nI think the problem is interesting and the results are promising. However, there are some problems about technical details:\n\n- Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states? Refer to (Vinyals et al. 2015a)?\n- Some notations in this paper are confusing. For example, what is the form of W in the feed-forward scoring function? Does it have the same form as the W in the bilinear score function?\n- What is the connection between the encoder and decoder in the proposed model? How to combine them together? I read something relevant from the caption of Figure 1, but it is still not clear to me.\n", "title": "Comparison", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}