{"paper": {"title": "GraphCGAN: Convolutional Graph Neural Network with Generative Adversarial Networks", "authors": ["Sheng Zhang", "Rui Song", "Wenbin Lu"], "authorids": ["~Sheng_Zhang8", "~Rui_Song2", "~Wenbin_Lu1"], "summary": "", "abstract": "Graph convolutional networks (GCN) achieved superior performances in graph-based semi-supervised learning (SSL) tasks. \nGenerative adversarial networks (GAN) also show the ability to increase the performance in SSL.\nHowever, there is still no good way to combine the GAN and GCN in graph-based SSL tasks.\nIn this work, we present GraphCGAN, a novel framework to incorporate adversarial learning with convolution-based graph neural networks, to operate on graph-structured data. \nIn GraphCGAN, we show that generator can generate topology structure and features of fake nodes jointly and boost the performance of convolution-based graph neural networks classifier.\nIn a number of experiments on benchmark datasets, we show that the proposed GraphCGAN outperforms the baseline methods by a significant margin. ", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper presents a method to combine graph convolutional neural networks (GCNs) with generative adversarial networks (GANs) for graph-based semi-supervised learning.\n\n**Strengths:**\n  * It is a reasonable attempt to combine GCN with GAN for semi-supervised node classification.\n  * The proposed method is general in that it can work with different graph neural networks.\n\n**Weaknesses:**\n  * The novelty of this work is limited.\n  * The proposed method, GraphCGAN, has no significant performance improvement over state-of-the-art methods.\n  * The writing has much room to improve in terms of both clarity and the linguistic quality.\n\nSince both the novelty and the significance of this paper in its current form are limited, it is premature for publication. There is consensus among all the reviewers that this paper is not up to the acceptance standard of ICLR.\n"}, "review": {"397IGU6zj24": {"type": "rebuttal", "replyto": "MElkMy8Rznu", "comment": "Thank you for all valuable comments.\n\nRegarding to \u201cMotivation and significance are not clear\u201d:\n\nCompared to previous method, proposed method generates nodes other than hidden vectors. The generated nodes help boost the performance in state-of-the-art classifiers (GCN and its variants). The intuition can be summarized as we want to generate fake nodes to help improve the graph convolutional network and its variants.  It is important to move from intuition to a solid body of work.\n\nRegarding to \u201ctechnical issue (1)\":\n\nMinimizing the loss function for generator considers the dependency on the existing nodes, because the mapping from latent variable z to fake node v is shaped by the existing nodes. \n\nRegarding to \u201ctechnical issue (2)\":\n\nPeople are more interested in the classification of real nodes. Based on Equation (7), the layer propagation for real nodes is constructed based on their neighbors (both real and fake nodes). Hence, the links within fake nodes cannot affect the representation of real nodes in single propagation. \n\nRegarding to \u201ctechnical issue (3)\":\n\n\u201cPull-away item\u201d prevents the fake nodes to shrink together. \u201ccomplementary loss\u201d leads the fake nodes to complementary space for existed nodes. We would add more explanations in next version. The ablation study for the loss function necessity can be found in Table 2 in page 10. \n\nRegarding to \u201cgrammar errors, typos, and hard-parsing sentences\u201d:\n\nThank you for pointing out. Most problems are addressable and could be quickly revised in our next version.\n", "title": "Response to AnonReviewer4"}, "iV7y2LhAQv2": {"type": "rebuttal", "replyto": "qz85LtkiAmY", "comment": "Thank you for all constructive suggestions and valuable comments. The clarification of contribution and novelty could be added in our next version.\n\nRegarding to \u201cIt is unclear why the generated fake nodes can only link to existing nodes\u201d:\n\nPeople are more interested in the classification of real nodes. Based on Equation (7), the layer propagation for real nodes is constructed based on their neighbors (both real and fake nodes). Hence, the links within fake nodes cannot affect the representation of real nodes in single propagation. \n\nRegarding to \u201cEquations (9) and L_G1 are difficult to understand.\u201d:\n\nEquation (9), \\tilde{H}_i^{L-1} =  g(x_i, a_i; \\theta_C), represents the L-1 Hop for node v_i, which is denoted as (x_i, a_i). We would revise as \\tilde{H}_i^{L-1} =  g(v_i; \\theta_C, \\tilde A, \\tilde X) = g(x_i, a_i; \\theta_C, \\tilde A, \\tilde X) in next version.\n\nSimilarly, For L_G1, g(x_i, I_i; \\theta_c) can be revised as  g(v_i; \\theta_C, \\tilde I, \\tilde X) =  g(x_i, I_i; \\theta_C, \\tilde I, \\tilde X).  g(G_1(z), 0; \\theta_c) can be revised as g(v_0; \\theta_c, \\tilde A, \\tilde X) = g(G_1(z), 0; \\theta_c, \\tilde I, \\tilde X), where we use (G_1(z), 0) to represent v_0, because (x_i, I_i) means node v_i has feature x_i and connect to i-th real node, and (G_1(z), 0) means fake node v_0 has feature G_1(z) and no connection to real node.\n", "title": "Response to AnonReviewer1"}, "1_WJPVtNRMI": {"type": "rebuttal", "replyto": "pQpGI3zXN3", "comment": "Thank you for valuable comments.\n\nRegarding \u201cthe motivation is not strong enough\u201d:\n\nThe work is motivated by the idea that \u201cgenerated nodes can boost the performance in graph convolutional networks\u201d. It is important to move from intuitions to a solid body of knowledge.\n\nResponse to \u201cMore experiments involving different scale networks\u201d:\n\nPub dataset has a different scale from cora and citeseer datasets. \n", "title": "Response to AnonReviewer3"}, "YD5XKVzr7xv": {"type": "rebuttal", "replyto": "mb7vS5ZO8V0", "comment": "Thank you for all the constructive suggestions and valuable comments.\n\nRegarding \u201cHow does GraphCGAN compare to Graph U-Nets..\u201d:\nThank you for pointing out literature that we were unaware of.  Graph U-Nets insert gPool and gUnpool layers to Graph Convolutional Network (GCN), which incorporates the popular technique from image processing. Our method can be extended to variants of GCN including Graph U-Nets. We conduct the experiment following the same parameter setting, the results are attached. The gains come from complementary generated nodes.\n\n\nResponse to \u201cThe margin of error..\u201d:\nThe variations in our proposed method and GraphSGAN have two sources: classifier and generator. GCN and GAT methods only contain the variation from the classifier. \n\nResponse to \u201con what set were the plots in Figure 1\u201d:\nThe Figure 1 is ablation study on size of generated nodes, the train/dev/test datasets are used following the same partition in paper [1]. \n\n\ng-U-Nets                          | 84.4 \u00b1 0.6% | 73.2 \u00b1 0.5%    |79.6 \u00b1 0.2%\nGraphCGAN(g-U-Nets)  |84.5 \u00b1 0.7%  | 73.5  \u00b1 1.2%  | 80.2 \u00b1 1.2%\n\nReference [1]: Kipf, Thomas N., and Max Welling. \"Semi-supervised classification with graph convolutional networks.\" arXiv preprint arXiv:1609.02907 (2016).\n", "title": "Response to AnonReviewer2"}, "MElkMy8Rznu": {"type": "review", "replyto": "iy3xVojOhV", "review": "Pros: \n1.\tThis paper proposes the first combination of GNN with GAN for semi-supervised learning.\n2.\tThe structure of the paper is clear and easy to follow.\nCons:\n1.\tSome sentences are hard to parse and many grammar errors.\n2.\tThe contribution and novelty are limited.\n\nThis paper deals with semi-supervised learning on graphs based on GANs by proposing a framework named GraphCGAN.  The proposed framework can be easily extended to include other GNN methods. However, the novelty of the proposed model is limited, and the motivation of this paper is not strong. Also, the only motivation stated by the authors is to improve the performance on semi-supervised learning, while the improvement of the performance in the experiments is limited (i.e. ~1% improvement). The followings are the details of comments regarding this paper from three aspects.\n\n1.\tMotivation and significance are not clear.\n(1)\tThe author(s) claim(s) that combining GCN with GANs could boost the performance of semi-supervised learning, which however is not solidly validated by the experiment results. \n(2)\tThe author claims that GANs have never been applied to the SSL task in graphs. However, the existing method GraphSGAN [1] has already done this and GraphSGAN shows better scalability than GCN. The only difference between the proposed GraphCGAN and GraphSGAN is the selection of classifiers (MLP vs. GCN). Thus, the significance of the proposed GraphCGAN is not clear compared to the existing GCN and GraphSGAN. \nAs a summary, considering the limited improvement on performance, I would like to see more other motivations why we need graph-convolution-based GANs for semi-supervised learning on graphs.\n\n       [1] Ding, M., Tang, J., & Zhang, J. (2018, October). Semi-supervised learning on graphs with generative adversarial nets. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp. 913-922).\n\n2.\tThere are some technical issues.\n(1)\tThe generator seems very weak. As stated in Section 3.1, the generator generates each node and edge only based on randomized latent representation \u201cz\u201d. If so, each new node (and edge) is generated independently without considering the dependency on the existing nodes and the dependency between the nodes and its adjacent relations.  \n(2)\tEquation 5 is confusing. As expressed in Equation 5, the adjacent matrix of all the newly generated nodes is defined as a unit diagonal matrix I_B. It is not clear why the adjacent relation between each pair of the newly generated nodes is set to zero. \n(3)\tThe author mentions \u201cpull-away item\u201d and \u201ccomplementary loss\u201d without giving any explanations on these two items in the loss function.  An ablation study is better to be conducted to validate the necessity of these two terms in loss function.\n\n3\tThe presentation of the paper has many grammar errors, typos, and hard-parsing sentences.\n(1)\tIt is hard to parse \u201c\u2026as the map from feature vector and adjacency vector to the space of second to the last layer in convolution-based GNN...\u201d. It is not clear what \u201cthe space of second to the last layer in convolution-based GNN\u201d refers to.\n(2)\tIt is hard to parse \u201c\u2026.and extract the map to the intermediate layer g(:; :) \u2026.\u201d In Algorithm 1. The term \u201cmap\u201d is confusing. \n(3)\tIt is not clear what \"i\" refers to in the equation of L_{G_2}. If it refers to the index of one node, then there may lost a \u201csum\u201d symbol to sum up all the nodes into the loss.\n(4)\tThere are many grammar errors and typos, e.g., \n-----\u201cexisted methods\u201d\n                 -----the font of word \u201csoftmax\u201d is not unified, see Section 2.1 on Page 2.\n                 -----\u201c\u2026Later on, Dai et al. provided a theoretical\u2026.\u201d lacks the hyper-reference.\n----- \u201c: =\u201d  in Equation 4.\n", "title": "An interesting work yet with limited significance.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "qz85LtkiAmY": {"type": "review", "replyto": "iy3xVojOhV", "review": "This paper combines adversarial learning with graph neural networks to improve the performance of GNN for semi-supervised node classification. To generate fake nodes, the authors designed generator G1 to generate node attributes of fake nodes and G2 to generate the links of the fake nodes to existing nodes.\n\n+ Positive\n1. The idea of combining GNN with GAN for semi-supervised node classification makes sense\n2. The authors provides visualization of the generated fake nodes to help understand the proposed method.\n3. The proposed method is flexible, which can be used for various GNNs\n\n- Negative\n1. The novelty of the paper is limited. It is simple extension of GAN for semi-supervised learning to GNNs. The proposed method heavily relies on existing techniques with little novelty.\n2. It is unclear why the generated fake nodes can only link to existing nodes, not to fake nodes. Intuitively, the fake nots are more realistic if they can also link to each other. The authors may need to give some explanations on such design.\n3. Equations (9) and L_G1 are difficult to understand. For \\tilde{H}_i^{L-1}, it aggregates the information of the (L-1)-hop neighborhood of x_i.  It is unclear to me why \\tilde{H}_i^{L-1} can be written as g(x_i, a_i; \\theta_C), i.e., only relevant to x_i and a_i. In L_G1, why do we have I_i for x_i while 0 for G_1(z). Shouldn\u2019t both be \\tilde{I}_i as we treat the adjacency matrix as I.\n\nQuestion for rebuttal:\nPlease answer 2 and 3 above.\n\nJustification for score\uff1a\nThe paper studies an important problem. However, it lacks novelty. Some part of the paper is unclear. I will change the rating if the authors can clarify the contribution and novelty, and address my questions.\n", "title": "incremental work combining GAN and GNN", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "pQpGI3zXN3": {"type": "review", "replyto": "iy3xVojOhV", "review": "This paper proposes a novel framework to incorporate adversarial learning with convolution-based graph neural network, to operate on graph-structured data.\nThe proposed method is inspiring. However, some problems remain with the proposed technique.\n\n1.\tThis paper announces that it proposes a novel approach called GraphCGAN to deal with the three challenges of constructing a general framework. However, the motivation is not strong enough that the proposed approach looks like a patchwork of two models.\n\n2.\tMore experiments involving different scale networks are needed to prove the effectiveness of the proposed method. \n\nThis paper is globally well organized and clearly written. However, some important details are missing.\n\n1\tThe details about generator are unclear.\n2\tThe paper lacks of analysis on the experimental result.\n3\tThe details about networks are unclear.\n", "title": "The motivation is not strong enough that the proposed approach looks like a patchwork of two models", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "mb7vS5ZO8V0": {"type": "review", "replyto": "iy3xVojOhV", "review": "##### 1. Summary\nThe paper presents a method to combine graph convolutional neural networks (GCNs) with generative adversarial networks (GANs). The authors focus on the problem of semi-supervised learning on graphs and propose an end-to-end framework in which the generative model is followed by direct convolutions on the graph nodes. Experiments are conducted on standard benchmark datasets and the proposed method, GraphCGAN is compared against several state-of-the-art approaches.\n \n##### 2. Rationale for the score\nAs stated by the authors, the proposed method is an extension of GraphSGAN. In GraphSGAN a similar process of generating fake nodes is performed. As in GraphSGAN, the fake nodes in GraphCGAN are also linked to real nodes in the graph. The difference is that in GraphSGAN a graph Laplacian regularization is done while GraphCGAN applies a convolution on the graph. This could be considered a novel change if the authors were to show that the proposed approach outperforms GraphSGAN. Yet, if there exists an increase in classification performance, it is modest at best. The accuracy and margins of error in Table 1 do not show any actual increase in performance between GraphCGAN and GraphSGAN.\n\n##### 3. Positive aspects\n- The plots included in section 5.3 are interesting and match the behavior previously reported in GraphSGAN.\n- In the analysis, the authors show how their method compares to GCN and GAT when these two are included as classifiers in the end-to-end learning process of GraphCGAN. This is a good idea and the aim is to determine if there are advantages in using the generative model. Although the idea is good, there are issues with the reported values (see section 4 of this review).\n- The authors included the code associated with the method. This is always appreciated.\n\n##### 4. Negative aspects\n- The experiments are not comprehensive and it is not clear if the authors performed the experiments from scratch. For example, the performance values reported for GAT and GCN are the same as the ones listed in Table 3 of [R1].\n- The claim that GraphCGAN outperforms GCN and GAT is not supported by all the results in Table 1. For example, the performance on Pubmed is within the margin of error for GCN. Similarly for GAT, on all three datasets.\n- When comparing against state-of-the-art methods, the authors omitted the comparison against Graph U-Nets [R1]. This is, in my opinion, the main weakness of this manuscript. If we have a look at the results obtained by Graph U-Nets (reported in Table 3 of [R1]), GraphCGAN does not seem to outperform Graph U-Nets in any dataset, including margin of errors. \n- The authors refer to an ablation study but only a couple of parameters are analyzed, e.g., number of fake nodes in Figure 1 and loss function of the generator in Table 2. The many other parameters of the model are listed in section 5.2 without details about how they were derived.\n\n##### 5. Questions to be addressed during rebuttal period\n* How does GraphCGAN compare to Graph U-Nets on the Cora, Citeseer and Pubmed datasets?\n* The margin of errors are quite high, similarly to those reported for GraphSGAN. Can the authors elaborate on why this is the case?\n* Can the authors confirm on what set (hopefully a partition of the training set) were the plots in Figure 1 obtained? There is no margin of error associated to each point in the plots and it gives the impression of overfitting to the test set.\n\n##### 6. Additional references\n[R1] Gao, Hongyang, and Shuiwang Ji. \"Graph U-Nets.\" ICML 2019. PMLR 97:2083-2092", "title": "ICLR 2021 Review of paper 2790: GraphCGAN: Convolutional Graph Neural Network with Generative Adversarial Networks", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}