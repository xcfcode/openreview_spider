{"paper": {"title": "Learning Active Learning in the Batch-Mode Setup with Ensembles of Active Learning Agents", "authors": ["Malte Ebner", "Bernhard Kratzwald", "Stefan Feuerriegel"], "authorids": ["~Malte_Ebner1", "bkratzwald@ethz.ch", "sfeuerriegel@ethz.ch"], "summary": "This paper proposes to perform active learning with a parametrised ensemble of agents and evaluates the approach in the batch-mode setting.", "abstract": "Supervised learning models perform best when trained on a lot of data, but annotating training data is very costly in some domains. Active learning aims to chose only the most informative subset of unlabelled samples for annotation, thus saving annotation cost. Several heuristics for choosing this subset have been developed, which use fix policies for this choice. They are easily understandable and applied. However, there is no heuristic performing optimal in all settings. This lead to the development of agents learning the best selection policy from data. They formulate active learning as a Markov decision process and applying reinforcement learning (RL) methods to it. Their advantage is that they are able to use many features and to adapt to the specific task.\n\nOur paper proposes a new approach combining these advantages of learning active learning and heuristics: We propose to learn active learning using a parametrised ensemble of agents, where the parameters are learned using Monte Carlo policy search. As this approach can incorporate any active learning agent into its ensemble, it allows to increase the performance of every active learning agent by learning how to combine it with others.", "keywords": ["active learning", "ensembles"]}, "meta": {"decision": "Reject", "comment": "The authors propose to linearly combine the utility functions of (batch) active learning algorithms. The linear combination coefficients are \"learned\" with Monte Carlo estimators to adapt the coefficients to different kinds of tasks automatically.\n\nThe reviewers find the presentation within the papers generally clear. The simplicity of the approach, which is highlighted in the authors' rebuttal, should be appreciated. The authors also addressed the issue of robustness with respect to the batch size. But the paper left quite a few unanswered issues even after the authors rebuttal. The novelty with respect to several earlier papers require clarification and concrete comparisons, such as the ones in reinforcement learning and bandit learning as pointed out by reviewers. The lack of comparisons to those existing works, both illustratively and empirically, is a key weakness of the current paper. A more careful study of RL setting (such as reward shaping) is also important to understand the value of the work. Finally, the gap between the ensemble approach and the single approach also deserves more investigation to justify the significance of the contribution.\n"}, "review": {"SrwFtfy6S0G": {"type": "review", "replyto": "EBRTjOm_sl1", "review": "Summary:\nThe paper proposes learning a batch mode active learning (AL) policy as a weighted ensemble of existing AL techniques (or agents). In the proposed method, the ensemble weight vector (\\beta) is learnt from data. AL is simulated on a set of training tasks where performance for various choices of \\beta are estimated using a Monte Carlo approach. Subsequently, the optimal choice of \\beta is found using tree Parzen estimators, a black box hyperparameter optimization technique. Experimental results are shown on Checkerboards, Fashion MNIST, bAbI and 10 UCI datasets. \n\n+ves:\n+ The Related Work section is extensive and clearly explains the existing methods in literature, building up to the reasons behind developing the proposed approach\n+ The batch mode active learning framework is clearly defined and well-explained, thus setting the stage for the proposed ensemble based method. \n\nConcerns:\n\n- Sec 2.3, para 2, \u201cIn domains where it is not possible to learn the best parameters because there is no similar training task, an ensemble of active learning agents using such an engineered combination of them might be suitable.\u201d\nThe sentence suggests that an ensemble of AL agents is suitable for domains where similar tasks are not available. However, the second contribution in Section 1 states the following. \u201cWe show with our experiments that it is very important to train the ensemble on a similar task it is evaluated on.\u201d \nThis contribution seems to be in direct contradiction with the first statement. \n\n- In Sec 3.1, the reward is chosen as the improvement in current model\u2019s accuracy after a step i.e., choosing a sample or a batch of samples. However, Sec 3.3 and Alg 2 suggest that the objective of the \\beta optimization problem is calculated as the final AL performance i.e., the performance of the supervised model at the end of the AL process. If this is the case, then reward shaping technique mentioned in Sec 3.1 might not be necessary, because the intermediate rewards are not used to learn the ensemble weights. This was not clear, kindly clarify if I missed something here.\n \n- Sec 4.6, para 2, \u201c..ensemble trained on the checkerboard dataset has a weight of uncertainty sampling being zero..\u201d. \nThe sentence says that the weight for uncertainty sampling is zero for checkerboard, however Table 1 shows that uncertainty has weight 0.93 for the checkerboard dataset.\n\n- The results would\u2019ve been much stronger and more conclusive if the proposed method of learning ensemble weights had been compared against an ensemble of agents with equal weights (or random weights). Then, the superior performance of the proposed method would\u2019ve strongly validated the need to learn ensemble weights, as opposed to giving them equal weights.\n\n- While it is suggested in the introduction that learning a few weight parameters is much easier than reinforcement learning, the results would\u2019ve been more complete if details were included in the Experiments section, on the computational complexity of the proposed method, as compared to RL methods in terms of time taken. \n\n- The paper could have benefited with a more thorough discussion section. Since ensembling is a fairly well known idea, an in-depth discussion would\u2019ve helped to understand how well ensembling works in case of active learning. For instance: \n(1) Why is there a major difference of three orders of magnitude between the weights for representative sampling for MNIST and bAbI? (Table 1)\n(2) In Table 1, for bAbI dataset, uncertainty and diversity have zero weight. Then, why does uncertainty-diversity sampling has a positive and high weight of 21.5?\n(3) It would\u2019ve been interesting to see an ablation study on how strongly the prior of \\beta parameters would affect the final learned \\beta vector.\n(4) It is mentioned that if the \\beta parameters are learnt properly, then the proposed method will perform at least as good as the best heuristic, in the worst case. In this vein, a discussion on how likely the tree parzen method will attain those optimal parameters, could\u2019ve been insightful. Also, it would help the community to see the limitations of the proposed method. For instance, how do the following factors affect the optimality of \\beta parameters? (a) complexity and size of the dataset (b) choice of the hyper parameter optimization technique. \n\n- The details of what classifiers were used for the experiments on UCI datasets is not provided. A short section could have been introduced as Supplementary material to provide these details. These may be important, especially considering there seem to be different choices of classifiers for different datasets. Why was this justified, and why would this observation generalize? Some ablation studies and analysis on one dataset as to how the performance would change if another classifier was used would help showcase the effectiveness of the proposed method.\n\nIn summary, the paper aims to propose ensembling as a simple alternative to computationally costly RL techniques for active learning. While the method is well-motivated, it is missing some key experiments (especially on the significance of the contribution) and analysis, and has a weak discussion/analysis section.\n\nMinor comments:\n(which did not affect the decision):\n\nQuite a few editorial mistakes were across the paper. Here are a few examples and their corrections. (Note that this is not an exhaustive list, and only examples of similar mistakes across the paper).\n[Abstract, line 2] chose -> choose\n[Abstract, line 5] fix -> fixed\n[Abstract, line 8] applying -> apply\n[Abstract, para 1 last sentence] \u201cto adapt to\u201d -> \u201cadapt to\u201d\n[Introduction, para 3 line 1] shortcoming -> shortcomings\n[Sec 2.1, para 2, line 9] which a -> which are\n[Sec 2.2, para 1, line 1] dataset -> datasets\n[Sec 2.3, sentence 1] \u201ccombined to an\u201d -> combined to form an\n[Sec 3, line 3] ensemble of different active learning -> ensemble of different active learning methods\n\nPOST-REBUTTAL:\n\nI thank the authors for their response. While some of my concerns have been addressed, a few key questions haven't been answered.\n\n* The contribution is limited in novelty.\n\n* The general author response mentions that reward shaping is not used in the proposed method. In that case, Sec 3.1 seems a bit pedagogical and misleading, since MDP for AL is described in detail but is not even empirically compared with the proposed method.\n\n* Regarding the ablation studies on a different classifier, while I agree that SOTA networks need to perform well, the ablation study on a different classifier was suggested to rule out the effect of the choice of SOTA classifier in the effectiveness of the proposed method. Also, the authors respond to R1 that the gap between the ensembles' performance and each single classifier's performance is small since they chose SOTA models for the individual classifiers. This is perhaps even more reason to show how ensembling works when the base models are not SOTA.\n\n* Beyond just a statement in the response, it would have been good to see some empirical comparisons between the proposed method and, say, Q-learning-based RL methods - especially in terms of actual running time complexity.\n\n* I also agree with R3 that similar ideas have been explored before (papers cited in R3's review), and it is important to compare with those methods as baselines in the experiments.\n\n* Also, the choice of the datasets used is not justified appropriately.\n\nI stay with my original decision.\n", "title": "Review of AnonReviewer2", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ljyxBqwOBv": {"type": "rebuttal", "replyto": "DIhD0wdeKAr", "comment": "We are very grateful that the reviewer sees an interesting and valuable contribution in our paper. We are confident that we can incorporate all remarks in order to come up with an even more impactful paper.\n+ \u201crobust wrt batch size\u201d  \nWe have added plots showing the performance of our approach with different batch sizes in the revised version of the manuscript, Sec. A.3. We can also give you two theoretical insights: First, only the uncertainty feature depends on the current classifier, the other two are independent of it and thus also of the batch size. Hence ensembles not relying on the uncertainty feature like the ensemble trained on the bAbI dataset have a performance independent of the batch size. Second, we found an agreement in current literature that batch-mode AL is more difficult than sequential AL and sequential heuristics perform worse in the batch-mode setting. Our experiments also found this behaviour, the performance of the ensemble is monotonically decreasing with the batch size. \n+ \u201chow many more labeled examples\u201d  \nA comparison of different approaches w.r.t. the number of samples needed to reach a certain accuracy can be given by plots showing the performance of all approaches with the annotation budget set to the size of the whole dataset. We have added such plots for the UCI and checkerboard task, see Sec. A.2 in the revised version of the manuscript,", "title": "Response to AnonReviewer5"}, "Sj9QJXIoPWh": {"type": "rebuttal", "replyto": "SrwFtfy6S0G", "comment": "We are very grateful for the reviewer's comments and the in-detail description of propositions on how to improve our paper. We have addressed these points in the revised version of the manuscript.\n+ \u201ccontribution seems to be in direct contradiction\u201d  \nWe agree with the reviewer, the first statement was misleading and deleted.\n+ \u201creward shaping technique might not be necessary\u201d  \nWe kindly refer to the general response.\n+ \u201cweight of uncertainty sampling being zero\u201d  \nWe corrected it to \u2018 weight of REPRESENTATIVE sampling\u2019.\n+ \u201ccompared against an ensemble of agents with equal weights\u201d  \nWe added such an agent with all weights set to 1 to the plots. The ensemble learning the weights instead performs much better on the UCI and bAbI dataset.\n+ \u201ccompared to RL methods\u201d  \nWe kindly refer to the general response.\n+ \u201cDiscussion of \\beta parameters\u201d  \nWe agree that the question which \\beta parameters are found and how  likely it is that the best parameters are found is an important one. In our view, this question can be broken down into three components: i) What does the objective function of the optimization look like? ii) How does it depend on the size and complexity of the dataset? iii) How good can an optimizer find the minimum of the objective function? We have addressed i) and ii) by generating visualizations of the objective function for the different datasets and discussing properties of it like local minima and saddle points and added them to the revised version in Sec. A.5. To iii): The performance of different optimizers dependent on the qualitative structure of the objective function and the priors set for the parameters is a huge topic in itself. As the objective function only has a few parameters and no local minima, we assume that most black-box optimizers, e.g. also Bayesian Optimization, can find an optimum quite easily. \n+ \u201cDetails of classifiers\u201d  \nWe have added details of the classifiers and the reasoning for choosing them in the supplementary section, Sec. A.1.\n+ \u201cAblation study on a classifier\u201d  \nUnfortunately, we did not find the time to perform these experiments. However, we think it is more important for an active learning agent to perform well with the state-of-the-art classifier for a given task (e.g. a CNN with data augmentation for image classification) than being able to generalise well across different classifiers. We chose such state-of-the-art classifiers for the MNIST and bAbI task. \n+ \u201cEditorial mistakes\u201d  \nThank you very much for pointing them out, we have corrected them. ", "title": "Response to AnonReviewer2"}, "oDwD6JXMuF": {"type": "rebuttal", "replyto": "EBRTjOm_sl1", "comment": "We are deeply grateful for all reviewer's comments and their propositions on how to improve our paper. We have addressed their points in the revised version of the manuscript.\n\nOne point addressed by both reviewer 2 and 3 is that the MDP described includes reward shaping and thus seems to be the basis for a RL method like Q-learning. However we used a Monte Carlo method for finding the best policy, which does not need any intermediate rewards. This was found to be at least irritating. \nLet us explain why we chose this definition of the MDP: We agree that reward shaping is not necessary to learn the ensemble weights. However, it was nonetheless included to allow the reader to compare our MDP to existing MDPs in the literature, like the ones used for Q-Learning (see Sec. 2.2, para 2.2). The MDP we propose is an extension of existing MDPs to batch-mode AL.\n\nBoth reviewer 2 and 3 also requested a comparison of our approach to RL methods like Q-Learning. We have revised Sec. 3.1 accordingly, here a short summary: RL-methods like Q-Learning need to receive a reward after each action taken. Thus, they need a re-training of the supervised learning model every time a sample is added to the batch. The Monte-Carlo approach, however, only needs a re-training after each batch. Thus the computational complexity of Q-Learning in settings with a batch size of b is b times higher than Monte-Carlo methods. Because of this much higher computational complexity we did not include Q-Learning approaches as baselines. \nEven when using Monte-Carlo methods, running a complete active learning episode is still very expensive, as it requires multiple training of a supervised learning model. The high variance of the accuracy makes it additionally necessary to run many episodes to be able to estimate the performance of an approach with sufficient confidence. Thus sample-efficiency is a very important criterion for choosing a suitable RL approach. Monte Carlo with tree parzen estimators fulfils this criterion much better than Monte Carlo policy gradients.\n", "title": "General Response to all Reviewers"}, "hsfApIuEwls": {"type": "rebuttal", "replyto": "YRsNMf-hbPV", "comment": "We are very grateful for the reviewers comments and pointing out additional literature that will make our manuscript even stronger. We have addressed these points in the revised version of the manuscript.\n+ \u201cConnection to RL\u201d  \nWe kindly refer to the general response.\n+ \u201cEarlier work: Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds\u201d  \nWe appreciate pointing out the paper and included in our related work section. The flexibility of our approach would allow to include this paper into our learning framework by adding additional features for gradient diversity for the selection of a sample. Thus we believe the two approaches are orthogonal and can be combined together; rather than contradicting each other.\n+ \u201cEarlier work: Active discriminative network representation learning\u201d  \nWe admit that we have missed that paper and have added it to the related work section in our revised manuscript. We have also added a description of how our work differs from theirs. \n+ Minor comments  \nWe are thankful for them and have revised our manuscript as proposed. \n", "title": "Response to AnonReviewer3"}, "rdWBBYsLDO7": {"type": "rebuttal", "replyto": "gTigTRwAlIb", "comment": "We are very grateful for the reviewers comments and ideas on how to increase the contribution of our paper. We have addressed these points in the revised version of the manuscript.\n+ \u201cno real gap between the ensemble and the best individual approach\u201d  \nWe agree that the ensemble only performed marginally better than the best single approach. Nonetheless, we think that such a marginal performance increase is sufficient for the following two reasons: First, the single best approach out of a set of different approaches is already a very strong benchmark. Second, we cannot know beforehand which approach performs best. Doing so would need a suitable training dataset on which all single heuristics can be tried and the average performance of multiple runs of them has to be taken. Setting up such an evaluation process needs as much effort as our approach, but our approach provides more flexibility and a better performance. \n+ \u201cThe approach is pretty straightforward\u201d  \nWe agree that our approach is not overly complex. Yet, at the same time it outperforms conventional approaches across several experiments and is more interpretable than other approaches building on reinforcement learning. Thus, wie believe the low complexity is a key advantage of our approach. \n+ \u201csome theoretical insights should be given\u201d  \nWe think that our answers to the comments by reviewer 2 already give more insight into the approach. Nonetheless, we are happy for any proposals in which directions more theoretical insight should be provided. \n", "title": "Response to AnonReviewer1"}, "YRsNMf-hbPV": {"type": "review", "replyto": "EBRTjOm_sl1", "review": "Summary of the paper:\nThe paper proposes an algorithm for batch-mode active learning using an ensemble of 4 active learning heuristics. The basic idea is to use an ensemble of heuristics/agents as the utility function to select a batch of samples. The paper proposes to use black-box optimization for optimizing the ratio of combining the agents. The authors perform experiments on various datasets. The results show that the proposed method outperforms the baseline heuristics in most settings, and sometimes performs significantly better.\n\nReview:\n\nI would vote for rejection of this paper. \n\n1. The authors use the whole section 3.1 to describe the MDP formulation of the batch-mode active learning problem. However, the final method is not any RL algorithm but using BO. This is disappointing and also misleading. Why is policy-gradient/Q-learning not used? What will the performance be if we use RL algorithms? \n\n2. The experiments are only comparing to the baseline heuristics, and are missing comparison with other previously proposed learning AL methods as in Section 2.2. This is not the first paper on learning to active learn in batch-mode, see e.g.:\n\t Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford and Alekh Agarwal. Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds. ICLR 2020.\n\n3. The idea of ensembling different heuristics is not new either. For example, this paper uses a multi-armed bandit view to combine heuristics:\n\tGao L, Yang H, Zhou C, Wu J, Pan S, Hu Y. Active discriminative network representation learning. InIJCAI International Joint Conference on Artificial Intelligence 2018 Jan 1.\n\nMinor Comments:\n\n1. Last paragraph on page 4 - it should be $\\binom{n}{b}$ options to choose the batch (the orders do not matter for a batch).\n\n2. Algorithm 2: It is better to replace \"noEpisodes\" with \"numberEpisodes\".", "title": "Missing comparison and connection with prior work", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "gTigTRwAlIb": {"type": "review", "replyto": "EBRTjOm_sl1", "review": "The paper \"Learning Active Learning in the Batch-Mode Setup with Ensembles of Active Learning Agents\" proposes to deal with the problem of active learning via a weighted ensemble of agents. Each agent sequentially selects data to include in the batch to be labelled according to specific heuristics. Finally, the various agents are weighted according to parameters found by a gradient-less approach. \n\nWhile meta-learning of active learning is a very interesting and useful problem, I find the contribution of this paper too weak for a conference as ICLR. The approach is rather straightforward (only a linear combination of different heuristic agents), the experimental results are not fully convincing (there is no real gap between the ensemble and the best individual approach, so using the best agent at training time is maybe a strong alternative) and the paper lacks clarity and details. From my point of view the related work section should shortened to focus on mainly important aspects related to the presented work, a better detailed view (more formalized, less algorithmic) of the approach should be given, and some theoretical insights should be given before it could be considered for publication in a top machine learning conference. \n\n   ", "title": "Not fully convincing ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "DIhD0wdeKAr": {"type": "review", "replyto": "EBRTjOm_sl1", "review": "The authors introduce a novel, batch-mode ensemble approach to Learning to Active Learn that combines the best of both worlds: heuristic- and learning- based active learning. The main idea is to create an ensemble of parametrized active learning agents that will outperform any of the individual agents.\n\nThis is a well-written, easy to follow paper on an important topic. The work appears to be original, and the findings are significant. \n\nThe basic idea of the paper is to combine the-best-of-both-worlds of heuristic- and learning- based active learning. The authors introduce a simple & efficient way to do this. The empirical validation shows that the proposed approach is robust across a variety ML problems. \n\nThe paper would benefit by further strengthening up the empirical validation by answering the following questions:\n- is the proposed approach robust wrt sample size? In other words, what happens when, for all four evaluation tasks, we consider batch sizes of 1/2/48/16/32/64. Most of these graphs could be part of an appendix, with the main paper summarizing the findings\n- how many more labeled examples would take the \"loser\" approaches to reach the accuracy of the \"winning\" one(s)\n- how many more iterations (and at what cost) would the proposed approach reach state-of-the-art accuracy on each evaluation task?     \n\nOTHERS:\n- please run a spell-checker to avoid errors such as \"acitve learning\" (page 2) or \"lineracombination\" (page 8)", "title": "The authors introduce a novel, batch-mode ensemble approach to Learning to Active Learn that combines the best of both worlds: heuristic- and learning- based active learning. The main idea is to create an ensemble of parametrized active learning agents that will perform better than any individual agent.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}