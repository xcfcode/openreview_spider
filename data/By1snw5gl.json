{"paper": {"title": "L-SR1: A Second Order Optimization Method for Deep Learning", "authors": ["Vivek Ramamurthy", "Nigel Duffy"], "authorids": ["vivek.ramamurthy@sentient.ai", "nigel.duffy@sentient.ai"], "summary": "We describe L-SR1, a new second order method to train deep neural networks.", "abstract": "We describe L-SR1, a new second order method to train deep neural networks. Second order methods hold great promise for distributed training of deep networks. Unfortunately, they have not proven practical. Two significant barriers to their success are inappropriate handling of saddle points, and poor conditioning of the Hessian. L-SR1 is a practical second order method that addresses these concerns. We provide experimental results showing that L-SR1 performs at least as well as Nesterov's Accelerated Gradient Descent, on the MNIST and CIFAR10 datasets. For the CIFAR10 dataset, we see competitive performance on shallow networks like LeNet5, as well as on deeper networks like residual networks. Furthermore, we perform an experimental analysis of L-SR1 with respect to its hyper-parameters to gain greater intuition. Finally, we outline the potential usefulness of L-SR1 in distributed training of deep neural networks.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper proposes an interesting approach, in that (unlike many second-order methods) SR1 updates can potentially take advantage of negative curvature in the Hessian. However, all reviewers had some significant concerns about the utility of the method. In particular, reviewers were concerned that the method does not show a significant gain over the Adam algorithm (which is simpler/cheaper and easier to implement). The public reviewer also points out that there are many existing quasi-Newton methods designed for DL, so it is up to the authors to compared to at least one of these. For these reasons I'm recommending rejection at this time."}, "review": {"ryDIFuDIe": {"type": "rebuttal", "replyto": "SyNjWlG4x", "comment": "We think we have provided a sufficient theoretical argument for our approach, and it is not clear to us how this may be improved, given page limit constraints. In our new experiments, we have found the default version of L-SR1 to be slightly better than the default version of Adam, and competitive with the optimized SGD with momentum, for training a deep residual network. While we agree that there is potential for more experimentation, we also believe there is compelling evidence to suggest the usefulness of our approach.\n", "title": "New revision uploaded"}, "S1UNddw8e": {"type": "rebuttal", "replyto": "S1nGIQ-Vl", "comment": "We have addressed the time complexity of our approach. Our new experimental results suggest that our approach is competitive with Adam. O(mn) is the worst case time complexity, and in practice, L-SR1 is not m times slower than SGD. With an optimized implementation, we expect L-SR1 to be about 30% slower. Regarding your comment on second order information being useless when memory size is 2, it may be analytically shown that the Hessian approximation in this case is given by the secant equation, i.e. that it is equal to the ratio of the difference of gradients to the difference of iterates. This is second order information, that is used in the updates. It is not merely a linear combination of past gradients.", "title": "New revision uploaded"}, "HkbvSOv8l": {"type": "rebuttal", "replyto": "rk3f2SyVg", "comment": "\nWe have experimentally compared our approach to Adam. We have also discussed the Hessian free method of Martens and Pearlmutter fast exact multiplication by the Hessian in our literature review. We have added new experimentation that studies mini-batch insensitivity.", "title": "New revision uploaded"}, "Byl34_wLx": {"type": "rebuttal", "replyto": "B19tkwqfg", "comment": "We have contrasted our approach with the approach presented in \"Identifying and attacking the saddle point problem in high-dimensional non-convex optimization\", in the section where we review recent related work.", "title": "New revision uploaded"}, "ByNWr_PIx": {"type": "rebuttal", "replyto": "BJEB-tJmg", "comment": "Worst case time and space complexity is described, and a plot has been remade where x-axis is time and not epochs.", "title": "New revision uplaoded"}, "SyAB4ODUe": {"type": "rebuttal", "replyto": "SJOVb9kXe", "comment": "Thanks again for providing us the references to recent work on stochastic quasi-Newton methods. We did not have the time to implement, test and compare these approaches to our own, but we did cite the literature in our new draft, which we have just uploaded. One of our primary claims in this paper is that the indefinite Hessian approximation can help navigate saddle points better. In this regard, it was more important for us to compare our approach with more widely used first order methods. We hope to compare our approach to the recent stochastic second order methods in future.\n\nWe have changed our language from stochastic second order methods 'require larger minibatches' to 'can benefit from larger minibatches'.\n\nWe changed \"Finally, popular quasi-Newton approaches such as L-BFGS, require line search to make parameter updates, which requires many more gradient and/or function evaluations.\" to \"Finally, popular quasi-Newton approaches such as L-BFGS (in their default form), require line search to make parameter updates, which requires many more gradient and/or function evaluations.\"\n\nWe measured the frequency of skipped updates for L-SR1 and had some interesting observations from our experiments. To summarize, in most cases, updates were never skipped, and if they were, it was less than 2.5% of the time. We did notice an abnormally high proportion of skipped updates for the case of MNIST with batch normalization (ranging from 7% to higher than 60%), which somehow did not seem to affect performance adversely. This warrants future investigation.", "title": "A few comments"}, "BJX2791Xx": {"type": "rebuttal", "replyto": "B19tkwqfg", "comment": "1. Our current L-SR1 implementation is not optimized and we will add wallclock time shortly. But essentially it requires one more function calculation (about 30% of the time for a gradient calculation) each iteration. So we expect an optimized L-SR1 implementation to take 30% more time than SGD like methods.\n\n2. We will contrast our approach with the approach presented in \"Identifying and attacking the saddle point problem in high-dimensional non-convex optimization\", in our next revision.", "title": "Response to 'Clarifications'"}, "SJOVb9kXe": {"type": "rebuttal", "replyto": "ByaVQ4R-x", "comment": "Thank you very much for providing us the references for other recent work on stochastic quasi-Newton methods. We are in the processing of reviewing the references. We intend to implement at least one of the approaches suggested, and compare its performance with our approach. \n\nRegarding your comment on the convergence of SR1 matrices, we did not mean to imply that SR1 'must' perform better than BFGS, but that it 'would potentially' or 'could'. We will modify the language to clearly reflect this.  \n\nThanks for your comment on larger mini-batches and function evaluations. We will alter the language to be consistent with recent work.\n\nThank you for your suggestion on skipping updates. We will review the frequency of updates and report the same in our next revision.\n\nRegarding the minor issues with the numerical results:\n- For the case of the LeNet like networks, we chose arbitrary small values of mini-batch sizes for the first order methods, and larger values for  the second order methods. For the case of residual networks on CIFAR10, we chose the same mini-batch size of 128 that was used by He et al. in their paper 'Deep residual learning for image recognition'.\n- The initial SR1 matrix was always the identity matrix, i.e. \\gamma = 1. We followed the typical convention followed for L-BFGS, where the initial inverse Hessian approximation H_0 is set to the identity matrix. \n- The function values you see at the beginning of each experiment, are in fact the function values at the end of the first epoch (epoch 0 in our case). Furthermore, the network parameters were randomly initialized for the LeNet like network experiments, and they were initialized according to  'Delving deep into rectifiers: Surpass- ing human-level performance on imagenet classification' by He et al., for the residual network experiments. For these reasons, the function values at the beginning of each experiment are not the same across all methods.", "title": "Response to 'A few comments'"}, "BJEB-tJmg": {"type": "review", "replyto": "By1snw5gl", "review": "Please describe the time and space complexity of your approach \nand remake some plot where x-axis shows time and not epochs.L-SR1 seems to have O(mn) time complexity. I miss this information in your paper. \nYour experimental results suggest that L-SR1 does not outperform Adadelta (I suppose the same for Adam). \nGiven the time complexity of L-SR1, the x-axis showing time would suggest that L-SR1 is much (say, m times) slower. \n\"The memory size of 2 had the lowest minimum test loss over 90\" suggests that the main driven force of L-SR1 \nwas its momentum, i.e., the second-order information was rather useless.", "title": "complexity", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1nGIQ-Vl": {"type": "review", "replyto": "By1snw5gl", "review": "Please describe the time and space complexity of your approach \nand remake some plot where x-axis shows time and not epochs.L-SR1 seems to have O(mn) time complexity. I miss this information in your paper. \nYour experimental results suggest that L-SR1 does not outperform Adadelta (I suppose the same for Adam). \nGiven the time complexity of L-SR1, the x-axis showing time would suggest that L-SR1 is much (say, m times) slower. \n\"The memory size of 2 had the lowest minimum test loss over 90\" suggests that the main driven force of L-SR1 \nwas its momentum, i.e., the second-order information was rather useless.", "title": "complexity", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B19tkwqfg": {"type": "review", "replyto": "By1snw5gl", "review": "Dear Authors,\n\n1) How does the experimental results look in terms of the wallclock time?\n2) The generalized trust region method that overcomes the saddle point problem is presented in the work \"Identifying and attacking the saddle point problem in\nhigh-dimensional non-convex optimization\", where they avoid expensive Hessian computations by approach similar to Krylov subspace descent. This work has to be contrast in more details with your approach. Could you do that?The paper proposes a new second-order method L-SR1 to train deep neural networks. It is claimed that the method addresses two important optimization problems in this setting: poor conditioning of the Hessian and proliferation of saddle points. The method can be viewed as a concatenation of SR1 algorithm of Nocedal & Wright (2006) and limited-memory representations Byrd et al. (1994). First of all, I am missing a more formal, theoretical argument in this work (in general providing more intuition would be helpful too), which instead is provided in the works of Dauphin (2014) or Martens. The experimental section in not very convincing considering that the performance in terms of the wall-clock time is not reported and the advantage over some competitor methods is not very strong even in terms of epochs. I understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement L-SR1 to train their deep models? The work is not ready to be published.", "title": "Clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyNjWlG4x": {"type": "review", "replyto": "By1snw5gl", "review": "Dear Authors,\n\n1) How does the experimental results look in terms of the wallclock time?\n2) The generalized trust region method that overcomes the saddle point problem is presented in the work \"Identifying and attacking the saddle point problem in\nhigh-dimensional non-convex optimization\", where they avoid expensive Hessian computations by approach similar to Krylov subspace descent. This work has to be contrast in more details with your approach. Could you do that?The paper proposes a new second-order method L-SR1 to train deep neural networks. It is claimed that the method addresses two important optimization problems in this setting: poor conditioning of the Hessian and proliferation of saddle points. The method can be viewed as a concatenation of SR1 algorithm of Nocedal & Wright (2006) and limited-memory representations Byrd et al. (1994). First of all, I am missing a more formal, theoretical argument in this work (in general providing more intuition would be helpful too), which instead is provided in the works of Dauphin (2014) or Martens. The experimental section in not very convincing considering that the performance in terms of the wall-clock time is not reported and the advantage over some competitor methods is not very strong even in terms of epochs. I understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement L-SR1 to train their deep models? The work is not ready to be published.", "title": "Clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByaVQ4R-x": {"type": "rebuttal", "replyto": "By1snw5gl", "comment": "This is a very interesting paper and the results look interesting. The paper is missing references for a large body of stochastic quasi-Newton methods and at times, is a bit misleading with its claims. \n\nReferences:\nA Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (ICML, 2016): http://jmlr.org/proceedings/papers/v48/curtis16.html\nadaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (ECML, 2016): http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1 \nStochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization: https://arxiv.org/abs/1412.1196\n\nThese are specifically for deep learning/non-convex optimization. There are other methods like oLBFGS, RES (Mokhtari et. al.) and SQN (Byrd et. al.) for convex problems. The first and second references above have numerical experiments where they show that the L-BFGS variants work well as compared to first-order methods specifically for deep learning tasks. The authors should have opted for either of these three papers for their baselines instead of the older \u201cforgetting\u201d-based approach from 2011 since they improve upon this work and are more up-to-date with deep learning trends/heuristics. \n\nOther comments: \nOn Page 3, the authors claim \u201c...suggest that the approximate Hessian matrices generated by the SR1 method show faster progress towards the true Hessian than those generated by BFGS. This suggests that a limited memory SR1 method (L-SR1, if you like) would potentially outperform L-BFGS in the task of high dimensional optimization in neural network training.\u201d The argument that SR1 matrices better converge to the true Hessian is understood but the implication that it must hence perform better is misleading. BFGS updating has strong properties about self-correction (even in the stochastic case as in the first reference above) and superlinear convergence which are not true for SR1 (see for e.g., pp. 160-161 of Nocedal and Wright). \n\nOn Page 1, the authors claim that \u201cDue to their use of curvature information, they can often find good minima in far fewer steps than first order methods such as stochastic gradient descent (SGD). However, stochastic second order methods typically require larger mini-batches (Le et al., 2011)\u201d. This is not true. Most of the recent algorithms leveraging stochastic quasi-Newton framework (see above) operate reasonably well with batch-sizes similar to SGD. Similarly, the authors claim that \u201cFinally, popular quasi-Newton approaches such as L-BFGS, require line search to make parameter updates, which requires many more gradient and/or function evaluations.\u201d. With the newer body of work, this is not true and a constant or diminishing step-size rule often suffices both practically and for theoretical guarantees.\n \nOn Page 3, for instance, they claim that \u201c...SR1 performs well by skipping the update if the denominator is small, which does not occur very often anyway\u201d. While it is true that skipping for SR1 is less disastrous than skipping for BFGS and that generally is less frequent, maybe the authors could add details about how frequently they skipped updates for their MNIST/CIFAR runs. The reason for this is that if the algorithm skips continuously (or for a majority of the duration), the added cost is not justified given that the performance is similar to NAG. \n\nThere are a few minor issues with the numerical results. \n-The authors fail to explain how the batch sizes were chosen in the experiments conducted for the methods they compare against.\n-What was the initial SRI matrix used? In Algorithm 1, the authors mention \\gamma I, how was gamma chosen?\n-Why are the function values at the beginning of each experiment not the same across all methods?", "title": "A few comments"}}}