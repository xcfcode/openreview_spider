{"paper": {"title": "AE-OT: A NEW GENERATIVE MODEL BASED ON EXTENDED SEMI-DISCRETE OPTIMAL TRANSPORT", "authors": ["Dongsheng An", "Yang Guo", "Na Lei", "Zhongxuan Luo", "Shing-Tung Yau", "Xianfeng Gu"], "authorids": ["doan@cs.stonybrook.edu", "yangguo@cs.stonybrook.edu", "nalei@dlut.edu.cn", "zxluo@dlut.edu.cn", "yau@math.harvard.edu", "gu@cs.stonybrook.edu"], "summary": "", "abstract": "Generative adversarial networks (GANs) have attracted huge attention due to\nits capability to generate visual realistic images. However, most of the existing\nmodels suffer from the mode collapse or mode mixture problems. In this work, we\ngive a theoretic explanation of the both problems by Figalli\u2019s regularity theory of\noptimal transportation maps. Basically, the generator compute the transportation\nmaps between the white noise distributions and the data distributions, which are\nin general discontinuous. However, DNNs can only represent continuous maps.\nThis intrinsic conflict induces mode collapse and mode mixture. In order to\ntackle the both problems, we explicitly separate the manifold embedding and the\noptimal transportation; the first part is carried out using an autoencoder to map the\nimages onto the latent space; the second part is accomplished using a GPU-based\nconvex optimization to find the discontinuous transportation maps. Composing the\nextended OT map and the decoder, we can finally generate new images from the\nwhite noise. This AE-OT model avoids representing discontinuous maps by DNNs,\ntherefore effectively prevents mode collapse and mode mixture.", "keywords": ["Generative model", "auto-encoder", "optimal transport", "mode collapse", "regularity"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors present a different perspective on the mode collapse and mode mixture problems in GAN based on some recent theoretical results. \n\nThis is an interesting work. However, two reviewers have raised some concerns about the results and hence given a low rating of the paper. After reading the reviews and the rebuttal carefully I feel that the authors have addressed all the concerns of the reviewers. In particular, at least for one reviewer I felt that there was a slight misunderstanding on the reviewer's part which was clarified in the rebuttal. The concerns of R1 about a simpler baseline have also been addressed by the authors with the help of additional experiments. I am convinced that the original concerns of the reviewers are addressed. Hence, I recommend that this paper be accepted. \n\nHaving said that, I strongly recommend that in the final version, the authors should be a bit more clear in motivating the problem. In particular, please make it clear that you are only dealing with the generator and do not have an adversarial component in the training. Also, as suggested by R3 add more intuitive descriptions to make the paper accessible to a wider audience.\n\n"}, "review": {"S1ew3XnmsS": {"type": "rebuttal", "replyto": "SylWjepycS", "comment": "\n\n----------------------------\nQ1: My concern is whether the proposed method is overkill because the singular point detection can\nbe very tricky and relies on heavy linear programming.\n\nAnswer: The detection of singularities is direct and simple, for the convex polyhedron of the Brenier\npotential, just compute the inner product of the normals to each pair of adjacent facets. If the inner\nproduct is too big, then the projection of the intersection between the facets is in the singularity set.\nSo this work doesn\u2019t involve any linear programming at all.\n\n----------------------------\nQ2: Could you explain why not using the following substitute: Step 1. Fit an auto-encoder just as\nyou did in the paper and get an empirical distribution \u03bd. Step 2. Fit a Gaussian mixture model on \u03bd\nand do model selection over # clusters. Step 3. Sample from the Gaussian mixture model to generate\nfresh images.\n\nAnswer: We thank Reviewer #1 for the suggestions. The proposed approach is inspiring, but it has\npotential drawbacks:\n\u2022 If the empirical distribution has only one mode, but the support is concave, then the proposed\nmethod still can not avoid generating unrealistic samples.\n\u2022 If the empirical distribution has multiple modes, the resulting Gaussian mixture will fill\nthe gaps among the modes, therefore the proposed method still can not avoid generating\nunrealistic samples (mode mixture).\n\u2022 Fitting Gaussian mixture itself is expensive and without further assumptions, the convergence\nof the GMM fitting cannot be guaranteed.\n\nIn order to show the above claims, we did the following experiments: firstly we fit the 60K latent code\nof MNIST dataset by GMM, with the number of modes set to be 10, 30, 100. Then t-SNE is used to\nvisualize the data. The blue crosses are the generated data by the GMM model and the green circles\nare the training data. In the anonymous website https://drive.google.com/file/d/12HbiQNAoTpxnk-h10LY0O90j8QhSIKqw/view?usp=sharing, we provide the results: Fig. (a)(b)(c) show the generation results of GMM,\nfrom which we can see that there are huge number of generated samples in the regions among the\nmodes. While for the proposed method, as shown in Fig. (d), nearly no generated samples fall into\nthe gaps.\n\n----------------------------\nQ3: Since this method relies on a high-quality auto-encoder model, it is hard to say this paper\nmakes progress in fixing the GAN\u2019s mode collapsed problem. Besides, the paper does not involve an\nadversarial training module. So I will not treat it as a satisfactory improvement over GAN. Overall,\nthe proposed problem in GAN indeed exists. But the solution seems to deviate from the goal the\npaper aim to achieve.\n\nAnswer: The real goal of this work is to tackle mode collapse and mode mixture problems in general generative models, not only for GANs. Our work targets at analysis and improvement of generators in generic generative models, including VAEs and GANs. In fact, generators in these models tend to map a unimodal Gaussian to the complex data distribution, which will inevitably encounter the singularity problem proposed in our work. We thank the reviewer #1 for pointing out the ambiguity of our motivation. We have revised our abstract and introduction parts, which illustrate that the proposed AE-OT model solves the discontinuity problems encountered by both GANs and VAEs. Actually, in the original version of our paper, we have reviewed all the DNN based generative models in the related work part, and made comparisons with GANs, VAEs and other generated models in the experiment part. \nAccording to Figali\u2019s Fields medal work, it shows the intrinsic reason for mode collapse is the discontinuity of transportation map, caused by the concavity of the support of the data distributions. Based on this theoretic discovery, the AE-OT model is proposed. This model is not a conventional GAN model, but a novel generative model that exactly solves the main problems we are targeting at.\n", "title": "Response to Reviewer #1"}, "BkxxhH_wjS": {"type": "rebuttal", "replyto": "ryetcUV7iH", "comment": "\n----------------------------\nQ1:  I have some doubts about moving from the \"semi-discrete OT map\" to the piece-wise linear extension.   The illustration in Fig. 3,  and implicit in all the explanation charts is the fact that discontinuity can be found by a linear separation. This seems to be an extremely simplifying assumption, which leads to not so great visual results from the paper.\n\nAnswer: Here we want to clarify that singular set detection is *piece-wise linear* separation, rather than *linear* separation. In Fig. 3(a), the singular set (shown in red lines) is illustrated by a piece-wise linear curve. Also, Fig. 6 of the appendix shows another example with the numerically computed singular set (also piece-wise linear) by our method, and it is much more curved and complicated.\n\n----------------------------\nQ2: Although the numerical results seems promising, I feel that fewer images, but larger in size, and analysis of mode collapse phenomenon in real images would have been much better.\n\nAnswer:  As shown in the the last paragraph of Section 4.1, we conducted experiments of mode collapse on real images like stacked MNIST and CelebA on Section C.3 and C.4 of the appendix.\n\n----------------------------\nQ3:  Singular set detection seems to be the most tricky part in this paper, the Simplex projection assumption, renders this part not that tricky, but that is where I feel the biggest doubt about this paper lies.\n\nAnswer: (1) There is no simplex projection assumption in our paper. In fact, Fig.3(a) illustrates the Brenier potential and the corresponding power diagram. The upper hyperplane envelope in top of Fig.3(a) is the graph of Brenier potential, and the bottom of Fig. 3(a) shows the source domain of the Brenier potential, expressed as a cell decomposition structure. Each facet in the image of Brenier potential corresponds to a cell in the source domain (\u2126), and the ridges on the image of Brenier potential corresponds to edges of cells in the source domain.\n\n(2) In the image of the Brenier potential, the \"sharp ridges\" are composed of the edges where the angles between the corresponding pairs of adjacent facets are large (as shown in Fig.3(a)). In fact, the normal of a facet n= (p_1, p_2,..., p_d, \u22121) actually corresponds to a latent code y= (p_1, p_2,..., p_d). And the large angle between two adjacent facets means that the distance between the corresponding latent codes is large.  This often happens when the codes come from different modes.  Thus, the singular set, or equivalently the \"sharp ridges\" gives the information about different modes.\n\n(3) Singular set detection is proposed in our paper for the following reason. Firstly, the singular set is totally decided by the semi-discrete OT map, or equivalently, the Brenier potential (Fig.  3(a)). Secondly, the image of the semi-discrete OT map itself is the given discrete latent code, thus we extend it with a piece-wise linear manner, so that the extended OT map can be used to *generate new codes* (Fig. 3(b)). Thirdly, the samples around the singular set will be mapped to the gaps among the modes by our extended OT map and cause the mode mixture problem, thus the singular detection is needed. Finally, given a sample x, if it falls around the singular set (checked by Alg. 2), we just don\u2019t use it to generate new latent code.\n\n\n", "title": "Response to Reviewer #4"}, "SkgkludPiH": {"type": "rebuttal", "replyto": "SJx3_vOPiH", "comment": "\n----------------------------\nQ5: The authors themselves mention the need for a high quality auto encoder model to encode celebA dataset, which has been improved upon by numerous other papers, the claims seems not too strong. Also, the method does not have any adversarial training and hence, it studies the GAN idea from only fixing the generator point of view.\n\nAnswer: The main goal of this work is to tackle mode collapse and mode mixture problems in general generative models, not only for GANs. Our work targets at analysis and improvement of generators in generic generative models, including VAEs and GANs. In fact, generators in these models tend to map a unimodal Gaussian to the complex data distribution, which will inevitably encounter the singularity problem proposed in our work. We thank the reviewer #4 for pointing out the ambiguity of our motivation. We have revised our abstract and introduction parts, which analyze the discontinuity problems encountered by GANs and VAEs. Then we propose a new generative model called AE-OT. Actually, in the original version of our paper, we have reviewed all the DNN based generative models in the related work part, and made comparisons with GANs, VAEs and other generated models in the experiment part.\n\nBecause our main focus is to solve mode collapse/mixture problems, we didn\u2019t apply the most\nadvanced auto-encoder (AE). If the capacity of AE is insufficient, the result is not satisfying, such as\nthe celebA dataset noticed by the reviewer. But, as we explained in section 4.2, the 3rd paragraph, if\nthe capacity of AE is sufficient, our model outperform others.\n\nAs recent GAN improvements mostly focus on the discriminator, our work complements these\nworks by critically analyzing and making improvement on the generator. Future research on adding\nadversarial loss to our current model is also intriguing.\n", "title": "Response to Reviewer #4 "}, "SJx3_vOPiH": {"type": "rebuttal", "replyto": "BkxxhH_wjS", "comment": "\n----------------------------\nQ4: Singular set detection seems to be the most tricky part in this paper, which should have been explained further.\n\nAnswer: In the following, we justify our algorithm using the theoretic works summarized in the following book:\nFigalli, A. (2017). The Monge\u2013Amp\u00e8re equation and its applications.\n\nAccording to Brenier\u2019s theorem, the optimal transportation map T is the gradient of the convex Brenier potential u, and u satisfies the Monge-Amp\u00e9re equation.\nIn his book, the Fields medalist Figalli proved the existence and the uniqueness of the solution to the\nMonge-Ampere equation in Chapter 2, where he used Alexandrov\u2019s approach:\n1. Approximate the data distribution \u03bd to a sequence Dirac distributions \u03bdn, such that the sequence of {\u03bd_n} weakly converges to \u03bd;\n2. For each Dirac measure \u03bd_n, there exists an Alexandrov\u2019s solution u_n, which is exactly the discrete Brenier potential in our paper;\n3. The weak solutions {u_n} converges to the real solution u, u is C^1 almost everywhere, except at the singular set.\n\nOur Semi-Discrete OT algorithm is completely equivalent to Alexandrov\u2019s solution. In fact, the proof\nin Figalli\u2019s book is not constructive, (Alexandrov\u2019s original proof is based on Algebraic topology), which doesn\u2019t induce an computational algorithm. Therefore, the theorem 2 in the Appendix gives a variational framework to explicitly compute the discrete Brenier potential. By Figalli\u2019s work, the discrete Brenier potential {u_n} converges to the smooth Brenier potential, which is C^1 except at the singular set. The piece-wise linear map in Fig.3(a) converges to the real optimal transportation map.\n\nThe singular set is the non-differentiable points (only C^0 but not C^1) of the Brenier potentials, namely the ridges of the graph of u. This ridge structure becomes prominent and well-preserved in\nthe process of approximating u by piece-wise linear polyhedra {u_n} in Fig.3(a).\n\nCompared to Fig.3, Fig. 6 and Fig. 7 in the appendix gives better illustration for the singularity. The\noriginal version of Fig. 6 is given by Figalli as the Fig. 3.2 in the following article,\nFigalli, A. (2010). Regularity properties of optimal maps between nonconvex domains in the plane.\nCommunications in Partial Differential Equations, 35(3), 465-479.\n\nWe can see that the singular set has complicated geometric and topological structures, which can not\nbe captured by linear separation, but still can be found by *piece-wise linear approximation*. In\nfact, the optimal transport map shown in Fig. 6 is numerically computed by our algorithm, and the\nsingular set is piece-wise linear, approximating the singular set in the smooth case (shown as Fig. 3.2\nin the above mentioned article).\n\nNext we show that the singular set structure of the smooth Brenier potential is well preserved by\nour SDOT map. From chapter 2 in Figalli\u2019s book, we know that the piece-wise linear functions un\n(discrete Brenier potential) converges to the real smooth Brenier potential u, which is C^1\neverywhere except at the singular points. Therefore the graph of the smooth Brenier potential has ridges, these ridge structure are well preserved during the piece-wise linear approximation by the discrete Brenier\npotential. \n\nTherefore, singularity detection boils down to locate the ride structure of the graph of the discrete\nBrenier potential, which is a convex polyhedron. The ridge on a convex polyhedron can be easily\nfound by computing the angles between each pair of adjacent facets (dihedral angles for 2D case).\nBecause the discrete Brenier potential is convex, its projection induces a power diagram, each cell is\nconvex. The dual of this power diagram gives the power Delaunay triangulation of training samples\n(y_i\u2019s) (Fig. 2 of the following article). This geometric interpretation of semi-discrete OT doesn\u2019t\nrequire the linear separation assumption. The relation among discrete Brenier potential, power\ndiagram and power Delaunay triangulation is explained in details in\nGu, X., Luo, F., Sun, J., & Yau, S. T. (2016). Variational principles for Minkowski type problems,\ndiscrete optimal transport, and discrete Monge\u2013Amp\u00e8re equations. Asian Journal of Mathematics,\n20(2), 383-398. ", "title": "Response to Reviewer #4 "}, "Syeh0W3Xor": {"type": "rebuttal", "replyto": "r1e5aB-z9r", "comment": "\n\n----------------------------\nQ1: Although this paper brings a new perspective, based on optimal transport theory, as far as I can\nunderstand this paper does not establish formal new results. Thus I think some strong claims about\nproviding deep theoretical explanation should be more moderate. In essence, it seems that the paper\nverifies *numerically* (in section B.3) that Figalli\u2019s theorem (stated in Appendix B) holds in this\ncontext.\n\nAnswer: This work focuses on using Figalli\u2019s regularity theory of Optimal Transportation Map to\nexplain mode collapse/mixture in generative models and propose a novel model to tackle it, not to\ndevelop the new regularity theorems. We will follow the reviewer\u2019s suggestion to make our claims\nmore moderate.\n\n----------------------------\nQ2: This is just a suggestion. I think in some parts a lighter notation and a more intuitive explanation\ncould help.\n\nAnswer: We will follow reviewer\u2019s suggestion to add more intuitive explanations and simplify the\nnotations.\n\n----------------------------\nQ3: After Eq. (5) in the Appendix the authors mention Newton\u2019s method, and Thm 3 is also specific\nto Newton\u2019s method. Then they mention that *Gradient Descent* is used (and in the main part of the\npaper they mentioned Adam). This is confusing. All these algorithms are different, and Newton\u2019s\nmethod does not imply convergence results for gradient descent. I don\u2019t see how Thm 3 is relevant.\n\nAnswer: According to the variational framework of semi-discrete optimal transportation map,\ntheorem 2, the computation of OT map is reduced to a convex optimization. Hence both gradient\ndescend and Newton\u2019s method converge. We will modify Thm 3 accordingly. Furthermore, this work\nfocuses on gradient descend method, in the future work, we will explore Newton\u2019s method as well.\n\n----------------------------\nQ4: This is a simple doubt. To avoid non-differentiability of the gradient, the OT step computes the\nBrenier potential and is able to locate the singularities. I wonder if using a simpler approach through\noptimization for nosmooth problems (such as Moreau envelopes or proximal methods) could resolve\nthis issue? In the negative case, why not?\n\nAnswer: Although the OT map is discontinuous, and the Brenier potential is non-differentiable, the \nenergy to be optimized is C^2 in terms of h. Therefore, in the current work, for the optimization\npurpose it is unnecessary to use Moreau envelope or proximal methods. Specifically, the convex\nenergy E(h) we aim to optimize is differentiable with respect to h. With the optimal h, the OT map\ncan be induced. Therefore for the optimization, we actually do not need smoothing techniques to carry\nout the optimization. Secondly, the non-differentiability of Brenier\u2019s potiential uh(x) is considered\nwith respect to x given the optimal h. This is independent of the optimization process.\n\n----------------------------\nQ5: Some Minor comments: 1. Define OT in the abstract (Optimal Transportation?) 2. What is AE?\n(not defined also; Auto Encoder?) 3. There are lots of typos through the text, such as missing \"the\",\n\"a\", etc. and a couple mispelled words. I suggest the authors proofread the draft more carefully. 4.\npp. 4 ... what is a \"PL convex function\". PL is not defined.\n\nAnswer: We thank the reviewer 3 for the comments. In the paper, OT represents optimal transport,\nAE means autoencoder and PL is the abbreviation of piece-wise linear. We will add more explains to\nthe abbreviations and find native speakers to help proofread the updated manuscript.\n\n", "title": "Response to Reviewer #3"}, "SylGfl3XiH": {"type": "rebuttal", "replyto": "HkldyTNYwH", "comment": "We thank reviewers for carefully examine our work in such a short time.  Since our work involvesnon-trivial theories from optimal transportation, such as the brand new theorems of Figalli, andregularity theorems for Monge-Ampere equation, the review requires huge amount of efforts.  Wedeeply appreciate all reviewers from deep of our hearts. Since all the reviews and rebuttals will bepublic online, we prepared our rebuttal with great caution, and addressed all the questions raised byreviewers carefully", "title": "Thanks for your careful comments"}, "ryetcUV7iH": {"type": "review", "replyto": "HkldyTNYwH", "review": "This paper deals with an important problem of mode collapse and mode mixture. In order to\ntackle the both problems, the paper proposes to separate the manifold embedding and the\noptimal transportation problems; the first part being carried out using an autoencoder to map the\nimages onto the latent space and the second part is accomplished using a GPU-based\nconvex optimization to find the discontinuous transportation maps.\n\nI have some doubts about moving from the \"semi-discrete OT map\" to the piece-wise linear extension. The illustration in Fig. 3, and implicit in all the explanation charts is the fact that discontinuity can be found by a linear separation. This seems to be an extremely simplifying assumption, which leads to not so great visual results from the paper. Although the numerical results seems promising, I feel that fewer images, but larger in size, and analysis of mode collapse phenomenon in real images would have been much better.\n\nSingular set detection seems to be the most tricky part in this paper, which should have been explained further. The Simplex projection assumption, renders this part not that tricky, but that is where I feel the biggest doubt about this paper lies.\n\nThe authors themselves mention the need for a high quality auto encoder model to encode celebA dataset, which has been improved upon by numerous other papers, the claims seems not too strong. Also, the method does not have any adversarial training and hence, it studies the GAN idea from only fixing the generator point of view. ", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 3}, "SylWjepycS": {"type": "review", "replyto": "HkldyTNYwH", "review": "Contributions:\n1. This paper proposes a new problem in GAN distribution mapping: the concavity of support problem.\n2. This paper provides a solution to the concave support together with mode collapsed problem in GAN, via a discrete-continuous optimal transport model, given some post-processing techniques to rule out \"singular points\".\n3. Empirical results show the effectiveness of the proposed method.\n\nTo summarize their method. First, they fit a good auto-encoder model to get embeddings for the observed data as an empirical distribution \\nu on space Z. Second, they use a semi-discrete OT to map a noise distribution \\mu to \\nv. Since OT will be aware of all modes in \\nu, singular points can be detected by checking the angle between \"shards\" and those points that are around the \"ridge\" should be rejected. Thus, the proposed method could handle both the concave support problem and the mode collapse problem.\n\nMy concern is whether the proposed method is overkill because the singular point detection can be very tricky and relies on heavy linear programming. Could you explain why not using the following substitute: \nStep 1. Fit an auto-encoder just as you did in the paper and get an empirical distribution \\nu.\nStep 2. Fit a Gaussian mixture model on \\nu and do model selection over # clusters.\nStep 3. Sample from the Gaussian mixture model to generate fresh images.\n\nSince this method relies on a high-quality auto-encoder model, it is hard to say this paper makes progress in fixing the GAN's mode collapsed problem. Besides, the paper does not involve an adversarial training module. So I will not treat it as a satisfactory improvement over GAN. Overall, the proposed problem in GAN indeed exists. But the solution seems to deviate from the goal the paper aim to achieve.", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 2}, "r1e5aB-z9r": {"type": "review", "replyto": "HkldyTNYwH", "review": "General Comments:  The generator in Generative Adversarial Networks (GANS) computes an optimal transportation from the noise distribution to the data distribution.  However, such maps are in general discontinuous.  Since deep neural networks can only represent continuous maps, this brings two problems: mode collapse and mode mixture. This paper approaches both problems using Figalli's regularity theory. They separate the manifold embedding (here an autoencoder maps input data to a latent space) from the optimal transportation (this map is found by convex optimization). Composing these two steps yields the proposed method. Their method basically avoids representing discontinuous maps by the generator. Empirically, the proposed method performs similar or better than state-of-the-art.\n\nI think the idea of the paper is nice, and an interesting perspective  on GANs is presented. A new method is proposed. The numerical contributions are certainly significant. Therefore, I believe the paper deserves publication.\n\nNevertheless, I have some comments below.\n\n1) Although this paper brings a new perspective, based on optimal transport theory, as far as I can understand this paper does not establish formal new results. Thus I think some strong claims about providing deep theoretical explanation should be more moderate. In essence, it seems that the paper verifies *numerically* (in section B.3) that Figalli's theorem (stated in Appendix B) holds in this context.\n\n2) This is just a suggestion. I think in some parts a lighter notation and a more intuitive explanation could help.\n\n3) After Eq. (5) in the Appendix the authors mention Newton's method, and Thm 3 is also specific to Newton's method. Then they mention that *Gradient Descent* is used (and in the main part of the paper they mentioned Adam). This is confusing. All these algorithms are different, and Newton's method does not imply convergence results for gradient descent. I don't see how Thm 3 is relevant.\n\n4) This is a simple doubt. To avoid non-differentiability of the gradient, the OT step computes the Brenier potential and is able to locate the singularities. I wonder if using a simpler approach through optimization for nosmooth problems (such as Moreau envelopes or proximal methods) could resolve this issue? In the negative case, why not?\n\n5) Some Minor comments:\n1. Define OT in the abstract (Optimal Transportation?) \n2. What is AE? (not defined also; Auto Encoder?)\n3. There are lots of typos through the text, such as missing \"the\", \"a\", etc. \nand a couple mispelled words. I suggest the authors proofread the draft\nmore carefully.\n4. pp. 4 ... what is a \"PL convex function\". PL is not defined.\n", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 2}}}