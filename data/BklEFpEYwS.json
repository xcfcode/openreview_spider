{"paper": {"title": "Meta-Learning without Memorization", "authors": ["Mingzhang Yin", "George Tucker", "Mingyuan Zhou", "Sergey Levine", "Chelsea Finn"], "authorids": ["mzyin@utexas.edu", "gjt@google.com", "mingyuan.zhou@mccombs.utexas.edu", "svlevine@eecs.berkeley.edu", "cbfinn@cs.stanford.edu"], "summary": "We identify and formalize the memorization problem in meta-learning and solve this problem with novel meta-regularization method, which greatly expand the domain that meta-learning can be  applicable to and effective on.", "abstract": "The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes.  This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.\u00a0", "keywords": ["meta-learning", "memorization", "regularization", "overfitting", "mutually-exclusive"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "The paper introduces the concept of overfitting in meta learning and proposes some solutions to address this problem. Overall, this is a good paper. It would be good if the authors could relate this work to meta learning approaches, which are based on hierarchical (Bayesian) modeling for learning a task embedding.\n\n[1] Hausman et al. (ICLR 2018): Learning an Embedding Space for Transferable Robot Skills \nhttps://openreview.net/pdf?id=rk07ZXZRb\n[2] Saemundsson et al. (UAI 2018): Meta Reinforcement Learning with Latent Variable Gaussian Processes\nhttp://auai.org/uai2018/proceedings/papers/235.pdf\n"}, "review": {"DqG6VikXOm": {"type": "rebuttal", "replyto": "UT5c0E6gOU", "comment": "Thanks for the positive feedback!\n\nTo answer your question:\nOur derivation builds upon the Bayesian meta-learning framework, but the practical algorithm is not specific to Bayesian methods and can be applied to a wide range of methods. Further, Bayesian meta-learning methods, including the referenced papers, do not solve the fundamental memorization problem that we uncover and analyze.\n\nLastly, we'd like to point out that the form of overfitting that we analyze is not the only form of overfitting that can occur. The more obvious form of overfitting arises from overfitting the acquired adaptation procedure to the training tasks, while the version of overfitting we analyze results in no adaptation procedure at all.", "title": "Response to the meta-review"}, "HkgyWzXhFH": {"type": "review", "replyto": "BklEFpEYwS", "review": "\nSummary:\n\nIn this paper, the authors propose a new method to alleviate the effect of meta over-fitting. The designed method is based on the information-theoretic meta-regularization objective. Experiments demonstrate the effectiveness of the proposed model.\n\nStrong Points:\n\n+ The authors aim to alleviate the effect of meta over-fitting. In this paper, they mainly focus on alleviating the effect of brute-force memorization in the meta-training process. The problem is important in the meta-learning field.\n\n+ The motivation for this paper is clear. The authors try to maximize the mutual information between x*, \\theta and \\bar{y}^*, D. \n\n+ Experiments on both sinusoid regression, pose regression and image classification show that MR-MAML outperforms MAML and MR-CNP outperforms CNP. \n\nWeak Points:\n\n- My first concern is about the novelty of the proposed model. The framework and the derivations are straightforward. I think the problem is very important, however, the technical contribution may not enough to be accepted. It is better for the authors to clarify their contributions.\n\n- It will be more helpful if the authors can describe the algorithm of the meta-testing process in Appendix A.1. In the meta-testing process, do we need to sample \\theta from q(\\theta|\\tau)? If so, is the accuracy calculated by the averaged value of tasks with sampled weight?\n\n- I am a little curious about the results in Table 5. The results of MAML and TAML is quite high. It would be better if the authors explain more.\n\nAfter rebuttal\nI think the authors' response and the revised paper address most of my concerns. I raise my score to 6.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "Bylzova-cH": {"type": "review", "replyto": "BklEFpEYwS", "review": "This paper analyses a pitfall of current meta-learning algorithms, where the task can be inferred from the meta-training data alone, leaving the task-training data unused. Such a meta-learner would generalise well on the meta-training tasks, but will fail to generalise on new tasks at test time. This kind of overfitting is formalised as the memorization problem. This problem is implicitly resolved in current meta-learning algorithms by constructing mutually-exclusive meta-training tasks, which is not easy to construct in all scenarios. The paper introduces an information-theoretic meta-regularizer which forces information extraction from the task data (D) by restricting information flow from meta-parameters (\\theta) and input (x^*). Experimental evaluation with one gradient based and one contextual meta-learning method, on non-mutually-exclusive tasks bring out the mettle of the proposed regulariser. \n\n+ves:\n+ The characterization of the memorization problem and the proposed regularizer are novel contributions. \n+ The paper motivates the problem well, before proposing the methodology.\n+ The paper is well-organised and the experimental setting is designed in a thoughtful manner.\n+ The results are promising.\n\nConcerns:\n- The key hypothesis that the proposed meta-regularization method is based on - is that a model with memorization tends to be more complex. What is the basis for such an assumption? This is an important one for the work at the outset.\n\n- Would the proposed regularizer help if mutually-exclusive meta-training tasks are available, as it forces the model to extract maximum information from task training data (D)? The paper does not comment on this, and this would have been useful to know.\n\n- How much is the training overhead (in terms of time) incurred while adding the regularizer to the baseline methods (MAML and CNP)? The paper does not talk about this additional complexity.\n\n- Evidently, the most important results in the Experiments section are the ones in Sec 6.3. However, the results do not clearly distinguish whether the meta-regularization was performed on the activations or weights here (earlier subsections do talk about this). This makes it difficult to make a conclusive inference on what aspect of the methodology actually helped here.\n\n- There have been recent efforts that have attempted addressing overfitting in meta-learning. The paper mentions these efforts in Sec 5, and states that these have been used for existing settings where tasks are mutually exclusive. It would have been useful to include at least one of these methods in the experiments to see how the proposed regularization differs from them in practice.\n\nMinor issues:\n- The abstract says: \u201cThis causes the meta-learner to decide what should be learned from data and what must be inferred from the input.\u201d - what is the difference between data and input?\n- There are some minor typos in the work, which would benefit from a proofread. E.g: Sec 6.3 \u201cneigbhor\u201d -> \u201cneighbor\u201d\n\n===== POST-REBUTTAL COMMENTS =========\nI thank the authors for the response, the clarifications and the updated manuscript. I am happy to upgrade my rating to Accept. \n\nThe concerns regarding the \u2018higher complexity of memorized models\u2019 has been addressed convincingly in the narrative, and the visualization of weights for models with and without using the proposed regularizer makes the argument cogent. \n\nMentioning the number of  gradient steps used to obtain the results on mutually-exclusive meta-training tasks (Figure 9) in the narrative would help. Was the same number of steps used for MAML and MR-MAML experiments? The optimal \\beta value would be very important if we want to use MR-MAML in situations where mutual-exclusivity of tasks is not known a-priori (as alluded to in the rebuttal). \n\nI would encourage the authors to include training overhead (in terms of time) in the paper, even if it is minimal, as it would clear concerns of the reader. I would also highly encourage the authors to release the code as this would help easy reproducibility of the results. \n\n", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 3}, "rygYKn-hYr": {"type": "review", "replyto": "BklEFpEYwS", "review": "################################################################################\nSummary:\n\nThis paper illustrates, identifies, and formally defines a memorization problem in meta-learning -- the model can simply memorize meta-training tasks and ignore meta-training train sets. The paper proposes to optimize the mutual information between testing predictions and the training data (given input and meta model), and upper bound it by imposing a information bottleneck between output and input+model. Unlike related work, this paper specifically is able to generalize to meta-test even when the meta-train dataset is not made confusing enough (i.e. even when model can learn well from test data in meta-train alone), making it applicable to use cases where it is hard to make the dataset confusing.\n\n################################################################################\nDecision\n\nI vote for accepting this paper, since as far as I know this paper gives a novel insight to the overfitting problem in meta learning, and has formulated the problem formally with theoretical insight, and given a working solution with strong experiment results and clean comparative studies. Despite somewhat narrow experiments and sometimes confusing writing, the paper should provide new insight to meta-learning.\n\n################################################################################\nPros:\n! DISCLAIMER: I am not an expert in this field, so take my novelty judgements with a grain of salt.\n+ Novel view into meta-learning's overfitting problem\n(1) Large models can simply memorize which input data corresponds to which task, and memorize the meta-training tasks, without being able to generalize into meta-test tasks.\n(2) Formulates this into a low mutual information between meta-train training data and predictions.\n+ Easy to implement and quite widely applicable as a regularization loss addon to multiple existing meta-learning methods\n+ Impact-wise, the paper takes meta-learning further from memorization, making methods more capable of operating on less-carefully designed, more natural datasets (rather than permutation of datasets)\n+ Experiments are clean with ablation studies and hyperparameter sensitivity tests, and method performs well across real and toy datasets\n+ Motivation part is easy to read\n\n################################################################################\nCons:\n- I'm still skeptical of the novelty of the paper, since the conclusions are, in hindsight, very straightforward.\n- Sometimes sentences are very confusing to readers.\n(1) The term \"mutually-exclusive\" is confusing because the view-point example the paper gives seems to be mutually exclusive (each task has its own kind of data, hence \"exclusive\"). It is unclear whether the task data is exclusive, or the task function is exclusive, and not straightforward to see its relationship with memorization. Consider renaming it to e.g. \"mutually-confusing\" or \"mutually-contradictory mapping\".\n(2) Can you please rename \"information-theoretic meta-regularizer\" to \"meta-regularizer using information theory\"? It is hard to read for non-native speakers.\n(3) Paragraph under definition 1 is confusing and has redundancies.\n(4) Section 4.1, not very clear how the logic goes from the decomposition to adding the upper bound to the loss, and how the other term comes in.\n- For the motivation, it is better to give examples of use cases when it is impossible to make meta-train \"mutually-exclusive\". I'm sure even in the patient example you can shuffle classes or input dimensions.\n- An experiment comparing to others in mutually-exclusive datasets would be nice to have, in order to judge how much this compensates a badly-designed meta-learning dataset.\n\n################################################################################\nImprovements:\n- Clarify each point in the \"Cons\" section.\n- Please also clarify if all methods in all experiments are hyperparameter-tuned separately, i.e. that the experiments are not favoring the MR-* model in any way. (the paper only clarifies it in one of the experiments)\n- For future work, does recent developments in mutual information modeling (e.g. MINE https://arxiv.org/abs/1801.04062) help this method in any way? e.g. try increasing mutual information between some representation of the meta-train training data and some feature vectors before the prediction?\n- First parenthesis in Section 4.1 has a misplaced space\n\n\n\n################################################################################\nPost rebuttal\n################################################################################\nIt seems that reviewers agree that the contributions are novel (regardless of whether each reviewer thinks it is trivial). So that addresses my main concern. I think the contributions are novel enough since it gives theoretical guidance as well. Other concerns are mostly addressed by the rebuttal. I will keep my rating.\n\nAlthough I do urge the authors to reconsider the name choice \"mutual-exclusive tasks\" since it is not very informative and quite confusing to readers.\n", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 1}, "B1ey3QpBjB": {"type": "rebuttal", "replyto": "rygYKn-hYr", "comment": "Thank you for your insightful and constructive comments and suggestions. Please see our point-by-point response to your comments below.\n\nQ1)  I'm still skeptical of the novelty of the paper.\n\nA1: Our primary contributions and novelties are:\n\ni) We are the first to identify and formalize the memorization problem in meta-learning, a previously unappreciated issue. We find that its main cause is the non-mutually-exclusive task distribution.  Furthermore, as the reviewer notes, we demonstrate that it exists in multiple meta-learning algorithms and can significantly deteriorate performance. Hence, we believe the identification and formalization of this problem to be a significant contribution. \n\nii) We propose an effective and principled regularization approach, and it is not a trivial application of existing ideas. Firstly, as revealed in Table 2 and 3, vanilla regularization on all the model parameters does not solve the memorization problem. Knowing what parameters to be regularized is important for the meta-regularizer to be effective and the theory matters. Secondly, we identify that regularization on activations can fail to prevent the memorization problem and hypothesize why. This indicates that seemingly reasonable alternatives are insufficient. Finally, we consider the simplicity of our approach an advantage because it is then compatible with multiple meta-learning algorithms and easy to implement in practice.\n\niii) We designed and constructed a novel non-mutually-exclusive pose regression dataset which can serve as a benchmark for future algorithms.\n\nIn sum, the problem we identified and studied, the methods we proposed and the pose dataset we created are all novel. We believe this paper can bring awareness of the memorization problem when developing new meta-learning methodologies or applications. Moreover, the datasets and experiments we developed can provide a benchmark for further study of the memorization problem in meta-learning.\n\n\nQ2) Some sentences are confusing.\n\nA2: (1) We call it mutually exclusive because each task has its own kind of function, i.e. the task functions are exclusive so that a single neural net cannot solve all tasks. \n\n(2) We have changed the \u201cinformation-theoretic meta-regularizer\u201d to \u201cmeta-regularizer using information theory\u201d. \n\n(3) We use the paragraph to clarify the memorization and memorization problem. We have modified it to make it clear that if the meta-testing tasks are similar to meta-training ones, with memorization the model can generalize to new datapoints, which differs from common overfitting on datapoints. Memorization is undesirable when the problem requires using the new training data to solve the meta-test tasks. Does this clarify the confusion?\n\n(4) The logic is that to avoid memorization we want large mutual information between prediction $\\hat{y}^*$ and training data D. We encourage this by upper bounding the negative part in the decomposition. Intuitively, the training objective encourages low prediction error and low mutual information between $\\hat{y}^*$ and x*, which encourages the model to use the task training data D to make predictions. We added these explanations in the updated Sec. 4.1.\n\n\nQ3) It is better to give examples of use cases when it is impossible to make meta-train \"mutually-exclusive\" [...].\n\nA3:  We have revised the third paragraph of Sec.3 to make the patient example more specific. It now describes a scenario where it is not possible to make it mutually exclusive: the task is to recommend prescriptions based on the symptoms for each patient and the (X, Y) pair is (symptom, prescription).  Unlike the classification task, where the X (image class) can be assigned with an arbitrary label Y by random shuffling, here the X (symptom) and Y (prescription) have a highly correlated and relevant relationship that we want the model to learn, so we cannot assign random prescriptions Y to symptoms X to make the tasks mutually-exclusive.\n", "title": "Response to Reviewer #1, part1"}, "HJxkr-aroH": {"type": "rebuttal", "replyto": "HkgyWzXhFH", "comment": "Thanks for your acknowledgement on the significance of the memorization problem and the effectiveness of our method.  We believe that our response addresses each of the major weak points raised in the review -- we would appreciate it if you could let us know whether you have any remaining reservations, or if all of your concerns have been addressed.\n\nQ1) First concern is about the novelty of the proposed model. [...] It is better for the authors to clarify their contributions.\n\nA1: Our primary contributions and novelties are:\n\ni) We are the first to identify and formalize the memorization problem in meta-learning, a previously unappreciated issue. We find that its main cause is the non-mutually-exclusive task distribution.  Furthermore, as the reviewer notes, we demonstrate that it exists in multiple meta-learning algorithms and can significantly deteriorate performance. Hence, we believe the identification and formalization of this problem to be a significant contribution. \n\nii) We propose an effective and principled regularization approach, and it is not a trivial application of existing ideas. Firstly, as revealed in Table 2 and 3, vanilla regularization on all the model parameters does not solve the memorization problem. Knowing what parameters to be regularized is important for the meta-regularizer to be effective and the theory matters. Secondly, we identify that regularization on activations can fail to prevent the memorization problem and hypothesize why. This indicates that seemingly reasonable alternatives are insufficient. Finally, we consider the simplicity of our approach an advantage because it is then compatible with multiple meta-learning algorithms and easy to implement in practice.\n\niii) We designed and constructed a novel non-mutually-exclusive pose regression dataset which can serve as a benchmark for future algorithms.\n\nIn sum, the problem we identified and studied, the methods we proposed and the pose dataset we created are all novel. We believe this paper can bring awareness of the memorization problem when developing new meta-learning methodologies or applications. Moreover, the datasets and experiments we developed can provide a benchmark for further study of the memorization problem in meta-learning.\n", "title": "Clarification of primary contributions and novelties"}, "r1glYM6HiS": {"type": "rebuttal", "replyto": "HJxkr-aroH", "comment": "\nQ2) Describe the algorithm of the meta-testing process. Do we need to sample \u03b8 from q(\u03b8|\u03c4)? If so, is the accuracy calculated by the averaged value of tasks with sampled weight?\n\nA2: Yes, for each task at meta-test time, we sample \u03b8 from q(\u03b8|\u03c4). The current results are obtained by sampling a single \u03b8 for each meta-test task. We appreciate the suggestion to average over multiple samples, and we find that using multiple samples of \u03b8 for a task and taking the average of the predictions can further improve performance. For example, in the sinusoid example, using 100 \u03b8 samples improves the test MSE for MR-CNP from 0.11 to 0.07 in 5-shot case and 0.09 to 0.06 in 10-shot case. We now describe the meta-test process in Algorithm 3 in the modified Appendix A.1.\n\nQ3)  I am a little curious about the results in Table 5. The results of MAML and TAML is quite high.\n\nA3: Note that the numbers in Table 5 are the *pre-update* accuracies during *meta-training* (higher does not necessarily mean better test performance). Pre-update accuracy means the accuracy obtained by the initial parameters \u03b8 before adapting to a specific task. High pre-update accuracy during meta-training can indicate that the model does not effectively adapt to the task training data, which can result in poor meta-testing predictions, as shown in Table 4.  We have updated Appendix A.5 to clarify this point, with the following paragraph:\n\n\u201cIn Table 5, we report the pre-update accuracy in meta-training for the non-mutually-exclusive classification experiment in Section 6.3. The pre-update accuracy is obtained by the initial parameters \u03b8 rather than the task adapted parameters \u03c6. At meta-training time, for both MAML and MR-MAML the post-update accuracy obtained by using \u03c6 gets close to 1. High pre-update accuracy can reflect the memorization problem. For example, in 20-way 1-shot Omniglot example, the pre-update accuracy for MAML is 99.2% at the training time, which means only ~0.8% improvement in accuracy is due to adaptation, so the task training data is largely ignored. The pre-update training accuracy for MR-MAML is 5%, which means ~95% improvement in accuracy during training is due to the adaptation. This explains why in Table 4, the test accuracy of MR-MAML is much higher than that of MAML at the test-time, since the task training data is used to achieve fast adaptation.\u201d\n", "title": "Point-by-point response, part 2"}, "rkeazV6rjS": {"type": "rebuttal", "replyto": "B1ey3QpBjB", "comment": "\nQ4) An experiment comparing to others in mutually-exclusive datasets would be nice to have.\n\nA4:  As suggested, we added a comparison on a problem with mutually-exclusive tasks: the standard 20-way, 1-shot Omniglot problem. We report results in Figure 9 (Appendix).  We find that small values of regularization coefficient \u03b2 lead to slight improvements over MAML. This indicates that meta-regularization can be used in cases where it is not known a priori whether the task distribution is mutually-exclusive or not.\n\nQ5) Please also clarify if all methods in all experiments are hyperparameter-tuned separately.\n\nA5: Yes, the optimal hyperparameters for each experiment were chosen separately for each method via cross-validation (Sec. 6.2 second paragraph). We have further clarified this in Sec. A.3.2 in the updated paper.\n\nQ6) For future work, does recent developments in mutual information modeling help this method in any way?\n\nA6: This is an excellent point! Currently, we encourage the information in data D to be applied in the prediction of $\\hat{y}^*$ by restricting the information from input x* and meta-parameters \u03b8.  Alternatively, directly maximizing the mutual information $I(\\hat{y}^*; D | x^*, \\theta)$ in Definition 1, might be possible using recent MI estimators such as MINE. We now point this out as a promising direction for future investigation. \n", "title": "Response to Reviewer #1, part2"}, "BklKNearor": {"type": "rebuttal", "replyto": "Bylzova-cH", "comment": "Thank you very much for your constructive comments and suggestions.  We have made all these suggested minor revisions in the updated paper. Below, we address the concerns raised in your review, which we believe we address in full. Please do let us know if you have any further concerns, or whether this adequately addresses all the issues that you raised with the paper.\n\nQ1) The basis for \u201ca model with memorization tends to be more complex\u201d\n\nA1: We elaborate on this intuition in Section 1 of the revised paper, which we copy here: \u201cThe model acquired when memorizing tasks is more complex than the model that results from task-specific adaptation because the memorization model is a single model that simultaneously performs well on all tasks. It needs to contain all information in its weights needed to do well on task test examples without looking at task training examples. Therefore, we expect the information content of the weights of a memorization model to be larger, and hence the model should be more complex.\u201d  \nFor example, if each task is 1D regression on linearly related data with disjoint domains, the memorization model needs to learn a single piecewise linear function that simultaneously fits all tasks, whereas the adaptation model can simply fit a linear function and adapt the parameters for different tasks. To provide additional evidence for this statement, we added a visualization of the learned weights in the sinusoid regression example in Figure 6 which shows that the weights of the memorization model have more nonzero elements than the meta-regularized models.\n\nQ2) Would the proposed regularizer help if mutually-exclusive meta-training tasks are available? \n\nA2: As suggested, we added a comparison on a problem with mutually-exclusive tasks: the standard 20-way, 1-shot Omniglot problem. We report results in Figure 9 (Appendix).  We find that small values of regularization coefficient \u03b2 lead to slight improvements over MAML. This indicates that meta-regularization can be used in cases where it is not known a priori whether the task distribution is mutually-exclusive or not.\n\nQ3) It would have been useful to include at least one of recent methods addressing overfitting in the experiments to see how the proposed regularization differs from them in practice.\n\nA3: We agree with the reviewer. In fact, we do compare with a recently proposed regularized meta-learner, task agnostic meta-learning (TAML, Jamal & Qi 2019), (see Table 4) and find that our method significantly outperforms TAML.\n\nQ4) How much is the training overhead (in terms of time) incurred while adding the regularizer?\n\nA4: The additional computation time incurred by the meta-regularization is minimal. For example, in the sinusoid example, for 10000 iterations, CNP takes 21.8 seconds while MR-CNP takes 23.8 seconds. For MR-MAML, the regularization does not influence the unrolled inner loop which typically dominates the computation. If MAML and MR-MAML use the same network architecture, for 1000 iterations, MAML takes 72.5 seconds while MR-MAML takes 79.7 seconds with two inner loop gradient steps in the sinusoid example.\n\nWe\u2019d like to address all the other points you raised as follows:\n\n> What is the difference between data and input in the abstract?\n\nBy \u201cdata\u201d, we mean the task training data D, and by \u201cinput\u201d we mean the input x* of the task testing data. We clarified this in the abstract of the updated paper.\n\n> What regularization is used in Sec 6.3?\n\nWe used regularization on the weights. We found that regularization on the activations does not work consistently (discussed in Section 6.2). We have made this clear in Section 6.2 and 6.3 of the updated paper.\n", "title": "Response to Reviewer #2"}}}