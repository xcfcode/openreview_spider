{"paper": {"title": "Taking Notes on the Fly Helps Language Pre-Training", "authors": ["Qiyu Wu", "Chen Xing", "Yatao Li", "Guolin Ke", "Di He", "Tie-Yan Liu"], "authorids": ["~Qiyu_Wu1", "~Chen_Xing2", "yatli@microsoft.com", "~Guolin_Ke3", "~Di_He1", "~Tie-Yan_Liu1"], "summary": "We improve the efficiency of language pre-training methods through providing better data utilization.", "abstract": "How to make unsupervised language pre-training more efficient and less resource-intensive is an important research direction in NLP. In this paper, we focus on improving the efficiency of language pre-training methods through providing better data utilization. It is well-known that in language data corpus, words follow a heavy-tail distribution. A large proportion of words appear only very few times and the embeddings of rare words are usually poorly optimized. We argue that such embeddings carry inadequate semantic signals, which could make the data utilization inefficient and slow down the pre-training of the entire model. To mitigate this problem, we propose Taking Notes on the Fly (TNF), which takes notes for rare words on the fly during pre-training to help the model understand them when they occur next time. Specifically, TNF maintains a note dictionary and saves a rare word's contextual information in it as notes when the rare word occurs in a sentence. When the same rare word occurs again during training, the note information saved beforehand can be employed to enhance the semantics of the current sentence. By doing so, TNF provides a better data utilization since cross-sentence information is employed to cover the inadequate semantics caused by rare words in the sentences. We implement TNF on both BERT and ELECTRA to check its efficiency and effectiveness.  Experimental results show that TNF's training time is 60% less than its backbone pre-training models when reaching the same performance.  When trained with same number of iterations, TNF outperforms its backbone methods on most of downstream tasks and the average GLUE score. Code is attached in the supplementary material.", "keywords": ["Natural Language Processing", "Pre-training"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors propose an approach for pre-training that involves \"taking notes on the fly\" for rare words. The paper stirred a lively discussion on the reasons for the reported results, which the authors followed-up with new experiments and findings that convinced the reviewers that indeed their approach is valid and interesting. Thus, I am recommending acceptance."}, "review": {"Qc0DA7fj0Dn": {"type": "rebuttal", "replyto": "UjRfDZyugv3", "comment": "Your comments have indeed made us curious about what can actually happen if we optimize the notes via backprop. We will run this ablation and take a look at the gradients.", "title": "Thanks for your advice and appreciating our reply"}, "LD6h1-8EGhA": {"type": "rebuttal", "replyto": "lU5Rs_wCweN", "comment": "We thank AC for handling this paper and thank all reviewers for their kind help and useful suggestions. The comments have enlightened us to think deeper and made our work more solid than before.\n\nWe will add those discussions and new experimental results into the paper, including:\n\n1. The experiments on sentences without rare words which demonstrate that the Transformer model is improved by TNF.\n\n2. Make a better presentation regarding the reasons behind why TNF works.\n\n3. Add discussions about the empirical comparison between using word2vec as notes  v.s. using BERT output as notes to show that using BERT contextual embedding does improve the performance.\n\n\nDue to the intensive discussions and conducting multiple experiments during the rebuttal, we can hardly finish the revision of pdf before the rebuttal ddl. We will update the pdf as soon as possible. \n\nThanks!\n\nPaper416 Authors", "title": "General response"}, "eLEUZpJlpGm": {"type": "rebuttal", "replyto": "bwdCMiYR3IM", "comment": "Thanks for your clarification! We are running the 'BERT expanding hidden states' experiment as you suggested to see which explanation is more true. Since probably it cannot run to an okay stage before the rebuttal ddl, we try to find more clues to tell us which explanation is more true in our existing results on BERT-Large. Because BERT-Large has larger parameter size than BERT (345M vs. 110M) and is both deeper (24 layer vs 12 layer) and wider (1024 hidden states vs 768 hidden states) than BERT.\n\nIn Appendix, we show that TNF can achieve even larger improvements on BERT-Large (1.2 points compared with 0.8 points on BERT). Recent studies show that larger models have smaller margins for improvements when their model size is further increased or even doubled[1][2]. See Figure 2, 3, 4 and 5 in [1] and Figure 4 (left) in [2].\nIf TNF's performance improvement is purely brought by the increased parameter size, then according to the references above, it is likely that BERT-Large-TNF's performance - BERT-large's performance < BERT-base-TNF's performance - BERT-base's performance. \nHowever, we observe from the experiment that TNF can achieve even larger performance improvement on BERT-Large compared with BERT-Base. We think it may suggest that the increased model size is not the only reason for TNF's performance gain. Please let us know your further comments and suggestions.  \n\n\n[1] Li, Zhuohan, et al. \"Train large, then compress: Rethinking model size for efficient training and inference of transformers.\", (ICML 2020).\n\n[2] Kaplan, Jared, et al. \"Scaling laws for neural language models.\" arXiv preprint arXiv:2001.08361 (2020).", "title": "Third Reply to AnonReviewer1"}, "hwxn__QtOXB": {"type": "review", "replyto": "lU5Rs_wCweN", "review": "This paper proposes Taking Notes on the Fly, a technique to improve the training efficiency of language-modeling style pretraining. It works by identifying rare words in the pre-training and adding a \u201cnote-taking\u201d component to the masked language model which augments these words with an extra \u201cnote\u201d embedding at the input layer. The note embedding is constructed from an exponential moving average of mean-pooled contextualized representations of context windows in which that word was previously seen during training. The notes are dropped in fine-tuning. Experiments find that this pre-training method improves fine-tuning results on English NLP tasks in the GLUE benchmark when used in the original BERT pre-training setup. In particular, the model can achieve similar performance to the original BERT model with less than 40% of the training steps, and similarly for ELECTRA.\n\n### Strengths\n\nThis paper is clearly written, the proposed technique is simple, and the results seem strong. It is laudable that the authors give experiments in the appendix to give a sense of hyperparameter sensitivity. The paper has a strong backbone and it seems that the proposed technique or something similar may serve as the basis for solid future work.\n\n### Weaknesses\n\nWhile the backbone of the paper is strong, I think it could be improved in its head (motivation) and legs (experimental studies).\n\nFirst, motivation. While the framing around rare words with the COVID-19 example is interesting, I think it has gaps. The introduction argues that since \u201cCOVID-19\u201d is a rare word, in the course of training the model may lack the necessary signal to predict the masked word \u201clives.\u201d But isn\u2019t this fact exactly what should lead the model to improve its embedding of \u201cCOVID-19\u201d? Because gradients flow into the embeddings both through the softmax layer and the input layer.\n\nSo while adding to the context may help the model get a foothold with more effective training signal for the masked token, it seems to me that the note could also \u201cexplain away\u201d the rare word\u2019s embedding in the input layer, reducing the learning signal on it. If that\u2019s the case, then to the extent that TNF works, it would be by the tradeoff between improving the learning signal at the output layer for all words (and in contextualization) and degrading it at the input layer (for rare words).\n\nAs a broader example, see https://openreview.net/pdf?id=3Aoft6NWFej. That paper argues for a masking scheme which eliminates easy shortcuts from the prediction problem to increase learning efficiency, whereas this paper argues essentially the opposite\u2014that shortcuts must be added to hard cases in order to facilitate learning. It seems that there may be a line to walk here between a task being too hard to learn from and too easy to be useful. Because it\u2019s not clear where that line is, I think it\u2019s not enough to motivate TNF from only one direction. It would be better to also have an explanation of why the note-taking approach does not also make things \u201ctoo easy.\u201d It\u2019s not obvious to me how to best make this argument, though results from some of the ablations I will suggest below might help.\n\nThis brings me to my second point: Ablation experiments. If the motivation is to improve the representations of rare words in the input, then there are even simpler ways to do this. Experiments with simple baselines and ablations are important for figuring out why exactly TNF works.\n\nFirst, if the note is such a useful addition to the word embedding, why not just use it to update the embeddings directly? At that rate, the method for constructing the note embeddings looks quite similar to word embedding training objectives like word2vec and GloVe. This suggests a critical ablation:\n\n* Initialize the word embeddings with word2vec, GloVe, or similar run over the wordpieces in the pretraining corpus. (Weirdly, I can\u2019t find an example of this in the literature. It seems like an obvious thing to try. I may have just missed it.) Indeed, it seems to me that the framing in the paper could just as easily motivate this (much simpler) technique than TNF.\n\nIf TNF outperforms the critical ablation, that implies that its gains are coming from some of the other particulars of the technique, such as 1) the extra degree of freedom provided by decoupling the note embeddings from the wordpiece embeddings, or 2) the use of contextualized vectors for note embeddings (rather than the non-contextualized ones in the word embedding objectives). \n\nTo investigate these issues, I would suggest three more ancillary ablations on TNF:\n* Directly update the rare word\u2019s embedding with a version of Eq. 5 rather than keeping a separate note dictionary.\n* Update the note embeddings via backprop instead of Eq. 5. This would amount to \u201cpartially tying\u201d the input and output embeddings, giving more freedom to the input layer, which is partly what\u2019s happening in TNF.\n* Pool over non-contextualized instead of contextualized representations in Eq. 4.\n\nFinally, to address the \u201ctoo easy\u201d vs \u201ctoo hard\u201d distinction, two more ablations that might help would be:\n* Instead of using an exponential moving average for the note embedding update, just use the pooled context vectors from the last instance of the rare word (i.e., set $\\gamma$ to 1 in Eq. 5).\n* instead of using an explicit note dictionary, augment the input context with retrieved text containing the rare word. See TEK-enriched representations (https://arxiv.org/pdf/2004.12006.pdf) for an example of this. For consistency, the exact last-seen context of the rare word could be used.\n\nThe first will help identify to what extent aggregating over many multiple inputs to get a high quality representation is necessary for TNF. This could then serve as a reference point for the second ablation, which may help determine whether the fixed embedding size and pooling operation helps by creating a bottleneck for the retrieved information and preventing things from getting \u201ctoo easy.\u201d (although context window sizes might also be a confound here, that could also be controlled carefully.)\n\nAll together I think these ablations would shed a lot of light on why TNF works, and make this work much more useful to researchers who wish to build on it in the future. However, I know I\u2019ve suggested a lot of crazy experiments here. I would not expect all of this necessarily to be done and I leave it up to the discretion of the researchers which are most important. I am also sure the authors could come up with better ablations than these as well. But my sticking point is the first ablation \u2014 initializing with non-contextualized embeddings \u2014 which I think is critical. And I think it behooves the authors to address some of the lingering questions (including more written below), even if not all of them.\n\n### Recommendation\n\nUnfortunately, reject. The technique is simple and the results seem good, but the paper does not provide empirically-justified insight on why TNF works. I think ablations and investigation into the \u201cwhy\u201d aspect is the most important part of this kind of model engineering research.\n\n### More comments & questions\n\nI am left with some more questions about how TNF works:\n\n* How does the quality of the representations of rare words specifically compare in your approach? Does it improve the representations of common words and contextualization at the expense of rare words? While it may be tricky to try to directly assess embedding or contextualization quality, breaking down the MLM perplexities by word frequency (or presence of rare words in the context) after removing the note dictionary might be informative. I admit this might also be tricky because I imagine the model would have to be fine-tuned without the notes for a bit before doing such an experiment. But any insight into this issue would be appreciated.\n* If this method indeed works by more narrowly refocusing the training signal on the masked token than the context tokens, then would you be able to further increase the learning efficiency by oversampling rare words when determining the masks in training? I am not aware of anyone showing such a thing to work, though I might have missed it. Just a thought.\n\nWhile the pretraining corpus is huge, 100 occurrences still seems like a pretty high threshold for rare words given the justification provided in the paper. Questions:\n* What do the even rarer words look like? Are they just a source of noise? e.g., because they are components of names or don\u2019t have clear and consistent semantic content?\n* What proportion of contexts contain words appearing less than 100 times? It seems that the 20% figure in the paper is meant to apply to your definition of rare words, which appear between 100 and 500 times.\n* What is the word vocabulary size? i.e., how many words appear more than 500 times, and less than 100?\n* Did you do any preliminary experiments with other thresholds? Would you expect this to work with more common words as well? Why or why not? (This may also relate to the \u201ctoo easy\u201d vs \u201ctoo hard\u201d issue.)\n\nOn pre-training efficiency results: I think Figs 3a and 3b need to be explicitly qualified a little better. AFAICT, having lower loss here doesn\u2019t necessarily mean the model (modulo the note dictionary) is learning better, because it sees the notes in the input. So we\u2019re looking at the loss in a different setting than we intend to fine-tune in. It\u2019s still interesting to see, but I think it's best to include an explicit caveat.\n\nWhat about training the models for more steps? Will the trend hold and performance improve overall, or will the gains eventually level off as the representations of rare words get better? Especially for pretrained models, since they are used as the starting point for many models, it is often worthwhile to train them longer (as in the RoBERTa paper), so it\u2019s important to understand the usefulness of this method in that regime.\n\n### Typos etc.:\n\n* P.3: neglectable -> negligible\n* P.3: Representation -> Representations (in BERT acronym)\n* P.6 Sec. 4.1: after \u201cMNLI\u201d there is a space missing after the period.\n* P.6: \u201cFULL-SENTENCES\u201d would look better & be consistent with Liu et al if it were in small caps.\n* Please cite the individual dataset creators for the datasets in the GLUE benchmark.\n\n---\n\nUpdate: upped score from 4 to 5; see comment thread.\n\nUpdate again: score further updated from 5 to 6 with GloVe context ablations and perplexity results on sentences with rare words.\n", "title": "Technique is simple and results are good, but too many questions remain", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Fn1sq_QyNiH": {"type": "rebuttal", "replyto": "6VSXyrFpymN", "comment": "\nThanks for the quick and enlightening response! Regarding the new comments, we provide more results and analysis.\n\nFirst, we notice you have concerns about where the improvement over sentences without rare words comes from. We are not sure what you exactly mean by 'the model spend more memory on the sentences without rare words'. If it means the model can better 'memorize' the data during training, then it is very unlikely to be the reason. This is because in the 'sentence without rare words' experiments in our rebuttal, we did the evaluation on the validation set instead of the training set. Note that the model never sees the validation sentences during training, let along memorize them. In machine learning literature, better performance on validation set usually means 'the modeling is better'. If a model merely 'memorized' more of the training data  due to the extra space while the modeling is not improved, it doesn't necessarily lead to better generalization (validation performance) (see [2] in the first round of rebuttal). \n\nSecond, for cases with rare words, we have some new empirical results. We calculated the MLM validation loss of validation sentences containing rare words on BERT-TNF without using notes, and compare it with that of BERT and BERT-TNF with notes. Here are the results.\n\n#Iter 20k 50k 100k 200k 400k 600k 800k 1000k \n\nTNF-with notes 3.896 3.100 2.745 2.504 2.333 2.246 2.179 2.131 \n\nTNF-without notes 3.929 3.170 2.850 2.633 2.467 2.381 2.313 2.265 \n\nBERT 3.921 3.153 2.815 2.583 2.408 2.313 2.246 2.197 \n\nWe find that on sentences with rare words, the performance order is BERT-TNF(with notes) > BERT > BERT-TNF(without notes). We can see that first, BERT-TNF with note achieves the best performance. Interestingly, BERT is better than BERT-TNF without notes. We think it can act as a strong evidence showing that TNF indeed splits out some rare word information into the notes as you suggested. \n\nAs a summary, based on the results and discussion above, we think that for TNF, whether to use rare word notes at fine-tuning should be downstream task-dependent. For normal downstream tasks such as GLUE that contain little rare words, TNF without notes works well because it helps better modeling the common word interactions as we said. While for special downstream tasks such as biomedical terminology as you mentioned, if the task's data contains too much rare words, then using notes with TNF at fine-tuning would probably be the best choice.\nHowever, we would like to note that the goal of the paper is to improve pre-training efficiency. TNF can achieve this goal because splitting rare word information into notes can help reduce the noise of the input sequence therefore make the pre-training of the Transformer better. It is irrelevant with whether or not to use notes at fine-tuning. We will include this discussion into the paper.\n\nWe also really appreciate the reviewer's domain adaptation example. It has made us realize that what TNF can do is not just about rare words. It could have more potentials to address domain shift problems in NLP. \n\nWe hope our explanation and new experimental results can address your concerns.", "title": "Second round of response to AnonReviewer1"}, "A8bhnN49sbV": {"type": "rebuttal", "replyto": "X5Vd0tD6yDk", "comment": "Thanks for the quick response and clarification of the comments! We find that you have two remaining concerns in general: why using GloVe (w2v) doesn't work while TNF works, and whether the rare word embedding is improved or not. We provide the answers and new empirical analysis below. Please correct us if we misunderstand your comments.\n\n\n1. Why using BERT contextual outputs around rare words as notes would be better than simply using pre-trained GloVE or w2v rare word embeddings as notes and how much better?\n\nIf we understand it correctly, your concern behind this question is that \u201dAdditively incorporating information from nearby tokens in previously observed context windows (the way to update notes in TNF) is exactly how the update rules in methods like GloVe and word2vec work\u201c. \n\nFirst of all, the surrounding information of rare words that TNF uses is much better than Glove or w2v rare word embeddings. TNF leverages the surrounding information using the BERT's output representations, which are significantly superior to conventional word embeddings due to BERT's powerful semantic representation capacity. (If BERT contextual outputs are of the same quality or just slightly better than w2v word embeddings, then people would have kept building models on top of pre-trained word embeddings instead of switching to BERT.) Therefore compared to w2v, using BERT's contextual outputs as notes can provide more accurate signals and help the model train better.\n \n\nTo verify this and answer the 'how much better' question, we have run the ablation experiment that you suggested, using fixed GloVe rare word embeddings as the notes. We cannot finish the whole pre-training before the rebuttal deadline due to its long-running time. But both the current train loss and validation loss curve (before around 200k iterations) of this ablation are slightly lower than BERT while still significantly higher than BERT-TNF. It supports our argument above.\n\n\n\n2. Are the rare word embeddings trained with TNF worse or better than those of BERT?\n\nWe apologize for misunderstanding your original comments related to this. In order to answer this question, we calculate the MLM validation loss on validation sentences containing rare words. In particular, we evaluate the BERT-TNF models with/without notes, and compare it with the BERT baseline at different checkpoint. Here are the results.\n\n#Iter 20k 50k 100k 200k 400k 600k 800k 1000k \n\nTNF-with notes 3.896 3.100 2.745 2.504 2.333 2.246 2.179 2.131 \n\nTNF-without notes 3.929 3.170 2.850 2.633 2.467 2.381 2.313 2.265 \n\nBERT 3.921 3.153 2.815 2.583 2.408 2.313 2.246 2.197 \n\n\nFrom the results, we can see that on the rare-word sentence, the performance order is BERT-TNF with note > BERT > BERT-TNF without note. We can see that first, BERT-TNF with note achieves the best performance. Interestingly, BERT is better than BERT-TNF without note. From the previous experiment on popular sentences, we conclude that BERT parameter is better trained using TNF. The new observation may lead to a conclusion that: the rare word embedding trained using BERT-TNF is worse than that trained using BERT. We think it can act as strong evidence showing that TNF indeed splits out some rare word information into the notes as you hypothesized. \n\n\n\nWe would like to thank Reviewer 4 again for the efforts on reviewing this submission and conducting very active discussions. We will include our discussion in the next version and try our best to update the pdf as soon as possible.", "title": "Second round of response to AnonReviewer4"}, "qaon3QH0jIP": {"type": "review", "replyto": "lU5Rs_wCweN", "review": "The paper proposes an external memory architecture. When encountering the rare words (with a frequency between 100-500), the method will store the average contextualized word embedding of nearby words into a dictionary. Next time it encounters the same rare word, it will retrieve the average embedding and input it into BERT encoder. The experiment results show that given the same number of training steps, adding the external memory improves the MLM loss and significantly improves the results on RTE (Recognizing Textual Entailment) dataset, which leads to a slightly better GLUE score. The experiment also shows that keeping the external memory during the fine-tuning stage slightly degrades the performance.\n\nPros:\n1. The method is simple and easy to understand\n2. The experimental results on GLUE are quite surprising. It shows that we should take note when training BERT but throw away the note dictionary when fine-tuning the model.\n\nCons:\n1. Missing an important citation [1]\n2. The paper does not well explain the surprising results on GLUE. This is a crucial weakness. The comparison of the MLM loss is not very fair because the proposed method has a large external memory. The benefit of the proposed method relies on the improvement of the average GLUE score. However, Table 2 shows the most of the improvement of GLUE actually comes from the improvement of a single dataset, RTE. Without understanding why it improves RTE, the readers do not know when they want to adopt the proposed method for their downstream applications.\n\nClarity:\nThe text is fluent, but the main story is not well supported by the experiment results. The story is that using an external dictionary could accelerate the training, but the main experiment finding actually says that using an external dictionary can very significantly improve the results on RTE dataset while performing similarly on other datasets in GLUE.\n\nOriginality:\nThere has been some effort of using an external dictionary to help the training of BERT [1], but I am not aware of existing papers that apply the dictionary to only the rare words. I also do not know any other work that shows the external dictionary could improve the GLUE scores.\n\nSignificance of this work:\nIf the authors could well explain the experimental results on GLUE and justify the explanation using some analysis, this might lead to more important findings.\n\n\nFigure 3c seems to contradict with Table 1 and 2 because in Table 1 and 2, the GLUE score of BERT (ours) is 83.1 but all the points in the BERT curve in Figure 3c is below 83. \n\nUsually, when a study tries to sell its method as a way to accelerate the training, it means the method reaches some performance faster but the method will converge the same performance eventually. However, Figure 3 does not show that they will converge the same value, so selling the method as a way to accelerate the training is weird. Furthermore, I think the lower MLM loss is due to the extra parameters in the note dictionary rather than the note dictionary accelerates the training. \n\nIt is not surprising that taking notes for rare words could achieve lower loss/perplexity because the note dictionary gives the extra memory capacity [1]. It is also not surprising that it can achieve better performance on GLEU if using the note dictionary during the fine-tuning stage due to the extra parameters. The really interesting results are that the authors report that the model could very significantly improve the RTE task and mildly improve CoLA without using the note during the fine-tuning stage. \n\nIntuitively, the proposed model stores lots of knowledge about the rare words into the note dictionary. Does the fact that the note is not needed in the fine-tuning stage imply that the knowledge about rare words is actually not needed? Does it mean the RTE or CoLA do not contain many rare words or does it mean the rare words do not affect the decision of BERT and ELECTRA in RTE or CoLA? Is the reason of improvement that we could store more interactions between popular words in the parameters of BERT itself because the information of rare words has been stored in the note (maybe you can test this by reporting the MLM loss on the sentences without any rare words)? If that is the case, why do we only stably improve RTE and CoLA? If the authors can show the above hypothesis is true, I think this is a significant contribution because that means this paper provides a way to control what LM should learn when there is a mismatch between MLM training corpus and downstream applications (e.g., MLM training corpus contains many rare words but we should ignore the rare words in the downstream applications).\n\nThis paper lacks a good explanation of the above weird result (in my opinion, the most valuable finding in this paper) and lacks the analysis that supports the explanation. The main paper says that taking notes improves the tasks with the small datasets the most. The STS-b (7k) and MRPC (3.7k) have smaller training datasets than CoLA (8.5k). Why are the results of STS-b and MRPC cannot be stably improved? If the authors really want to explain the performance improvement using the training dataset size, the authors can just randomly sample several small subsets of training data from each dataset and show that the GLUE score improves a lot in that setting. In the appendix A.4, the authors hypothesize that the small proportion of rare words in each dataset of GLUE (from 0.47% to 2.31%) might be the reason that we can ignore the note dictionary during the fine-tuning stage. This also did not explain why most of the improvement of the GLUE score comes from RTE. Moreover, if the rare words are not important in the testing datasets, why do we want to take notes in the first place?\n\nI will vote for acceptance if the authors could answer these critical questions I raise above strongly.\n\n\nMinor:\n1. Although the chance is not high, I think it is possible that parts of MLM improvement could be achieved by simply sampling the sentences containing the rare words more (This is a minor concern. If you do not have time to finish the experiments for this baseline, you can choose not to do it or compare the results after training fewer steps).\n2. I guess the dictionary overhead is small but it should be measured and reported because you say the method accelerates the training. \n\n\n[1] Lample, Guillaume, et al. \"Large memory layers with product keys.\" Advances in Neural Information Processing Systems. 2019.", "title": "The method is simple and seems to be very effective in a certain situation, but we do not know what that situation is and why", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "6-Po8MUj1tg": {"type": "rebuttal", "replyto": "SRZWkYs76l9", "comment": "---\n5. Regarding whether easy/hard tasks relate to training efficiency. (Forth paragraph in \"Weakness\")\n\nWe respectfully disagree with the reviewer that a task being 'too easy' or 'too hard' is a proper metric to evaluate if it would be effective for language pre-training.\nA task will be more effective for the Transformer to learn if it is more semantically meaningful, so that the model can be guided to learn from more informative signals in the training corpora. \n\nFor example, ELECTRA[4] does that by making the model to discriminate more semantically ambiguous replaced tokens. PMI does that by providing a more semantically meaningful masking instead of forcing the model to predict meaningless tokens such as 'ness' or 'ing' after BPE. TNF's task is also more semantically meaningful compared with BERT. Because TNF provides a more semantically reasonable data utilization by using notes, to reduce the noise of the input sentences caused by rare words. That is the reason why TNF can further improve ELECTRA when applied on top of it. We also believe TNF can be combined with PMI together for further acceleration.\n\nWe would also like to note that, if a harder task is always better for language pre-training while an easier task is always more harmful, then it is impossible to explain why Electra is significantly better than BERT. Because ELECTRA actually changes BERT's multi-classification task of predicting the correct word into a binary classification problem of discriminating replaced tokens. A binary classification task should be much easier than a multi-classification task with 30k+ classes.\n\n---\n6. The ablation studies that R4 suggested to show why TNF works.(after 4th paragraph in \"Weakness\")\n\nFirst of all, we would like to note that the reason why TNF works is explained in bullet 2 with additional empirical evidences. While we do notice that we have run several of ablations that you suggested before the submission. Therefore we present the results here.\n\nAblation study 1. Initializing BERT word embeddings with pre-pretrained w2v or GloVE embeddings. \n\nWe conducted an experiment to initialize the embedding using word2vec, and train BERT further with respect to MLM loss. We found this model performs almost identical to BERT. Note that w2v or GloVE cannot improve the quality of rare word embeddings (actually they have this problem since day one, see related work in the submission), let alone improving language pre-training methods on top of it.\n\nAblation study 2. Directly update the rare word\u2019s embeddings with a version of Eq. 5. \n\nThis actually means that you are setting the $\\lambda$ to be 1 in Eq. 6 and still keeps the positional embedding. Because by doing so you basically ditched the rare word embeddings which are optimized normally with the MLM loss. We haven't tried setting $\\lambda$ to be 1, but we have tried setting $\\lambda$ as 0.9 in the submission (See Table 4 in the appendix, R9). The result shows that with $\\lambda$ being 0.9 TNF performs similar (only 0.1 minus) with the optimal TNF. \n\nAblation study 3. Update the note embeddings via backprop instead of Eq. 5. \n\nIf we understand correctly, you suggest keeping a note embedding emb1 for a rare word together with its word embedding emb2. The two embeddings are always used together (emb1+emb2) and updated by gradient descent. This is equivalent to setting an emb' = emb1+emb2 and directly update emb', which is further equivalent to the original BERT. Therefore the result of this ablation is predictably identical with that of BERT. \n\n---\n7. Regarding the quality comparison between rare/common word embeddings in TNF (The first bullet point under 'More comments & questions').\n\nWe would like to note that language pre-training is not just a game of word representations. The amount of Transformer parameters is significantly more massive than that of word embeddings, and the Transformer model significantly improves isolated word embedding by contextual embeddings. As illustrated in 2. and 3., TNF does not aim to strengthen word embeddings with a particular range of frequency, but to improve the model as a whole.\n\n---\n8. Regarding oversampling rare words (The second bullet point under 'More comments & questions')\n\nELECTRA[4] has tried that before (dynamically masking more rare words) and it doesn't lead to any performance improvements.\n\n", "title": "(Following the previous comment)"}, "MR7CS_Vyu47": {"type": "rebuttal", "replyto": "cdFOrFGz6wJ", "comment": "We thank Reviewer 2 for appreciating our work and providing insightful comments. We appreciate Reviewer 2's highlight of our view that the poor embeddings of rare words could slow down the training of all model parameters. We find that you have concerns about several design choices of our work. We try to address them below. \n\n* Regarding using words or sub-words as keys in the note dictionary\n\nThis is a good question. Sub-word tokens are indeed easier to be applied as keys for the note dictionary and could potentially cover more of the corpora. However, we find that quite a few rare sub-word tokens, either generated by BPE or in Google's word piece, don't have specific understandable semantic meanings to humans. Here we list a few ''195@@, \u2153, canter, elids, al, ch,  di'', separated by commas.\n\nFor such sub-words, their contexts can be very diverse due to their vague semantic meanings. Therefore, saving notes for those sub-words would potentially bring irrelevant context into the current sentence and even add noise into the learning process, which we found not improving the pre-training. While if we save notes for words instead of sub-words, given that most words have concrete semantics, their notes can act as effective auxiliary semantics to enhance the current input sentence.\n\n* Regarding using different occurrence range other than (100,500)\n\nThanks for this question. We have tuned this frequency range to build the note dictionary before the submission. We apologize for not having put the results into the paper. Specifically, we have tried setting the range to be 50-100 and 100-1000. \n\n\nRange 50-100 performs almost identical to the BERT baseline. The reason is that rare words with numbers of occurrences between 50-100 can only cover 0.3% of the whole training corpora. With such low coverage, very limited training sentences would be enhanced by rare word notes. As a comparison, words whose occurrences between 100-500 cover 2% of the training corpora, which is shown that could impact the pre-training empirically. We cannot further include lower-frequency (e.g., < 50) words in the dictionary as the number of such words would exponentially increase due to the long-tail problem of language corpora, occupying a massive amount of GPU memory. \n\nRange 100-1000 performs similar to 100-500, which suggests the rare word range in TNF is robust to some extent. Although TNF targets rare words, we do agree with the reviewer that it is worth trying to apply TNF to common words (or even all words) and check the performance. However, we may not be able to finish the experiment within the rebuttal period (due to the long pre-training and hyper-parameter tuning time). We will explore it as future work. \n\n* Regarding using different window size\n\n\nWindow size is a hyper-parameter in our method. We have provided the ablation study for this hyper-parameter (and other hyper-parameters) in the appendix. See Page 12. The experimental results show that a larger window size usually leads to better performances.\n\n------\nWe hope the above responses can address your questions about the design choices. Please let us know if you have further questions!", "title": "Response to AnonReviewer2"}, "ZWmZFGPFit": {"type": "rebuttal", "replyto": "Lud0JlJRD9h", "comment": "\nWe would like to thank Reviewer 3\u2019s support and constructive comments. We notice that similar to Reviewer 1, Reviewer 3 also has concerns about our motivation and analysis about why TNF works. Therefore, we first describe why our method works and present empirical evidence, which we think can largely help the reviewer better understand our paper. Then we answer each question.\n\n* The motivation of TNF\n\n\nFirst of all, we would like to emphasize that poor rare word embeddings will hurt the training of all model parameters (such as the Transformer layers). The reasons behind are as follows. According to many previous works (see the related work section), rare words' embeddings are usually poorly optimized. There are even recent works suggesting that rare word embeddings act as noise[1]. Training a model from noisy inputs is less effective in general (see [2](Figure 1a) for more observations about how noisy inputs hurt the optimization and generalization of a machine learning model). For language pre-training tasks specifically, the input sequences with noisy rare word embeddings contain less semantically meaningful information for the Transformer to learn, making the whole model training ineffective. \n\nGiven the above facts, we aim to reduce the noise from rare word embeddings in a sentence, to improve the pre-training of the whole model. Specifically, we achieve this by providing more precise semantics of rare words in a sentence using surrounding contexts from other sentences (contextualized information saved in notes). For the original method, Transformer receives thousands of noisy embeddings like 'Covid-19' containing little semantic meanings. Suppose we equip 'Covid-19' with a note that contains previous surrounding contextual information such as 'pandemic' and 'global crisis' (which are popular words). One could imagine that the sentence `Covid-19 (+pandemic + global crisis) causes thousand of lives' could have more precise semantics, which makes the training of the Transformer model more effective.\n\nWe conduct additional experiments to check whether the Transformer model is better trained in BERT-TNF. To show this, we calculate MLM loss on sentences in the validation set that do not have rare words for both BERT and BERT-TNF at different checkpoints. The total number of sampled sentences that satisfy the condition is roughly 20k.  As those testing sentences don't contain rare words, the note dictionary will not be called, and we can purely compare the performance between the two Transformer models in BERT and BERT-TNF. From the table below, we can see that BERT-TNF's loss is consistently lower than BERT at all checkpoints, which suggests the entire Transformer model was improved using our method. \n\n\n#Iter   20k    50k  100k  200k   400k  600k  800k 1000k\n\nBERT 2.709 2.180 1.947 1.788 1.667 1.602 1.556 1.522\n\n TNF  2.692 2.145 1.902 1.736 1.619 1.558 1.513 1.479\n\n\nIn the submission, we also showed that TNF without notes works well on finetuning downstream tasks. Moreover, even for sub-tasks with almost no rare words occurring in the training set (0.47% rare word coverage in CoLA), BERT-TNF can still outperform BERT on it. \n\nAll empirical results above support our motivation and indicate that the entire model is better pre-trained with TNF.\n\n* Regarding the frequency range of words in the note dictionary.\n\nThanks for this question. We have tuned this frequency range to build the note dictionary before the submission. We apologize for not having put the results into the paper. Specifically, we have tried setting the range to be 50-100 and 100-1000. \n\nRange 50-100 performs almost identical to the BERT baseline. The reason is that rare words with numbers of occurrences between 50-100 can only cover 0.4% of the whole training corpora. With such low coverage, very limited training sentences would be enhanced by rare word notes. As a comparison, words whose occurrences between 100-500 cover 2% of the training corpora, which is shown empirically that could impact the pre-training. We cannot further include lower-frequency (e.g., < 50) words in the dictionary as the number of such words would exponentially increase due to the long-tail problem of language corpora, occupying a massive amount of GPU memory.  Range 100-1000 performs similar to our main results from 100-500, which suggests the rare word range in TNF is robust to some extent.\n\nThe '3.4B' refers to the total number of words in the 16G training corpora.\n\n* Regarding the measurement of acceleration.\n\nYes, the 2 days vs 5.7 days is actual wallclock measurement. \n\n----\n\nWe hope the above responses can address your questions about the design choices. Please let us know if you have further questions!\n\n[1] Li, Yangming, et al. \"Handling Rare Entities for Neural Sequence Labeling.\", (ACL 2020).\n\n[2] Zhang, Chiyuan, et al. \"Understanding deep learning requires rethinking generalization.\"(ICLR 2017).", "title": "Response to AnonReviewer3"}, "H_-2owwtcd": {"type": "rebuttal", "replyto": "qaon3QH0jIP", "comment": "\nWe would like to thank Reviewer 1 for the constructive comments and careful reading. From your comments, we realize that we didn't describe our motivation clearly. It leads to some difficulties in reading and further leads to some critical misunderstandings of our work. We first describe why our method works and present more empirical evidence, which we think can largely help better understand our paper. Then we answer each question separately.\n\n## The motivation of TNF\n\nFirst of all, we would like to emphasize that poor rare word embeddings will hurt the training of all model parameters (such as the Transformer layers). The reasons behind are as follows. According to many previous works (see the related work section), rare words' embeddings are usually poorly optimized. There are even recent works suggesting that rare word embeddings act as noise[1]. Training a model from noisy inputs is less effective in general (see [2](Figure 1a) for more observations about how noisy inputs hurt the optimization and generalization of a machine learning model). For language pre-training tasks specifically, the input sequences with noisy rare word embeddings contain less semantically meaningful information for the Transformer to learn, making the whole model training ineffective. \n\nGiven the above facts, we aim to reduce the noise from rare word embeddings in a sentence, to improve the pre-training of the whole model. Specifically, we achieve this by providing more precise semantics of rare words in a sentence using surrounding contexts from other sentences (contextualized information saved in notes). For the original method, Transformer receives thousands of noisy embeddings like 'Covid-19' containing little semantic meanings. Suppose we equip 'Covid-19' with a note that contains previous surrounding contextual information such as 'pandemic' and 'global crisis' (which are popular words). One could imagine that the sentences  'Covid-19 (+pandemic + global crisis) causes thousand of lives' could have more precise semantics, which makes the training of the Transformer model more effective.\n\nWe notice that Reviewer 1 has also reached a similar understanding (`` we could store more interactions between popular words in the parameters of BERT itself'') and ask for more supporting empirical evidence. We follow your advice to conduct the experiment below. \n\nWe calculate MLM loss on sentences in the validation set that do not have rare words for both BERT and BERT-TNF at different checkpoints. The total number of sampled sentences that satisfy the condition is roughly 20k. As those testing sentences don't contain rare words, the note dictionary will not be called, and we can purely compare the performance between the two Transformer models in BERT and BERT-TNF. From the table below, we can see that BERT-TNF's loss is consistently lower than BERT at all checkpoints, which suggests the entire Transformer model was improved using our method. \n\n#Iter        20k       50k       100k         200k        400k       600k     800k     1000k \n \nBERT       2.709    2.180     1.947       1.788       1.667      1.602    1.556     1.522  \n \n TNF         2.692    2.145     1.902       1.736       1.619     1.558     1.513     1.479 \n\nIn the submission, we also showed that TNF without notes works well on finetuning downstream tasks. Moreover, even for sub-tasks with almost no rare words occurring in the training set (0.47% rare word coverage in CoLA), BERT-TNF can still outperform BERT on it. \n\nAll empirical results above support our motivation and indicate that the entire model is better pre-trained with TNF.\n\n## Response to other questions\n\n* Regarding the imbalanced performance improvements of TNF\n\n\nThanks for the careful checking. We think the reason for this imbalanced improvement gain is that sub-tasks in GLUE have different margins for improvements in general. For example, BERT-Large is three times larger than BERT-Base. Its improvements over BERT-Base on RTE and CoLA is more than 3 points. While for the rest of tasks like MRPC and STS-B, their performance gaps are relatively small, e.g., 0.4 and 0.7. This indicates that some tasks, like RTE and CoLA, have a larger improvement space when the model is more powerful. While for other tasks, the improvement space may be limited. Similar trends can also be found in other language pre-training methods such as SpanBERT[4] and ELECTRA[5]. \n\nWe understand that imbalanced performance improvements look weird when readers have concerns about why TNF works. While given that it is a common trend of a lot of other language pre-training methods, we think this phenomenon is orthogonal to TNF.\n", "title": "Response to AnonReviewer1"}, "c74UxImYUn": {"type": "rebuttal", "replyto": "H_-2owwtcd", "comment": "* Regarding selling TNF as an acceleration method\n\nWe agree that ideally, we need to check the 'converged' point. However, language training methods such as BERT are severely under-trained[6]: as BERT data is huge, we usually can not observe the valid loss ``converge'' even for one-week training. Given such a situation, we present the performance (pre-training and fine-tuning) given the same computational budget (iterations) as evidence.\n\nWe agree that the lower MLM loss during pre-training we report can be brought by both the extra memory and the better Transformer parameters. According to the additional experiment above, we can see that TNF's MLM validation loss on sentences with no rare words is also consistently lower than that of BERT, which indicates that the BERT model itself is also better-trained. \n\n* Regarding the inconsistency between Figure 3c and the tables\n\nThanks for the careful checking and we apologize for the inconsistency. Results in Table 1 and 2 are the correct results we wanted to present. When we run the last batch of hyper-parameter tuning experiments to update the main results, we forgot to update the figure in the last minute. We will fix it in the upcoming version of the submission. \n\n* Regarding the missing citation\n\nWe will add the citation in the upcoming version of the submission. \n\n---\nReference.\n\n[1] Li, Yangming, et al. \"Handling Rare Entities for Neural Sequence Labeling.\", (ACL 2020).\n\n[2] Zhang, Chiyuan, et al. \"Understanding deep learning requires rethinking generalization.\"(ICLR 2017).\n\n[3] Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\", (ACL 2019).\n\n[4] Joshi, Mandar, et al. \"Spanbert: Improving pre-training by representing and predicting spans.\", (ACL 2020).\n\n[5] Clark, Kevin, et al. \"Electra: Pre-training text encoders as discriminators rather than generators.\", (ICLR 2020).\n\n[6] Liu, Yinhan, et al. \"Roberta: A robustly optimized bert pretraining approach.\" arXiv preprint arXiv:1907.11692 (2019).\n\n------\nWe would like to express our appreciation again to Reviewer 1's critical questions, which greatly help us polish our work. We will include the above discussions in the rebuttal version of the submission. We hope our response can address your concerns, and please let us know if you have any further questions.\n", "title": "Following the previous  Comment"}, "QYOYB5Tajks": {"type": "rebuttal", "replyto": "6-Po8MUj1tg", "comment": "---\n9. Regarding questions about the occurrence range [100-500].\n\nThanks for this question. We have tuned this frequency range to build the note dictionary before the submission. We apologize for not having put the results into the submission. Specifically, we have tried setting the range to be 50-100 and 100-1000. \n\nRange 50-100 performs almost identical to the BERT baseline. The reason is that rare words with numbers of occurrences between 50-100 can only cover 0.3% of the whole training corpora. With such low coverage, very limited training sentences would be enhanced by rare word notes. As a comparison, words whose occurrences between 100-500 cover 2% of the training corpora, which is shown empirically that could impact the pre-training. We cannot further include lower-frequency (e.g., < 50) words in the dictionary as the number of such words would exponentially increase due to the long-tail problem of language corpora, occupying a massive amount of GPU memory. \n\nRange 100-1000 performs similar to our main results from 100-500, which suggests the rare word range in TNF is robust to some extent.\n\nThe total size of word vocabulary is 9435484, while the number of words with occurrences under 100 is 9163284. Given the massive amount of words under 100, we can't check all of them. While we do agree with the reviewer that there are indeed some components of names or don\u2019t have clear semantic meanings and they could act as noises.\n\n---\n10. Regarding Figure 3.\n\nThanks for this question. We agree that the lower MLM loss during pre-training we report in Figure(3) can be brought by both the extra memory and the better Transformer parameters. While according to the new additional experiment above, we can see that TNF's MLM validation loss on sentences with no rare words is also consistently lower than that of BERT, which indicates that the BERT model itself is also better-trained. \n\n---\nWe appreciate Reviewer 4's active comments despite the disagreements we illustrate earlier. Please do let us know if we understand your comments correctly and if we have addressed your concerns.\n\n\n\n---\nReferences.\n\n[1] Gao, Jun, et al. \"Representation degeneration problem in training natural language generation models.\", (ICLR 2019).\n\n[2] Li, Yangming, et al. \"Handling Rare Entities for Neural Sequence Labeling.\", (ACL 2020).\n\n[3] Zhang, Chiyuan, et al. \"Understanding deep learning requires rethinking generalization.\"(ICLR 2017).\n\n[4] Clark, Kevin, et al. \"Electra: Pre-training text encoders as discriminators rather than generators.\", (ICLR 2020).", "title": "(Following the previous comment)"}, "SRZWkYs76l9": {"type": "rebuttal", "replyto": "hwxn__QtOXB", "comment": "\nWe notice that Reviewer 4's concern about why TNF works firstly lies in why rare word embeddings are poorly trained in BERT and if pre-pretraining word embedding methods can solve this problem. We will start from there to explain our analysis about why TNF works and present our empirical evidence.  \n\n---\n1. Regarding the comments \"the gradient flow in BERT can already lead to good rare word embeddings. \" (Second paragraph in \"Weakness\")\n\nWe respectfully disagree with the reviewer's claim that \"the gradient flow in BERT can already lead to good rare word embeddings. \" The learning of rare word embeddings has been a challenging problem since the first day of deep learning for NLP. Even when the input embedding and the output embedding (you mentioned softmax layer) are tied, the embedding of rare words are still very poorly optimized, see [1]. There are even recent works suggesting that rare word embeddings act as noise[2]. We have also listed several related and latest references in the related work section. \n\n---\n2. Regarding why TNF works.\n\n\nWe think TNF works because it can benefit the BERT model as a whole by providing a more effective data utilization. The reasons behind are as follows. \n\nAccording to many previous works (see the related work section), rare words' embeddings are usually poorly optimized. There are even recent works suggesting that rare word embeddings act as noise[2]. Training a model from noisy inputs is less effective in general (see [3](Figure 1a) for more observations about how noisy inputs hurt the optimization and generalization of a machine learning model). For language pre-training tasks specifically, the input sequences with noisy rare word embeddings contain less semantically meaningful information for the Transformer to learn, making the whole model training ineffective. \n\nGiven the above facts, we aim to reduce the noise from rare word embeddings in a sentence, to improve the pre-training of the whole model. Specifically, we achieve this by providing more precise semantics of rare words in a sentence using surrounding contexts from other sentences (contextualized information saved in notes). For the original method, Transformer receives thousands of noisy embeddings like 'Covid-19' containing little semantic meanings. Suppose we equip 'Covid-19' with a note that contains previous surrounding contextual information such as 'pandemic' and 'global crisis' (which are popular words). One could imagine that the sentence 'Covid-19 (+pandemic + global crisis) causes thousand of lives' could have more precise semantics, which makes the training of the Transformer model more effective.\n\n---\n3. Empirical evidences for supporting 2.\n\nTo support such a claim, we conduct a new experiment as below:\n\nwe calculate MLM loss on sentences in the validation set that do not have rare words for both BERT and BERT-TNF at different checkpoints. The total number of sampled sentences that satisfy the condition is roughly 20k.  As those testing sentences don't contain rare words, the note dictionary will not be called, and we can purely compare the performance between the two Transformer models in BERT and BERT-TNF. From the table below, we can see that BERT-TNF's loss is consistently lower than BERT at all checkpoints, which suggests the entire Transformer model was improved using our method. \n\n#Iter 20k 50k 100k 200k 400k 600k 800k 1000k\n\nBERT 2.709 2.180 1.947 1.788 1.667 1.602 1.556 1.522\n\nTNF 2.692 2.145 1.902 1.736 1.619 1.558 1.513 1.479\n\nIn the submission, we also showed that TNF without notes works well on finetuning downstream tasks, indicating that TNF can provide a better pre-trained model itself (without notes saved in downstream tasks). Moreover, even for sub-tasks with almost no rare words occurring in the training set (0.47% rare word coverage in CoLA), BERT-TNF can still outperform BERT on it. \n\nAll empirical results above support our motivation and indicate that the model as a whole (including the Transformer layers and token embeddings) is better pre-trained with TNF.\n\n---\n4. Regarding the comment \"the notes of rare words could 'explain away' rare word embeddings.\" (Third paragraph in \"Weakness\")\n\nWe respectfully disagree with Reviewer 4 that the notes of rare words could 'explain away' rare word embeddings. Given that rare-word embeddings are already poorly-trained and act as noise, their quality has little margins of being hurt. That being said, adding notes for rare words does not 'explain away' from rare word but 'explains' the rare word better. It is because as illustrated above, after adding notes, the input sentence with this rare word contains more accurate semantics about the rare word for the model to learn.\n", "title": "Response to AnonReviewer4"}, "Lud0JlJRD9h": {"type": "review", "replyto": "lU5Rs_wCweN", "review": "*Summary*: This paper proposes a method for improving pretraining convergence speed by augmenting the representations of rare words with the mean-pooled representations from their previously-occuring contexts (\u201cnotes\u201d, stored in a \u201cnote dictionary\u201d). The method considerably speeds up the convergence of pretraining BERT and ELECTRA, and the authors furthermore show that these models perform better when fine-tuning on downstream GLUE tasks (likely because the models were undertrained to begin with, so converging faster alleviates this issue).\n\n*Strengths*: The method is surprisingly simple and empirically quite effective. It's especially interesting to see that BERT + TNF at 400K steps has better GLUE performance than BERT at 1M steps.\n\n*Weaknesses*: the paper does not do a convincing job of arguing that the reasons for the faster convergence comes from better modeling of rare words---I\u2019m still not entirely sure why this works so well. Do these rare words commonly show up in GLUE (and thus, the method is helping because your representations of rare words are better)? It seems like TNF is actually improving the representations of more-common words as well.\n\n*Recommendation*: 7 Despite the lack of clarity around why exactly this method works so well, the method seems empirically useful and straightforward to apply. I expect that this will be useful to practitioners interested in applying BERT and similar pretraining strategies to new corpora and domains.\n\n*Questions*:\n\nIt\u2019s a bit unclear to me that note-taking itself is required for this to work well...in the COVID example presented in the introduction, if you see the sentence \u201cThe COVID-19 pandemic is an ongoing global crisis\u201d, isn\u2019t it possible that MLM itself is sufficient to associate the embedding of \u201cCOVID-19\u201d with \u201cpandemic\u201d and \u201cglobal crisis\u201d? Do you have further evidence to show that note-taking is actually improving the representations of rare words, besides GLUE score (which might not be very indicative, since the rare words might not show up in GLUE).\n\nThe Construction of Note Dictionary: Does 3.47B refer to the number of types or the number of tokens? Why not define keys with frequencies less than 100 in the dictionary as well (since you only use types that show up between 100 to 500 times)?\n\n\u201cIt means that to reach the same performance, TNF can save 60% of pre-training time. If models are trained on 16 NVIDIA Tesla V100 GPUs, BERT-TNF can reach BERT\u2019s final performance within 2 days while it takes BERT 5.7 days.\u201d: Is the 2 days vs 5.7 days an actual wallclock measurement? Or, are you hypothesizing this based off of the loss curves?\n\n*Missing / Erroneous Citations:*\n\n\u201cIt is well-known that in a natural language data corpus, words follow a heavy-tail distribution (Larson, 2010)\u201d This is more-commonly known in the NLP community as Zipf\u2019s law. Better cites would be:\n  - Zipf G. The Psychobiology of Language. London: Routledge; 1936.\n  - Zipf G. Human Behavior and the Principle of Least Effort. New York: Addison-Wesley; 1949.\n\n*Miscellaneous comments:*\n\n\u201cMoreover, completely removing those sentences with rare words is not an applicable choice either since it will significantly reduce the size of the training data and hurt the final model performance.\u201d: I agree that it\u2019s a bad idea to remove sentences with rare words, but I disagree that the issue is reducing the size of the data---you can always go collect more data and filter it to not include rare words. It\u2019s more likely that the issue is that removing sentences with rare words would reduce the diversity of the pretraining data, which would be harmful\n\n\u201cOur method to solve this problem is inspired by how humans manage information.\u201d: I think the connection to human note-taking is tenuous at best, and would omit it; the motivation remains clear without this.\n", "title": "Interesting paper with strong empirical results, but lacks convincing explanations for why the method works well.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "cdFOrFGz6wJ": {"type": "review", "replyto": "lU5Rs_wCweN", "review": "This work aims at accelerating pre-training by leveraging the contextual embeddings for the rare words. It is argued that the inadequate training of rare words slows down the pre-training. The authors then proposed to keep a moving average of the contextual embeddings for the rare words and use it to augment the input embeddings of the rare words. This technique is applied to BERT and ELECTRA and is shown to improve over the baseline. \n\nStrength:\n\n1. This work proposes a simple approach to accelerate the pre-training, with only a small memory and compute cost during training. The empirical study on BERT and ELECTRA supports the claimed improvements. \n\n2. It provides an interesting view towards the rare words problem that the rare word not only has worse embeddings but also slows down training of the whole model. \n\nWeakness:\n\n1. It is argued that the proposed approach helps with rare words problem. But it will help to add more experiments to see how much more benefit we can get from it. For example, maybe the use of contextual embeddings are actually helpful for all the words or sub-words instead of just the rare words. \n\nSpecifically, regarding \" we define keys as those words with occurrences between 100 and 500 in the data corpus\", How are the range 100 to 500 chosen? Have you tried it on words appearing lower than 100 or higher than 500? As mentioned above, it would be interesting to see if this approach can be applied to more words or subwords to get even more gains. \n\n2. Some design choices needs more details or explanations. \n\nFor example, why does the NoteDictionary use \"words\" instead of \"sub-words\" as keys? It seems using \"sub-words\" could cover a broader range of sentences with a NoteDictionary of the same size. It will also be easier to use during pre-training, for example, you could use the contextual embeddings to improve the word embeddings of the sub-words directly to avoid having an extra NoteDictionary. \n\nAnother example is how the window size is chosen, since it seems an important new hyperparameter. \n", "title": "An interesting way to accelerate pretraining, it would help to add more analyses and details.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}