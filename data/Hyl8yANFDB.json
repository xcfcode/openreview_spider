{"paper": {"title": "Assessing Generalization in TD methods for Deep Reinforcement Learning", "authors": ["Emmanuel Bengio", "Doina Precup", "Joelle Pineau"], "authorids": ["bengioe@gmail.com", "dprecup@cs.mcgill.ca", "jpineau@cs.mcgill.ca"], "summary": "Empirical investigation showing TD, in particular TD(0), may be preventing generalization in DeepRL", "abstract": "Current Deep Reinforcement Learning (DRL) methods  can exhibit both data inefficiency and brittleness, which seem to indicate that they generalize poorly. In this work, we experimentally analyze this issue through the lens of memorization, and show that it can be observed directly during training. More precisely, we find that Deep Neural Networks (DNNs) trained with supervised tasks on trajectories capture temporal structure well, but DNNs trained with TD(0) methods struggle to do so, while using TD(lambda) targets leads to better generalization.", "keywords": ["reinforcement learning", "deep learning", "generalization"]}, "meta": {"decision": "Reject", "comment": "This paper received three reviews. R1 recommends Weak Reject, and identifies a variety of concerns about the motivation, presentation, clarity and soundness of results, and experimental design (e.g. choice of metrics). In a short review, R2 recommends Weak Accept, but indicates they are not an expert in this area. R3 also recommends Weak Accept, but identifies concerns also centering around clarity and completeness of the paper as well as some specific technical questions. In their response, authors address these issues, and have a constructive back-and-forth conversation with R1, who remains unconvinced about significance of the empirical results and thus the conclusion of the overall paper. After the discussion period, R3 indicated that they weakly favored acceptance but agreed that the paper had significant presentation issues and would not strongly advocate for it. R1 advocated for Reject, given the concerns identified in their reviews and followup comments. Given the split decision, the AC also read the paper. While the work clearly has merit, we agree with R1's comment that it is overall a \"potentially interesting idea, but the justification and presentation/quantification of results is not good enough in the submitted paper,\" and feel the paper really needs a revision and another round of peer review before publication. "}, "review": {"rkeaCnz9jr": {"type": "rebuttal", "replyto": "B1xpP13YsH", "comment": "(1&2)\nEven if no quantity is varied but the random seed of the experiment, this trend still exists. It also seems to hold across RL methods; as mentioned we tried PER, which had slightly better gain and also rewards; we also briefly tried C51, which seemed to also have better gains and rewards. Are there other quantities that you suggest we should try varying?\nOf course, the gain is a high variance measure. A number of other things contribute to the final agent\u2019s performance. This gain measure does not perfectly explain performance, that would be _very_ surprising, but it certainly looks like there is some link. \nThe case we make is *not* that researchers should be using this measure to choose their hyperparameters, it is simply an observational one: we know that DNNs on supervised learning (SL) tasks have a certain sets of behaviours, but we find that some of these behaviours are lacking in Deep RL. All the differences we observe are consistent with many other papers, including common intuitions in SL with DNNs.\n\nAn important rationale for studying this metric is to be able to compare SL and RL, not even necessarily RL algorithms together. Considering we do find major differences between SL and RL using this metric, it seems logical that something non-trivial is causing these differences. Considering that after some decent amount of investigating we are still unable to identify these causes, it seems natural to share this problem with the community.\n\nAnother important rationale for this metric, mentioned in the paper, is that \u201cany kind of generalization must arise through the accumulation of parameter updates\u201d. We should expect something about these parameter updates that makes them \u201cgeneral\u201d. This metric is the most successful measure of such a phenomenon we could find.\n\nOne of your concerns is that our metric measures TD gain rather than e.g. True Gain. It would be extremely surprising for models trained with TD to generalize to other losses. This is visible in Fig 1a where the models trained with MC do not have a nice TD gain curve.\nAnother concern was that improvements in the metric did not reflect improvements in RL. This is addressed above, but a sub-concern was that it wasn\u2019t clear which behaviour is preferable. This is of course an open question, but an easy target for desirable behaviour that we would like to see for TD methods is the behaviour of SL methods (considering SL methods converge quite quickly to the target V and are known to generalize well), which are identified in Figure 1.\n\n\nAdditional worry:\nAs mentioned in the feedback, the magnitudes of gain, both at 0-offset and elsewhere, are fairly consistent across environments and runs. Also note that the learning rate is kept constant across environments and runs for the figure. We did try dozens of different normalizations, including normalizing by the 0-offset gain, but none proved to be informative.\n\n(3)\nThere is a fairly generalized complaint, no pun intended, among RL researchers that current Deep RL methods have extremely poor generalization. A concise statement of the open problem thus seemed unnecessary, but perhaps it bears repeating: \u201cwhy are DNNs doing decently at generalization in SL, but doing very poorly in RL?\u201d\nTo rehash the Discussion section: the fundamental problem being raised by the paper is that we *can* measure these differences quantitatively, but we *can\u2019t* explain where they come from. This problem, we believe, requires attention and further research beyond our group.\n", "title": "re: Feedback"}, "ByxGozwwsS": {"type": "rebuttal", "replyto": "rJx71n4AtS", "comment": "Dear reviewer, thank you for your feedback. We agree that there are many nuances to our work that make presentation challenging, and any suggestions to improve this are welcome.\n\n> On the metric name\nWe acknowledge that calling (9) a measure of generalization conflicts somewhat with existing notions. We gave a lot of thought to what would be an appropriate name for this phenomenon, in the end we settled for the lengthy \u201cgradient update generalization\u201d. \u201cGradient\u201d ecompasses the parameter sharing aspect of this, \u201cupdate\u201d refers to the measure being related to a single discrete update, and \u201cgeneralization\u201d refers to the desire to improve on more than some given samples (which is related to generalization in the classical sense). Would something like \u201cgradient update improvement\u201d or \u201cgradient update gain\u201d make more sense?\n\n> On optimization aspects\nWe agree that the exposition of the optimizers adds complexity. The current presentation is meant to be more compact, and to highlight the consistent differences we found between optimizers that are also consistent with the findings of other papers.\n\n> On Figure 3\nWe will revise the caption of this Figure as well as the references to it within the text. Here is the current new caption:\n\u201cFigure 3: Policy evaluation on Atari: evolution of the distribution of $\\Delta$, the difference since last visit, during training. In (a) the DNN is forced to memorize, as such the density of $\\Delta$ is concentrated around 0 (thin red/white band). In (b-c), Q-Learning and Sarsa, the density is much less peaked at 0 (larger yellow/green bands) as the DNN learns about states without visiting them. In (d) the DNN learns quickly presumably without memorizing (the distribution of $\\Delta$ is more spread out and not as concentrated around 0, seen by the larger yellow/green band), as it is trained on Monte-Carlo returns, and quickly converges as can be seen by the high density of positive $\\Delta$s early. In (e,f) we see the effect of using $\\lambda$ returns (see appendix A.6 for all values of $\\lambda$).\u201d\n\n> On Q_MC\nUnless Q_MC refers to something else, we already have this experiment. In section 2.2 we consider Q_MC to be a \u201csupervised\u201d task, using eq (10), and present its results in Figure 1a which we simply refer to as MC. Should we replot the MC curve for 1a in Figure 2 for completeness? (both experiments are using the same data)\n\n> Memorization section\nIn retrospect the numbers indeed did seem unlikely. We investigated and found a typo in the code. Here is the full table (added in appendix, Table 1, page 13):\n\nD / N     2       10        50\n10k      5.7      8.7      8.5\n100k    38.8    45.6    56.5\n500k    44.7    79.3    85.8\n       (error %)\n", "title": "Feedback"}, "rkeifMwDiS": {"type": "rebuttal", "replyto": "rklkuYFl5r", "comment": "Dear reviewer, we very much agree that this is an interesting problem. If you have any further questions we will be happy to answer them.", "title": "Feedback"}, "rkxJ-GwvsB": {"type": "rebuttal", "replyto": "SkenPkootB", "comment": "Dear reviewer, thank you for your feedback. The nuance of the phenomenon we are attempting to characterize does bring challenges, and requires simultaneously considering new questions, new experiments, and new metrics.  We welcome your suggestions for developing and presenting findings in a clearer manner. \n\n> On (1) and (2), motivation and importance of the metric\nA previous version of this paper had a scatter plot showing the link between the average magnitude of update gain and lifetime rewards of an agent. It appeared obvious to us that this link existed, but we will reintroduce this plot, which is empirical evidence of $Y^{near}$ being indicative of speed of learning (correlation coefficient of r=0.433, see Fig 15 in revised version).\n\n> On (3), vague and uninformative conclusions\nWe believe we have identified a behaviour which, at least to us and most RL researchers with whom we discussed this, is surprising and unexpected. We have attempted in many ways to understand how this behaviour arises, ruling out many hypotheses, unfortunately without much success so far. Knowing its cause would of course bring even greater insight, but we nonetheless strongly believe that publishing this unexpected result would be highly valuable for the community.\nWe disagree that these results are uninformative. Most papers have a conclusion along the lines of \u201cdo this\u201d or \u201cdon\u2019t do that\u201d; our paper instead suggests that the Deep RL community may be unaware of a problem with large ramifications. It is our opinion that identifying directions of research is valuable.\n\n> On difference since last visit\nA common observation of memorizing deep neural networks is that they act like nearest-neighbour classifiers or even tabular lookups. This measure simply aims to disprove this for DQN by showing that, unlike for a DNN trained to memorize, states change value without being \u201cvisited\u201d for a gradient update. So while the first results suggest that DQN could just be \u201cmemorizing\u201d states, these results suggest that DQN doesn\u2019t 100% memorize. \n\n> On averaging plots over states and environments\nWe tried many different plots to see if patterns would emerge. For example, we tried separating curves by how far the next reward was, or by distance to a terminal state. We were unable to see any robust pattern specific to some characterization of the MDP/trajectories.\nFor environments, there is some amount of difference, usually depending on how dense rewards are, but these differences did not strongly affect the shape nor the magnitude of the update gain curves. As such, we simply averaged over all environments.\n\n> On plot scale\nWe agree that this should be clearer in the text. Figure 1 and Figure 2\u2019s y axes are an order of magnitude apart (Figure 2a\u2019s y axis is 10x smaller, Figure 2b\u2019s 100x). Showing them both in the same plot is feasible but it seemed to us more informative to separate the two.\n\n> On using \u201cdistance\u201d\nYou are right, we will replace \u201cdistance\u201d with \u201coffset\u201d.\n\n> On visual artifacts\nWe are sorry to hear this. We are unable to reproduce this problem, but will investigate on other browsers/OS. Here is a rasterized version of the pdf if you need it: https://imgur.com/a/Qdv0JLt\n\n> Architectures\nAs mentioned previously we will add a scatter plot showing the effect of varying the architecture and capacity.\n\n> Replay buffer\nYes, the buffer is sampled uniformly. We also briefly experimented with Prioritized Experience Replay, which had slightly larger update gain but still in the same order of magnitude.", "title": "Feedback"}, "SkenPkootB": {"type": "review", "replyto": "Hyl8yANFDB", "review": "Summary:\nThis paper performs an empirical evaluation of generalization by TD methods with neural nets as function approximators. To quantify generalization, the paper considers the change in the loss function at similar states to the one where the update rule is being applied (where \u201csimilar\u201d is usually defined as nearby in time). It comes to a variety of conclusions including that TD(0) does not induce much generalization, that TD(0) does not induce as much generalization as supervised learning, and that the choice of optimizer and objective changes the behavior according to their generalization criteria in various ways.\n\n\nDecision:\nThis paper should be rejected because (1) the motivation is unsubstantiated, (2) the main metrics used (\u201cgain\u201d at nearby states and difference since last visit) are of questionable importance, and (3) the conclusions are often vague and not informative.\n\n\nMain argument:\nMotivation:\n- The paper motivates the need for an evaluation of \u201cgradient update generalization\u201d by claiming that it is related to sample complexity and brittleness of RL algorithms. While I agree that this is plausible, there is nothing empirical or theoretical to support this claim in the paper or in the references. This is a significant problem since this assumed connection underlies everything in the paper.\n- Also, it is this sort of generalization that differentiates the function approximation setting (where there are no convergence guarantees for TD without strong assumptions) from the tabular setting (where there are convergence and sample complexity guarantees). \n\nMetrics:\n- The main metric used is TD gain on temporally nearby states. The TD gain is defined as the reduction in the squared TD error at some state s\u2019 when an update is applied at some other state s. Note this metric does not capture all update generalization, but only update generalization as it effects the TD error.\n- It is not evident, nor supported by the paper, that improvements in this metric at nearby states necessarily improve performance of the algorithm. This is especially true when there is a tradeoff between improvement at very nearby vs. somewhat nearby states, it is not clear which behavior is preferable (and this behavior seems to occur in experiments). As a result there is no clear way to use this metric to determine which algorithms are preferable. \n- The other metric used in the paper is the change in value function at a state since the last time that state was sampled from the buffer. It is also not clear whether this measurement is necessarily important for the same reasons as above. \n\nConclusions:\n- In general the results are presented in plots that do not give clear implications and are difficult to read. I understand that we cannot expect completely clean results on such empirical questions, but the results would be potentially much more convincing and clear if the hypotheses were clearly stated and then one plot could summarize the result with respect to each hypothesis. For example, the gain plots are difficult to compare and interpret clearly and the memorization plots do not give such clear results (eg. 3a and 3d look visually fairly similar). Another example is the comparisons between optimizers, which are fine to have as a specific point in one section, but do not need to be in every plot.\n- The main result claimed in the paper is that there is little generalization from TD(0) when compared to supervised learning. This seems to be born out by the difference between figure 1a and 2a, but the difference in scale makes it a bit difficult. It is also not clear how to quantify this result or what the implications are. \n- The plots are averaged across all states over all of training and all environments, while this is somewhat rationalized in figure 11, I am worried that this may be covering some additional complexity/ambiguity in the results. \n- The result about TD(lambda) seems to be born out by figures 3e and f and 5, but is also unsurprising since the objective explicitly averages across temporally nearby states. Again it is not clear how this temporal consistency of updates should be interpreted in terms of the goals of the RL algorithms.\n\nAt a higher level, the paper feels like a solid preliminary set of experiments rather than a paper organized around a clear motivating idea with clear hypotheses to test. The results would become more interesting if the metrics used could be connected back (either empirically or theoretically) to an objective. For example, does generalization in the sense defined in the paper give better performance at value estimation or return maximization? Can these results be quantified in a more direct way than the plots presented in the paper? \n\n\nAdditional feedback:\n- The plots all have different scales which makes them difficult to compare\n- Using \u201cdistance\u201d to refer to the relative tilmestep of samples in the replay buffer is confusing (distance cannot be negative)\n- There are strange visual artifacts (horizontal lines) in Figures 3, 5, 10, 12, 13, and 14\n- Sections 3.7 and 3.8 very briefly present results that seem to distract from the main thread of the paper\n- Using one network architecture across all experiments seems like it may have a significant impact on the results. I understand that the architecture chosen is standard and testing different architectures is time consuming, but making broad claims about the algorithms based on only one architecture is potentially dangerous. \n- I assume that the replay buffer is always being sampled uniformly, but I could not find this detail in the paper. ", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "rJx71n4AtS": {"type": "review", "replyto": "Hyl8yANFDB", "review": "The manuscript is analyzing the \"generalization\" in TD(lambda) methods. It includes supervised learning from trajectories, on-policy imitation learning, and basic RL setting. Moreover, memoization performance has also been measured. Main conclusion is the fact that TD(0) performs very similar to tabular learning failing to transfer inductive biases between states. There are also additional surprising results about optimization.\n\nThe empirical study is rather complete and significant. It raises interesting questions for the community and states some clear open problems. \n\nResults are conclusive and interesting. I believe it is a study which a practitioner using TD-based method should be aware of. Hence, I believe it is impactful.\n\nOn the other hand, the manuscript has some significant issues which need to be resolved as follows:\n\n- One major issue is calling the analyzed metric \"generalization\". Generalization by definition requires something beyond what is seen. I believe the quantity defined in (9) is generalization. However, it can not be computed. Hence, calling its empirical version, \"generalization\" is confusing and a clear misuse of the term. I strongly urge authors to call the observed quantity something else. \"Empirical expected improvement\", \"gradient regularity\", \"expected gain\", etc. are some candidates come to my mind. \n\n- The optimization aspect is very interesting; however, it confuses the exposition significantly. I think it is better to give all results using adam first, and then showing the comparisons between adam and rmsprop later would be much more readable and easier to understand.\n\n- There are some clarity issues in the explanation of the experiments. Figure 3 is very confusing and it requires multiple reading to be understandable. A clearer visualization or a better explanation would improve the paper.\n\n- I am puzzled about why the authors did not use Q_MC in policy evaluation experiments (Section 3.3). I think it can very well be used in a straightforward manner. It would be an interesting addition to the experiments.\n\nMinor Nitpicks:\n- Memorization section is not clear. The discussion on N is very confusing as \"14.4% for N = 2 and of 16.1% for N = 50\" does not match any of \"10.5%, 22.7%, and 34.2%\" Can you give full error table in appendix?\n\nOverall, I like the study and suggest to accept it hoping authors can fix the issues I raise during rebuttal period.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "rklkuYFl5r": {"type": "review", "replyto": "Hyl8yANFDB", "review": "This paper studies the generalization property of DRL.  Fundamentally, this is a very interesting problem. The authors experimentally analyze this issue through the lens of memorization, and showed that it can be observed directly during training. This paper presents the measure of gradient update generalization, best understood as a side-effect of neural networks sharing parameters over the entire input space. This paper is very written, and well organized.  The experiments are quite solid. However  I may be not capable in judging the novelties and contributions of this paper, since I did not conduct research on this topic.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}}}