{"paper": {"title": "Exploration in Reinforcement Learning with Deep Covering Options", "authors": ["Yuu Jinnai", "Jee Won Park", "Marlos C. Machado", "George Konidaris"], "authorids": ["yuu_jinnai@brown.edu", "jee_won_park@brown.edu", "marlosm@google.com", "gdk@cs.brown.edu"], "summary": "We introduce a method to automatically discover task-agnostic options that encourage exploration for reinforcement learning.", "abstract": "While many option discovery methods have been proposed to accelerate exploration in reinforcement learning, they are often heuristic. Recently, covering options was proposed to discover a set of options that provably reduce the upper bound of the environment's cover time, a measure of the difficulty of exploration. Covering options are computed using the eigenvectors of the graph Laplacian, but they are constrained to tabular tasks and are not applicable to tasks with large or continuous state-spaces. \nWe introduce deep covering options, an online method that extends covering options to large state spaces,  automatically discovering task-agnostic options that encourage exploration. We evaluate our method in several challenging sparse-reward domains and we show that our approach identifies less explored regions of the state-space and successfully generates options to visit these regions, substantially improving both the exploration and the total accumulated reward.", "keywords": ["Reinforcement learning", "temporal abstraction", "exploration"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper considers options discovery in hierarchical reinforcement learning. It extends the idea of covering options, using the Laplacian of the state space discover a set of options that reduce the upper bound of the environment's cover time, to continuous and large state spaces. An online method is also included, and evaluated on several domains.\n\nThe reviewers had major questions on a number of aspects of the paper, including around the novelty of the work which seemed limited, the quantitative results in the ATARI environments, and problems with comparisons to other exploration methods. These were all appropriately dealt with in the rebuttals, leaving this paper worthy of acceptance."}, "review": {"Byx79-ee9r": {"type": "review", "replyto": "SkeIyaVtwB", "review": "The paper proposes an algorithm to extend the recently proposed method of \u201ccovering options\u201d from a tabular setting to continuous state spaces (or large discrete state spaces). The proposed algorithm approximately computes the second eigenfunction of the normalized laplacian of the state space, uses it to identify an under-explored region and trains an option to terminate in such a region. Each new learnt option is added to the initial set of primitive actions and a policy over this growing set of actions is learnt separately. An online algorithm is also proposed that does the above option learning process intermittently in addition to training for an external task. The paper shows empirical evidence of better or equal performance to base algorithms which do not discover options, prior work such as DIAYN (Eysenbach et. al, 2019) (that discover options via mutual information maximization between visited states and options), as well as ablations of their proposed method with different number of options.\n\nI vote for weak reject due to (1) the idea of covering options (Jinnai et. al., 2019b) and the approximation for the graph laplacian (Wu et. al., 2019) both have been shown in prior work and the novelty in this paper seems to be limited to putting together these two ideas, and (2) the paper shows quantitative results for options discovered for simple environments whereas only qualitative options are shown for harder exploration tasks, and (3) given that exploration is a key problem being addressed, a comparison to other exploration algorithms which are non-option based methods has not been shown -- which makes for a weak argument for using an option-based method for exploration as opposed to existing methods for exploration.\n\nOther comments:\n- The paper does a good job at explaining covering options, the approximations involved in their algorithm and how options can be learnt with the help of such approximations. It makes sense that an algorithm that takes into account state connectivity will do better than DIAYN (Eysenbach et. al., 2019) which just promotes diversity of visited states across options. \n\n- The paper made some assumptions on the implementation of DIAYN, citing lack of details in Eysenbach et. al (2019). Were these details not available in the code released by DIAYN on their project page? (I\u2019m not sure if I should post the link here, but it is easy to find). I have some concern that a fair comparison may not have been made due to the paper\u2019s implementation of this baseline, but I am also not sure how important of a comparison this is given that their method for discovering options is quite different. I would argue that DIAYN is different enough that the proposed method deserves comparisons with methods for exploration that do not use options.\n\n- In the intro, \u201c...extend a theoretically principled approach for option discovery to the non-linear function approximation case\u201d seems to be highly misleading, since it suggests that the theoretical guarantees are extended, which is not the case in this paper as the approximations have no states guarantees.\n\nThe proposed method seems to have the potential to be a good at exploration but the paper does not show quantitative experiments for hard exploration tasks such as Montezuma\u2019s Revenge, or other Atari ALE environments. I am curious to see how many levels of Montezuma\u2019s revenge can be covered by simply applying deep covering options to it.\n\nReferences:\nAll references are same as the paper.\n\nAfter rebuttal: The authors have addressed most major concerns and I am increasing my score to weak accept.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "rygIzO_noH": {"type": "rebuttal", "replyto": "SJxcXSBuoS", "comment": "Please let us know if our response addresses your concerns, or if some clarification is needed.", "title": "Response to Reviewer #2"}, "HJlGfUk0KH": {"type": "review", "replyto": "SkeIyaVtwB", "review": "Summary\nThe authors introduce deep covering options, an online mechanism to extend the covering options to large state spaces. They claim their method discovers options that are task agnostic. The method is evaluated in sparse reward domains and claims to gain improvement in exploration and performance as well.  The authors extend the recent developments in eigenfunction estimation of the Laplacian to a principled approach for option discovery to non-linear function approximation. \n\nCovering options compute the second smallest eigenvalue and the corresponding eigenvector f of the Laplacian exactly by solving a constrained optimization problem (Eq2). However, this requires the adjacency matrix A as input, and a constrained optimization problem is hard to solve using gradient-based methods. To overcome this, the authors propose an approximation of the computation of the Laplacian with Eq3. This allows them to have a constraint-free objective to compute the eigenfunction which is now dependent on the trajectories and avoids requiring the state-space graph. I believe the paper presents interesting ideas and is definitely a very useful contribution.  However, the paper needs work on thorough empirical analysis: could use more rigorous baselines especially to fairly evaluate the gains in exploration. \n\nDetailed comments:\n(I) Major Concern:  Figure 1 of this work is exactly the same as Figure 2 of the Jinnai et al., 2019b. It is not clear to me whether a) this is being referred for the purpose of giving intuitions. If yes, then it should be cited as Figure from Jinnai et al., 2019b or b) this is a new figure generated differently, it is not clear what the difference is. Overall, there is overlapping content and should be clarified what is original work and what is being referred to. \n\nReusing content from another paper without proper attribution is normally considered plagiarism. More precisely; Jinnai et al., 2019b. mention that the \u201csecond smallest eigenvalue of L is known as the algebraic connectivity of the graph and its corresponding eigenvector is called Fiedler vector\u201d and caption this figure:\n\u201cFigure 2: The distance between the red state and all other states, measured via the Fiedler vector (left) and Euclidean distance (right). The Fiedler vector captures the connectivity of the graph, so distances measured using it reflect path lengths in the graph; the pair of nodes with the maximum and the minimum value are the farthest apart\u201d \nThis  work (currently in review) captions this figure as: \n\u201cFigure 1: The distance between the red state and all other states, measured via the second eigenvector (left) and Euclidean distance (right). The second eigenvector captures the connectivity of the graph, so distances reflect path lengths in the graph; the pair of nodes with the maximum and minimum values are the farthest apart.\u201d The only words changed are Fiedler vector to the second eigenvector. Please explain!\n\n(II) An important step in the algorithm is line 3\u201d identify an under-explored region in the state-space using the eigenfunctions\u201d. Where does the parameter k come from? Is this a hand-designed parameter such as in PinBall it is somehow set to 30, for Mujoco tasks this is somehow set to 10, and for Atari, this is set to 4.  Would it be possible to comment on the hyperparameter threshold percentile k? Why is this a hard-coded choice? Would it be possible to learn this *simultaneously* as the options? \n\n(III) Can you comment on to what extent do you see theoretical guarantees of covering options apply to the function approximation case as they no longer hold when going from tabular setting to the nonlinear function approximation? Would it be possible to also comment on what can be said about any guarantees at all if the state space is very very large? Forex: imagine a lifelong learning scenario where the environment is really big, and it is just not almost impossible to visit all states, how does this objective function of minimizing the upper bound on the expected cover time constitute the right choice? \n\n(IV) Intuitively the pseudo-reward seems a lot related to goal-based rewards, where the skill is learned to reach different goals such as in DIAYN. Can you comment on how is this different and why is the proposed approach better in principle?\n Empirically, It is not clear why DIAYN was not compared as it is and with all the stated modifications.  In DIAYN, setting the initiation set to be the whole state space seems counterintuitive. \n\n(V) Regarding the connectivity of the states to generate a diverse set of options: there seem to be connections to the work on constructing options using stronger guarantees Castro & Precup, 2011. It might be useful to comment on this and discuss this in the paper.\n\n(VI) \u201cWe sampled 200 episodes of length 2000 with a uniform random policy to generate each option.\u201d Would there be smarter ways to generate each option? Is there a reason to generate each option in this fashion?\n\n(VII) Since the idea here is to connect states that are closer in terms of time, but further apart, how does the proposed approach comparison to successor options. I would imagine a comparison to successor options would be quite valuable to this. Please comment on this. \n\n(VIII) \u201cTermination set generated by deep covering options tends to be larger than..\u201d Intuitively speaking, it is not clear why this is a good idea. Wouldn't we want the options to be peaked in places they terminate and initiate in, and therefore have smaller termination/initiation set? \n\n(IX) In the Mujoco tasks: I would expect to see the baseline performance reported in the main paper i.e. DIAYN for continuous control tasks too. It is not clear by \u201cdid not outperform the baseline\u201d as to how the proposed approach fairs in comparison to DIAYN. Looking into the appendix, It is still not clear why the plot of DIAYN in mujoco tasks is not included. \nSince the key ideas in this work propose overcoming exploration as the main challenge, one would expect a comparison to the state-of-art in exploration, for instance [1]. Also since the work builds on eigenoptions, it would help compare with [2] as well. Without these comparisons, I find the empirical analysis rather weak. \n\n(X) Pinball exploration visuals show concrete gains in state-space explored 4a-d. This is also evident in continuous control tasks 4e-f-g. However, in both cases, I find the comparison weak as the authors do not compare against state-of-the-art exploration baselines.  \n\n(XI)  The termination sets in ALE are interesting in that they do convey that options terminate in different regions of the state space: but one can also see options terminating in different regions of the state space in Harb et al, 2018 in Amidar here for example. It is also counter-intuitive as to why the options terminate in regions that do not overlap with visible goals for example in Montezuma Revenge in the key or skull. Perhaps one benefit here is that there is no reward information, but we do not see either the performance curve or the nature of options in ALE, so it is really hard to make a strong claim either way.\n\nAlthough the options in pre-training were generated without a reward- I would recommend using these options in different tasks to make the claim \u201ctask agnostic options are discovered\u201d much stronger. \n[1] Count-based exploration with the successor representation.\n[2] A laplacian framework for option discovery in reinforcement learning\n\nOverall\nScales a principled approach to function approximation for deep covering options. The method seems to be computationally tractable. The approach can be applied to both settings where an unsupervised pre-training phase is available and also in a fully online setting.\n\nHowever, DIAYN is the only baseline in Mujoco and is not shown. It\u2019s unclear why other baselines were not used such as Eigenoptions, which seems like a very valid baseline. In addition, baselines of well-known exploration algorithms also have not been fully explored. The paper needs a more thorough evaluation of the proposed method to make the claims stronger.\n\nPlease note that although I have marked a weak reject, I am open to adjusting my score if the rebuttal addresses enough issues. ", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "r1x9SnV2iB": {"type": "rebuttal", "replyto": "ryxH9Ld9sS", "comment": "Thank you very much for the comments.\n\n> If we chose k such that for large state-spaces the termination sets won't be too large, yet we observe that deep covering options tend to have large termination set. Could you elaborate on the impact of different values of k. It would be nice to have an ablation study with different values of k.\n\nWe will add an ablation study with different values of k to understand its impact to the performance in the camera ready.\nWe didn\u2019t have enough resources to run the ablation study before the deadline of the rebuttal.\n\n\n> The termination sets in ALE are interesting in that they do convey that options terminate in different regions of the state space: but one can also see options terminating in different regions of the state space in Harb et al, 2018 in Amidar here for example. It is also counter-intuitive as to why the options terminate in regions that do not overlap with visible goals for example in Montezuma Revenge in the key or skull.  We do not see either the performance curve or the nature of options in ALE, so it is really hard to make a strong claim either way in ALE.\n> But we do see learning curves for option-critic methods [1,2] for instance in ALE. Although I agree that the options found in [1,2] do not aim to improve exploration, it would still be meaningful to evaluate performance curves of deep-covering options in ALE, and how they compare to other existing methods.\n\nThe goal of deep covering options is to construct a skill to efficiently explore the state-space agnostic to its reward structure whereas option-critic methods learn options using the reward from the environment. Thus, our method is orthogonal to option-critic so we can apply both algorithms to construct options. One may initialize options using deep covering options and then refine them by option-critic using the reward information.\n", "title": "Response to the follow up comments"}, "BkxTsrSOoH": {"type": "rebuttal", "replyto": "r1lDKBHusH", "comment": "> I would expect to see the baseline performance reported in the main paper i.e. DIAYN for continuous control tasks too. It is not clear by \u201cdid not outperform the baseline\u201d\n\nWe added the learning curve of DIAYN for MuJoCo tasks in the paper. DIAYN did not outperform DDPG (with no options) in the tasks we evaluated. \n\n\n> one would expect a comparison to the state-of-art in exploration, for instance [1]. Also since the work builds on eigenoptions, it would help compare with [2] as well. [1] Count-based exploration with the successor representation. [2] A laplacian framework for option discovery in reinforcement learning\n\n[1]: The goal of deep covering options is to construct a skill to efficiently explore the state-space agnostic to its reward structure. Therefore, generated options can be used for any tasks in the same state dynamics, thus transferable across tasks which is hard to achieve by non-option based methods. In fact, our method is orthogonal to non-option based exploration methods, so we can apply both algorithms.\n[2]: Our method is an extension of covering options that is shown to outperform eigenoptions in tabular domains (Jinnai et al 2019b).", "title": "(Cont.) Response to Reviewer #3"}, "r1lDKBHusH": {"type": "rebuttal", "replyto": "HJlGfUk0KH", "comment": "We appreciate the time you took reviewing our submission and hope that our response addresses some of your concerns.\n\n\n>  Figure 1 of this work is exactly the same as Figure 2 of the Jinnai et al., 2019b\n\nThank you very much for pointing out. We added an acknowledgement to the Figure. \u201cadapted from Jinnai et al. (2019b), Figure 2.\u201d We apologize for the omission.\n\n\n>  Where does the parameter k come from? Is this a hand-designed parameter such as in PinBall it is somehow set to 30, for Mujoco tasks this is somehow set to 10, and for Atari, this is set to 4. Would it be possible to comment on the hyperparameter threshold percentile k?\n\nThe threshold percentile is set smaller for tasks where the state-space size is large so that the termination set won\u2019t be too large. Anecdotally, this parameter was quite easy to set by hand. Learning an appropriate setting for this hyperparameter automatically is future work.\n\n\n> Can you comment on to what extent do you see theoretical guarantees of covering options apply to the function approximation case as they no longer hold when going from tabular setting to the nonlinear function approximation?\n\nThe advantage of having underlying theoretical guarantee is that we can hope that the method is more likely to have the desirable characteristic if we make the approximation finer grained. In fact, Wu et al. 2019 empirically showed that the eigenfunction constructed by their approximation method is fairly close to the exact solution in grid world.\n\n\n> It is not clear why DIAYN was not compared as it is and with all the stated modifications. In DIAYN, setting the initiation set to be the whole state space seems counterintuitive.\n\nOur code is indeed based on the implementation by the author of DIAYN (https://github.com/haarnoja/sac/blob/master/sac/algos/diayn.py) with several modifications to make it compatible to our implementation. We observed that the same set of hyperparameters do not outperform a non-hierarchical agent in our tasks. We suspect that the diversity objective matches nicely to tasks evaluated in Eysenbach et. al (2019) (e.g. Cheetah Hurdle, Ant Navigation) but is not as good a fit to our maze-like tasks.\n\n\n> \u201cWe sampled 200 episodes of length 2000 with a uniform random policy to generate each option.\u201d Would there be smarter ways to generate each option?\n\nOur method is not restricted to samples from random trajectories but applicable to any valid trajectories. We chose to generate options from trajectories generated by a uniform random policy without using any task-specific information as our goal is to improve the performance of an agent which has no prior knowledge of the task. \n\n\n> how does the proposed approach comparison to successor options.\n\nThe eigenfunctions of the Laplacian are equivalent to the eigenfunctions of the successor representation used by successor options (Stachenfeld et al. 2014). Therefore, both successor options and deep covering options are based on the same idea to identify subgoals in the state-space. The advantage of covering options is that it only uses the subgoal identified by the second eigenfunction which reduces the cover time the most among all the eigenfunctions in tabular domain whereas other eigenfunction methods use other eigenfunctions which reduces the cover time less than the second eigenfunction.\n\nStachenfeld, Kimberly L., Matthew Botvinick, and Samuel J. Gershman. \"Design principles of the hippocampal cognitive map.\" Advances in neural information processing systems. 2014.\n\n\n> \u201cTermination set generated by deep covering options tends to be larger than..\u201d Intuitively speaking, it is not clear why this is a good idea. Wouldn't we want the options to be peaked in places they terminate and initiate in, and therefore have smaller termination/initiation set?\n\nWe do not claim that larger termination set is advantageous. We observed that deep covering options tend to have large termination set. How large the termination/initiation set should be is an open question.", "title": "Response to Reviewer #3"}, "SJxcXSBuoS": {"type": "rebuttal", "replyto": "Byx79-ee9r", "comment": "We appreciate the time you took reviewing our submission and hope that our response addresses some of your concerns.\n\n\n> the idea of covering options (Jinnai et. al., 2019b) and the approximation for the graph laplacian (Wu et. al., 2019) both have been shown in prior work and the novelty in this paper seems to be limited to putting together these two ideas\n\nOne of the main limitations of eigenoptions is that the existing algorithms discover too many options at once and it is not clear how to prune them. The work on covering options addressed this limitation but only in a constrained, theoretical setting. Extending this idea to the function approximation setting is actually really challenging. We argue that the strength of our paper is to show that covering options can be easily extensible to function approximator setting by applying and adapting the techniques shown by Wu et al.\n\n\n> only qualitative options are shown for harder exploration tasks\n\nFinding efficient options to improve the exploration in the ATARI domain is a very challenging task. In fact, Machado et al. (2017; 2018) showed that eigenoptions is applicable to the ATARI domain but did not present learning curves. They constructed 1024 options from all the generated eigenfunctions and manually picked some of the options out of them that seem to have useful behavior (Machado et al. 2017, Figure 8 and 9). Deep covering options improves over their results as we only construct the options generated by the second eigenfunction which is likely to be more useful than the other ones.\n\n\n> a comparison to other exploration algorithms which are non-option based methods has not been shown\n\nThe goal of deep covering options is to construct a skill to efficiently explore the state-space agnostic to its reward structure. Therefore, generated options can be used for any tasks in the same state dynamics, thus transferable across tasks which is hard to achieve by non-option based methods. In fact, our method is orthogonal to non-option based exploration methods, so we can apply both of the algorithms.\n\n\n> In the intro, \u201c...extend a theoretically principled approach for option discovery to the non-linear function approximation case\u201d seems to be highly misleading, since it suggests that the theoretical guarantees are extended, which is not the case in this paper as the approximations have no states guarantees.\n\nThank you very much for pointing out. We changed the sentence to \u201c...extend a principled approach for option discovery to the non-linear function approximation case\u201d.\n\n\n> The paper made some assumptions on the implementation of DIAYN, citing lack of details in Eysenbach et. al (2019). Were these details not available in the code released by DIAYN on their project page? (I\u2019m not sure if I should post the link here, but it is easy to find)\n\nOur code is indeed based on the implementation by the author of DIAYN (https://github.com/haarnoja/sac/blob/master/sac/algos/diayn.py) with several modifications to make it compatible to our implementation. We observed that the same set of hyperparameters do not outperform a non-hierarchical agent in our tasks. We suspect that the diversity objective matches nicely to tasks evaluated in Eysenbach et. al (2019) (e.g. Cheetah Hurdle, Ant Navigation) but is not as good a fit to our maze-like tasks.\n", "title": "Response to Reviewer #2"}, "BylqKGBdir": {"type": "rebuttal", "replyto": "BygSuR-qFB", "comment": "We appreciate the time you took reviewing our submission and hope that our response addresses some of your concerns.\n\n\n> The proposed method seems mainly a combination of covering options with the techniques from (Wu et al.) to compute the eigenfunctions. As such it could be argued  to the core ideas in the paper aren\u2019t very novel.\n\nOne of the main limitations of eigenoptions is that the existing algorithms discover too many options at once and it is not clear how to prune them. The work on covering options addressed this limitation but only in a constrained, theoretical setting. Extending this idea to the function approximation setting is actually really challenging. We argue that the strength of our paper is to show that covering options can be easily extensible to function approximator setting by applying and adapting the techniques shown by Wu et al.\n\n\n> What is the cost of repeatedly solving the minimization problem in (5)?\n\nThe computation for solving the minimization problem was roughly the same as training the DQN agent for one iteration.\n\n\n> For several experiments the reward curve starts out high or rapidly becomes high and then decreases during learning. Can the authors explain this odd behaviour?\n\nWe observed that the value of the options gets higher during the training and sometimes its value overly generalizes to states where calling the options are not appropriate (e.g. you cannot reach the terminal states easily under the policy of the option). \n\n\n> Why are there no learning curves for the ATARI domain?\n\nFinding efficient options to improve the exploration in the ATARI domain is very challenging. While Machado et al. (2017; 2018) showed that eigenoptions is applicable to the ATARI domain, they did not present learning curves. They constructed 1024 options from all the generated eigenfunctions and manually picked some of the options out of them that seem to have useful behavior (Machado et al. 2017, Figure 8 and 9). Deep covering options improves over their results as we only construct the options generated by the second eigenfunction which is likely to be more useful than the other ones.\n", "title": "Response to Reviewer #1"}, "BygSuR-qFB": {"type": "review", "replyto": "SkeIyaVtwB", "review": "This paper proposes deep covering options. This method extends laplacian based option generation techniques to continuous domains. The resulting options are task independent and enable efficient exploration. The method can be applied in continuous state (and action) domains and options can be learnt in an online manner.\n\nI favour acceptance of this paper. While the method seems to be mostly a combination of the ideas in 2 referenced previous works, the method seems to work well in a diverse set of circumstances.\n\nDetailed comments: \n\n-The proposed method seems mainly a combination of covering options with the techniques from (Wu et al.) to compute the eigenfunctions. As such it could be argued  to the core ideas in the paper aren\u2019t very novel. Nonetheless, I found the approach interesting. Moreover, it is interesting to see an option generation technique that applies to such a wide range of problems.\n\n-The paper gives a broad overview of related work and situates the method with respect to previous option generation methods. The background given on the laplacian, its eigenfunctions and their properties was short to the point of not being very clear. The algorithms are given without much context. I found myself referring to the earlier papers for a more thorough explanation of the core ideas. I would suggest shortening the other sections somewhat in favour of a more intuitive and self contained explanation.\n\n- The effectiveness of the method is demonstrated on a set of very diverse problems that go well beyond the traditional grid worlds often used in option literature. The method is also compared to another unsupervised option generation method.\n\n- It is interesting to see a method that can incrementally grow the set of options while learning, without any pre-training or requiring additional samples. This has the potential to have a large impact on large scale learning approaches\n\n- What is the cost of repeatedly solving the minimization problem in (5)?\n\nMinor comments:\n\n- For several experiments the reward curve starts out high or rapidly becomes high and then decreases during learning. Can the authors explain this odd behaviour? This seems to mainly happen with the option methods in the continuous control domains\n\n- Why are there no learning curves for the ATARI domain?\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 4}}}