{"paper": {"title": "Distributed Distributional Deterministic Policy Gradients", "authors": ["Gabriel Barth-Maron", "Matthew W. Hoffman", "David Budden", "Will Dabney", "Dan Horgan", "Dhruva TB", "Alistair Muldal", "Nicolas Heess", "Timothy Lillicrap"], "authorids": ["gabrielbm@google.com", "mwhoffman@google.com", "budden@google.com", "wdabney@google.com", "horgan@google.com", "dhruvat@google.com", "alimuldal@google.com", "heess@google.com", "countzero@google.com"], "summary": "We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.", "abstract": "This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.", "keywords": ["policy gradient", "continuous control", "actor critic", "reinforcement learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "As identified by most reviewers, this paper does a very thorough empirical evaluation of a relatively straightforward combination of known techniques for distributed RL. The work also builds on \"Distributed prioritized experience replay\", which could be noted more prominently in the introduction."}, "review": {"Byqj1QtlM": {"type": "review", "replyto": "SyZipzbCb", "review": "A DeepRL algorithm is presented that represents distributions over Q values, as applied to DDPG,\nand in conjunction with distributed evaluation across multiple actors, prioritized experience replay, and \nN-step look-aheads. The algorithm is called Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG.\nSOTA results are generated for a number of challenging continuous domain learning problems,\nas compared to benchmarks that include DDPG and PPO, in terms of wall-clock time, and also (most often) in terms\nof sample efficiency.\n\npros/cons\n+ the paper provides a thorough investigation of the distributional approach, as applied to difficult continuous\n  action problems, and in conjunction with a set of other improvements (with ablation tests)\n- the story is a bit mixed in terms of the benefits, as compared to the non-distributional approach, D3PG\n- it is not clear which of the baselines are covered in detail in the cited paper:\n  \"Anonymous. Distributed prioritized experience replay. In submission, 2017.\", \n   i.e., should readers assume that D3PG already exists and is attributable to this other submission?\n\nOverall, I believe that the community will find this to be interesting work.\n\nIs a video of the results available?\n\nIt seems that the distributional model often does not make much of a difference, \nas compared to D3PG non-prioritized.  However, sometimes it does make a big difference, i.e., 3D parkour; acrobot.\nDo the examples where it yields the largest payoff share a particular characteristic?\n\nThe benefit of the distributional models is quite different between the 1-step and 5-step versions. Any ideas why?\n\nOccasionally, D4PG with N=1 fails very badly, e.g., fish, manipulator (bring ball), swimmer.\nWhy would that be? Shouldn't it do at least as well as D3PG in general?\n\nHow many atoms are used for the categorical representation?\nAs many as [Bellemare et al.], i.e., 51 ?\nHow much \"resolution\" is necessary here in order to gain most of the benefits of the distributional representation?\n\nAs far as I understand, V_min and V_max are not the global values, but are specific to the current distribution.\nHence the need for the projection. Is that correct?\n\nWould increasing the exploration noise result in a larger benefit for the distributional approach?\n\nFigure 2: DDPG performs suprisingly poorly in most examples. Any comments on this,\nor is DDPG best avoided in normal circumstances for continuous problems? :-)\n\nIs the humanoid stand so easy because of large (or unlimited) torque limits?\n\nThe wall-clock times are for a cluster with K=32 cores for Figure 1?\n\n\"we utilize a network architecture as specified in Figure 1 which processes the terrain info in order to reduce its dimensionality\"\nFigure 1 provides no information about the reduced dimensionality of the terrain representation, unless I am somehow failing to see this.\n\n\"the full critic architecture is completed by attaching a critic head as defined in Section A\"\nI could find no further documenation in the paper with regard to the \"head\" or a separate critic for the \"head\".\nIt is not clear to me why multiple critics are needed.\n\nDo you have an intuition as to why prioritized replay might be reducing performance in many cases?\n", "title": "Thorough investigation of distributional policy gradients for continuous problems", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1Wcz1clz": {"type": "review", "replyto": "SyZipzbCb", "review": "The paper investigates a number of additions to DDPG algorithm and their effect on performance. The additions investigated are distributional Bellman updates, N-step returns, and prioritized experience replay.\n\nThe paper does a good job of analyzing these effects on a wide range of continuous control tasks, from the standard benchmark suite, to hand manipulation, to complex terrain locomotion and I believe these results are valuable to the community.\n\nHowever, I have a concern about the soundness of using N-step returns in DDPG setting. When a sequence of length N is sampled from the replay buffer and used to calculate N-step return, this sequence is generated according a particular policy. As a result, experience is non-stationary - for the same state-action pair, early iterations of the algorithm will produce structurally different (not just due to stochasticity) N-step returns because the policy to generate those N steps has changed between algorithm iterations. So it seems to me the authors are using off-policy updates where strictly on-policy updates should be used. I would like some clarification from the authors on this point, and if it is indeed the case to bring attention to this point in the final manuscript.\n\nIt would also be useful to evaluate the effect of N for values other than 1 and 5, especially given the significance this addition has on performance. I can believe N-step returns are useful, possibly due to effectively enlarging simulation timestep, but it would be good to know at which point it becomes detrimental.\n\nI also believe \"Distributional Policy Gradients\" is an overly broad title for this submission as this work still relies on off-policy updates and does not tackle the problem of marrying distributional updates with on-policy methods. \"Distributional DDPG\" or \"Distributional Actor-Critic\" or variant perhaps could be more fair title choices?\n\nAside from these concerns, lack of originality of contributions makes it difficult to highly recommend the paper. Nonetheless, I do believe the experimental evaluation if well-conducted and would be of interest to the ICLR community. ", "title": "good evaluation, but lacking in originality", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Bk3bXW5gM": {"type": "review", "replyto": "SyZipzbCb", "review": "\nComment: The paper proposes a simple extension to DDPG that uses a distributional Bellman operator for critic updates, and introduces two simple modifications which are the use of N-step returns and parallelizing evaluations. The method is evaluated on a wide variety of many control and robotic talks. \n\nIn general, the paper is well written and organised. However I have some following major concerns regarding the quality of the paper:\n\n- The proposal, D4PG, is quite straightforward which is simply use the idea of distributional value function by Bellemare et al. (previously used in DQN). Two modifications are also simple and well-known techniques. It would be nicer if the description in Section 3 is less straightforward by giving more justifications and analysis why and how distributional updates are necessary in the context of policy search methods like DDPG. \n\n- A positive side of the paper is a large set of evaluations on many different control and robotic tasks. For many tasks, D4PG performs better than the variant that does not use distributional updates (D3PG), however by not much. There are some tasks showing no-difference. On the other hand, the choice of N=5 in comparisons is hard to understand and lacks further experimental justifications. Different setting and new performance metrics (e.g. data efficiency, number of episodes in total) might also reveal more properties of the proposed methods.\n\n\n\n* Other minor comments:\n\n- Algorithm 1 consists of two parts but there are connection between them. It might be confused for ones who are not familiar with the actor-critic framework.\n\n- It would be nicer if all expectation operators in Section 3 comes with corresponding distributions. \n\n- page 2, second paragraph: typos in \"hence my require less samples to learn\"\n\n- it might be better if the reference on arXiv should be changed to relevant publication conferences with archival proceeding: work by Marc G. Bellemare at ICML 2017", "title": "the proposed method is simple", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryuoGzKMz": {"type": "rebuttal", "replyto": "Byqj1QtlM", "comment": "Thank you!\n\nAs to the baselines, we use the same framework for distributing computation and prioritization as in the cited paper (Anonymous. Distributed prioritized experience replay, also submitted to ICLR). However this other work focuses primarily on discrete-action tasks.\n\nVideos of the parkour performance can be found at https://www.youtube.com/playlist?list=PLFU7BiIwAjPDqsIL9OLm1z7_RXZA1Jyfj.\n\nWe have also found that the distributional model helps most in harder, higher-dimensional tasks. The main characteristic these tasks seem to share is the time/data required with which the solve the task. Potentially due to the complexity of learning the Q-function.\n\nWe found in general, for both distributional and non-distributional, that the 5-step version provided better results. Although this is not fully corrected for (see answers to the above reviewers) we found this to experimentally provide quite a bit of benefit to all variations of the algorithm.\n\nWe used 51 atoms across all tasks except for the humanoid parkour task which used 101. The level of resolution necessary will depend on the problem under consideration, and controlled by the combination of the number of atoms as well as the V_{min,max} values, however we found this to be relatively robust. Here we changed the number of atoms for the humanoid task in order to keep the resolution roughly consistent with other tasks.\n\nThe V_min and V_max values are global values that bound the support of the distribution. However, you are correct that this is what requires the projection. When applying the Bellman operator to a distribution it will more than likely lie outside the bounds given by V_min/V_max, so we project in order to ensure that our distributions are always within these bounds. Again, we also found these values to be relatively robust and we generally set these given knowledge of the maximum immediate reward of the system.\n\nWe did not extensively experiment with increasing the exploration noise, but from preliminary experiments we saw that the algorithm was fairly robust to this value. Deviating from the values we used did not significantly hurt nor hinder the algorithm\u2019s performance.\n\nThe poor performance of DDPG in these experiments is primarily due to the fact that DDPG is quite slow to learn. For the easier control suite tasks DDPG is actually a feasible algorithm if given enough time. However for the harder tasks (any of the humanoid tasks, manipulator, and parkour tasks) DDPG would take much too long to work effectively. Finally, one of the bigger problems DDPG has is that it can exhibit quite unstable learning which is not exhibited by D4PG.\n\nThe easy-ness of the humanoid stand task is more due to the fact that it has less complicated motions to make than any of the other humanoid tasks.\n\nThe wall-clock times are for 32 cores on separate machines. We found communication across machines to be fast enough that having them all be on the same machine was not a requirement.\n\nWe apologize that the description of the network architecture was poorly explained and will correct it. The networks have two branches, one of which process the the terrain info to produce a lower-dimensional hidden state before combining it with the proprioceptive information. Utilizing this second branch to process the proprioceptive information and reduce it to a smaller number of hidden units is what we refer to as \u201creducing its dimensionality\u201d however we will explain this better.\n\nWe will also explain critic architecture and what we refer to as \u201cheads\u201d further. Here we refer to the \u201cdistributional output\u201d component of the network as a head. In this way we can replace the Categorical output with a Mixture of Gaussians output as described in section A. By \u201chead\u201d we only mean this final component which takes the last set of hidden units, passes them through a linear layer, and outputs the parameters of a distribution.\n", "title": "Response to AnonReviewer2"}, "r14lUMKMf": {"type": "rebuttal", "replyto": "HJt1T-2R-", "comment": "We have found that D4PG was very stable and robust to its hyperparameter settings. We generally found that carefully tuning the learning rates was unnecessary, and this also allowed us to eliminate the Ornstein-Uhlenbeck noise.\n\nAs to the results on parkour: we have not yet re-run the experiments on the humanoid, however for the 2d-walker these results are approximately at their maximum. And we can see that D4PG outperforms PPO in this setting.\n\nWith regards to stability it is well known the DDPG can be quite unstable. As noted above we don\u2019t really see any of these issues with D4PG and it is in fact very stable, both in terms of its behavior during a run, across different seeds, and across different settings of hyperparameters. We haven\u2019t significantly experimented with scaling the number of actors for D4PG, however while we do tend to see performance improvements as we increase the number of workers, we kept this number fixed with the number used in PPO.\n\nFinally, videos of the parkour performance can be found at: https://www.youtube.com/playlist?list=PLFU7BiIwAjPDqsIL9OLm1z7_RXZA1Jyfj.\n", "title": "Response to comment"}, "HJ3eWMFzG": {"type": "rebuttal", "replyto": "r1Wcz1clz", "comment": "Thanks for the helpful review!\n\nThe reason for our use of N-step returns is it allows us to compute the returns as soon as they are collected and insert into replay without storing full sequences. This is done for efficiency reasons. For N>1 this ignores the difference between the behavior and target policies. This could be corrected using an off-policy correction such as Retrace (Safe and Efficient Off-Policy Reinforcement Learning, Munos et al., 2016) but that would require storing full trajectories.\n\nHowever, for reasonably small N this difference is not great, which is what we show in our experiments. With N much larger than the value of 5, we see a degradation in performance for exactly this reason. We include further discussion of exactly this point.", "title": "Response to AnonReviewer1"}, "SkfngGYMG": {"type": "rebuttal", "replyto": "Bk3bXW5gM", "comment": "Thank you for the review!\n\nAs to the necessity of the distributional updates, the DPG algorithm relies heavily on the accuracy of the value function estimate due to the fact that the gradient computed under the DPG theorem is based only on gradients of the policy pi and gradients of the Q-function. By better estimating the Q-function we directly impact the accuracy of the policy gradient. We will include further discussion of this.\n\nIt is true that the distributional version (D4PG) does not always out-perform the non-distributional version (D3PG). However this is typically on easier tasks. In the control suite of tasks the distributional version significantly out-performs on the acrobot, humanoid, and swimmer set of tasks. For manipulation tasks this holds for the hardest pickup and orient task. And finally for all parkour tasks. So for tasks that are already somewhat easy to solve there are limited immediate gains, but for harder tasks this update tends to help (and help significantly for the parkour tasks).\n\nThe choice of a higher N is suggested by algorithms such as A3C and Rainbow, among others. Note that the Rainbow algorithm (Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al, 2017) utilizes an off-policy Q-learning update with uncorrected n-step returns, in a very similar way to that used by D4PG. In order to fully correct for this we should be using an off-policy correction, which we have not used for reasons of efficiency (see our response to the next reviewer). However, experimentally we have shown that this minor modification helps quite significantly and can be used directly in any off-policy algorithm. In all of our experiments and across both distributional and non-distributional updates it tends to be better to use the higher N. We did find that increasing N much higher than N>5 tended to degrade performance, which makes sense as this would be more off-policy. We will include further discussion of this aspect of the algorithm.\n", "title": "Response to AnonReviewer3"}}}