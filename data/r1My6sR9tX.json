{"paper": {"title": "Unsupervised Learning via Meta-Learning", "authors": ["Kyle Hsu", "Sergey Levine", "Chelsea Finn"], "authorids": ["kyle.hsu@mail.utoronto.ca", "svlevine@eecs.berkeley.edu", "cbfinn@eecs.berkeley.edu"], "summary": "An unsupervised learning method that uses meta-learning to enable efficient learning of downstream image classification tasks, outperforming state-of-the-art methods.", "abstract": "A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods.", "keywords": ["unsupervised learning", "meta-learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "Reviewers largely agree that the paper proposes a novel and interesting idea for unsupervised learning through meta learning and the empirical evaluation does a convincing job in demonstrating its effectiveness. There were some concerns on clarity/readability of the paper which seem to have been addressed by the authors. I recommend acceptance. "}, "review": {"Bye9Tru00X": {"type": "rebuttal", "replyto": "HyeT4Mp5Rm", "comment": "Code for producing part of the results has been released, but for anonymity reasons, we will not link to it here. We will update the code and add a link to the code in the paper and here after the review process is complete.", "title": "Thank you for your interest."}, "BkemmQw9Am": {"type": "rebuttal", "replyto": "HJlNixAFAX", "comment": "We have addressed your comments on presentation by specifying U(P) as the uniform distribution and revising the explanation in \u201ctask generation for meta-learning\u201d.\n\nWe agree that the entire pipeline consists of several hyperparameters, which we chose and fixed based on prior work and heuristics (Section 4.1, Appendix E). We found it to be straightforward to select these parameter values, suggesting that the algorithm is not particularly sensitive to their values.", "title": "Thank you for your review. We appreciate your thorough and accurate summary as well as your feedback."}, "HJlNixAFAX": {"type": "review", "replyto": "r1My6sR9tX", "review": "summary\nThe goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. SoTA meta-learning frameworks (MAML and ProtoNet) typically require rather large labeled datasets and hand-specified task distributions to define a sequence of tasks on which the algorithms are trained on. This paper proposes to unsupervised generate the sequence of tasks using multiple partitions as pseudo labels via k-means and other clustering variants on the embedding space. Empirical experiments show the benefit of the meta-learning on the M-way K-shot image classification tasks.  Also, \u201csampling a partition from U(P)\u201d on page 4, the U(P) notation seems not defined.\n\nEvaluation\n- The writing and presentation of the paper are in general well carried, except some part seems a little unclear, taking me quite a while to understand. For example,  in the \u201ctask generation for meta-learning\u201d paragraph on page 3, the definition of task-specific labels (l_n) is puzzling to me at first glance.     \n\n- The proposed task construction in an unsupervised manner for the meta-learning framework is indeed simple and novel. \n\n- The empirical experiments are thorough and well-conducted with good justifications. The benefit of unsupervised meta-learning compared to simply supervised learning on the few-shot downstream tasks is shown in Table 1 and 2; Different embedding techniques have also been studied; the results of Oracle upper bound are also presented; task construction ablation is also shown. \n\n- Unsupervised meta-learning consists of multiple components such as learning embedding space, clustering methods, and various choices within the meta-learning frameworks. This together consumes a lot of hyper-parameters and the choice can somehow seem heuristic.\n\nConclusion\n- In general, I like this paper especially the empirical analysis section. Therefore, I vote for accepting this paper.\n", "title": "nice and simple idea with well carried and thorough empirical experiments", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SklSFCJyaQ": {"type": "review", "replyto": "r1My6sR9tX", "review": "The paper proposes to employ metalearning techniques for unsupervised tasks. The authors construct tasks in an automatic way from unlabeled data and run meta-learning over the constructed tasks.\n\nAlthough the paper presents a novel approach and the experiments included in the work show promising results, in my opinion, the paper is still not mature. There are some importants problems:\n* The motivation of the paper is weak. The authors include the problem statement as well as the definitions used in the paper without knowing what is the goal of the proposed algorithm. A clear example of a real problem where the proposed framework could be applied is necessary to motivate the work.\n* The paper is difficult to read and follow. The paper is composed by a set of parts without many links. This makes difficult to read the paper to not very experienced readers. A running example could be useful to increase the readability of the work. In my opinion, the paper contains too much material for the length of the conference. In fact, some important information has been moved to the appendices. \n*Experimental section is specially hard to follow. The authors want to solve too many questions in a short space. Comparisons with other related papers should be included. \n\n", "title": "Interesting approach but still not finished", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJldF4T8hQ": {"type": "review", "replyto": "r1My6sR9tX", "review": "This paper proposes to construct multiple classification tasks from unsupervised data.\n\nQuality:\nThe detail of the proposed method is not mathematically presented and its performance is not theoretically analyzed.\nAlthough the proposed method is empirically shown to be superior to other approaches, the motivation is not clearly presented.\nHence the overall quality of this paper is not high.\n\nClarity:\nThe readability of this paper is not high as it is redundant or unclear at several points.\nFor example, Sections 2.1, 2.3 and Sections 2.2, 2.4 can be integrated, respectively, and more mathematical details can be included instead.\n\nOriginality:\nThe proposal of constructing meta-learning based on unsupervised learning seems to be original.\n\nSignificance:\n- The motivation is not clear. The proposed method artificially generates a number of classification tasks. But how to use such classifiers for artificially generated labels in real-world applications is not motivated.\n  It is better to give a representative application, to which the proposed method fits.\n- There is no theoretical analysis on the proposed method.\n  For example, why is the first embedding step required? Clustering can be directly performed on the give dataset D = {x_i}.\n- Although the paper discusses using unsupervised learning for meta-learning, only k-means is considered in the proposed method.\n  There are a number of types of unsupervised learning, including other clustering algorithms and other tasks such as outlier detection, hence analyzing them is also interesting.\n- The proposed method includes several hyper-parameters. But how to set them in practice it not clear.\n\nPros:\n- An interesting approach to meta-learning is presented.\n\nCons:\n- Motivation is not clear.\n- There is no theoretical analysis.\n", "title": "Interesting approach but motivation is not clear.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1lYZ6Ht37": {"type": "review", "replyto": "r1My6sR9tX", "review": "In this paper, the task of performing meta-learning based on the unsupervised dataset is considered. The high-level idea is to generate 'pseudo-labels' via clustering of the given dataset using existing unsupervised learning techniques. Then the meta-learning algorithm is trained to easily discriminate between such labels. This paper seems to be tackling an important problem that has not been addressed yet to my knowledge. While the proposed method/contribution is quite simple, it possesses great potential for future applications and deeper exploration. The empirical results look strong and tried to address important aspects of the algorithm. The writing was clear and easy to follow. I especially liked how the authors tried to exploit possible pitfalls of their experimental design. \n\nMinor comments and questions:\n- Although the problem of interest is non-trivial and important, the proposed algorithm can be seen as just a naive combination of clustering and meta-learning. It would have been great to see some clustering algorithm that was specifically designed for this type of problem. Especially, the proposed CACTUs algorithm relies on sampling without replacement from the clustered dataset in order to enforce \"balance\" of the labels among the generated task. This might be leading to suboptimal results since the popularity of each cluster (i.e., how much it represents the whole dataset) is not considered. \n\n- CACTUs seems to be relying on having random scaling of the k-means algorithm in order to induce diversity on the set of partitions being generated. I am a bit skeptical about the effectiveness of such a method for diversity. If this holds, it would be interesting to see the visualization of such a concept.\n\n- Although only MAML was considered as the meta-learning algorithm, it would have been nice to consider one or more candidates to show that the proposed framework is generalizable. Still, I think the experiment is persuasive enough to expect that the algorithm would work well at practice.\n\n- Would there be a trivial generalization of the algorithm to semi-supervised learning?  \n\n-------\n\nI am satisfied with the author's response and changes they made to the text. I still think the paper brings significant contributions to the area, by showing that even generating the pseudo-tasks via unsupervised clustering method allows the meta-learning to happen.  ", "title": "Great paper tackling important problem with nice experiments", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkxrw3v-6X": {"type": "rebuttal", "replyto": "HJldF4T8hQ", "comment": "\u201cAlthough the paper discusses using unsupervised learning for meta-learning, only k-means is considered in the proposed method.\u201d\nWe do consider multiple types of unsupervised learning: this is described in \u201cDifferent embedding spaces\u201d and \u201cTask construction\u201d in Section 4. For the embeddings, we consider and evaluate four unsupervised learning methods/objectives covering discriminative clustering, generative modeling, interpolation, and information maximization. For constructing tasks from embeddings, we consider and evaluate random sampling and hyperplane slicing in addition to k-means.\n\n\u201cwhy is the first embedding step required? Clustering can be directly performed on the give dataset D = {x_i}.\u201d\nGiven that the x_i in our experiments are images, we believe it is clear from intuition that clustering on x_i would not work well: distance metrics in pixel-space do not correspond well to semantic meaning. We will add this as a comparison in the paper. \n\n\u201cThe proposed method includes several hyper-parameters. But how to set them in practice it not clear.\u201d\nThe hyperparameters associated with the embedding learning stage can be tuned on the unlabeled meta-validation split. For clustering, we fix the number of clusters k across all dataset/embedding/task difficulty combinations presented in the main text. We demonstrate that the number of partitions P is unimportant for our method: P=1 and P=50/100 (for miniImagenet and CelebA / Omniglot) both perform well (Section 4.2). There is ample justification of the hyperparameters used for the task construction: choose N, the number of classes in each task, by upper-bounding the number of classes expected to be seen in a downstream task, and choose K to be 1 (Section 4.1). As motivated in the first paragraph of Section 4.1, all other hyperparameters were selected based on prior work.\n\n>> \u201cperformance is not theoretically analyzed.\u201d\nWe found that, given the generality of the problem statement, it was difficult to make headway on theoretical analysis. We therefore opted to prioritize a solid experimental evaluation of the proposed method. Historically, theoretical analysis has not been a requirement for high-quality contributions in this community. There are numerous examples of high-quality, impactful papers devoid of theoretical analysis or guarantees presented at ICLR in recent years [1, 2, 3, 4, 5].\n\nReferences\n[1] Zoph et al. ICLR 2017, https://openreview.net/forum?id=r1Ue8Hcxg\n[2] Karras et al. ICLR 2018, https://openreview.net/forum?id=Hk99zCeAb\n[3] Jaderberg et al. ICLR 2017, https://openreview.net/forum?id=SJ6yPD5xg\n[4] Lazaridou et al. ICLR 2017, https://openreview.net/forum?id=Hk8N3Sclg\n[5] Ravi & Larochelle ICLR 2017, https://openreview.net/pdf?id=rJY0-Kcll\n", "title": "[2/2] Additional responses to the reviewer\u2019s points."}, "rkgGnSmOam": {"type": "rebuttal", "replyto": "r1My6sR9tX", "comment": "We have updated the paper with the following changes to address reviewer comments:\n- combined sections 2.1 and 2.3, and sections 2.2 and 2.4 (R2)\n- reduced redundancy in the exposition (R2)\n- added more mathematical details to section 2 (R2)\n- added comparison to clustering on pixels (R2)\n- added further discussion of limitations of our method in the discussion (R2)\n- provided more motivation and justification for our approach in section 2.2 (R1, R2)\n- improved the clarity of the problem statement and its motivation in sections 1 and 2.1 (R1, R2)\n- emphasized throughout the text that the downstream tasks we evaluate on at meta-test time are standard benchmark few-shot learning tasks (R1, R2)\n- added a brief discussion on sampling clusters in section 2.2 (R3)\n- added a set of experiments based on Prototypical Networks (R3)\n\nWe would appreciate it if the reviewers could take a look at our changes and additional results, and let us know if they would like to either revise their rating of the paper, or request additional changes that would alleviate their concerns. Thank you!", "title": "Paper updated to address reviewer feedback"}, "HJlTH6DZT7": {"type": "rebuttal", "replyto": "B1lYZ6Ht37", "comment": "\u201cAlthough only MAML was considered as the meta-learning algorithm, it would have been nice to consider one or more candidates to show that the proposed framework is generalizable. Still, I think the experiment is persuasive enough to expect that the algorithm would working well at practice.\u201d\nTo address your suggestion, we have updated the paper to add results (in Tables 1, 2, and 3) obtained with Prototypical Networks [1] as the meta-learner instead of MAML. We find that the improvement of CACTUs over the comparison methods still generally holds, with a few exceptions. We hypothesize the exceptions are due to a dependence of ProtoNets performance on matching train shot with test shot, i.e. on providing the meta-learner with tasks that have supervision commensurate to that expected in held-out tasks. We elaborate on this in the updated paper (\u201cBenefit of Meta-Learning\u201d in Section 4.2).\n\n\u201cAlthough the problem of interest is non-trivial and important, the proposed algorithm can be seen as just a naive combination of clustering and meta-learning. It would have been great to see some clustering algorithm that was specifically designed for this type of problem.\u201d\nThe reviewer is correct in that some more sophisticated clustering methods may be better-suited for our method. We found that this simple procedure (with hyperparameter k fixed) worked surprisingly well across datasets and task structures, and did not see a need to make the method more complex. \n\n\u201cEspecially, the proposed CACTUs algorithm relies on sampling without replacement from the clustered dataset in order to enforce \"balance\" of the labels among the generated task. This might be leading to suboptimal results since the popularity of each cluster (i.e., how much it represents the whole dataset) is not considered.\u201d\nIf we view k-means as the hard limit of a mixture of Gaussians and decompose the joint embedding-cluster distribution p(z,c)=p(c)p(z|c), one way the reviewer\u2019s proposal can be realized is sampling from p(c). However, as we mention in Section 5, the datasets we consider (Omniglot and miniImageNet) are evenly balanced amongst classes, and we fear that comparing between sampling from U(c) and p(c) on these datasets may be misleading, as in general datasets can be heavily imbalanced. We leave a thorough evaluation of the question of how to better sample clusters for tasks to future work, but there are a couple of hints we can already think about. First, consider a toy imbalanced dataset for which, after clustering, there is one heavily populated cluster and four small ones. Because we have no guarantees about the meta-test distribution, it is \u201csafer\u201d for the meta-learner to learn to distinguish between all clusters equally well than to have the popular cluster dominate the meta-training task distribution. Second, prior work [2] also considers a similar question (\u201cTrivial parametrization\u201d, page 6), and concludes that uniform sampling over clusters is more suitable.\n\n\u201cCACTUs seems to be relying on having random scaling of the k-means algorithm in order to induce diversity on the set of partitions being generated. I am a bit skeptical about the effectiveness of such a method for diversity. If this holds, it would be interesting to see the visualization of such a concept.\u201d\nWe found that this diversity was helpful but not critical for our method to perform well: compare the P=1 and P={50,100} entries in the tables of Appendix F. Other mechanisms for encouraging task diversity would be an interesting direction for future work, and we welcome any suggestions on this front!\n\n\u201cWould there be a trivial generalization of the algorithm to semi-supervised learning?\u201d\nFor the scenario in which some labeled data, not necessarily from the same classes as in tasks from meta-test time, is available during meta-training, there are indeed some obvious extensions. One can: i) have some of the tasks be generated only from the labeled data and others from CACTUs, ii) encourage the calculation of partitions to respect the labeled data, and/or iii) use the labeled data as meta-validation to do early stopping and hyperparameter tuning. We leave this for future work.\n\nReferences\n[1] Snell et al. NIPS 2017, https://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning\n[2] Caron et al. ECCV 2018, https://arxiv.org/abs/1807.05520\n", "title": "Thank you for your insightful comments and feedback!"}, "Skxii3P-a7": {"type": "rebuttal", "replyto": "HJldF4T8hQ", "comment": "Thank you for your time in reviewing our work! We would like to improve the paper based on your feedback, and would benefit from a few clarifications on your part.\n\n\u201cThe motivation is not clear. The proposed method artificially generates a number of classification tasks. But how to use such classifiers for artificially generated labels in real-world applications is not motivated. It is better to give a representative application, to which the proposed method fits.\u201d\nOur evaluation and results are on real-world image classification tasks first proposed by prior work (Omniglot: [1], miniImageNet: [2], CelebA: [3]) and used by virtually all few-shot learning works in the last few years [1, 2, 3, 4, 5, 6, 7, 8, 9]. The test tasks are not artificially generated, but are real few-shot image classification tasks. We give the general use-case of our method in the last sentence of Section 2.1. We have clarified these points in our revision. Do you still find a lack of representative application? If so, do you have suggestions for better evaluation tasks?\n\n\u201cThe detail of the proposed method is not mathematically presented\u201d, \u201c \u2026 more mathematical details can be included instead.\u201d\nWe have added the optimization objective of k-means to the paper. Are there any other parts of the method that require more formalism?\n\n\u201cThe readability of this paper is not high as it is redundant or unclear at several points.\nFor example, Sections 2.1, 2.3 and Sections 2.2, 2.4 can be integrated, respectively, and more mathematical details can be included instead.\u201d\nWe have implemented your suggested re-organization to reduce redundancy. Which portions of the text, specifically, remain redundant or unclear?\n\nReferences\n[1] Santoro et al. ICML 2016, http://proceedings.mlr.press/v48/santoro16.pdf\n[2] Ravi & Larochelle ICLR 2017, https://openreview.net/pdf?id=rJY0-Kcll\n[3] Finn et al. NIPS 2018, https://arxiv.org/abs/1806.02817\n[4] Vinyals et al. NIPS 2016, https://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning\n[5] Munkhdalai et al. ICML 2017, https://arxiv.org/abs/1703.00837\n[6] Finn et al. ICML 2017, https://arxiv.org/abs/1703.03400\n[7] Snell et al NIPS 2017, https://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning\n[8] Oreshkin et al. NIPS 2018,\nhttps://arxiv.org/abs/1805.10123\n[9] Yoon et al. NIPS 2018, https://arxiv.org/abs/1806.03836\n", "title": "[1/2] Thank you for your review. Can you elaborate on your feedback?"}, "rkels4tJp7": {"type": "rebuttal", "replyto": "SklSFCJyaQ", "comment": "Thank you for your comments. Our evaluation tests on few-shot Omniglot, miniImageNet, and CelebA classification datasets, which are a real-world few-shot image classification task proposed by [1,2,3] respectively, and evaluated in virtually all few-shot classification papers since 2016: [1,2,3,4,5,6,7,8,9]. We can of course evaluate our method on other problems as well, but the current tasks are real-world image datasets and problems that have been studied extensively in the literature, for which our method achieves excellent results. Are there particular additional datasets that the reviewer would prefer a comparison to? Or anything else we can do to address the concern about the motivation?\n\nWe would be happy to revise the problem statement and writing as per the reviewer's suggestions, though we would appreciate more specific pointers about what in particular is difficult to follow. The problem statement is quite simple: we aim to propose an algorithm whereby meta-learning can be used to acquire an efficient few-shot learning procedure without any hand-specified labels during meta-training. This problem is important for two reasons: (1) meta-learning currently relies on large labeled datasets, and in practice, the burden of obtaining such labeled datasets is a major obstacle to widespread use of meta-learning for few-shot classification, and (2) state-of-the-art unsupervised learning methods often neglect downstream use-cases, such as few-shot classification, leaving substantial room for improvement. Our work proposed a way to begin addressing these challenges, and compares extensively to four prior papers [10,11,12,13] and several ablations. Beyond updating the problem statement, are there important comparisons that we missed?\n\n[1] Santoro et al. ICML 2016, http://proceedings.mlr.press/v48/santoro16.pdf\n[2] Ravi & Larochelle ICLR 2017, https://openreview.net/pdf?id=rJY0-Kcll\n[3] Finn et al NIPS 2018, https://arxiv.org/abs/1806.02817\n[4] Vinyals et al. NIPS 2016, https://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning\n[5] Munkhdalai et al. ICML 2017, https://arxiv.org/abs/1703.00837\n[6] Finn et al. ICML 2017, https://arxiv.org/abs/1703.03400\n[7] Snell et al NIPS 2017, https://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning\n[8] Oreshkin et al. NIPS 2018,\nhttps://arxiv.org/abs/1805.10123\n[9] Yoon et al. NIPS 2018, https://arxiv.org/abs/1806.03836\n[10] Donahue et al. ICLR 2017, https://arxiv.org/abs/1605.09782\n[11] Caron et al. ECCV 2018, https://arxiv.org/abs/1807.05520\n[12] Berthelot et al. arXiv 2018, https://arxiv.org/pdf/1807.07543\n[13] Chen et al. NIPS 2016, https://arxiv.org/abs/1606.03657", "title": "Thank you for the feedback. Can you elaborate on your suggestions?"}}}