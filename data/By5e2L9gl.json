{"paper": {"title": "Trusting SVM for Piecewise Linear CNNs", "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "summary": "Formulating CNN layerwise optimization as an SVM problem", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM. \n \n Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.\n \n Thus, I recommend this paper be accepted."}, "review": {"Hk3s8owUe": {"type": "rebuttal", "replyto": "By5e2L9gl", "comment": "tldr: New results on ImageNet, CIFAR-100, improved results on CIFAR-10.\n\nWe thank the reviewers for their helpful feedbacks. We list here the changes made in the revisions of the paper (version 1 being the original submission read by the reviewers).\n\nList of changes in version 2:\n\n1) New results with batch-normalization on CIFAR-10 (new subsection 5.2)\n2) Clarification of the objective of the paper and experiments (Methods paragraph in subsection 5.1)\n\nList of changes in version 3:\n\n1) New results on CIFAR-10: deeper architecture for a stronger baseline (subsection 5.2)\n2) New results on CIFAR-100 (subsection 5.2)\n3) Re-wording of the experiments section and removal of previous experiments on CIFAR-10 (with and without batch normalization) (section 5)\n4) New Appendix about the computation of the feature vectors and detailed example (Appendix B).\n5) Infeasibility of standard line-search in Introduction\n6) New references, including suggestions from the reviewers (section 2)\n7) More compact abstract\n8) Inclusion of batch normalization in Discussion (section 6)\n9) Minor rewording and typo fixes throughout the paper.\n\nList of changes in version 4:\n1) Added ImageNet results (subsection 5.3)\n\n", "title": "Submission Revision"}, "SkI4atOEx": {"type": "rebuttal", "replyto": "H1OCNwe4g", "comment": "We thank the reviewer for the comments, and provide some clarifications below.\n\n\nComment: \u201clayer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs [...] It is not clear to me how the loss/gain balances each other\u201d\n\nResponse:  The performance impact of using coordinate descent is indeed an open question. For this work, we point out that: \n(i) LW-SVM improves over SGD solutions despite optimizing only one layer at a time,\n(ii) LW-SVM trained from scratch can also reach competitive results on CIFAR-10, \n(iii) The connection between PL-CNNs and latent SVMs opens an interesting research direction that may further improve on LW-SVM (for example,  by extending LW-SVM to a global optimization algorithm that updates all layers simultaneously).\n\n\nComment: \u201cThis paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc).\u201d\n\nResponse: In the revised version of the paper, we report results with batch-normalization and show similar or greater improvements of LW-SVM over the baselines.\n\n\nComment: \u201ca better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting\u201d\n\nResponse: The experiments of this work suggest that the improvement made by LW-SVM on the training data generalize well in our settings. In particular, results with batch-normalization show significant improvements on the testing accuracy. A better optimization algorithm may also lead to a better understanding of the deficiencies of the current learning objective, which could in itself be of interest to the representation learning community.\n\n\nComment: \u201cOnly CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet.\u201d\n\nResponse: In principle the architectures used on ImageNet (such as VGG-16, ResNets etc.) can be treated like the models used in our experiments, and we expect to obtain similar results on these. In addition to the current results, we are currently evaluating our method on CIFAR-100 and ImageNet.", "title": "Answer"}, "BJZR2tuEg": {"type": "rebuttal", "replyto": "BkrnA0UEg", "comment": "We thank the reviewer for his comments, which we discuss below:\n\nComment: \u201cAlthough the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task.\u201d\n\nResponse: In principle the architectures used on ImageNet (such as VGG-16, ResNets etc.) can be treated like the models used in our experiments, and we expect to obtain similar results on these. In addition to the current results, we are currently evaluating our method on CIFAR-100 and ImageNet, which are more challenging multi-class classification tasks. We do take note that it would be interesting to test the method on structured prediction tasks as well.\n\n\nComment: \u201cThe test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%.\u201d\n\nResponse: The best score of 70% is obtained with a network which does not use batch normalization, which makes an important difference. The results with batch-normalization in the revised version of the paper reach around 78% testing accuracy. Taking into account the reviewer\u2019s comments, we are currently running experiments with deeper architectures to improve this score.\n\n\nComment: \u201cIf I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. In practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm.\u201d\n\nResponse: The reviewer is correct that during the inner optimization, only the dual objective is guaranteed a monotonic improvement. In practice, we do run BCFW with a sufficient number of iterations to yield a very small duality gap. Therefore the method provides a monotonic decrease in practice. To illustrate this, we have plotted the training objective and the dual gap for an experiment presented in the paper (Adadelta with batch-norm + LW-SVM, Figure 3 in the revised paper): https://drive.google.com/file/d/0BxXMf_vDT8vCSFQwOWl0aldzeWs/view?usp=sharing (one data point at the end of each inner iteration of the CCCP). We will add a note about these points in the future revision of the paper.", "title": "Answer"}, "By1NljIVe": {"type": "rebuttal", "replyto": "HkmIo1QNe", "comment": "We thank the reviewer for the comments and provide some clarifications below.\n\nComment: \u201ccomparison between backprop and the discussed CCCP approach is not really appropriate\u201d\n\nResponse: We use the term \u2018backpropagation\u2019 to denote the overall subgradient descent (SGD) algorithm. This includes the forward pass and the backward pass (which computes the subgradient), as well as the state of the art variants of the step size computations, namely, Adagrad, Adadelta and Adam. As our main aim is to improve the optimization of the learning objective (3), a comparison with the existing optimization algorithms provides an appropriate baseline.\n\n\nComment: \u201cmini-batch optimization alleviates any form of monotonic decrease\u201d. \n\nResponse: Does the reviewer mean that by using mini-batch optimization we have lost our monotonic decrease guarantee? If so, this is not correct. There are two distinct monotonic improvements that we guarantee here, even when using mini-batches:\n(i) a monotonic decrease in the overall objective function at each outer iteration of the CCCP algorithm. This is because we solve a convex problem and therefore obtain the optimal solution, which is at least as good as the initial point.\n(ii) a monotonic increase in the dual of the convex problem at each step. Because the dual is smooth and quadratic, we perform a coordinate ascent on the dual variables with an optimal step-size. This feature of the BCFW algorithm (Lacoste-Julien et al., 2013) guarantees a monotonic increase despite the fact that the conditional gradient is computed using samples from a mini-batch / block of coordinates.\n\n\nComment: \u201cMore justification regarding the argument that search for the step-size is a disadvantage seems necessary\u201d, \u201cline-search would for example result in convergence guarantees\u201d.\n\nResponse: Another noteworthy advantage of our work is that we can analytically compute the optimal step-size for the conditional gradient obtained over a mini-batch. Indeed, this fact has been exploited successfully for structural SVM optimization with significant improvements over SGD methods.\n\nFor backpropagation style algorithms (that is, SGD on learning objective (3)), the computation of the step-size is not computationally feasible. This is due to the fact that the evaluation of the objective for each putative step-size would require an entire epoch. Furthermore, for PL-CNN, the objective (3) is only sub-differentiable. Hence, even an exhaustive line search cannot guarantee a monotonic improvement.\n\nIn other words, by establishing a connection between deep networks and structural SVMs, we have been able to successfully transfer the two advantages of BCFW to the deep learning domain (monotonic improvement of dual using mini-batches, efficient computation of the optimal step-size).\n\n\nComment: \u201c In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (https://openreview.net/pdf?id=ROVmA279BsvnM0J1IpNn) ICLR Workshop Track 2016\u201d. \n\nResponse: We thank the reviewer for the reference (we have found a full version at https://arxiv.org/pdf/1609.07152v2.pdf). While our method deals with convexity w.r.t. the parameters to improve the training task, the aforementioned work analyzes how convexity w.r.t input or the output of the network helps with the inference (for cases with complex outputs). We will include this reference in the future revision of the paper.\n\n\nComment: \u201cThe observation 1 required to convert the layer wise procedure into an SVM seems rather ad-hoc.\u201d\n\nResponse: In our revised version of the paper, we have updated Observation 1 with a principled algorithm that is guaranteed to find the optimal solution of the convex problem. \n\n\nIn summary, all the theoretical guarantees presented in our work are applicable, despite the use of mini-batches and the use of a modified conditional gradient algorithm.", "title": "Clarifications"}, "B1SmZffQe": {"type": "rebuttal", "replyto": "SJl75t5fx", "comment": "We thank the reviewer for the questions regarding the experiments section.\n\nOur main goal is to compare different algorithms that optimize the same objective function, namely problem (3), in terms of three measures: (i) training accuracy; (ii) the testing accuracy; and (iii) the learning objective value. Our layerwise optimization is complementary to backprop. To highlight this, we have incorporated new results that minimize the learning objective of a network that incorporates batch normalization (subsection 5.2). Similar to the original experiments, our algorithm leads to significant improvements on all three measures.\n\nIt is worth noting that our method can be extended to handle other forms of difference-of-convex non-linearities, such as batch normalization. However, in this work we focus on PL functions that have the added advantage of being amenable to Frank-Wolfe style optimization.\n\nIn the appendix of the revised paper, we also include new results that highlight the robustness of our method to hyperparameter settings compared to all the standard backprop variants. Please note that all the methods use the same learning objective. By obtaining lower objective values, our method is able to escape the bad local minima that are encountered by backprop.", "title": "Clarification of the goal & new results with batch-normalization"}, "SJl75t5fx": {"type": "review", "replyto": "By5e2L9gl", "review": "1) The baseline in the experiment is a strawman variant of backprop (without batch norm, drop out, etc). I think this is unfair. Batch norm and dropout have been proved to be very useful for CNN models. If the proposed method cannot take advantage of batch norm/dropout, then this is an advantage of backprop. \n2) The dataset is too small for today's standard. Why not use something like ImageNet?\n\nThis paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.\n\nOverall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.\n\nOf course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).\n\nThe experiment is a bit weak.\n1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.\n\n2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place. \n", "title": "comparison with backprop", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1OCNwe4g": {"type": "review", "replyto": "By5e2L9gl", "review": "1) The baseline in the experiment is a strawman variant of backprop (without batch norm, drop out, etc). I think this is unfair. Batch norm and dropout have been proved to be very useful for CNN models. If the proposed method cannot take advantage of batch norm/dropout, then this is an advantage of backprop. \n2) The dataset is too small for today's standard. Why not use something like ImageNet?\n\nThis paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.\n\nOverall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.\n\nOf course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).\n\nThe experiment is a bit weak.\n1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.\n\n2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place. \n", "title": "comparison with backprop", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}