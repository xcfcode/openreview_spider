{"paper": {"title": "Variable Computation in Recurrent Neural Networks", "authors": ["Yacine Jernite", "Edouard Grave", "Armand Joulin", "Tomas Mikolov"], "authorids": ["yacine.jernite@nyu.edu", "egrave@fb.com", "ajoulin@fb.com", "tmikolov@fb.com"], "summary": "We show that an RNN can learn to control the amount of computation it does at each time step, leading to better efficiency and performance as well as discovering time patterns of interest.", "abstract": "Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.", "keywords": ["Natural language processing", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper describes a new way of variable computation, which uses a different number of units depending on the input. This is different from other methods for variable computation that compute over multiple time steps. The idea is clearly presented and the results are shown on LSTMs and GRUs for language modeling and music prediction.\n \n Pros:\n - new idea\n - convincing results in a head to head comparison between different set ups.\n \n Cons:\n - results are not nearly as good as the state of the art on the reported tasks.\n \n The reviewers and I had several rounds of discussion on whether or not to accept the paper. One reviewer had significant reservations about the paper since the results were far from SOTA. However, since getting SOTA often requires a combination of several tricks, I felt that perhaps it would not be fair to require this, and gave them the benefit of doubt (especially because the other two reviewers did not think this was a dealbreaker). In my opinion, the authors did a fair enough job on the head to head comparison between their proposed VCRNN models and the underlying LSTMs and GRUs, which showed that the model did well enough."}, "review": {"HyI81NlDg": {"type": "rebuttal", "replyto": "Hk6ApMlwx", "comment": "We would like to stress once again that one of the primary goals of our paper was to improve the performance of RNNs for a given computational budget (and not to get the best possible result without constraints). We have demonstrated that to this purpose, our model shows better performance than Elman RNN, GRU, and LSTM (our results and those of others, e.g. Architectural Complexity Measures of Recurrent Neural Networks, Zhang et al., 2016). We note that the results mentioned by the reviewer were obtained using different architectures often with larger dimensions or stacked structures, which drastically increase the number of operations required for a model, and are therefore not really relevant to our presentation. To motivate our approach further, we will add that being able to run models with limited and/or fixed computational resources is extremely important for the application of machine learning methods, and we provide a first step towards such gains. We sincerely hope that the reviewer will reconsider the value of our approach given this additional context.\n\n", "title": "Context"}, "r1B95flDg": {"type": "rebuttal", "replyto": "S1LVSrcge", "comment": "We would like to thank the reviewers again for a discussion which we feel helped us improve the paper significantly. We have provided a newer version which implements the items we discussed in response to their initial comments. Most notably:\n- We added the description of the Variable Computation GRU to the paper\n- We added comparisons to the LSTM and GRU architectures on the Penn TreeBank dataset\n- We added experiments on the larger English corpus Text8 (http://mattmahoney.net/dc/textdata), with RNN, LSTM, GRU and VCGRU, and results for a range of target \\bar{m} values\n- We removed mentions of the ambiguous phrase \"computational efficiency\", and made the nature of the gains in number of operations clearer (cf especially the second paragraph of the Experiments section).\n\n", "title": "New Version"}, "SyP6xbwIl": {"type": "rebuttal", "replyto": "HkicR4ZEg", "comment": "We hope that the general answer above will have clarified the choice of \\bar{m}: the PTB result corresponds to the best validation perplexity, and Figure 5 illustrates the influence of the choice on the model performance and number of operations. Regardless of the question of the number of hyper-parameter (would the RNN dimension count as one?), whether it has a regularizing effect is certainly worth investigating: it seems that VCRNN overfits a bit less than the RNN with the same number of operations, but the VCGRU / GRU comparison gives the opposite result. We will be sure to investigate this further.\n\nWe will alter the language to clarify the fact that the efficiency gain is indeed currently conceptual: a reduced number of operations, which will require a lower-level programming effort to produce savings in wall-clock times. We mostly hope that these results will inspire other work in variable computation, to the point where the advantage of such adaptive low-level implementation will make it unequivocally worthwhile.\n \nWe will also add the LSTM and GRU baselines to our results.\n", "title": "Specific answers to Reviewer 2."}, "rJzleWDIl": {"type": "rebuttal", "replyto": "rJ0I-lfEe", "comment": "We hope that our general answer will have addressed most of the issues raised in the above review, and that the following will answer the rest.\n\n=== Gating Mechanism ===\nEpsilon was introduced to ensure the soundness of the model description: if the mask value is smaller than epsilon for dimension i, then h_i can be carried over from the previous time step without any computation. In practice however, it was given a fixed value to match the GPU floating point sensitivity.\n\nThe reason for the choice of this specific \u201cfirst-D\u201d shape is that we wanted to be able to control the computational complexity efficiently. Thus, we only have one parameter; a more general solution using any subset of the dimensions would be as expensive to compute as the full new hidden state, which would limit the usefulness of the mechanism.\n\n=== Evaluation ===\nIndeed, the magnitude of the change for the GRU hidden state at each time step does follow a similar pattern, in that it changes slightly more at the beginning of each word (the average gate value goes down monotonically from 0.8 at the beginning of a word to 0.6 at the end).\n\nThere are two main differences however. First, the GRU gate controller seems to only rely on the information provided by encountering a space character, and does not exhibit any of the morphological phenomena that can be observed with the VCRNN (as illustrated in Figure 4). \n\nSecondly, and more importantly, that measure is given a posteriori: computing the gate is as expensive as computing the full new state. One of the purposes of our work was to show that it is possible to obtain that information for much cheaper.\n\nWe will add this discussion to the paper, as these are indeed important distinctions.\n", "title": "Specific answers to Reviewer 3."}, "SJaOxbPUg": {"type": "rebuttal", "replyto": "SJpiju-Ve", "comment": "The general answer above provides results on PTB for LSTMs and GRUs, both with the same dimension and with comparable numbers of operations as the RNN or VCRNN (or VCGRU).\n\nWe also discuss our gating mechanism as it relates to others.\n\nWe will add and expand on both in the next version of the paper.", "title": "Specific answers to Reviewer 4."}, "Skkq1bD8l": {"type": "rebuttal", "replyto": "S1LVSrcge", "comment": "We thank the reviewers for several helpful suggestions. We first address the issues raised by two or more reviewers here. As a general comment, we would like to emphasize again the exploratory nature of our work. We aimed to explore the following question: is it possible for a recurrent model to (cheaply) determine a priori  how much computation it is going to need at each time step? We designed our model and experimental section with this interrogation in mind, and believe that we were able to provide a positive answer.\n\nAs regards the wall-clock time, the computational gains are currently, to quote Reviewer 2 conceptual only, as we are using the Torch GPU kernels, and optimizing the hardware implementation was beyond the scope of this project. Our goal was to make the possible implications of the result clear, but we will clarify the language of the paper to reflect this more accurately.\n\nFor each experiment, we tried a range of \\bar{m} values for the Variable Computation recurrent network and a range of hidden state dimensions for its vanilla counterpart. While we only provided one result on PTB, Figure 5 illustrates how both of these choices affect the model\u2019s computation and perplexity on a Czech and a German dataset; note that the Variable Computation curve for perplexity/number of operations is always under the Vanilla one, meaning fewer operations for a better perplexity. We will provide the same figure for PTB.\n\nOur original reasoning for only providing vanilla RNN results on PTB was that we were trying to compare apples to apples, so to speak, and to reinforce the message that we are considering a paradigm which can be extended to other architectures.\n\nHowever, we have since successfully applied our method to the GRU architecture, which gives us the Variable Computation Gated Recurrent Unit (VCGRU). Between this and all of the reviewers\u2019 well-received advice to provide more context, we re-ran experiments with the RNN and VCRNN, GRU and VCGRU, and the LSTM architecture. The results are as follow. As in the paper, the second column provides the dimension of a vanilla RNN which would perform the same number of operations.\n\n------------------------------------------------------------------------\nModel\t\t\t| RNN-equiv\t| train / valid / test\n------------------------------------------------------------------------\nLSTM_1024\t\t| 2048\t\t| 1.21 / 1.46 / 1.42\nGRU_1024\t\t| 1450\t\t| 1.17 / 1.47 / 1.42\n------------------------------------------------------------------------\nRNN_1024\t\t| 1024\t\t| 1.31 / 1.51 / 1.47\nLSTM_512\t\t| 1024\t\t| 1.31 / 1.49 / 1.44\nGRU_725\t\t| 1024\t\t| 1.26 / 1.47 / 1.43\n------------------------------------------------------------------------\nVCRNN_1024\t\t| 760 \t\t| 1.39 / 1.51 / 1.46\nRNN 760\t\t        | 760 \t\t| 1.37 / 1.52 / 1.47\nLSTM 380 \t\t| 760 \t\t| 1.34 / 1.48 / 1.43\nGRU 538\t\t        | 760 \t\t| 1.28 / 1.49 / 1.44\n------------------------------------------------------------------------\nVCGRU_1024\t\t| 648 \t\t| 1.30 / 1.47 / 1.42\nLSTM 324 \t\t| 648 \t\t| 1.41 / 1.51 / 1.46\nGRU 458\t\t        | 648 \t\t| 1.37 / 1.52 / 1.47\n------------------------------------------------------------------------\nVCGRU_1024\t\t| 538 \t\t| 1.32 / 1.49 / 1.44\n------------------------------------------------------------------------\n\nThe VCGRU behaves similarly to the VCRNN: the number of operations per character follows the same patterns, and it achieves the same perplexity as its constant computation counterpart with fewer operations. It also outperforms both GRUs and LSTMs for a given number of operations. We will add these results and discussion in the paper.\n\nFinally, we will add further discussion of our gating mechanism and comparison to other existing ones in the next version of the paper. Compared to the rest of the literature, the VCRNN gating paradigm is closest to that of the GRU: the main difference between the two is that where the GRU gate can take any shape, ours is more constrained and parametrized by a single value m_t, which allows to cheaply control the number of operations for the current time step.\n\nWe address the other comments in individual replies.\n", "title": "To Reviewers 2, 3 and 4:"}, "BybbKP4Qe": {"type": "rebuttal", "replyto": "H11FuIJQx", "comment": "Since the variable computation (VC) model has more parameters than the vanilla unit, our intuition was that it would be more prone to overfitting; however, early experiments showed that the gap between training and text perplexities was actually the same for both models. This makes regularization orthogonal to what we hope to achieve, so we decided that adding other forms of regularization beyond early stopping would confuse rather than clarify the point we are trying to make.\n\n  Furthermore, both the vanilla and variable computation numbers provided in Table 3 correspond to the best results on the validation set of a hyper-parameter search. between the learning rate, range for the random initialization, etc\u2026, we used as many settings for the vanilla as for the VC RNN.\n\n  Finally, Figure 5 illustrates how different values of \\bar{m} and the l1 penalty (from the first paragraph of Section 5) affect the trade off between computational cost and performance. (To be perfectly clear, the amount of computation for the VCRNN is not fixed, \\bar{m} simply determines how much we penalize using more computation in our objective)\n", "title": "Regularization is orthogonal to our main question."}, "HJKsFvVmx": {"type": "rebuttal", "replyto": "SyRipm0Ge", "comment": "The gating mechanism proposed is indeed motivated in good part by the direct correspondence it introduces between the value of m_t and the computational budget. As to the effect of imposing a particular ordering of the dimensions on the model, it is difficult to assess without more empirical evidence. On the one hand, it reduces the number of local minima. On the other, it constrains the optimization somewhat, especially in the early stages. Our intuition is that neither of these effects is significant enough to justify a more complex model.\n\n\nWe certainly do not believe that ease of introspection should be a requirement for all models proposed. However, the nature of the present work is exploratory: we wish to examine the feasibility and usefulness of a general paradigm which we feel is under-researched in the literature; specifically, the ability of recurrent networks to perform variable amounts of computation to fit the needs of sequential data. Thus, our focus in this paper is on interpretability rather than pure performance. We strongly believe that a clear understanding of the underlying mechanisms which contribute to the success of popular methods is essential to guiding the medium- and long-term evolution of the field, and aim to contribute to such progress here.\n\n\nAs regards our choice not to include sota char-level Penn TreeBank perplexity numbers in Table 3: we designed our experimental section to provide the clearest possible answer to the above-mentioned question, by trying to avoid possible confounding factors. Hence, we compare the results of the Variable Computation RNN to that of its closest constant computation counterpart (the Elman unit) in a regime where there is a clear trade-off between the amount of computation the model performs and the resulting test set perplexity (i.e. hidden dimensions between 100 and 1000).\n\n\nLSTMs also perform more computation than vanilla RNNs. The equivalent LSTM dimension for VCRNN-1000-a, for example, would only be 85. Even then, because the LSTM unit focuses on different mechanisms, it  would be unclear what conclusions one should draw from the difference in performances (if any). Additionally, while our gating mechanism works poorly with LSTMs (compounding gating mechanisms seems to make optimization difficult), the general paradigm can certainly be applied to produce Variable Computation LSTMs (we hope to spend some more time working on that project in the future).\n\n\nFinally, we did not find any other reported bit-level perplexity numbers in the literature than those provideded here. For character level, the best reported result to our knowledge is that of (HyperNetworks, Ha et al. 2016), with Valid/test bpc of 1.25/1.24. The reported results in recent literature for Elman-RNN-based methods (usually with more layers and hidden units) vary between 1.42 and 1.48.\n", "title": "Answers to Reviewers 4 and 3"}, "H11FuIJQx": {"type": "review", "replyto": "S1LVSrcge", "review": "How was the target amount of computation \\bar{m} chosen? Was it tuned using held-out data? If that's the case, it would be more fair to use some held-out data tuned form of regularization for the baseline RNNs as well. If the baseline RNNs are already regularized I couldn't find information about this in the text.This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs. The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks. Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account.\n\nThe proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent. Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step. It\u2019s very interesting to see how the model seems to use more resources at the start of each word or ASCII character. I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task. The superior performance of the architecture is impressive but I\u2019m not yet convinced that the baseline models had an equal number of hyperparameters to tune. I\u2019ll come back to this point in the next paragraph because it\u2019s mainly a clarity issue.\n\nThe abstract claims that the model is computationally more efficient than regular RNNs. There are no wall time measurements supporting this claim. While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources. This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up. I also find it unfortunate that it\u2019s not clear from the text how the hyperparameter \\bar{m} was chosen. Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky). I don\u2019t consider this a very serious flaw because I\u2019m impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text. Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value.\n\nTo my knowledge, the proposed architecture is novel. The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive. Originality is one of this paper\u2019s strongest points. \n\nIt\u2019s currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level. That said, the architecture may turn out to be useful for learning long-term dependencies. I also think that the interpretability of the value m_t is a nice property of the method and that it\u2019s visualizations are very interesting. It might shed some more light into what makes certain tasks difficult for RNNs. \n\nPros:\nOriginal clever idea.\nNice interesting visualizations.\nInteresting experiments.\n\nCons:\nSome experimental details are not clear.\nI\u2019m not convinced of the strength of the baseline.\nThe paper shouldn\u2019t claim actual computational savings without reporting wall-clock times.\n\nEdit:\nI'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score. Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper. On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time. I think it's fine that they are conceptual only as long as this is clear from the paper and abstract. I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed. \n\nEdit: \nSince I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this. I still think it's a good paper but with these results I cannot say that it stands out.\n", "title": "How was the target amount of computation \\bar{m} chosen?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkicR4ZEg": {"type": "review", "replyto": "S1LVSrcge", "review": "How was the target amount of computation \\bar{m} chosen? Was it tuned using held-out data? If that's the case, it would be more fair to use some held-out data tuned form of regularization for the baseline RNNs as well. If the baseline RNNs are already regularized I couldn't find information about this in the text.This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs. The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks. Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account.\n\nThe proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent. Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step. It\u2019s very interesting to see how the model seems to use more resources at the start of each word or ASCII character. I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task. The superior performance of the architecture is impressive but I\u2019m not yet convinced that the baseline models had an equal number of hyperparameters to tune. I\u2019ll come back to this point in the next paragraph because it\u2019s mainly a clarity issue.\n\nThe abstract claims that the model is computationally more efficient than regular RNNs. There are no wall time measurements supporting this claim. While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources. This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up. I also find it unfortunate that it\u2019s not clear from the text how the hyperparameter \\bar{m} was chosen. Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky). I don\u2019t consider this a very serious flaw because I\u2019m impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text. Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value.\n\nTo my knowledge, the proposed architecture is novel. The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive. Originality is one of this paper\u2019s strongest points. \n\nIt\u2019s currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level. That said, the architecture may turn out to be useful for learning long-term dependencies. I also think that the interpretability of the value m_t is a nice property of the method and that it\u2019s visualizations are very interesting. It might shed some more light into what makes certain tasks difficult for RNNs. \n\nPros:\nOriginal clever idea.\nNice interesting visualizations.\nInteresting experiments.\n\nCons:\nSome experimental details are not clear.\nI\u2019m not convinced of the strength of the baseline.\nThe paper shouldn\u2019t claim actual computational savings without reporting wall-clock times.\n\nEdit:\nI'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score. Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper. On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time. I think it's fine that they are conceptual only as long as this is clear from the paper and abstract. I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed. \n\nEdit: \nSince I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this. I still think it's a good paper but with these results I cannot say that it stands out.\n", "title": "How was the target amount of computation \\bar{m} chosen?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyRipm0Ge": {"type": "review", "replyto": "S1LVSrcge", "review": "- This method introduces a clever gating mechanism which imposes an ordering on the hidden units. In addition to allowing for easy control of the computational budget, it also breaks symmetry in the parameterization. Do you believe this is a component of its success?\n- \"However, introspection [in multi-layer LSTMs] is more challenging and it is difficult to determine whether they are actually exhibiting significant temporal behaviors.\" It seems like this family of complaint could be leveraged to some degree against all neural network methods. Do you consider this a disincentive to their use?\n- For readers less familiar with the language modeling literature, how do the bit- and character-level model perplexities compare against other published results on the same task?This is high novelty work, and an enjoyable read.\n\nMy concerns about the paper more or less mirror my pre-review questions. I certainly agree that the learned variable computation mechanism is obviously doing something interesting. The empirical results really need to be grounded with respect to the state of the art, and LSTMs are still an elephant in the room. (Note that I do not consider beating LSTMs, GRUs, or any method in particular as a prerequisite for acceptance, but the comparison nevertheless should be made.)\n\nIn pre-review responses the authors brought up that LSTMs perform more computation per timestep than Elman networks, and while that is true, this is an axis along which they can be compared, this factor controlled for (at least in expectation, by varying the number of LSTM cells), etc. A brief discussion of the proposed gating mechanism in light of the currently popular ones would strengthen the presentation.\n\n---\n2017/1/20: In light of my concerns being addressed I'm modifying my review to a 7, with the understanding that the manuscript will be amended to include the new comparisons posted as a comment.", "title": "gating mechanism, etc.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJpiju-Ve": {"type": "review", "replyto": "S1LVSrcge", "review": "- This method introduces a clever gating mechanism which imposes an ordering on the hidden units. In addition to allowing for easy control of the computational budget, it also breaks symmetry in the parameterization. Do you believe this is a component of its success?\n- \"However, introspection [in multi-layer LSTMs] is more challenging and it is difficult to determine whether they are actually exhibiting significant temporal behaviors.\" It seems like this family of complaint could be leveraged to some degree against all neural network methods. Do you consider this a disincentive to their use?\n- For readers less familiar with the language modeling literature, how do the bit- and character-level model perplexities compare against other published results on the same task?This is high novelty work, and an enjoyable read.\n\nMy concerns about the paper more or less mirror my pre-review questions. I certainly agree that the learned variable computation mechanism is obviously doing something interesting. The empirical results really need to be grounded with respect to the state of the art, and LSTMs are still an elephant in the room. (Note that I do not consider beating LSTMs, GRUs, or any method in particular as a prerequisite for acceptance, but the comparison nevertheless should be made.)\n\nIn pre-review responses the authors brought up that LSTMs perform more computation per timestep than Elman networks, and while that is true, this is an axis along which they can be compared, this factor controlled for (at least in expectation, by varying the number of LSTM cells), etc. A brief discussion of the proposed gating mechanism in light of the currently popular ones would strengthen the presentation.\n\n---\n2017/1/20: In light of my concerns being addressed I'm modifying my review to a 7, with the understanding that the manuscript will be amended to include the new comparisons posted as a comment.", "title": "gating mechanism, etc.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}