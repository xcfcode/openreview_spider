{"paper": {"title": "Dirichlet Variational Autoencoder", "authors": ["Weonyoung Joo", "Wonsung Lee", "Sungrae Park", "and Il-Chul Moon"], "authorids": ["weonyoungjoo@gmail.com", "aporia@kaist.ac.kr", "sungraepark@kaist.ac.kr", "icmoon@kaist.ac.kr"], "summary": "", "abstract": "This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior for a continuous latent variable that exhibits the characteristic of the categorical probabilities. To infer the parameters of DirVAE, we utilize the stochastic gradient method by approximating the Gamma distribution, which is a component of the Dirichlet distribution, with the inverse Gamma CDF approximation. Additionally, we reshape the component collapsing issue by investigating two problem sources, which are decoder weight collapsing and latent value collapsing, and we show that DirVAE has no component collapsing; while Gaussian VAE exhibits the decoder weight collapsing and Stick-Breaking VAE shows the latent value collapsing. The experimental results show that 1) DirVAE models the latent representation result with the best log-likelihood compared to the baselines; and 2) DirVAE produces more interpretable latent values with no collapsing issues which the baseline models suffer from. Also, we show that the learned latent representation from the DirVAE achieves the best classification accuracy in the semi-supervised and the supervised classification tasks on MNIST, OMNIGLOT, and SVHN compared to the baseline VAEs. Finally, we demonstrated that the DirVAE augmented topic models show better performances in most cases.", "keywords": ["Variational autoencoder", "Unsupervised learning", "(Semi-)Supervised learning", "Topic modeling"]}, "meta": {"decision": "Reject", "comment": "This paper applies Dirichlet distribution to the latent variables of a VAE in order to address the component collapsing issues for categorical probabilities. The method is clearly presented, and extensive experiments are carried out to prove the advantage against VAEs with other prior distributions.\n\nThe main concern of the paper is the limited novelty. The main methodology contribution of this paper is to combine the decomposition a Dirichlet distribution as Gamma distributions, and approximating Gamma component with inverse Gamma CDF, but both components are common practices. \n\nR3 also points out that the paper is distracted by two different messages the authors try to convey. The presentation and experiments are not designed to provide a cohesive message. The concern is not solved in the authors' feedback.\n\nBased on the current reviews, this paper does not meet the standard for ICLR publication. Despite the limited novelty in the proposed model, if the paper could be revised to show that a simple modification is good for solve one problem with general applications, it would make a good publication in a future venue."}, "review": {"BJxNbEr6CX": {"type": "rebuttal", "replyto": "r1x7APd30m", "comment": "Dear Reviewer3:\n\nWe respond to your point in the below.\n\n\n1: \nWe checked the experiment with the 'normalizing flows' approach, but we were not able to find an error. It should be noted that the experiment code is our creation, but from the authors of the normalizing flows. With our creation, we also tested OMNIGLOT by comparing the normalizing flow and the Gaussian VAE, and we were able to see that the normalizing flow is significantly better than the Gaussian VAE. Also, in this OMNIGLOT experiment, DirVAE was better than the normalizing flow. I think that MNIST is too simple dataset to judge the validation of our implementation of the normalizing flow.\n\nFor a fair comparison, I think that the implementations of both cases should not include techniques, such as Max-out. When we remove these techniques besides the core of the models, it is natural to see a different result compared to the previous literature. I think that these conflict or practices of conducting experiments should be discussed within our research community.\n\n\n2: \nI think that we had a miscommunication between us and Reviewer3. We interpreted the \"interpretability\" as a further qualitative analysis on the topic model results, which can be discovered in the previous literatures. Often, a topic model is evaluated by a qualitative observation on the topic words and a quantitative measure of the held-out log-likelihood or perplexity. We think that our paper delivers these evaluation aspects.\n\nMoreover, our paper introduces how to put a prior over a latent variable of a variational autoencoder. This prior will determine the characteristics of the latent variable to a certain extent. The prior of Dirichlet distribution, which we choose, is a typical prior to handling a sparse dataset, so we have not been suspecting that Reviewer3 was questioning such characteristics.\n\nEvaluating the model fitness to a dataset generated from a sparse latent variable can be answered by calculating the quantitative evaluation point, such as the held-out perplexity or the log-likelihood. If a model cannot capture the inherent sparsity, the quantitative measurement will be bad. From this perspective, DirVAE improves the quantitative aspect of the neural topic models in general.\n\n\n3:\nIn some part, I agree with you that this paper can be regarded as a \"distracted\" paper. However, from our perspective, we have had a consistent motivation of this paper. \n\nOur motivation is \"An explicit deep generative model should further extend the choices of the probability distributions and their causal relation modelings to reflect the true latent dynamics of the generative process on the modeled domain.\" For example, many prior works, such as LDA, have succeeded because of the careful choices of probability distributions and causal relations to capture the text generation process. It should be noted that the only difference between LDA and pLSI is the Bayesian treatment with a Dirichlet prior. Then, it is natural that the explicit DGM community needs to extend and improve the modeling of the prior and the latent variables by accepting distributions other than the Gaussian. Our paper provides a methodology of modeling the Dirichlet prior in the VAE setting and an improvement of its usage in the text domain. From these logical flow, we consider the message is fairly simple.\n\nHaving said that, the discussion on the component collapsing and the evaluation on the classifiers could be distracting. On the other hand, the enabled priors are always investigated in its properties, so we had to include those discussions.\n\n\nAgain, we appreciate your reviewer service to our research community.\n", "title": "Thank you for your review."}, "SJeB-IILRQ": {"type": "rebuttal", "replyto": "H1xeSd-7R7", "comment": "We agree that we should tone-down the statement that you've mentioned.\n\nWe re-write the paragraph as following: \"It should be noted that there has been a practice of utilizing the combination of decomposing a Dirichlet distribution and approximating each Gamma component with inverse Gamma CDF. However, such practices have not been examined with its learning properties and applicabilities. The following section shows a new aspect of component collapsing that can be remedied by this combination on Dirichlet prior in VAE, and the section illustrates the performance gains in a certain set of applications, i.e. topic modeling.\"\n\nThanks again for your valuable comment.\n\nSincerely.", "title": "Response to Reviewer2."}, "S1lfpr8L0m": {"type": "rebuttal", "replyto": "r1ergtDq37", "comment": "For better representation, we modified all t-SNE figures in the paper.\n\nSincerely.", "title": "t-SNE figures are modified."}, "ByluKKS3aQ": {"type": "rebuttal", "replyto": "r1ergtDq37", "comment": "To respond to your comments on quality & novelty part, we did additional experiments, and some figures and tables are added or modified.\n\n1. Regarding the paper \"Variational inference with normalizing flows\" (Rezende et al.), we add followings: Table 5, Figure 5, and Table 6. In our experimental setting, the performances on pure VAE of GVAE and GVAE-NF20 were barely different, and DirVAE shows discriminative results compared to the both GVAEs. Not only the DirVAE gives better quantitative results as in Table 5 and 6, but it also has better learned latent representations which can be supported by t-SNE visualization such as Figure 3(b) and 5(b), and the decoder weight collapsing of GVAE-NF20 is one reason for such worse quality.\n\n2. We add SBVAE augmentation on topic models as a baseline in the topic modeling experiments. The related results can be found in Table 4, Figure 7, 8, and 9. As you can see, SBVAE augmentation is better than the original model in some sense, and for a certain case, it is quite comparable. However, still, the DirVAE augmentation shows better results in most of cases in terms of datasets and the performance measures.\n\n3. To show the interpretability of DirVAE augmentation on topic models, we add Table 8 which lists top probability words per topic. We manually re-ordered and put the topics together if there are similar semantic meanings. Also, Figure 7, 8, and 9 shows that DirVAE augmentation on topic models brought better learned latent representation than others, as in the case of pure VAE experiments.\n\nThanks again for your comments.\n\nSincerely.", "title": "Responses to Reviewer3 with additional experiments. "}, "r1lXXtoI6m": {"type": "rebuttal", "replyto": "Hke6jd7w27", "comment": "Thank you for your review. \n\nFirstly, we would like to say thank you for introducing the paper [2]. Even though the paper [2] and our paper lie on the same path in terms of reparametrizing Gamma distribution, paper [2] deals with a general reparametrization trick on various probabilistic distributions while our paper focuses on the advantages and the applicabilities of Dirichlet prior in VAE. We believe that our contribution is not based on reparametrizing Gamma or Dirichlet distiribution, but introducing Dirichlet prior, which is a conjugate multi-modal prior of categorical distribution, on VAE which has better learned latent representation due to no component collapsing. To support this, we did extensive experiments including topic modeling experiments and experimentally showed that DirVAE with the Dirichlet prior does not have component collapsing for the first time in this field. This component collapsing was not experimented and discussed in the prior work of [2]. Moreover, our experiments on the topic modeling shows the consistent performance increases when we apply the DirVAE, which was not discusses in [2].\n\nThe below is the response to your questions.\nThe log likelihood (LL) is the log-probability that a learner optimizes to train a model given an observed dataset. However, since the likelihood or log-likelihood function is intractable given a latent variable in VAE, so we use the evidence lower bound (ELBO) to optimize the LL. ELBO is a tractable alternative of LL, so the optimization on ELBO is feasible. ELBO term consists of two parts: Reconstruction Error (or Reconstruction Loss, which you asked) and KL divergence terms. Here, Reconstruction Error measures the error between the input and the output, which is an auto-encoder reconstructed input. Additionally, for your information, equation (1) in our paper, can be re-written as follows: Negative Log-likelihood <= Negative ELBO = Reconstruction Loss + KL Divergence.\n\nThe author of paper [3] on the inverse Gamma recommends a finite difference approximation method when alpha>1. We only encountered such alpha>1 cases when we updated alphas, and the topic modeling often sets the alpha to be in the range of [0,1]. In the cases of alpha>1, we approached this problem via approximating the inverse function of the Gamma CDF with a Newton method, but the learning performance was not satisfactory. Thus, we left the updated alpha parameters with values greater than one in the appendix. \n\nSincerely.\n\nReferences\n[1] Diederik P Kingma, Max Welling, Auto-encoding variational Bayes, ICLR, 2014.\n[2] Michael Figurnov, Shakir Mohamed, Andriy Mnih, Implicit reparametrization gradients, arXiv, 2018.\n[3] David. A. Knowles. Stochastic gradient variational bayes for gamma approximating distributions. arXiv, 2015.", "title": "Responses to Reviewer2."}, "ryx1y01Lpm": {"type": "rebuttal", "replyto": "ByxO1XWLsm", "comment": "Thank you for your review. \n\nFirstly, as a motivation for the better result, we can state as the following, and if you are okay with the below sentences, we would like to add it to the introduction part.\n\"Due to the component collapsing issues, the existing VAEs have less meaningful latent values or could not effectively use its latent representation. Meanwhile, DirVAE does not have component collapsing due to the multi-modal prior which possibly leads to superior qualitative and quantitative performances. We experimentally showed that the DirVAE has more meaningful or disentangled latent representation by image generation and latent value visualizations.\"\n\nSecondly, although the techniques are already known, we've rather wanted to focus our paper on the characteristic of Dirichlet prior on VAE such as better latent representation due to no component collapsing, or its applicability like topic modeling.\n\nThanks again for your valuable comments.\n\nSincerely.", "title": "Responses to Reviewer1."}, "Hyg1EakUTX": {"type": "rebuttal", "replyto": "r1ergtDq37", "comment": "Thank you for your review. \n\nCurrently, we are doing additional experiments to respond to your constructive comments. Sorry for the delay, but we will give you proper responds to your review with results as soon as possible. Up to the current status of experiments, the VAE with the normalizing flow still suffers from the decoder weight collapsing problem. Hence, the performance would not match to our approach, but we are going to make a certain on this premature result with the experiments, next few days.\n\nSincerely.", "title": "Responses to Reviewer3."}, "r1ergtDq37": {"type": "review", "replyto": "rkgsvoA9K7", "review": "In this paper, authors proposes an algorithm to use Dirichlet prior on the variational auto-encoder (VAE). They used this prior as natural conjugate to likelihood distributtion of multinomial (categorical). The paper proposes a way to use scalability power of VAE for data distributed by categorical distribution. In order to apply reparametrization trick, authors have used iid Gamma random variable to construct draw from Dirichlet distribution and have used approximation with inverse gamma CDF,  it is discussed how this method has better performance than other approximations method for gamma distribution such as Weibull and logistic Gaussian.\n\nAuthors pointed out, one of the weak points in competing models such as  Guassian softmax prior or Griffith -Engen-McCloskey prior which has been used for Stick breaking VAE is to not encouraging of having multi-modal posteriori, while this prior empower having multi-modal posteriori distribution which give them advantage over previous papers. \n\n In experimental results, paper has used different datasets of MNIST, MNIST+rotation , OMNIGLOT , 20newsgroup and RCVI and used different measures to compare the existing method with the baselines. \n\nTo summarize the contribution of this paper, following three points can be named as main contribution of this paper:\n- proposed a Dirichlet prior, for categorical likelihood which encourages having multi-modal posteriori. paper demonstrates couple of techniques  to apply the reparametrization trick on Dirichlet distribution, by using sum of iid Gamma random variables.  \n\n- used method of moments estimator to update the hyper parameter of the Dirichlet distribution which helps to have closer approximation of log likelihood. They update hyper-parameters after every few updates of VAE parameters.\n\n-discussed how to overcome  Stick-breaking VAE \u201ccomponent collapse\u201d issue. Experiments show superior results on supervised and semi supervised, and authors claimed the main reason of this superiority being due to not having disadvantage of component collapse which happens in SBVAE.\n\n\nQuality and Novelty:\nclaims in paper are supported by proofs and/or experimental results and there does not exist significant technical issues with the details of claims made in this paper and proofs provided. There are following issues with novelty and quality of paper that I would like discuss them under following three points:\n\n- Authors need to be clear about the motivation of the paper, if the motivation of the paper is to encourage the multi-modality in posteriori distribution, using Gaussian prior and methods like normalizing flow Rezende, Danilo Jimenez, and Shakir Mohamed. \"Variational inference with normalizing flows.\"\u00a0arXiv preprint arXiv:1505.05770\u00a0(2015) or similar may be able to do the same work in which case paper should compare its results to those ideas which has not been done in this paper.\n\n- second appealing point that this paper can make is to use Dirichlet prior for the purposes like community detection, topic modeling and LDA  etc etc. In this case, I did not find significant difference between the proposed method and what is found in Srivastava, Akash, and Charles Sutton. \"Autoencoding variational inference for topic models.\"\u00a0arXiv preprint arXiv:1703.01488\u00a0(2017), but due to the encourages of multi-modality authors show in average DirVAE performs better in measures like perplexity and NPMI. Under this condition, my main concern is interpretablity of posteriori. That will be discussed under next point\n\n- Main motivation behind using Dirichlet prior, is to have posteriori with a few significant related topic and many unrelated topic for every word. By changing the concentration parameter in stick-breaking, it is possible that performance of stick-breaking method increase in perplexity and NPMI scores in cost of loosing interpretability of the model. So having higher concentration parameter can show better performance in the cost of interpretablity that put second point of the paper at risk\n\n\nClarity: \nThe paper is well written and previous relevant methods have been reviewed well. The organization of paper is good, experiments well explained and proofs and mathematical reasoning are clear.\n\n\n\nSignificance of experiments:  \nAs discussed,in previous sections, the results show superior performance and compared to other methods on semi-supervised and supervised classification on different datasets. Also it has shown in average better perplexity and NPMI score for topic modeling, the only issue can be these scores come as cost of interpretablity of the model. Also it is possible that other competing models can be matching to this results if they do not aim for sparse posteriori.", "title": "well written paper with novelty concerns ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hke6jd7w27": {"type": "review", "replyto": "rkgsvoA9K7", "review": "This paper proposes DirVAE, a variational autoencoder with Dirichlet prior on latent variables. The advantage of using Dirichlet distribution is that due the nature of Dirichlet distribution the model does not suffer from decoder weight collapsing and latent value collapsing. Stochastic gradient variational Bayes with inverse CDF reparametrization of gamma distribution is presented.\n\nThe motivation behind using Dirichlet instead of GEM makes sense, but other than that I fail to find any novelty in the paper. The authors should tone down the statement \"to our knowledge, combining the two statistical results is the first finding in the machine learning field\". Even though left unpublished, I've been using this combination of inverse CDF gamma reparametrization and transformation to Dirichlet all the time for my own problems. It's just trivial once we have both techniques. See also [2], where an improved way of reparametrizing gamma and Dirichlet distribution is presented. The observation that DirVAE does not suffer from latent value collapsing is interesting, but not really surprising. \n\nMinor question\n- What is the difference between negative LL's and reconstruction losses in experiments?\n- The approximation for inverse CDF of gamma works well only when alpha << 1. How did you treat the regime alpha > 1?\n\n\nReferences\n[1] Diederik P Kingma, Max Welling, Auto-encoding variational Bayes, ICLR, 2014.\n[2] Michael Figurnov, Shakir Mohamed, Andriy Mnih, Implicit reparametrization gradients, arXiv, 2018.", "title": "Limited novelty", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ByxO1XWLsm": {"type": "review", "replyto": "rkgsvoA9K7", "review": "Review:\n\nThis paper proposes to change the typical Gaussian posterior distribution (and prior) for the latent features z associated to an image x that is used in Variational Autoencoders by a Dirichlet distribution. The work improves over previous attempts based on a soft-max + Gaussian distribution and the soft-max + Weibull distribution. The trick proposed to make feasible training the model includes approximating the inverse CDF of the gamma distribution and using the fact that the Dirichlet distribution can also be obtained as a normalized sum of gamma random variables. The method is compared in several problems. Some analysis of the reasons why it performs better is also carried out.\n\nQuality: \n\n\tI think the quality of the paper is high. It is a well written paper in which the choices made are well supported. It also has a strong experimental section.\n\nClarity: \n\n\tThe paper is well written and reads very smoothly. I have missed however a more clear statement in the introduction supporting the use of the Dirichlet for the prior and posterior of the latent variables, simply because it seems to give better results and the typical Gaussian choice.\n\nOriginality: \n\t\n\tThe paper is based on ideas already known. E.g., Dirichlet a normalized sum of gamma random variables and approximation of the inverse CDF of the gamma random variable. The combination of these two techniques is however novel. \n\nSignificance:\n\n\tThe results obtained indicate that the proposed approach improves over previous work on the Dirichlet VAE and on the Gaussian VAE. So I believe the significance of the paper is high.\n\npros:\n\n\t- Good results.\n\n\t- Simple method proposed.\n\n\t- Extensive experiments.\n\n\t- Well written paper.\n\ncons:\n\t\n\t- The idea is a combination of already known techniques put in practice for the VAE.\n\n\t- A better motivation that the Dirichlet VAE gives good results should be given at the introduction.\n", "title": "A simple method giving improved results", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}