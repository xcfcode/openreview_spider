{"paper": {"title": "Program Synthesis for Character Level Language Modeling", "authors": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "authorids": ["pavol.bielik@inf.ethz.ch", "veselin.raychev@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "summary": "", "abstract": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.\n\nOur experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "This work was a very controversial submission, with two strong accepts and one initial strong reject. There was significant discussion about the replicability of the paper, although all reviewers seem interested in the methods. \n \n Pro:\n - There is strong originality to this work. One of the few submissions in the area not just using RNNs for LM.\n - The paper shows strong empirical results on PL language modeling, comparative results in natural language modeling, fast lookup compared to neural models, and presents a significantly different approach then the currently accepted NN methods. \n \n \n Cons:\n - The original draft has clarity issues. In particular the MCMC approach is very difficult to follow and lacks clear explanation for this community, the interpretability of the method is not demonstrated, and too much of the work is devoted to laying out the language itself. (Note The authors have gone out of their way to include an appendix which caused one reviewer to go from recommending strong rejection to weak rejection, but also requires a much larger submission size. While two reviewers do like the work, their reviews are mainly based on the empirical sucess at program language modeling.)\n \n Overall, the PCs have determined that this work deserves to appear at the conference."}, "review": {"r1fH4zVUx": {"type": "rebuttal", "replyto": "Sk4mNvGEe", "comment": "Thanks for the suggestions, we added a thorough Appendix that provides all details. We also included details about the used hardware in our experimental setup in the updated version. All of our experiments were performed on a machine with Intel(R) Xeon(R) CPU E5-2690 with 14 cores. All training times are reported for parallel training on CPU. Using GPUs for training of the neural networks is likely to provide additional improvement in training time.", "title": "Response"}, "Skpm4GEIg": {"type": "rebuttal", "replyto": "HkPzKlz4x", "comment": "Given the limits on the length of the submission it was not possible to include all details. However, as suggested by all reviewers, we added an Appendix describing all relevant details of the synthesis, formal semantics and how the probability distributions are built. We also updated Table 2 in the main paper to include results for n-gram (1.94 BPC).", "title": "Response"}, "H1C-4zVIg": {"type": "rebuttal", "replyto": "BkcQb8X4x", "comment": "Thanks for the suggestions. In the Appendix, we clarify the training procedure as well as how the probabilistic model is built. It is true the model is based on discrete and deterministic decisions obtained by executing the learned program. This is also true for the probabilistic model which uses maximum likelihood estimation based on counting (as in n-gram models). It is an interesting future work direction to extend to model with continuous word representations as done in neural networks.", "title": "Reponse"}, "HJnk4G48x": {"type": "rebuttal", "replyto": "S1kUe2RHx", "comment": "To address the main questions raised by the reviewers, we extended our submission with a new 7-page Appendix that precisely describes:\n\n\u2192  how a probabilistic model is obtained.\n\u2192  how programs are synthesised from data, and\n\u2192  formal semantics of TChar programs. \n\nWe believe this makes our approach self-contained and reproducible, but we are happy to add more details, if requested.", "title": "Response"}, "S1kUe2RHx": {"type": "rebuttal", "replyto": "ry_sjFqgx", "comment": "Currently this paper has a quite controversial score 8/8/3. Generally reviewers seem quite willing to consider a paper like this for ICLR, particularly due to the experimental results, but are concerned that there is very-little-to-zero details given about the synthesis approach. This makes the details of the paper hard to check and unreplicable. \n\nIf you feel like there is something you can do to sway them on this point, now would be a great time to respond. ", "title": "Authors: Please post a response"}, "rk4oXvVQx": {"type": "rebuttal", "replyto": "Hk3LqNz7l", "comment": "Thanks for the questions. Please see our answers provided below:\n\nQ1: Could the authors provide and explain specific examples where this method yields interpretable behavior?\n\nA: Yes, by inspecting the learned program for the Linux kernel, also provided at http://www.srl.inf.ethz.ch/charmodel.html, we can see that the program initially learns to predict indentation at the beginning of each line (programs 67, 110), new lines and spaces (program 949), static constants (program 1250), first character of a word (program 275), end of line semicolons and pointer access (program 328) and the rest. These programs are then further refined (e.g., to predict whether the return value is void or a value).\n\nFor example, in label 110, we have:\nswitch LEFT LEFT WRITE: on \"10|92\" goto 74; on \"41\" goto 85; on \"44|58|123\" goto 86; on \"47\" goto 91; on \"59\" goto 100; on \"62\" goto 101; on \"125\" goto 105; else goto 109\nThe numbers in the quotes are ASCII values. We can see that the program reads a character that is two positions left of the current position and applies different models (goto \u2026) if it is new line (10) or \u201c/\u201d (92), closing bracket \u201c)\u201d (41) , etc.\n\nQ2: Are there experimental settings that differ from the experiments in Karpathy et al. 2015 for the Linux Kernel Dataset? The BPC corresponding to the CE loss that Karpathy et al. obtain seems significantly lower than in Table 1 in this paper.\n\nA: Using the publicly available implementation of Karpathy et.al. we were able to reproduce the LSTM results from their paper. However, these numbers differ from what we report as unfortunately, upon closer inspection of their implementation, we found what we believe is a bug that affects their results. The issue is that they do not split the dataset in the standard way which would be say: first 80% for training, next 10% for validation, and last 10% for testing. Instead, they draw the validation and training data periodically throughout the whole dataset: first 8 batches for training, next for validation, next for testing, next 8 again for training, etc. This is a problem as the split is not representative of how the model is queried in practice (e.g., with previously unseen code). We fixed the implementation so it uses a standard split, retrained their LSTM model, and reported the numbers. We also could not reproduce their n-gram baseline that had very similar CE loss compared to the LSTM/GRU models (in our experiments the n-gram was worse than LSTM). We contacted the authors to determine the issue, but they did not respond (they did respond to other inquiries, but not to this one).\n\nIn terms of notation, they report entropy in nats (also seen in their code) whereas we report entropy in bits. It is easy to convert nats to bits to match our results by multiplying their numbers by ~1.44.\n\nQ3:  What smoothing method was ultimately used for the n-gram benchmarks on the Linux Kernel Dataset?\n\nA: Our evaluation results use Witten-Bell smoothing everywhere. We note that KN has similar performance, but we did not include the numbers.", "title": "Answers"}, "B1-x7PVQx": {"type": "rebuttal", "replyto": "ByhJuwZXe", "comment": "Thanks for the questions. Please see our answers provided below:\n\nQ1a: It would be helpful if the paper had an example of the program. \n\nA: We provide a visualization of the learned program for the Linux Kernel dataset at http://www.srl.inf.ethz.ch/charmodel.html (link is also provided in the paper).\n\nQ1b: If I understand things correctly, after training is done, there is one program synthesized for the whole dataset?\n\nA: Yes, this is correct.\n\nQ2a: What is the synthesis algorithm used? \n\nA: The challenging part of the synthesis algorithm is finding SwitchPrograms that contain branches. The algorithm we use is based on decision tree learning and is described in detail in the following paper: Probabilistic model for code with decision trees. ACM OOPSLA\u201916:\nhttp://www.srl.inf.ethz.ch/papers/oopsla16-dt.pdf\n\nQ2b: How you estimate the cost of a program?\n\nA: For a given dataset, the cost of a program is the average bits-per-character (log likelihood of a given prediction) required by the language model specified by the program (evaluated on the validation set). This is also the metric that is minimized during training.\n\nQ2c: Are there any tricks to make the cost estimation fast?\n\nA: The cost estimation in our model is inherently fast as it is based on counting, as in simple language models such as n-gram, and requires only several hash lookups for each prediction (in addition to executing the learned program which is fast). Therefore it is enough to use a high performance hashtable implementation. We did not use any speed-up tricks to estimate the cost faster.\n", "title": "Answers"}, "Hk3LqNz7l": {"type": "review", "replyto": "ry_sjFqgx", "review": "1. Could the authors provide and explain specific examples where this method yields interpretable behavior?\n2. Are there experimental settings that differ from the experiments in Karpathy et al. 2015 for the Linux Kernel Dataset? The BPC corresponding to the CE loss that Karpathy et al. obtain seems significantly lower than in Table 1 in this paper.\n3. What smoothing method was ultimately used for the n-gram benchmarks on the Linux Kernel Dataset?The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program. Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late. The model should also be much quicker to apply at query time. Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task. A detailed description of the language syntax is provided.\n\nCons/suggestions:\n- The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions.\n- The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context.\n- More compact/convincing examples of human interpretability would be helpful.\n\nOther comments\n- Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc.\n", "title": "Questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "Sk4mNvGEe": {"type": "review", "replyto": "ry_sjFqgx", "review": "1. Could the authors provide and explain specific examples where this method yields interpretable behavior?\n2. Are there experimental settings that differ from the experiments in Karpathy et al. 2015 for the Linux Kernel Dataset? The BPC corresponding to the CE loss that Karpathy et al. obtain seems significantly lower than in Table 1 in this paper.\n3. What smoothing method was ultimately used for the n-gram benchmarks on the Linux Kernel Dataset?The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program. Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late. The model should also be much quicker to apply at query time. Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task. A detailed description of the language syntax is provided.\n\nCons/suggestions:\n- The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions.\n- The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context.\n- More compact/convincing examples of human interpretability would be helpful.\n\nOther comments\n- Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc.\n", "title": "Questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "ByhJuwZXe": {"type": "review", "replyto": "ry_sjFqgx", "review": "1. It would be helpful if the paper had an example of the program. An example would be much more helpful than the grammar specification. If I understand things correctly, after training is done, there is one program synthesized for the whole dataset? In that case, it would be good to include the synthesized program in the paper.\n2. What is the synthesis algorithm used? I understand that it's MCMC, but it would be useful to allocate some space to the algorithm. In particular, I'm also interested in knowing how you estimate the value of a program. Are there any tricks to make the value estimation fast?This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.\n\nExperiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.\n\nIt's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such \"outside\" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.\n\n*Pros*\n1. Novel approach.\n2. Good results.\n\n*Cons*\n1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.\n\n*Comments*\n1. Please include n-gram results in the table for Wikipedia results.", "title": "Questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkPzKlz4x": {"type": "review", "replyto": "ry_sjFqgx", "review": "1. It would be helpful if the paper had an example of the program. An example would be much more helpful than the grammar specification. If I understand things correctly, after training is done, there is one program synthesized for the whole dataset? In that case, it would be good to include the synthesized program in the paper.\n2. What is the synthesis algorithm used? I understand that it's MCMC, but it would be useful to allocate some space to the algorithm. In particular, I'm also interested in knowing how you estimate the value of a program. Are there any tricks to make the value estimation fast?This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.\n\nExperiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.\n\nIt's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such \"outside\" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.\n\n*Pros*\n1. Novel approach.\n2. Good results.\n\n*Cons*\n1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.\n\n*Comments*\n1. Please include n-gram results in the table for Wikipedia results.", "title": "Questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}