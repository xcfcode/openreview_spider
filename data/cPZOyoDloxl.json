{"paper": {"title": "SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments", "authors": ["Glen Berseth", "Daniel Geng", "Coline Manon Devin", "Nicholas Rhinehart", "Chelsea Finn", "Dinesh Jayaraman", "Sergey Levine"], "authorids": ["~Glen_Berseth1", "dangengdg@berkeley.edu", "~Coline_Manon_Devin1", "~Nicholas_Rhinehart1", "~Chelsea_Finn1", "~Dinesh_Jayaraman2", "~Sergey_Levine1"], "summary": "Using Bayesian surprise as an unsupervised intrinsic reward function to learn complex behaviors in unstable environments.", "abstract": "Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning.", "keywords": ["Reinforcement learning"]}, "meta": {"decision": "Accept (Oral)", "comment": "The paper is studying a new intrinsic motivation RL setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it. The resulting idea is simple but also surprising that it works so well. All reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing. In addition, they identified the following strengths of the paper:\n- The experiments are exhaustive, identifying many domains where the approach can be applied\n- The presented results are compelling\n- The paper is well written\n- The paper introduces a new problem setup that has not been studied before\n\nI agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance.\n\n"}, "review": {"J0VfFbrJ7Vv": {"type": "review", "replyto": "cPZOyoDloxl", "review": "This work proposes an RL approach SMiRL that is able to learn effective policies in unstable environments without the need for external reward. The idea at a high-level is almost the opposite of intrinsic motivation RL approaches, which encourage novelty-seeking behaviors. The proposed method instead aims to minimize surprise or state entropy. To train the agent, rewards come from state marginal estimates, but because this distribution is changing, the authors create an augmented MDP. Through experiments on game domains and robot control tasks, the authors show that SMiRL outperforms intrinsic motivation methods. The authors also show that SMiRL can be used to do imitation and can be combined with regular reward signals.\n\nPros:\n- The problem formulation is interesting and novel. Intrinsic motivation is well studied, but this problem considers the setting where the environment is unstable rather than static, which requires new methods.\n- The paper is written well and is clear. The motivation is described well.\n- The authors evaluate on many domains, highlighting the diversity of settings in which the approach can be applied.\n\nCons:\n- It seems like this approach is only applicable to unstable environments. Does the approach fail for regular, static environments? I\u2019m assuming the agent might just end up staying still because it\u2019s trying to seek stable states. It can be combined with the external reward signal but will the minimizing entropy objective hurt you?\n- Without common sense knowledge, this approach would take many iterations to learn. So while this formulation might be more similar to the real world, the real world would only allow for a few interactions with the world.\n\nComments:\n- How is SMiRL doing better than the oracle in Figure 3 center?\n- In Figure 3 left, it might be a problem that with minimal episodes, SMiRL does worse. If SMiRL is useful for more real-world unstable environments, this would require a simulator good enough to model the real world.\n\nRecommendation:\nOverall, the paper is interesting and novel. The approach is reasonable and experiments show the value of the method in unstable environments. I recommend \u201caccept\u201d.\n\n------------\nPost-rebuttal response:\nThe authors addressed most of my concerns so I continue to recommend acceptance of the paper. Specifically, they answered my question about whether the approach will work in static environments and how prior data can be used to improve sample efficiency. They also conducted additional experiments to verify some of my questions.", "title": "Official Review #2", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "K9NW0mzoYle": {"type": "review", "replyto": "cPZOyoDloxl", "review": "------------------------------------        \n**Summary:**\n\nThis paper proposes a new intrinsic objective for RL agents: surprise minimization. This may come as a surprise, as other related works usually propose to maximize surprise, or to maximize novelty. The authors present motivations and conduct an empirical study on several environments to support their idea.\n\n------------------------------------        \n**Strong points:**\n\nOverall, I think it is a good paper. Let me list some strong points:\n\n* The idea is simple, novel and well motivated. The paper positions this new intrinsic objective with respect to variants of novelty maximization objectives and brings a new perspective.\n* It\u2019s well written and organized\n* The algorithm is tested on a relevant selection of environments and against state-of-the-art algorithms using intrinsic objectives.\n* The empirical evidence seems to support the claims.\n* The related work is quite complete.\n* I liked the discussion about stable vs unstable environments. It is the first time I see it discussed.\n* The website brings visualizations of trained policies.\n------------------------------------        \n**Weak points:**\n\nI will now list a few weak points of the paper.\n* Some descriptions of the results are missing. In figures, what does the shaded area represent? (std, sem, confidence intervals, etc). In Table 1, what are the numbers? (what is the central tendency, what is the error, how many seeds?). Same for Appendix D.\n* The paper does not provide all necessary information to reproduce the results. There is no detail about the RL algorithms (TRPO and DQN), no description of the architectures, and no hyperparameters. This is important and should be contained somewhere in the Appendix. \n* Will the code be released? If not, why so? Same questions for the environments, are they accessible somewhere? \n* It seems to me that this approach could potentially tackle harder problems, but the paper is limited to a simplification of the tetris game, planar humanoid variants and Doom. The x-axes of the figures also tell us that only a few episodes were needed to solve them. I am not saying that I need the hardest games solved to find an algorithm interesting, but I am wondering whether it would scale. Could you tell us whether you attempted to tackle harder environments, and if so, why do you think SMiRL failed? I think we can gain a lot of understanding by looking at negative results. \nFor example, I feel like testing this algorithm on a 3D humanoid would better demonstrate the power of this approach. \n* It seems to me that there might be a confounding factor that could partially explain the success of the surprise reward. Indeed, it seems that all environments presented here can terminate when the agent dies. This is a guess, as I could not find this information in the paper (please add it). If so, then the expected cumulative rewards is an increasing function of the lifetime of the agent in the game. Because of this, maximizing the cumulative surprise might be a good idea because it goes in the same direction (by construction) as maximizing survival.\nA counter-argument from the paper can be the performance of SMiRL + reward in Walk, that is superior to the performance of reward alone. However, this is unclear as I could not find the description of the reward function in the paper (please add it). \nAnother way to disprove this hypothesis would be to compare the performance of SMiRL to the performance of an agent maximizing a reward function that gives +1 whenever the agent is alive (survival bonus). If it performs better, then surprise maximization brings something to the table, if it does not, then it might work because it is correlated to the survival time.\nAnother way could be to have episodes that do not include death-related resets (fixed length episodes).\n\n------------------------------------        \n**Recommendation and justification:**\nThis is overall a strong paper. However I\u2019m concerned about the potential confounding factor of the survival time. I\u2019ll give a score of 6, but I would happily increase that score if \n1. The authors convince me that the success of the surprise maximization is not due to the survival confounding factor.\n2. The authors include all necessary details for reproducibility and/or release the code. \n3. Discuss the scalability to harder problems (e.g. 3D Humanoid).\n\n------------------------------------        \n**Feedback to improve the paper (not part of assessment)**\n\n* I would move the discussion about how surprise minimization and novelty maximization can be complementary to the intro. These two approaches seem to go in opposite directions and, as a reader, I would be happy to read this discussion early.\n* It would be interesting to discuss how it plays out in natural agents. The intuition is that minimizing surprise leads to finding a stable configuration and staying there. In practice, it is probably balanced with other driving needs like the need for food. I guess it is discussed in related papers like Friston 2009. In natural agents, surprise minimization must also be model-based. Indeed, animals do not need to jump out of cliffs several times to know that it\u2019s bad. \n* Not sure I understand the inequality in Eq1. Maybe I missed something.\n* Discuss the surprise maximization approach of Achiam et al and whether it differs from yours. \n* Table 1: the legend seems to disagree with the results. It seems to me that the entropy difference is as low in your environment as in the others, but the caption says \u201cnote the clear negative  entropy gap on our tasks, whereas this clear trend is absent on the Atari games\u201d.\n* Fig 4. Is it really training in 80 episodes? There are very few images to train a VAE, especially if the episode resets when the agent falls (does it?) How many steps per episode?\n* How do you get demos for humanoid tasks? I guess there are not human demos but previously trained agents? \n* Results in Fig. 6 are not super satisfying, they are quite far from the target (although I guess it is a difficult task). I am not even sure the second example is achievable. In the traditional Tetris, wouldn\u2019t cubes fall due to gravity?\n\n**Typos:**                \n* \u201cour results are available online\u201d \u2192 missing full stop.\n* \u201cIn such environments, which we believe are more reflective of the real world\u201d \u2192 previous sentences do not discuss environments but intrinsic objectives.\n* \u201cunexpected events don\u2019t happen\u201d \u2192 \u201cdo not\u201d\n* \u201cdeep DQN\u201d \u2192 \u201cDeep Q-Networks\u201d, or \u201cDQN\u201d\n\n\n**Update post-rebuttal**\nThe authors addressed most of my concerns, especially the one about the confounding factor. I am updating the score from 6 to 7.\n\n", "title": "This paper introduces suprise minimization as a intrinsic objective. The idea is novel, well motivated and seem to be supported by empirical evidence. However, some details are missing and there might be a confounding factor.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "McceVXFRRC6": {"type": "review", "replyto": "cPZOyoDloxl", "review": "This paper presents and studies an unsupervised learning approach to the emergence of \"meaningful/useful\"\nbehaviour in a Deep RL setting, based on an algorithm which objective consists in minimizing surprise.\nIt is possible to see this paper as studying a Deep RL implementation of cognitive homeostasis, an old idea\nrecently popularized through the work on the free energy principle by Friston and colleagues.\nThe paper presents two versions of this algorithm, one without representation learning, and one with\nrepresentation learning (VAE), and compares it to Deep RL algorithms that maximize prediction error/novelty\nfrom two perspectives: emergence of behaviours and utility as an auxiliary reward to solve sparse reward problems.\nThe paper discusses the relevance of the approach in environments that are said to be \"unstable\" (with an attempt\nto formalize this concept). The paper also shortly presents an application of the algorithm for imitation learning.\nExperiments are presented in a variety of environments.\n\nMajor strengths:\n+ The paper is globally clear and well-written\n+ The qualitative results showing and analyzing emergent behaviour are stimulating\n+ Implementation of the free-energy principle in high-dimensional spaces is known to be challenging and this\npaper contributes towards understanding how to do it (yet the precise formal articulation between SMIRL and the\nFEP remains to be done, and this does not seem to be the primary objective of the paper)\n+ The discussion of the complementarity of within-episode SMIRL and across-episode novelty seeking intrinsic\nrewards is interesting\n+ The shortly described application for imitation learning is promising\n+ The are many and varied environments in experiments \n\nMajor weaknesses:\n- The environments chosen for experimentation are such that minimizing surprise aligns very well with either\nproducing \"interesting\" behaviour or maximizing an external reward. One could easily consider slight modifications\nof most environments where a trivial solution to minimizing surprise could be found instead (e.g. giving the ability to push\non the \"pause\" button in the Tetris game or having safe rooms in opposite directions than enemies in Vizdoom/HauntedHouse).\nIn a real robot, this would lead a robot to hide in the corner of a room and just look at a uniform wall that does not move.\nThis problem of trivial solutions to surprise minimization approaches has been called the Dark Room problem in\nthe FEP literature. This is known to be a limit of this general approach when applied to the real world, and how to address it\noperationnally (in a tractable manner) under this paradigm remains an open question.\n- Related to the point above, the fact that different environments are used in different experiments of the paper does not\nallow to get a good grasp of the general behaviour of the algorithm: I would recommend to run all experiments in all\nenvironments (maybe to report in Annex), and at least to justify why environmnets change across experiments.\n-  The paper is positioned in comparison with novelty seeking/prediction error maximizing agents, and motivates\nthe surprise minimizing approach by arguing that real-world situations are problematic for novelty seeking agents,\nas the real world spontenously generates novelty through distractors that should be avoided rather than explored.\nThis is accurate, but an incomplete discussion of the literature in the area of unsupervised learning of behaviours:\napproaches maximizing learning progress were precisely introduced to adress these limits of novelty seeking approaches\nand to scale to real world environments with two families of distractors: novel unlearnable parts of the environments\n(which is a problem for novelty seeking approaches) and trivial low-entropy parts of the environments (which are a problem\nin principle for surprise minimization approaches). Thus, the paper should position itself in comparison with LP-based\napproaches and show experiments with at least some of the existing LP-based algorithms (e.g. Schmidhuber, 1991; Lopes et al., 2012;\nKim et al., 2020).\n- As the authors acknowledge, the version of SMIRL using represention learning is not fully \"within episode surprise minmization\",\nas the learned representation depends on past episodes: as a consequence, it is unclear what one should conclude about the relation\nbetween within vs across episode mechanisms and the properties of emergent behaviours\n- The paper lacks discussion of related work aiming to implement surprise minimization approaches that scale to high-dimensional\nspaces, especially in a RL framework, e.g. Tschantz et al., 2020; Annabi et al., 2020\n- The paper does not provide code to enable reproducibility of results (and does not say it will)\n\nOverall, the topic of this paper is interesting and the work could make a valuabe contribution, especially from the perspective\nof studying incentives to emergent structured behaviour in unsupervised learning,. However, the weaknesses mentionned above need\nto be addressed to better establish the contributions of this paper, both in terms of understanding the generality of the\nresults and the contributions in relation to the existing litterature.\n\nReferences:\n\nAnnabi, L., Pitti, A., & Quoy, M. (2020). Autonomous learning and chaining of motor primitives using the Free Energy Principle. arXiv preprint arXiv:2005.05151.\nKim, K. H., Sano, M., De Freitas, J., Haber, N., & Yamins, D. (2020, January). Active world model learning in agent-rich environments with progress curiosity. In International Conference on Machine Learning (ICML).\nLopes, M., Lang, T., Toussaint, M., & Oudeyer, P. Y. (2012). Exploration in model-based reinforcement learning by empirically estimating learning progress. In Advances in neural information processing systems (pp. 206-214).\nJ. Schmidhuber, \u201cCurious model-building control systems,\u201d in Proc. Int. Joint Conf. Neural Netw., Singapore, 1991, vol. 2, pp. 1458\u20131463.\nTschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2020, July). Scaling active inference. In 2020 International Joint Conference on Neural Networks (IJCNN) (pp. 1-8). IEEE.\nTschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020). Reinforcement Learning through Active Inference. arXiv preprint arXiv:2002.12636.\n", "title": "Interesting approach, but several issues need to be addressed", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "2NyNfK3bcb_": {"type": "rebuttal", "replyto": "ilMUnVnMmW7", "comment": "In Figure 3 (top-left and top-center), we included an additional experiment, Oracle (rows cleared), where the reward function used for the agent was the number of rows cleared. SMiRL also performs better than this Oracle.\n", "title": "Additional Oracle Experiment"}, "7q8UvEHGZ49": {"type": "rebuttal", "replyto": "20AbzRwvZpN", "comment": "Re: SMiRL vs survival reward.\n\nAfter rerunning the humanoid experiments without resets, most environments still learn to minimize entropy with fixed episode length. See Figure 9 at the end of the paper.\n\nRe: 3D biped results.\n\nWe have attempted to produce 3D biped results. Thus far, we have not been able to produce policies that significantly reduce entropy. This may be due to the increased complexity of the environment and much larger action space.", "title": "Followup "}, "YvdlzuI9Lm6": {"type": "rebuttal", "replyto": "TqpDjZl5Atg", "comment": "We have completed the requested additional experiments. See the additional analysis in Figure 3 and three new plots in Figure 3. We have also updated the paper text to integrate these results. With these additional experiments, the conclusions remain largely the same. Here we provide a summary of these additional experiments. \n- Emergent Behavior:\n  - Added SMiRL + VAE on Tetris (Figure 3 top-left and top-center) and DefendTheLine (Figure 3 bottom-left)\n  - Added SMiRL + ICM on Tetris (Figure 3 top-left and top-center) and TakeCover (Figure 3 top-right ) DefendeTheLine (Figure 3\n  - Evaluation on DefendTheLine (Figure 3 bottom-left) \nbottom-left)\n- Auxiliary reward experiments \n  - Performed this for the Tetris environment (Figure 3 bottom-center) \n  - Performed this for the VizDoom TakeCover environment (Figure 3 bottom-right)\n", "title": "RE: Additional Experiments"}, "Xlq8cHVusYM": {"type": "rebuttal", "replyto": "TqpDjZl5Atg", "comment": "Q5: within- versus across- episode SMiRL with representation learning\n\nA5 (updated): Our study is in the setting in which a reliable state representation is available or easily learned from existing observation. Training the learned state representation online does affect the approximation in Eq (1) as the learning process is changing the representation for which the entropy is computed. While we do not have any theoretical analysis of how this representation learning process interacts with our algorithm, empirically we did not find this non-stationarity to be an issue, and generally observed learned representation to improve performance in experiments with large image observations or complex dynamics. Using these learned representations is not meant to address the difficulty of estimating and minimizing surprise across episodes or more generally over long sequences (possibly a single episode) which is a challenge for surprise minimization-based methods. We are happy to comment on this more if there are additional questions. We have expanded our existing discussion on this limitation to clarify this point.\n\n\nAdditional related work:\nThank you, the work in Biehl et al., 2015 provides a discussion on connecting FEP, empowerment and Predictive information maximization in the same framework to explain their similarities and differences. We have added this content to the related work section.\n", "title": "R4 Discussion"}, "GiX41yiyTXH": {"type": "rebuttal", "replyto": "p5A297hPqzi", "comment": "A3: Teris is not one of the common openAIGym or Atari benchmarks, however, the simple reward of -1 when the agent dies was seen as an easy and simple reward function to use to train the agent. We will run another experiment using rows cleared as the reward function and include that in Figure 3.", "title": "Q3 Oracle clarification"}, "nOOu2IVTFHb": {"type": "rebuttal", "replyto": "20AbzRwvZpN", "comment": "Thank you for your quick response.\n\nA1: For the humanoid tasks death occurs when the agent falls 5 meters below the ground. At that point, it is impossible for the agent to recover and balance on the ground. These resets prevent the agent from using time, collecting data in this bad area of the state space and as a result speed up learning. However, we are running experiments without these resets and will include the results in the paper.\n", "title": "Follow up Q1"}, "ZbXu1S7QD8a": {"type": "rebuttal", "replyto": "McceVXFRRC6", "comment": "Reviewer 4 Rebuttal:\nWe thank the reviewer for their time and comments on the paper. We\u2019ve modified the paper to address the following issues raised in the review. (1) The related work section has been updated to include the suggested citations and a discussion of the Dark Room issue with FEP based methods. We explain these changes below, with a more detailed discussion in the appendix (due to space constraints). (2) In regard to experiments and environments, we are currently running additional experiments to include all domains for each section and will add them to the paper. (3) With respect to missing discussion and comparison to learning progress (LP) approaches, we have included discussion about their differences, and are in the process of adding additional experiments that use an LP-based method. (4) Lastly, we have released the code for the paper on the website (https://sites.google.com/view/surpriseminimization) and included more details in Appendix A on the training algorithms, networks and hyperparameters. We believe that these additions and modifications address all of the concerns raised in the review, but please let us know if there are any further issues to address.\n\n----------------------------------------\nQ1: Dark Room problem in the FEP literature (trivial entropy minimizing solutions)\n\nA1:The Dark Room problem is a known issue for Free Energy Principle (FEP) based methods and surprise minimization methods. As discussed in Table 1, we do not aim for our method to work in either partially-observed environments or environments that do not satisfy our unstable assumption. In practice, we expect the number of truly dark rooms in the real world to be low, as the robot\u2019s internal state (battery life, motor drift) should always be observable, but we agree that this is an important potential issue.  We have added content to the Related Work section to note this issue and updated Section 4 to further describe the types of environments in which SMiRL should work well.\n\n----------------------------------------\nQ2: Different environments are used in different experiments or  justify why environments change across experiments\n\nA2: We will run the missing experiments and add them to the paper. The missing experiments are the VAE-based experiments in the Tetris environment and SMiRL + ICM on Tetris and Vizdoom. We do not expect that the VAE will help for Tetris because the Tetris observation is already well abstracted. For both Tetris and Vizdoom, we did not find that SMiRL alone suffered from a lack of exploration, but it is possible that adding ICM could speed up learning.\n\n----------------------------------------\nQ3: Discussion of the literature in the area of unsupervised learning of behaviours\nA3: We thank the reviewer for the suggestion of positioning our work in relation to LP-based methods, which direct the agent to investigate complex but learnable dynamics in the environment. We have revised the related work section and the appendix to include a discussion of these methods. Our work differs from LP-based methods because SMiRL is learning to control the marginal state distribution rather than identifying the system parameters. We are investigating running experiments for an LP-based method, however, the papers cited do not include code. Would using the \u201cVIME: Variational Information Maximizing Exploration\u201d method be acceptable in this regard? \n\nQ4: Discussion of active-inference based surprise minimization in high-dimensional spaces, e.g. Tschantz et al., 2020; Annabi et al., 2020\nA4: We added text on the works by Tschantz et al. 2020 and Annabi et al. 2020 in the related work. These approaches require the task reward function as a \u201cglobal prior\u201d and minimize a KL between the agent\u2019s observation distribution and this \u201cglobal prior\u201d. Our work is instead in the area of unsupervised reinforcement learning (i.e. in the absence of task rewards). \n\n", "title": "Responce for R4"}, "RDN2aWp-GKz": {"type": "rebuttal", "replyto": "K9NW0mzoYle", "comment": "Thank you very much for your time and valuable feedback!  \n\nQ1: Is SMiRL effective only by providing a survival bonus to avoid death-related resets?\n\nA1: In our current experiments, we use fixed-length episodes during training for all Tetris, VizDoom and HauntedHouse experiments. The use of fixed-length episodes avoids the issue of SMiRL appearing to be a survival bonus, as there are no death-related resets in those experiments. The humanoid experiments do use death-related resets. However, for these environments, a survival bonus would treat the desired behaviour of remaining on the cliff, treadmill, or pedestal the same as falling off. We have added these details on the use of fixed-length episode training to Appendix A.\n\n----------------------------------\nQ2: Information to reproduce the results.\nA2: We have addressed this in two ways. First, additional information has been added on  environments, training networks and hyperparameters in the appendix. Second, we have posted the code on the paper website (https://sites.google.com/view/surpriseminimization) and will include it with the published paper. \n\n-------------------------------\n\nQ3: Harder problems for SMiRL\nA3: We agree that running SMiRL in more interesting and difficult environments could provide additional information. We have not investigated many environments beyond the ones used in the paper. However, as per your request, we are running SMiRL on a 3D humanoid and will include these results in the paper as well.\n\n----------------------------------\nQ4: Clear entropy gap and how are the values computed.\n\nA4: We examined a set of environments, Tetris, Vizdoom and a collection of Atari games, to determine their relative entropy gap. We found that the environments we focused on in the paper (DefendTheLine, Tetris, TakeCover) do have negative relative entropy gaps, shown in Table 1. In addition, we found that two Atari environments (Assult and SpaceInvaders) also have a negative entropy gap, indicating that SMiRL should work well on them. We have revised the wording for the caption to clarify this relationship.\nThe measure used to approximate the entropy is the same as the right side of equation (1) multiplied by -1. We added this detail to the last paragraph of section 4 as well. In the table, we provide the mean and std over 3 random seeds for each method on each environment.\n\n-------------------------------\nQ5: What does the shaded area represent in the figures?\nA5: The shaded areas in the figures indicate the std of the random seeds for each experiment. We have added notes to the figures to explain this.\n\n-------------------------------\nQ6: Compare to  the surprise maximization approach of Achiam et al\nA6: Thank you for bringing this paper to our attention. This paper maximizes surprise to encourage exploration and increase entropy, whereas our method does the opposite, and minimizes surprise. The paper\u2019s goal is similar to RND and ICM, in that it is aimed at promoting visitation of novel states, in this case by training a model over recent experience and adding a bonus to the reward when the model does not predict the data well (because the agent is in a novel area of the state space). We have included this in Section 2 of the paper to make our review of related work more thorough. \n\n-------------------------------\nQ7: Fig 4. Is it really training in 80 episodes?\nA7: There was a multiplication error, the number of episodes is actually 4 times that amount. We have updated the figures in the paper.\n\n-------------------------------\nQ8: How do you get demos for humanoid tasks?\nA7: We use the policy that was trained using the reward function (Reward in Figure 7) to collect 2048 samples of states to initialize \\theta with. This is described briefly in \u201cSMiRL as an auxiliary reward\u201d section. We have provided more details on how this data is collected and used in Appendix A.\n\n-------------------------------\nQ9: \u201cResults in Fig. 6 are not super satisfying, they are quite far from the target (although I guess it is a difficult task)\u201d\nA9: SMiRL does fail to reach the targets in Fig 6, but please consider looking at the videos at https://sites.google.com/view/surpriseminimization, linked in the paper, which may provide a better sense of the performance on this task and the difficulty involved. Also note that for the second row of Fig 6, the target checkerboard pattern is actually impossible to achieve in the Tetris game. \n", "title": "Responce to R3"}, "JIZVrl10Sve": {"type": "rebuttal", "replyto": "XssQGoHC2vD", "comment": "Thank you very much for the feedback! \n\nIt is true the unstable environment does produce some underlying exploration but the algorithm does not rely on this noise. This may be verified by observing that all algorithms experience the unstable environment, yet SMiRL performs significantly better than the baselines across our experiments. As far as adding explicit exploration strategies to SMiRL, yes, this is possible, and we have experimented with this idea by adding traditional novelty-seeking bonuses to SMiRL (SMiRL + ICM) agents and obtained improved results in some environments (described in Pg 7, results in Figs 4 and 5).\n", "title": "Responce to R1"}, "Pjv9HSXro7E": {"type": "rebuttal", "replyto": "J0VfFbrJ7Vv", "comment": "Clarifications and additions\n\nQ1: SMiRL in static environments: \nA1: We acknowledge in Section 3, SMiRL is designed to work in unstable environments. In a completely static environment that does not evolve without agent action, a SMiRL agent would have no incentive to explore the environment, and may indeed avoid exploration under the influence of an external reward function, hurting performance. Note, however, that such misalignment between the intrinsic objective and an external reward signal is not specific to SMiRL in static environments; it would also occur for more traditional novelty-seeking methods in unstable environments. Section 4 has been updated to better describe this issue and the types of environments SMiRL should work well on. \n\nQ2: Sample-inefficient, requires realistic simulation and/or common sense:\n\nA2: While our method is indeed slower to learn on Tetris, this is not true across all environments. For environments that have sparse rewards (VizDoom Figure 7) or complex dynamics (humanoid, Figure 7 right) learning benefits from using SMiRL. However, we agree that SMiRL in its current form is sample-inefficient, and infusing prior knowledge is indeed one way to alleviate this issue. We show in Fig 7 that \u201cReward +SMIRL + prior data\u201d, which has access to expert walking demonstration data for initializing the state distribution, learns a successful walking policy much faster than methods without access to such demonstration data. Section 5 has been edited to describe this and how using prior knowledge can be an avenue to speed up learning.\n\n\n-----------------------------\nQ3: \u201cHow is SMiRL doing better than the oracle in Figure 3 center?\u201d\n\nA3: Figure 3 left and center show data from the same experiment that plots two different metrics, where the Oracle is optimized for minimizing deaths (left). The Oracle in our submission was trained with a reward that gives -1 when the player fails or dies (by stacking blocks to high) and 0 otherwise. In Figure 3 center, SMiRL performs better than the Oracle because the SMiRL objective, which wants to keep the board clear, aligns better with clearing rows than the Oracle\u2019s goal of minimizing deaths. We have added additional information to Section 4 to explain the reward function to which the Oracle is optimizing. In Section 5, we have added more detail on how Figure 3 left and middle are the same experiment showing 2 different metrics.\n", "title": "Clarifications and additions"}, "XssQGoHC2vD": {"type": "review", "replyto": "cPZOyoDloxl", "review": "The authors target the unsupervised reinforcement learning problem. An opposite idea from the existing approaches by maximizing state entropy is adopted to minimize state entropy. It is interesting that such an idea has achieved good performance in unstable environments. A state distribution is fitted during the interaction with an environment and the probability of the current state is used as a virtual reward. The parameters or sufficient statistics are also applied to the policy. The motivation is clear and verified. It is generally a good paper.\n\nIt is surprising that the exploration is achieved in the long term even minimizing state entropy. Is that possible the exploration events are from the 'unstable' environment? What if there are some patterns underlying the exploration events but only part of the 'unstable' environment? Is that OK to totally rely on unexpected events from the environment to explore the environment? Is that possible to add some exploration strategy in the developed model? ", "title": "The authors target the unsupervised reinforcement learning problem. An opposite idea from the existing approaches by maximizing state entropy is adopted to minimize state entropy. It is interesting that such an idea has achieved good performance in unstable environments.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}