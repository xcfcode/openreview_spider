{"paper": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "summary": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"]}, "meta": {"decision": "Reject", "comment": "The paper presents an approach to mitigate the presence of noisy labels during\ntraining by trying to forget wrong labels. Reviewers pointed out a few\nconcerns, including lack of novelty, lack of enough experimental support, and\nlack of theoretical support. Authors have added some experiments and details\nabout the experimental section, but reviewers still think it's not enough\nfor acceptance. I concur with the reviewers to reject the paper."}, "review": {"S1eMFCbt0m": {"type": "rebuttal", "replyto": "HyG1_j0cYQ", "comment": "Dear Area Chair and Anonymous Reviewers,\n\nOn behalf of all co-authors, we appreciate your great efforts in our paper review. Except our point to point response to each reviewer (see details in following posts), we hope to highlight several important points that we revised in the high level.\n\n1. To further justify our Pumpout idea empirically,  we add a text dataset called NEWS, and conduct the corresponding experiments on Pumpout_{SL} and Pumpout_{BC}. This dataset is very important to justify that our Pumpout can be leveraged not only in vision tasks but also in text tasks, and various datasets will be more convincing (AnonReviewer1 and AnonReviewer3).\n\n2. To remedy the concerns to our experiments, we add a lot of detailed explanations in Section 4. For example, 1) \"Note that, the focus of our paper is to explore the efficacy of Pumpout. Therefore, we use Adam optimizer in all experiments for fair comparison without using data augmentation trick (Zhang & Sabuncu, 2018; Ma et al., 2018).\"; 2) \"Note that, the choice of two baselines is to justify whether Pumpout can benefit representative state-of-the-art algorithms. The readers are encouraged to upgrade other methods, such as Reed et al. (2015), Goldberger & Ben-Reuven (2017), and Kiryo et al. (2017) by using Pumpout.\". We just showcase 2 points here, and more explanations have been merged into the revised paper.\n\n3. To make readers easily know our algorithms, we changed the structure of Section 3. We first introduce the background of MentorNet (Backward Correction). Then, we propose  our Pumpout_{SL} (Pumpout_{BC}). Lastly, we explain the relations between MentorNet (Backward Correction) and Pumpout_{SL} (Pumpout_{BC}).\n\nTo sum up, we try our best to revise the whole paper, and hope reviewer can feed us more suggestive comments in order to make our paper better. Many thanks for all your efforts!!!\n\nRegards,\nThe authors", "title": "Summary of Changes"}, "r1l3TQxVAQ": {"type": "rebuttal", "replyto": "SJxMUmdJnm", "comment": "We are sorry your concerns. However, the degradation of the test accuracy in the early stage of training is due to the memorization effects of deep networks. Namely, the model first learns the simple and general patterns of the real data before over-fitting and memorizing the noise (which results in decreasing test accuracy). This observation has been well demonstrated by a lot of researchers recently (e.g. [1-3]), and this observation has become a common sense. From Figure 7(b) of [2] and Figures 3, 5, 6 of [3], we can clearly see the same phenomenon of memorization effects of deep networks on noisy labels.\n\nActually, the degradation of Figure 2(c) is small. Please note that the lowest Y-axis is 0.88. The performance degradation of this case is about 0.1, which is similar to [2] and [3].\n\nBy the way, to the best of our knowledge, we are not aware of a top conference/journal paper in this area claiming that \"Standard training of neural network is very robust to label noise.\" Could you please point out a paper for our reference?\n\nReferences:\n[1] Zhang C, et al. Understanding deep learning requires rethinking generalization. In ICLR, 2017.\n[2] Arpit D, et al. A closer look at memorization in deep networks. In ICML, 2017.\n[3] Han B, et al. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NeurIPS, 2018.", "title": "My main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs?"}, "BJlAGblVRQ": {"type": "rebuttal", "replyto": "SJeMcexNRm", "comment": "Thanks for your comments. The background of MentorNet and Backward Correction has been moved before their upgraded version via Pumpout. Please check the new organization of Section~3. We have modified the Table 1 by introducing the details of two vision datasets (\\textit{MNIST} and \\textit{CIFAR-10}) and one text dataset (\\textit{NEWS}), and added the description of the activation function \"Leaky ReLu (LReLU) activation function\".", "title": "Regarding the presentation, in section 3, I suggest to move the explanation of MentorNet and BackwardCorrection before their upgrade by PumpOut."}, "ryltU0y4RX": {"type": "rebuttal", "replyto": "ByxT2oFn2X", "comment": "Thanks for your suggestion. Recently, in the area of deep learning with noisy labels, training on selected instances is a totally new direction, which attracts a lot of attention [1-3]. However, once some false-positive instances (real noisy data) are selected during training, DNNs will memorize them finally, which inevitably degrades the generalization performance (i.e., test accuracy) in the test phase. The key novelty of this paper is how to actively mitigate memorizing the negative effects of noisy labels, instead of following the path of training on selected instances.\n\nSpecifically, to address such an issue, we introduce a meta approach called Pumpout. Intuitively, it squeezes the negative effects of noise labels actively by scaled stochastic gradient ascent. We can leverage Pumpout to upgrade orthogonal methods, such as MentorNet [1] (training on selected instances) and Backward Correction [4] (estimating the noise transition matrix).\n\nTo the best of our knowledge, it is the first attempt in deep learning that studies \"how to forget memorized information in an active manner\", which is critical to improve the performance of some existing state-of-the-art methods. Nevertheless, we agree that some theoretical justification for Pumpout can be useful to understand and improve our method. We leave this in future works.\n\nReferences:\n[1] Jiang L, Zhou Z, Leung T, et al. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.\n[2] Ren M, Zeng W, Yang B, Urtasun R. Learning to reweight examples for robust deep learning. In ICML, 2018.\n[3] Han B, Yao Q, Yu X, Niu G, et al. Co-teaching: Robust training of deep Neural networks with extremely noisy labels. In NeurIPS, 2018.\n[4] Patrini G, Rozza A, Menon A, et al. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.", "title": "The idea using instance selection is not new. The novelty could be improved."}, "rJeM7llN0Q": {"type": "rebuttal", "replyto": "HkeWAkx4Am", "comment": "We agree that a well-defined minimization problem usually has a lower bounded objective. However, a sufficient optimization on such a bounded objective does not mean a better generalization on the test set, due to the gap between the true Risk Minimization (RM) and Empirical Risk Minimization (ERM). Previous work [1] provides a counterexample that a negative but lower bounded objective function can still result in terrible over-fitting issue in PU learning. We follow a similar motivation and introduce nnBC, and present an aggressive version of nnBC ($\\rm Pumpout_{BC}$) by using Pumpout.\n\nFor the standard classification problem with cross-entropy loss, the training objective should be lower bounded by zero. However, when we conduct Backward Correction on cross-entropy loss for noisy labels, the corrected cross-entropy loss may become negative, which will arise the over-fitting issue. Therefore, we should bound the objective of corrected cross-entropy loss with a $\\max$ operator (Theorem 2), which can avoid over-fitting issue. To achieve better performance, we extend Theorem 2 to Algorithm 3 ($\\rm Pumpout_{BC}$) that actively forget negative effects of noisy labels.\n\nReferences:\n[1] Kiryo R, Niu G, du Plessis M C, et al. Positive-unlabeled learning with non-negative risk estimator. In NeurIPS, 2017.", "title": "The non-negative version of the Backward Correction, and its appeal to Kiryo et al 17 is interesting, but it sidesteps its justification. A loss that can be negative does not necessarily means that it not lower-bounded."}, "BJlSZVx4Am": {"type": "rebuttal", "replyto": "r1l3TQxVAQ", "comment": "Thanks for the comments. In this paper, we introduce a meta algorithm to actively squeeze out the negative effects of noisy labels from training model, instead of passively forgetting these effects. We consider how our meta algorithm can simultaneously benefit different orthogonal algorithms. Thus, in experimental section, we evaluate the efficacy of our meta algorithm Pumpout by applying it to the representative method of training on selected instances (MentorNet [1]), and the representative method of estimating the noise transition matrix (Backward Correction [2]). Note that, the choice of two baselines is to justify whether Pumpout can benefit representative state-of-the-art algorithms. The readers are encouraged to upgrade other methods by using Pumpout.\n\nReferences:\n[1] Jiang L, Zhou Z, Leung T, et al. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.\n[2] Patrini G, Rozza A, Menon A, et al. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.", "title": "At the beginning of the experiment section you mentioned several algorithms for training with noisy labels. I expect to compare your results to at least one of them."}, "ByxhtWe4CX": {"type": "rebuttal", "replyto": "BJlAGblVRQ", "comment": "Thanks for your comments. For SET2, we have added experimental results on another vision dataset \\textit{CIFAR-10} and a text dataset \\textit{NEWS}. Due to the limited time, we only focus on these standard benchmark datasets at current stage, and we will remain the evaluation on \\textit{Open Images} dataset or \\textit{Clothing1M} dataset as a future work. For $\\beta$, we directly set it to zero and $\\gamma$ is chosen among $\\{0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1\\}$ via a validation set.", "title": "SET2 is only run on MNIST. Why not even on CIFAR10 which is used in SET1? Any future reader will wonder \"did it work on CIFAR10\"?"}, "SJeMcexNRm": {"type": "rebuttal", "replyto": "rJeM7llN0Q", "comment": "Thanks for your comments. We have modified the statement of Theorem 2. Non-negative version of BC is closely related to BC and $\\rm Pumpout_{BC}$ (Algorithm 3). The corrected loss by Backward Correction can be negative, which yields over-fitting issue. Non-negative version of BC is a strategy to overcome the over-fitting issue caused by the negative loss. However, the non-negative version of BC is passive, since max operator means stopping gradients computation on negative-risk instances. $\\rm Pumpout_{BC}$ is an aggressive version of non-negative BC. $\\rm Pumpout_{BC}$ conducts not only stochastic gradient descent on non-negative-risk instances (when $ \\mathbf{1}^{\\top}\\mathbf{T}^{-1}\\ell(\\mathbf{x}, y; w_f)\\geq0$), but also scaled stochastic gradient ascent on negative-risk instances (when $\\mathbf{1}^{\\top}\\mathbf{T}^{-1}\\ell(\\mathbf{x}, y; w_f)\\leq0$). As non-negative BC is highly related to BC and our $\\rm Pumpout_{BC}$, we present it on the text and use it as a baseline. Note that, in Algorithm 3, we set a tuning \"safety valve\" $\\beta$, and use the fitting condition as $ \\mathbf{1}^{\\top}\\mathbf{T}^{-1}\\ell(\\mathbf{x}, y; w_f)\\geq \\beta$. However, in all experiments, we directly set $\\beta = 0$.", "title": "The statement of Theorem 2 is trivial. In fact, no proof is given as it would be self-evident. Moreover, the Theorem is not used by PumpOut. Algorithm 3 uses a if-else conditional on the scale of the backward correction, without the max(0, .)."}, "HkeWAkx4Am": {"type": "rebuttal", "replyto": "Bkgn5kxN0Q", "comment": "We are sorry for such confusing. In the high level, sample selection bias means that we assign different weights to different samples, and such bias will prioritize some of samples during training. Thus, both MentorNet and $\\rm Pumpout_{SL}$ (MentorNet + Pumpout) have sample selection bias. We leverage sample selection bias to overcome the label noise issue.", "title": "Why MentorNet + Pumpout does not suffer from the selection bias effect of CoTraining? This is unclear to me."}, "Bkgn5kxN0Q": {"type": "rebuttal", "replyto": "rJlfPNcx2X", "comment": "We conduct the experiments on several benchmark datasets, including vision and text. We empirically find that the best performance is usually chosen when $\\gamma$ is in-between 0 and 1 by using the validation set.", "title": "Section 2: the scaling factor $\\gamma$. Why using $\\gamma$=1 is suboptimal?"}, "H1l5aC1E0Q": {"type": "rebuttal", "replyto": "ryltU0y4RX", "comment": "Thanks for your suggestion. We have added one text dataset called NEWS in updated experiments. As can be seen in Figure 4 and Figure 7, obvious improvements (similar to other figures) are achieved compared to baseline approaches, which confirms the benefits of Pumpout. Namely, Pumpout indeed can overcome the issue of memorizing noisy labels.", "title": "Experiments are too standard. More divers and various data sets would be more convincing."}, "ByxT2oFn2X": {"type": "review", "replyto": "HyG1_j0cYQ", "review": "This paper presents a meta algorithm to improve the robustness of learning methods under noisy labels. The idea is to squeeze out the negative effects of noisy labels actively. The paper trains deep neural networks by stochastic gradient descent on \u201cfitting\u201d labels; while trains deep neural networks by scaled stochastic gradient ascent on \u201cnot-fitting\u201d labels. Experimental results show the improvement on robustness. \n\nThe good things of the paper are clear. \n1.\tTechnical sound with reasonable idea\n2.\tProblem is well motivated\n3.\tPaper is general well written.\n\nSome comments\n1.\tThe idea using instance selection is not new. The novelty could be improved. If the paper could make more insight from either theoretical or application value, would be more interesting.\n2.\tExperiments are too standard. More divers and various data sets would be more convincing. \n", "title": "This paper presents a meta algorithm to improve the robustness of learning methods under noisy labels.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJlfPNcx2X": {"type": "review", "replyto": "HyG1_j0cYQ", "review": "A new method for defending against label noise during training of deep neural networks is presented. The main idea is to \u201cforget\u201d about wrongly labeled examples during training by an ascending step in the gradient direction. This is a meta-approach that can be combined with other methods for noise robustness; the detection of the noisy examples is specific of the base method utilised. Experimental results are promising.\n\nIn general, I find the idea original, and of potential practical use, but I believe the contribution of paper to be limited and not well supported by experiments. Moreover, I think that some claims made in this work are poorly justified.\n\n== Method\n\nThe paper would greatly benefit from some theoretical backing of the proposed optimization scheme, even on a simplified scenario. An idea would be to prove that, given a dataset with noisy labels, PumpOut converges close to the best model (= the one learned without noise), for certain hyperparameters. I think this would be new and interesting. A result of similar fashion was proven in [A].\n\nI found the following arguments not well or only heuristically supported:\n* section 2: the scaling factor \\gamma. Why using \\gamma=1 is suboptimal? One could claim that as much as you want to memorize the true image-label patterns, you also want to forget the image-noise ones.\n* Why MentorNet + PumpOut does not suffer from the selection bias effect of CoTraining? This is unclear to me\n* The non-negative version of the BackwardCorrection, and its appeal to Kiryo et al 17 is interesting, but it sidesteps its justification. A loss that can be negative does not necessarily means that it not lower-bounded. In fact, for a minimization problem to be well-defined, all you need is a lower bounded objective. Then, adding the lower bound makes your loss non-negative. Notice that Patrini et al 17 did not state that BC is unbounded, but only that it can be negative. Can you show more that that -- maybe, at least experimentally?\n\nThe statement of Theorem 2 is trivial. In fact, no proof is given as it would be self-evident. Moreover, the Theorem is not used by PumpOut. Algorithm 3 uses a if-else conditional on the scale of the backward correction, without the max(0, .). I suggest to remove this part. I have notice later that the non-negative version of BC is used as a baseline in the experiment, but I think that is the only use.\n\nRegarding the presentation, in section 3, I suggest to move the explanation of MentorNet and BackwardCorrection before their upgrade by PumpOut.\n\n== Experiments\n\nTable 1 can be removed as these are extremely common datasets.\n\nThe experimental results look very promising for applications. As a side effect of this analysis, I can also notice an improvement over BC given to the nnBC, which is nice per se. Although, I would have strengthen the empirics as follow.\n* SET2 is only run on MNIST. Why not even on CIFAR10 which is used in SET1? Any future reader will wonder \u201cdid it work on CIFAR10?\"\n* A much harder instance of noise, for instance open set [B] or from a real dataset [Xiao et al 15] would have more clearly supported the use of PumpOut for real applications.\n* Can the authors elaborate on \u201cthe choices of \\beta and \\gamma follows Kirkyo et al 2017\u201d ? And how assuming their knowledge gives a fair comparison to BC which does not require them? I believe this is a critical point for the validity of the experiments.\n\nMinor:\n* \u201cLRELU active function\u2019 -> activation function. What is a LRELU? LeakyReLU?\n\n[A] Malach, Eran, and Shai Shalev-Shwartz. \"Decoupling\" when to update\" from\" how to update\".\" Advances in Neural Information Processing Systems. 2017.\n[B] Veit, Andreas, et al. \"Learning From Noisy Large-Scale Datasets With Minimal Supervision.\" CVPR. 2017.", "title": "Original idea with promising experimental results, but a limited contribution", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJxMUmdJnm": {"type": "review", "replyto": "HyG1_j0cYQ", "review": "The paper proposes a meta algorithm to train a network with noisy labels.\nIt is not a general algorithm but a simple modification of two proposed methods.  It is presented as a heuristics and it  would be helpful to derive a theoretical framework or motivation for the proposed algorithm. \n\nMy main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs? Standard training of neural network is very robust to label noise.  In case of  20% symmetric error  (figure 2c)  the performance degradation using standard training should be very small. \nHence it is difficult to evaluate to performance of the proposed method.\nAt the beginning of the experiment section you mentioned several algorithms   for training with noisy labels.  I expect to compare your results to at least one of them. \n    \n", "title": "Non convincing experiments   ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJeNFlCVcQ": {"type": "rebuttal", "replyto": "S1luH11e5X", "comment": "Sorry, we should make our point clearer about how to estimate the noise level in practice. It's true the noise level can be estimated from clean validation data. However, note that clean data require domain experts to label, and thus a small set of clean data can be as costly as a huge set of noisy data. On the other hand, we could give a representative sub-sampling of our noisy data and directly ask the domain expert to estimate the noise level. In this way, we won't obtain a small set of clean data from the domain expert. As a result, we could pay much less to only obtain some key parameters in the underlying data generation/corruption process. This is the scenario of the current paper. Therefore, the experimental comparisons are fair.", "title": "The experimental comparisons are fair"}, "BkefgvT4qm": {"type": "rebuttal", "replyto": "Ske2_4yxcm", "comment": "Both [2] and [3] are great papers, we will definitely cite them in the suitable place later. When our experiments involve the \\epsilon estimation, we will compare them and conduct the analysis.", "title": "Both [2] and [3] are great papers"}, "S1lJiNpNcQ": {"type": "rebuttal", "replyto": "HkeJt51eqX", "comment": "The main reason is that training strategies are very different. All above works [1-4] use SGD with a momentum of 0.9, weight decay of 0.0001, and data augmentation. Note that, data augmentation has been widely used in computer vision community, which significantly improves the classification performance of a deep learning model. \n\nHowever,  in our machine learning paper, our focus is to explore the efficacy of Pumpout. Therefore, we use Adam optimizer in all experiments for fair comparison without using data augmentation trick. \n\nFor Figure 3 (c), we have also tested the performance of \"Normal\" baseline using ResNet32 model, and the training strategy follows above works [1-4] using data augmentation. We achieved a similar test accuracy of 81%.", "title": "The reason comes from Data Augmentation"}, "H1eGMrT49m": {"type": "rebuttal", "replyto": "S1luH11e5X", "comment": "The differences between Pumpout and Co-teaching [1] is obvious and significant. The idea of Co-teaching [1] is to train two deep neural networks, and each network samples small-loss instances to update the parameters of its peer network. \n\nHowever, Pumpout is a meta approach, which aims to benefit orthogonal algorithms in deep learning with noisy labels (i.e., MentorNet, Co-teaching, Backward Correction etc.). The idea of Pumpout is to actively squeeze out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. Specifically, Pumpout conducts stochastic gradient descent on \u201cfitting\u201d labels; and conducts scaled stochastic gradient ascent on \u2018not-fitting\u2019 labels instead of stopping gradient computation. The \u201cfitting\u201d labels are not limted to be the selected clean labels. \n\nTo verify the efficacy of Pumpout, we leverage Pumpout to upgrade two representative but orthogonal approaches: MentorNet and Backward Correction. Note that, Co-teaching shares the similar direction with MentorNet. Thus, we do not need to compare it with [1] here. The potential comparison in future should between \u201cCo-teaching\u201d and \u201cCo-teaching + Pumpout\u201d. \n\nMoreover, we will add citation to Figure 1 in the updated version. For [2] and [3], we will cite them in our updated version, when our experiments involve the \\epsilon estimation.", "title": "Pumpout and Co-teaching are totally different"}, "Sklly4ZkqQ": {"type": "rebuttal", "replyto": "S1lK0T3stQ", "comment": "Yes, your understanding is correct. The idea of our Pumpout is to use scaled stochastic gradient ascent to actively squeezes out the negative effects of noisy labels from the training model. The realization is simple and general, which will benefit orthogonal techniques in the area of deep learning with noisy labels.", "title": "Your understanding is correct"}}}