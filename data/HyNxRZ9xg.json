{"paper": {"title": "Cat2Vec: Learning Distributed Representation of Multi-field Categorical Data", "authors": ["Ying Wen", "Jun Wang", "Tianyao Chen", "Weinan Zhang"], "authorids": ["ying.wen@cs.ucl.ac.uk", "jun.wang@cs.ucl.ac.uk", "tychen@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn"], "summary": "an unsupervised pairwise interaction model to learning the distributed representation of multi-field categorical data", "abstract": "This paper presents a method of learning distributed representation for multi-field categorical data, which is a common data format with various applications such as recommender systems, social link prediction, and computational advertising. The success of non-linear models, e.g., factorisation machines, boosted trees, has proved the potential of exploring the interactions among inter-field categories. Inspired by Word2Vec, the distributed representation for natural language, we propose Cat2Vec (categories to vectors) model. In Cat2Vec, a low-dimensional continuous vector is automatically learned for each category in each field. The interactions among inter-field categories are further explored by different neural gates and the most informative ones are selected by pooling layers. In our experiments, with the exploration of the interactions between pairwise categories over layers, the model attains great improvement over state-of-the-art models in a supervised learning task, e.g., click prediction, while capturing the most significant interactions from the data. ", "keywords": ["Unsupervised Learning", "Deep learning", "Applications"]}, "meta": {"decision": "Reject", "comment": "There is consensus among the three reviewers that (1) the originality of the proposed approach is limited and (2) the experimental evaluation is too limited in that it lacks strong baseline models as well as an ablation study that explores the different aspects of the proposed model."}, "review": {"Bkv34lQXl": {"type": "rebuttal", "replyto": "ByklrMyQg", "comment": "Q1: Yes, after the first round to enumerate the pair of the category, take K-best combinations, and get the third one by enumerating the combination of the K-best pair combinations to get the K-best triple combinations. For example, in the first round, we get 4-best pairs (ab, ac, db, cd), then we can explore the interaction between each pair to obtain the triple combinations e.g. abc, abd etc. (we also tried in Section 5.2 to using K-max pooling results and category embeddings to explore high-order interactions) \n \nQ2: The covariance matrix are randomly generated at each time. In our synthetic experiments, the covariance matrix is a 4x4 matrix, the values in the matrix are between 0-10, and sampled by the truncated multivariate normal distribution. \n \nQ3:  \nAll the models used same training(with positive:negative=1:10 downsampling) and testing dataset. And Nadam is chosen as the SGD optimiser. \nLR:{batch_size = 512, dropout = 0.2} \nFM:{batch_size = 256, embedding_size = 8, dropout = 0.1} \nCCPM:{batch_size = 4096, embedding_size = 8, dropout = 0.0, nb_filter = 256, filter_length = 3, activation = 'relu'} \nFNN: {batch_size = 64, embedding_size = 16, dropout = 0.1,  dnn_layers= [128, 32], activation = 'sigmoid' } \nWe can add these details in the revised version.", "title": "Response for the questions"}, "rJSfNxQQe": {"type": "rebuttal", "replyto": "BkBICm1Ql", "comment": "Q1&Q2: Honestly, we did not know the existence of the models mentioned above. The models in that paper and its related work focus on the  image annotation task, which aims to find the right pair between the image and the annotation. These model may have some commons with our model, in essence, they are totally different.  We can take the WSABIE model as an example.  \n \nDetailed comparisons are given as below. \n \nCommons: \n1. Embed the inputs to low-dimension dense vector. \n2. Evaluate the 'influence' of a pair of embedding vectors by magnitude(normal) of the result of the f(x_1, x_2), where x_1 and x_2 are two input vectors. \n \nDifferences: \n1. Our Cat2Vec model aims to automatically extract salient discrete feature interactions and estimate the category vectors, which can be adopted in various tasks. But the WSABIE is only used for the image annotation task. \n2. Our model takes multi-filed categorical data as input. In image annotation task, technically, it only has two fields of data as input, i.e. image and annotation. Besides, only the annotation can be taken as categorical data, the image is continuous data. \n3. We proposed an unsupervised training method for our model, and tested it on the synthetic and real-world data; Besides, we also have the experiments showing that our Cat2Vec model can attain great performance improvement in supervised learning tasks. While the WSABIE is focused on the supervised learning task. \n4. Our model would consider the inter-field feature interaction by the gate function f(x_1, x_2), and the gate function can be variously implemented. The gate functions output the interaction vector of input vectors . In WSABIE's case, the model only uses dot product between two embedding vectors to get a real value result for ranking. \n5. Our model has the K-Max pooling operation for interaction result selection. In WSABIE, a WARP loss is used for optimising precision at k. \n \nConsequently, we do not think we have to take experimental comparisons between these models. ", "title": "Response for the questions"}, "BkBICm1Ql": {"type": "review", "replyto": "HyNxRZ9xg", "review": "Q1. I was wondering if the authors are aware of very similar work/model called wsabie (http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf) and other works which are quite similar to the one proposed in the paper? \n\nQ2. Following the above question, in the experimental section there is no comparison to such models. How is your model different to the ideas already proposed in the literature? \n\nThe paper proposes a way to learn continuous features for input data which consists of multiple categorical data. The idea is to embed each category in a learnable low dimensional continuous space, explicitly compute the pair-wise interaction among different categories in a given input sample (which is achieved by either taking a component-wise dot product or component-wise addition), perform k-max pooling to select a subset of the most informative interactions, and repeat the process some number of times, until you get the final feature vector of the given input. This feature vector is then used as input to a classifier/regressor to accomplish the final task. The embeddings of the categories are learnt in the usual way. In the experiment section, the authors show on a synthetic dataset that their procedure is indeed able to select the relevant interactions in the data. On one real world dataset (iPinYou) the model seems to outperform a couple of simple baselines. \n\nMy major concern with this paper is that their's nothing new in it. The idea of embedding the categorical data having mixed categories has already been handled in the past literature, where essentially one learns a separate lookup table for each class of categories: an input is represented by concatenation of the embeddings from these lookup table, and a non-linear function (a deep network) is plugged on top to get the features of the input. The only rather marginal contribution is the explicit modeling of the interactions among categories in equations 2/3/4/5. Other than that there's nothing else in the paper. \n\nNot only that, I feel that these interactions can (and should) automatically be learned by plugging in a deep convolutional network on top of the embeddings of the input. So I'm not sure how useful the contribution is. \n\nThe experimental section is rather weak. They authors test their method on a single real world data set against a couple of rather weak baselines. I would have much preferred for them to evaluate against numerous models proposed in the literature which handle similar problems, including wsabie. \n\nWhile the authors argued in their response that wsabie was not suited for their problem, i strongly disagree with that claim. While the original wsabie paper showed experiments using images as inputs, their training methodology can easily be extended to other types of data sets, including categorical data. For instance, I conjecture that the model i proposed above (embed all the categorical inputs, concatenate the embeddings, plug a deep conv-net on top and train using some margin loss) will perform as well if not better than the hand coded interaction model proposed in this paper. Of course I could be wrong, but it would be far more convincing if their model was tested against such baselines. ", "title": "Missing comparison to prior work", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SyJMcUrNg": {"type": "review", "replyto": "HyNxRZ9xg", "review": "Q1. I was wondering if the authors are aware of very similar work/model called wsabie (http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf) and other works which are quite similar to the one proposed in the paper? \n\nQ2. Following the above question, in the experimental section there is no comparison to such models. How is your model different to the ideas already proposed in the literature? \n\nThe paper proposes a way to learn continuous features for input data which consists of multiple categorical data. The idea is to embed each category in a learnable low dimensional continuous space, explicitly compute the pair-wise interaction among different categories in a given input sample (which is achieved by either taking a component-wise dot product or component-wise addition), perform k-max pooling to select a subset of the most informative interactions, and repeat the process some number of times, until you get the final feature vector of the given input. This feature vector is then used as input to a classifier/regressor to accomplish the final task. The embeddings of the categories are learnt in the usual way. In the experiment section, the authors show on a synthetic dataset that their procedure is indeed able to select the relevant interactions in the data. On one real world dataset (iPinYou) the model seems to outperform a couple of simple baselines. \n\nMy major concern with this paper is that their's nothing new in it. The idea of embedding the categorical data having mixed categories has already been handled in the past literature, where essentially one learns a separate lookup table for each class of categories: an input is represented by concatenation of the embeddings from these lookup table, and a non-linear function (a deep network) is plugged on top to get the features of the input. The only rather marginal contribution is the explicit modeling of the interactions among categories in equations 2/3/4/5. Other than that there's nothing else in the paper. \n\nNot only that, I feel that these interactions can (and should) automatically be learned by plugging in a deep convolutional network on top of the embeddings of the input. So I'm not sure how useful the contribution is. \n\nThe experimental section is rather weak. They authors test their method on a single real world data set against a couple of rather weak baselines. I would have much preferred for them to evaluate against numerous models proposed in the literature which handle similar problems, including wsabie. \n\nWhile the authors argued in their response that wsabie was not suited for their problem, i strongly disagree with that claim. While the original wsabie paper showed experiments using images as inputs, their training methodology can easily be extended to other types of data sets, including categorical data. For instance, I conjecture that the model i proposed above (embed all the categorical inputs, concatenate the embeddings, plug a deep conv-net on top and train using some margin loss) will perform as well if not better than the hand coded interaction model proposed in this paper. Of course I could be wrong, but it would be far more convincing if their model was tested against such baselines. ", "title": "Missing comparison to prior work", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ByklrMyQg": {"type": "review", "replyto": "HyNxRZ9xg", "review": "In Fig. 1, all possible category pairs are enumerated (Eqn. 5) and and K-best are picked. The experiment also mentioned triple-recovery but did not mention the details. Did you also enumerate all possible category triples and pick the K-best one (which might be slow), or first enumerate a pair of category, take K-best combination, and combine the third one in the second stage? \n\nAlso, in the synthetic experiments, what covariance matrix do you use to generate data? I understand that if you use binary covariance matrix, then the most frequent category pairs/triples can be easily inferred. But I am not sure whether this is the case. \n\nIn Section 5.2, a few approaches are compared. Could you also list the number of parameters used in each models? In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? \n\nThere are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling, https://arxiv.org/abs/1511.06062 has been proposed a year ago and yields state-of-the-art performances in Visual Question Answering https://arxiv.org/abs/1606.01847. So the author might need to compare against those methods. I understand that the proposed approach incorporates more nonlinear operations (rather than bilinear) in pairwise combination, but it is not clear whether bilinear operations is sufficient to achieve the same level of performance, and whether complicated operations (e.g., Eqn. 4) are needed.\n\nIn the experiment, the performance seems to be not as impressive. There is about 1%-2% difference in performance between the proposed approach and baselines (e.g., in Tbl. 2, and Tbl. 3). Is that a big deal for click-rate prediction? When comparing among LR, FFM, CCPM, FNN, and proposed approach, the number of parameters (i.e., model capacity) are not shown. This could be unfair since the proposed model could have more parameters (note that the authors seems to misunderstand the questions). Besides, claiming that previous approaches does not learn representations seems to be a bit restrictive, since typical deep models learn the representation implicitly (e.g., CCPM and FNN listed in the paper as baselines). ", "title": "Some questions.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bkh10nWEl": {"type": "review", "replyto": "HyNxRZ9xg", "review": "In Fig. 1, all possible category pairs are enumerated (Eqn. 5) and and K-best are picked. The experiment also mentioned triple-recovery but did not mention the details. Did you also enumerate all possible category triples and pick the K-best one (which might be slow), or first enumerate a pair of category, take K-best combination, and combine the third one in the second stage? \n\nAlso, in the synthetic experiments, what covariance matrix do you use to generate data? I understand that if you use binary covariance matrix, then the most frequent category pairs/triples can be easily inferred. But I am not sure whether this is the case. \n\nIn Section 5.2, a few approaches are compared. Could you also list the number of parameters used in each models? In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? \n\nThere are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling, https://arxiv.org/abs/1511.06062 has been proposed a year ago and yields state-of-the-art performances in Visual Question Answering https://arxiv.org/abs/1606.01847. So the author might need to compare against those methods. I understand that the proposed approach incorporates more nonlinear operations (rather than bilinear) in pairwise combination, but it is not clear whether bilinear operations is sufficient to achieve the same level of performance, and whether complicated operations (e.g., Eqn. 4) are needed.\n\nIn the experiment, the performance seems to be not as impressive. There is about 1%-2% difference in performance between the proposed approach and baselines (e.g., in Tbl. 2, and Tbl. 3). Is that a big deal for click-rate prediction? When comparing among LR, FFM, CCPM, FNN, and proposed approach, the number of parameters (i.e., model capacity) are not shown. This could be unfair since the proposed model could have more parameters (note that the authors seems to misunderstand the questions). Besides, claiming that previous approaches does not learn representations seems to be a bit restrictive, since typical deep models learn the representation implicitly (e.g., CCPM and FNN listed in the paper as baselines). ", "title": "Some questions.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}