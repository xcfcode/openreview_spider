{"paper": {"title": "Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks", "authors": ["David Balduzzi", "Brian McWilliams", "Tony Butler-Yeoman"], "authorids": ["david.balduzzi@vuw.ac.nz", "brian@disneyresearch.com", "butlertony@ecs.vuw.ac.nz"], "summary": "We provide the first convergence result for rectifier neural networks and investigate implications for exploration in shattered landscapes.", "abstract": "Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets. The guarantee matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation -- a straightforward application of Taylor expansions to neural networks -- and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization.\n\nThe second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets: that gradients are shattered. We investigate the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions.", "keywords": ["Deep learning", "Optimization", "Theory", "Supervised Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The paper presents a theoretical analysis of the convergence of the training error. The presented result is rather general and can potentially apply to many neural networks. \n \n Reviewers pointed out several important concerns regarding the mathematical rigor and precision of the claims. The authors' response partially addressed concerns about the scope of the claims and unclear arguments of the proofs. \n \n We invite the authors to submit a revision of the paper, where all statements and proofs are mathematically clear, to the workshop track."}, "review": {"BJaxt8dNg": {"type": "rebuttal", "replyto": "SJivDoD4l", "comment": "We thank the reviewer for their quick, detailed and informed response. We sincerely appreciate the criticism and the reviewer\u2019s openness to discussion.\n\nFirst, there is a misunderstanding. The previous response did not claim the Taylor optima converge -- only that they are bounded.\n\n\nResetting expectations\n-----------------------\nRectifier networks are complicated. Some of the reviewers expectations about what can and cannot be proved are unrealistic.\n\n1. AR3 remarks \u201cif the running average is bounded by a \u2018meaningful\u2019 constant (e.g. the optimum of the true loss)\u201d.  The reviewer is overly optimistic. RECTIFIER NEURAL NETWORKS ARE NOT CONVEX. It is unrealistic to hope for bounds relative to the global optimum. Bad local optima exist and NNs can and do converge to them (fortunately this doesn\u2019t happen often -- explaining why is an active area of research beyond the scope of the paper).\n\n2. AR3 continues in the same sentence \u201cgood performance of the algorithms could be possible, under further assumptions like smooth loss function\u201d. The reviewer has fundamentally missed the point of the paper. RECTIFIER NEURAL NETWORKS ARE NOT SMOOTH. Nevertheless, they are the dominant feedforward NN architecture, e.g. winning the ImageNet classification challenge every year since 2012. If we as a community are interested in theoretical guarantees at all then we should not only prove strong theorems about toy algorithms but should also prove (potentially weaker) theorems about the algorithms that are actually used in practice.\n\nTo the best of our knowledge there are no convergence results for rectifier neural networks in the literature. The problem is difficult and unlikely to be definitively solved in an 8-page+appendix conference paper. It is therefore to be expected that, despite making substantial progress, the paper also leaves important questions open.\n\n\nMeaningfulness of the Taylor optimum\n---------------------------------------\nAR3\u2019s core criticism is that the RHS of the bound is not meaningful. It underlies points 2+3 of \u201cconvergence to the Taylor optimum\u201d and points 2+3 of \u201cregret for the running average\u201d.  \n\nTo illustrate the point that just having an upper bound is not good enough, AR3 writes \u201cConsider a training error sequence: 0, 1, 0, 1 \u2026 . It is running average is bounded and converges to 0. But in practice, it won\u2019t be treated as a good behaviour.\u201d Further, \u201c[if] the loss function [were] bounded [then] an upper bound on the running average training error is straightforward. \u201c\n\nWe agree. Having just any upper bound is not good enough. It would be nice to prove no-regret relative to the global optimum but this is impossible since we know neural networks don\u2019t necessarily converge to the global optimum. The question then is whether the Taylor optima are meaningful bounds or vacuous. \n\nThe Taylor optimum is the optimal solution to a convex problem. It is not the optimal solution to an arbitrary convex problem or to a problem designed to provide a \"safe\" upper bound. It is the optimal solution to the Taylor losses, which are the best convex approximations to the actual losses; best in the sense that they have the same value and have the same gradient for the encountered weights. The Taylor optimum is not pessimistic and it is not arbitrary. \n\nThe paper bounds the losses arising when optimizing a nonsmooth nonconvex optimization objective with the optimal solution to convex problems that arise naturally during gradient descent. We believe this is significant progress: we have replaced a seemingly intractable problem with a sequence of convex problems. \n\nProving convergence of the Taylor optima is an important loose end. However, it\u2019s worth recalling that the paper extensively investigates the empirical behavior of the Taylor optima and regret, showing that the Taylor optimum is a \u201ctough target\u201d in practice and that the regret behaves as predicted by the theory for: MNIST and CIFAR10; supervised and unsupervised learning; on a variety of optimizers (Adam, SGD, RMSProp). The experiments looked at the behavior of both individual neurons and entire layers. \n\n\nAdam\n------\nThe main theorem of the Adam paper assumes a series of convex losses provided by an adversary (adagrad assumes the same), see second sentence of section 4 and theorem 4.1 itself in version 8 on arxiv. The theorem does not assume the functions f_t are realizations of the same function f, even if this was the intended application.\n", "title": "Second response to AR3"}, "Bknnet84e": {"type": "rebuttal", "replyto": "ByUX0ABNl", "comment": "We thank the reviewer for their detailed comments.  \n\nFirst, we emphasize that Theorem 2 is correct as stated. The formulas are correct as is. However, the underbrace on the LHS of Eq. (3) has been relabeled \u201crunning average of training errors\u201d instead of \u201ctraining error\u201d. \n\nThe reviewer\u2019s points concern the ASYMPTOTIC BEHAVIOR of the left and right hand sides of Eq. (3). The paper analyses the behavior of gradient descent under a FINITE NUMBER OF ITERATIONS. The main contribution is to show: \n    a) the first convergence rate for rectifier nets\n    b) relative to a sequence of convex losses capturing the geometry of backprop\n    c) that is meaningful in practice (i.e for finite iterations)\nAlthough asymptotic results are valuable, in practice NNs are trained using a finite number of iterations.\n\nResponses to comments:\n\n1. Convergence of Taylor optimum.\nThe reviewer is correct. We showed ||a_n-a_{n+1}|| approaches 0 instead of ||a_n-a_m|| for arbitrary m > n. This was a sloppy mistake made in a rush in response to the reviewer\u2019s initial question. Fortunately, all that we require is that the Taylor optima are bounded so we have removed the discussion of Taylor optima from the appendix.\n\n2. LHS is not equivalent to training error.\nThe reviewer is correct. The LHS is better termed the \u201crunning average of errors during training\u201d or the \u201c(average) cumulative loss\u201d: it averages the entire loss curve during training rather than the final point on the curve (i.e. the training error of the final weights). \n\nAnalysing the cumulative loss is standard. The reviewer\u2019s criticism applies equally to, for example, the theorems provided for AdaGrad by Duchi et al and for Adam by Kingma and Ba in the original papers introducing the respective algorithms. \n\nOur results have a different flavor from results on stochastic gradient descent where convergence is shown in expectation. Eq. (3) is much stronger than a bound that holds in expectation since it always holds as is. It is for this reason that we (and Duchi and Kingma) use what we are calling the running average of errors. \n", "title": "Response to AR3"}, "BkAme3S4e": {"type": "rebuttal", "replyto": "r1mUvnfVe", "comment": "We thank the reviewer for their comments.\n\n1. While SGD typically shows improved performance when used with annealed learning rates and momentum, it is unclear how any improvements due to these factors would be due to increased exploration. The comparison between RMS and SGD shows that it is the adaptivity and not the learning rate which encourages exploration which is made clear in observation 3 of the paper.\n\n2. Generalization. \nWe make two remarks. Firstly, despite its importance, generalization performance is beyond the scope of the paper. The main contribution is a fundamental result about optimizing neural networks. Rectifier nets are the most commonly used feedforward architecture and yet prior to our work there were no convergence guarantees available. \n\nSecondly, there has been exciting recent work connecting the convergence rate of SGD to generalization performance, see: Hardt, Recht, Singer \u201cTrain faster, generalize better: Stability of stochastic gradient descent\u201d in ICML 2016. Unfortunately, they require smoothness so their result do not apply to rectifier nets. An possible future direction is to investigate generalization in rectifier networks by applying the methods in Hardt et al to the Taylor losses (which are smooth whenever the loss is a smooth function of the network\u2019s output -- i.e. in almost all cases).\n\nMore generally, the Taylor losses provide a bridge between rectifier nets and the vast literature on smooth and convex problems.\n\n3. Why does the Jacobian change to a_l in the <Gl,V> definition?\nThis was a typo, now corrected. Thanks.", "title": "response to AR1"}, "HkIKCJ37l": {"type": "rebuttal", "replyto": "rkcUQd1Xg", "comment": "Good question.We\u2019ve added Section A.3 to the appendix explaining convergence of the Taylor optima (under a very weak assumption)\u00a0in detail. In short, we show that the Taylor optima form a Cauchy sequence, and therefore must converge.\n", "title": "Convergence of Taylor optimum"}, "rkcUQd1Xg": {"type": "review", "replyto": "HyWG0H5ge", "review": "Hi,\n\nIn the paper, it is pointed out that the Taylor optimum is path-dependent. Theoretically, is it possible that the Taylor optimum does not converge at all? In the experiment, it seems not the case, but do we have theoretical guarantee?This paper develops a theoretical guarantee for the convergence of the training error. The result is quite general that covers the training of a wide range of neural network models. The key idea of the paper is approximate the training loss by its linear approximation. Since its linearity in the variables (thus convex), the authors plug in results that has been developed in the literature of online learning. \n\nThis paper has good novelty in using the Taylor approximation thus greatly simplifying the analysis of the behaviour of the model. However, there are two problems about the main result of this paper, Theorem 2.\n\n1. It is not clear if the Taylor optimum would converge or not. \nAs noticed by the authors, the upper bound is path dependent. Appendix 3 tries to claim that this Taylor optimum indeed converges, but the proof is buggy. In the proof of Lemma 2, it is proved that the difference between two sequential Taylor optimum is approaching 0. Note that this is actually weaker than being Cauchy sequence and insufficient to guarantee convergence.\n\n2. The lefthand side of Equation (3) (I will denote it by L3 in this review) is not equivalent to training error. \nAn upper bound on this average error is not sufficient to guarantee the convergence of the training error neither. Take the gradient descent for example (thus each minibatch x_0^n is the whole training set), the convergence of the training error should be lim_{n -> \\infty} l(f_{w^n}(x_0^n), y^n). The convergence of L3 is necessary but not sufficient to imply the convergence of the training error.\n\nAnother concern about Theorem 2 (but it is minor compared to the two problems mentioned above) is that to achieve the O(1/\\sqrt{n}) rate, the algorithm has to pick a particular learning rate. Larger or smaller learning rate (in the order of n) will lead to significantly worse regret. But in the experiments of the paper, the learning rates are not picked according to the theorem.\n\nOverall, this paper has a good motivation and good novelty. It could be further developed into a good paper. But due to the two problems and a buggy proof mentioned above, I think it is not ready for publish yet.\n", "title": "The convergence of the Taylor optimum", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByUX0ABNl": {"type": "review", "replyto": "HyWG0H5ge", "review": "Hi,\n\nIn the paper, it is pointed out that the Taylor optimum is path-dependent. Theoretically, is it possible that the Taylor optimum does not converge at all? In the experiment, it seems not the case, but do we have theoretical guarantee?This paper develops a theoretical guarantee for the convergence of the training error. The result is quite general that covers the training of a wide range of neural network models. The key idea of the paper is approximate the training loss by its linear approximation. Since its linearity in the variables (thus convex), the authors plug in results that has been developed in the literature of online learning. \n\nThis paper has good novelty in using the Taylor approximation thus greatly simplifying the analysis of the behaviour of the model. However, there are two problems about the main result of this paper, Theorem 2.\n\n1. It is not clear if the Taylor optimum would converge or not. \nAs noticed by the authors, the upper bound is path dependent. Appendix 3 tries to claim that this Taylor optimum indeed converges, but the proof is buggy. In the proof of Lemma 2, it is proved that the difference between two sequential Taylor optimum is approaching 0. Note that this is actually weaker than being Cauchy sequence and insufficient to guarantee convergence.\n\n2. The lefthand side of Equation (3) (I will denote it by L3 in this review) is not equivalent to training error. \nAn upper bound on this average error is not sufficient to guarantee the convergence of the training error neither. Take the gradient descent for example (thus each minibatch x_0^n is the whole training set), the convergence of the training error should be lim_{n -> \\infty} l(f_{w^n}(x_0^n), y^n). The convergence of L3 is necessary but not sufficient to imply the convergence of the training error.\n\nAnother concern about Theorem 2 (but it is minor compared to the two problems mentioned above) is that to achieve the O(1/\\sqrt{n}) rate, the algorithm has to pick a particular learning rate. Larger or smaller learning rate (in the order of n) will lead to significantly worse regret. But in the experiments of the paper, the learning rates are not picked according to the theorem.\n\nOverall, this paper has a good motivation and good novelty. It could be further developed into a good paper. But due to the two problems and a buggy proof mentioned above, I think it is not ready for publish yet.\n", "title": "The convergence of the Taylor optimum", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}