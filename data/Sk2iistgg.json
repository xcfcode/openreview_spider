{"paper": {"title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "authors": ["Ravi Garg", "Anders Eriksson", "Ian Reid"], "authorids": ["ravi.garg@adelaide.edu.au", "anders.eriksson@qut.edu.au", "ian.reid@adelaide.edu.au"], "summary": "Predicting causal factors of an inverse problem which lie near unknown low-dimensional non-linear manifold defined by a mercer kernel.", "abstract": "Consider an ill-posed inverse problem of estimating causal factors from observations, one of which is known to lie near some (unknown) low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel. Solving this problem requires simultaneous estimation of these factors and learning the low-dimensional representation for them. In this work, we introduce a novel non-linear dimensionality regularization technique for solving such problems without pre-training.\nWe re-formulate Kernel-PCA as an energy minimization problem in which low dimensionality constraints are introduced as regularization terms in the energy.\nTo the best of our knowledge, ours is the first attempt to create a dimensionality regularizer in the KPCA framework. Our approach relies on robustly penalizing the rank of the recovered factors directly in the implicit feature space to create\ntheir low-dimensional approximations in closed form. Our approach performs robust KPCA in the presence of missing data and noise.\nWe demonstrate state-of-the-art results on predicting missing entries in the standard oil flow dataset. Additionally, we evaluate our method on the challenging problem of Non-Rigid Structure from Motion and our approach delivers promising results on CMU mocap dataset despite the presence of significant occlusions and noise.", "keywords": ["Computer vision", "Optimization", "Structured prediction"]}, "meta": {"decision": "Reject", "comment": "There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews."}, "review": {"SkhfCsuBg": {"type": "rebuttal", "replyto": "ryNp9bzre", "comment": "Firstly, we want to clarify that  the aim of the paper is to solve an ill-posed inverse problem using low dimensionality of the inferred solution as a prior. We would like for the benefit of the reviewer provide STANDARD definition of: Inverse problem and causal factors while explaining the utility of dimensionality reduction in the context of solving an ill-posed inverse problem. \n\nAn inverse problem in science is the process of calculating from a set of observations the causal factors that produced them: for example in figure 1 and section 4.2 of the paper, estimating the 3D structure (S_i) at every time instance (i) of human performing an action, which when filmed with a camera, leads to a set of 2D projections (W_i) onto the image which we observe. This type of problems are named inverse problem because we start with the results (2D image measurements) and then calculate the causes (3D structure S_i and camera locations R_i at any given time).  \nThe inverse problem we are interested in are ill-posed in nature. I.e. the 2D projection of some 3D structure albit is a well defined physical process (forward problem), we lose depth information while projecting the 3D point on the image plane. Thus, in this case starting from the observations, it is impossible to recover the causal factor without any further assumption. I.e. f(W,S) =0 is ill-posed for estimating S.\nIn this work our assumption is that the causal factor (S_i) lie on a unknown nonlinear manifold expressed by a mercer kernel. Now we want to simultaneously: (i) solve the inverse problem to estimate the $S_i$\u2019s which explain $W_i$\u2019s and (ii) estimate a manifold $C$ on (or near) which all these causal factors lie. \n\nGiven the manifold is known, part (i) in the literature is called pre-image estimation. I.e. find the data point whose mapping in the feature space is known on a given subspace of the feature space.\nPart (ii) Estimate the manifold on which all $S_i$\u2019s lie. This part is non-linear dimensionality reduction for which KPCA /LVMs can be used if $S_i$\u2019s were known. \nWe have to solve both (i) and (ii) to achieve the goal we have set for ourselves. So it is no accident that we have to discuss KPCA, other non-linear dimensionality reduction (e.g. LVMs) and use of non-linear dimensionality reduction for solving ill-posed inference problems.\n\nEquipped with above explanation now we can answer the quarries of reviewer more precisely: \n\n\n\u201cwhy one is interested in step (iii) outlined on page 2\u201d\nAs out goal in NRSfM is to find out what are the 3D shapes which lead to 2D projections we have observed, merely finding the projection of $\\Phi(S_i)$ onto a manifold $C_i$ is almost useless for NRSfM (or for matrix completion or any other reconstruction task in general).  We have to find out the 3D shape (pre-image) which can be projected to the manifold and explains the observation $W_i$. This operation is not as straight forward in the case of non-linear manifolds as it is for linear subspace.\n\n\u201ca data point (pre-image) corresponding to each projection in the input space\u201d is not a standard step in KPCA. \u201c\nThis is not a disadvantage of the KPCA. It is the disadvantage of  a naive pre-image estimation framework which starts with an initial point ($S_i$) in the data space (which we do not have) , project its non linear transformation \\phi(S) onto the manifold $C^T$ and then try to reconstruct the \\bar(S) which can be projected at the estimated location in the feature space. The issue is that the mapping from data space to feature space is not one to one and onto.  \n\n\u201c$\\mathcal{X} \\times N$, $\\mathcal{Y} \\times N$, $\\mathcal{H} \\times N$\u201d\nSorry for the confusing notations, these are simply 3 matrices (of dim X by N/ Y by N etc..)  formed by stacking N tuples related to N observations.\n\n\u201cExperiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered \u201c\nThe goal of the Matrix completion experiment on oil flow dataset was to mainly compare the robust non-linear dimensionality reduction methods which are equipped to deal with missing data with our energy minimization approach minimizing the dimensionality of the manifold. SVT and OptSpace to our understanding targets matrix completion problem under the assumption that the columns of the completed matrix lie on a linear subspace of low-rank. As oil flow data can be modeled better with a low dimensional nonlinear manifold (it is inherently 2D), extensively evaluating all linear dimensionality reduction methods on this data was considered less important. As linear low rank assumption is suboptimal here. However will shortly include the numbers with optspace and SVT for reference.\n\n\u201cExperiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.\u201d  \nWe have included state of the art NRSfM method of Dai et.al. CVPR2012 /Garg et.al. CVPR2013 (with modifications to handle missing 2D measurement we call it TNH) as a very strong baseline for NRSfM. In the supplementary material, this method is compared favourably on full CMU Mocap against other NRSfM solutions for which the code is publically available. These additional comparisons were left in supplementary material because of the lack of space. \n     \n\n\"The authors do not address the out-of-sample problem\"/  \"KPCA is trained via a closed-form update, but there is still training (R3)\":\n\nThese two quarries stem from the misunderstanding of the problem 1 to be dimensionality reduction alone. In this paper we are NOT trying to learn a non-linear manifold using a representative training-set (noisy or clean) which can be used to infer/reconstruct an out-of-sample instance (to check the generalization capability of learned manifold). With the exception of experiment in appendix A where we validate our close form solution, all experiments aims to only infer the instances on/near a manifold (not know apriori) for which the observations are given. For example, we do not suggest that a manifold which explain the 3D shapes for a given set of 2D observations, say a human doing push-ups will explain the 3D structures of a human running or vice-versa. Each such instance of inverse problem in our framework is solved separately and the estimated manifolds do not generalize for out of sample problem. More importantly, we have NO 3D shapes available to train our manifold but the low-dimensionality constraints are used (and are critical) to infer 3D shapes purely from the 2D observations. \n\n \u201cThe problem reformulation of Kernel PCA uses somewhat standard tricks\u201d\nWe agree that assuming gaussian noise in the feature space and doing L1 approximation of L0 (trace norm relaxation of rank) are standard tricks. However, it is important to reiterate here that designing an unified energy minimization framework to penalize non-linear dimensionality, which can deal with implicitness of feature mapping (it is non-trivial to map from feature space to input space) and non-trackability of feature space (which is infinite dimensional for example with RBF kernel) while solving a highly ill-posed inverse problem is non-trivial and to our knowledge is not done before.\n\n\n\n", "title": "reply to AnonR1"}, "BJMN0OOBg": {"type": "rebuttal", "replyto": "rJXJeHYEg", "comment": "\nWe thank the reviewer for his very valuable inputs.\n\nFirst, We would like to clarify a crucial aspect of the proposed framework which differs from KPCA and GP-LVMs leading to some misinterpretations in the review. As explained in the motivation and introduction the goal of this work is not to merely propose a robust KPCA framework (although it is a part of the work) but to develop a dimensionality regularizer for ill-posed inference. Our aim is to penalize the non-linear dimensionality of the causal factors associated with an ill-posed (under-constrained) inverse problem which we assume lie on a nonlinear manifold. \n\nThe main difference between our work to the counterpart which we call \"pre-training\" followed by constrained inference (with iterative pre-image estimation for example) on manifold is explained clearly in para 3 of the introduction. \n\nLet us explain with the example problem of non-rigid structure from motion again what we mean by a \"pre-training\". In the NRSfM, cost function f(W,Z,R,S) relates the causal factors i.e. 3D shapes (a 3d vector per-frame per-landmark on a human skeleton as shown in the images) to the 2D image observations W (2d vector per-frame, per-landmark). \nAssuming gaussian noise in the 2D measurement W, the least square framework for inferring S given R is highly under-constrained and can not be solved directly. It is important to note here that in NRSfM,  ONLY the 2D projections of the 3D landmarks on deformable object as observed from various viewpoints are known. Although it is reasonable to believe that a non-linear manifold of low-dimension must explain all the 3D shapes we want to infer, we do not know apriori what that manifold is for a given capture.\n\n\nTo KPCA/RKPCA/LVMs can only be applied to estimate a manifold of human poses if a representative 3D dataset was available. If we were presented with a large (maybe noisy) dataset of human skeletons where we know the 3D location of the joints as the humans perform different actions, we can indeed allpy LVM\u2019s or any robust form of KPCA to learn a manifold which then can be used to solve the problem of recovering the 3D shapes (near manifold) which leads to the observed 2D projections in a new sequence. This setup which uses a large set of 3D human skeleton  is what we call pre-training based method.\n\nA standard approach to solve this NRSfM with \"pre-training\" followed by iterative inference would be:\nStep 1-> Assume a noise model (say gaussian) and learn a manifold spanned by human skeletons performing different actions. Variants of KPCA like RKPCA or LVMs could be used for this step we call \u201cpre-training\u201d.  \nStep 2 ->  solve f(W,R,Z,S) = 0, which is highly under constrained and gives trivial solutions of large dimensionality/rank when solved without manifold/rank constraints.\nStep 3 -> project the solution to the pre-estimated non-linear manifold and refine the solution iteratively by estimating the pre-image of the projection.\n\nIn the proposed framework we solve the NRSfM problem without requiring a dataset of 3D human skeleton (noisy or noise free). Our solution can be simply considered as a nonlinear extension of the energy minimization frameworks which rely on nuclear norm regularization to minimize the rank of matrices while solving ill-posed problems. These frameworks have been used to solve various problems ranging from netflix recommendations to NRSfM (See references) without using PCA/RPCA or its variants as the first step. To the best of our knowledge no variant of GPLVM or KPCA is well established in the literature as non-linear counterpart to trace norm regularization. Proposing such regularizer which can be used in a general energy minimization framework is the main contribution of the paper we present.\n\nNow we would try to address some of the technical errors R3 suggests are in the manuscript:\n\n1)  \"paper proposes a closed form solution to robust KPCA\"- This is indeed correct. Let us clarify. Problem 1 as explained in the paper decomposes into two parts. The first part is solving the ill-posed problem (NRSfM for example) with the constraint that the inferred causal factors lie on (or near) a GIVEN manifold defined by mercer kernel (RBF) and nonlinear principal component denoted by matrix $C$ in section 3.1 are known. It is this pre-image estimation problem (inferring a data point on the given manifold to explain observation) which requires the LM iterations. In our knowledge other known solution to this problems are iterative as well. However this problem does not correspond to KPCA as this part assumes $C$ to be known.\n\nRobust KPCA is the second part (subproblem 7) of problem 1, where the noisy data (or noisy kernel matrix) is given and the goal is to estimate a non-linear manifold near which the data lie. We assume gaussian noise (in feature space) as correctly pointed out by the reviewer and estimate this non-linear manifold in closed form. i.e. you can replace KPCA/GPLVMs directly with our closed form solution given in algorithm 2 to estimate a robust manifold given noisy kernel matrix.\n\n2) \"proposed approach can be trivially generalized to incorporate other cost functions\" : \"cost function\" here correspond to the the function f(W,S) associated with the inverse problem. The decoupling of equation 5 into 2 subproblems (6,7) guarantees that the function f(W,S) does not feature in the subproblem (7). Thus the \"close form solution of the inner loop\" remain intact even after changing the function f. For example instead of using L2 loss for matrix completion on can use a robust huber loss or more interestingly, f can actually be defined by physical process of camera projection relating 3D structures with their image projections.\n\n3) \"novel energy minimization framework to solve problems of the general form of eq. (2)\" : The experiments with oil-flow dataset (in appendix and main body) along with NRSfM suggests that the relaxation proposed works in practice. We will reword the introduction accordingly and explicitly say that we solve a relaxed version of the problem 1. The relaxation proposed in this framework however are (i) trace norm approximation of rank and (ii) gaussian noise assumption in the feature space both of which are studied independently by other body of works (RPCA and GP-LVMs respectively) and used extensively.\n\n4) LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\": In the text we are referring to the use of the LVMs as the pre-training step for low dimensional inference by Prisacariu & Reid (2011) for segmentation; Dame et al. (2013) for 3D reconstruction and other works which indeed use clean training data. The crucial point however is that these approaches learn the latent space APRIORY with a training set which in our case does not exist. \n\nOn \"The experimental evaluation should demonstrate robustness to more complex noise and outliers\"\nIn a very loose sense, NRSfM can be seen as doing dimensionality reduction for severely corrupted data with a very structured pattern which arises from the loss in the information while doing image projections of 3D structures. In context of NRSfM every data sample we want to infer (3P vectors storing the 3D locations of P skeleton joints) is corrupted severely by removing the depth coordinate from each landmark point in every frame. This kind of structured corruption occurs naturally in practical inverse problems which is much severe to recover from than synthetic noise.\n\nFinally on the datasets: Oil flow dataset, we agree is old and simple. It is used here only to facilitate direct comparisons with a large body of work done on robust non-linear dimensionality reduction in presence of noise and missing data. It is important to note that our framework is more generic than robust and missing data KPCA/LVMs for matrix completion. Most of the baselines used for matrix completion on oil flow dataset are not trivially extensible to solve problem 1 (or its relaxations).\nHowever, a direct comparison (which show significant performance gain over baselines on simple but non saturated dataset) helps us answering questions like \"how solving for a local optima of our doubly relaxed problem helps in solving the original problem\".\n\nCMU mocap for NRSfM is a very challenging real dataset and is used to benchmark current state-of-the-art NRSfM solutions till date. We show benefits of non-linear regularizer over its linear (trace norm regularization) counterpart in solving a practical and challenging problem like NRSfM -- for which dimensionality reduction is essential component in most state of the art solutions. That being said, if the reviewers can point us to the newer non-linear dimensionality reduction frameworks which can solve an inverse problems without requiring a \"training stage\", we will be happy to include the relevant experiments.\n\n", "title": "response to AnonReviewer3"}, "SJnGDHbNl": {"type": "review", "replyto": "Sk2iistgg", "review": "N/AThis paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  \n", "title": "N/A", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJXJeHYEg": {"type": "review", "replyto": "Sk2iistgg", "review": "N/AThis paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  \n", "title": "N/A", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}