{"paper": {"title": "Demystifying Graph Neural Network Via Graph Filter Assessment", "authors": ["Yewen Wang", "Ziniu Hu", "Yusong Ye", "Yizhou Sun"], "authorids": ["wyw10804@gmail.com", "bull@cs.ucla.edu", "yusongye@g.ucla.edu", "yzsun@cs.ucla.edu"], "summary": "Propose an assessment framework to analyze and learn graph convolutional filter", "abstract": "Graph Neural Networks (GNNs) have received tremendous attention recently due to their power in handling graph data for different downstream tasks across different application domains. The key of GNN is its graph convolutional filters, and recently various kinds of filters are designed. However, there still lacks in-depth analysis on (1) Whether there exists a best filter that can perform best on all graph data; (2) Which graph properties will influence the optimal choice of graph filter; (3) How to design appropriate filter adaptive to the graph data. In this paper, we focus on addressing the above three questions. We first propose a novel assessment tool to evaluate the effectiveness of graph convolutional filters for a given graph. Using the assessment tool, we find out that there is no single filter as a `silver bullet' that perform the best on all possible graphs. In addition, different graph structure properties will influence the optimal graph convolutional filter's design choice. Based on these findings, we develop Adaptive Filter Graph Neural Network (AFGNN), a simple but powerful model that can adaptively learn task-specific filter. For a given graph, it leverages graph filter assessment as regularization and learns to combine from a set of base filters. Experiments on both synthetic and real-world benchmark datasets demonstrate that our proposed model can indeed learn an appropriate filter and perform well on graph tasks.", "keywords": ["Graph Neural Networks", "Graph convolutional filter analysis", "representational power"]}, "meta": {"decision": "Reject", "comment": "The paper investigates graph convolutional filters, and proposes an adaptation of the Fisher score to assess the quality of a convolutional filter. Formally, the defined Graph Filter Discriminant Score assesses how the filter improves the Fisher score attached to a pair of classes (considering the nodes in each class, and their embedding through the filter and the graph structure, as propositional samples), taking into account the class imbalance.\n\nAn analysis is conducted on synthetic graphs to assess how the hyper-parameters (order, normalization strategy) of the filter rule the GFD score depending on the graph and class features. As could have been expected there no single killer filter.\n\nA finite set of filters, called base filters, being defined by varying the above hyper-parameters, the search space is that of a linear combination of the base filters in each layer. Three losses are considered: with and without graph filter discriminant score, and alternatively optimizing the cross-entropy loss and the GFD; this last option is the best one in the experiments.\n\nAs noted by the reviewers and other public comments, the idea of incorporating LDA ideas into GNN is nice and elegant. The reservations of the reviewers are mostly related to the experimental validation: of course getting the best score on each dataset is not expected; but the set of considered problems is too limited and their diversity is limited too (as demonstrated by the very nice Fig. 5).\n\nThe area chair thus encourages the authors to pursue this very promising line of research and hopes to see a revised version backed up with more experimental evidence. "}, "review": {"S1liHkwIiB": {"type": "rebuttal", "replyto": "rygcY3OCFB", "comment": "Thank you so much for the positive feedback! We really appreciate your support for our paper as well as your constructive suggestions. We have improved our paper based on your advice (we marked the modifications related to your suggestions with blue text, and highlighted the previous version with strikethrough): \nhttps://drive.google.com/file/d/1wJYwz1oPDK1-NbpesHUR6ZMCSBRVxdh3/view?usp=sharing\n\nFor Eq. (3): to get the GFD score for a multi-class setting, we first calculate the Fisher Difference for each possible pair of classes, then normalize them based on class size, and finally sum them together to get GFD score. Based on the normalization, the class imbalance would not be a problem. We have improved our writing to make this part more clear.\nFor the writing errors, thank you for pointing them out. We have corrected all the errors pointed out by you and also have carefully gone through the paper to improve the writing.\n", "title": "Thank you for your support for our paper!"}, "BkeIi3y3iH": {"type": "rebuttal", "replyto": "Bkg0BVP8oB", "comment": "Thank you for your valuable feedback! We improved our paper based on your advice (we marked the modifications related to your suggestions with orange text, and highlighted the previous version with strikethrough)\uff1a \nhttps://drive.google.com/file/d/1qAe72_w9Zn_mXg7rwu25o54kcQ6QhsME/view?usp=sharing\n\nThe reviewer is majorly concerned about whether our pointed problem indeed exists on real-world datasets, and whether our proposed method can solve it. To alleviate this concern, we added a new real-world dataset and show consistent results with our analysis, which is discussed in Q1. \n\nNow we address your comments and concerns in detail:\n\nQ1: Synthetic datasets appear to be extreme and unrealistic and look carefully selected in favor of the proposed method.\nA1: The problem we found out is not unrealistic and carefully selected, but indeed appear in real-world datasets. Each synthetic dataset we choose corresponds to a specific graph data property we analyze in Section 3.2, including \"small density gap\" (i.e., the graph structure is not highly correlated with labels) and \"large label ratio gap\" (i.e., classes are imbalanced). These properties widely occur in real-world datasets. To further justify this, we choose the \"large label ratio gap\" as an example and try to find a graph dataset that has this problem. We download a large scale academic graph called Open Academic Graph (OAG) [1][2][3] and choose two fields that have a large disparity in the number of papers: (1) \u201cHistory of ideas\u201d, which consists of 1041 papers; and (2) \u201cPublic history\u201d, which consists of 150 papers. Obviously, these two classes are imbalanced so that the graph data has the \"large label ratio gap\u201d problem. We then compare our method with baselines on this OAG graph data (We open-source this dataset in the github. Detailed experiment settings and results are in Appendix A.10). According to our results (Table 8), our proposed AFGNN_inf achieves 88.22 macro F1-score, which outperforms all the other baselines (our macro F1 is at least 3% higher than all the baselines). Such a result on the real-world dataset is consistent with what we achieve on the same synthetic dataset, which indicates that the problem we reveal is not \u201cunrealistic\u201d, but exists in real-world datasets. The widely adopted benchmark graph datasets (cora, citeseer, pubmed), however, do not have these potential problems. Thus our analysis can also benefit the GNN research community to find other representative benchmark datasets for the node classification task.\n\nQ2: Model is not novel\nA2: We\u2019d like to emphasize that the main contribution of our work is the proposed graph filter assessment tool (GFD score) and the insights we found with the tool, which provides a unique perspective in understanding why GNN will work and how we should choose graph filters for graph data with different properties. The AFGNN model is our first attempt to learn a flexible filter that is adaptive to the graph data by leveraging the GFD score, which has successfully demonstrated the power of our assessment tool. The basic idea of the AFGNN is simple, but it works well with much less memory and time consumption than the sophisticated model as GAT. According to our results (Table 6 and 7), on Cora and Citeseer, GAT's time cost is at least three times of AFGNN's time cost, and GAT\u2019s memory cost is two times of AFGNN's memory cost.  \n\nQ3: \"Regularization term\" is inadequate.\nA3:  Thanks for pointing it out. The standard definition of regularization is: regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting. Our GFD is added to the loss function to guide the learning process of the filter and to avoid overfitting, therefore, previously we call this GFD term a regularization. To avoid confusion, we have changed it into \u201cGFD loss\u201d.\n\nQ4: $AFGNN_{infinity}$ is not equivalent to applying infinite lambda\nA4: Thanks for pointing it out, we agree that our previous writing in this part is not accurate enough. We use the previous writing to emphasize we optimize $\\alpha$ and $W$ iteratively by minimizing CrossEntropy loss and GFD loss respectively. We have improved our writing now. \n\nQ5: For $AFGNN_1$, is it also iteratively optimized as $AFGNN_{infinity}$? \nA5: $AFGNN_1$ and $AFGNN_{infinity}$ are different. For $AFGNN_1$, we learn $\\alpha$ and W simultaneously by directly minimizing the overall objective function (=CrossEntropy loss + GFD loss), but for AFGNN_inf, we learn $\\alpha$ and $W$ separately. We have improved our writing for this part to make it more clear. \n", "title": "Response to Reviewer1"}, "BklViGPUsB": {"type": "rebuttal", "replyto": "H1gL4Eq0tr", "comment": "Thank you for your constructive comments! We improved our paper based on your advice (we marked the modifications related to your suggestions with red text, and highlighted the previous version with strikethrough): \nhttps://drive.google.com/file/d/1adtmKH61RLyBKzn46DWdEj5JQvu5M5WO/view?usp=sharing\n\nFirst, we want to emphasize this paper\u2019s contribution. Our work provides a theoretical understanding to  GNNs in a novel way, and we are the first to analyze GNNs for the node classification task from a data perspective. Since rich literature demonstrated that the key of GNNs lies in their graph convolutional filters, we propose a new assessment tool (GFD) to evaluate the effectiveness of filters given a specific graph data Further, this tool is applied to analyze existing filters and found some meaningful insights. Finally, we propose the AFGNN model to automatically learn the best filter from the given family (i.e., learn the best coefficients for the linear combination of a set of filters) for the given graph data.\n\n\nNow we address your comments and concerns in detail:\n\nQ1: Is it reasonable to use the Fisher score\u201d to support the second term in equation (3), where the Fisher score is used to evaluate before the filters applied?\nA1: We\u2019d like to clarify our GFD is an assessment tool to see whether a filter is good for a particular graph data.\nFisher Score is used to evaluate the separability of data. GFD defined in  Eq. (3), which is the Difference of Fisher Score before and after the filter, is to evaluate whether a filter can increase the data separability. \nAs we show that a good graph convolutional filter can help the non-linear separable data to be linearly separable, we can expect the GFD score is higher for these filters. Note that not every graph filter has this property for every dataset, and our final model is to learn the best filter that could enhance this property for a given dataset. Experimental results in Figure 5 and Table 2 also support this claim.\n\nQ2: The presentation of the last paragraph of \u201cgraph filter discriminant score\u201d in page 4 can be improved. The Figure references seem incorrect and confusing.\nA2: As mentioned in Q1, we use Fisher score to evaluate the separability of two classes, we use Fisher Score Difference to evaluate the power of a filter on two classes, and we use GFD, which is a weighted sum of Fisher Score Difference of each pair of classes, to evaluate the power of a filter on the given graph. We have revised our writing and corrected our Figure reference to make this more clear. \n\nQ3: The analysis of the influence of label ratio seems not accurate enough.\nA3: Suppose the density and density gap are fixed, when label ratio drops, which means the two classes become more imbalanced, and nodes in a larger class tend to have more neighbors. Then, with the column normalization strategy that does not have any constraint on the range of representation, those nodes with a larger degree tend to aggregate more information and thus have larger new representations. This would be helpful to differentiate the two classes. Take Figure 6 (g) as an example, nodes in the large-size class (green nodes) are gathered in the upper right part while nodes in the small-size class (purple nodes) are gathered in the lower left part, so the two classes become more separable after applying a column-normalized filter. We revised our writing to make this more clear.\n\n\nQ4: For the GFD score comparison in Figure 4, why choose order 1,3,7 for density and different order 2,3,6 for the density gap?\nA4: Thanks for pointing out. Previously we just pick the orders that can show our findings most clearly, but we agree it is important to use consistent choice in two subfigures. We now choose the same set of orders in these two figures. The result remains the same. \n\nQ5: What is the meaning of the symbol psi(l)?\nA5: It is a learnable intermediate weight (before normalization) for each base filter. We then apply softmax normalization to it to get alpha(l). We revised our writing to make this more clear.\n\n", "title": "Response to Reviewer3"}, "BJxgrTJ3iH": {"type": "rebuttal", "replyto": "BkeIi3y3iH", "comment": "Q6: The results on three real datasets do not show significant gains.\nA6: First, our method performs the best among all the baselines that also adopt the same base filter family, and only perform worse than GAT, which has a much wider filter family space than us. However, GAT actually requires much more computation resources than us. Compared with GAT, our model needs less time and memory costs (According to our results in Table 6 and 7, GAT's time cost is at least three times of AFGNN's, and GAT\u2019s memory cost is two times of AFGNN's.). Also, our model can deal with class imbalance issues much better than GAT (the performance of GAT on SmallRatio and Imbalanced OAG are not good, our $AFGNN_{infinity}$ achieves 93.8 and 96.3 on these two datasets, while GAT  only achieves 82.1 and 95.1). Second, currently we only use a family of simple base filters, and the performance of AFGNN is expected to be further improved by enlarging the filter family. We leave the design of the filter family as future work. Finally, we want to emphasize again that this paper\u2019s biggest contribution is to understand and evaluate GNNs\u2019 filter rather than to propose another GNN model. We find some hard cases that existing GNNs with fixed filter can not handle well, and propose AFGNN to enhance the performance under these hard cases. Existing benchmark real-world datasets are not sensitive enough to differentiate existing baseline models, so we also analyze how to find a more powerful dataset that can differentiate different models and generate synthetic datasets based on our analytics. \n\nQ7: Inductive learning (e.g., PPI) is not tested.\nA7: Thanks for pointing out. Our current proposed model is designed mainly for transductive semi-supervised node classification, and may have some limitations for the inductive setting in some cases. But our analysis result can help design the inductive filter learning model.\n\nOur analysis assumes that for a set of feature information (X), structure information (A), and their dependency relationship with labels (A|Y and X|Y), there should exist an optimal filter, and our algorithm is designed to learn a good filter for a single graph, which can approximate such optimal filter. For a transductive setting (for example, Cora, Citeseer, Pubmed), where we only need to deal with a single graph, our algorithm has shown to be effective to learn a good filter for it. For an inductive setting (for example, PPI dataset), where we are dealing with multiple graphs with different graphs, the structure information (A) is different for each graph, and thus the optimal filter for each graph can be different. Since our current algorithm can only learn a single filter for all the graphs, if the optimal filters for testing graphs are different from what the ones for training graphs, our current algorithm cannot improve the performance too much. For cases where the graph structure property doesn\u2019t change too much, we can assume that there still exists a single optimal filter and our current algorithm can generalize well.\n\nWe evaluate our model on PPI and found our AFGNN obtains better performance than all the other baselines but is worse than GAT. This is mainly because PPI have different chemical graphs that have totally different structures, and thus fall into the first case where our algorithm that only learns a single filter cannot improve the performance too much.\n\nIn spite of the limitation of our current model, we\u2019d like to point out that it is feasible to adopt our analysis result for designing an inductive filter learning algorithm. Since we\u2019ve already found that we can infer the optimal filter based on the graph data\u2019s properties (e.g., label imbalance will benefit column normalization, etc), we can design an model (f) that takes these graph properties as input and infer the optimal alpha, instead of learning alpha from scratch for a new graph. If we can train the f with graphs with various properties, ideally it should learn to get optimal filter for any kind of graph data. In this way, such a model can be well suitable for inductive node classification. Therefore, our analysis can still be useful to deal with inductive node classification and even other graph-related tasks. Since such model improvement is out of range to what we want to focus on in this paper, we leave it as future work.\n\n\n\n[1] https://www.openacademic.ai/oag/\n[2] Fanjin et al. OAG: Toward Linking Large-scale Heterogeneous Entity Graphs. In Proc. of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'19)\n[3] Arnab Sinha, et al. An Overview of Microsoft Academic Service (MAS) and Applications. In Proc. of the 24th International Conference on World Wide Web (WWW \u201915)\n\n\n\n\n\n\n", "title": "Response to Reviewer1 (cont.)"}, "Bkg0BVP8oB": {"type": "rebuttal", "replyto": "SklzDh7NcB", "comment": "We appreciate your valuable comments and we are now actively working on the supplementary experiments on real-world datasets!", "title": "Response to Reviewer1"}, "rJxjpfv8or": {"type": "rebuttal", "replyto": "BklViGPUsB", "comment": "Q6: It would be better if the explanation of the training loss section is more detailed and clear.\nA6: Generally, our overall loss is a weighted sum of cross-entropy loss in terms of node classification and GFD loss in terms of the filter\u2019s capability in enhancing linear separability. By changing the value of the weight, we have $AFGNN_0$, $AFGNN_1$, and $AFGNN_{infinity}$. $AFGNN_0$ and $AFGNN_1$ correspond to the case that the weight of GFD loss is 0 and 1 respectively, and parameter $\\alpha$ and W are optimized simultaneously with overall loss. $AFGNN_{infinity}$ is different, and it is not exactly the case where the weight equals infinity. To train $AFGNN_{infinity}$, we iteratively optimize $\\alpha$ and W with GFD loss and classification loss respectively. Thanks for pointing out this unclear part, we have revised our writing to make it more clear.\n\nQ7: What is \u201cAFGNN_P\u201d in the experiment analysis?\nA7: It should be $AFGNN_{infinity}$, thanks for pointing out this typo.\n\nQ8: It could be interesting to see the time comparison between the proposed method and the GAT.\nA8: Thanks for the valuable suggestion! We have added the time, memory comparison table in Appendix A.9, our experiment results show our AFGNN models need less time and memory consumption. According to our results (Table 6, 7), on Cora and Citeseer, GAT's time cost is at least three times of AFGNN's time cost, and GAT\u2019s memory cost is two times of AFGNN's memory cost. GAT does not have recorded time and memory cost for Pubmed dataset because it requires too much memory cost and is not able to run on GPU. Therefore, we claim that AFGNN needs less time and memory cost than GAT.\n\nQ9: For the graph filter discriminant analysis, is it fair to compare the learned layer with the other base filter using the GFD score? Since the learned layer is picked with the highest GFD score. Maybe one or two sentences on this will be helpful.\nA9: First, we\u2019d like to clarify that we do not pick the best filter from the filter family. Instead, the combination weights (alpha) are learned in an end-to-end fashion on the training dataset, while the evaluated GFD scores are calculated on the test dataset. Therefore, it\u2019s not guaranteed that a learned filter will definitely generalize better than all the base filters. Second, this experiment is to verify whether we can learn an optimal filter adaptively instead of using a fixed filter. For different datasets, there exist different optimal base filters (for example, column normalization is the best for SmallRatio and row normalize is the best for benchmark citation network), and our algorithm can indeed learn a good combination of them that generalizes well, as we expected.\n\nQ10: The writing of the paper must be improved. Too many typos and grammar problems will impair the presentation and the reader can be distracted.\nA10: Thank you for pointing them out.  We have carefully proofread our paper again and polished the paper to alleviate typos and grammar errors.\n\n", "title": "Response to Reviewer3 (cont.)"}, "rygcY3OCFB": {"type": "review", "replyto": "r1erNxBtwr", "review": "This is a very interesting study about GNN. Authors proposed to extend LDA as a discrimination evaluator for graph filleters. Also authors proposed Adaptive Filter Graph Neural Network to find the optimal filter within a limited family of graph convolutional filters. The whole study is novel, and beneficial for the community of graph neural network study. It provides a new way to understand and evaluate GNN.\n\nThere are some questions authors should clarify. And some writing errors to correct.\nEq (3) defines GFD for a pair of classes i and j. For a graph with more than two classes, the GFD will be the average of all pairs? Will class imbalance will have any impact on this GFD measure?  \nErrors:\n\u2022\twe studies the roles of this two components \n\u2022\tthere exist a best choice \n\u2022\twe only consider to to find\n", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 4}, "H1gL4Eq0tr": {"type": "review", "replyto": "r1erNxBtwr", "review": "In this paper, the authors raise and address three questions about graph neural network: (1) Whether there is a best filter that works for all graphs. (2) Which properties of the graph will influence the performance of the graph filter. (3) How to design a method to adaptively find the optimal filter for a given graph.\nThe paper proposes an assessment method called the Graph Filter Discriminant Score based on the Fisher Score. It measures how well the graph convolutional filter discriminate node representations of different classes in the graph by comparing the Fisher score before and after the filter.\nBased on the GFD scores of different normalization strategy and different order of the graph filter in the experiments on synthetic data, the authors answer the first two questions: (1) There is no optimal normalization for all graphs. (2) row normalization performs better with lower power-law coefficient, but works worse with imbalanced label classes and large density gap.\nFor the third question, the authors propose a learnable linear combination of a limited family of graph convolutional filters as the layer of model AFGNN, which can learn the optimal arguments of the combination based on the FGD score.\nThe paper focuses on a significant topic and proposes an assessment tool for the graph filters. Based on that, it also introduces a model to choose filters from a family of filters for any specific graph.\nThe description of preliminaries is clear.\nThe observations of the impact of the graph properties on the filter choice are interesting and explanations are provided.\nThe results of the test accuracy on both bench mark and synthetic datasets demonstrate the good performance of the proposed model.\nIt is good that the paper provides proof for the claim that the graph convolutional can help the non-linear separable data to be linearly separable, so it is reasonable to use Fisher score. However, does this claim support the second term in equation (3), where the Fisher score is used to evaluate before the filters applied?\nThe presentation of the last paragraph of \u201cgraph filter discriminant score\u201d in page 4 can be improved. The Figure references seem incorrect and confusing.\nThe analysis of the influence of label ratio seems not accurate enough.\nFor the GFD score comparison in Figure 4, why choose order 1,3,7 for density and different order 2,3,6 for density gap?\nWhat is the meaning of the symbol psi(l)?\nIt would be better if the explanation of the raining loss section is more detailed and clear.\nWhat is \u201cAFGNN_P\u201d in the experiment analysis?\nIt could be interesting to see the comparison of time between the proposed method and the GAT.\nFor the graph filter discriminate analysis, is it fair to compare the learned layer with the other base filter using the GFD score? Since the learned layer is picked with highest GFD score. Maybe one or two sentences on this will be helpful.\nThe writing of the paper must be improved. Too many typos and grammar problems will impair the presentation and the reader can be distracted.\nMinor comments:\nThe layout of the sub caption of Figure 1 can be improved.\nThe usage of capital letter in the phrase \u201cdensity gap\u201d is inconsistent.\n\u201cAs shown in figure\u201d instead of \u201cAs is shown in figure\u201d.\nMany sentences miss article.\nThere are many typos in the writing.\nFor example, \u201cNote that for given (feature)\u201d, \u201c\u2026make the representation of nodes in different (class) more separable.\u201d, \u201cNoted that there are some other (variant) of GNN filters that (does) not fall into\u2026\u201d in page 4.\n\u201cHere we give (a) empirical explanation to this phenomenon\u201d, \u201cthis normalization strategy (take) into account\u2026\u201d, \u201cThus even in the case that the other two (doesn\u2019t) perform well\u2026\u201d in page 5.\n\u201c\u2026a very important factor that (influence) the choice of normalization strategy\u201d, \u201cwhen power-law coefficient (decrease)\u201d, \u201cwhen the (sizes) of each class (become) more imbalanced\u201d,  \u201cThis is because column normalization better (leverage) \u2026\u201d , \u201cin a similar manner (with) label ratio\u201d, \u201cwhen the density or density gap (increase)\u201d, \u201chigh-order filters can help gather\u2026 and thus (makes) the representations\u2026\u201d, \u201cwhen the density gap (increase)\u201d in page 6.\nThese can be continued but it is obvious that this paper needs proofreading.\n", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 1}, "SklzDh7NcB": {"type": "review", "replyto": "r1erNxBtwr", "review": "This paper introduces an assessment framework for an in-depth analysis of the effect of graph convolutional filters and proposes a novel graph neural network with adaptable filters based on the analysis. The assessment framework builts on the Fisher discriminant score of features and can also be used as an additional (regularization) term for choosing optimal filters in training. The assessment result shows that there is no single graph filter for all types of graph structures. Experiments on both synthetic and real-world benchmark datasets demonstrate that the proposed adaptive GNN can learn appropriate filters for different graph tasks. \n\nThe proposed analysis using the Fisher score is reasonable and interesting, giving us an insight into the role of graph filters. Even though the analysis is limited (using simple graph models and filter family) and the result is not surprising (given no free lunch theorem, there is very likely to be no single silver bullet fo graph filters), I appreciate the analysis and the result. But, I have some concerns as follows. \n\n1) The proposed GNN and the optimization process\nThe proposed method is to extend CNN to a simple linear combination of different filter bases with learnable weights, which I don't think is very novel. Adding the GFD score as an additional constraint term is interesting, but the way of optimizing the whole objective function is unclear. (In addition, I think calling it the \"regularization term\" is inadequate since the term actually involves data observation, rather than a prior on parameters only.) \nIn the case of AFGNN_inf, I don't think it is equivalent to applying infinite lamda. If lamda is infinite, L_CE needs to be completely ignored. This needs to be clarified. \nIn the case of AFGNN1, I don't clearly understand how the whole objective function is properly optimized with fixed data representation. Is it also iteratively optimized? I hope this is also clarified in more detail. \n\n2) Unconvincing experiments\nThe results on three real datasets do not show significant gains, and two of them are even worse than those of GAT. Furthermore, inductive learning (e.g., protein-protein interaction (PPI) dataset used in GAT) is not tested, which I think needs to be also evaluated. While two synthetic datasets (SmallGap and SmallRatio) created by the authors show significant improvement, these datasets appear to be extreme and unrealistic and look carefully selected in favor of the proposed method. I recommend the authors use for evaluation more realistic datasets that can be found in related research.  \n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 2}, "S1xLBztAuH": {"type": "rebuttal", "replyto": "SJlmRFw2OB", "comment": "Thank you so much for the comment!\n\nThe key point of our work is to analyze the GNNs for node classification from a data perspective. We pointed out that there\u2019s no best GNN filter for all datasets, and we proposed a GFD score that can assess the power of filter and help to find the optimal filter for a given dataset as well. \n\nFor question 1: We proposed the AFGNN to verify our analysis, according to the experiment result, for whichever dataset, the performance of our proposed AFGNN is always among the best, which shows AFGNN is robust and effective and indicates GFD can help to select the best filter for a given dataset. Also, the current benchmark datasets cannot clearly differentiate different graph neural networks (as shown in Table 2, while the order is the same, scores of different filters are close to each other). So in our work, we identify some challenging cases for existing GNNs, then create corresponding synthetic benchmark datasets to test all GNN models. It will also guide us to look for real-world graph data to serve as the new benchmark datasets.\n\nFor question 2: For the benchmark datasets split, we used 20 samples each class for training, 500 samples in total for validation, and 1000 samples in total for test. This is a standard split strategy, and most existing works, including all of our baselines (GCN, GAT, GFNN, SGC), follow this strategy. It\u2019s true that using the mean results of multi-splitting methods may help to reduce the impact of dataset partitioning on experimental results. But in order to have fair comparisons between our model and baselines, we follow the split convention. \n", "title": "Re: Two questions for this work "}, "BkxQrdIyOH": {"type": "rebuttal", "replyto": "r1erNxBtwr", "comment": "We've found a typo when we define training loss in formula 5. The GFD score should be the cumulative negation GFD score of the filter in each layer with respect to its previous layer's output. Previously we missed the GFD. The corrected version is in https://github.com/conferencesub/ICLR_2020/blob/master/DissectingGNN_ICLR%20(3)%20(1).pdf\n\nSorry for the mistake.", "title": "Corrections of Formula Typo"}, "rklZkk_ZuH": {"type": "rebuttal", "replyto": "rJgE7xBJur", "comment": "Thank you for your comment and your interest in our work!", "title": "Thank you!"}}}