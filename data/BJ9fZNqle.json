{"paper": {"title": "Multi-modal Variational Encoder-Decoders", "authors": ["Iulian V. Serban", "Alexander G. Ororbia II", "Joelle Pineau", "Aaron Courville"], "authorids": ["julianserban@gmail.com", "ago109@psu.edu", "jpineau@cs.mcgill.ca", "aaron.courville@umontreal.ca"], "summary": "Learning continuous multimodal latent variables in the variational auto-encoder framework for text processing applications.", "abstract": "Recent advances in neural variational inference have facilitated efficient training of powerful directed graphical models with continuous latent variables, such as variational autoencoders. However, these models usually assume simple, uni-modal priors \u2014 such as the multivariate Gaussian distribution \u2014 yet many real-world data distributions are highly complex and multi-modal. Examples of complex and multi-modal distributions range from topics in newswire text to conversational dialogue responses. When such latent variable models are applied to these domains, the restriction of the simple, uni-modal prior hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution. To overcome this critical restriction, we propose a flexible, simple prior distribution which can be learned efficiently and potentially capture an exponential number of modes of a target distribution. We develop the multi-modal variational encoder-decoder framework and investigate the effectiveness of the proposed prior in several natural language processing modeling tasks, including document modeling and dialogue modeling.", "keywords": ["Deep learning", "Structured prediction", "Natural language processing"]}, "meta": {"decision": "Reject", "comment": "This paper explores a variational autoencoder variant.\n \n ICLR gives authors some respect that other conferences don't. It is flexible about the length of the paper, and allows revisions to be submitted. The understanding should be that authors should in turn treat reviewers with respect. The paper should still be finished. Reviewers can't be expected to read a churn of large revisions. The final paper should be roughly the right length, unless with very good reason.\n \n This paper was clearly not finished, and now is too long, with issues remaining. I hope that it will be submitted again, but not until it is actually ready."}, "review": {"HJmyjGmVx": {"type": "rebuttal", "replyto": "HJeBaC-Vl", "comment": "\nThank you for your review\n\n> I find the motivation of the paper suspicious...\n\nIndeed, it may appear that many researchers now focus on multi-modality, but we believe it is an important area of research. In fact, our quantitative results on the document modeling tasks strongly support this claim, where the multi-modal latent variable models significantly outperform the uni-modal Gaussian latent variable models across three different tasks.\n\nIn fact, multi-modality is one of the major motivations for incorporating discrete latent variables into VAEs, which is what at least 3 other ICLR submissions focus on:\n\u201cDiscrete Variational Autoencoders\u201d by Rolfe\nThe Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\u201d by Maddison et al.\n\u201cCategorical Reparameterization with Gumbel-Softmax\u201d by Jang et al.\n\nOf course, our approach differs from these, because our goal is to learn continuous latent variables with multi-modal probability density functions.\n\nFor a lot of NLP applications, there are plenty of intuitive arguments for why one would like multi-modality in the latent variable space. Suppose we have a generative model over sports articles. Some of the generated articles might be related to football and others to hockey. But clearly there exists no sport that lies in between football and hockey (at least not yet!), so if the latent variable contains different regions for football and hockey articles, then there must be a region of low probability mass between them. By definition, such a probability density is multi-modal and cannot be represented by a uni-modal Gaussian variable. In theory, it could be captured in the decoder module which takes as input a uni-modal latent variable sample, but in practice this is going to be very difficult to learn as our experiments have shown.\n\n> the 14 page document can be significantly condensed without loss\n\nWe agree, and will shorten the paper for the camera-ready version (see our response to Reviewer #3 above).\n\n> It was submitted with many significant incomplete details\n\nWe sincerely apologize for the incomplete submission. We did not plan for this, but found bugs in our experiments and had to re-launch several of them just before the deadline, which is why it took a whole week extra before we had the final results.\n\n\n-Alex And Iulian\n", "title": "Rebuttal: Reviewer #1"}, "r1p-nzmVg": {"type": "rebuttal", "replyto": "BJw-pyfVe", "comment": "\nThank you for your detailed review.\n\n> paper is not well-written (even 14 pages long)\n\nWe agree that the paper is too long. We aim to shorten it by moving the mathematical derivations and the equations for the NVDM and VHRED models to the appendix. Hopefully, this will also make the ideas more clear.\n\n> the experiments fails to demonstrate the most of the claims \u2026\n\nWe disagree with this conclusion. We have carried out experiments on four different tasks (20-Newsgroup, RCV1, CADE, and Twitter dialogues), while comparing to the most competitive baseline model (a similar model with multivariate Gaussian latent variables). On all document modeling tasks, we demonstrate significant performance improvements w.r.t. perplexity. On the document modeling tasks we further demonstrate the utility of the piecewise constant variables through word query similarity and gradient analysis. On the dialogue task, we demonstrate their utility through gradient analysis and qualitatively using examples.\n\nWe believe our experimental evaluations are at least as well-designed and conclusive --- if not more conclusive --- compared to several other ICLR submissions. OUR EXPERIMENTS ARE ON COMPLEX, REAL-WORLD TEXT DATASETS, which contain plenty of MULTI-MODAL STRUCTURE. Most of these experiments took several days (sometimes weeks) to execute on machines with TitanX GPUs. Please compare the impact and computational complexity of these experiments to other ICLR submissions such as: \u201cThe Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\u201d and \u201cCategorical Reparameterization with Gumbel-Softmax\u201d. These submissions focus their experiments on MNIST and OMNIGLOT, which their reviewers seem to be satisfied with.\n\n> However, z=<z_gaussian1, z_gaussian2> can be multimodal as well...\n\nAll Gaussian distributions are uni-modal by definition, so concatenating two samples from multivariate Gaussian distributions is still uni-modal. Therefore, z=<z_gaussian1, z_gaussian2> cannot be multi-modal.\n\n> a fair comparison would at least be z=<z_gaussian, z_piecewise> and z=<z_gaussian1, z_gaussian2> which equals to a double sized z_gaussian. \n\nWe did take this into account during hyper-parameter search. For the G-NVDM model, we experimented with up to 200 latent Gaussians variables, but found that this performed worse due to overfitting.\n\n> The results shown in Table 3 are implausible\u2026\n\nResearchers routinely analyse hidden unit activations in neural network models. For example, see Miao et al (2015) and Karpathy et al. (2015). In our case, we were interested in seeing how much changing a word would affect the latent variable models, which is why we compute the gradient w.r.t. the word embedding. Of course, this does not yield quantitative evidence showing how well the model performs on different tasks, but it does illustrate what words the latent variables are sensitive to which, in turn, shows what they have learned to encode.\n\nReferences\n\nMiao et al, Neural Variational Inference for Text Processing. 2015.\n\nKarpathy et al, Visualizing and Understanding Recurrent Networks, 2015.\n\n\n- Alex And Iulian", "title": "Rebuttal: Reviewer #3"}, "HkZQbrvVx": {"type": "rebuttal", "replyto": "SyrSisIVl", "comment": "\n> \"The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz\"\n\nWe did not argue for multi-modality of the output space. Naturally, with a powerful decoder, the marginal p(x) and the conditional p(x|z) could potentially be multi-modal. However, using a Gaussian prior, by definition the latent variable prior p(z) is uni-modal. This could in principle hurt the representations learned. If the latent factors are truly multi-modal, trying to represent them or compress them down to a uni-modal space will act as a strong regularization and make the training process more difficult. That's why our goal is to learn a multi-modal prior p(z).\n\nWe will clarify this distinction in the final version of the paper.\n\n> \"I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood\"\n\nThere are other differences between NVDM and G-NVDM beyond learning the prior parameters. One important difference is the gating mechanism which allows the model to interpolate between the posterior and prior in order to calculate the final generated posterior parameters (beyond minor details such as different activation functions, technical modifications, etc. as mentioned in the paper). This is simply something that we found helped improve performance in preliminary experiments (as well as in some experiments in the past).  We felt that a fairer comparison would be to first improve the baseline model (i.e., the NVDM) and then compare our proposed hybrid models against the improved baseline (as opposed to only/exclusively reporting a previously published as is commonly done). This was especially important given that we intended to jointly learn the priors and use the gating mechanism in the proposed models as well.\n\nThe mission of the paper was to show that the proposed prior improved the encoder-decoder models in the challenging text problems we chose to explore (or at least uncovered interesting information using the piecewise variables).  Many of the models trained in the paper (especially the dialogue models) are fairly expensive to train, and we further felt that reporting various degradations of the models would clutter the paper and detract from focusing on the piecewise variables themselves.  However, we do agree that an ablation test would be appropriate and will add an appendix in the final version exploring the effects of each modification.\n\n> \"... a hypercube-based tiling of latent code space is a sensible idea.\"\n\nWe appreciate this interpretation, and will give it more thought.\n\n> \"I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words\"\n\nThis is exactly what our analysis using gradients was aimed at.\n\nWe will try to visualize the impact of the latent variable components in another way.\n\n> \"The experiments on dialog modeling are mostly negative results\"\n\nThat is not accurate. The standard G-VHRED model is neither better than nor worse than the H-VHRED. Human subjects simply cannot tell them apart.\n\n-Alex And Iulian", "title": "Rebuttal: Reviewer #4"}, "SkgDpKXVx": {"type": "rebuttal", "replyto": "SkmTNVmVl", "comment": "We agree that some of the language in the experiments can be revised to address your concern (and make some of the claims less vague) and will do so for the final version. (See response to second comment as well as these two are related.)\n\n-Alex And Iulian", "title": "RE: experiments"}, "ryvAit74g": {"type": "rebuttal", "replyto": "SkT0lEXNe", "comment": "Yes, in theory, the lower levels of the decoder could potentially learn to represent multi-modal latent variables. However, this is very difficult to learn through SGD optimization. On the other hand, the piecewise constant latent variables can represent multi-modal random variables directly (e.g. for each dimension, the model has n=3 or n=5 specific parameters controlling one multi-modal distribution). This makes it a lot easier to learn multi-modal aspects of the data, which is why the piecewise constant latent variables achieve substantially better results on our 3 document modeling tasks, and the reason why they qualitatively encode different aspects of the data distribution.\n\n-Alex And Iulian", "title": "RE: operations of z"}, "ryySs1gml": {"type": "rebuttal", "replyto": "Sk23pFyQl", "comment": "Hello,\n\nThe focus of the paper is to show that the proposed piecewise variables can benefit neural variational encoder-decoder architectures, such as the NVDM and the VHRED. As such (and to also avoid further clutter/yet more space for our manuscript) we focused on comparing the models with piecewise variables against the improved baseline, the G-NVDM (which encompasses the NVDM -- our target was to beat out the improved competition/baseline, the G-NVDM). The work of Miao et al. (2015) already showed that the NVDM, or rather, neural variational inference in general, was a better approach to document modeling than docNade and the Replicated Softmax model (reinforcing the initial results of Mnih & Gregor (2014)), so we felt that investigation along this line would have been redundant and chose to focus on the effect that using different latent variables had.  We chose to analyze the decoder matrix because in the document models (unlike the VHRED models, where the decoder is not directly connected to the latents), the direct input to the decoder is the generated latent variable state (when conditioned on a given document). As such, examining the synaptic connections connecting latent variables to word/output units can lend some insight towards the \"impact\" the latents have on the outputs in general (this is different than what is learned in the weights connecting the input units to the first hidden layer of the inference network, which would effectively be the word-embeddings). More importantly, we went beyond analyzing only the decoder matrix (since this is still, admittedly, a bit too indirect when measuring impact) and provided an approximate posterior analysis, found in the appendix, to confirm if the piecewise variables themselves were picking up different aspects of the data much as the dialogue encoder-decoder models appeared to.\n\nHowever, we do recognize that yet even further analysis of the piecewise variables learned by our proposed architectures is desirable (on perhaps even lesser-explored document data-sets in order to see what might be uncovered), and as such we will update the manuscript to reflect AnonReviewer3's comment, mentioning that a complementary, focused investigation is the subject of future work. In addition, we will modify the statement that AnonReviewer3 quoted such that it is less severe in its claim.\n\nThanks,\n\nAlex And Iulian", "title": "RE: Word query similarity test"}, "Sk23pFyQl": {"type": "review", "replyto": "BJ9fZNqle", "review": "Why there is no comparison between your models and the baseline momdels (e.g. NVDM, docNADE or Replicated Softmax) in the word query similarity test? The argument \"It is clear that the piecewise variables affect what is uncovered by the model with respect to the data, as each model returns different, but relevant results with respect to the query word\" does not seem to stand. The latent variable represents the document level semantics which is multi-modal, but the evaluation is carried out by only using the decoder parameter matrix which is fixed. Basically, the words are tied to fixed embeddings. How is the multi-modality of the document semantics related to this?This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models. \nThe experiments of neural variational document models and variational hierarchical recurrent encoder-decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues.\nThe idea of having a piecewise constant prior for latent variables is interesting, but the paper is not well-written (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims.  \n\nThe detailed comments are as follows:\n\n--The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1. Hence, a multimodal prior would help the VAEs overcome the issues of optimisation. However, there is a lack of evidence showing the multimodality of the prior helps break the bottleneck. \n\n--In the last paragraph of 6.1, the author claimed the decoder parameter matrix is directly affected by the latent variables. But what the connects the decoder is a combination of a piecewise constant and Gaussian latent variables. No matter what is discovered in the experiments, it only shows z=<z_gaussian, z_piecewise> is multimodal. However, z=<z_gaussian1, z_gaussian2> can be multimodal as well. None of the claims in this paragraph stands.\n\n--In the quantitative evaluation of NVDM, there is an incremental model from z=z_gaussian to z=<z_gaussian, z_piecewise>. As the prior is learned together with the variational posterior, a more flexible prior would alleviate the regularisation imposed by the KL term. Certainly, more parameters are applied as well, so a fair comparison would at least be z=<z_gaussian, z_piecewise> and z=<z_gaussian1, z_gaussian2> which equals to a double sized z_gaussian. \n\n--The results shown in Table 3 are implausible. I cannot believe the author used gradients to evaluate the model. \n\n--Eq. 5 is confusing, adding a multiplication sign might help.\n\n--3.1 can be deleted because people attending ICLR are familiar with VAEs.\n\nTypos:\nas well as the well as the generated prior->  as well as the generated prior", "title": "Word query similarity test", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJw-pyfVe": {"type": "review", "replyto": "BJ9fZNqle", "review": "Why there is no comparison between your models and the baseline momdels (e.g. NVDM, docNADE or Replicated Softmax) in the word query similarity test? The argument \"It is clear that the piecewise variables affect what is uncovered by the model with respect to the data, as each model returns different, but relevant results with respect to the query word\" does not seem to stand. The latent variable represents the document level semantics which is multi-modal, but the evaluation is carried out by only using the decoder parameter matrix which is fixed. Basically, the words are tied to fixed embeddings. How is the multi-modality of the document semantics related to this?This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models. \nThe experiments of neural variational document models and variational hierarchical recurrent encoder-decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues.\nThe idea of having a piecewise constant prior for latent variables is interesting, but the paper is not well-written (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims.  \n\nThe detailed comments are as follows:\n\n--The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1. Hence, a multimodal prior would help the VAEs overcome the issues of optimisation. However, there is a lack of evidence showing the multimodality of the prior helps break the bottleneck. \n\n--In the last paragraph of 6.1, the author claimed the decoder parameter matrix is directly affected by the latent variables. But what the connects the decoder is a combination of a piecewise constant and Gaussian latent variables. No matter what is discovered in the experiments, it only shows z=<z_gaussian, z_piecewise> is multimodal. However, z=<z_gaussian1, z_gaussian2> can be multimodal as well. None of the claims in this paragraph stands.\n\n--In the quantitative evaluation of NVDM, there is an incremental model from z=z_gaussian to z=<z_gaussian, z_piecewise>. As the prior is learned together with the variational posterior, a more flexible prior would alleviate the regularisation imposed by the KL term. Certainly, more parameters are applied as well, so a fair comparison would at least be z=<z_gaussian, z_piecewise> and z=<z_gaussian1, z_gaussian2> which equals to a double sized z_gaussian. \n\n--The results shown in Table 3 are implausible. I cannot believe the author used gradients to evaluate the model. \n\n--Eq. 5 is confusing, adding a multiplication sign might help.\n\n--3.1 can be deleted because people attending ICLR are familiar with VAEs.\n\nTypos:\nas well as the well as the generated prior->  as well as the generated prior", "title": "Word query similarity test", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkiAnw1Qg": {"type": "rebuttal", "replyto": "rktRfIJXx", "comment": "Hello,\n\nThe main difference between the G-NVDM and NVDM is that: 1) the prior mean and variance parameters are learned (by taking the derivative of the variational lower bound with respect to each), and 2) the posterior is calculated as an interpolation (or gating) of both the generated prior and posterior parameters.\n\nOur choice for the above design decisions, for both the document models and dialogue models, is largely experimental as we empirically found that this led to better performance on the data-sets used. We interpret our choice for learning the prior as simply updating the model's general beliefs about the data (especially in the case of document models, where the prior is not conditioned on any context information like the dialogue models) since it might find that a standard Gaussian space is, perhaps, a less-than-reasonable prior. Since calculating the gradients with respect to the prior is rather cheap in a fully-differentiable architecture, we saw no reason not to try updating the prior in an effort to see if the baseline performance could be further improved. However, the interpolation mechanism led to the greatest improvement in performance.\n\nThanks,\n\nAlex And Iulian", "title": "RE: AnonReviewer4 Question"}, "rktRfIJXx": {"type": "review", "replyto": "BJ9fZNqle", "review": "Since it gives such dramatically better results on 20-NG, what is the exact difference between G-NVDM and NVDM? Is it the interpolation/gating between prior and posterior? Or is it the learned prior? Why does learning a variance and mean for the Gaussian prior help, when the latent space is already transformed by an affine transformation by the decoder and encoder networks? It seems that a standard Gaussian prior should have the same expressive capabilities since the posterior should be able to learn to transform itself into the \"whitened\" prior space. The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (\"it cannot possibly capture more complex aspects of the data distribution\", \"critical restriction\", etc).\n\nWhile the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to \"fill up\" parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don't explore this much, a hypercube-based tiling of latent code space is a sensible idea.\n\nAs stated, I found the message of the paper to be quite sloppy with respect to the concept of \"multi-modality.\" There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don't think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data.\n\nI think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables.\n\nAs I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.\n\nThe fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses.\n\nThe experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days.\n\nIn conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.", "title": "Question", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SyrSisIVl": {"type": "review", "replyto": "BJ9fZNqle", "review": "Since it gives such dramatically better results on 20-NG, what is the exact difference between G-NVDM and NVDM? Is it the interpolation/gating between prior and posterior? Or is it the learned prior? Why does learning a variance and mean for the Gaussian prior help, when the latent space is already transformed by an affine transformation by the decoder and encoder networks? It seems that a standard Gaussian prior should have the same expressive capabilities since the posterior should be able to learn to transform itself into the \"whitened\" prior space. The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (\"it cannot possibly capture more complex aspects of the data distribution\", \"critical restriction\", etc).\n\nWhile the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to \"fill up\" parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don't explore this much, a hypercube-based tiling of latent code space is a sensible idea.\n\nAs stated, I found the message of the paper to be quite sloppy with respect to the concept of \"multi-modality.\" There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don't think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data.\n\nI think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables.\n\nAs I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.\n\nThe fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses.\n\nThe experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days.\n\nIn conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.", "title": "Question", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Hk-KPr-Wx": {"type": "rebuttal", "replyto": "B1NoL21-l", "comment": "Hi Christian,\n\nYes, certainly deep latent Gaussian models (and other deep latent variable models) can model multi-modality when we consider their marginal latent variable representations. However, our approach is orthogonal to such models, because we explicitly model multi-modality in a single latent variable (both in the prior and posterior).\n\nIn practice, which model works better is going to be task-dependent. Still, the piecewise latent variables are different because:\n1) they explicitly represent the multi-modality within each latent variable,\n2) they can potentially represent an exponential number of modes (it's not clear that any deep model with continuous variables can do this), and\n3) they only require sampling one layer of latent variables (which reduces the gradient estimate variances, thus making it easier to learn complex latent variable distributions and scale up to larger tasks compared to deeper models).\n\nAlso, thanks for your comments! We really appreciate them, and we are updating our paper based on them!\n\nCheers,\n\nIulian", "title": "Uni-modal Prior"}, "B1NoL21-l": {"type": "rebuttal", "replyto": "SkZoCOyZx", "comment": "Wouldn't a VAE typically use more than one layer of latent variables (\"deep latent Gaussian models\")? Although each conditional distribution is a unimodal (diagonal) Gaussian, the marginals (and joints) of later latent layers are not. I am just trying to understand the motivation behind the idea of using the piece-wise constant density, which is very interesting!\n\nCheers,\nChristian", "title": "Uni-modal Prior"}, "SkGLBnkWx": {"type": "rebuttal", "replyto": "B1ONh_yWe", "comment": "The formalization using distribution theory will (I think) lead to something that is non-zero. So the gradient you obtain by making this approximation will be biased and it is unclear what effect this will have on the results. However, the variational bound is unaffected as you mention.\n\nBest regards,\nChristian", "title": "Differentiability of indicators"}, "SkZoCOyZx": {"type": "rebuttal", "replyto": "SJlipLJZe", "comment": "Hi Christian,\n\n> Also, it is unclear if the paper is actually arguing that VAE learns unimodal generative distributions? The generative distribution that is learnt using a VAE is typically not unimodal, even though the prior p(z) is.\n\nWhen we discuss multi-modality, we are referring to the latent variable space. For example, when a model uses a Gaussian as prior distribution p(z), then the latent variable distribution is uni-modal. In this case, multi-modality can only be represented by the decoder model generating the data x: p(x|z).\n\nCheers,\n\nIulian", "title": "Uni-modal Prior"}, "B1ONh_yWe": {"type": "rebuttal", "replyto": "SJlipLJZe", "comment": "Dear Christian,\n\nI think I understand now. My assumption that the indicator functions in eq. (8) were almost everywhere differentiable was incorrect. As such, we cannot take the derivative outside the expectation in eq. (1) in my last post.\n\nThere might be a mathematically more rigorous argument justifying this, for example using the distribution theorem you mentioned or perhaps by considering the indicator function a limit of a continuous and differentiable function. However, for the sake of this paper I think we can simply consider it an approximation. Whenever we sample one of the two boundary points --- f_1(x) = \\epsilon or f_2(x) = \\epsilon ---  we set the derivative to zero. Still, as I pointed out before, this happens with zero probability.\n\nAlso note this does not affect the validation and testing procedure. The gradients are not required for computing the variational bound itself, hence this is exact.\n\n", "title": "Differentiability of indicators"}, "SJlipLJZe": {"type": "rebuttal", "replyto": "B1WUcqAlg", "comment": "Hi Iulian,\n\nThanks for your input! The point I am trying to make is that changing the order of expectation and derivative is not a straightforward operation in this case. Typically to interchange the order we have to make some assumptions to show that it is valid, http://planetmath.org/differentiationundertheintegralsign explains several common options. Adapted to your problem omega=epsilon and x is your parameters. Normally Thm 2 or 3 is enough to guarantee that this is a valid thing to do. However, as I have shown above the sufficient conditions for Thm 1-3 are not satisfied in your case, the partial derivative of your function f is not even differentiable a.e. with respect to the parameters. It might be possible to salvage the problem by considering distribution theory like in Thm 4, but it is unclear what would fall out and requires further derivation.\n\nNote that my first example above makes use of the fact that f_i(x) = f_i(cx) for all positive scalars c.\n\nAlso, it is unclear if the paper is actually arguing that VAE learns unimodal generative distributions? The generative distribution that is learnt using a VAE is typically not unimodal, even though the prior p(z) is.\n\nBest regards,\nChristian", "title": "Differentiability of indicators"}, "B1WUcqAlg": {"type": "rebuttal", "replyto": "Hk4Ayc0xx", "comment": "Dear Christian,\n\n> Even if we define the derivative of the indicator functions to be zero everywhere but at the point where a change occurs (where it would be undefined), the points where a change occurs in your setup is not measure zero. Assume that the point a is such a point, then any c*a where c > 0, i.e. a positive constant, is also such a point. This basically means we can characterize these as rays which are not measure zero sets.\n\nI believe this argument is incorrect. You are assuming that any point c*a, for c > 0, is a non-differentiable point. This may be true for certain indicator functions (Dirac delta functions), but it is not true for eq. (8).\n\nHere is a more detailed explanation. The indicator functions in eq. (8) have the form:\n\n    1_{f_1(x) \\lt \\epsilon \\lt f_2(x)}\n\nwhere f_1 and f_2 are functions of the parameter x, and where we assume f_1(x) < f_2(x).\n\nOur claim is that almost everywhere this derivative is zero:\n\n    E_{\\epsilon}[d/dx 1_{f_1(x) \\lt \\epsilon \\lt f_2(x)}] = 0,         where \\epsilon is a random variable (1)\n\nWe need to consider three cases before taking the expectation:\n\nCase #1: When the indicator is on:\n\n    f_1(x) < \\epsilon < f_2(x)\n\nIn this case, given any infinitesimal change to f_1(x) or f_2(x), the indicator function is one. Hence, the derivative is exactly zero in expectation.\n\nCase #2: When the indicator is off:\n\n    \\epsilon < f_1(x) < f_2(x) or f_1(x) < f_2(x) < \\epsilon\n\nIn this case, given any infinitesimal change to f_1(x) or f_2(x), the indicator function is zero. Hence, the derivative is exactly zero in expectation.\n\nCase #3: When the indicator is changing:\n\n    f_1(x) = \\epsilon < f_2(x) or f_1(x) < \\epsilon = f_2(x)\n\nIn this case, the derivative is indeed undefined. However, this point has zero probability:\n\n    P(f_1(x) = \\epsilon) = P(f_2(x) = \\epsilon) = 0\n\nThe reason is that \\epsilon is continuous random variable; the point probability of any continuous random variable with a continuous PDF is zero (such as a Gaussian or uniform random variable). So we can safely fix this derivative to zero without affecting the expectation in my equation (1) above. Hope this clarifies things!\n\nCheers,\n\nIulian", "title": "Differentiability of indicators"}, "Hk4Ayc0xx": {"type": "rebuttal", "replyto": "SkJldK0gx", "comment": "Hi Iulian and Alex,\n\nEven if we define the derivative of the indicator functions to be zero everywhere but at the point where a change occurs (where it would be undefined), the points where a change occurs in your setup is not measure zero. Assume that the point a is such a point, then any c*a where c > 0, i.e. a positive constant, is also such a point. This basically means we can characterize these as rays which are not measure zero sets. Removing this scenario by constraining \\sum_i a_i = 1 we can take another example: For a fix epsilon we can find a value a_1 such that we end up on the boundary between e.g. i=1 and 2. Now because a_i for i>1 can be arbitrary values between 0 and 1 that sums to one, we again have a set that is not measure zero.\n\nBest regards,\nChristian", "title": "Differentiability of indicators"}, "SkJldK0gx": {"type": "rebuttal", "replyto": "Sk7DiL0ex", "comment": "Hello,\n\nThe Dirac delta functions are differentiable almost everywhere. The only non-differentiable point is the switch from 0 to 1. However, this happens with zero probability, or rather, the probability of sampling that exact point is zero. In implementation, we set the gradients at these switching points to zero. So in expectation our gradients are correct.\n\nThanks,\nIulian And Alex", "title": "RE: Differentiability of the reparameterization"}, "Sk7DiL0ex": {"type": "rebuttal", "replyto": "BJ9fZNqle", "comment": "The reparameterization does not seem to be differentiable? I.e. eq. (8) consists of indicator functions of the parameters you are optimizing with respect to.", "title": "Differentiability of the reparameterization"}, "HJoHAzRge": {"type": "rebuttal", "replyto": "r1j28MRex", "comment": "Hello there, \n\nWe understand you may feel it is unfair to submit a paper without the final results.\n\nThe reason our results tables are still incomplete is because we found problems in our experiments a few weeks ago. We have been working hard to fix them since then. The tasks we are working on are computationally expensive. For example, on the Twitter task discussed in the paper, it takes over two weeks to train up our model and baselines on a TitanX GPU (compare this to the MNIST or sequential MNIST tasks, which other ideas related to VAEs are typically evaluated on). Since ICLR explicitly encourages updates/revisions of papers and since the majority of the work has already been done (the paper has been written and revised already, experiments are almost complete), we believe it is acceptable to upload the remaining results past the deadline within a short period of time.\n\nCheers,\n\nIulian And Alex", "title": "RE: Is is ok to submit an incomplete paper?"}, "r1j28MRex": {"type": "rebuttal", "replyto": "BJ9fZNqle", "comment": "This paper is incomplete. Most of results are blank.  What is the meaning of \"table XXXX\"? \nSuch strategy seems unfair....\n\nHowever, I agree that methods are good.  This paper should be submitted to ICML or workshop in ICLR.\n\nIf this type of method is allowed, I would wonder the credibility of papers in this conference. \n\n", "title": "Is is ok to submit an incomplete paper?"}}}