{"paper": {"title": "RenderGAN: Generating Realistic Labeled Data", "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"], "summary": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper."}, "review": {"HJ1GfUvvg": {"type": "rebuttal", "replyto": "rkTJXsyDe", "comment": "Thank you for your response. You proposed two baselines, a cGAN version and an\nunconstrained GAN. As we previously explained (see general rebuttal letter),\nthese are no viable or credible baselines.\n\nAs you mentioned correctly, the cGAN approach requires labeled data. We do not\nhave such a dataset. Annotating enough data for this approach to represent\na credible baseline would be prohibitively expensive. Remember that we have\nmultiple labels such as 3D orientation and 12 binary labels for each sample.\nAnnotating one sample can easily take 2 minutes. If we would annotate 20 samples\nfor every possible ID, it would take a person 341 days. If we would have this\nkind of labeled data, we would directly use it to train the decoder network. The\nscarcity of labeled data is the main motivation for our approach and we think\nthere exist many similar problems that can be solved with the RenderGAN\napproach.\n\nSecondly, you proposed an \u201cunconstrained\u201d generator network as a baseline. We\nargue in our rebuttal:\n\n> One of our early approaches was to add an offset to the 3D model, i.e.\n> x = t + g(t) where x is the synthesized image, t is an image from the 3D model,\n> and g an unconstrained generator. However, in our experiments, the generator\n> learned to synthesize realistic images but ignored the given template\n> t completely. Thus, no valid labels of the synthetic images could be collected.\n\nAgain, we cannot use a generic GAN to add missing image characteristics. This\nintuitive notion is incorrect. The GAN generates images that deviate from the\ninput labels. The images might display a different ID or a different spatial\norientation compared to the input labels. Our experiments then led us to the\nidea of introducing constrained augmentations that ensure that the labels remain\nvalid in the output image. This is the central idea of the paper and as far as\nwe see a straight-forward solution to the problem stated above.\n", "title": "Answer to about generic GAN"}, "B16Sw-BUl": {"type": "rebuttal", "replyto": "BkGakb9lx", "comment": "We uploaded a new version of the paper based on the peer review feedback.\n\n* Clarified that an unconstrained GAN is not a suitable baseline.\n* Added additional references pointed out by reviewer 2.\n* Extracted a related work section to improve the overall clarity and also stated our contribution explicitly.\n* Various modifications to improve clarity.", "title": "Updated paper"}, "HJDMVfhSl": {"type": "rebuttal", "replyto": "HkosYQzEx", "comment": "Thank you very much for your review. Your comments have helped to improve the\npaper.  We think your main point of critique was that we did not evaluate our\nsystem sufficiently:\n\n> \u201cHowever, I won\u2019t champion the paper as the evaluation could be improved.\n> A critical missing baseline is a comparison against a generic GAN. Without this\n> it\u2019s hard to judge the benefit of the more structured GAN. \u201c\n\nSince similar comments were made by the other reviewers, we formulated a general\nresponse to this point. Please refer to the \u201cgeneral rebuttal\u201d in which we give\na detailed rationale of why a generic GAN cannot be used as a baseline as you\n(and others) suggested. We think this is a crucial point representing the\ncentral motivation of the RenderGAN method.\n\n> \u201cAlso, it would be worth seeing the result when one combines generated and\n>  real images for the final task.\u201d\n\nIn previous experiments, we used a mixture of real vs. generated data (5/95 and\n50/50). The real data was augmented to prevent overfitting as described in\nAppendix C in the paper. Both experiments did not result in significant\nimprovements, and we therefore decided not to include them in the paper. We\nagree that this might be interesting and we will update the manuscript\naccordingly.\n\nThank you for pointing out additional references. We will include both of them in our next update.\n\n> \u201cThe problem domain (decoding barcode markers on bees) is limited.  It would\n> be great to see this applied to another problem domain, e.g., object detection\n> from 3D models as shown in paper reference [A], where direct comparison against\n> prior work could be performed. \u201c\n\nWe are currently working on more popular problem domains. We, however, feel\nthat, although the problem has limited applicability for the general reader, it\nnonetheless represents a complex problem with analogies to other, potentially\nmore popular problems, like e.g. pose estimation for cars or faces.\n\n> \u201cI found the writing to be somewhat vague throughout.  For instance, on first\n> reading of the introduction it is not clear what exactly is the contribution of\n> the paper.\u201d\n\nThank you for this feedback. We are currently revising the manuscript to improve\noverall clarity and will update the paper in the following days.\n\nThank you also for your minor comments, the manuscript will be updated with\nappropriate changes.\n\n> \u201cFig 3 - Are these really renders from a 3D model? The images look like 2D\n> images, perhaps spatially warped via a homography.\u201d\n\nYes, the bee marker is represented by a 3D mesh which is rotated in 3D space\naccording to the parameters and then projected to the image plane. Here is\na link to the code (https://goo.gl/9UWuhi). This is a very simple 3D model and\nwe believe this is one of the advantages of the RenderGAN method. Only\na rudimentary object and camera model are required. All the rest of the imaging\nprocess, lighting, shadows, blur, reflections, compression artifacts, etc. is\nlearned by the RenderGAN.\n\n> \u201cPage 3: \"chapter\" => \"section\".\u201d\n \nChanged accordingly.\n\n> \u201cIn Table 2, what is the loss used for the DCNN?\u201d\n\nThe loss is the mean log-loss for the classification of the individual bits of\nthe barcode marker. We will include this in the caption of the figure in the\nnext revision.\n\n> \u201cFig 9 (a) - The last four images look like they have strange artifacts. \n> Can you explain these?\u201d\n\nWe mentioned them briefly in the text (Section 3, Training):\n\n> In some cases, the generator creates artifacts that destroy the images. As\n> these bad images are scored unrealistic by the discriminator without exception,\n> we can discard them for the training of the supervised algorithm.\n\nWe suspect that during training the generator can deceive the discriminator by\nproducing out of distribution samples i.e. very unrealistic images. The\ndiscriminator learns quickly to score them as fake, and now the gradient for the\ngenerator might be too small to escape this local minimum. However, this is only\na hypothesis which we did not investigate further.  The images with the strange\nartifacts are rated unrealistic by the discriminator without exception.\nTherefore, we can sort them out before training the DCNN.\n", "title": "Rebuttal Reviewer 2"}, "rJJxNM3Bx": {"type": "rebuttal", "replyto": "SJVdBxG4x", "comment": "\nThank you very much for your review. You summarized our work well, and your\nfeedback contributed significantly to improving our paper.\n\n> As Reviewer3 points out, it would be interesting to analyze if restricting GAN\n> to a fixed set of transformations is necessary here, [...].\n\nPlease see our general rebuttal for a detailed explanation why the\ntransformations are necessary and cannot be left out. We will revise the\nmanuscript to make this point clearer.\n\n> The downside is that experiments are limited to a fairly simple and\n> not-widely-known domain of honeybee marker classification.\n\nWe agree, there are more complex datasets e.g. ImageNet. However, our problem,\ni.e. extraction of the marker\u2019s ID and rotation in space, was unsolvable with\nstate-of-the-art supervised learning methods due to the time-consuming nature of\nmanual labeling.  We are currently working on more popular problem domains. We,\nhowever, feel that, although the problem has limited applicability for the\ngeneral reader, it nonetheless represents a complex problem with analogies to\nother, potentially more popular problems, like pose estimation for cars or\nfaces.\n\n> The authors should tone down their claims such as \u201cOur method is an improvement\n> over previous work  <...> Whereas previous work relied on real data for training\n> using pre-trained models or mixing real and generated data, we were able to\n> train a DCNN from scratch with generated data that performed well when tested on\n> real data. \u201c. This is not a fair comparison: the domain studied by authors in\n> this work is much simpler than what was studied in these previous works, so this\n> comparison is not appropriate.\n\nWe fully agree and will change the paragraph accordingly.\n", "title": "Rebuttal Reviewer 1"}, "ByLcQf3Sx": {"type": "rebuttal", "replyto": "HJ-3KJzEe", "comment": "\nThank you very much for your review. Your pre-review questions already helped to\nimprove the paper. However, we disagree with several points raised in your\nreview.\n\n> The main novelty are parametric modules that emulate different transformations\n> and artefact that allow to match the natural appearance.  [...] the proposed\n> method is more model driven that previous GAN models. But does it pay off? how\n> would a traditional GAN approach perform?  [...] The key proposal of the\n> submission seems parameterised modules that can be trained to match the real\n> data distribution. but it remains unclear why not a more generic\n> parameterisation can also do the job. E.g. a neural network - as done in regular\n> GANs.\n\nWe disagree with your summary that our main contribution are augmentation\nfunctions that allow a GAN to learn the real data distribution. In fact, the\naugmentation functions are specifically designed to constrain the generative\ncapabilities of the model. A generic GAN cannot be used as a baseline for our\nmethod. Please refer to our general rebuttal for a more thorough reply.\n\n> Using a render engine to generate the initial sample appearance if of limited novelty.\n\nThe point of using a render engine is not to generate an initial appearance but\nto allow collecting a _labeled_ sample that subsequently is rendered more\nrealistic by a constrained GAN. To the best of our knowledge, this has never\nbeen done before.\n\n> The answers of the authors only partially addresses the point.\n\nThis is incorrect. We explained in our pre-review reply that a generic GAN\ncannot be used as a baseline. Furthermore, we elaborated on this point in our\ngeneral rebuttal. Could you please clarify your point if you feel that we still\ndidn\u2019t address your questions completely?\n\n> The authors reply that plenty of such augmentation was used and more details are\n> going to be provided in the appendix. it would have been appreciated if such\n> information was directly included in the revision - so that the procedure could\n> be directly checked. right now - this remains a point of uncertainty.\n\nIt was already included in the revision from 9 Dec 2016 which is based on your\npre-review feedback (see appendix C page 14).\n\n> The authors do evaluate the effect of hand tuning the transformation stages vs.\n> learning them. it would be great to also include results of including/excluding\n> stages completely\n\nWe agree, excluding different transformation stages would shed light on the\nrelative importance of each stage, and this would be an interesting aspect. We\nwill try to include this in the manuscript by the submission deadline. For now,\nthe differences in performance obtained by hand-designed and learned\naugmentations can be used as a rough proxy for the importance of the individual\naugmentations. For example, there is a large improvement in validation\nperformance when replacing the hand-designed lighting augmentation with\na learned one. We added a respective remark to the latest revision (9 Dec).\n\n> - and also reporting how much the initial jittering of the data helps.\n\nIt is not clear to us what you mean with initial jittering of the data. Would\nyou mind clarifying this point?\n", "title": "Rebuttal Reviewer 3"}, "SJ9NXznSe": {"type": "rebuttal", "replyto": "BkGakb9lx", "comment": "\nThank you very much for your reviews. Your feedback helped to improve the\nmanuscript significantly, and we are preparing a revised version of the\nmanuscript with changes outlined either below or in our responses to each\nreviewer. Multiple valid points of criticism were raised during the review\nprocess and have already been worked into the current version of the document.\nFor example, we included hand-designed augmentations for comparison with the\nlearned ones.\n\nHowever, in two of the three reviews there seems to be a major misunderstanding\nthat we would like to clarify here. Since this relates to the central finding of\nour paper, we would like to provide a detailed response to this point. We hope\nthat, in the light of this fact, the reviewer\u2019s rating of our contribution\u2019s\nimportance and novelty will be reconsidered.\n\n> Reviewer 2: \u201cA critical missing baseline is a comparison against a generic GAN.\n> Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also,\n> it would be worth seeing the result when one combines generated and real images\n> for the final task.\u201d\n\n> Reviewer 3: \u201c [...] the proposed method is more model driven that previous GAN\n> models. But does it pay off? how would a traditional GAN approach perform? [...]\n> The answers of the authors only partially addresses the point. The key proposal\n> of the submission seems parameterised modules that can be trained to match the\n> real data distribution. but it remains unclear why not a more generic\n> parameterisation can also do the job. E.g. a neural network - as done in regular\n> GANs. The benefit of introducing a stronger model is unclear.\u201d\n\nThe main point of critique here is that a comparison with a generic GAN\n(Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN\nand RenderGAN) share the same task domain, which is incorrect. The task we\naddress is generating _labeled_ data. We emphasize that we do not refer to the\nbinary class label but rather to higher dimensional labels. In our example\nscenario, this corresponds to images of bee markers and their respective bit\nconfiguration (its ID) and rotations in 3D space. A generic GAN cannot generate\nlabels, it learns to generate realistic images _without_ labels. Ultimately, we\nwant to train a convnet (\u2018decoder network\u2019) in a supervised setting to map an\nimage to its respective labels. Thus, we need labeled samples and hence,\na conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN\nsamples from the joint distribution p(l, x) of labels l and data x whereas\na conventional GAN can only sample from the data distribution p(x).\n\nThere are two alternative approaches to our RenderGAN, one being a conventional\n3-dimensional rendering pipeline that can be used to generate images of bee\nmarkers with known ID and spatial orientation. Secondly, one could train the\ndecoder network with manually labeled data. Both approaches have been\nimplemented and tested against the RenderGAN and do not perform satisfyingly. To\nimprove both alternatives\u2019 performance one would need to either tune the\nrendering pipeline to match the details of the real world imaging process, or\nlabel more data manually. Both measures are time-consuming and do not generalize\nwell when changing parts of the imaging process (lighting, cameras, compression,\netc.) or the marker design.\n\nOur approach is to extend a generic GAN by adding several network modules, the\nfirst being the network equivalent of a simple 3D model. Secondly, we learn\na number of parameterised augmentation functions. We would like to point out\nthat this approach was _not_ chosen to improve the generative capabilities of\nthe network but to constrain it in such a way that the image produced by the GAN\nis correct with respect to the labels fed into the network. In our use case,\neach image produced by the GAN has to preserve the given bit pattern and\nrotation in space provided by the 3D model for the labels to remain valid. This\npoint was already addressed in our paper and the pre-review questions:\n\n> Paper Introduction: \u201c[...] We constrain the augmentation of the images such that\n> the high-level information represented by the 3D model is preserved. The\n> RenderGAN framework allows us to generate images of which the labels are known\n> from the 3D model, and that also look strikingly real due to the GAN framework.\n> The training procedure of the RenderGAN framework does not require any labels.\n> We can generate high-quality, labeled data with a simple 3D model and a large\n> amount of unlabeled data.\u201d\n\n> Our reply to Reviewer 3: \u201c[...] The payoff is that we can generate labeled data\n> with only a simple 3D model and unlabeled data. You are right. A DCGAN\n> architecture can model all mentioned effects, even affine transformations. We\n> trained a DCGAN on the data, and the quality of the synthesized images is\n> similar. However, no labels can be collected in the conventional GAN framework.\n> [...]\u201d\n\nAll reviewers question the necessity of the constraints we introduced. One of\nour early approaches was to add an offset to the 3D model, i.e. x = t + g(t)\nwhere x is the synthesized image, t is an image from the 3D model, and g an\nunconstrained generator. However, in our experiments, the generator learned to\nsynthesize realistic images but ignored the given template t completely. Thus,\nno valid labels of the synthetic images could be collected. Since a decoder\nnetwork cannot be trained without labels, this approach cannot be used as\na baseline. We will revise our paper to clarify that an unconstrained GAN is not\na suitable baseline for our task.\n", "title": "General Rebuttal"}, "ryl8W4umg": {"type": "rebuttal", "replyto": "BkGakb9lx", "comment": "We updated our paper based on the feedback from the pre-review questions.  We\nincluded handmade augmentation in the evaluation.  We also retrained the DCNN on\nthe real data. Thanks for the feedback.", "title": "Update paper"}, "Hk2sFXQQe": {"type": "rebuttal", "replyto": "S1NOaOkXl", "comment": "> the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\n\nThe payoff is that we can generate labeled data with only a simple 3D model\nand unlabeled data. You are right. A DCGAN architecture can model all mentioned\neffects, even affine transformations. We trained a DCGAN on the data, and\nthe quality of the synthesized images is similar. However, no labels can be collected in the conventional GAN framework. The latent space is too entangled, i.e. there is no simple relationship of the labels to the latent space. A conditional GAN can be used to generated labeled data but also requires labeled data for training.\n\n\n> how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\n\nIs your question about augmenting the images of the 3D model or the real data?\nFor the 3D model, see our answer to question 2. from reviewer number 1.\nThe real data that is used for training is highly augmented. This is mentioned in the result section (random translations, rotations, shear transformations, histogram scaling, and noise). We will include the specific parameters used to augment the data in the appendix. \n\n\n> How do the different stages (\\phis) effect performance? Which are the most important ones?\n\nWe agree that this should be addressed in the paper. We are currently evaluating the effects of the different stages on the network performance by replacing learned augmentations (\\phis) with hand-designed augmentations (see also our reply to reviewer 1 / question 2). We will update the paper shortly to include this evaluation.\n", "title": "pre-review answer"}, "rkNPFmmme": {"type": "rebuttal", "replyto": "ByZazmk7x", "comment": "> 1. Would it make sense to use a differentiable renderer (like OpenDR by Loper & Black) in your framework ?\n\n\nThanks for the suggestion. Yes, it would make sense. In future work, this could make it possible to partially infer the 3D model from data.\n\n\n> 2. Have you tried a baseline approach to synthetic data generation with simple parametric distributions of orientations and augmentations, hand-designed or estimated with simple statistics? Basically, do you really need a GAN to  learn them in this relatively simple scenario, or can a human quickly make up   a reasonable distribution of transformations which will perform as well?\n\n\nWe initially tried a very basic approach to augment the 3D model data with only noise and shifting of the pixel intensities. A model trained on this data did not generalize to real data.\nBased on your suggestion, we implemented hand designed augmentations similar to the ones learned by the GAN. Preliminary results indicate a network trained on this data overfits much more than when trained on the GAN data. We will update the paper shortly. \n\n\n> 3. Do you think your work is related to Style and Structure Adversarial Networks\n   https://arxiv.org/pdf/1603.05631.pdf , Spatial Transformers\n   https://arxiv.org/abs/1506.02025 ?\n\n\nIn the Style and Structure Adversarial Networks paper, a GAN is adapted to not only generate\nimages but also to capture the structure representation in the form of a normal map.\nContrary to our work, their training procedure requires large amounts of labeled data.\nThey rely on 200K normal maps and matching real images from\nthe NYU Depth Dataset V2.  Still, Wang and Gupta successfully learn to separate\nstyle from structure. Something we are also able to achieve with the RenderGAN\nframework. The 3D model would be the structure and the augmentations the style.\nWe think this work is related to our work and we should mention it.\n\nSpatial Transformers (STs) learn an affine transformation of\nconvolutional feature maps. It was shown that this can improve the performance\non rotated or otherwise deformed data.\nOur approach is only related to STs such that STs could\nbe used as an augmentation function. For example, in our application the 3D model directly incorporates\ntranslation and rotation, it should instead be possible to learn them with a ST.\nIn future work, ST could help to reduce the complexity of the 3D model.\n\n\n> 4. Have you had a chance to run experiments on other datasets?\n\nCurrently, we are investigating to apply the RenderGAN framework on a different\ndataset but we have no results yet.", "title": "pre-review answer"}, "S1NOaOkXl": {"type": "review", "replyto": "BkGakb9lx", "review": "- the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\n- how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\n- How do the different stages (\\phis) effect performance? which are the most important ones?The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.\nThe main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.\n\nseveral points were raised during the discussion:\n\n1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\nThe answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.\n\n\n2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\nThe authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.\n\n3. How do the different stages (\\phis) effect performance? which are the most important ones?\nThe authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.\n\nWhile there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJ-3KJzEe": {"type": "review", "replyto": "BkGakb9lx", "review": "- the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\n- how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\n- How do the different stages (\\phis) effect performance? which are the most important ones?The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.\nThe main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.\n\nseveral points were raised during the discussion:\n\n1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\nThe answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.\n\n\n2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\nThe authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.\n\n3. How do the different stages (\\phis) effect performance? which are the most important ones?\nThe authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.\n\nWhile there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByZazmk7x": {"type": "review", "replyto": "BkGakb9lx", "review": "Hello,\n\nAn interesting idea! A couple of questions:\n\n1. Would it make sense to use a differentiable renderer (like OpenDR by Loper & Black) in your framework ?\n\n2. Have you tried a baseline approach to synthetic data generation with simple parametric distributions of orientations and augmentations, hand-designed or estimated with simple statistics? Basically, do you really need a GAN to learn them in this relatively simple scenario, or can a human quickly make up a reasonable distribution of transformations which will perform as well ?\n\n3. Do you think your work is related to Style and Structure Adversarial Networks https://arxiv.org/pdf/1603.05631.pdf , Spatial Transformers https://arxiv.org/abs/1506.02025 ?\n\n4. Have you had a chance to run experiments on other datasets? \n\nThanks.The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. \n\nThe topic of the paper \u2014 using machine learning (in particular, adversarial training) for generating realistic synthetic training data \u2014 is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.\n\nI appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.\n\nAs Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.\n\nThe authors should tone down their claims such as \u201cOur method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. \u201c. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJVdBxG4x": {"type": "review", "replyto": "BkGakb9lx", "review": "Hello,\n\nAn interesting idea! A couple of questions:\n\n1. Would it make sense to use a differentiable renderer (like OpenDR by Loper & Black) in your framework ?\n\n2. Have you tried a baseline approach to synthetic data generation with simple parametric distributions of orientations and augmentations, hand-designed or estimated with simple statistics? Basically, do you really need a GAN to learn them in this relatively simple scenario, or can a human quickly make up a reasonable distribution of transformations which will perform as well ?\n\n3. Do you think your work is related to Style and Structure Adversarial Networks https://arxiv.org/pdf/1603.05631.pdf , Spatial Transformers https://arxiv.org/abs/1506.02025 ?\n\n4. Have you had a chance to run experiments on other datasets? \n\nThanks.The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. \n\nThe topic of the paper \u2014 using machine learning (in particular, adversarial training) for generating realistic synthetic training data \u2014 is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.\n\nI appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.\n\nAs Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.\n\nThe authors should tone down their claims such as \u201cOur method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. \u201c. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}