{"paper": {"title": "Neural Combinatorial Optimization with Reinforcement Learning", "authors": ["Irwan Bello*", "Hieu Pham*", "Quoc V. Le", "Mohammad Norouzi", "Samy Bengio"], "authorids": ["ibello@google.com", "hyhieu@google.com", "qvl@google.com", "mnorouzi@google.com", "bengio@google.com"], "summary": "This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning.", "abstract": "This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent neural network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent neural network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items. These results, albeit still far from state-of-the-art, give insights into how neural networks can be used as a general tool for tackling combinatorial optimization problems.", "keywords": ["Reinforcement Learning", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "This was one of the more controversial submissions to this area, and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though, even after these consideration, none of the reviewers strongly supported the work and all of them expressed some reservations. \n \n Pros:\n - All agree that the work is extremely clear, going as far as saying the work is \"very well written\" and \"easy to understand\". \n - Generally there was a predisposition to support the work for its originality particularly due to its \"methodological contributions\", and even going so far as a saying it would generally be a natural accept.\n \n Cons:\n - There was a very uncommonly strong backlash to the claims made by the paper, particularly the first draft, but even upon revisions. One reviewer even saying this was an \"excellent example of hype-generation far before having state-of-the-art results\" and that it was \"doing a disservice to our community since it builds up an expectation that the field cannot live up to\" . This does not seem to be an isolated reviewer, but a general feeling across the reviews. Another faulting \"the toy-ness of the evaluation metric\" and the way the comparisons were carried out.\n - A related concern was a feeling that the body of work in operations research was not fully taken account in this work, noting \"operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality\". The authors did fix some of these issues, but not to the point that any reviewer stood up for the work."}, "review": {"rkeiFaBDx": {"type": "rebuttal", "replyto": "H1SVk-MVx", "comment": "We thank the reviewer for such a detailed review and several constructive comments which significantly improve the quality of the paper. We updated the paper to address all comments. Specifically:\n\n- \u201c... The authors just need to make it crystal clear that, as of now, their method is still very far away from the state of the art...\u201d\n\nWe updated the abstract, introduction, and conclusion to clearly emphasize that the results are far from the state-of-the-art. We also modified the statement on the performance of greedy approaches: instead of stating that they are \u201cjust a few percents from optimality\u201d, we express that they are \u201cstill quite far from optimality\u201d\n\n- \u201c ...that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy)...\u201d\n\nThe removal of the dynamic programming (with quantization) approach was an editorial mishap. We meant to only remove the mention of ExpKnap and MinKnap (for brevity since this section was introduced to illustrate the flexibility of our method) but we mistakenly also removed the mention of the optimal dynamic programming approach, using which we obtained optimal solutions in our experiments. Following your suggestion, we have added all of the baselines back into the paper.\n", "title": "Rebuttal to the update"}, "rk8K_pHPl": {"type": "rebuttal", "replyto": "rJY3vK9eg", "comment": "We ask reviewers to have a look at the new version of the paper again given the changes outlined below:\n\n- We state clearly in the abstract, introduction, and conclusion that our results are still far from the state-of-the-art (this includes adding an updated version of Figure 1 back into the introduction).\n\n- We include the original KnapSack baselines back into the paper.\n\n- We explain in details how the running time of the LKH baseline is obtained.\n\n- We modify the statement on the performance of greedy approaches: instead of stating that they are \u201cjust a few percents from optimality\u201d, we express that they are \u201cstill quite far from optimality\u201d.\n\nWe thank reviewers for their help in improving the quality of the paper.", "title": "Summary of new changes to the paper"}, "SyP15KSIx": {"type": "rebuttal", "replyto": "S1-I2vhSx", "comment": "Please find replies to your questions and comments below.\n\n- \"Do there exist many (or any) problems that are both interesting and have not been, and cannot be, addressed by the existing combinatorial optimization community?\"\n\nTasks that require perceptual reasoning about the input objects seem particularly suitable for machine learning techniques. An example of tasks that are difficult to tackle using standard combinatorial optimization approaches is combinatorial optimization over neural architectures (e.g. see https://arxiv.org/abs/1611.01578).\n\n- \"If every worthwhile problem has \"highly optimized\" solutions, what good is your work?\"\n\nWhen encountering a (new) combinatorial optimization problem, selecting an appropriate heuristic and search method can be a time consuming process requiring expert knowledge. This paper is a step toward automating this process by proposing a machine learning framework that works reasonably well in a wide range of settings.\n\n- \"Please stop calling existing TSP solvers such as concorde a heuristic. Concorde produces solutions which are provably correct.\"\n\nWe clearly state in the paper that Concorde is an exact solver and that it solves input graphs to optimality when it terminates. A heuristic may refer to a particular branching decision to speed up a search algorithm that guarantees optimality.\n\n- \"Concorde produces solutions which are provably correct [...] From a practical perspective, this is an important distinction; I don't see why anyone would choose the latter when given the choice.\n\nPractitioners usually care about running time, performance, robustness, ease of use, but they may not worry too much about exact optimality. This preference is further accentuated by the mismatch that the modeling process introduces between the reality and the combinatorial optimization problem being solved.\n\n- \"Also in the related work - you say it solves cities with \"thousands of cities\" when it has solved a 85k problem.\"\n\nThis sentence does not specifically refer to Concorde. The 85k instance took about 136 CPU years according to the authors, which is not the typical scenario.", "title": "Answers"}, "S1-I2vhSx": {"type": "rebuttal", "replyto": "rJY3vK9eg", "comment": "I posted this question in a response below, but it seems to be getting ignored so I thought I'd bring it to the top, with some additional points.\n\nThanks for the update. The natural question to ask, then is - do there exist many (or any) problems that are both interesting and have not been, and cannot be, addressed by the existing combinatorial optimization community? You knock existing algorithms for being \"highly optimized\" to particular problems, but if every worthwhile problem has \"highly optimized\" solutions, what good is your work? \n\nAlso, please stop calling existing TSP solvers such as concorde a heuristic. Concorde produces solutions which are provably correct. Your approach does not, nor is it remotely close. From a practical perspective, this is an important distinction; I don't see why anyone would choose the latter when given the choice. The second paragraphs of the related work and introduction are guilty of this. Also in the related work - you say it solves cities with \"thousands of cities\" when it has solved a 85k problem. \n\nI'd also echo concerns about the toy-ness of the evaluation metrics here - 100 cities is 800x smaller than existing SOTA of 85k from TSPLib - a gap made exponentially larger by the combinatorial nature of the problem.\n\n", "title": "Question"}, "BkOd1Z3Hg": {"type": "rebuttal", "replyto": "SJKhbIlSe", "comment": "We thank the reviewer for the review and appreciate their interest in the method and its novelty. \n\n1) We have added a discussion on how to tackle problems with complex constraint structures in Section 6 (Generalization to other problems). A simple approach, whose efficiency needs to be empirically demonstrated, consists in penalizing the network for violating constraints by having negative rewards.\n\n2) Randomly generated instances with cities sampled uniformly in the unit square (usually referred to as random euclidean TSP instances) are also a historically standard testbed for algorithms [1]. While applying NCO to the TSPLib benchmarks would add to the paper, we have already run a great number of experiments on relatively large test sets (a thousand graphs per task) so we leave it as future work.\n\n3) See Critic\u2019s architecture for TSP in Section 4\n\n4) We updated the notation.\n\n5) Idem.\n\n[1] The Traveling Salesman Problem: A Case Study in Local Optimization. (See page 11 on Standard Test Instances)\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.434&rep=rep1&type=pdf", "title": "Rebuttal"}, "rk6ol-nHg": {"type": "rebuttal", "replyto": "r1PQL5gEx", "comment": "We thank the reviewer for the feedback and appreciate their interest in the method and its novelty. \n\nWe have added complexity comparisons based on precise running time evaluations for all of the methods. We also experimented with a new method at inference time, called RL pretraining-Greedy@16, that balances speed with performance. RL pretraining-Greedy@16 decodes greedily from a set of 16 models and returns the shortest tour found. It runs in similar time to the state-of-the-art solvers with a relatively small degradation in performance.\n\nRegarding the practicality of our method, we emphasize that this is a new approach to combinatorial optimization. In the history of deep learning, especially computer vision, speech recognition, or machine translation, the first proposals tend to be worse in some dimensions against the state-of-art methods, and it takes many years until they fulfill all of their promise. We believe that our proposal is a first step in using neural networks\u2019 recent advancements for combinatorial optimization and expect that future contributions from the field will make it even better (cf greedy@16 for example).\n\nIn addition, our method operates at a higher level of generality than the highly specific solvers we compare against. As partly demonstrated, it can be applied to many optimization problems, which is a strong advantage for the proposed framework.", "title": "Rebuttal"}, "H1_elbnBg": {"type": "rebuttal", "replyto": "H1SVk-MVx", "comment": "We thank the reviewer for their comments and feedback.\n\nIn the first drafts, Figure 1 compared our best method to optimality and Christofides. We later changed the Christofides baseline to a simple generic local search baseline because of a pre-review question and the argument that the simplest local search operators were simple enough to not be considered as specific as the highly optimized Concorde solver. We agree that the previous \u201clocal search\u201d term in the legend of Figure 1 could be ambiguous to the non-attentive reader due to many possible interpretations of local search. To address this issue, we removed Figure 1 as it was not central to the paper. Note that we did not claim to outperform the strongest TSP solvers as we presented the results from such solvers outperforming our technique. Regarding the computation efficiency of our methods, please review the new figures.\n\nWe appreciate the reviewer\u2019s interest in the method and its novelty. We strongly hope that the review will update their rating given that we addressed their concerns.", "title": "Rebuttal"}, "HJLwAenHl": {"type": "rebuttal", "replyto": "rJY3vK9eg", "comment": "We thank reviewers for their valuable feedback that helped us improve the paper. We appreciate their interest in the method and its novelty. We have made several changes to the paper which are summarized below. We ask reviewers to evaluate the new version of the paper and adjust their reviews if necessary.\n\n1) Previous Figure 1, which was problematic due to different possible interpretations of \u201clocal search\u201d was removed.\n\n2) We added precise running time evaluations for all of the methods in the paper. Table 3 presents running time of the RL pretraining-greedy method and the solvers we compare against. Table 4 presents the performance and corresponding running time of RL pretraining-Sampling and RL pretraining-Active Search as a function of the number solutions considered. It shows how they can be stopped early at the cost of a small performance degradation. Table 6 contains the same information for the metaheuristics from OR-Tools vehicle routing library solver. We controlled the complexity of these approaches by letting all of them evaluate 1,280,000 solutions. Section 5.2 was rewritten in light of the new results.\n\n3) We experimented with a new approach, called RL pretraining-Greedy@16, that decodes greedily from 16 different pretrained models at inference time and selects the shortest tour. It runs as fast as the solvers while only suffering from a small performance cost.\n\n4) We added a discussion in Section 6 (Generalization to other problems) explaining how one may apply Neural Combinatorial Optimization to problems for which coming up with a feasible solution is challenging by itself.\n\n5) We added a more detailed description of the critic network (see Section 4 - Critic\u2019s architecture for TSP).\n\nPlease take a look and let us know your thoughts.", "title": "Summary of paper's revision"}, "HklYkueEl": {"type": "rebuttal", "replyto": "Hyrehf-bx", "comment": "Thanks for the update. The natural question to ask, then is - do there exist many (or any) problems that are both interesting and have not been, and cannot be, addressed by the existing combinatorial optimization community? If there isn't, which I suspect to be the case (although I don't know for sure), then this flavor of work seems unlikely to yield significant impact.", "title": "Still doubtful"}, "H1KzLLeVl": {"type": "rebuttal", "replyto": "B1E2EHlNl", "comment": "Thank you for your interest. The process of obtaining the solutions from Concorde on each graph (x[1], y[1]), ..., (x[n], y[n]) is as follows:\n\n1. Multiply (x[i], y[i]) by a constant C and round them to the nearest integer. In particular, x[i] -> floor(C*x[i] + 0.5)\n2. Apply Concorde on the rounded numbers. This will give us a tour\n3. Use that tour on the original (x[i], y[i])\n\nI want to add that when choosing the constant C sufficiently large, the tours returned by Concorde are provably optimal on the original sequence (x[i], y[i]) as well. One can do that maths to verify that. However, the intuition is that scaling (x[i], y[i]) by C and then rounding is equivalent to adding a small disturbance epsilon to them in the original graph, up to a homothetic transformation. When C is large, the corresponding epsilon is smaller (think about you zooming closer to the picture and perturbing the points only your visible region).\n\nWe use C=10^7 for the numbers obtained in the paper, and then after your comment, I just tried C=10^9 and observed no change in all the tours found.", "title": "Rounding on Concorde"}, "HJoBPSx4g": {"type": "rebuttal", "replyto": "B1E2EHlNl", "comment": "Thanks for your interest! As you said, the common trick around this is to simply scale the distances by a large number, round them to integers and then divide the tour length by the scaling factor. You are correct that the approximation may sometimes introduce a tiny deviation from optimality but this is negligible in practice.", "title": "Rounding on Concorde"}, "B1E2EHlNl": {"type": "rebuttal", "replyto": "rJY3vK9eg", "comment": "This is very interesting to me! Thank you for this.\n\nAfter reading this paper, I tested the Concorde. I think the Concorde allows only integer distances(if use Euclidean distance, they round off), so cannot provide optimal solution of Euclidean TSP.\nBut error can be small if multiply the distance by a large constant.\n\nI want to know that, if I correct, does 'optimal' means a solution which is very closed to optimal?", "title": "About Concorde"}, "rkRCoHkVl": {"type": "rebuttal", "replyto": "SkBHNPame", "comment": "We have revised the paper. Running time can now be found in Appendix A.3. on page 13.", "title": "New version of the paper with runtime"}, "ryqfAfJ4e": {"type": "rebuttal", "replyto": "SkBHNPame", "comment": "As stated in the paper, Google's solver relies on its implementation of the LK heuristic (and not of LKH - this was a mistake in the previous comment but leaving it for clarity of the thread). LKH, which originates from LK but is much more complex, is known to perform extremely well in size of hundreds/thousands and we found it finds optimal solutions on our test sets too. Similarly to Concorde, it was designed very specifically for the TSP and is the result of many decades of research and engineering. On the other hand, Google's solver is optimized for general routing problems and is a common choice for such applications (while not optimal for TSP, it works reasonably well).\n\nConcerning computational costs, we find it tricky to fairly compare the methods in the paper as per our reply to Reviewer #2. \nComparing computational costs also shifts the conversation on specific solvers (and their corresponding implementations) which is not our goal here as we also do not claim to outperform any solvers. We also didn't optimize for speed.\n\nOur claim is that neural networks can learn distributions over good solutions of combinatorial tasks simply from a reward signal without relying on hand-engineered heuristics. We also compare different strategies at inference time, namely decoding greedily, searching by sampling repeatedly, actively searching by sampling and refining the parameters and even learning the parameters from scratch on a single graph. Irrespective of the existence of better alternatives (although those are not necessarily generic), we still believe our results to be new and exciting.\n\nBased on your feedback, we've added a discussion on running times and clear mentions of LKH (instead of just LK). We've also made it clearer that the local search corresponds to Google's solver's implementation.", "title": "local search baseline (reply)"}, "SkBHNPame": {"type": "rebuttal", "replyto": "HywYgyLml", "comment": "If Google's solver does not work for TSP then you probably should not use it unless you are forced to do so.\nBy a) omitting LKH and b) hiding computational costs you mislead the reader. There must be another way to get your paper accepted.", "title": "local search baseline"}, "r1VN2tYml": {"type": "rebuttal", "replyto": "SJgDqRw7l", "comment": "Thanks for your question.\n\nProviding a comprehensive computational resources comparison of our approach against other solvers is out of the scope of the paper.\n\nMethods in the paper are carried out on different hardware. We rely on GPUs for the computationally heavy steps of our approach while solvers run on CPUs, making it non-trivial to compare the number of CPU cycles. The methods also depend on many intricate implementation factors, some of which aren't even available.\nFor example, Concorde uses a black-box LP solver (QSopt, http://www.math.uwaterloo.ca/~bico/qsopt/) whose implementation is not open-sourced. Additionally, it is also very likely that both Concorde and Google's solver have well-optimized implementations compared to ours.\nCpu cycles also don't account for memory consumption. Solvers have some logic to keep track of their search history: for example, genetic algorithms and tabu search keep a number of past solutions in the memory (this number is specified by the user) while guided local search updates a penalty for each edge. In comparison, our model needs a fixed amount of memory.\nFinally, cpu cycles don't necessarily differentiate between sequential and parallelizable algorithms. For example, the search performed by the metaheuristics are sequential to varying degrees, while RL-pretraining Sampling is fully parallelizable.\n\nWe could, however, control for the number of calls to the verifier (although this also doesn't account for parallelism). In earlier experiments, we stopped the meta-heuristics after not seeing any improvement for a while, running guided local search for 300k iterations, and tabu search / simulated annealing for 100k iterations. \nThe fair (but potentially wasteful) comparison would be to run all meta-heuristics for 1.28M iterations (the equivalent of 10k batches) on all thousand test graphs for TSP50 and TSP100.\nThat will take some time so we can remove the comparison to other meta-heuristics for now if necessary (reviewer 3 suggested to use local search only as a baseline). Controlling for the number of iterations of local search is less relevant since it only visits a (variable but) limited number of solutions.\n\nHere are some relevant runtime numbers:\nA forward+backward pass typically takes ~0.1x second for our network (TSP100 with batch size of 128) on a Tesla K80. For example, sampling from 50 batches of solutions for one graph (ie 6400 candidate solutions) on a single K80 takes about 5 seconds and gives an average tour length of 7.97 (see table 3). Using a larger batch size of 256 would bring that time down but requires more memory...", "title": "About computation cost"}, "SJgDqRw7l": {"type": "review", "replyto": "rJY3vK9eg", "review": "Is there a comparison that controls for computational cost? e.g. number of cpu cycles?\n\nIt doesn't seem fair to compare to search algorithms if one is allowed much more computation than the other.This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.\n\n", "title": "Control for computation cost?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1PQL5gEx": {"type": "review", "replyto": "rJY3vK9eg", "review": "Is there a comparison that controls for computational cost? e.g. number of cpu cycles?\n\nIt doesn't seem fair to compare to search algorithms if one is allowed much more computation than the other.This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also \"fintunes\" on test examples with active search to achieve better performance.\n\nThe proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.\n\nHowever, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.\n\nFurther more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.\n\n", "title": "Control for computation cost?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HywYgyLml": {"type": "rebuttal", "replyto": "B1zXR3J7g", "comment": "Thanks for the feedback. \nWe first experimented with Google's open-sourced solver (https://developers.google.com/optimization/routing/tsp/tsp) which relies on a large set of heuristics (including 2-opt and LKH) for local search and guided local search, simulated annealing or tabu search for the meta-heuristic. In earlier experiments, we tried all meta-heuristics as well as simple local search on our TSP test sets. \nWe found that our best methods outperform local search by quite a margin (TSP50/100), as well as simulated annealing (TSP50/100) and tabu search (for TSP100 only).\n\nWe'll add those results in the paper and change the baseline in figure 1 to local search in the few next days.", "title": "local search baseline"}, "rkOu-J8me": {"type": "rebuttal", "replyto": "ryww1pJQx", "comment": "T* is the best temperature hyperparameter to sample with when sampling 1k batches (according to a grid search over an evaluation set). We'll update the paper because this isn't explicitly stated.\n\nConcerning the comparison between RL pretraining sampling and RL pretraining AS, it might indeed be that RL pretraining Sampling T=T* with 10k batches performs better than RL pretraining AS (for TSP50 at least, maybe not for TSP100 which has a much bigger solution space).\n\nHowever, we do not believe this changes the overall conclusion of the paper.\nWhen sampling towards an infinity of solution batches, RL-Sampling (just like any fixed probability distribution that has mass everywhere) would always hit the optimal solution. RL-AS is governed by more complex dynamics since the probability distribution changes as batches of solutions are sampled but is also guaranteed to hit optimality when the logits are clipped.\n\nTherefore, we believe it is important to compare the methods at different number of batches rather than just for some really large number of batches. Table 3 shows that RL-AS consistently does better than RL-Sampling for the same number of batches up to 1k batches at least. RL-AS also gets most of its improvement with the first few batches (for example on TSP100, RL-AS with 100 batches does better than RL-Sampling with 1k batches). \n\nIn other words,  RL-AS is better than RL-Sampling for reasonable number of batches. Also RL-Sampling T* requires a grid search over the temperature hyperparemeter while RL-AS does not. \n\nWe'll clarify this in our analysis section in the few next days. We will also run RL pretraining Sampling for 10k batches (we might however not have those results before Dec 16.)", "title": "On the comparison on RL pretraining AS and RL pretraining Sampling"}, "Byz1gkLXg": {"type": "rebuttal", "replyto": "S1zElTymx", "comment": "We'll be releasing the code next year well in time for the conference.", "title": "Code release"}, "H1DAsHQQl": {"type": "rebuttal", "replyto": "B1fegm-Gx", "comment": "Thanks for the reply, the additional section is very helpful.\nThe idea is clever - though how practical, I'm not sure. And I'm still not super convinced that this approach is more general and applicable (or, for that matter, effective) than, say, any local search or genetic algorithm you can think of. But I'm looking forward to get access to the code and try what it can do.", "title": "helpful addition"}, "B1zXR3J7g": {"type": "review", "replyto": "rJY3vK9eg", "review": "I believe Christofides' algorithm is the wrong baseline in Figure 1. How would your approach work compared to state-of-the-art heuristic search algorithms, such as Lin Kerningham Helsgaun, or at least compared to something trivial as 2-opt (https://en.wikipedia.org/wiki/2-opt)?\n\nThe reference implementation for Lin Kerningham Helsgaun is available here: http://www.akira.ruc.dk/~keld/research/LKH/\n2-opt is so basic that there is no reference implementation, but a quick Google search came up with this implementation: http://www.technical-recipes.com/2012/applying-c-implementations-of-2-opt-to-travelling-salesman-problems/\n\nNote that I think it's perfectly OK if those hand-designed algorithms perform way better than your automatic approach, but I believe they are the right thing to compare to. For Concorde, I agree with your previous remark about 50 years of research, but for something as simple and generic as local search with 2-opt this doesn't apply as strongly, since local search can very easily be used in other domains as well.This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance. However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU. \n\nSeeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for \"local search\" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don't think it was by the authors), the narrative being \"RNNs now also clearly perform better than local search\". Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions. \n\nThe right course of action upon realizing the real strength of local search with LK-H would've been to make \"local search\" the same line as \"Optimal\", showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search. Probably the authors didn't even think about this, but this of course will mislead the many superficial readers. To people outside of deep learning, this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. \n\n------------------------\n\nUpdate after rebuttal and changes:\n\nI'm torn about this paper. \n\nOn the one hand, the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So, from that point of view a clear accept.\n\nOn the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true... E.g., they make statements, such as \"We find that both greedy approaches are time-efficient and just a few percents worse than optimality.\"\nThat statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. \n(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum. LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).\n\nNevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it. That is, until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:\n\n\"Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002).\" That version then went on to show that these simple heuristics were already optimal, just like their own method.\n\nIn a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here, as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising). The results for greedy are interesting to see. However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines. Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly: https://developers.google.com/optimization/bin/knapsack\n\nReally, all I'm after is a responsible (and if you wish, humble) presentation of what I believe to be great results that are really promising for the field. The authors just need to make it crystal clear that, as of now, their method is still very far away from the state of the art. And that's OK; you don't typically beat an entire field with one paper. If the authors clearly stated that throughout, I would clearly argue for acceptance ... (Maybe that's not the norm in machine learning, but I don't think you have to beat everything quite yet if your approach is very different and promising -- see DL and ImageNet.)\nConcretely, I would recommend that the authors do the following:\n\n- Put the original Figure 1 back in, but dropping the previous poor local search and labelling the line at 1.0 \"local search (LK-H) = exact (Concorde)\". If the authors would like to, they could also in addition leave the previous poor local search in and label it \"Google OR tools (generic local search)\" \n- Put the other baselines back into the Knapsack section\n- Make sure the wording clearly states throughout that the method is still quite far from the state of the art (it's perfectly fine to strongly state that the direction is very promising).\n\n\nOverall, this paper has to watch out for not becoming an example of promising too much (some would call it hype-generation) before having state-of-the-art results. (I believe the previous comparison against local search fell into that category, and now the current section on Knapsack does.) I believe that would do a great disservice to our community since it builds up an expectation that the field cannot live up to (and that's a recipe for building up a bubble that has no other chance but burst).\n\nHaving said all that, I think the paper can be saved if the authors embrace that they are still far away from the state of the art. I'll be optimistic and trust that they will come around and make the changes I suggested above. Hoping for those changes, since I think the method is a solid step forward, I'm updating my score to a weak accept.", "title": "Christofides algorithm wrong baseline", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1SVk-MVx": {"type": "review", "replyto": "rJY3vK9eg", "review": "I believe Christofides' algorithm is the wrong baseline in Figure 1. How would your approach work compared to state-of-the-art heuristic search algorithms, such as Lin Kerningham Helsgaun, or at least compared to something trivial as 2-opt (https://en.wikipedia.org/wiki/2-opt)?\n\nThe reference implementation for Lin Kerningham Helsgaun is available here: http://www.akira.ruc.dk/~keld/research/LKH/\n2-opt is so basic that there is no reference implementation, but a quick Google search came up with this implementation: http://www.technical-recipes.com/2012/applying-c-implementations-of-2-opt-to-travelling-salesman-problems/\n\nNote that I think it's perfectly OK if those hand-designed algorithms perform way better than your automatic approach, but I believe they are the right thing to compare to. For Concorde, I agree with your previous remark about 50 years of research, but for something as simple and generic as local search with 2-opt this doesn't apply as strongly, since local search can very easily be used in other domains as well.This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance. However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU. \n\nSeeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for \"local search\" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don't think it was by the authors), the narrative being \"RNNs now also clearly perform better than local search\". Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions. \n\nThe right course of action upon realizing the real strength of local search with LK-H would've been to make \"local search\" the same line as \"Optimal\", showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search. Probably the authors didn't even think about this, but this of course will mislead the many superficial readers. To people outside of deep learning, this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. \n\n------------------------\n\nUpdate after rebuttal and changes:\n\nI'm torn about this paper. \n\nOn the one hand, the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So, from that point of view a clear accept.\n\nOn the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true... E.g., they make statements, such as \"We find that both greedy approaches are time-efficient and just a few percents worse than optimality.\"\nThat statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. \n(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum. LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).\n\nNevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it. That is, until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:\n\n\"Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002).\" That version then went on to show that these simple heuristics were already optimal, just like their own method.\n\nIn a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here, as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising). The results for greedy are interesting to see. However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines. Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly: https://developers.google.com/optimization/bin/knapsack\n\nReally, all I'm after is a responsible (and if you wish, humble) presentation of what I believe to be great results that are really promising for the field. The authors just need to make it crystal clear that, as of now, their method is still very far away from the state of the art. And that's OK; you don't typically beat an entire field with one paper. If the authors clearly stated that throughout, I would clearly argue for acceptance ... (Maybe that's not the norm in machine learning, but I don't think you have to beat everything quite yet if your approach is very different and promising -- see DL and ImageNet.)\nConcretely, I would recommend that the authors do the following:\n\n- Put the original Figure 1 back in, but dropping the previous poor local search and labelling the line at 1.0 \"local search (LK-H) = exact (Concorde)\". If the authors would like to, they could also in addition leave the previous poor local search in and label it \"Google OR tools (generic local search)\" \n- Put the other baselines back into the Knapsack section\n- Make sure the wording clearly states throughout that the method is still quite far from the state of the art (it's perfectly fine to strongly state that the direction is very promising).\n\n\nOverall, this paper has to watch out for not becoming an example of promising too much (some would call it hype-generation) before having state-of-the-art results. (I believe the previous comparison against local search fell into that category, and now the current section on Knapsack does.) I believe that would do a great disservice to our community since it builds up an expectation that the field cannot live up to (and that's a recipe for building up a bubble that has no other chance but burst).\n\nHaving said all that, I think the paper can be saved if the authors embrace that they are still far away from the state of the art. I'll be optimistic and trust that they will come around and make the changes I suggested above. Hoping for those changes, since I think the method is a solid step forward, I'm updating my score to a weak accept.", "title": "Christofides algorithm wrong baseline", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1fegm-Gx": {"type": "rebuttal", "replyto": "rkvMApw-x", "comment": "Thanks for your interest. Section 3 is explicitly named \"Neural Network Architecture for TSP\" because different network architectures can be considered for different types of problems. The optimization process, however, doesn't rely on any TSP specific heuristics, which is why we did not anticipate the need to clarify how to use the framework for other combinatorial problems. We've added a brief discussion on how to apply the framework to other problems than the TSP and showcased its flexibility by running some experiments on the knapsack problem (which again took minimal changes to our code), for which we consistently get optimal results on instances with up to 200 items.\n", "title": "Added discussion about the generality of the approach"}, "rkvMApw-x": {"type": "rebuttal", "replyto": "Hyrehf-bx", "comment": "The approach is interesting from a technical standpoint. The claim is that this is a framework for combinatorial optimization problems and you say it is very generic and easily applicable. However, to me the paper only shows a method to solve the standard TSP, failing to explain clearly how (and if) the approach can actually be extended to general combinatorial problems [not simply involving a permutation of the input]. Some words about the classes of problems the approach could potentially be applied to would also be welcome. Rather underwhelming, given the big claim.", "title": "On the generality of the approach"}, "Hyrehf-bx": {"type": "rebuttal", "replyto": "HJBRyNCee", "comment": "Thank you for the feedback.\n\nIn the original draft, we used a solver that is applicable to a wider range of combinatorial optimization problems, but we agree that Concorde is a superior solver for basic TSP. In the meantime, we switched our baseline to Concorde and updated the paper. We found that our original solver already attained optimal solutions on all instances of TSP50 and more than half of the instances of TSP100. \n\nExact TSP solvers like Concorde are the results of half a century of research on combinatorial optimization and specifically TSP. Although impressive, they need to be significantly revised once the problem statement changes slightly. By contrast, our proposed approach is very generic and easily applicable to a wide range of combinatorial optimization problems with minimal hand engineering.", "title": "Now using Concorde as a baseline"}, "HJBRyNCee": {"type": "rebuttal", "replyto": "rJY3vK9eg", "comment": "There is a large body of work on solving TSP instances that this paper ignores. In particular, the concorde algorithm has produced provably optimal solutions to problems as large as 85,900 cities, and can solve 100+ city problems in a few seconds on a single 500MHz core. Thus, the claims made that this is even close to being a useful tool for solving TSP problems are demonstrably untrue.\n\nhttp://www.math.uwaterloo.ca/tsp/concorde.html", "title": "Related work incomplete"}}}