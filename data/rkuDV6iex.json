{"paper": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"], "summary": "Analyzing the loss surface of deep neural network trained with different optimization methods", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "keywords": ["Deep learning"]}, "meta": {"decision": "Reject", "comment": "The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.\n \n The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims. \n \n A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue."}, "review": {"B1fXDwwQe": {"type": "rebuttal", "replyto": "BJcBxFJ7e", "comment": "When we referred to \u201cminima\u201d and \u201clocal minima\u201d, we were being loose with our language, and should have used the term \u201ccritical points\u201d, as the weight vectors compared were the convergence points of the stochastic gradient descent algorithms. We do not believe that these points are global minima, but that many of them are local minima as opposed to saddle points. We have changed our terminology to clarify this. In addition, we have added evidence that the following evidence that the convergence points are local minima (see Figure 11).", "title": "Clarifications "}, "SyvTIDvXl": {"type": "rebuttal", "replyto": "SklPsulXl", "comment": "We have rewritten the Discussion section of the paper to clarify our conclusions. \nMinor typos are fixed and all occurrences of \u201cAdam\u201d has been changed to \u201cADAM\u201d.", "title": "Updates on conclusions ! "}, "rkzFLDwQl": {"type": "rebuttal", "replyto": "rJMQjUqMx", "comment": "(1) We used batch-normalization and dropout to regularize our networks. This information has been added to Section 3.4 of the revised paper. \n\n\n(2) Our paper provides an empirical analysis of deep learning, and the main contributions are the experiments we performed, using a method similar to Goodfellow et al., and their conclusions. Goodfellow et al. used only one optimization algorithm, and their main experiment involved interpolating between the initial and final weights. In contrast, our experiments compared different optimization algorithms. We performed several novel experiments, and reached the following novel conclusions:\n\n- Comparison of local minima found by different optimization algorithm (Sections 4.1 and 4.2). Based on these experiments, we concluded that different optimization algorithms reach different local minima from the same initialization, and that the shape of the loss function around these local minima are characteristic for each algorithm. \n\n- Comparison of the local minima found when switching optimization algorithm during the late \u201cminimization\u201d phase of learning (Section 4.3). We observed that different local minima are found by different algorithms even when we switch algorithms very late in the optimization -- when the training accuracy is improving only very slowly. It had previously been assumed that, at this point in training,  the local minimum had been selected, and that it was just being localized within a neighborhood. We conclude that this does not appear to be the case. \n\n- Comparison of loss function shape around local minima found with and without batch normalization (Section 4.4). Based on these experiments, we found that the shape of the loss function was extremely stereotyped when using batch normalization, but not without it (thus, there was a much smaller effect of the initialization with batch normalization). \n\n\n(3,4) Thank you for pointing out the typographic errors. We have fixed them in the latest version.", "title": "Some clarifications "}, "SklPsulXl": {"type": "review", "replyto": "rkuDV6iex", "review": "This is a comprehensive and useful evaluation. Some remarks:\n\n- conclusions are somewhat missing and are not entirely clear\n- there are some minor typos, e.g. page 8 on bottom (swithced) and inconsistencies in the capitalization of AdamThis paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.\n\nPros:\n\n- Important analysis\n- Good visualizations\n\nCons:\n\n- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)\n- Some fonts are very small (e.g. Fig. 5)\n", "title": "conclusion", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJyJl4eEe": {"type": "review", "replyto": "rkuDV6iex", "review": "This is a comprehensive and useful evaluation. Some remarks:\n\n- conclusions are somewhat missing and are not entirely clear\n- there are some minor typos, e.g. page 8 on bottom (swithced) and inconsistencies in the capitalization of AdamThis paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.\n\nPros:\n\n- Important analysis\n- Good visualizations\n\nCons:\n\n- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)\n- Some fonts are very small (e.g. Fig. 5)\n", "title": "conclusion", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJcBxFJ7e": {"type": "review", "replyto": "rkuDV6iex", "review": "This paper refers to \"local minima\" and \"minima\" without demonstrating a support for their presence. \nPlease describe the conditions of the latter and demonstrate whether they are satisfied. \nI appreciate the work but I do not think the paper is clear enough. \nMoreover, the authors say \"local minimia\" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima. \nThe authors do not talk about that fact that slices of a non-convex problem can look like the ones they show. \nIt is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases, the place/solution where they fail to make progress is not necessarily a local minimum. \nSome sentences like the one given below suggest that the study is too superficial:  \n\"One of the interesting empirical observation is that we often observe is that the incremental improvement\nof optimization methods decreases rapidly even in non-convex problems.\"", "title": "local minima", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJJNbw-Ne": {"type": "review", "replyto": "rkuDV6iex", "review": "This paper refers to \"local minima\" and \"minima\" without demonstrating a support for their presence. \nPlease describe the conditions of the latter and demonstrate whether they are satisfied. \nI appreciate the work but I do not think the paper is clear enough. \nMoreover, the authors say \"local minimia\" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima. \nThe authors do not talk about that fact that slices of a non-convex problem can look like the ones they show. \nIt is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases, the place/solution where they fail to make progress is not necessarily a local minimum. \nSome sentences like the one given below suggest that the study is too superficial:  \n\"One of the interesting empirical observation is that we often observe is that the incremental improvement\nof optimization methods decreases rapidly even in non-convex problems.\"", "title": "local minima", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJMQjUqMx": {"type": "review", "replyto": "rkuDV6iex", "review": "Dear Authors,\n\n1) What network regularizations were used in the experiments?\n2) The paper seems incremental over Goodfellow et al. 2015 (it extends this work though). Could you clarify more the significant novelty of this paper and the message it conveys? What future research directions it gives rise to? Could you discuss what are the algorithmic/other consequences of the findings in the paper?\n3) The sentence starting at \"For every pair of optimization algorithms, we...\" on page 4 seems to have a missing word.\n4) Similarly, sentence on page 4: \"This suggests to use that...\" seems to have a missing part.\nThe paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. \n\nIt would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.", "title": "Clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJI1peGEe": {"type": "review", "replyto": "rkuDV6iex", "review": "Dear Authors,\n\n1) What network regularizations were used in the experiments?\n2) The paper seems incremental over Goodfellow et al. 2015 (it extends this work though). Could you clarify more the significant novelty of this paper and the message it conveys? What future research directions it gives rise to? Could you discuss what are the algorithmic/other consequences of the findings in the paper?\n3) The sentence starting at \"For every pair of optimization algorithms, we...\" on page 4 seems to have a missing word.\n4) Similarly, sentence on page 4: \"This suggests to use that...\" seems to have a missing part.\nThe paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. \n\nIt would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.", "title": "Clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}