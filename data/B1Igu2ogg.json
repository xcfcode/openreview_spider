{"paper": {"title": "Efficient Vector Representation for Documents through Corruption", "authors": ["Minmin Chen"], "authorids": ["m.chen@criteo.com"], "summary": "a simple document representation learning framework that is very efficient to train and test", "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.\n", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The introduced method for producing document representations is simple, efficient and potentially quite useful. Though we could quibble a bit that the idea is just a combination of known techniques, the reviews generally agree that the idea is interesting.\n \n Pros:\n + interesting and simple algorithm\n + strong performance\n + efficient\n \n Cons:\n + individual ideas are not so novel\n \n This is a paper that will be well received at a poster presentation."}, "review": {"S19DnijlZ": {"type": "rebuttal", "replyto": "B1Igu2ogg", "comment": "Extended the paper with experiments on the word relationship dataset, showing Doc2VecC generates better word embeddings in comparison to Word2Vec or Paragraph Vectors. ", "title": "Better word embeddings"}, "SkFcq0B0x": {"type": "rebuttal", "replyto": "BJz3FYxnl", "comment": "Thank you for your interest! For this experiment, I set a cut-off of 100 to remove words that appear less than 100 times throughout the document. These words will have very small norm as they are rare. It was explained in page 7 of the paper. You can also see that in table 3 the words of smallest norms learned by the other methods are the ones appearing around 100 times in the corpus. ", "title": "On data-dependent regularization"}, "BJz3FYxnl": {"type": "rebuttal", "replyto": "B1Igu2ogg", "comment": "I'm using https://github.com/mchen24/iclr2017 to learn embeddings and then compute the l2 norm of embeddings. I find the least 10 words are\n '</s>',\n '--shelly',\n 'willett',\n '-ap3-',\n 'gruel',\n '-celluloid',\n '10*',\n 'massie',\n '****/****',\n 'hush-hush',\nTheir l2 norms are 7.79849624e-04,   2.45744940e-01,   2.53457799e-01, 2.56453203e-01,   3.34406160e-01,   4.00243759e-01, 4.22770766e-01,   5.10860541e-01,   5.40468018e-01, 5.50735000e-01. \nThey are close to 0, but except the </s>, other words are pretty unpopular.  Can you please check whether the inconsistency? ", "title": "About Table 3: Words with embeddings closest to 0 learned by different algorithms."}, "r1As2C1De": {"type": "rebuttal", "replyto": "BJUqiLcBl", "comment": "Dear reviewer, \n\nThank you for taking time to read the paper again and update the review. \n\nMinmin", "title": "Updated review"}, "SysfGOtHe": {"type": "rebuttal", "replyto": "rkslf-rVl", "comment": "Dear reviewer, \n\nI have added more experiments (a new dataset and comparison to LSTM-based methods) and explanations to the manuscript regarding some of the concerns you pointed out. I believe that the additional information provided could worth a second view and would be very much interested in an open discussion to find out if there are any remaining unfavorable factors.\n\nThank you again for your time. ", "title": "Additional information added"}, "r1Ba7anNx": {"type": "rebuttal", "replyto": "rkslf-rVl", "comment": "Dear reviewer, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218.", "title": "New dataset and baselines added"}, "Hy_gmUiVl": {"type": "rebuttal", "replyto": "B1Igu2ogg", "comment": "Dear reviewers, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218.", "title": "new dataset and baselines added"}, "B1sb3uD4x": {"type": "rebuttal", "replyto": "B1Igu2ogg", "comment": "Dear reviewers, \n\nThank you for your feedback. The updated manuscript included skip-thought as another baseline method. We will test this idea on more datasets, in particular the ones experimented in Skip-thought vectors in the submission. ", "title": "revisions"}, "BkxfBmS4l": {"type": "rebuttal", "replyto": "rkslf-rVl", "comment": "Dear reviewer, thank you for your constructive feedback. Indeed our main goal is to come up with a simple and efficient framework for generating document representations. I would like to argue that simplicity does not deny originality. The reason we can simply average word embeddings at test time to form document representation is because of the new model architecture proposed, which represents documents with corrupted average of word embeddings at learning time, and learns the document embedding with word embeddings together. The corruption at learning time enables fast learning (comparing to [2][3]), as well as a data-dependent regularization. As far as I know, it is the first do so. \n\nI believe the paper contains quite thorough analyses of the proposed work on the sentiment analysis and document classification tasks. I would like to see the community start exploring and benefiting from this simple idea, while we work on testing it on more tasks. \n\nFor RNN-LM, we used the implementation provided by the author, which was tested on the same dataset in their 2015 ICLR submission [1]. It builds two language models, one for the positive class and one for negative. It then computes the probability of each LM generating the document and assigns the one with higher score as the prediction. \n\nI included skip-thought vectors as another baseline in the manuscript thanks to the feedback from another reviewer. The encoder and decoder in the method are constructed from gated RNN. The method produces two models, uni-skip and bi-skip. Among them, the bi-skip is a bi-directional model that generates one forward and one backward encoding of the document.  Its performance is not satisfactory on this dataset, and it takes long time to test due to the high-dimensional encoders used. \n\nAgain thanks for your feedback and please let me know if you have other questions. \n\n[1] Mesnil, Gr\u00e9goire, et al. \"Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews.\" arXiv preprint arXiv:1412.5335 (2014).\n[2] Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improving word representations via global context and multiple word prototypes. In ACL, pp. 873\u2013882, 2012.\n[3] Lebret, R\u00e9mi, and Ronan Collobert. \"\" The Sum of Its Parts\": Joint Learning of Word and Phrase Representations with Autoencoders.\" arXiv preprint arXiv:1506.05703 (2015).", "title": "Thanks for your feedback"}, "r1NcFF74e": {"type": "rebuttal", "replyto": "rk1DcoWVx", "comment": "Dear reader, thank you for your suggestion. I updated the manuscript to include skip-thought vector as a baseline in the IMDB movie review dataset. It performs surprisingly poor on this dataset comparing to other methods. We hypothesize that it is due to the difference between the dataset used to train the model and the one we are testing it on. We notice that the paragraphs in the movie review dataset are much longer than the sentences in the book corpus dataset, as well as the other datasets experimented in the original paper.  As pointed out in the original paper, learning the representations, even on small datasets, are likely to out-perform a generic unsupervised representation learned on much bigger datasets. \n\nDue to the high-dimensional encoders employed in the method to generate generic representations, Skip-thought is not very efficient in testing, which is an important factor we would like to address with our method. We will try to benchmark against more baselines in follow up works.\n", "title": "Skip-thought Vectors"}, "rk1DcoWVx": {"type": "rebuttal", "replyto": "B1Igu2ogg", "comment": "Unsupervised document representations is an active area of research, so it would be useful to benchmark against something more recent than doc2vec, which was in ICML 2014. Skip-thought vectors, in particular, should really be included.", "title": "Need more recent baselines"}, "B14NQP-Xx": {"type": "rebuttal", "replyto": "H1yJ5ByQx", "comment": "Thank you for your questions. Indeed, the norm of the learned document embedding decreases with document length. I added a figure in the appendix to demonstrate this effect on the embeddings learned on the Imdb dataset. I did not find the performance in particular depends on the length of the document. The classification error on the different document length buckets are comparable. ", "title": "Effect of document length on resulting embeddings"}, "H1yJ5ByQx": {"type": "review", "replyto": "B1Igu2ogg", "review": "Have you analyzed the effect of document length on the resulting embeddings, e.g. does the norm decrease with length? Does performance depend strongly on document length?This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.\n\nJoint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.\n\nOn the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.\n\nOverall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.", "title": "Effect of document length", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJBM9YbVg": {"type": "review", "replyto": "B1Igu2ogg", "review": "Have you analyzed the effect of document length on the resulting embeddings, e.g. does the norm decrease with length? Does performance depend strongly on document length?This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.\n\nJoint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.\n\nOn the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.\n\nOverall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.", "title": "Effect of document length", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyohGtRfg": {"type": "rebuttal", "replyto": "BySwY1AGl", "comment": "Thank you for your questions. For both tasks, a bigger unlabeled set is used to learn representation for all the representation learning methods.  A classifier is then trained using the learned representation on the training data to perform either sentiment analysis or document classification. \n\nRNN-LM builds one language model  per class, each one taking long time to train. Its performance is not as competent as the others in the sentiment analysis task. Due to time limit, I omitted this method for the multi-class document classification task. ", "title": "About experiment setup"}, "BySwY1AGl": {"type": "review", "replyto": "B1Igu2ogg", "review": "- Were the word embeddings for the Word2Vec+ baselines only trained on the training data? Did you try training them on a much bigger (unlabeled) corpus?\n- Why did you exclude the RNN-LM baseline for the document classification task?\n\nThanks!This paper presents a framework for creating document representations. \nThe main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. \nExperiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. \n\nWhile I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.\nMost of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. \nFor this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  \nFor RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?\nOne of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. \nI think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.", "title": "pre review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkslf-rVl": {"type": "review", "replyto": "B1Igu2ogg", "review": "- Were the word embeddings for the Word2Vec+ baselines only trained on the training data? Did you try training them on a much bigger (unlabeled) corpus?\n- Why did you exclude the RNN-LM baseline for the document classification task?\n\nThanks!This paper presents a framework for creating document representations. \nThe main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. \nExperiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. \n\nWhile I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.\nMost of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method. \nFor this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  \nFor RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states?\nOne of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. \nI think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.", "title": "pre review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}