{"paper": {"title": "Language GANs Falling Short", "authors": ["Massimo Caccia", "Lucas Caccia", "William Fedus", "Hugo Larochelle", "Joelle Pineau", "Laurent Charlin"], "authorids": ["massimo.p.caccia@gmail.com", "lucas.page-caccia@mail.mcgill.ca", "liam.fedus@gmail.com", "hugolarochelle@google.com", "jpineau@cs.mcgill.ca", "lcharlin@gmail.com"], "summary": "GANs have been applied to text generation and are believed SOTA. However, we propose a new evaluation protocol demonstrating that maximum-likelihood trained models are still better.", "abstract": "Traditional natural language generation  (NLG) models are trained using maximum likelihood estimation (MLE) which differs from the sample generation inference procedure. During training the ground truth tokens are passed to the model, however, during inference, the model instead reads its previously generated samples - a phenomenon coined exposure bias. Exposure bias was hypothesized to be a root cause of poor sample quality and thus many generative adversarial networks (GANs) were proposed as a remedy since they have identical training and inference.  However, many of the ensuing GAN variants validated sample quality improvements but ignored loss of sample diversity. This work reiterates the fallacy of quality-only metrics and clearly demonstrate that the well-established technique of reducing softmax temperature can outperform GANs on a quality-only metric. Further, we establish a definitive quality-diversity evaluation procedure using temperature tuning over local and global sample metrics. Under this, we find that MLE models consistently outperform the proposed GAN variants over the whole quality-diversity space.  Specifically, we find that 1) exposure bias appears to be less of an issue than the complications arising from non-differentiable, sequential GAN training;  2) MLE trained models provide a better quality/diversity trade-off compared to their GAN counterparts, all while being easier to train, easier to cross-validate, and less computationally expensive.", "keywords": ["NLP", "GAN", "MLE", "adversarial", "text generation", "temperature"]}, "meta": {"decision": "Accept (Poster)", "comment": "Main content:\n\nBlind review #1 summarizes it well:\n\nRecently many language GAN papers have been published to overcome the so called exposure bias, and demonstrated improvements  in natural language generation in terms of sample quality, some works propose to assess the generation in terms of diversity, however, quality and diversity are two conflicting measures that are hard to meet. This paper is a groundbreaking work that proposes receiver operating curve or Pareto optimality for quality and diversity measures, and shows that simple temperature sweeping in MLE generates the best quality-diversity curves than all language GAN models through comprehensive experiments. It points out a good target that language GANs should aims at. \n\n--\n\nDiscussion:\n\nThe main reservation was the originality of the idea of using temperature sweep in the softmax. However, it turns out this idea came from the authors in the first place, which they have not been able to state directly due to the anonymity requirement. Per the program chair's instruction to direct this to the area chair, I think this has been handled correctly.\n\n--\n\nRecommendation and justification:\n\nThis paper should be accepted. It provides readers with insight in that it illuminates a misconception of how important exposure bias has been assumed to be, and provides a less expensive MLE based way to train than GAN counterparts."}, "review": {"qxoA3BDBda": {"type": "rebuttal", "replyto": "HkxLcNH2oB", "comment": "We found that [1] and [2] were already performing temperature sweeps on harder datasets, namely IMDB and Wikitext-103 respectively. Furthermore, [1] used the Texygen platform, (owned by SeqGAN and LeakGAN group) to perform the experiments. Both papers arrived at the same conclusions as us, as previously discussed. \n\nThus, we kindly ask the reviewer if we could be relieved of having to re-run these experiments. Has the results are already available to the community, it would seem like a waste of resources.\n\nThanks", "title": "results on more challenging datasets"}, "rJeZWKD2sS": {"type": "rebuttal", "replyto": "rJeSMAtijS", "comment": "thanks for pointing this out, we'll carefully study this other submission and will consider citing it", "title": "Response to Reviewer #1"}, "rkx9Nl1zcH": {"type": "review", "replyto": "BJgza6VtPB", "review": "This paper concerns the limitation of the quality-only evaluation metric for text generation models. Instead, a desirable evaluation metric should not only measure the sample quality, but also the sample diversity, to prevent the mode collapse problem in gan-based models generation. The author presents an interesting, but not too surprising finding that, tuning the temperature beam search sampling consistently outperform all other GAN/RL-based training method for text generation models. The idea of sweeping temperature during beam search decoding is not new in the NLP community, which limits the novelty of this paper. What\u2019s more, some parts of the experiment results is also somehow not new, in the sense that the SBLEU vs Negative BLEU tradeoff curve is also shown in [1,2,3,4].\n\n[1] Jointly measuring diversity and quality in text generation models, 2019\n[2] Training language gans from scratch, 2019\n[3] On accurate evaluation of gans for language generation, 2018\n[4] Towards Text Generation with Adversarially Learned Neural Outlines, 2018\n\nI would love to increase my score if the author could address the following comments:\n(1) Are the comparing methods, say MLE models and other GAN-based models, have the similar number of model parameters? It is not clear from the paper. Otherwise, one can use a 12/24 layer Transformer-XL to have dominative performance?\n(2) Since this is an empirical study paper. It would be great if this paper can also present more SoTA models trained by MLE such as Transformer-XL on more challenging datasets, such as Wikitext-2 or Wikitext-103. In this kind of large vocabulary datasets, I think the RL/GAN-based training methods would easily breakdown, and far worse than MLE-based training.\n(3) To make the empirical study more comprehensive, the author could perhaps evaluate with the n-gram and FED metric.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "ByeapaqosB": {"type": "rebuttal", "replyto": "rJlyE45jsr", "comment": "Great - glad we could clarify comments 1 and 3!\n\nAh! Yes, we misunderstood your suggestion.  We can certainly run on a more challenging language modeling before the camera ready deadline (but not before end of rebuttal tomorrow).   Our expectation, given the mode collapse documented in Fedus et al. (2017) on PTB and IMDB movies dataset (see Figure 2 in Appendix C.4), is that our conclusion via temperature sweeping will hold.  Will that resolve your second comment?", "title": "More Challenging Datasets"}, "HkxpmSPjjr": {"type": "rebuttal", "replyto": "SkeCCwZWoB", "comment": "As the rebuttal period draws to a close we wanted to ensure we have resolved all your concerns and answered any remaining questions.  Regarding your comments on GAN + Transformer training, we agree that is an interesting open-research direction, but this is not the goal of our research.  In this work, we establish strong baselines for all text GANs but do not seek to design new text GAN variants.  To our knowledge, there are no successful applications of GAN + Transformers and this is an area of open research.  However, if and when a GAN + Transformer variant is released, we will eagerly benchmark it using our temperature sweep methodology, as we did with other released text GAN variants.  \n\nDo you have any further questions? Thanks again for your review!", "title": "Response to Reviewer #2"}, "rkeOHFSDoH": {"type": "rebuttal", "replyto": "Sygas2rMor", "comment": "Thanks again for helping us with our empirical results. The original EMNLP2017 News dataset is much bigger than the one in Texygen used by the SeqGAN/LeakGAN lab and by us. This could be the reason why the reviewer as seen better results on this dataset. We found further evidence in \u201cCoT: Cooperative Training for Generative Modeling of Discrete Data\u201d (v1: https://arxiv.org/pdf/1804.03782v1.pdf), another paper authored by the same lab, which indicates that our results are in line with the literature. Specifically, they report a BLEU5 and SBLEU5* of 0.10 and 0.27 for SeqGAN and 0.27 and 0.53 for LeakGAN. This is close to our 0.09 and 0.27 for SeqGAN and 0.27 and 0.51 for LeakGAN. \n\nFurthermore, preprocessing can have an impact on the results. E.g. in \u201cA Quality-Diversity Controllable GAN for Text Generation\u201d, the dataset is processed differently and their results are different than ours. Nonetheless, they arrived at the same conclusion regarding MLE outperformance over GANs.  \n\nWe, however, found that Texygen and the other papers from the same lab don\u2019t specify a validation set. Thus, the reviewer might be right that some noise could have been inserted in our empirical results. We have opened an issue on the Texygen repository https://github.com/geek-ai/Texygen/issues/42 . We emailed the authors as well. We will re-do the experiments if there is a discrepancy is the data partition.\n\nNevertheless, the comparison between MLE and our RL-GAN (which includes SeqGAN, LeakGAN and other tricks from other Language GANs) was performed with a single codebase (ours) and data partition. Thus, our conclusion that MLE training is superior to GAN training when considering the full quality-diversity tradeoff holds for our data partition.\n\n\n* we multiplied the values in Table 3 of v1 https://arxiv.org/pdf/1804.03782v1.pdf by 0.24, which is the SBLEU5 of the real data. \n\n", "title": "Response to Reviewer #1"}, "SkeCCwZWoB": {"type": "rebuttal", "replyto": "rkx9Nl1zcH", "comment": "Thanks for your time reviewing and help improving our paper!  Thank you for pointing out this related work. Indeed, [1,2] are related, however, we note that our work preceded them. While this fact is easily verifiable, we do not provide the outside evidence here to preserve anonymity.  Our temperature evaluation framework has been successfully employed by the community as discussed in our submission.  We additionally encourage the reviewer to revisit Page 4, Paragraph 2 reproduced for convenience here:\n\n\u201cIn the next section we show, using temperature sweep, that MLE models consistently outperform the new proposed GAN variants everywhere in the quality-diversity space. MLE performs equally on synthetic data to CoT and outperforms it on real data, whilst being computationally and algorithmically less complicated. Our results are further validated in independent follow-up works (d\u2019Autume et al., 2019 [2]; Alihosseini et al., 2019 [1]) where temperature sweeps show MLE outperforming GANs.\u201d\n\nRegarding the other two papers, [3] was concurrent work that analyzes the BLEU and SLEU tradeoff but does not show any temperature curves. They show something akin to our Figure 2, i.e. a single point for each model in quality-diversity space. It\u2019s important to realize, as explained in our introduction (see Figure 1) that this evaluation can lead to inconclusive results. This is one of the contributions of our work. Note that [4] similarly does not propose temperature sweeps. They do report results at two different temperatures (0.5 and 1.0). However, their conclusion is that a pre-trained general-purpose sentence encoder can be leveraged in unconditional text generation. Our conclusion is that MLE training is still superior to GAN training when considering the full quality-diversity tradeoff.  Though this may seem like a simple point, it is an important message for the community that may impact many future works. \n\nComment (1): Thanks for the suggestion to clarify the model sizes. We will add these details to the paper!  For both MLE and RL-GAN, we hyperparameter searched for the best depth {1,2} and width {128, 256, 512} of the LSTM. More details about hyperparameter values can be found in the hyperparameter search script https://github.com/AnonSubmitter2/iclr2020/blob/master/cc_folder/news_rs.py. So the comparison is equivalent on a hyperparameter computational budget.  Furthermore, the text GANs are O(2x) as large due to the discriminator and generally require longer training.\n\nComment (2): We definitely agree RL/GAN-based methods could easily breakdown on more challenging datasets and harder to optimize architectures like Transformer-XL. As you correctly point out, policy-gradient and reinforcement learning methods in enormous action spaces are problematic.  We therefore agree and also expect given all the evidence we have presented, that the MLE-based training will be superior to GAN-based training - inline with the central message of our paper.  Our goal here is not to construct a new Transformer-based text GAN, but rather, compare all the SOTA GAN methods proposed and published.\n\nComment (3): Could you please clarify your request regarding n-gram metrics?  BLEU and SBLEU are already n-gram metrics.  In the main part of the paper, BLEU-5 and SBLEU-5 are reported, which are weighted averages of n-gram overlaps (n from 2 to 5). BLEU2-4 and SBLEU2-4 are shown in the Appendix. If by FED the reviewer meant Frechet InferSent Distance (FID), it was shown in [3] that Reverse LM score is a better metric (see their Figure 2). Although both metrics are sensitive to mode collapse and word dropping, FID is insensitive to word swapping whereas Reverse LM score is not (see page 7 paragraph 2 of their paper).\n", "title": "Response to Reviewer #2"}, "r1lEsKWzoH": {"type": "rebuttal", "replyto": "HyeAqdnCFr", "comment": "Thanks for your encouraging comments! \n\nThanks for the suggestion to clarify the data partition. Some results are taken from another paper, but we made sure the data partition was the same. The text generation benchmarking platform Texygen https://github.com/geek-ai/Texygen was used to generate the performance results we copied from https://arxiv.org/pdf/1803.07133.pdf . This platform and this paper were developed and written by the same lab who proposed SeqGAN and LeakGAN. We used the dataset generating and metrics computing scripts from Texygen to ensure comparability. Thus, the reported results in Figure 4(a) are legitimate. \n\nWe thank the reviewer for pointing out the missing citation, which we will add to the next version of the paper. It is however not possible at the time to provide a fair comparison to its results, as the paper only reports quality metrics (NLL_oracle and BLEU). Furthermore, we could not find a publically available implementation. We would be willing to compare with this methodology if either condition was addressed. \n\nThanks for pointing out the typo. \n", "title": "Response to Reviewer #1"}, "HyeAqdnCFr": {"type": "review", "replyto": "BJgza6VtPB", "review": "Recently many language\u00a0GAN papers have been published to overcome the so called exposure bias, and demonstrated\u00a0improvements\u00a0 in natural language generation in terms of sample quality, some works propose to assess the generation in terms of diversity, however, quality and diversity are two conflicting\u00a0measures that are hard to meet. This paper is a groundbreaking work that proposes receiver operating curve or Pareto optimality for quality and diversity measures, and shows that simple temperature sweeping in MLE generates the best quality-diversity curves than all language GAN models through comprehensive experiments. It points out a good target that language GANs should aims at.\u00a0\n\nFor the experiments on long-text generation using EMNLP news 2017, it is not clear how the data is partitioned as training data, validation data and test data to get the results in Figures 4(a), moreover for the results for LeakGAN, RankGAN SeqGAN, MaliGAN, it seems that they are copied from other papers, but again how the data set is partitioned is not clear\u00a0 for example in SeqGAN's paper, and most likely, the data is partitioned in a different way, so the results are not comparable. The authors should run the code and get the results on it own.\n\nAn important RL-free language GAN paper is missing,\u00a0\nZhongliang Li, Tian Xia, Xingyu Lou, Kaihe Xu, Shaojun Wang, Jing Xiao: Adversarial Discrete Sequence Generation without Explicit NeuralNetworks as Discriminators. AISTATS 2019: 3089-3098.\nThis paper directly and adversarially train a language model without MLE pre-training and obtains good results, it is better to compare the results.\n\nTypo: page 1, the second line from bottom, as a computationally, not an", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 3}}}