{"paper": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "summary": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"]}, "meta": {"decision": "Reject", "comment": "This paper presents an understudied bias known to exist in the learning patterns of children, but not present in trained NN models.  This bias is the mutual exclusivity bias: if the child already knows the word for an object, they can recognize that the object is likely not the referent when a new word is introduced.  So that is, the names of objects are mutually exclusive. \n\nThe authors and reviewers had a healthy discussion. In particular, Reviewer 3 would have liked to have seen a new algorithm or model proposed, as well as an analysis of when ME would help or hurt.  I hope these ideas can be incorporated into a future submission of this paper."}, "review": {"BklyWIdlcS": {"type": "review", "replyto": "S1lvn0NtwH", "review": "This paper targets at studying the mutual exclusive bias which existed in children learning, to help understand whether there exists similar bias in deep networks. \n\nIn general, the whole paper tries to tell a very interesting, and good story. The paper is very well organized and written. However, I have the following concerns.\n\n1, the ME problem is quite similar to the concept ontology, e.g. , a \u201cDalmatian,\u201d a \u201cdog\u201d, or a \u201cmammal\u201d.  So what\u2019s the key difference? Hierarchical learners can avoid this problem.\n\n2, I would say, the SOTA neural networks fundamentally are just representation learned, i.e., feature representation learning. The learned features could in principle be employed to construct advanced learners, .e.g., hierarchical Bayesian. It\u2019s probably a bit unfair or misleading to claim neural networks suffering from ME bias. \n\n3, In Sec. 3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments. Please give more explanations.\n\n4, in Sec. 3.1, considering the small number of training instances, whether it is large enough to train the NN/ not overfitting issues?\n\n5. Whether the ME bias mostly attributed to the one-hot representation? If one uses word2vec as the representation in NN, the ME bias will be solved. Actually, this is the standard practice in some learning tasks, e.g., zero-shot learning. \n\n6, the experimental design of Sec. 4.2 is also a bit unfair. It seems to me that the tasks are organized as unbalanced instances of each classes, and asking the common NNs to learn this  task. Of course, this common NNs can not address it.\n\n----\nI read the rebuttal. the authors clarified and answered the questions. I would like to raise the score.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "S1xAWnjRFH": {"type": "review", "replyto": "S1lvn0NtwH", "review": "Summary:\n\nThis paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another. Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation. Authors argue that ME bias could help the model to handle new classes and rare events better.\n\nMy comments:\n\nI very much enjoyed reading this paper. I support accepting this paper. It highlights one of the missing inductive biases in ML and proposes it as a challenge. As the authors also agree, ME bias is missing not just in DNNs. It is the issue of MLE. It would be good to have some non-NN results too. I see this is a challenge for MLE than DNNs.\n\n1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training?\n2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail.\n3. Section 4.2 need more clarity. For example, what do you mean by classifying the image as \u201cnew\u201d? Is \u201cnew\u201d a class name? Also, how is P(N|t) computed? Please explain.\n4. Are the authors willing to release the code and data to reproduce the results?\n\nMinor comments:\n\n\n1. Page 3: second para, line 4: \u201cour aim is to study\u201d\n2. Page 5: last line: estimate for -> estimated for\n3. Section 4.2: 3rd line: \u201cthe class for the from\u201d\n\n=====================================================\n\nAfter rebuttal: I have read the authors' response and  I stand by my decision.\n", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 2}, "SkedBT5ntB": {"type": "review", "replyto": "S1lvn0NtwH", "review": "*** Increased to weak accept after discussion of merits of ME bias was improved in the paper *** \n\nThis paper investigates whether neural networks exhibit a \u2018mutual exclusivity (ME) bias\u2019, whereby novel inputs tend to be associated with previously unseen outputs, an inductive bias that is cited to be present in children when learning to associate new words and objects. Via three different sets of experiments, the authors conclude that, under standard training procedures, neural networks in fact display an anti-ME bias: when faced with a novel input, they tend to assign less probability mass to unobserved outputs than is justified by the incoming data. The authors go on to argue that explicitly designing neural networks to reason by mutual exclusivity could lead to faster and more flexible learning. While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias. The authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses ME reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of ME bias on task performance for a toy example.\n\nComments / questions:\n* The authors duly acknowledge that ME bias is not necessarily desirable in all circumstances, namely in tasks that feature many-to-one mappings, citing polysemy and synonymy in machine translation as examples. Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.\n* For the synthetic dataset one-to-one mapping of one-hot vectors it is clear that an ME bias would incur an advantage for classification on a zero-shot basis, i.e. an increased an accuracy for the first time a new input is observed. Is ME bias considered to be useful here because (i) we care directly about improving this type of zero-shot classification, or is it (ii) because it's implicitly assumed that a better initial prediction will lead to faster learning on these examples?\n    * If it is (i) we care about, then it would be useful to quantify the advantage gained either empirically or analytically. It\u2019s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes). So at the beginning of training, while there are lots of new classes to benefit from the ME bias, the maximum advantage per example is small, and vice versa at the end of training - how do these conflicting effects balance each other out over the course of training and how do the benefits of ME bias scale with the number of classes? Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.\n    * The authors say that \u201cME can be generalized from applying to 'novel versus familiar\u2019 stimuli to instead handling \u2018rare versus frequent\u2019 stimuli\u201d and they cite the fact that neural networks take longer to learn from rare stimuli, suggesting that ME could help for reason (ii) above. It\u2019s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.\n* It would be interesting to see a more detailed analysis of the predictions of the image classifiers in section 4.2. When a new image is presented, how is the probability mass distributed across previously seen classes versus across unseen ones? From a strategic standpoint, in a scenario where it is difficult to determine whether a new image belongs to a new class or not, could it plausibly be more sensible for the network to make a strong prediction on its best guess out of previously seen classes (that it knows more about) rather than a scattered prediction on the unseen classes? Admittedly this is a difficult question to quantify, but my point is to question whether the anti-ME bias shown in Figure 6 is necessarily suboptimal, given the difficulty of classifying a new image as a new class or not.\n\nMinor comments /questions not affecting review:\n* Is the acronym ME pronounced like the word \u201cme\u201d or is it spelled out \u201cM-E\u201d? If the latter, then all cases of \u201ca ME bias\u201d should be corrected to \u201can ME bias\u201d.\n* Section 4.2 line 3: \u201csample the class [for the] from a power law distribution\"", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 1}, "BklvRdsjsr": {"type": "rebuttal", "replyto": "H1ew95gooS", "comment": "Thanks for continuing the discussion of our work. Your point is well received and we agree that the ME bias is not appropriate for every use case. Moreover, even if it is optimal given the inherent tradeoffs in any inductive bias, it\u2019s bias that is going to get some cases right and other cases wrong. As you rightly point out, the issue is complex and warrants more nuance in the discussion, and in response we uploaded another revision that expands the discussion in this way (see Section 5). We summarize the main points here too. As we say, the ME bias may not be helpful in every case, but it\u2019s equally clear that the status quo is sub-optimal: models shouldn\u2019t have a strong anti-ME bias regardless of what the task and dataset demands. So how do we proceed? An ideal model would decide for itself how strongly to use ME based on the task demands. For instance, in our synthetic example, an ideal learner would discover the one-to-one correspondence and use this as a meta-strategy (through a perfect ME bias). If the dataset has more many-to-one correspondences, it would adopt another meta-strategy. This meta-strategy could even change depending on the stage of learning. There are promising results in this direction: Santoro et al. (2016) and Lake (2019) [see references in paper] do not build a ME strategy into the model, but rather observe one as an emergent consequence of meta learning. We see potential in this direction but there are other means of tackling the challenge too. We hope that by introducing this challenge, our paper will stimulate debate and ultimately progress in addressing it. Our discussion is now more appropriately nuanced, and we thank you again for your feedback on this point.\n", "title": "A discussion on the utility of ME"}, "rklpjoQ9oH": {"type": "rebuttal", "replyto": "S1xAWnjRFH", "comment": "Thank you for your supportive review. \nWe answer the specific queries below and have also added them to the revised version of the paper. \n1.         We found that the entropy regularizer produces an ME score that stays constant across training, at the cost of the model being less confident about predictions made for seen classes. We added details regarding this condition to the manuscript.\n2.         The base rate is the probability of observing a new word in the target at that particular point in training. We go through the remaining sentences in the corpus from the target compute the probability of sampling a sentence with at least one new word. Thus, the base rate at time t in training is defined as:\n$$P(\\text{new in target at t}) = \\frac{ \\text{# of unseen sentences in target with new words}} {\\text{# of unseen sentences}}$$\n3.         In Section 4.2, we use \u201cnew\u201d to refer to the set of all the unseen classes at a particular timepoint t. For the classifier, P(N|t) is calculated by adding the probabilities the model assigns to all the \u201cnew\u201d classes when iterating through the remaining corpus (similar to Equation 1 in our paper). For the dataset, we compute P(N|t) by sampling all unseen images in the corpus and compute the proportion from \u201cnew\u201d classes given their ground truth labels.\n4.         We will release our code and data with the publication of the paper. Most of our experiments are easy to replicate as they use standard datasets, models, loss functions and optimizers. We sincerely hope that our challenge and these resources will stimulate progress in this area.\n\nPlease also see above where we write a general response to all reviews.", "title": "Specific Response to R1"}, "HkeodF7qir": {"type": "rebuttal", "replyto": "SkedBT5ntB", "comment": "We thank you for your constructive feedback. We reply to all reviews in a general response above.", "title": "Thank you and please see above"}, "rklsz575oB": {"type": "rebuttal", "replyto": "BklyWIdlcS", "comment": "Thanks for your feedback. We reply to all reviewers jointly in our comments above.", "title": "Thank you and please see general response above"}, "rklKFaQ9jr": {"type": "rebuttal", "replyto": "B1xHDTm9iS", "comment": "Our paper is unique for introducing a challenge that does not yet have a solution, but we see this as the best way forward for stimulating research in this important area. There is a wide gap between the centrality of ME in human language development and the strong anti-ME effects we found in standard DNNs, and MLE-based approaches more generally. We see our work as the beginning of a larger effort to integrate ME and inductive biases from cognitive science into modern AI models, and we thank you for considering our paper in your further discussions.", "title": "General Response (2/2)"}, "B1xHDTm9iS": {"type": "rebuttal", "replyto": "S1lvn0NtwH", "comment": "We thank each of the reviewers for their thoughtful feedback on our work. We uploaded a revised paper that incorporates your suggestions, and we describe the changes in our response below. To echo R1, our paper \u201chighlights one of the missing inductive biases in ML and proposes it as a challenge.\u201d We demonstrate that popular deep neural network (DNNs) architectures do not show the mutual exclusivity (ME) bias that children use to help learn new words; in fact, they have an anti-ME bias that is completely backwards. We show how this anti-ME bias is poorly aligned with machine translation and object recognition tasks on common datasets, especially in more realistic lifelong learning settings. Our paper proposes a challenge to develop DNN architectures that can use the ME bias, like children, to support better zero-shot inferences and rapid learning.\n\nAs requested by R2, we clarify how the synthetic experiments (Section 3) implement the classic ME paradigm from Ellen Markman. As shown in Fig. 1 of our paper, children tend to select a novel object over a familiar object when asked to \u201cShow me the dax\u201d (Markman and Wachtel, 1988). Translating this into a synthetic experiment, input units denote words and output units denote objects. We ask the neural network to \u201cShow me the dax\u201d by activating the \u201cdax\u201d input unit and asking it to select amongst possible referents. The network produces a probability distribution a set of candidate referents, but it can make relative (two object) comparisons by isolating the scores of two candidates referents. To quantify the overall propensity toward ME, our \u201cME score\u201d measures the relative probability of choosing any of the novel objects (as opposed to the familiar objects), which directly translates to higher scores on Markman\u2019s forced choice task. We revised the task description in Section 3.1 to make these links clearer.\n\nR2 offers suggestions for how to get models to exhibit the ME bias, and we appreciate your ideas here. We have been thorough in our explorations and have tried some of these, without success, which is why we see ME as such an interesting challenge! We have revised the paper to provide more details related to your suggestions, which we summarize here. R2 suggests that the anti-ME bias may arise from \u201cthe small number of training instances,\u201d but we have verified that DNNs do not learn the ME regularity no matter how much data is presented, either in one-to-one synthetic mappings (Section 3) or on real datasets (Section 4). We clarify this in Section 3.1. The suggestion to use pre-trained embeddings (word2vec) is interesting but not pertinent to the ME paradigm which always tests novel words which are out of sample (\u201cdax\u201d, \u201czup,\u201d \u201cfep\u201d, etc.), corresponding to novel concepts in a lifelong learning setting. Additionally, R2 suggests that adding hierarchy through a concept ontology or a hierarchical Bayesian learner might help induce the bias. This is an interesting idea that we can explore in future work, and we added it to the discussion section, but it\u2019s not obvious to us that adding superordinate level categories like \u201canimal\u201d that have multiple referents would help map novel names to novel referents. Finally, with regards to the unbalanced classes in Section 4 (as mentioned by R2), an intelligent learner should be capable of modeling an open world that is inherently unbalanced, like children and adults do, with the possibility of encountering a new class at any point.\n\nR3\u2019s main critique is that it is hard to quantify the improvement ME would provide, in part because we do not \u201cprovide an empirical analysis of an algorithm that uses ME reasoning to improve learning.\u201d Simply put, we can\u2019t provide such an analysis because no such algorithm exists (yet)! This is why we are challenging the community to work in this direction, and we see our experiments as a demonstration that there will be applications on real tasks (Section 4). There is a clear misalignment between the inductive biases of standard neural nets and the statistical structure of common tasks especially when interpreted as lifelong learning. Recently, there have been some demonstrations of using ME to improve performance on specific tasks [Santoro et al. (2016), Lake, Linzen and Baroni (2019), Cohn-Gordon (2019), Lake (2019); see paper for bibliography], but none of these approaches represent a general solution. We see a general solution as an advance that will improve both zero-shot predictions and the speed of learning after just a few examples of a new input, by mitigating the strong bias to familiar responses.\n \nFinally, we thank R1 for their positive feedback on our work. R1 asked for elaboration on some of the technical details of how some of the quantities are computed. We respond to your comment directly with answers to the questions and make corresponding updates to the main text. \n", "title": "General Response (1/2)"}}}