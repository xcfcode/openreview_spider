{"paper": {"title": "DeepOBS: A Deep Learning Optimizer Benchmark Suite", "authors": ["Frank Schneider", "Lukas Balles", "Philipp Hennig"], "authorids": ["frank.schneider@tuebingen.mpg.de", "lukas.balles@tuebingen.mpg.de", "philipp.hennig@uni-tuebingen.de"], "summary": "We provide a software package that drastically simplifies, automates, and improves the evaluation of deep learning optimizers.", "abstract": "Because the choice and tuning of the optimizer affects the speed, and ultimately the performance of deep learning, there is significant past and recent research in this area. Yet, perhaps surprisingly, there is no generally agreed-upon protocol for the quantitative and reproducible evaluation of optimization strategies for deep learning. We suggest routines and benchmarks for stochastic optimization, with special focus on the unique aspects of deep learning, such as stochasticity, tunability and generalization. As the primary contribution, we present DeepOBS, a Python package of deep learning optimization benchmarks. The package addresses key challenges in the quantitative assessment of stochastic optimizers, and automates most steps of benchmarking. The library includes a wide and extensible set of ready-to-use realistic optimization problems, such as training Residual Networks for image classification on ImageNet or character-level language prediction models, as well as popular classics like MNIST and CIFAR-10. The package also provides realistic baseline results for the most popular optimizers on these test problems, ensuring a fair comparison to the competition when benchmarking new optimizers, and without having to run costly experiments. It comes with output back-ends that directly produce LaTeX code for inclusion in academic publications. It supports TensorFlow and is available open source.", "keywords": ["deep learning", "optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "The field of deep learning optimization suffers from a lack of standard benchmarks, and every paper reports results on a different set of models and architectures, likely with different protocols for tuning the baselines. This paper takes the useful step of providing a single benchmark suite for neural net optimizers. \n\nThe set of benchmarks seems well-designed, and covers the range of baselines with a variety of representative architectures. It seems like a useful contribution that will improve the rigor of neural net optimizer evaluation. \n\nOne reviewer had a long back-and-forth with the authors about whether to provide a standard protocol for hyperparameter tuning. I side with the authors on this one: it seems like a bad idea to force a one-size-fits-all protocol here. \n\nAs a lesser point, I'm a little concerned about the strength of some of the baselines. As reviewers point out, some of the baseline results are weaker than typical implementations of those methods. One explanation might be the lack of learning rate schedules, something that's critical to get reasonable performance on some of these tasks. I get that using a fixed learning rate simplifies the grid search protocol, but I'm worried it will hurt the baselines enough that effective learning rate schedules and normalization issues come to dominate the comparisons.\n\nStill, the benchmark suite seems well constructed on the whole, and will probably be useful for evaluation of neural net optimizers. I recommend acceptance.\n\n"}, "review": {"B1xd12Y037": {"type": "review", "replyto": "rJg6ssC5Y7", "review": "This paper presents a new benchmark suite to compare optimizer on deep neural networks. It provides a pipeline to help streamlining the analysis of new optimizers which would favor easily reproducible results and fair comparisons.\n\nQuality\n\nThe paper covers well the problems underlying the construction of such a benchmark, discussing the problems and models selection, runtime estimation, hyper-parameter selection and visualizations. It falls short however in some cases:\n\n1. Hyper-parameter optimization\n    While they mention the importance of hyper-parameter tuning for the benchmark, they leave it to the user to tune them without providing any standard procedure. Furthermore, they use grid search to build the baselines while this is known to be a poor optimizer [1].\n\n2. Estimated runtime\n    Runtime is estimated for a single set of hyper-parameters of the optimizer, but some optimizer may have similar or roughly similar results for a large set of hyper-parameters that widely affects the runtime. The effect of the hyper-parameters should be taken into account for this part of the benchmark.\n\n3. Interpretation\n    Such a benchmark should makes it easier for interpretation of results as the authors suggests. However, the paper does not convey much interpretation in section 4, beside the fact that results are not conclusive for any baseline. Results of the paper seem low, but they are difficult to verify since the plots are not very precise. For instance Wide ResNet-18-8 reports 1.54% test accuracy on SVHN [6] while this paper reports ~ 15% for the Wide ResNet 18-4 version. Figure 2 is a good attempt at making interpretations of sensitivity of optimizers' hyper-parameters but has limited interpretability compared to what can be found in the literature [2].\n\n4. Problems\n    There is an effort to provide varied types of problem, including classical optimization functions, image classification, image generation and language modeling. The number of problems consists mostly of image classification however and is very limited for image generation and language modeling.\n\nClarity\n\nThe paper is well written and easy to understand in general. \n\nOn a minor note, most figures are difficult to read. Side nodes on figure 1 does not divide clearly without any capital letter or punctuation at the end of sentence. Figure 2 should be self contained with its own legend. Figure 3 is useful for a visual impression of the speed of convergence but a histogram would be necessary for a better visual comparison of the different performances.\n\nSection 2.2 has a confusing terminology for the \"train valid set\". Is it a standard validation set? \n\nOriginality\n\nThere is virtually no benchmarks for optimizers available for the community. I believe a standardized procedure for comparing optimizers can be viewed as an original contribution. \n\nSignificance\n\nReproducibility is a problem in machine learning [3, 4] and optimizers' efficiency on deep neural networks generalization performance is still not very well understood [5]. Therefore, there is a strong need for a benchmark for sound comparisons and to favor better reproducibility.\n\nConclusion\n\nThe benchmark presented in this paper would be an important contribution to the community but lacks a few important features in my opinion, in particular, sound hyper-parameter optimization procedure and sound interpretation tools. On a skeptical note, I doubt the benchmark will be used extensively if the results it provides yield no conclusive interpretation as reported for the baselines. As I feel there is more work needed to support the goals of the paper, I would suggest this paper for a workshop. Nevertheless, I would not be upset if it was accepted because of the importance of the subject and the originality of this work.\n\n[1] Bergstra, James, and Yoshua Bengio. \"Random search for hyper-parameter optimization.\" Journal of Machine Learning Research 13, no. Feb (2012): 281-305.\n[2] Biedenkapp, Andre, Joshua Marben, Marius Lindauer and Frank Hutter. \u201cCAVE : Configuration Assessment , Visualization and Evaluation.\u201d In International Conference on Learning and Intelligent Optimization (2018).\n[3] Lucic, Mario, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. \u201cAre GANs Created Equal? A Large-Scale Study.\u201d arXiv preprint arXiv:1711.10337 (2017).\n[4] Melis, G\u00e1bor, Chris Dyer, and Phil Blunsom. \u201cOn the state of the art of evaluation in neural language models.\u201d arXiv preprint arXiv:1707.05589 (2017).\n[5] Wilson, Ashia C., Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. \"The marginal value of adaptive gradient methods in machine learning.\" In Advances in Neural Information Processing Systems, pp. 4148-4158. 2017.\n[6] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016\n\n-----------\nRevision\n-----------\n\nIn light of the discussion with the authors, the revision made to chapter 4 and in particular the proposed modifications to section 2.4 for a camera-ready paper, I revise my score to 6.", "title": "An important _first_ step towards standardized procedures for benchmarking optimizers in deep learning.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkeqB9k30Q": {"type": "rebuttal", "replyto": "H1lmbe0tC7", "comment": "We do think that the protocol we suggest is sound and provides insightful analyses, even though we agree that we should strive for even better practices in the future.\n\nRegarding the hyperparameter tuning: During the last revision we added the following comment to Chapter 4 (which discusses the baseline results): \"While we are using a log grid search, researchers are free to use any other hyperparameter tuning method, however this would require re-running the baselines as well.\"\n\nUnfortunately, the ICLR revision period has ended, but should this paper be accepted, we would like to put more emphasis on this comment in the camera-ready version. We would also like to add some clarifying comments to Section 2.4 (which discusses hyperparameter tuning) to\n- explain our choice for a log grid search as the baseline hyperparameter tuning procedure,\n- point out and cite other hyperparameter tuning methods,\n- and emphasize that using another hyperparameter tuning method requires re-running the baselines with that protocol.\n\nWe thank you for your comments and suggestions and are looking forward to the final decision.", "title": "Fourth Response"}, "B1g4zLh-C7": {"type": "rebuttal", "replyto": "SkgW52MsaQ", "comment": "Thanks for your continuing interest in the conversation! We answer to your points briefly below, but at this point we think it is important to return to the core premise of the paper and not get tied up in details. We understand that you would like to have the ideal solution. But at this point practice in the field is very far away from what you are asking for. We wanted to provide a benchmark tool that is practical, easy to use, and does not impose unrealistic or counterproductive constraints on researchers. We continue to believe that our work significantly improves the current standards in benchmarking for deep learning.\n\nBriefly on your points:\n\n1. You asked for \"one set of baselines optimized with a specific [hyperparameter optimization] framework defined by you\". We are providing that, using the most basic framework possible: a (log) grid search with a fixed budget. You are arguing that this is a bad method and we wholeheartedly agree. But we are sure that you would agree that it might well be the most common method in practice. As you point out yourself, researchers are free to use any other hyperparameter optimization method, given that they re-tune the hyperparameters of the baseline methods (SGD, etc.) using the same method (we will add a paragraph to the paper to point this out more clearly). The baselines provided by us are purely for convenience and that's why we think it makes sense to keep the overhead needed to compare a new method to them as small as possible by choosing a simplistic tuning protocol.\n\n2. We acknowledge that a runtime evaluation across hyper-parameter settings would be ideal (many things are ideal, but not all are feasible). We do not enforce this because for many methods (in particular first-order ones) runtime per step is independent of the hyper-parameters, and it seems silly to force people to show this over and over again. Note, though, that anyone can use the provided tools to provide exactly the sweep you suggested. We include a remark in the updated paper, suggesting to estimate runtime as a function of hyper-parameter settings where necessary.\n\n3. We plan to include SGD (as well as Momentum and Adam) with learning rate decay schedules as a second, more challenging baseline in the future.", "title": "Third Response"}, "HylEM34GT7": {"type": "rebuttal", "replyto": "Byxn8_ae6Q", "comment": "A general remark regarding all your comments. There is a very important trade-off between exhaustive benchmarking (all the information and plots that we want to see) and ease of use/computational cost. When designing DeepOBS we tried to not increase the (computational) effort for the researchers, while substantially increasing the quality of the benchmark. \n\n1. We take your point about hyperparameter fitting. But let\u2019s be clear: There is no widely accepted framework for the adaptation of hyperparameters such as step and batch sizes, etc. Which framework would you recommend we impose on our community to provide a level playing field for *all* ongoing research in deep learning optimization? Hypergradients, Learning to learn, Probabilistic Line searches, or Barzilei-Borwein? If we decided to pick any of these and force people to use it, would that convince you to accept this paper? And do you think it would increase the user base, or rather restrict it?\n\n2. What you are describing is definitely something that is desirable. However, a runtime sensitivity would increase the effort to run such a benchmark drastically, especially in the case of many hyperparameters, which is why it is not common in deep learning optimization papers. One possible trade-off would be to estimate the runtime of the optimal hyperparameter setting of each test problem and report it in Table 2 along with the iterations as a relative factor. This would require at most 8 runtime estimations, while still providing some insights into what runtimes are to be expected in practice.\n\n3. We are looking into our setup of this test problem to see why it produces worse results than reported in the paper. We don't believe that a drastically better results can be obtained by simply tuning the learning rate more. When looking at Figure 2, we would argue that it seems very unlikely that a drastically improved performance can be found from a learning rate, we didn't test. If sampling the hyperparameters even more (say on a loggrid with 100 points) would convince you, we can update our baselines accordingly.\n\nWe realize now that calling it a \"train eval *set*\" might be confusing. It is not a separate data set (it is just in many ways implemented like one). What we call \"train eval set\" is just the evaluation of the training data set in the same fashion you would evaluate the test set (same size, not using dropout, etc.) This is a better estimator for training performance, than using the regular mini-batch train loss and train accuracy, we get while training. Since we want to assess training performance, it would not make sense to use a validation set.", "title": "Second Response"}, "Syx2vZqgTQ": {"type": "rebuttal", "replyto": "B1xd12Y037", "comment": "Dear Reviewer 2,\n\nthank you very much for your constructive review.\nWe are happy that you agree with us that a benchmarking suite would be an important step. While we acknowledge that the presented solution is not optimal, we would argue that it significantly improves on the current status quo. Just like Reviewer 1, we worry \"that people will still find minor quibbles with particular choices or tasks in this suite, and therefore continue to use bespoke comparisons\". We believe that the improvement of DeepOBS compared to the status quo (which often is to just use MNIST and CIFAR10 and compare to SGD or Adam) is larger than the step from DeepOBS to where we hope to be.\n\nWe also want to address the shortcomings you mentioned in your review.\n\n1. We believe that we can split this critique in two aspects. Firstly, the hyper-parameter optimization that we do for our baselines. While, we agree that grid search is not at all an optimal approach, we would argue that it is the method most common in practice (for example [1, 2]). The main goal of our baselines are to be a realistic comparison. We plan to include more sophisticated baselines in the future, for example ones that include learning rate decay schedules. We could tune these schedules with more complex methods than grid search, to provide a more challenging competition.\nThe second part is that we don't provide a hyperparameter tuning method for the user. We did this on purpose. A hypothetical user of DeepOBS might want to highlight that their new optimization method gets good results using default hyperparameter values, while also showing that tuning those parameters a little bit can give you even better results. Therefore, we believe that the choice of hyperparameter tuning method should be left to the user. As long as they document this tuning process, and report the final hyperparameters on each test problem, the results are still comparable even when different tuning methods are used.\n2. It is an interesting point you raised here. Indeed we only estimate the runtime for a single set of hyperparameters. However, the used hyperparameters for this estimation is flexible. In the scenario that you describe, the best option for the user of DeepOBS would be to do the estimation step twice for both settings and report both numbers.\n3. We will indeed double-check the results of the Wide ResNet on SVHN. In contrast to the original paper, we do not use Nesterov momentum, nor a learning rate decay schedule. We also train for less epochs. The point of the test problems is not to provide state of the art results, but to compare the performances of optimization methods. Nevertheless, we will check our SVHN results and are currently running new experiments. Thanks for pointing this out.\n4. While we agree that the set of test problems is a bit biased towards image classification, we also believe that this set is much more exhaustive than what is currently used in practice (which is often just MNIST and CIFAR10). If there is a specific test problem that you would like us to add, we would gladly do so. We see this set of test problems as a starting point and DeepOBS can be continuously improved and extended.\n\nWe also tried to address the notes on clarity and changed the figures accordingly.\n\nIn section 2.2 we mention a \"train eval set\", which is not a standard validation set. We use this train eval set, whenever we want to evaluate our training performance. We distinguish between using the training data to train, and using the training data to evaluate the performance on it. During this \"training evaluation phase\", we evaluate on the a set that is as large as the test set and also use the neural network in architecture in \"evaluation mode\" (for example we do not use dropout). This allows for a fairer comparison between test loss and train loss as both are computed in the same way.\n\n\nWe hope that by addressing your points we were able to alleviate some of your concerns. You agree with us and the other reviewers that a benchmarking suite for deep learning optimizer would be a significant step and a useful tool for the field and that currently no such tool exists. We kindly ask you to reconsider your evaluation of the paper in light of this response.\n\n\n[1] Diederik Kingma, and Jimmy Ba. \"Adam: A Method for Stochastic Optimziation\" Proceedings of 3rd\nInternational Conference on Learning Representations (ICLR), 2015.\n\n[2] Tao Lin, Sebastian Stich, and Martin Jaggi. \"Don't Use Large Mini-Batches, Use Local SGD\" arxiv, 2018.", "title": "Response to the Comments of Reviewer 2"}, "S1xprW9lp7": {"type": "rebuttal", "replyto": "r1eC9mIc2X", "comment": "Dear Reviewer 3.\n\nthank you for your positive review. We are happy that you agree with us, that benchmarking stochastic optimization methods is a relevant project.\n\nWe also want to address some of the points you have raised.\n\nCons:\n1) We agree, that it might take a large effort to convince others to use and contribute to DeepOBS. We designed DeepOBS to be as easy as possible to add new optimization methods. As long as you can implement your new optimizer in TensorFlow, you can add it to DeepOBS by sending us a pull request. We will invest time to run new optimization methods ourselfs, and, provided they give state-of-the-art performance, add them to the baselines.\nAdditonally, benchmarking new optimization methods can take a lot of time, from setting up realistic test problems to computing fair baselines. With DeepOBS, this is unnecessary and researchers can spend more time on developing their optimization methods and less time on thinking about the benchmarking aspect. We hope that this is incentive enough to use DeepOBS.\n2) We agree, that offering DeepOBS in other frameworks could be beneficial. However, we chose TensorFlow as it is arguably the most popular framework at the moment, and we had to start somewhere. We want to note that the actual software implementation is only a part of this paper.\n3) If you can point us to some examples of bad writing in the paper, we would be very happy to address and re-write them and improve or clarify the sections.\n\nWe also addressed the minor points you mentioned:\nWe changed the names in Figure 1 to be more consistent. We hope that the picture is now more informative.\nIn the current version, Figure 2 and 3 are switched now.\nWe fixed the \"?\" in Table 1. It was the result of a typo in a citation. Thanks for noting this.\nWe added an explanation for Table 2.\n\nPlease note, that by making these changes the paper is now longer than 8 pages. We will work to reduce it to 8 pages again for the final version.", "title": "Response to the Comments of Reviewer 3"}, "Bye00eclTQ": {"type": "rebuttal", "replyto": "r1l6IbIc2X", "comment": "Dear Reviewer 1.\n\nthank you very much for your positive review.\n\nWe want to address the minor comments you raised.\n-  We added a remark in section 2.3 regarding the automated estimation of per-iteration cost in DeepOBS.\n- With the current setup, computing the baseline performances on all 26 test problems would require more than 3500 runs. As these test problems also include the ImageNet data set, this could take quite a while. We therefore doubt, whether we could finish this in time for ICLR. However, we will add these results to the DeepOBS package as soon as they are finished so that the software package has baselines performances for all test problems.\n- Thank you for the reference. We will look into performance profiles to see how we can use them.", "title": " Response to the Comments of Reviewer 1"}, "r1l6IbIc2X": {"type": "review", "replyto": "rJg6ssC5Y7", "review": "The authors propose a benchmark for optimization algorithms specific to deep learning called DeepOBS. They provide code to evaluate an optimizer against a suite of standard tasks in deep learning, and provide well tuned baselines for a comparison. The authors discuss important considerations when comparing optimizers, including how to measure speed and tunability of an optimizer, what metric(s) to compare against, and how to deal with stochasticity.\n\nA clear, standardized optimization benchmark suite would be very valuable for the field. As the others clearly state in the introduction, there have been many proposed optimization algorithms, but it is hard to compare many of these due to differences in how the optimizers were evaluated in the original papers. In general, people have different requirements for what the expect from an optimizer. However, this paper does a good job of discussing most of the factors that people should consider when choosing or comparing optimizers. Providing a set of well tuned baselines would save people a lot of time in making comparisons with a new optimizer, as well as providing a canonical set of tasks to evaluate against. I particularly appreciated the breadth and diversity of the included tasks.\n\nI am a little worried that people will still find minor quibbles with particular choices or tasks in this suite, and therefore continue to use bespoke comparisons, but I think this benchmark would be a valuable resource for the community.\n\nSome minor comments:\n- In section 2.3, there is a recommendation for how to estimate per-iteration cost. I would mention in this section that this procedure is automated and part of the benchmark suite.\n- I wanted to see how the baselines performed on all of the tasks in the suite (not just on the 8 tasks in the benchmark sets). Perhaps those figures could be included in an appendix.\n- The authors might want to consider including an automated way of generating performance profiles (https://arxiv.org/abs/cs/0102001) across tasks as part of DeepOBS, as a way of getting a sense of how optimizers performed generally across all tasks.", "title": "An important and useful tool for the field.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1eC9mIc2X": {"type": "review", "replyto": "rJg6ssC5Y7", "review": "As the paper claims there is no common accept system for benchmarking deep learning optimizer. It is also hard to repeat others' results. The paper describes a benchmarking framework for deep learning optimizer. It proposes three performance indicators, and includes 20 test problems and a core set of benchmarks. \n\nPro: \n1) It is a very relevant project. There is a need for unified benchmarking framework. In traditional optimization field, benchmarking is well studied and architectured. See an example at http://plato.asu.edu/bench.html\n2) The system is at its early stage, but its design seems complete\n3) The paper shows some performance of vanilla SGD, momentum, and Adam\n\nCon:\n1) It will take tremendous efforts to convince others to join the party and contribute\n2) It only support tensorflow right now\n3) Writing can be better\n\nIn Figure 1, make sure the names of components are consistent: either all start with nouns or verbs. The whole picture is not too illustrative. \n\n\n\nCan switch the order of Figure 2 and Figure 3?\n\nIn Table 1, the description of ALL-CNN-C has a '?'. Is it intended?\n\nWhy not explain Table 2? \n\n\n\n", "title": "Good initiative at its beginning stage", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}