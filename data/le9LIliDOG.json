{"paper": {"title": "Efficient Long-Range Convolutions for Point Clouds", "authors": ["Yifan Peng", "Lin Lin", "Lexing Ying", "Leonardo Zepeda-Nunez"], "authorids": ["pyf04142017@sjtu.edu.cn", "~Lin_Lin1", "~Lexing_Ying1", "~Leonardo_Zepeda-Nunez1"], "summary": "", "abstract": "The efficient treatment of long-range interactions for point clouds is a challenging problem in many scientific machine learning applications. To extract global information, one usually needs a large window size, a large number of layers, and/or a large number of channels. This can often significantly increase the computational cost. In this work, we present a novel neural network layer that directly incorporates long-range information for a point cloud. This layer, dubbed the long-range convolutional (LRC)-layer, leverages the convolutional theorem coupled with the non-uniform Fourier transform. In a nutshell, the LRC-layer mollifies the point cloud to an adequately sized regular grid, computes its Fourier transform, multiplies the result by a set of trainable Fourier multipliers, computes the inverse Fourier transform, and finally interpolates the result back to the point cloud. The resulting global all-to-all convolution operation can be performed in nearly-linear time asymptotically with respect to the number of input points. The LRC-layer is a particularly powerful tool when combined with local convolution as together they offer efficient and seamless treatment of both short and long range interactions. We showcase this framework by introducing a neural network architecture that combines LRC-layers with short-range convolutional layers to accurately learn the energy and force associated with a $N$-body potential.  We also exploit the induced two-level decomposition and propose an efficient strategy to train the combined architecture with a reduced number of samples.", "keywords": ["global convolution", "point cloud", "graph-cnn", "NUFFT"]}, "meta": {"decision": "Reject", "comment": "This work proposes an efficient method for modelling long-range connections in point-cloud data. Reviewers found the paper to be generally well-written. On the less positive side, reviewers felt that the novelty of the work was marginal, and that the experimentation, limited to synthetic data in one domain, was too limited. These concerns remain after the discussion phase. In addition, the authors stated during the discussion that \"Our goal is indeed to develop an efficient strategy to model LRIs in real chemical and materials systems\u201d, which conflicts with the presentation of the work as motivated by more general point cloud modelling problems. Given these weaknesses, the final decision was to reject."}, "review": {"rr9e3-tD-55": {"type": "rebuttal", "replyto": "79A_NK33Fgy", "comment": "-------\n\n*A bit more of commenting on the experiments' results would be appreciated. For instance, it seems that the 2-scale training strategy is especially efficient (not needing many samples) when the LRIs are sufficiently strong (figure 3, right). This is probably an effect of the \"screening\" of LRI by short-range Interactions when alpha_1 is too strong compared to alpha_2, making it harder to learn about LRIs. Also, the fact that all curves essentially collapse beyond a given N_sample could receive a comment. A couple of remarks like that about the strengths and limits of the approach would be nice.*\n\n\n**Answer**\\\nThank you for the observation. Indeed, figure 3 (now figure 4 in the revised manuscript) seems to indicate that the two-scale strategy is more efficient when the LRI is stronger, in the sense that the test error is less sensitive with respect to the number of training samples. However, note that when the LRI is stronger, the test error is also slightly larger. Thus we cannot conclude that our training strategy becomes more efficient in the presence of strong LRIs. The qualitative behavior is also present in the 1D example (now figure 3 (left) in the revised manuscript). We have added some comments on page 9.\n\nThe fact that the curve collapse is due basically to the training dynamics' stability, i.e., using different random seeds to train the network provides very similar accuracy. We have added several comments on pages 7 and 8 and some comments on Appendix C.5.\n\n--------\n\n\n*Aside from these 2 main comments, I have minor remarks of presentation:*\n\n*figure 2 left and figure 3 both sides: poor choice of colors/linestyles. Curves come in pairs, and this should be suggested in the choice of display, using similar colors for pairs, and different linetyles in different pairs. It would improve readability (also, think of the color blind people!)*\n\n\n**Answer**\\\nThank you for your comment. We have modified the plots to improve readability. \n\n-------\n\n\n*fig2 , right: the O(N), O(N^2) scalings are un-readable. make them parallel to the plots you do and simple black dashed to improve clarity.*\n\n\n**Answer**\\\nThank you for your comment. We have modified the plots.\n\n--------\n\n*In the tables, although here results are \"trivial\" (always the same line has the smallest error), you could use the convention of putting in bold the better numbers.*\n\n**Answer**\\\nThank you for your comment we have, modified the Table and used a bold font for the best results.\n\n-------\n\n*Table 1, you say \"mu_2 can be arbitrary here\". What you mean is that it needs not be defined, because alpha_2=0 ? I found this sentence confusing (maybe it's just me).*\n\n**Answer**\\\nThank you for your comment. What we meant, is that given that $\\alpha_2$ is zero, $\\mu_2$ can take any particular value. We have modified the phrasing of the sentence to avoid unnecessary confusion. \n", "title": "Answers to comments (part 2) "}, "79A_NK33Fgy": {"type": "rebuttal", "replyto": "KthYYqQls1a", "comment": "Thank you for your review and feedback, please find below the answers to your comments.\n\n*Edit: I was unaware that papers could be submitted to arXiv simultaneously, I am sorry for that. Here is my (very late) review. I made it before reading other reviewers' reviews.*\n\n*An efficient method for fitting long-range style interactions in point clouds is presented. It makes use of NUFFT rather than convoluting a long-ranged (thus system-size and expensive) kernel directly in real space. Along with this architecture, a method for training it efficiently is presented. This 2 steps strategy consists in training the short-range part of the kernels well with a lot of (supposedly inexpensive) short range data, and fitting the long-range kernels with less data in a second time.*\n\n*Overall, the paper is clearly written and clearly exposes the methods used. The results are interesting for applications but do not seem ground-breaking (although I am not an expert of point clouds -specialized networks). In terms of experimental results, I think a couple of points would deserved to be answered (see below).*\n\n*In conclusion, I think the paper is marginally above acceptance level.*\n\n*In terms of results, the paper clearly shows that NUFFT scales essentially like O(N) (with N the points number) whereas naive direct space convolution scales as O(N^2). However, when I read the algorithm (page 3), my main curiosity is : how well does the algorithm deals with large systems (large domain \\Omega), and the competing parameters seems to be the resolution of the function g_\\tau (which plays the role of mollified dirac) versus the system size \\Omega or L. Concretely, I expect that for too large \\tau/L (presented in appendix, Equation 20), the precision of the long range kernels will be poor and error will be large; and in the opposite case of very small \\tau/L, precision will be good but compute time will increase (I guess it would increase as FFT does, in O(L/\\tau log(L/\\tau))). Probably there is a regime where direct convolution (which does not need the approximation introduced by g_\\tau, as far as I understood), is better then the NUFFT approach introduced here. I would guess that in the large system size (large \\Omega=L^d) and small particles number N limit (i.e. the low density limit), the direct approach does well ? I think such a discussion (and possibly a couple of experiments) would improve the paper a lot, showing the limits of the method and letting the reader understand the reasons for its strengths (which are very real, I do believe it !)*\n\n**Answer**\\\nThank you for your comment; this is an excellent point. \n\nWe agree that if $\\tau/L$ is very large, the underlying grid $N_{FFT}$ will be too small, and the error would increase due to an inadequate sampling of the kernel in the Fourier domain. Similarly, if $tau/L$ is too small, i.e., when $N_{FFT}$ is large, the computational cost would increase accordingly. \n\nWe have added a new section in the Appendix, namely Appendix C.5, to illustrate this issue. The results in Appendix C.5 show that the current methodology is relatively insensitive to the density of the Fourier grid (and the underlying gridding in space). This means that in practice, we may be able to choose a relatively larger $\\tau$, and hence a relatively small $N_{FFT}$ without sacrificing accuracy. \n\nWe agree that the crossover between the NUFFT approach and the direct approach will be system dependent, and the crossover point is very worth understanding in the neural network context. However, since we received this comment at a relatively late stage of the review, we were not able to finish running the results for an in-depth study of this matter. Hence we plan to explore this direction more thoroughly in future work.\n\n\n", "title": "Answers to comments (part1)"}, "n4H6UhkOQz-": {"type": "rebuttal", "replyto": "zm2EgYTfCcp", "comment": "Thank you for your review and feedback, please see below the answers to your comments.\n\n*The paper proposes an efficient long-range convolution method for point clouds by using the non-uniform Fourier transform. The long-range convolutional (LRC)-layer mollifies the point cloud to an adequately sized regular grid, computes its Fourier transform, multiplies the results by a set of trainable Fourier multipliers, computes the inverse Fourier transform, and finally interpolates the result back to the point cloud. The method is demonstrated to be effective by a N-body problem.*\n\n*Overall, the paper is clearly written with high quality. The originality of the paper seems to be not very strong since it directly adapts the NUFFT to this work. Besides that, there are several concerns about this paper that need to be addressed:*\n\n\n**Answer**\\\nThank you for your comments. The contribution of this paper is two-fold: \n- the introduction of the NUFFT to problems involving point-cloud, which to our knowledge has not appeared in the literature. \n- the introduction of a two-scale training procedure that could be helpful in the context of large scale simulation, where one can only have access to a limited number of snapshots. \n\nEven though we agree that the NUFFT is itself a well-known technique, it has not been used to accelerate the performance of neural network layers in machine learning problems. Our work introduces this method to a broader audience and provides a systematic way to treat long-range interactions, which is competitive with state-of-the-art localized networks. \n\n-------\n\n\n*How to choose the grid size L_{FFT} in the Fourier Space? How does this parameter affect the results?*\n\n**Answer**\\\nThank you for your comment. This is an excellent question. We have added a new section in the Appendix, namely Appendix C.5 to illustrate this issue. The results in Appendix C.5 show that the current methodology is relatively insensitive to the density of the Fourier grid (and the underlying gridding in space). We can expect that when $L_{FFT}$ is small, i.e., when we have a coarse gridding resolution, the error would increase due to the inability to sample the kernel Fourier domain adequately. However, due to the smooth parametrization of the kernel in Fourier domain, we can use a relatively small $L_{FFT}$ with a small penalty to the accuracy, even if we severely reduce $N_{FFT}$ as shown in the newly added Appendix C.5. In a nutshell, as long as the parametrized kernel is properly sampled in Fourier domain, the method can efficiently capture the LRIs.\n\nBesides Appendix C.5, we have added a couple of comments on page 4.\n\n-------\n\n\n*The global pooling layers in DNN can also capture the long-range information to some extent, and are also very efficient. How does the LRC compare with the global pooling layers?*\n\n\n**Answer**\\\nThank you for your comment. Yes, global pooling layers can capture long-range information, particularly when the output is a single number. In our case, however, we suppose that both that input and output are point-clouds, and, to our knowledge, there is no standard implementation of global pooling layers in this context. One non-standard fashion of implementing a global pooling layer in this context is the short-range neural network that we use as a comparison. In a nutshell, we can think of the short-range neural network in Eq. 5 as the composition of a very non-linear convolution layer followed by a global pooling layer, as shown in Eq. 3, which outputs the energy, plus a derivative of the output with respect to the inputs, which produces the forces. As shown in the experiments, in particular, Tables 1, 2 and 3, long-range interactions can not be seamlessly captured, even when using global pooling layers.\n\nTo make this clearer, we added a sentence on page 3 to make sure that we are focusing on problems in which the input and output are point clouds; thus, traditional global pooling layers may not be readily applicable. Also, we added a small paragraph on page 6, which provides a concise interpretation of the short-range network as a non-linear graph-convolution followed by a global pooling layer.\n\n", "title": "Answer to comments"}, "WYzq0_SRSt3": {"type": "rebuttal", "replyto": "K6oGTndTwG", "comment": "Thank you for your review and feedback.\n\n \n*Concerns:*\n \n*1. In the introduction part, the authors describe many tasks that rely on point-cloud presentation. However, the authors ignore pointing out the existing issues. The authors should clearly present them.*\n\n\n**Answer**\\\nThank you for raising this question. In a nutshell, we are aiming to obtain a long range convolutional layer that satisfies four requirements: \n-it can seamlessly respect symmetries, such as rotation, translation and permutation invariance, \n-it has a nearly linear cost with respect the number of input/output dimension\n-it can generalize to bigger problems seamlessly\n-it can capture long-range interactions accurately.\n\nIn particular, machine learning methods can satisfy two or perhaps three items on our wish list, but so far, to the best of our knowledge no method can satisfy all of them. We have added several comments in the introduction to discuss explicitly the issues above. \n\n----------\n\n*2. In the algorithmic section, the authors claim that \"NUFFT serves as the cornerstone of the LRC-layer\". So \"NUFFT\" is the unique solution? Whether or not some operations can replace FFT?*\n\n\n**Answer**\\\nThank you for your comment. You are right; NUFFT is the cornerstone of the LRC-layer presented in this paper. One can use, e.g., the fast multipole method (FMM) or other fast summation methods to achieve nearly-linear time. However, we find that using the NUFFT makes the implementation more straightforward, given that we only need to train the Fourier multiplier while keeping the rest of the operations unchanged. On the other hand, when using fast summation methods such as FMM, one needs to change the full algorithm if the kernel $\\phi_{\\theta}$ changes. We added a paragraph and a footnote on page 3 to further explain this point.\n\n----------- \n \n*3. Point-cloud is indeed important for many tasks as described in the introduction part, but the authors just explore the effects of the proposed algorithm in a \"synthetic\" experiment. The experimental results are not convincing for readers, the authors should conduct more real-world tasks to verify the effectiveness of the proposed method.*\n\n**Answer**\\\nThank you for your comment. Our goal is indeed to develop an efficient strategy to model LRIs in real chemical and materials systems. This is the first work along this line, and we selected the model N-body potential as the target application in order to unambiguously demonstrate that 1) the computational scaling is nearly linear, and 2) the sample complexity for large scale data can be reduced. The latter is particularly important in molecular modeling, since the training data are usually obtained from first principle electronic structure calculations, and the large scale data are very expensive to obtain.  We expanded the discussion along this line both in the introduction (page 3) and the conclusion (page 9). \n\n------\n \n*4. The presentation of this work needs to improve, if possible, the authors should provide an intuitive schematic diagram to present the procedure of proposing this idea.*\n\n**Answer**\\\nThank you for your comment. We have added a diagram of the algorithmic pipeline of the LRC-layer, in particular, in the revised manuscript we illustrate in Fig. 1, how the NUFFT computes the Fourier Transform, which is the cornerstone of our instance of the LRC-layer.\n\n--------- \n\n*5. In the experimental section, the author should replot Figure2-3 to ensure clear enough for a better read.*\n\n**Answer**\\\nThank you for your comment. We have redrawn those plots to increase readability.\n\n--------- \n\n\n#########################################################################\n \n*Minor Comments:*\n \n*(1) \"N_sample\" and \"N\" maybe exist the inclusion relation\uff0cit will be better to replace one of them with the other form;*\n \n\n**Answer**\\\nThank you for your comment. Actually they are different: $N_{sample}$ is the number of snapshots used for training, whereas, $N$ is the number of particles (or points in the cloud point) in each snapshot. We have added a comment in the last paragraph in page 7 making the distinction clear.  \n\n--------- \n\n\n*(2) The formula system is a little vague, if possible, the authors can simplify them to clearly describe.*\n\n\n**Answer**\\\nThank you for your comment. We added a table listing all the symbols and their meaning in the appendix.\n\n", "title": "Answers to comments "}, "sfgv0xrau5R": {"type": "rebuttal", "replyto": "bTR8mYhdojA", "comment": "\n*The quantitative experimental results in Tables 1, 2, and 3 dwell on varying the screening coefficient mu, hence investigates how the test error changes for point configurations with varying screening parameters, which signifies the amount of diffusion of the potential. It can be seen that the test error for the proposed approach falls as the screening gets higher, that is when we have more localized interactions between the particles. What could be the significance of this finding for a practical application is not discussed, therefore not made clear.*\n\n**Answer**\\\nThank you for your comment. \n The main message of Tables 1, 2 and 3 is to study the behavior of the error as the interaction length becomes larger. Even though this is a potential similar to a diffusion equation, we don't consider the tuning \\mu to tune the diffusion. Instead we use it to tune the interaction length between different particles, i.e., in the computation of the energy and forces in Eq. 2, how many particles actually contribute to the calculation at each point. Thus, as we reduce mu, i.e.,  as we increase the characteristic interaction length, the number of particles contributing to the Energy and Forces increase.\nThus in Tables 1, 2, and 3 we observe that the accuracy of local networks decreases as this interaction length increases, however, we can be recover the accuracy by using the new architecture, which consists of a local network plus one single LRC-layer, which accounts for the LRIs. \n\nPerhaps the most direct consequence of this paper related to the molecular modeling community. So far applications of machine learning to molecular dynamics have mostly considered short localized interaction. This works has the potential to broaden the spectrum of application to more system in which the long-range interactions can not be easily neglected.\n\nWe have added a paragraph in the introduction, as well as the conclusion to clarify these points.\n\n--------\n\n*The authors state that the proposed method could be a useful tool in a wide range of Machine Learning tasks. However, in the paper, only estimation of potential energy for Coulomb particle configurations is demonstrated as an application. I am curious to see how the presented approach could find use in learning tasks of real point cloud configurations.*\n\n**Answer**\\\nThank you for the comment. This is the first work along this line, and we indeed expect that the LRC-layer can be useful in other contexts, such as machine learning tasks of regression and classification types. One of our current research direction is to employ the LRC layer for image classification processing tasks, and we will report the performance in a future work.", "title": "Answers to comments (part 2) "}, "bTR8mYhdojA": {"type": "rebuttal", "replyto": "0WQeS4qeZCk", "comment": "Thank you for your review and feedback, please find below the answer to your comments. \n\n*The paper is clearly written, and presents an approach to efficiently utilize long range convolutions through a nonuniform FFT in for coulomb particle configurations.*\n\n*Pros:*\n\n*Presents a long range convolution layer, with an efficient implementation so that a neural network model can benefit from both short and long range interactions among data points.*\n*To model pairwise relations between points of the N-body potential, two short range descriptor networks are utilized, where the input relations are constructed both repulsively and attractively.*\n\n*Cons: Experimentally, the new long-short range NN model is validated only for computing the energy and forces for a N-body model. There are no experiments with real-life point cloud data.*\n\n**Answer**\\\nThank you for your comment. Please find below our response to the comment above, as well as  the question\n\n\"The paper presents a way to incorporate long range convolutions in neural networks, which could be beneficial. However, experimental evaluation is not satisfactory, as immediate implications in point cloud analysis is not obvious.\" \n\nOur goal is indeed to develop an efficient strategy to model LRIs in real chemical and materials systems. This is the first work along this line, and we selected the model N-body potential as the target application in order to unambiguously demonstrate that 1) the computational scaling is nearly linear, and 2) the sample complexity for large scale data can be reduced. The latter is particularly important in molecular modeling, since the training data are usually obtained from first principle electronic structure calculations, and the large scale data are very expensive to obtain.  We expanded the discussion along this line both in the introduction (page 3) and the conclusion  (page 9). \n\nConcerning the real-life point cloud data, the evaluation of N-body potential is a foundational component in molecular modeling. It takes the atomic positions as the input point cloud, and the output (local potential and force) can also be viewed as point clouds. We expect the LRC-layer to become a useful component in designing neural networks for modeling real chemical and materials systems, where the LRI cannot be accurately captured using short ranged models.\n\n-------\n*In the proposed algorithm, a sampling in a regular grid is required. A fixed size Cartesian grid is utilized in the experiments, therefore, the effect of the gridding resolution is not analyzed.*\n\n**Answer**\\\nThank you for your comment. This is an excellent question. We have added a new section in the Appendix, namely Appendix C.5 to illustrate this issue. The results in Appendix C.5 show that the current methodology is relatively insensitive to the density of the Fourier grid (and the underlying gridding in space). We can expect that when $L_{FFT}$ is small, i.e., when we have a coarse gridding resolution, the error would increase due to the inability to sample the kernel Fourier domain adequately. However, due to the smooth parametrization of the kernel in Fourier domain, we can use a relatively small $L_{FFT}$ with a small penalty to the accuracy, even if we severely reduce $N_{FFT}$ as shown in the newly added Appendix C.5. In a nutshell, as long as the parametrized kernel is properly sampled in Fourier domain, the method can efficiently capture the LRIs.\n\nBesides Appendix C.5, we have added a couple of comments on page 4.\n\n", "title": "Answers to comments (part 1)"}, "KthYYqQls1a": {"type": "review", "replyto": "le9LIliDOG", "review": "Edit: I was unaware that papers could be submitted to arXiv simultaneously, I am sorry for that. Here is my (very late) review.\nI made it before reading other reviewers' reviews.\n\nAn efficient method for fitting long-range style interactions in point clouds is presented.\nIt makes use of NUFFT rather than convoluting a long-ranged (thus system-size and expensive) kernel directly in real space.\nAlong with this architecture, a method for training it efficiently is presented. This 2 steps strategy consists in training the short-range part of the kernels well with a lot of (supposedly inexpensive) short range data, and fitting the long-range kernels with less data in a second time.\n\nOverall, the paper is clearly written and clearly exposes the methods used.\nThe results are interesting for applications but do not seem ground-breaking (although I am not an expert of point clouds -specialized networks).\nIn terms of experimental results, I think a couple of points would deserved to be answered (see below).\n\nIn conclusion, I think the paper is marginally above acceptance level.\n\n\nIn terms of results, the paper clearly shows that NUFFT scales essentially like O(N) (with N the points number) whereas naive direct space convolution scales as O(N^2).\nHowever, when I read the algorithm (page 3), my main curiosity is : how well does the algorithm deals with large systems (large domain \\Omega), and the competing parameters seems to be the resolution of the function g_\\tau (which plays the role of mollified dirac) versus the system size \\Omega or L. Concretely, I expect that for too large \\tau/L (presented in appendix, Equation 20), the precision of the long range kernels will be poor and error will be large; and in the opposite case of very small \\tau/L, precision will be good but compute time will increase (I guess it would increase as FFT does, in O(L/\\tau log(L/\\tau))).\nProbably there is a regime where direct convolution (which does not need the approximation introduced by g_\\tau, as far as I understood), is better then the NUFFT approach introduced here. I would guess that in the large system size (large \\Omega=L^d) and small particles number N limit (i.e. the low density limit), the direct approach does well ?\nI think such a discussion (and possibly a couple of experiments) would improve the paper a lot, showing the limits of the method and letting the reader understand the reasons for its strengths (which are very real, I do believe it !)\n\n\nA bit more of commenting on the experiments' results would be appreciated. For instance, it seems that the 2-scale training strategy is especially efficient (not needing many samples) when the LRIs are sufficiently strong (figure 3, right). This is probably an effect of the \"screening\" of LRI by short-range Interactions when alpha_1 is tooo strong compared to alpha_2, making it harder to learn about LRIs.\nAlso, the fact that all curves essentially collapse beyond a given N_sample could receive a comment.\nA couple of remarks like that about the strengths and limits of the approach would be nice.\n\n\nAside from these 2 main comments, I have minor remarks of presentation:\n- figure 2 left and figure 3 both sides: poor choice of colors/linestyles. Curves come in pairs, and this should be suggested in the choice of display, using similar colors for pairs, and different linetyles in different pairs. It would improve readability (also, think of the color blind people!)\n- fig2 , right: the O(N), O(N^2) scalings are un-readable. make them parallel to the plots you do and simple black dashed to improve clarity.\n\nIn the tables, although here results are \"trivial\" (always the same line has the smallest error), you could use the convention of putting in bold the better numbers.\n\nTable 1, you say \"mu_2 can be arbitrary here\". What you mean is that it needs not be defined, because alpha_2=0 ?\nI found this sentence confusing (maybe it's just me).\n\n", "title": "good improvement of existing method is presented, experiments could be improved", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "zm2EgYTfCcp": {"type": "review", "replyto": "le9LIliDOG", "review": "The paper proposes an efficient long-range convolution method for point clouds by using the non-uniform Fourier transform. The long-range convolutional (LRC)-layer mollifies the point cloud to an adequately sized regular grid, computes its Fourier transform, multiplies the results by a set of trainable Fourier multipliers, computes the inverse Fourier transform, and finally interpolates the result back to the point cloud. The method is demonstrated to be effective by a N-body problem.\n\nOverall, the paper is clearly written with high quality. The originality of the paper seems to be not very strong since it directly adapts the NUFFT to this work. Besides that, there are several concerns about this paper that need to be addressed:\n\n1. How to choose the grid size L_{FFT} in the Fourier Space? How does this parameter affect the results?\n\n2. The global pooling layers in DNN can also capture the long-range information to some extent, and are also very efficient. How does the LRC compare with the global pooling layers?\n\n", "title": "An interesting work", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "K6oGTndTwG": {"type": "review", "replyto": "le9LIliDOG", "review": "This paper concentrated on exploring how to efficiently extract the interaction information from the point clouds.  A key point lies in utilizing the non-uniform Fourier transform, rather than the regular Fourier transform. However, there exist some issues that need to be solved. \n\n+ves: \n+ The exploration of long-range interactions for point clouds is interesting.\n\n+ The paper is well written. The related work makes a clear description of many fields about point cloud.\n\nConcerns: \n1. In the introduction part, the authors describe many tasks that rely on point-cloud presentation. However, the authors ignore pointing out the existing issues. The authors should clearly present them.\n\n2. In the algorithmic section, the authors claim that \"NUFFT serves as the corner-stone of the LRC-layer\". So \"NUFFT\" is the unique solution? Whether or not some operations can replace FFT? \n\n3. Point-cloud is indeed important for many tasks as described in the introduction part, but the authors just explore the effects of the proposed algorithm in a \"synthetic\" experiment. The experimental results are not convincing for readers, the authors should conduct more real-world tasks to verify the effectiveness of the proposed method.\n\n4. The presentation of this work needs to improve, if possible, the authors should provide an intuitive schematic diagram to present the procedure of proposing this idea.\n\n5. In the experimental section, the author should replot Figure2-3 to ensure clear enough for a better read.  \n\n#########################################################################\n\nMinor Comments:  \n\n(1)  \u201cN_sample\u201d and \"N\" maybe exist the inclusion relation\uff0cit will be better to replace one of them with the other form;\n\n(2)  The formula system is a little vague, if possible, the authors can simplify them to clearly describe.\n ", "title": "This paper needs to make changes in some aspects.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "0WQeS4qeZCk": {"type": "review", "replyto": "le9LIliDOG", "review": "The paper is clearly written, and presents an approach to efficiently utilize long range convolutions through a nonuniform FFT in for coulomb particle configurations. \n\nPros: \n- Presents a long range convolution layer, with an efficient implementation so that a neural network model can benefit from both short and long range interactions among data points. \n\n- To model pairwise relations between points of the N-body potential, two short range descriptor networks are utilized, where the input relations are constructed both repulsively and attractively. \n\nCons:\nExperimentally, the new long-short range NN model is validated only for computing the energy and forces for a N-body model. There are no experiments with real-life point cloud data.\n\nIn the proposed algorithm, a sampling in a regular grid is required. A fixed size Cartesian grid is utilized in the experiments, therefore, the effect of the gridding resolution is not analyzed.\n\nThe quantitative experimental results in Tables 1, 2, and 3 dwell on varying the screening coefficient mu, hence investigates how the test error changes for point configurations with varying screening parameters, which signifies the amount of diffusion of the potential. It can be seen that the test error for the proposed approach falls as the screening gets higher, that is when we have more localized interactions between the particles. What could be the significance of this finding for a practical application is not discussed, therefore not made clear. \n\nThe authors state that the proposed method could be a useful tool in a wide range of Machine Learning tasks. However, in the paper, only estimation of potential energy for Coulomb particle configurations is demonstrated as an application. I am curious to see how the presented approach could find use in learning tasks of real point cloud configurations.\n\nThe paper presents a way to incorporate long range convolutions in neural networks, which could be beneficial. However, experimental evaluation is not satisfactory, as immediate implications in point cloud analysis is not obvious. ", "title": "Utilizing the nonuniform FFT, a long range convolutional layer (LRC) is presented. A neural network which combines both LRC and short range conv layers are built. In a two-scale training strategy, first many small-scale data are trained without the LRC, then a small set of large-scale data are trained with both short and long range conv layers. The model is tested on 1D and 2D screened Coulomb particle configurations to estimate the overall potential energy and forces. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}