{"paper": {"title": "Unsupervised Domain Adaptation through Self-Supervision", "authors": ["Yu Sun", "Eric Tzeng", "Trevor Darrell", "Alexei A. Efros"], "authorids": ["yusun@berkeley.edu", "etzeng@eecs.berkeley.edu", "trevor@eecs.berkeley.edu", "efros@eecs.berkeley.edu"], "summary": "We use self-supervision on both domain to align them for unsupervised domain adaptation.", "abstract": "This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data. Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously.  Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain.  The presented objective is straightforward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.", "keywords": ["unsupervised domain adaptation"]}, "meta": {"decision": "Reject", "comment": "Thanks for your detailed replies to the reviewers, which helped us a lot to clarify several issues.\nAlthough the paper discusses an interesting topic and contains potentially interesting idea, its novelty is limited.\nGiven the high competition of ICLR2020, this paper is still below the bar unfortunately."}, "review": {"HJxNH7jusH": {"type": "rebuttal", "replyto": "rklFudcVqB", "comment": "Thank you for your thoughtful review. We have added qualitative comparisons in Appendix G of our latest revision (page 16).\n", "title": "Thank you and answers to your questions"}, "BJxK3M2OsH": {"type": "rebuttal", "replyto": "Bygt1CBfsH", "comment": "For the ICCV 2019 paper, please see our reply to reviewer 1 for a thorough comparison of the differences, both algorithmic and conceptual.\n\nFor the domain generalization paper [Carlucci et al], we believe that you have misunderstood our words. We say in our paper that \u201cbecause their problem setting is very challenging, the accuracy is low for both their proposed method and the baseline.\u201d We are not criticizing Carlucci et al, but illustrating how their setting is different (as well as their method). You seem to think that we do not understand how their setting is different.\n\nWe are then asked to perform experiments under the domain generalization setting of Carlucci et al. First, since this is a setting that our paper does not work on, these experiments are irrelevant to our purpose. Second, such experiments are in fact undefined. What does it mean to run a domain adaptation method in the domain generalization setting, where no target data is available in any form?\n\nYou claim that our experiments are \u201cquite sub-par\u201d, because we could have obtained \u201clarge improvements by a combination of very small factors, such as data augmentation, network architecture, optimization method, and even hyperparameter tuning,\u201d citing your own experience as evidence, without a publication, reference or code. First, there is no reason why our improvements are especially prone to such problems, in comparison to previous work on the benchmarks we use, such as DIRT-T published at ICLR 2018. We do not use data augmentation to keep a fair comparison with the baselines, the hyperparameters are set by our selection rule, and we use the default optimization method that comes with our network architecture, which is widely adopted in all of computer vision and without our method performs no better than the source only results of the baselines.\n\nSecond, it is ambiguous what \u201clarge improvements\u201d are - over source only (no adaptation) or the previous methods? If over source only, then the runs of our method share all the \u201csmall factors\u201d mentioned in your comment as the runs of our source only, so the difference cannot be explained by these factors. If over the previous methods, this criticism is in fact undefined. Our method is not based on any of the baselines in Table 2, so does not even share their hyper-parameters; how can we then improve on them by hyper-parameter tuning?", "title": "The criticisms are based on misunderstandings and lack justification"}, "BkeNnpsdoS": {"type": "rebuttal", "replyto": "HkxybvFNqH", "comment": "Thank you for your time giving us feedback. Here we answer your numbered concerns.\n\n1. \u201cThe concept of self-supervision is not first proposed by this paper.\u201d Since being proposed in the 1990s, self-supervised learning has become a wide and vibrant field of inquiry,  with hundreds, if not thousands of papers published in respected venues. So, we are perplexed by the statement:  is this arguing that all these papers were published in error?  \n\u201cThe proposed method is not novel.\u201d  Such statements are unhelpful without references to prior work. We have stated in the introduction what we perceive to be the novelties of our method. Please provide references to previously published papers that render our novelties invalid.\n\u201cPerformance is not better than previous results such as DIRT-T.\u201d  Our results are shown in Table 2, and many of them are better than DIRT-T. Our method is also simpler and derived from a different perspective.\n\n2. Below are the requested results for R+L+F:\n\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\nSource\t\tMNIST\t\tMNIST\t\tSVHN\t\tMNIST\t\tMNIST\nTarget\t\tMNIST-M\tSVHN\t\tMNIST\t\tUSPS\t\tUSPS\n\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\nAccuracy (%)\t98.7\t\t        63.2\t\t        85.7\t\t        95.8\t\t        87.0\n\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\nThere is not much difference between these numbers and the ones for R only.\n\n3. \u201c[The authors] do not provide any way for how to design self-supervision task\u201d. Please see Section 3 titled \u201cdesigning self-supervised tasks for adaptation\u201d.\n\n4. Please see results on Office-31 in our reply to R3.\n\n5.  First, please note that ICCV 2019 papers are considered concurrent work, not prior work, to ICLR 2020 (ICCV\u201919 happened in November, whereas deadline for ICLR was in September).  Second, S4L, which is designed for semi-supervised learning, differs from ours both algorithmically and conceptually. We have already discussed this in the related work section in the context of semi-supervised learning methods, but to make our point clearer, here are the results for our implementation of the algorithm described in their equation (1) and (2) on MNIST -> MNIST-M, where improving upon the source only (no adaptation) baseline should have been very easy:\n\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\n\t\t\t        | Accuracy (%)\n\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\nSource only\t\t|  44.9\nS4L method\t\t|  56.6\nOur method\t\t|  98.9\n\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\nThe S4L result is barely better than source only, and qualitatively different from ours i.e. the difference should not come from merely implementation details. The most important difference between their algorithm and ours is that they train the supervised task on labeled data, and self-supervised task on unlabeled data, while we train the self-supervised task(s) simultaneously on both domains (labeled and unlabeled). Conceptually, training the self-supervised task on both domains is critical for alignment, which is the main objective for adaptation. Because for semi-supervised learning, the labeled and unlabeled data come from the same domain, methods for semi-supervised learning e.g. S4L do not need to consider the alignment problem. These comments are not intended to criticize S4L, as it is solving a different problem. In fact, theoretical analysis for semi-supervised learning [Cohen, Cozman] [Ghifary et al] suggests that training the self-supervised task on both domains is not helpful for semi-supervised learning; it is interesting to see how this picture is different for domain adaptation.\n\n\u201cI think it is an interesting paper, but not enough as a conference paper, maybe a workshop paper.\u201d   We are happy you found the paper interesting.  We do ask you to please reconsider your recommendation in light of the arguments presented above.  Also, as similar works using self-supervision as a tool, e.g. S4L, were published at respectable conferences instead of workshops, it seems reasonable to argue that this work, too, deserves to be accepted to ICLR. \n\nReferences:\nCohen, I., Cozman, F.G.: Risks of semi-supervised learning: how unlabeled data can degrade performance of generative classifiers. In: Semi-Supervised Learning. MIT Press (2006)\nMuhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep reconstruction-classification networks for unsupervised domain adaptation. In European Conference on Computer Vision, pp. 597\u2013613. Springer, 2016.\n", "title": "Thank you and answers to your questions "}, "rkgvKBsujH": {"type": "rebuttal", "replyto": "rkxxcB1Rtr", "comment": "Thank you and we are happy you found our paper thought-provoking. Here we address the cons you wrote:\n1. It is true that we have no theory backing our approach. On the other hand, it is rarely to see any deep learning paper with theory adequate enough to give \u201cguarantees\u201d for datasets we actually care about.\n2. This connects well with the comment at the end of the review, asking us for \u201cguidance as to how to choose the set of self-supervised tasks.\u201d We have in fact given some practical guidance in the paper, which we summarize below as two necessary conditions:\n- The self-supervised task is well defined and nontrivial on both domains. This rules out the case of rotation prediction on SVHN, since as we explain in the paper, \u201cthe rotation head learns to look at the periphery and cheat\u201d.\n- \u201cThe labels created by self-supervision should not require capturing information on the very factors where the domains are meaninglessly different.\u201d as said and explained in section 3. This is rules out tasks such as colorization and autoencoder, for which it is important to learn the low-level details of the image.\nThese two conditions are easy to reason about in practice. If the \u201cbattery of self-supervised tasks\u201d satisfy them, there should be notable improvement on top of the source only baseline as we observe empirically, but there won\u2019t be a guarantee. In addition, we would like this paper to add to the toolbox of available domain adaptation methods instead of becoming the only tool. When a good self-supervised task satisfying the two conditions cannot be found (SVHN), previous methods have provided different tools to use. When a good self-supervised task naturally exists, our method provides a simple and effective choice.\nIn the end, this is a valuable question from the reviewer and we plan to be more explicit about those conditions in the next revision.\n3. Please see results on Office-31 in our reply to R3.\n\nYour notes / questions: Thank you very much for pointing out our error with the highlighting. This is an honest typo. In the latest revision, we have improved our results to match that of DIRT-T; the modification we made for the improved results, as well as the original results, can be found in the last paragraph of Appendix B.", "title": "Thank you and answers to your questions"}, "rkxxcB1Rtr": {"type": "review", "replyto": "S1lF8xHYwS", "review": "This paper presents a novel unsupervised domain adaptation framework for neural networks. Similarly to existing approaches, it performs adaptation by aligning representations of the source and the target domains. The main difference is that this alignment is achieved not through explicitly minimizing some distribution discrepancy (this usually leads to challenging minimax optimization problems). Instead, the authors propose to use a battery of auxiliary self-supervised learning (SSL) tasks for both domains simultaneously. Each task is meant to align the source and the target representations along a direction of variation relevant to that task. Assuming that the battery is diverse enough, optimizing the representation for all the tasks leads to matching of the distributions. \n\nPros:\n+ The paper is well-written and easy to read.\n+ I like the simplicity of the idea and the fact that it achieves competitive performance without any adversarial learning (which may be very tricky to deal with).\n+ The paper presents a reasonable procedure for hyper-parameter tuning and early stopping which seems to work well in practice.\n\nCons:\n- The paper is purely practical with no theory backing the approach. As a result, the discussion of guarantees and limitations is quite brief.\n- It\u2019s unclear how easy it is to come up with a reasonable set of SSL tasks for a particular pair of domains. It seems that it may become a serious problem when the method is applied to something other than benchmarks. Table 2 reveals that there is no consistent improvement over the existing approaches which suggests that the chosen battery of SSL tasks is not universal (as the authors themselves admit). On a related note, it\u2019s a bit disappointing that the authors mention SVHN results as a failure case but never provide a way to address the issue.\n- It would be nice to some results for the Office dataset for completeness. The authors could use a pre-trained network as a starting points just like it\u2019s done in other papers. According to the last paragraph of Section 6 this experiment should be feasible.\n\nNotes/questions:\n* Table 2, last column: The performance of DIRT-T seems to be better than that of the proposed method and yet the latter is highlighted and not the former.\n\nOverall, I think it\u2019s a good paper presenting a thought-provoking idea. In my opinion, the weakest point of the work is the lack of any (neither principled nor practical) guidance as to how to choose the set of self-supervised tasks. Despite this I feel that this submission should be accepted but at the same time I\u2019m curious to see what the authors have to say regarding the concerns I raised in my review.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "rklFudcVqB": {"type": "review", "replyto": "S1lF8xHYwS", "review": "This paper describes an approach to domain adaptation that uses\nself-supervised losses to encourage source/target domain alignment for\nunsupervised domain adaptation. The authors propose to use four\nself-supervised tasks (variants of tasks used in the self-supervised\nrepresentation learning for object recognition literature) that are\nused with a combined loss including unlabeled source and target\ntraining samples. The authors also propose an alignment heuristic for\nguiding early stopping. Experimental results on a standard battery of\ndomain adaptation problems are given, plus some intriguing baseline\nresults for semantic segmentation.\n\nThe paper is written very well and the technical development and\nmotivations for each decision are well discussed and argued.\n\n1. The experimental evaluation is a bit limited as the object\n   recognition datasets are a bit limited. Results on Office or\n   Office-Home would be nice.\n\n2. Using location classification for semantic segmentation seems\n   intuitively to be encouraging the network to learn coarse spatial\n   priors (which should be invariant across the two domains). Have you\n   looked at how alignment is actually happening? More qualitative\n   analysis in this direction would be useful to appreciate the\n   proposed approach.\n\n3. Related to the previous point, it would be interesting to see how\n   semgmentations in the unsupervised domain gradually change and\n   improve with increasing alignment.\n\nIn summary: the ideas are simple, intuitive, and well-explained -- I\nthink the results reported would be easy to reproduce with minimal\nhead scratching. The experiments are interesting and not overstated.\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "HkxybvFNqH": {"type": "review", "replyto": "S1lF8xHYwS", "review": "This paper introduces an unsupervised domain adaptation method that uses self-supervised tasks to bring the two different domains closer together. It runs experiments on some classic benchmarks.\n\nMy score for this paper is weakly rejected because \n\n(1) the concept of self-supervision is not first proposed by this paper. The proposed method is not novel. It introduces three simple self-supervision tasks: flip, rotation and location, and the performance is not better than previous results such as DIRT-T; \n\n(2) there are 7 benchmarks in Table2, but only 2 of 7 has result on R+L+F. In the paper, it mentioned because the result is not better, but the author should still provide them. \n\n(3) it emphasizes the contribution of encouraging more study of self-supervision for unsupervised domain adaptation. It doesn\u2019t provide any way for how to design self-supervision task or whether more tasks is better. I think it is an interesting paper, but not enough as a conference paper, maybe a workshop paper. \n\n(4) there are some classic unsupervised domain adaption benchmarks like Office Dataset, and Bing-Caltech dataset, why not run the method on them?\n\n(5) In ICCV 2019, there is a paper \"S4L: Self-Supervised Semi-Supervised Learning\". The proposed method is almost same. I think the difference is this paper changes the setting and considers the unsupervised data as target domain and supervised data as source domain. ", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "SJx5Q7Ua5B": {"type": "rebuttal", "replyto": "SyxF2CBaqB", "comment": "Unlike what the comment said, we perform worse than [Shu et al. 2018] only on the two benchmarks using SVHN. The datasets are standard in domain adaptation and far from \"very small\". In addition, [Shu et al. 2018] has been a state-of-the-art method. Comparing with this strong method does not make ours \"weak\". The anonymous commenter also attributes our improvements to hyper-parameter tuning without any evidence. Our method is not based on [Shu et al 2018] or any other baseline in table 2; we cannot make improvements just by tuning hyper-parameters.", "title": "This comment is based on incorrect readings of table 2"}, "H1eZ5xsSKr": {"type": "rebuttal", "replyto": "B1xColtrYr", "comment": "We agree with this comment that recent works since 20`18 have better segmentation results. We would also like to emphasize that:\n- We are only claiming to improve segmentation results when our method is added on top of a prior work. Note that a separate self-supervised head can also be added to the prior works listed in the comment.\n- Segmentation is not the main result of the paper and only comprises a minor portion of our empirical section, while the methods listed above are explicitly designed for segmentation. In fact, all of our baselines in Table 1 have been accepted to major conferences without any result on segmentation, except CyCADA (which we do compare with on segmentation).", "title": "Our point is not to have state-of-the-art segmentation results"}, "r1x1BXafOr": {"type": "rebuttal", "replyto": "SJxUhmL6vr", "comment": "Office has an average of only 44 images per class per domain. Many other recent works e.g. many of our baselines do not use it because it is considered very small by the standard of modern deep learning.", "title": "Office very small"}}}