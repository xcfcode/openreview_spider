{"paper": {"title": "GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation", "authors": ["Chence Shi*", "Minkai Xu*", "Zhaocheng Zhu", "Weinan Zhang", "Ming Zhang", "Jian Tang"], "authorids": ["chenceshi@pku.edu.cn", "mkxu@apex.sjtu.edu.cn", "zhaocheng.zhu@umontreal.ca", "wnzhang@sjtu.edu.cn", "mzhang_cs@pku.edu.cn", "jian.tang@hec.ca"], "summary": "A flow-based autoregressive model for molecular graph generation. Reaching state-of-the-art results on molecule generation and properties optimization.", "abstract": "Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68\\% chemically valid molecules even without chemical knowledge rules and 100\\% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization. ", "keywords": ["Molecular graph generation", "deep generative models", "normalizing flows", "autoregressive models"]}, "meta": {"decision": "Accept (Poster)", "comment": "All reviewers agreed that this paper is essentially a combination of existing ideas, making it a bit incremental, but is well-executed and a good contribution.  Specifically, to quote R1:\n\n\"This paper proposes a generative model architecture for molecular graph generation based on autoregressive flows. The main contribution of this paper is to combine existing techniques (auto-regressive BFS-ordered generation of graphs, normalizing flows, dequantization by Gaussian noise, fine-tuning based on reinforcement learning for molecular property optimization, and validity constrained sampling). Most of these techniques are well-established either for data generation with normalizing flows or for molecular graph generation and the novelty lies in the combination of these building blocks into a framework. ... Overall, the paper is very well written, nicely structured and addresses an important problem. The framework in its entirety is novel, but the building blocks of the proposed framework are established in prior work and the idea of using normalizing flows for graph generation has been proposed in earlier work. Nonetheless, I find the paper relevant for an ICLR audience and the quality of execution and presentation of the paper is good.\""}, "review": {"Igv3PyV24z": {"type": "rebuttal", "replyto": "teE0IZ_tBr3", "comment": "Hi, Ziqi,\nWe are actually aware of this issue and will report the new results as soon as possible.", "title": "Thanks for pointing it out."}, "ByxZqYuWcr": {"type": "review", "replyto": "S1esMkHYPr", "review": "# Post Rebuttal\n\nThe authors have partially and satisfactorily addressed my concerns. In line of this I am raising my score to Weak Accept.\n\nThis paper proposes a new molecular graph generative model (GraphAF) which fuses the best of two worlds of generative networks - reversible flow and autoregressive mode. Such integration enjoys a) faster training due to parallel computation b) molecular validity checker during inference supported by sequential sampling process and c) exact likelihood maximisation due to invertible encoder. In lieu of such advantages, the model trains two times faster than the existing state-of-the-art and generates 100% valid molecules when trained on ZINC dataset. Further, it also demonstrates that additionally if the chemical properties are optimised during training with reinforcement learning policy then GraphAF outperforms all the prior works.\n\nAlthough the paper presents an interesting fusion of different generative models, in its current form it leans towards rejection due to the following factors:\n1) The empirical validation of GraphAF is contained to single dataset - ZINC with a maximum of 38 atoms. From the table 2, it seems to me every prior method works pretty well on important metrics. There is very little room for improvement. I recommend including results on QM9 and CEPDB datasets. \n2) The model being data-agnostic, it makes sense to evaluate them on generic graph datasets - synthetic and real.\n3) The novelty of the model is limited. The flow-based graph generative model is introduced in Graph Normalizing Flow (GNF) (NeurIPS'19, NeurIPS'18 workshop). The reversible flow is extended to whole graph in GraphNVP. Unlike GNF, GraphNVP and GraphAF do away with decoder. The major difference being the sampling process - one-shot to sequential.\nI am willing to improve my rating given that some of this points are addressed.\n\nClarification:\n1. What are the inputs edge-mlp's operate on ? Given the generation step is sequential, it is not clear to me why all the node embeddings H_i^L is given as input in eq (8). I also noted that the dimension of H_i^L varies with size of sub-graphs. Also note mismatch in the notation 'f' used in algorithm 1 and 'g' from the main text. \n2. Please compare inference time. \n\nOther weakness:\n1. Due to invertible flow modeling, the latent space is usually restricted to small dimension. In current case it is 9 for node feature and 3 for edge features. This drawback alongside the sequential edge generation prevents GraphAF from scaling to complex and large graphs with many labels.\n2. Moreover, GraphAF utilizes only single layer of flow i.e., eq (9). This is clearly not sufficient to model complex graphs. And in its current form it is not clear how one can extend to multi-layer flow.\n3. The encoder modeling in GraphAF also shares similarity with Variational graph auto-encoder. Instead of constraining latent distribution using KL divergence, GraphAF maximizes graph likelihood to enforce base distribution.\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "H1gP43WoiB": {"type": "rebuttal", "replyto": "rJgzzsWjjH", "comment": "Thanks for your quick response! Your reviews really help improve the paper. We really appreciate it.", "title": "Response"}, "ryxiyLlF_S": {"type": "review", "replyto": "S1esMkHYPr", "review": "This paper proposes a generative model architecture for molecular graph generation based on autoregressive flows. The main contribution of this paper is to combine existing techniques (auto-regressive BFS-ordered generation of graphs, normalizing flows, dequantization by Gaussian noise, fine-tuning based on reinforcement learning for molecular property optimization, and validity constrained sampling). Most of these techniques are well-established either for data generation with normalizing flows or for molecular graph generation and the novelty lies in the combination of these building blocks into a framework. Training can be carried out in parallel over the sequential generation process, as no hidden states with sequential dependency are assumed (unlike a regular RNN). Experimental validation is carried out on a standard ZINC molecule generation benchmark (graphs with up to 48 nodes) and the reported metrics are competitive with recent related work.\n\nOverall, the paper is very well written, nicely structured and addresses an important problem. The framework in its entirety is novel, but the building blocks of the proposed framework are established in prior work and the idea of using normalizing flows for graph generation has been proposed in earlier work (see [1] and [2]). Nonetheless, I find the paper relevant for an ICLR audience and the quality of execution and presentation of the paper is good.\n\nI have two major (technical) concerns with the flow-based formulation used in the paper with regards to order-invariance and the utilized de-quantization scheme.\n* Order-invariance: The paper states that the \u201cexact density of each molecule can be efficiently computed by the change-of-variables formula\u201d. This seems to be incorrect, as the exact density is a product over all order-specific densities for all possible permutations in which the molecular graph can be represented. The change-of-variables formula does not provide an efficient way to circumvent this order-invariance issue, at least not in the way it is presented in the paper. Even when using BFS-ordered representations, the subspace of possible permutations is still typically too large to allow for efficient evaluation of the exact density. I suspect that the authors assume a canonical ordering of the graph representations, which is a strong assumption, but does not seem to be mentioned in the paper. How is the canonical ordering chosen? How is local structural symmetry broken in a consistent manner?\n* De-quantization: The de-quantization scheme used in this paper seems to be ill-suited for categorical variables. What motivates the use of adding Gaussian noise to categorical (one-hot encoded) variables, other than that it seems to work OK in the reported experiments? Adding Gaussian noise in this way can move these variables outside of the probability simplex \u2014 is this a valid technique in the framework of normalizing flows? Adding Gaussian noise makes sense if the data represents quantized continuous data, e.g. bit-quantized image data, but I have concerns about the validity of using this method for categorical data (both edge type and node features are categorical in this application). Other comparable generative models for graph-structured data use a relaxed discrete distribution (concrete / Gumbel softmax), e.g. in MolGAN [De Cao & Kipf (2018)], to address this issue \u2014 would this also be applicable here?\n\nI think that these two issues will have to be addressed before this paper can be considered for publication, and I recommend a weak reject at this point.\n\n[1] Madhawa et al., GraphNVP: An invertible flow model for generating molecular graphs. (2019)\n[2] Liu et al., Graph Normalizing Flows. (2019) \u2014 not cited\n\n\nUPDATE:\n\nMy two main technical concerns have been addressed in the rebuttal and I think that the revised version of the paper can be accepted to ICLR (my comment w.r.t. novelty still holds and hence I recommend 'weak accept').", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "SJg3GOZssr": {"type": "rebuttal", "replyto": "H1e1KNvYsS", "comment": "Thank you for your quick response! The discussion is really helpful. Looking forward to your reply.  The answers to your concerns are listed below.\n\nQ3: As mentioned in my initial review, RealNVP and Glow use their de-quantization technique on bit-quantized image data (unless I misunderstand their technique) where simply adding noise is justified as it \"spreads out\" the discrete values. For binary or categorical data, however, I still think that this technique is problematic, as my point from my original review (adding noise can move points outside of the probability simplex) still holds even if you use uniform noise instead of Gaussian noise (apologies for this misunderstanding). GraphNVP seems to have a similar issue (this paper is currently under review at ICLR as well and one reviewer pointed out the same concern), so I think it is not valid to point to them for justification of this approach.\n\nA3:  Moving the points outside of the probability simplex actually does not matter as the normalizing flows are actually defined on general continuous data. The dequantization techniques allow mapping the discrete data into continuous one by adding a small noise to each dimension. By adding a noise from U[0,1), we can assure that the range of different categories will not overlap. For example, after dequantization, the value of 1-entry in the one-hot vector lie in [1,2) while the 0-entry lie in [0,1). We can also map the generated continuous data back to the discrete data by using the argmax function in the generation process.\nTheoretically, as shown in [1] (eq3-6) and [2],  training a continuous density model on uniform dequantized data can be interpreted as maximizing a lower bound on the log-likelihood for the original discrete data. This statement holds for both image data and binary/categorical data mathematically.  In addition, as suggested in [2], instead of adding random uniform noise to each discrete data for dequantization, a more advanced dequantization technique is to treat the noise as hidden variables and use variational inference to infer the optimum noise added to each discrete data. We will explore this in our future work.\n\nWe have added a section to discuss the dequantization techniques in the appendix. We are happy to further discuss this if you still have questions.\n\n[1] Theis, Lucas, A\u00e4ron van den Oord, and Matthias Bethge. \"A note on the evaluation of generative models.\" arXiv preprint arXiv:1511.01844 (2015).\n[2] Ho, Jonathan, et al. \"Flow++: Improving flow-based generative models with variational dequantization and architecture design.\" arXiv preprint arXiv:1902.00275 (2019).\n", "title": "Response to the AnonReviewer1  "}, "SyxTYv-iiS": {"type": "rebuttal", "replyto": "H1gdnWwFsr", "comment": "Thank you for your quick response! The discussion is really helpful. Looking forward to your reply.  The answers to your concerns are listed below.\n\nQ1: You mention that GraphNVP is not compatible with a reinforcement learning objective for fine-tuning. Is this because of the one-shot nature of the generation process that you refer to? One example that comes to my mind where a one-shot generative process is combined with an RL objective is MolGAN [1] -- maybe you can comment on this. \n\nA1: Very good point! Indeed, GraphNVP is compatible with the objective used in MolGAN. However, note that the approach used in MolGAN is not based on RL and is actually based on the one-step policy gradient algorithm, which is not the classical RL. The classical RL problem is a sequential decision process, which involves a series of states and actions. This process also allows us to introduce intermediate rewards (e.g., the penalization for valency check in each step defined in Section 4.4) and final rewards. For the approach used in MolGAN---which is only one-step decision---we are only able to provide final rewards but not able to leverage the intermediate rewards (e.g., the chemical rules for valency check). \n\nTo avoid misunderstanding, we have removed this statement in section 2.\n\n\nQ2: How do you canonically order nodes *within* the BFS front of the BFS-ordering? To me, it seems like typical BFS-ordering only gives you a partial ordering of the nodes in a graph as nodes within the BFS front are still ordered arbitrarily. Hence you would still not have the exact likelihood unless you find a way to break the symmetry consistently within the BFS front. Please correct me if I'm wrong.\n\nA2: Thanks for raising this point again. Yes, you are right that nodes within the BFS front are still ordered arbitrarily.  GraphAF is trained on all possible BFS orderings. This can be done by first randomly permuting the adjacency matrix and then randomly pick a node as BFS front. By canonical order(BFS-order), we meant that all the orders of graphs we used to train GraphAF are BFS orders. We meant the exact density of each molecule under a given order (which is sampled each time that  we load a batch of training graphs) can be efficiently computed by the change-of-variables formula. We have revised the section 4.2 because we think the word \u201ccanonical\u201d is a little bit misleading. Thanks for the suggestions!!\n", "title": "Response to the AnonReviewer1  "}, "H1gY_VP_iB": {"type": "rebuttal", "replyto": "S1esMkHYPr", "comment": "We would like first to thank all the reviewers for your constructive reviews. We\u2019ve revised the paper according to your reviews. Specifically, we have made the following changes: \n1. We conduct additional experiments on another two molecule data sets QM9 and MOSES and two generic graph data sets in Section 5.2. The results are available at Table 3, 4. Results show that our proposed method GraphAF can still get state-of-the-art or competitive results on these data sets. We also present some generated examples of generic graph in the appendix.\n2. We expand the description of normalizing flows to make the paper more self-contained. A citation of the survey paper on normalizing flows is also given for reference. We also explained the RL process in Section 4.4 in more details. \n3. We discuss the difference between our work and existing work including Graph Normalizing Flows and GraphNVP in more detail in Section 2.\n4. We give a detailed explanation on why GraphAF + RL pipeline works well on the tasks of property optimization in Section 5.2. \n5. We revise the statement of \u201ccalculating the exact density of each molecule\u201d with GraphAF to \u201ccalculating the exact density of each molecule under a given order\u201d.\n6. We added a section to discuss the dequantization techniques in the appendix. \n", "title": "Response to all the reviewers and area chair"}, "HklSxJ_diS": {"type": "rebuttal", "replyto": "BkxrWaDOor", "comment": "Hi,\nWe have just uploaded the paper. I believe it is all set now. \n\nBests", "title": "Response to the AnonReviewer2  "}, "ByeVKuvusr": {"type": "rebuttal", "replyto": "ryxiyLlF_S", "comment": "\nQ3: * De-quantization: The de-quantization scheme used in this paper seems to be ill-suited for categorical variables. What motivates the use of adding Gaussian noise to categorical (one-hot encoded) variables, other than that it seems to work OK in the reported experiments? Adding Gaussian noise in this way can move these variables outside of the probability simplex \u2014 is this a valid technique in the framework of normalizing flows? Adding Gaussian noise makes sense if the data represents quantized continuous data, e.g. bit-quantized image data, but I have concerns about the validity of using this method for categorical data (both edge type and node features are categorical in this application). Other comparable generative models for graph-structured data use a relaxed discrete distribution (concrete / Gumbel softmax), e.g. in MolGAN[3], to address this issue \u2014 would this also be applicable here?\nA3: Actually, instead of Gaussian noise, we used the uniform noise (Equation 5) for de-quantization.  \nThe same techniques have also been used in other normalizing flow methods for discrete data  (e.g. GraphNVP[3], RealNVP[5], Glow[6]) and also shown very effective. \n\nNote that Gumbel softmax and dequantization are techniques for two very different problems on discrete data. The former one is used to backpropagate the gradient through discrete variables, while dequantization is used to transform discrete data into continuous since the invertible mappings defined by normalizing flows are mainly for continuous data.\n\nWe hope the above response could address your concerns. Please let us know if you have other questions. We\u2019re happy to further answer. \n\n\n[1] Liu et al., Graph Normalizing Flows. arXiv 2019.05.\n[2] You et al. GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models. ICML 2018.\n[3] Madhawa et al., GraphNVP: An invertible flow model for generating molecular graphs. arXiv 2019.05.\n[4] Popova et al. Molecularrnn: Generating realistic molecular graphs with optimized properties. arXiv preprint arXiv:1905.13372, 2019.\n[5] Dinh et al., Density Estimation using Real NVP. ICLR\u201917.\n[6] Diederik P. Kingma, Prafulla Dhariwal. Glow: Generative Flow with Invertible 1\u00d71 Convolutions. NIPS\u201918.\n", "title": "Response to the AnonReviewer1 cont."}, "Bygv3OP_ir": {"type": "rebuttal", "replyto": "ryxiyLlF_S", "comment": "Thanks for your comments and suggestions. The response to some of your concerns are listed below:\nQ1: The framework in its entirety is novel, but the building blocks of the proposed framework are established in prior work and the idea of using normalizing flows for graph generation has been proposed in earlier work (see GraphNVP[4] and GNF). Nonetheless, I find the paper relevant for an ICLR audience and the quality of execution and presentation of the paper is good.\nA1: Our work is indeed related to the two work Graph Normalizing Flows[1] (GNF, we have added the missing reference) and GraphNVP[3]. However, our work is fundamentally different from their work. GNF defines a normalizing flow from a base distribution to the hidden node representations of a pretrained Graph Autoencoders.  The graph generation is done through two separate stages by first generating the node embeddings with the normalizing flows and then generate the graphs based on the generated node embeddings in the first stage. In GraphAF, we define an autoregressive flow from a base distribution to the molecular graph structures, which can be trained end-to-end. GraphNVP also defines a normalizing flow from a base distribution to the molecular graph structures. However, the generation process of GraphNVP is one-shot, which cannot effectively capture graph structures and also cannot guarantee the validity of generated molecules (only 40% validity rate). In our GraphAF, we formulate the generation process as a sequential decision process and effectively capture the subgraph structures based on graph neural networks, based on which we define a policy function to generate the nodes and edges. The sequential generation process also allows to incorporate the chemical rules. As a result, the validity of the generated molecules can be guaranteed (100% validity rate). Moreover, we can effectively optimize the properties of generated molecules by fine-tuning the policy with reinforcement learning, which is not feasible in GraphNVP.  \n\nQ2: Order-invariance: The paper states that the \u201cexact density of each molecule can be efficiently computed by the change-of-variables formula\u201d. This seems to be incorrect, as the exact density is a product overall order-specific densities for all possible permutations in which the molecular graph can be represented. The change-of-variables formula does not provide an efficient way to circumvent this order-invariance issue, at least not in the way it is presented in the paper. Even when using BFS-ordered representations, the subspace of possible permutations is still typically too large to allow for efficient evaluation of the exact density. I suspect that the authors assume a canonical ordering of the graph representations, which is a strong assumption, but does not seem to be mentioned in the paper. How is the canonical ordering chosen? How is local structural symmetry broken in a consistent manner?\nA2: Thanks for pointing this out!! Following existing work on graph generation\u2014GraphRNN[2] and MolecularRNN[4]\u2014we use the BFS-ordering of a graph (mentioned in Section 4.2). BFS-ordering has been shown very effective for graph generation in previous work, which can effectively limit the number of edge predictions made for each node and hence significantly accelerate training [2]. \n\nAnd you\u2019re right on that we cannot calculate the exact likelihood of a molecule, which requires calculating all the permutation of the molecule. What we mean is that we can calculate the exact likelihood of the canonical order (BFS-order) of a molecule. We\u2019ve already revised this in the new version. \n\n", "title": "Response to the AnonReviewer1"}, "rklLSrw_iB": {"type": "rebuttal", "replyto": "ByxZqYuWcr", "comment": "\nQ3: The encoder modeling in GraphAF also shares similarities with Variational graph auto-encoder. Instead of constraining latent distribution using KL divergence, GraphAF maximizes graph likelihood to enforce base distribution.\nA3: In general, normalizing flows are indeed related to variational auto-encoders, both of which tried to explicitly model the data density and aim to maximize the data likelihood. However, flow-based methods are fundamentally different from VAE in the following perspectives: (1) flow-based methods define an invertible mapping between the latent space and observation space; (2) flow-based methods allow to calculate the exact likelihood while VAE methods can only optimize a lower bound. \n\nWe hope the above responses address your concerns. Please let us know if you have other questions. We\u2019re happy to further answer the questions. \n\n[1] You et al. Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation. NeurIPS 2018.\n[2] Jin et al. Junction Tree Variational Autoencoder for Molecular Graph Generation. ICML 2018.\n[3] Liu et al., Graph Normalizing Flows. arXiv 2019.05.\n[4] You et al. GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models. ICML 2018.\n[5] Madhawa et al., GraphNVP: An invertible flow model for generating molecular graphs. arXiv 2019.05.\n[6] Zachary M. Ziegler,  Alexander M. Rush. Latent Normalizing Flows for Discrete Sequences. ICML\u201919.  \n", "title": "Response to the AnonReviewer3 cont."}, "rJlKI8w_sS": {"type": "rebuttal", "replyto": "ByxZqYuWcr", "comment": "Thank you very much for your constructive comments! We have conducted more experiments according to your suggestions. The response to some of your questions are listed below:\n\nQ1: The empirical validation of GraphAF is contained to the single dataset - ZINC with a maximum of 38 atoms. From table 2, it seems to me every prior method works pretty well on important metrics. There is very little room for improvement. I recommend including results on QM9 and CEPDB datasets. \nA1: Thanks for your suggestion on more datasets. The reason that we only use ZINC data set is that we followed the experiment setting in existing work including GCPN[1] and JT-VAE [2]. According to your suggestions, we conducted additional experiments on QM9 (134K molecules in total) and MOSES (1.9M molecules in total). We didn\u2019t use CEPDB since currently we can\u2019t access to this dataset. The results on QM9 and MOSES data set are summarized below: \n|  Data.  | Valid     | Valid w/o check  | Uniqueness | Novelty | Reconstruction |\n| QM9     |    100    |    67                     |       94.51      |    88.83  |       100               | \n|MOSES  |     100   |     71                   |       99.99      |    100     |       100                |\nWe can see that our method can generate valid, unique, and novel molecules with different training data sets. \n\nOn the task of density modeling, we agree that different methods perform comparably. However, noting that GraphAF (1) can achieve 68% validity rate even without leveraging chemical domain knowledge thanks to the strong capacity of normalizing flow framework (GCPN can only reach 20%); (2) enjoys parallel training and is therefore much more efficient than existing methods. Moreover,  on the more challenging and more important tasks of drug discovery\u2014Property Optimization and Constrained Property Optimization\u2014GraphAF achieves the state-of-the-art performance (Table 5 and Table 6).  \n\n\nQ2: The model being data-agnostic, it makes sense to evaluate them on generic graph datasets - synthetic and real. \nA2: Note that GraphAF is mainly designed for molecular graph generation. However, it is indeed very general and can be generalized to generate different types of graphs by changing the Edge-MLPs and Node-MLP functions (Equation 8).  Specifically, we follow the experiment setup of GNF[3](Sec.5.2 in the original paper) and run GraphAF on two generic graph datasets: COMMUNITY-SMALL(synthetic) , EGO-SMALL(real). The results are as follows:\nCommunity Small  \t\t\t\t\tEgo Small\nDegree     Cluster     Orbit \tDegree       Cluster      Orbit\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nGraphRNN   0.08           0.12       0.04\t\t  0.09           0.22         0.003\nGNF\t\t0.20\t       0.20      0.11\t\t  0.03\t          0.10         0.001\nGraphAF      0.18            0.20      0.02\t\t 0.03 \t          0.11\t    0.001\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nGraphRNN   0.03           0.01       0.01\t\t  0.04           0.05         0.06\nGNF\t\t0.12\t       0.15      0.02\t\t  0.01\t          0.03        0.0008\nGraphAF      0.06            0.10      0.015\t\t 0.04 \t          0.04 \t    0.008\n \nThe above results demonstrate that GraphAF, when applied to generic graphs, can consistently yield comparable or better results compared with existing state-of-the-art approaches GNF[3] and GraphRNN[4]. We have added these results in the revised version.\n", "title": "Response to the AnonReviewer3"}, "rJediPDdsS": {"type": "rebuttal", "replyto": "S1gjCzZAFr", "comment": "Thanks for your comments and suggestions. The response to your concerns are listed below:\n\nQ1: The introduction of the normalizing flows (Sec 3.1) can be expanded to reach non-expert users. Advantages of using invertible flows (against other generative models such as GANs and VAEs) are not described rigorously in the current manuscript. I also suggest citing a nice review for invertible flows appeared recently.\nExplanations of the Sec 4.4 (+ appendix B) is simply insufficient to reproduce the experiments. More descriptions or references are required. \nA1: Thank you for suggestions. We\u2019ve already revised this section and also cited a new introduction and survey paper on normalizing flows [1]. Advantages of flow are briefly introduced in the introduction. We\u2019ve also revised and extended Sec 4.4 in the revised version.\n\nQ2: No discussion why the combination of the autoregressive flow and the RL performs greatly, compared to baselines. Some discussions will help the community to further improve the optimization tasks in the future. \nA2: This is a very good point!! As defined in Sec 4.4, our RL process is close to the one in previous work GCPN[2]. Therefore, the good property optimization performance is believed to come from the flexibility of flow. Compared with the GAN model used in GCPN[2], which is known to suffer from the mode collapse problem, flow is flexible at modeling complex distributions and generating diverse data (as shown in Table 2). This allows to explore a variety of molecule structures in the RL process for molecule properties optimization.\n\nWe hope the above response could address your concerns. \n\n[1] Ivan Kobyzev, Simon Prince, Marcus A. Brubaker. Normalizing Flows: Introduction and Ideas. arXiv:1908.09257. \n[2] You et al. Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation. NeurIPS 2018.\n", "title": "Response to the AnonReviewer2"}, "r1e4JLwdsS": {"type": "rebuttal", "replyto": "ByxZqYuWcr", "comment": "\nQ3: The novelty of the model is limited. The flow-based graph generative model is introduced in Graph Normalizing Flow (GNF) (NeurIPS'19, NeurIPS'18 workshop). The reversible flow is extended to whole graph in GraphNVP. Unlike GNF, GraphNVP[4] and GraphAF do away with decoder. The major difference being the sampling process - one-shot to sequential.\nA3: Thanks for pointing out the related work. Our work is indeed related to the two work Graph Normalizing Flows (GNF) and GraphNVP[5]. However, our work is fundamentally different from the two works. GNF defines a normalizing flow from a base distribution to the hidden node representations of a pre-trained Graph Autoencoders.  The graph generation is done through two separate stages by first generating the node embeddings with the normalizing flows and then generate the graphs based on the generated node embeddings in the first stage. In GraphAF, we define an autoregressive flow from a base distribution to the molecular graph structures, which can be trained end-to-end. GraphNVP also defines a normalizing flow from a base distribution to the molecular graph structures. However, the generation process of GraphNVP is one-shot, which cannot effectively capture graph structures and also cannot guarantee the validity of generated molecules (only 40% validity rate). In our GraphAF, we formulate the generation process as a sequential decision process and effectively capture the subgraph structures based on graph neural networks, based on which we define a policy function to generate the nodes and edges. The sequential generation process also allows incorporating the chemical rules. As a result, the validity of the generated molecules can be guaranteed (100% validity rate). Moreover, we can effectively optimize the properties of generated molecules by fine tuning the policy with reinforcement learning, which is not feasible in GraphNVP.  \n\n\nClarification:\nQ1: What are the inputs edge-mlp's operate on? Given the generation step is sequential, it is not clear to me why all the node embeddings H_i^L is given as input in eq (8). I also noted that the dimension of H_i^L varies with size of sub-graphs. Also note mismatch in the notation 'f' used in algorithm 1 and 'g' from the main text. \nA1: H_i^L is the node embeddings of the current sub-graphs in the i-th step. The inputs of Edge-MLP\u2019s include the graph-level embedding of the current sub-graphs h_i, the embedding of new node X_i, and the embedding of the previous j-th node X_j. We have added these details of Eq. (8) in the revised version. \nThanks for pointing this out. We\u2019ve already corrected the notations in Algorithm 1.  \n\n\nQ2: Please compare inference time. \nA2: Let {V, E} be the set of nodes and edges in a graph. The inference time of our GraphAF is O(|V|+ c|V|), where c is the maximum number of link predictions for each node in BFS order. \n\nOther weakness:\nQ1. Due to invertible flow modeling, the latent space is usually restricted to small dimension. In current case it is 9 for node feature and 3 for edge features. This drawback alongside the sequential edge generation prevents GraphAF from scaling to complex and large graphs with many labels.\nA1: A good point! With BFS order, the complexity of GraphAF scales linearly to the number of nodes or edges. Therefore, the size of the graph is not an issue. To scale to graphs with many node types, we can represent each node type with a low-dimensional vector instead of with a one-hot high-dimensional vector. Similar ideas have already been explored in using normalizing flows for text generation [6] (the size of vocabulary is very large).  \n\nQ2. Moreover, GraphAF utilizes only single layer of flow i.e., eq (9). This is clearly not sufficient to model complex graphs. And in its current form it is not clear how one can extend to multi-layer flow.\nA2:  We only use one single layer of flow since it has already been shown very powerful for modeling molecular graph structures in different data sets. However, the framework is very general and can be easily extended to multi-layer flow for modeling complex graphs. Specifically, we can construct a T-layer flow to map molecular graph structures to base distribution (z->\\epsilon_T->\\epsilon_{T-1}->...->\\epsilon_1). In the t-th layer, we take \\epsilon_t as the features of nodes and edges. Since \\epsilon_t is continuous, we can directly input it into R-GCN (see Eq.(8)) without the dequantization process, and then perform the transformation to \\epsilon_{t-1} as defined in Eq.(10) . \n", "title": "Response to the AnonReviewer3  cont."}, "S1gjCzZAFr": {"type": "review", "replyto": "S1esMkHYPr", "review": "This paper proposes a new invertible-flow based graph generation model. \nThe main difference from the previous flow-based generation model (GraphNVP) is the choice of flow mappings (coupling flow --> autoregressive flow). \nThe authors formulate conditional probabilities of iterative node/edge generation. Iterative sampling scheme naturally allows incorporating validity checks in each sampling step, assuring 100% validity in graph generation. The paper also proposes an implementation of molecule lead optimization combined with a RL framework. The experimental results show superiority in terms of valid graph generation and property optimization. \n\n\nOverall, the paper is written well. I feel no difficulty in understanding the main idea and the equations in the paper.\n \nIntroduction of the normalizing flows (Sec 3.1) can be expanded to reach non-expert users. Advantages of using invertible flows (against other generative models such as GANs and VAEs) are not described rigorously in the current manuscript. I also suggest citing a nice review for invertible flows appeared recently: \n\nIvan Kobyzev, Simon Prince, and Marcus A Brubaker, ``Normalizing Flows: Introduction and Ideas'', arXiv: 1908.09257, 2019.\n\n\nExplanations of the Sec 4.4 (+ appendix B) is simply insufficient to reproduce the experiments. More descriptions or references are required. \n\n\nExperimental results seem promising. A better validity score than the previous flow model illustrates the efficacy of the autoregressive flow against the coupling flow. \nThe performance on the property optimization (Table 3) seems brilliant. However, there is no discussion why the combination of the autoregressive flow and the RL performs greatly, compared to baselines. Some discussions will help the community to further improve the optimization tasks in the future. \n\n+ Overall, a good paper. well written, easy to understand. \n+ A new variant of the invertible-flow based graph generation model.  The novelty lies in the iterative generation process, naturally combined with the autoregressive flow. \n+ Superior to the one-shot flow baseline (GraphNVP) even if additional validity checks are omitted (Table 2)\n+ Good performances in property optimizations (Table 3, 4) \n- The explanation for RL process is simply insufficient for reproduction.\n-- No discussions about reasons why GraphAF+RL performs great in property optimization. \n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}}}