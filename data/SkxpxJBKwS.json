{"paper": {"title": "Emergent Tool Use From Multi-Agent Autocurricula", "authors": ["Bowen Baker", "Ingmar Kanitscheider", "Todor Markov", "Yi Wu", "Glenn Powell", "Bob McGrew", "Igor Mordatch"], "authorids": ["bowen@openai.com", "ingmar@openai.com", "todor@openai.com", "jxwuyi@openai.com", "glenn@openai.com", "bmcgrew@openai.com", "imordatch@google.com"], "summary": "", "abstract": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.", "keywords": []}, "meta": {"decision": "Accept (Spotlight)", "comment": "This paper describes how multi-agent reinforcement learning at scale leads to the evolution of complex behaviors. Actually, \"at scale\" may be an understatement - a lot of computing power was used here. But the amount of compute used is not the point, rather the point is that complex and fascinating behavior can emerge from a long co-evolutionary process (though gradient-based RL is used here, the principle is the same) where the arms race forms an implicit curriculum. This is the existence proof that people in artificial life and adaptive behavior have been looking for for so long. \n\nTwo reviewers were positive about the paper, with a third being negative because the paper does not give any new insights about how to do RL at scale. But that was not the stated aim of the paper, as the authors clarify in a response.\n\nThis paper will draw quite some attention and deserves an oral presentation."}, "review": {"H1lbBl1SjH": {"type": "rebuttal", "replyto": "SkxpxJBKwS", "comment": "In response to reviews, we\u2019ve made the following updates to the paper:\n\n1. Slightly modified language of contribution statement\n2. Changed figures 1 and 3 to plot the mean across 3 seeds and show the seeds\n3. Add sentence describing out of bounds condition and reward to Section 3\n4. Add more description of policy architecture to caption of Figure 2.\n5. Add more description of how/why box surfing is possible to Section 5\n6. Remove acknowledgements section (will be added back for camera ready paper)\n7. Remove Appendix section on multiple seeds as now figures 1 and 3 include this information\n8. Add footnote describing non-shared weights experiments to Section 4.\n", "title": "Paper Revisions"}, "HylGoCREir": {"type": "rebuttal", "replyto": "BJlxTZh6Yr", "comment": "Thank you for the review and questions!\n\n\u2014 \u201cHide&seek rules and safety issues: is it not supposed that hiders and the seekers could not get together (i.e., hiders cannot push seekers or as we can see in some videos)? Furthermore, it is surprising (one would say worrying) that hiders identified the barriers as an impediment to the seeker (not only as a way to hide). I wouldn\u2019t say that this is a \u201c human-relevant strategies and skills \u201c as the authors claim. Hider agents even double walled seekers!\u201d\n\nIn the environment as is, the hiders can push the seekers during the preparation phase. It\u2019s unclear that this is bad, but we agree that we could easily make it not the case, though it likely would not change the skill progression in the main hide-and-seek environment. However, as you note, this can definitely change the resulting skill progression in other game variants (Figure A.8). We also believe finding methods that can make agents converge on safe outcomes is an important direction for future research!\n\n\u201cHave the authors thought about joining the Animal-AI Olympics (http://animalaiolympics.com/) competition?\u201d\n\n\u2014 We thought this would be outside the scope of our current work, but we agree this challenge is very interesting. However, from the description it feels slightly different than the transfer tasks we propose. They say \u201cThe goal will always be to retrieve the same food items by interacting with previously seen objects,\u201d where in our transfer tests agents are given very different objectives from the original objective of hide-and-seek.", "title": "Response to Official Review #3"}, "BJxWUARNsr": {"type": "rebuttal", "replyto": "rkxcmygpFr", "comment": "\u2014 \u201cIs the agent's embedding concatenated with the other embeddings? If so, why and how (concat, sum, multiply, conditional batch norm, etc.)?\u201d\n\nWe concatenate all the entities together, such that the tensor has shape (number entities, entity dimension), run residual self attention, and then average pool getting a fixed sized vector of size (entity dimension). We\u2019ll add this clarification to the text.\n\n\u2014 \u201cIn the center and on the right you use a blue block to indicate the agent's embedding and then at the bottom right you seem to use it as a network component or something (between the \"LSTM\")? If you're trying to signal that this is the agent's perception at different stages in the network, I'd use a different color to separate it from the agent's lidar and pos/vel.\u201d\n\nAll the colored blocks represent activations in this diagram, but we agree re-using the blue coloring could be confusing. We\u2019ll change the color of the final two blue blocks for the camera ready version of the paper.\n\n\u2014 \u201cYou don't mention that \"x,v\" stands for \"position, velocity\".\u201d\n\nGood catch! Thank you, we\u2019ll add a clarification to the text.\n\n\u2014 \u201cFigure 3: \"environment-specific\" (add dash). Draw skill development boundaries like in Fig.1.\u201d\n\nGood suggestions. Thank you!\n\n\u2014 \u201cHow exactly does the \"surfing\" work? The seekers step (not jump, right, since there is no jumping?)\u201d\n\nWe do classify this as an exploit of the rules we designed in the last paragraph of Section 7, but we will add more clarification on how it works and that it is an exploit of our intended game rules in Section 5. You are right in that they more \u201cstep\u201d or \u201claunch\u201d themselves from a ramp to the box. Once on top of the box, they can still \u201cgrab\u201d the box, which keeps the relative orientation and position between agent and box fixed. The agents\u2019 movement action puts a force on the agent regardless of whether the agent is on the ground or not. So if the agent does this while grabbing the box, they will both move together since they have a fixed relative orientation and position.\n\n\u2014 \u201cYou mention in footnote 3 that the developmental stage and changes in reward aren't necessarily correlated. The same seems to be true for the metrics in Fig.3, which raises the question how did you come up with those boundaries for the different developmental stages in Fig.1? Did someone look at rollouts from the trained policy every couple of million steps?\u201d\n\nWe used a combination of looking at the reward, behavioral statistics (Figure 3), and watching trajectories. It is a very interesting line of future research to automatically detect large shifts in agent strategy!\n\n\u201cAnd do all agents learn new skills at the same time or is there a delay? From my understanding, they are all using the same policy and critic networks but maybe dependent on the proximity of an agent to an object/obstacle, it's easier or harder to execute.\u201d\n\n\u2014 All agents have the same weights so they would learn the skill at the same time. However, we\u2019ve run some experiments where each agent has a different policy and we did not notice any significant differences to the shared weight case. Using shared weights is simpler to implement and cheaper to train, which is why we use them for all of our experiments. Anecdotally, agents\u2019 seems to learn the skills for easier cases in the environment first; for instance we notice they often learn to construct a 1 block barricade using existing walls before they learn to construct a 3 block fort in the center of the room.\n", "title": "Response to Official Review #2 (Part 2)"}, "B1l0zA0NsH": {"type": "rebuttal", "replyto": "rkxcmygpFr", "comment": "Thank you for the very detailed review and constructive criticisms!\n\n\u2014 \u201cThe majority of the paper presents essentially a case study of what happened during a single seed of policy training...\u201d\n\nGreat point, we will update Figures 1 and 3 to be the average across the 3 seeds we show in the appendix. We found very little seed dependence throughout the project, which is likely why we made this oversight.\n\n\u2014 \u201cThe contributions section is overselling the work: (1) states that autocurricula lead to changes in agent strategy - Maybe I'm mistaken here but that sounds like a tautology. In other words, \"a self-generated sequence of challenges\" (\"Autocurricula\", according to [Leibo et al., 2019][1]) lead to changes in strategy.\u201d\n\nIn this sentence we were trying to place emphasis on \u201cdistinct and compounding phase shifts\u201d \u2014 we agree with you that autocurricula by definition are causing changes in strategy, but there is no guarantee that they are distinct shifts (they could just as easily be small changes in strategy). Distinct shifts make it easier to see the effects of an autocurriculum, as small shifts can be hard to detect or analyse. We\u2019ve changed this clause to \u201cclear evidence that multi-agent self-play can lead to emergent autocurricula with many distinct and compounding phase shifts in agent strategy\u201d.\n\n\u2014 \u201cAnd (3) advertises \"a proposed framework for evaluating agents in open-ended environments\" and also \"a suite of targeted intelligence tests for our domain\". The former of those two is either not in the paper or you mean your section \"6.2 Transfer and Fine-Tuning as Evaluation\", which isn't novel (see e.g. [Alain & Bengio, 2016][2])\u201d\n\nAfter re-reviewing this sentence we agree with you in that it was misleading; we did not intend claim transfer as our idea but rather that we would like to use transfer to evaluate skill progression in open-ended environments. The reason we think it is a contribution is that in most MARL settings, progress is evaluated through play against humans or through metrics like ELO against past versions or other populations. We will modify it to \u201ca proposal to use transfer as a framework for evaluating agents in open-ended environments...\u201d.\n\n\u2014  \u201cYour acknowledgments should be anonymized until publication. Otherwise, reviewers might draw conclusions which group published this work, thus violating the double-blind review procedure.\u201d\n\nThank you for pointing this out!\n\n\u2014  \u201c\"evidence that ... competition may scale better with increasing environment complexity\" - that's only shown in the appendix\u201d\n\nWe believe Figure 5 is also evidence for this, as you increase the observation space complexity, meaningful interaction with objects goes down when you use intrinsic motivation methods.\n\n\u2014 \u201cYou mention TD-Gammon as a game, but I think it's an algorithm for the game Backgammon, similarly to how \"Go\" is the game and \"AlphaGo\" is an algorithm for playing.\u201d\n\nThank you for catching this!\n\n\u2014  \u201cArena boundaries: What's the penalty and what's \" too far outside the play area\"?\u201d\n\nGreat point. We will update the paper with more clear language around this. We give a -10 reward if the agents go outside an 18 meter square (which is 9 times the area of the quadrant game shown in the appendix).\n\n\u2014 \u201cPolicy network and fusion are underspecified: How do you deal with the varying number of agents, boxes, obstacles? Do you just set the x/v of the missing pieces to zero or is the observation actually of a different shape in case there are more/fewer objects or agents? How's the embedding done that is depicted in Figure 2? Also, I didn't see the embedding being mentioned in the text - any reason for that?\u201d\n\nWe have more detail in appendix section B.7, which answers some of your questions, but we will move some of these details to the main text/caption of Figure 2 and add more clarification. The architecture is an attention and pooling based architecture so it naturally deals with varying numbers of objects. We mask out anything not visible to the agent in the attention and pooling operations so that they do not receive privileged information. The embedding weights are shared within object type, e.g. all box entities pass through the same shared embedding function. We currently show in figure 2 that the observations pass through fully connected layers to create these embeddings, but we\u2019ll add the comment about shared weights to the caption.\n\n\u2014 \u201cWhy does the agent's embedding say \"1\"? Why do the other agents' embeddings have a \"-1\" at the end of the orange box and the others don't?\u201d\n\nThe agents have an ego-centric architecture, so that \u201c1\u201d shows that that entry is the agent\u2019s observation of itself (the agent itself is just 1 entity as opposed to boxes or other agents which are many entities). There are (# agents - 1) other agents (from the view of any single agent). We\u2019ll add this clarification to the figure caption.\n", "title": "Response to Official Review #2 (Part 1)"}, "S1eB3pAVsS": {"type": "rebuttal", "replyto": "SkxqkHzRYH", "comment": "Thank you for your review and constructive criticisms! We\u2019ll try to address each piece of criticism in turn.\n\n\u2014 \u201cThe main point of the paper is empirical RL at scale\u201d\n\nThe main point we hope to convey is that large-scale multi-agent reinforcement learning (MARL) can lead to self-supervised autocurricula in which agents learn successively more complex human-relevant skills such as construction and tool use. We absolutely agree that there have been many amazing previous results from MARL at scale, and we acknowledge many of the works you mention and more in our introduction and related work sections. We believe our work differs from these in that our environment is built from very simple components in a physically grounded simulator, making it extremely extensible. It is much more clear how one could add to or modify the hide-and-seek environment to include more human-relevant components than it is how one could modify games like Go, Dota, or Starcraft. \n\n\u2014 \u201cThere has also been work on object-level RL \u2026 the observation that RL agents learn human-interpretable uses of objects does not seem surprising.\u201d\n\nWe agree that there has been much work on object-level RL. We didn\u2019t advertise this as a novel portion of our work, and we\u2019ve already included many citations that use object-level architectures and attention at the end of Section 4. We also acknowledge that there have been prior works where RL learns human-interpretable uses of objects, which is why we include a paragraph in Section 2 on prior work in tool-use; however, our work can be distinguished from these and the work you cite in that we provide no explicit signal for interacting with objects; the pressure to interact with the objects is solely a result of multi-agent competition. \n\n\u2014 \u201cThe paper also does not give new insights in how to make large-scale RL work\u201d\n\nThis paper was not on how to make large-scale RL work, but rather on showing the power of current large-scale RL algorithms in a new setting that is more physically grounded and human-relevant than previous settings like DotA, Starcraft, and Go. The main argument of the paper is that multi-agent autocurricula can lead to agents learning many human-relevant skills like tool-use and construction; the fact that we required no new significant algorithmic modifications actually strengthens this point in our opinion, as the results can\u2019t be confused as a pathology of a new specific algorithm. That being said, we agree that it is a great direction for future research to incorporate methods that can learn faster or better in this environment.\n\n\u2014 \u201cThe paper also does not introduce new concrete evaluation metrics that can apply to other tasks / RL problems...\u201d\n\nIt\u2019s very hard to create transfer tasks that are valid across domains. However, we hope that the tasks we proposed can be used as transfer metrics for any future research within our domain (both of which we will open source).\n\n\u2014 \u201cThere is one actor model, all agents share weights\u201d\n\nUsing shared weights, or at least some portion of training data coming from self-play, is very common (AlphaGo, DotA, Alphastar, Capture-the-Flag, NeuralMMO, etc.), and it doesn\u2019t alter the multi-agent optimization objective. Each agent still takes a greedy gradient and has its own observations and memory state so that at execution they use no privileged information. Shared weights does not mean uni-brain (one brain many actions), which would indeed reduce this to a single agent problem. That being said, we\u2019ve run the hide-and-seek experiment with separate weights for each agent and as expected have seen no difference in learned strategy.\n\n\u2014 \u201call agents use a central value function that can see the entire state. This makes the setting basically a single-agent problem and is far simpler in the multi-agent assumptions from other decentralized multi-agent work\u201d\n\nThis is a commonly used method to reduce policy gradient variance in partially observed settings without letting agents cheat at execution time both for MARL (MADDPG, Counterfactual RL, AlphaStar) and also single agent RL (Dactyl, Asymmetric Actor Critic). We ablate this choice in the appendix and find that it is important at the given scale of compute but agents still learn without it.\n\nAs for other MARL algorithms, we cite both of the works you mention in our paper already, and it is an excellent line of future research to incorporate methods like these into setups such as hide-and-seek to see if they bring benefit to learning. However, we don\u2019t think algorithmic simplicity is a fault of our work but rather a strength. We show that with only standard simple algorithms, multi-agent autocurricula can lead to human-relevant skills like construction and tool-use in physically grounded environments, which we believe provides a good baseline for future algorithmic research.", "title": "Response to Official Review #1"}, "rkxcmygpFr": {"type": "review", "replyto": "SkxpxJBKwS", "review": "# Review ICLR20, Emergent Tool Use...\n\nThis review is for the originally uploaded version of this article. Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper.\n\nI apologize in advance for being reviewer 2.\n\n## Overall\n\n**Summary**\n\nThe article introduces a new multi-agent physics environment called \"hide-and-seek\". The authors trained agents in this environment and studied the emergence of and changes in strategies. The authors also study the performance of these same agents in new \"targeted intelligence tests\" compared to training from scratch and compared to agents trained with curiosity.\n\n**Overall Opinion**\n\nI think the environment is very appealing and the paper is overall well-structured and demonstrates novel work. Therefore I'd recommend this paper to be accepted. That being said, there are glaring issues with some of the writing that need to be addressed before I think this work conforms to the standards of ICLR. However, if these issues are addressed, I have no issue increasing my review score.\n\nMain problems:\n\n- The majority of the paper presents essentially a case study of what happened during a single seed of policy training. For RL literature that's very uncommon and I think it's consensus that DRL is very sensitive to random seeds. I know that you do have additional seeds in the appendix, but why didn't you mention those in the main body of the paper? You seem to have found some robustness against multiple seeds, so why not show it? And also the fact that Figure 1 & 3 only apply to 1 seed is not mentioned. I think this is easy enough to fix - I suggest since you're already at 10 pages, to just bring in the additional seeds from the appendix and average over their performance in Fig.1&3.\n- The contributions section is overselling the work: (1) states that autocurricula lead to changes in agent strategy - Maybe I'm mistaken here but that sounds like a tautology. In other words, \"a self-generated sequence of challenges\" (\"Autocurricula\", according to [Leibo et al., 2019][1]) lead to changes in strategy. And (3) advertises \"a proposed framework for evaluating agents in open-ended environments\" and also \"a suite of targeted intelligence tests for our domain\". The former of those two is either not in the paper or you mean your section \"6.2 Transfer and Fine-Tuning as Evaluation\", which isn't novel (see e.g. [Alain & Bengio, 2016][2])\n- Your acknowledgments should be anonymized until publication. Otherwise, reviewers might draw conclusions which group published this work, thus violating the double-blind review procedure.\n\n[1]: https://arxiv.org/pdf/1903.00742.pdf\n[2]: https://arxiv.org/pdf/1610.01644.pdf\n\nLike I mentioned above, I think these are all easy to address, which should allow acceptance of this work. Here are some additional questions, comments, and nitpicks:\n\n## Specific comments and questions\n\n### Abstract\n\n- \"evidence that ... competition may scale better with increasing environment complexity\" - that's only shown in the appendix\n\n### Intro\n\n- You mention TD-Gammon as a game, but I think it's an algorithm for the game Backgammon, similarly to how \"Go\" is the game and \"AlphaGo\" is an algorithm for playing.\n\n### Rel. Work\n\n- all good\n\n### Hide And Seek\n\n- Arena boundaries: What's the penalty and what's \" too far outside the play area\"? And in all depictions, it looks like the geometry of the arena is elevated around the edges and the agents don't have a jump action, so how would they ever go out of borders? After watching the videos: Apparently, the jagged-looking arena boundary in the videos is purely cosmetic and agents can still access that space. This is unclear from just the paper and the renderings in Figure 1.\n\n### Policy Optimization\n\n- Policy network and fusion are underspecified: How do you deal with the varying number of agents, boxes, obstacles? Do you just set the x/v of the missing pieces to zero or is the observation actually of a different shape in case there are more/fewer objects or agents? How's the embedding done that is depicted in Figure 2? Also, I didn't see the embedding being mentioned in the text - any reason for that?\n- Figure 2 - This diagram is visually appealing but confusing and needs to be improved. Why does the agent's embedding say \"1\"? Why do the other agents' embeddings have a \"-1\" at the end of the orange box and the others don't? Is the agent's embedding concatenated with the other embeddings? If so, why and how (concat, sum, multiply, conditional batch norm, etc.)? In the center and on the right you use a blue block to indicate the agent's embedding and then at the bottom right you seem to use it as a network component or something (between the \"LSTM\")? If you're trying to signal that this is the agent's perception at different stages in the network, I'd use a different color to separate it from the agent's lidar and pos/vel. You don't mention that \"x,v\" stands for \"position, velocity\".\n \n### Auto-curriculum and Emergent Behavior\n\n- Figure 3: \"environment-specific\" (add dash). Draw skill development boundaries like in Fig.1.\n- How exactly does the \"surfing\" work? The seekers step (not jump, right, since there is no jumping?) onto the boxes and then what? The momentum propels the box forward? Do other seekers push the box? Their movement on top of the box somehow moves the box (this seems to be the case judging by the videos but this is the least physically plausible)? This is a super interesting adaptation but I'd suspect the physics simulation to have a bug/glitch that's being exploited here.\n- You mention in footnote 3 that the developmental stage and changes in reward aren't necessarily correlated. The same seems to be true for the metrics in Fig.3, which raises the question how did you come up with those boundaries for the different developmental stages in Fig.1? Did someone look at rollouts from the trained policy every couple of million steps? And do all agents learn new skills at the same time or is there a delay? From my understanding, they are all using the same policy and critic networks but maybe dependent on the proximity of an agent to an object/obstacle, it's easier or harder to execute.\n\n### Evaluation\n\n- clear and well-written, slightly too much content in the appendix and not enough in the main paper. Weird appendix numbering - A.6 appears in the main paper pages after A.7\n\n### Discussion and Future Work\n\n- all good\n\n### Appendix\n\n- I appreciate the TOC. I did not look into Appendix B-D because it's another 10 pages on top of the 10 pages of the article.\n\nAll in all an interesting work. Good luck with the rebuttal/discussion.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "BJlxTZh6Yr": {"type": "review", "replyto": "SkxpxJBKwS", "review": "Authors in introduce a new competitive/cooperative physics-based environment in which different teams of agents compete in a visual concealment and search task with visibility-based team-based rewards (although There are no explicit incentives for agents to interact with objects in the environment). They show that, complex behaviour emerge as the episode progresses and agents are able to learn 6 emergent skills/(counter-)strategies (including tool use), where agents intentionally change their environment to suit their needs. Agents trained using self-play \n\nIn my opinion, this is an excellent paper which main contribution is to provide experimental evidence that relevant and complex skills and strategies can emerge from multi-agent RL competing scenarios.\n\nMinor comments:\n\n- Hide&seek rules and safety issues: is it not supposed that hiders and the seekers could not get together (i.e., hiders cannot push seekers or as we can see in some videos)? Furthermore, it is surprising (one would say worrying) that hiders identified the barriers as an impediment to the seeker (not only as a way to hide). I wouldn\u2019t say that this is a \u201c human-relevant strategies and skills \u201c as the authors claim. Hider agents even double walled seekers!  \n\n- Have the authors thought about joining the Animal-AI Olympics (http://animalaiolympics.com/) competition? It would be a great opportunity to to test the skills of your agents in a further general testing scenario. They provide an arena (test-bed) which contains 300 different intelligent tests for testing the cognitive abilities of RL agents (https://www.mdcrosby.com/blog/animalaiprizes1.html) which have to interact with the environment. \n", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 2}, "SkxqkHzRYH": {"type": "review", "replyto": "SkxpxJBKwS", "review": "1. Summary\n\nThe authors report on an empirical study of emergent behavior of multiple RL agents learning to play hide-and-seek (a sparse reward task). The main point of this paper is that RL agents learning at scale (large number of samples, batch-size 64000). can learn to solve tasks with strategies that are human-interpretable (e.g., using ramps, boxes). Scale also requires various simplifications (e.g., keeping the learning setup as close as possible to a single-agent problem as possible).\n\nAgents are grouped in 2 teams (seekers, hiders). Each agent receives a team reward, e.g., it can be punished for events that it did not participate in, e.g., if a team-mate is seen by an opponent. If hiders are hidden, seekers also automatically see reward. The first 40% of the episode there is no reward to let hiders hide.\n\nThere is one actor model, all agents share weights. Hence this is self-play: hiders and seekers use the same agent model. Also, all agents use a central value function that can see the entire state (decentralized execution, centralized learning). This makes the setting basically a single-agent problem, with the only decentralized aspect being each actor model only receiving its own observation. Note that a large body of multi-agent RL work in fact uses agents that do not share weights, etc.\n\nOther features described:\n- Auto-curricula: e.g. agents find new strategies (using ramps, boxes) that other agents have to counteract.\n- Human-relevant skills: They report that the agent model learns multiple ways to interact with (objects in) the environment that are semantically interesting (resembles something humans might do).\n- Authors compare with policies learning via intrinsic motivation.\n- Evaluation through transfer learning shows some benefit of transfer of hide-seek agents to auxiliary tasks. However, it is not so clear how this evaluation informs future work on transfer learning (e.g., how would you pick evaluation tasks for a given train-task?) \n\n1. Decision (accept or reject) with one or two key reasons for this choice.\n\nReject.\n\nThe main point of the paper is empirical RL at scale. Although the learned behaviors are human-interpretable, this does not seem surprising given the fact that in many (large-scale) RL applications (Atari games, Go, DotA 2, Starcraft), it has been observed that RL agents can learn to manipulate and use their environment (which includes other agents!) in unexpected ways / find creative ways to exploit the reward function (see e.g. demos in https://www.alexirpan.com/2018/02/14/rl-hard.html). There has also been work on object-level RL [Agnew, Domingos 2018], which involves agents interacting with objects in the environment. Compared to this, the observation that RL agents learn human-interpretable uses of objects does not seem surprising.\n\nThe paper also does not give new insights in how to make large-scale RL ``'work'. For instance, there are no significant differences in algorithm / model structure from DotA / Starcraft agents that can inform future large-scale experiments.\n\nThe paper also does not introduce new concrete evaluation metrics that can apply to other tasks / RL problems, skill detection / segmentation methods to learn the structure of auto-curricula. Furthermore, the setup is very close to a single-agent problem (see above), and is far simpler in the multi-agent assumptions from other decentralized multi-agent work (Foerster 2018, Jacques 2019, etc).", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "ryeCRG40Or": {"type": "rebuttal", "replyto": "SJx87rUm_r", "comment": "Thank you for the praise and questions!\n\nAccording to our understanding, object permanence is typically defined as: the understanding that objects continue to exist even when they cannot be perceived (Piaget J., The construction of reality in the child. Basic Books; New York: 1954.). In the proposed task, the objects are only visible for about 20% of the episode, meaning that for the remaining 80% the agent cannot sense the objects in any way and must make the prediction based only on its memory (in our case the memory unit is an LSTM) of where it saw objects going. The agent must remember how many objects went to one side or the other, so in a sense it is required to understand that objects continue to exist in that area after they have been obscured.  When training on this task, we keep all of the original policy weights, e.g. embedding weights and LSTM weights, fixed such that we are only evaluating the existing representation the agent has after training in hide-and-seek or with intrinsic motivation (the task would be trivial without keeping these weights fixed).\n\nIn our policy architecture, all \u201centities\u201d (meaning objects and other agents)  are pooled together before going into the LSTM. You are correct that before this, there are an equal number of embedding vectors as there are entities in the environment, but this information is lost after masked pooling. For instance, if there are no visible entities at a given timestep, then the output of that masked pooling operation is a vector of 0\u2019s with dimension independent of the number of entities in the game, meaning there is no way for the agent to know how many entities exist past what\u2019s in its memory. We hope this clarifies any confusions on the policy architecture from the current text, and we will try to give more clarification in the next version of the paper.", "title": "Re: Great task and fascinating results, but a question about object permanence claims"}}}