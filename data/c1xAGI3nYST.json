{"paper": {"title": "NCP-VAE: Variational Autoencoders with Noise Contrastive Priors", "authors": ["Jyoti Aneja", "Alex Schwing", "Jan Kautz", "Arash Vahdat"], "authorids": ["~Jyoti_Aneja2", "~Alex_Schwing1", "~Jan_Kautz1", "~Arash_Vahdat3"], "summary": "We address the prior hole problem in VAEs using an energy-based prior, trained with noise contrastive estimation.", "abstract": "Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in various domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs\u2019 poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.", "keywords": ["Variational Autoencoders", "Noise Contrastive Estimation", "Sampling"]}, "meta": {"decision": "Reject", "comment": "This paper is rejected.\n\nAll of the reviewers found the empirical results strong. However, R3 and R4 pointed out concerns with the positioning of the work relative to prior work and that their approach is conceptually similar to previous work. The authors have tried to address these concerns in their rebuttal. Both reviewers appreciate the changes, but still have remaining concerns that I agree with. Based on these concerns, it is unclear if the strong empirical results are mostly due to using the NVAE architecture, rather than a methodological improvement over previous methods. The authors should work on positioning their paper in the context of prior work and the comparisons requested by R3 and R4 for a resubmission.\n"}, "review": {"YqFM6Qn5Ok": {"type": "review", "replyto": "c1xAGI3nYST", "review": "Summary:\n\nThe authors highlight an important problem in VAE - the prior-hole problem - which is that the approximate posterior and the simple gaussian prior do not match in spite of the KL term in the ELBO which makes sampling an issue - leading to the prior putting probability mass on latents that are not decoded to high probability mass regions in data manifold. Prior approaches have overcome this problem by increasing the expressivity of the prior through autoregressive models, and/or using hierarchical latents, EBMs with MCMC sampling. This paper proposes a very simple two stage method - (1) train a regular VAE, (2) train a binary classifier in NCE style to distinguish samples from prior and approx. posterior; use the re-weighting term from the NCE score to sample from a better re-weighted prior - either through langevin dynamics or re-sampling. The authors combine this approach with the use of hierarchical latents and produce really good performing generative models on a host of benchmarks with good looking samples.\n\nPros:\n\nvery simple and neat approach that produces really good results.\nlooks easy enough to reimplement and adopt widely for future research in VAEs.\nsamples look great, FID scores are good.\nablations on LD and SIR are very useful.\nCons:\n\nwould have been nice not to have a 2-stage approach. figuring out a way to have an online way of trying to train the re-weighting classifier in a GAN-like manner would be much more preferred.\npaper can do a better job at citing related work - ex - should cite Variational Lossy Autoencoder - Chen et al 2016 - on using autoregressive priors and pointing out a connection to IAF posterior.\nwould be nice to show results even w/o hierarchical latents - how much improvement can be obtained over vanilla deep VAE with this two-stage approach\nreport results on log-likelihood benchmarks - CIFAR10, ImageNet-32, ImageNet-64 - compare to Flow models, Autoregressive Models, etc.\n\nPost Rebuttal:\nI believe the positioning of the paper could be improved but at the same time empirical results in the paper are strong. I am adjusting my score to 6 with a confidence of 4.", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "9c6qSywGeH6": {"type": "review", "replyto": "c1xAGI3nYST", "review": "Authors approach the \"hole problem\" of variational autoencoders where the aggregate posterior fails to match the prior, causing some areas of the prior distribution to be left out. Consequently, the decoder is not trained properly to operate in such regions, and the whole generate models is then subject to suboptimal performance. To attack this problem authors introduce two changes:\n- Once a standard VAE is trained, a post-hoc learning of the prior distribution is performed. This is expected to move the prior closer to the aggregate posterior.\n- In order to increase prior's expressivity, authors employ Energy Based Models (EBMs). To sidestep the difficulties of the likelihood-based training of EBMs authors switch to Noise Contrastive Estimation (NCE) procedure. Two techniques are proposed to perform approximate sampling from the EBM.\n\n**Strong points**:\nThe paper is clearly written, technically correct and presents a new state-of-the-art method that significantly improves sample quality of Variational Autoencoders.\n\n**Weak points**:\nMy major problem with the paper is the seeming disparity between the models in the experimental comparison. In particular, authors build NCP-VAE upon NVAE [1], a state-of-the-art VAE architecture notorious for its large size and use of many stochastic layers (groups of latent variables in authors' terms). At the same time, for both multistage VAE baselines (*2 stage VAE* and *RAE*) the FID metrics were taken from the corresponding papers, which were based on much simpler architectures with a single group of latent variables.\n\nA more fair comparison would be to take the NVAE architecture and apply techniques from *RAE* [2] / *2 stage VAE* [3] to it. In the end of the sec. 4 authors say \"It is not clear how 2s-VAE or RAE are applied to state-of-the-art hierarchical models\". I believe this is a doable task. For example, consider (perhaps appropriately downscaled) NVAE's encoder (fig. 2a from [1]) and replace the input x with a random noise \u03b5 \u2013 the resulting latent variable model could be used as a second stage VAE [3]. For the *RAE* remove the red bottom-up path completely and make distribution over z1 a mixture. It might also be interesting to consider a mixture of size 1 -- a single gaussian distribution. In this case the 2nd stage training would be moving the prior towards the aggregate posterior. While in theory one might expect p(z) already be a decent approximation for q(z) (as shown in appendix B), in practice suboptimality of stochastic joint optimization might prevent p(z) from matching the aggregated q(z) to the fullest. The NVAE shares some weights between the prior and the encoder, which could either alleviate this problem by ensuring both encoder and prior change in a similar fashion, or hurt prior's expressivity since the top-down path has to be able to work in two somewhat different scenarios.\n\nFinally, the qualitative evaluation seems disconnected from the problem being solved. It is my understanding that lessening the prior hole problem should result in a fewer unappealing / blurry samples generated by the model. The additional samples in appendix H still contain some such samples. Unfortunately, no such samples are presented for the baseline models (in [1] the samples were curated). It's therefore hard to tell if the frequency of such samples was reduced.\n\n**Conclusion**: the results presented in the paper are impressive and the quality of samples is indeed very high. Unfortunately, I have to vote for rejection of this paper in its current form for its lack of rigorous experimental evaluation. It feels like the authors did overly focus on presenting a new method and have not given the prior work their best effort.\n\nMy suggestions to authors:\n- Evaluate two stage approach to the NVAE: 1) nested VAE prior as in [3], 2) mixture over z1 as in [2], 3) fine-tune NVAE's prior. The later will also disentangle the benefits of an energy-based prior from those of the two-stage training.\n- Perform qualitative evaluation based on frequency of visually unappealing samples from NCP-VAE versus that of the NVAE.\n\n[1]: Arash Vahdat and Jan Kautz.  NVAE: A deep hierarchical variational autoencoder. \n\n[2]: Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael Black, and Bernhard Scholkopf.  From variational to deterministic autoencoders.\n\n[3]: Bin Dai and David Wipf.  Diagnosing and enhancing vae models.\n\n\n\n## Post-rebuttal update\n\nAuthors have addresses some of the issues outlined above, in particular the additional comparison in sec. 5.4 and table 7 is informative. However, the RAE numbers indicate that a simple 10-component Gaussian Mixture is superior to the complex model of NCP-VAE with a Gaussian prior on a small VAE, and the superiority of NCP with GMM prior base provides little information as RAE with 50-components GMM could have performed even better. It's also not clear how 2s-VAE, RAE and WAE would scale to bigger architectures such as NVAE, which raises the question if the NCP was the optimal one.\n\nIt's surprising to see the 2-stage VAE to perform worse than the standard VAE, whereas (Dai & Wipf, 2018) have shown a significant improvement in the original paper. I think, this result needs further elaboration and validation.\n\nRegarding the qualitative evaluation: figure 12 does show that the NCP-NVAE has fever poor samples, although it's hard to tell whether the difference is significant. Another issue is that the comparison does not include prior works.\n\nAs a result I bump by score to 5, but I think the paper still needs more work to make a sound argument for the proposed idea.\n", "title": "Issues with experimental setup", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "XwhaqeYiPAx": {"type": "review", "replyto": "c1xAGI3nYST", "review": "**GENERAL**\nThe goal of the paper is to model the marginal over latents in VAEs in such a way to minimize the mismatch with the aggregated posterior. The paper proposes a new class of marginal distributions over the latent space that is a product of two experts: the first expert is a non-trainable probability distribution, and the second expert is an unnormalized probability distribution parameterized using neural networks. Since training a product of experts requires to apply an approximate inference (e.g., MCMC sampling), the authors propose to use the likelihood ratio trick. Eventually, a VAE is trained in two stages. First, they assume the marginal over z's to be simply the non-trainable distribution, and the VAE is trained. At the second stage, they propose to train the second expert (i.e., the binary classifier that distinguishes z ~ q(z) and z ~ p(z)) in order to obtain the final NCP that better matches the aggregated posterior. Further, the idea is extended to hierarchical VAEs, and a separate binary classifier is trained per each stochastic level.\n\n**Strengths:**\nS1: The idea is very interesting and allows to enrich the marginal distribution over z's in the VAE framework.\n\nS2: The paper is well positioned in current trends in generative modeling. I find the combination of energy-based models and VAEs as a very appealing research direction.\n\nS3: The proposed two-stage learning procedure is very logical and seem to be efficient. Its simplicity increases its reproducibility.\n\nS4: Obviously, introducing an energy-based component results in intractability of the likelihood function due to the partition function. However, the FID scores and the quality of generated images are very convincing.\n\n**Deficiencies:**\nD1: In this review, I kept using \"a non-trainable expert\" or \"a non-trainable marginal\" for the base prior (the term used by the authors). However, I am not quite sure whether the base prior is non-trainable. I was unable to find any information about it in the paper. Therefore, I assumed it is a standard Gaussian after inspecting Figure 1. It would be easier for a reader, if this information was included in the text.\n\n\n**Remarks:**\nR1: The Appendix B.1 is closely related to the following papers:\n- Rezende, D. J., & Viola, F. (2018). Taming vaes. arXiv preprint arXiv:1810.00597.\n- Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, 2016.\n- Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artificial Intelli- gence and Statistics, pp. 1214\u20131223, 2018.\nIt is maybe worth to mention these papers there. Otherwise, the text may sound as a new contribution.\n\nR2: Section 3, first paragraph: The authors stated that: \"Recently, energy-based models have shown promising results in representing complex distributions\". This statement is very misleading, because the energy-based models (e.g., Boltzmann machines) have been used in ML for over 30 years, e.g.:\n- Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive science, 9(1), 147-169.\n- Hinton, G. E., & Sejnowski, T. J. (1986). Learning and relearning in Boltzmann machines. Parallel distributed processing: Explorations in the microstructure of cognition, 1(282-317), 2.\n- Salakhutdinov, R., & Hinton, G. (2009, April). Deep boltzmann machines. In Artificial intelligence and statistics (pp. 448-455).\n- Larochelle, H., & Bengio, Y. (2008, July). Classification using discriminative restricted Boltzmann machines. In Proceedings of the 25th international conference on Machine learning (pp. 536-543).\n\nThe word \"recently\" suggests that this is a new invention that is simply not true.\n\nR3: The proposed prior (NCP) could be seen as a specific form of a product of experts (e.g., Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural computation, 14(8), 177). In my opinion, it is an interesting connection.\n\nR4: I wonder whether it is feasible to use some sort of importance sampling (e.g., Annealed Importance Sampling, Salakhutdinov, R., & Murray, I. (2008, July). On the quantitative analysis of deep belief networks. In Proceedings of the 25th international conference on Machine learning (pp. 872-879).) or other procedure (e.g., Perturb-and-MAP, Hazan, T., & Jaakkola, T. (2012, June). On the partition function and random maximum a-posteriori perturbations. In Proceedings of the 29th International Conference on International Conference on Machine Learning (pp. 1667-1674).) to estimate the partition function.\n\n**AFTER REBUTTAL**\nI would like to thank the authors for their rebuttal and all updates. The paper is related to other ideas in the literature, however, it constitutes an interesting contribution to the field. I appreciate all new results and discussions. I keep my original score.", "title": "A new energy-based marginal distribution over latents for VAEs", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "kl8TKq2YLTH": {"type": "rebuttal", "replyto": "c1xAGI3nYST", "comment": "Dear reviewers,\n\nWe thank all of you for your detailed comments and feedback. We have submitted the response to all the reviews. Additionally, we provide an updated draft of the paper. The draft contains the following updates:\n\n1. Section 5.4 (Tables 7 & 8), which includes an additional comparison to prior works. We majorly address the concern of comparisons with prior works, especially with non-hierarchical models. We provide FID scores for the CelebA-64 dataset and the negative log-likelihood for MNIST. For reference, we also provide the tables below.\n\n2. Also, in Appendix-I we provide a new qualitative comparison using a synthetic 2d dataset of a 25 Gaussian Mixture on a grid, demonstrating that our method avoids sampling from the low-density region in the original data distribution.\n\n3. Finally, Appendix J contains a new qualitative comparison of random samples from NVAE and our NCP-VAE approach. We demonstrate that random samples generated by our method contain a smaller number of corrupt images.\n\n&nbsp;\n\n\n| Model                         | FID   |\n|-------------------------------|-------|\n| VAE w/ Gaussian Prior         | 48.12 |\n| 2s-VAE (Dai & Wipf, 2018)     | 49.70 |\n| WAE (Tolstikhin et al., 2018) | 42.73 |\n| RAE (Ghosh et al., 2020)      | 40.95 |\n| NCP w/ Gaussian prior as base | 41.28 |\n| NCP w/ GMM prior as base      | 39.00 |\n\n**Table 7 from the main paper**\n(Generative performance on CelebA-64 with the RAE (Ghosh et al., 2020) architecture)\n\n\n&nbsp;\n&nbsp;\n\n\n| Model                                   | NLL   |\n|-----------------------------------------|-------|\n| VAE w/ Gaussian Prior                   | 84.82 |\n| VAE w/ LARS prior (Bauer & Mnih, 2019)  | 83.03 |\n| VAE w/ SNIS prior (Lawson et al., 2019) | 82.52 |\n| NCP-VAE                                 | 82.82 |\n\n**Table 8 from the main paper**\n(Likelihood results on MNIST on single latent group model)\n\nWe hope that our responses have addressed your concerns. We will be glad to answer any of your concerns and questions. Thank you!\n\n\nAuthors of submission 959\n", "title": "Responses Added and Paper Updated"}, "gc-LEjk-z2r": {"type": "rebuttal", "replyto": "9c6qSywGeH6", "comment": "We thank the reviewer for the feedback.\n\n\u2022\t**Additional comparison**\n\nThe reviewer suggests applying the models from 2s-VAE and RAE to NVAE. Applying these models to NVAE may require making design decisions for 2s-VAE and RAE that are not originally addressed in these works. Instead, we suggest applying our noise contrastive prior (NCP) to the RAE architecture [1]. Since RAE [1] is compared to 2s-VAE and other baselines, we can compare our method against both RAE and 2s-VAE on exactly the same architecture (see Table 1 in [1]). \n\nOur results are reported in Table 7 in the revised version. Here, we examine how our NCP-VAE performs when the base prior is i) a Gaussian distribution or ii) a Gaussian mixture model with 10 components as suggested by RAE [1]. Our NCP-VAE improves the performance on the base VAE, improving the FID score to 41.28 from 48.12. Additionally, when NCP is applied to the VAE with GMM prior (the RAE model), it improves its performance from 40.95 to the FID score of 39.00. This experiment demonstrates the efficacy of our model over RAE and 2s-VAE implemented on exactly the same base VAE architecture.\n\n\u2022  **Qualitative Evaluation**\n\nFor a qualitative comparison to NVAE, in Appendix-J, we provide random images sampled from NVAE and NCP-VAE at a temperature t = 0.7. Additionally, similar to experiments by Elfeki et al. [4], in Appendix-I, we also present results on synthetic 2D data, constructed using a mixture of Gaussians on a grid. Results show that our method doesn\u2019t sample from low density regions in the true data distribution. \n\n&nbsp;\n\n[1] From Variational to Deterministic Autoencoders, Ghosh et al., https://arxiv.org/abs/1903.12436\n\n[2] GDPP: Learning Diverse Generations Using Determinantal Point Process, Elfeki et al., https://arxiv.org/abs/1812.00068\n", "title": "Response to Reviewer 4"}, "BtzBqxsp7Kh": {"type": "rebuttal", "replyto": "lw-sHMeMBU", "comment": "Thank you for pointing out the connection to SNIS [1]. Note that Lawson et al. [1] introduce energy-inspired models (EIMs) that define distributions induced by sampling processes (used by Bauer and Mnih [2] as well as our SIR sampling). Although EIMs have the advantage of end-to-end training and can be used as either a prior or decoder in VAEs, they require multiple samples during training (up to 1K samples). This makes the application of EIMs to deep hierarchical models such as NVAEs very challenging as these models are often very memory intensive and are trained with a few training samples per GPU.\n\nOur proposed two-stage training approach is simple to train. And, it can be easily applied to hierarchical models, as our noise contrastive training is done in parallel for all the groups. With recent studies showing the efficacy of very deep hierarchical VAEs [3], we believe that the appealing scalability of NCP-VAE permits its adoption to large hierarchical models. \n\nTo ensure that we treat the prior art properly, we added a discussion on Lawson et al. [1] to our related work section. In our newly added Table 8, we provide an empirical comparison to both [1] and [2] using architectures/hyperparameters used in these works. We also added Table 7 which provides an additional comparison to several other works (suggested by the reviewers) using identical architectures/hyperparameters. We do hope that these additional experiments position our NCP-VAE better w.r.t. prior work.\n\n&nbsp;\n\n[1] Energy-Inspired Models: Learning with Sampler-Induced Distributions, Lawson et al., https://arxiv.org/abs/1910.14265 \n\n[2] Resampled Priors for Variational Autoencoders, Bauer and Mnih,  https://arxiv.org/abs/1810.11428\n\n[3] Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images, Anonymous submission, ICLR 2020. \nhttps://openreview.net/forum?id=RLRXCV6DbEJ\n", "title": "Positioning NCP-VAE Compared to the Prior Work"}, "yJOk_JQDhgV": {"type": "rebuttal", "replyto": "XwhaqeYiPAx", "comment": "We thank the reviewer for the positive and encouraging feedback. \n\n\u2022\t**Discussion in Sec B.1**\n\n Thanks for pointing this out. We added a short discussion to Sec. B to clarify that the derivation in this section is not a new contribution.\n\n\u2022\t**Energy-based models**\n\nThe word \u201crecently\u201d in the description of prior work on EBM was meant for neural network based energy functions. We agree with you that EBMs have a rich history in machine learning and physics. We added an additional discussion to Sec. 4 to point out early work in this space.\n\n\u2022\t**Importance sampling and AIS for partition function estimation**\n\n The main issue with using importance sampling or AIS for partition function estimation is that they provide stochastic lower bounds on $log Z$. Since $log Z$ participates in the log-likelihood expression with a negative sign, these methods tend to overestimate the log-likelihood. This makes a comparison to VAEs unfair as they are usually examined with lower bounds on their log-likelihood. Moreover, $log Z$ estimation is more challenging for EBMs defined in a large space. For example, Du & Mordatch NeurIPS 2019 [1] observed that AIS could not reliably estimate $log Z$ of an EBM defined on CIFAR-10 even after 2 days.\nWe provide log-likelihood estimates for NCPs that are defined in a very small \u2018hierarchical\u2019 latent space in Tab. 4. Additionally we provide a comparison to other \u2018non hierarchical\u2019 models with a single group implementation in Tab. 8. We intentionally kept the latent space very small so that our $log Z$ estimates are reliable. \n\n\u2022\t**Gaussian Prior**\n\nThank you for pointing this out. We indeed use a Gaussian prior. We clarified this in Sec. 3.4.\n\n&nbsp;\n\n[1] Implicit Generation and Generalization in Energy-Based Models, Du & Mordatch, https://arxiv.org/abs/1903.08689 \n", "title": "Response to Reviewer 1"}, "nch4z9MDVv": {"type": "rebuttal", "replyto": "YqFM6Qn5Ok", "comment": "We thank the reviewer for the positive feedback. \n \n*  **Two-stage Approach**\n\nOur two stage training approach is simple to train. Moreover, it can be easily applied to hierarchical models, as the noise contrastive training is done in parallel for all the groups. With recent studies showing the efficacy of very deep hierarchical VAEs [3], we believe that the appealing scalability of NCP allows its adoption to large hierarchical models. \n\n* **Citing Related Work**\n\nThank you for the suggestion to cite \u201cVariational Lossy Autoencoder\u201d. We have added the citation in the introduction.\n\n* **Using complex priors**\n\n1. Normalizing flows: Note that Sec. 3.2 by Che et al. [2] shows that applying a normalizing flow on the prior is equivalent to applying its inverse to the approximate posterior. Given that NVAE comes with normalizing flows in its approximate posterior, we don\u2019t believe adding additional normalizing flows increases the expressivity of the prior. We believe EBMs may provide additional expressivity that is not available through normalizing flows. \n\n\n2. Autoregressive models: Using autoregressive latent spaces following the spirit of PixelCNN could be applied. But note that these approaches tend to be very slow during sampling.\n\n* **Non-hierarchical latent variable models**\n\nFor a fair comparison to gauge the efficacy of our approach on non-hierarchical models, in Tab. 7 and Tab. 8 we provide FID and log-likelihood respectively. We apply NCP to commonly used small VAE models.\n\n1.\tLog-Likelihood: For a fair comparison with non-hierarchical approaches, in Tab. 8, we compare log-likelihood of our NCP approach applied to the architecture of Lawson et al. [4]. Our NCP approach applied to a Gaussian prior results in a negative log-likelihood (NLL) of 82.82 which is comparable to that of [4], i.e., 82.52. Note that our approach results in a better NLL when compared to that obtained from using a Gaussian prior (84.82) and also the non-hierarchical methods of Bauer and Mnih [5] (83.03).\nHowever for other datasets, we provided FID scores which are commonly used when the log-likelihood is not available.\n\n\n2.\tFID: In Tab. 7 we show the generative performance of our approach applied to the VAE architecture of RAE by Gosh et al. [1]. Note that this VAE architecture has only one latent variable group. The same base architecture was used in the implementation of 2s-VAE by Dai and Wipf [6] and WAE by Tolstikhin et al. [7]. We borrow the training setup from RAE [1] on the CelebA-64 dataset, and compare to the baselines reported in this work. We apply our NCP-VAE on top of a vanilla VAE with a Gaussian prior as well as a 10-component Gaussian mixture model (GMM) prior that was proposed by Gosh et al. [1]. Our NCP-VAE improves the performance on the base VAE, improving the FID score to 41.28 from 48.12. Additionally, when NCP is applied to the VAE with GMM prior (the RAE model), it improves its performance to an FID of 39.00. \n\n&nbsp;\n\n\n[1] From Variational to Deterministic Autoencoders, Ghosh et al.,  https://arxiv.org/abs/1903.12436\n\n[2] Variational Lossy Autoencoder, Che et al., https://arxiv.org/abs/1611.02731\n\n[3] Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images, Anonymous submission, ICLR 2020. \nhttps://openreview.net/forum?id=RLRXCV6DbEJ\n\n[4] Energy-Inspired Models: Learning with Sampler-Induced Distributions, Lawson et al., https://arxiv.org/abs/1910.14265 \n\n[5] Resampled Priors for Variational Autoencoders, Bauer and Mnih,  https://arxiv.org/abs/1810.11428\n\n[6] Diagnosing and Enhancing Vae Models, Dai and Wipf, https://arxiv.org/abs/1903.05789\n\n[7] Wasserstein auto-encoders, Tolstikhin et al., https://arxiv.org/abs/1711.01558\n", "title": "Response to Reviewer 2"}, "kMJbeHWJTcQ": {"type": "rebuttal", "replyto": "jSQ_GJeMTur", "comment": "We thank the reviewer for the feedback.\n\n\u2022\t**Comparison to Prior Work - Non-hierarchical latent variable models**\n\nFor a fair comparison to gauge the efficacy of our approach on non-hierarchical models, We apply NCP to commonly used small VAE models. In Tab. 7, and Tab. 8 we provide FID and log-likelihood respectively. \n\n1.\tLog-Likelihood: For a fair comparison with non-hierarchical approaches in Tab. 8 we compare log-likelihood of our NCP approach applied to the architecture of Lawson et al [4]. Our NCP approach applied to a Gaussian prior results in a negative log-likelihood (NLL) of 82.82 which is comparable to that of [4], i.e., 82.52. Note, that our approach results in a better NLL when compared to that obtained from using a Gaussian prior (84.82) and also the non-hierarchical methods of Bauer and Mnih [5] (83.03)\nHowever, for other datasets, we provided FID scores which are commonly used when log-likelihood is not available.\n\n\n2.\tFID: In Tab. 7 We show the generative performance of our approach applied to the VAE architecture in RAE by Gosh et al. [1]. Note that this VAE architecture has only one latent variable group. The same base architecture was used in the implementation of 2s-VAE by Dai and Wipf [6] and WAE by Tolstikhin et al. [7]. We borrow the training setup from RAE [1] on the CelebA-64 dataset, and compare to the baselines reported in this work. We apply our NCP-VAE on top of vanilla VAE with a Gaussian prior as well as a 10-component Gaussian mixture model (GMM) prior that was proposed in RAEs. Our NCP-VAE improves the performance on the base VAE, improving the FID score to 41.28 from 48.12. Additionally, when NCP is applied to the VAE with GMM prior (the RAE model), it improves its performance to an FID of 39.00. \n\n\u2022\t**Comparison to VQ-VAE**\n\n VQ-VAE uses powerful autoregressive PixelCNN priors, applied to latent variables as large as 128x128 dimensions (in VQ-VAE v2). We expect the sampling speed from VQ-VAE to be very slow.\n\n\u2022\t**Comparison to VAE-GAN**\n\n Our main focus is to improve the expressivity of the VAE prior through simple energy-based models. We believe that it is unfair to compare our method against VAE-GAN models that apply adversarial training in the data space. \n\n\u2022\t**More Complex Priors**\n\n1.\tNormalizing Flows applied to prior:  Note that section 3.2 by Che et al. [2] shows that applying a normalizing flow on the prior is equivalent to applying its inverse to the approximate posterior. Given that NVAE uses normalizing flows in its approximate posterior, we don\u2019t believe adding additional normalizing flows increases the expressivity of the prior. We believe EBMs may provide additional expressivity that is not available through normalizing flows. \n\n\n2.\tAutoregressive Priors: Using autoregressive latent spaces following the spirit of PixelCNN could be applied. But note that these approaches tend to be very slow during sampling.\n\n\n\u2022\t**Qualitative comparison/Synthetic Example**\n\nFor a qualitative comparison to NVAE we provide random images sampled from NVAE and NCP-VAE at a temperature t = 0.7. We specifically point out the corrupt images for both the methods in red in Appendix-J. Additionally, following the experiments of Elfeki et al. [4], we also perform experiments on 2D synthetic data using a mixture of Gaussians on a grid. Appendix-I shows the efficacy of our method in avoiding to generate samples from the low density regions in the true data distribution. \n\n&nbsp;\n\n[1] From Variational to Deterministic Autoencoders, Ghosh et al.,  https://arxiv.org/abs/1903.12436\n\n[2] Variational Lossy Autoencoder, Che et al., https://arxiv.org/abs/1611.02731\n\n[3] Resampled Priors for Variational Autoencoders, Bauer and Mnih,  https://arxiv.org/abs/1810.11428\n\n[4] GDPP: Learning Diverse Generations Using Determinantal Point Process. Elfeki et al., https://arxiv.org/abs/1812.00068\n\n[5] Energy-Inspired Models: Learning with Sampler-Induced Distributions, Lawson et al., https://arxiv.org/abs/1910.14265 \n", "title": "Response to Reviewer 3"}, "UjTceeDgMek": {"type": "rebuttal", "replyto": "fpd0NP9H-JB", "comment": "Dear reviewer and AC,\n\nWe are in the processing of submitting a revised version with additional experiments, addressing the reviewers' comments. We apologize for the delay and we do hope that these additional experiments will demonstrate the efficacy of our proposed model compared to the prior arts.\n\nThanks,\n", "title": "Response document is coming soon"}, "jSQ_GJeMTur": {"type": "review", "replyto": "c1xAGI3nYST", "review": "The main concerns are,\n* The idea is somewhat novel but very similar to ideas developed in  [1,2] and (Bauer & Mnih, 2019). Note that, in [1] rejection sampling is performed on samples from the prior distribution based on the likelihood ratio obtained from a discriminator. While the results reported in [1] are not competitive to those reported in this work, it is probably be due to differences in model architecture. Therefore, a fair comparison to prior works [1,2] and (Bauer & Mnih, 2019) is required using a common model architecture. E.g. experiments using a simple DCGAN style architecture on CIFAR-10  would be sufficient to indicate efficacy.\n\n* Prior resampling vs more complex priors: While the proposed NCP sampling scheme is applied on a Gaussian base prior with the NVAE architecture, it is unclear whether a similar gain in performance can be obtained using a more complex prior e.g. flow [3] or autoregressive (Van Den Oord et al., 2017; Razavi et al., 2019), [4]. A simple experiment which could help here is: keeping the architecture constant compare NCP with Gaussian base vs a flow or autoregressive prior.\n\n* Additionally, no comparison to state of the art VAE based image synthesis approaches like VQ-VAE-2 (Razavi et al., 2019) are provided e.g. on ImageNet 256 \u00d7 256.  \n\n* No comparison to VAE-GAN based approaches [5,6], which achieves FIDs of  23.4 (vs 24.08 of this paper). It it also unclear whether the proposed approach can be applied on top of VAE-GAN based approaches.\n\n* The current work uses the proposed NCP sampling scheme on a Gaussian base prior. Can the proposed prior be applied to more complex priors e.g. flow or autoregressive priors?\n\n* No qualitative comparison of sample quality: Figure 3 only shows samples for the NCP approach. The paper should qualitative demonstrate that NCP leads to reduction in the number of \"corrupted\" images. To do this one possibility would be to demonstrate that the worst sample image in a batch from the NCP approach is on average significantly better than that from the plain NVAE approach. Alternatively, an even simpler experiment can be performed on the 2d Ring and Grid [7] datasets to demonstrate gains in sample quality.\n\n[1] Dual Rejection Sampling for Wasserstein Auto-Encoders, ECAI 2020.\n\n[2] Variational Rejection Sampling, AISTATS 2018.\n\n[3] Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings, ICLR 2020.\n\n[4] Normalizing Flows with Multi-scale Autoregressive Priors, CVPR 2020.\n\n[5] Variational Approaches for Auto-Encoding Generative Adversarial Networks, arXiv 2017.\n\n[6] \"Best-of-Many-Samples\" Distribution Matching, NeurIPS Workshop 2019.\n\n[7] GDPP: Learning Diverse Generations using Determinantal Point Process, ICML 2019.", "title": "Novelity is unclear and additional experimental evidence needed", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}