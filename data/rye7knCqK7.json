{"paper": {"title": "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks", "authors": ["Amanpreet Singh", "Tushar Jain", "Sainbayar Sukhbaatar"], "authorids": ["amanpreet@nyu.edu", "tushar@nyu.edu", "sainbar@cs.nyu.edu"], "summary": "We introduce IC3Net, a single network which can be used to train agents in cooperative, competitive and mixed scenarios. We also show that agents can learn when to communicate using our model.", "abstract": "Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multi-agent scenarios, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards foreach agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability.", "keywords": ["multiagent", "communication", "competitive", "cooperative", "continuous", "emergent", "reinforcement learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "All reviewers agree that the proposed is interesting and innovative. One reviewer argues that  some additional baseline comparisons could be beneficial and the other two suggest inclusion of additional explanations and discussions of the results. The authors\u2019 rebuttal alleviated most of the concerns. All reviewers are very appreciative of the quality of the work overall and recommend probable acceptance. I agree with this score and recommend this work for poster presentation at ICLR."}, "review": {"B1eTMQPYkE": {"type": "rebuttal", "replyto": "rye7knCqK7", "comment": "As the discussion period is coming to end, we would like to once again thank the reviewers for their feedback which has helped a lot in improving our paper. Let us know about any additional questions/thoughts/feedback. We would be happy to address them in detail.\n\nBest,\nAuthors. ", "title": "End of discussion period - Thanks for the feedback and review"}, "ryeJRL41J4": {"type": "rebuttal", "replyto": "Skl2aq3q37", "comment": "Hi Reviewer2,\nThanks once again for the helpful feedback on our work. Following our response and updated paper, if you have any updated questions/feedback/thoughts, we would be very happy to address them. We have open sourced our code at https://github.com/IC3Net/IC3Net . Thanks :) .", "title": "Request for feedback"}, "rJlZELVkyV": {"type": "rebuttal", "replyto": "BkxEvnt337", "comment": "Hi Reviewer1,\nThanks once again for the great feedback on our work. As suggested, we have made our code available at https://github.com/IC3Net/IC3Net . If you have any updated questions/feedback/thoughts about our paper following our response, we would be happy to address them. Thanks :) .", "title": "Request for feedback"}, "SJlmcrN1JV": {"type": "rebuttal", "replyto": "SygAse7OaX", "comment": "Hi Reviewer3,\nThanks once again for feedback on our work. We were wondering if you have any updated thoughts/feedback/questions following our response. We would be happy to address them. We have also made our code available at https://github.com/IC3Net/IC3Net . Thanks :) ", "title": "Request for feedback"}, "HylF2N4kJ4": {"type": "rebuttal", "replyto": "H1en6bXKCX", "comment": "As promised, we have released our code on https://github.com/IC3Net/IC3Net . We have added a simple starter reader which can be used to run most of our experiments. \n\nAlong with that, a video showing an episode of Predatory Prey Hard Mixed Version is shown at https://gfycat.com/SimplisticInbornBluetonguelizard\n\nSimilarly, a video showing an episode of Traffic Junction Hard version is shown at: https://gfycat.com/MilkyWhimsicalEagle\n\nWe hope these links further resolve your concerns/questions. Your feedback has already been very useful in improving the paper. We look forward to any additional comments/questions/feedback you have regarding our paper. ", "title": "Code open sourced"}, "H1en6bXKCX": {"type": "rebuttal", "replyto": "rye7knCqK7", "comment": "We thank reviewers for their insightful comments and suggestions. We hope our rebuttal clarifies your concerns. We summarize the major updates in this thread:\n\n1. We are working on code release which will be available at https://github.com/IC3Net/IC3Net . We will update the thread when we have published it.\n2. We have rephrased and improved coherency in some sections as suggested by the reviewers (Section 4, Tables etc.).\n3. We have added new section on analysis of patterns in our results to Appendix 6.2 which includes hypothesis on variance in IC3Net results for StarCraft, explanations of StarCraft Combat results and CommNet results in StarCraft explore task (including a video from run on PP-hard).\n4. We have added results on Predator-Prey cooperative scenario to Appendix 6.3 which show that IC3Net can work equivalently or better than CommNet even in case of cooperative scenarios.\n5. We have added a new subsection on performance of IRIC and IC to Appendix 6.4.\n", "title": "General Response to Reviewers"}, "Sye0fmXF07": {"type": "rebuttal", "replyto": "Skl2aq3q37", "comment": "Dear Reviewer,\n\nWe thank you for your insightful and helpful comments. We have responded to your comments, suggestions and concerns inline below. We hope our answers clarify your concerns.\n\n1.In a non-fully-cooperative environment, sharing hidden state entirely as the only option for communicate is not very reasonable; I think something like sending a message is a better option and more realistic (e.g., something like the work of Mordatch & Abbeel, 2017)\n\nWe agree with the reviewer that sharing hidden state entirely is not the only option for communication but we will like to point out that it is a very reasonable one. We build this claim on the fact that using fixed vocabularies is not scalable to diverse scenarios such as StarCraft with a lot of actions. We believe that vocabulary of agents should be as vast as humans as should be able to capture a lot of state-space scenarios. Even though discrete messages are more interpretable, as we increase the vocabulary size to make it more vast, it gets harder to train agent to learn all the symbols using RL which in-effect limits the scalability of the approach. Since, hidden state is direct encoding of observations in latent space, it effectively capture most of the information necessary in encoded form and thus, create a version of vast vocabulary which is easily trainable.\n\n2. The experiment \"StarCraft explore\" is similar to predator-prey; therefore, instead of explaining StarCraft explore, I would like to see how the model works in StarCraft combat. Right now, the authors explain a bit about the model performance in Starcraft combat, but I found the explanation confusing.\n\nDue to space constraints, we pushed out intrinsic and trivial details of StarCraft Combat task into Section 6.4 (Appendix). We have revised our text to reduce the details on StarCraft Explore and added reference to proper appendices for Combat task. We updated the paper to provide detailed  explanation about the performance of the model in Section 6.2.1.\n\n3. Is there any difference between the results of table 1, if we look at the cooperative setup?\n\nWe have added Table 3 in Section 6.2.1 which shows results of Table 1 in cooperative setup. IC3Net performs equivalently or better than the baselines which shows that IC3Net is a suitable choice for any cooperation setting. As the difficulty level increases the performance gap between IC3Net and baselines increases. IC3Net is able to perform comparable and better than CommNet which is designed for cooperative scenarios and works well in them which suggests that agents can learn to communicate in cooperative scenarios.\n\n4. I think having a baseline that has global communication with IR can show the effect of selective communication better. Does their model outperform a model which has global communication with IR?\n\nWe would like to note that a baseline that has global communication with IR will essentially point to IC3Net with gating action always set to 1.  As our experiments had suggested and we had observed, communication action is learnable but adds extra layer of unnecessary dependency in case of cooperative scenarios. That\u2019s why we suggest users to use communication action to 1 and increase training speed in case of cooperative scenarios. \n\nThis might even work in case of  mixed scenarios if the competitive component is not as strong as cooperative component. In competitive scenarios, such a baseline doesn\u2019t make sense and doesn\u2019t fit in the structure of the task as it is useless to share everything with the opponent. So, we add results on PP-mixed hard version to Appendix 6.3.1 when communication action is set to 1. We would also like to reiterate that these results can be better or equivalent to IC3Net as learning the gating action (even though learnable) can add extra complexity overhead (as observed in the result). At the same time, gating action allows the same model to work in all scenarios regardless of the cooperation setting which is not possible with global communication.  \n", "title": "Thank you for your detailed comments, suggestions and feedback (1/2) "}, "Ske44-mFCm": {"type": "rebuttal", "replyto": "SygAse7OaX", "comment": "Dear Reviewer,\n\nWe thank you for your insightful and helpful comments. We have responded to your comments, suggestions and concerns inline below. We hope our answers clarify your concerns.\n\n1. I feel like the paper can be strengthened by comparing to additional baselines. The authors compare mainly to Sukhbataar et al., but I think a more detailed comparison to other approaches (e.g. Foerster et al.).\n\nCommNet and Foerster et. al. are similar approaches based on continuous communication. But different from our framework, Foerster et. al. is based on Q-learning, which makes it difficult to perform a fair comparison since our approach is based on policy gradient. This is due to the fact that there is no straightforward way to compare the sample complexity of Q-learning and policy gradients method because of the replay buffer. For the same reason, we didn\u2019t compare with other approaches in StarCraft multiagent systems because those were defined for different environments than ours making fair comparison even harder.\n\n2. One of the advantages of this method is that it can be used in non-cooperative settings. I am not familiar with this regime, and I would like a better explanation about why we would train competing agent with the same controller, rather than using a different controller for each team.\n\nUsing same controller over different controller leads to shared weights which is reasonable given that all of the agents are performing the same task. Now, instead of training multiple weights, training single weights leads to faster training times and convergence which is beneficial. Further, we would like to point that our framework is independent of weight sharing and can even be used in scenarios where weights are not shared.\n\n3. In several experimental results, the proposed method seems to have significantly higher variance than the baselines. I would like to see some discussion about why it is the case.\n\nWe agree with the reviewer about the significant variance in IC3Net than the baselines. We would like to reiterate our response to R1 on a similar question about variance:\n\nWe have performed a lot of experiments on StarCraft and can attribute the  significant variance to stochasticity in the environment. There are a huge number of possible states in which agents can end up due to millions of possible interactions and their results in StarCraft and we believe it is hard to learn each one of them. This stochasticity variance can even be seen in simple heuristics baselines like \u201cattack closest\u201d (Win ratio 76.6 +- 8 calculated over 5 runs)  and is in-fact an indicator of how difficult is it to learn real-world scenarios which also have same amount of stochasticity. \nAlbeit, one can ask why don\u2019t we see similar variance in CommNet and others. We believe that this might be due to the fact that adding gating action increases the action-state-space combinations which yields better results while being difficult to learn sometimes. Other point to be noted is that this variance is generally seen when the Win %  (requires learning more states) is above some particular threshold which is close to nothing in baselines. We have added a section 6.2.2 on variance to the paper.\n\n\n4. Also, in some places (e.g. Table 1), the method is highlighted in bold, even though it doesn\u2019t actually outperform the baseline. Please correct this and only highlight the best method (if several methods are tied, either highlight them one, or don\u2019t highlight any).\n\nWe thank reviewer for pointing this out. We have fixed this in the updated version.\n", "title": "Thank you for your detailed feedback and review"}, "r1gNHf7YCQ": {"type": "rebuttal", "replyto": "BkxEvnt337", "comment": "Dear Reviewer,\n\nWe thank you for your insightful and helpful comments. We have responded to your comments, suggestions and concerns inline below. We hope our answers clarify your concerns.\n\n1.  It would be great if the code associated to this could be released but the presentation allows for reproducibility.\n\nAs requested and promised, code will be available with a simple starter README at the link mentioned in the paper (https://github.com/IC3Net/IC3Net ) soon. We will update the thread when we do so.\n\n2. I have found section 4.2 a bit dry. For instance, I had to read the plots caption and the text several times to map get at the deductions made in 4.2.  \n\nWe have modified and made Section 4.2 more coherent so that it can be followed easily. \n\n3. Given the importance of gating in this work, I recommend expanding on this a bit (if space allows it).\n\nWe have decided to expand on gating in the camera-ready version as it will have more space. Further, we would like to take this moment to also mention that while gating is important, our work\u2019s other major contribution is showing how well individualized rewards can work.\n\n4. Small note: in the caption for Figure 3, on the fourth line, did you mean (f) instead of (d) when arguing that agents stop communicating once they reach the prey ( or am I missing something here)?\n\nThanks for notifying about this. You are correct; we did mean (f) there. We have fixed this in the updated version of the paper.\n\n5. Also, would it be possible to provide more insights on why IC3Net is doing better than CommNet except for the Combat-10Mv3Ze task (last table before the conclusion, what makes this task harder for IC3Net)?\n\nOur experiments and visualizations of actual strategy suggested that compared to exploration, combat can be solved far easily if the units learn to stay together. Focused firepower with more quantity in general results in quite good results on combat.  We verify this hypothesis by running a heuristics baseline \u201cattack closest\u201d in which agents have full vision on map and have macro actions available. By attacking the closest available enemy together the agents are able to kill zealots with success ratio of 76.6 +- 8 calculated over 5 runs  even though initialized separately. Also, as described in Section 6.5.2, the global reward in case of win in Combat task is relatively huge compared to the individual rewards for killing other units. We believe that with coordination to stay together, huge global rewards and focus fire--which is achievable through simple cooperation--add up to CommNet's performance in this task.\n\nFurther, in exploration we have seen that agents go in separate direction and have individual rewards/sense of exploration which usually leads to faster exploration of an unexplored area. Thinking in simple terms, exploration of an house would be faster if different people handle different rooms. Achieving this is hard in CommNet because global rewards don\u2019t exactly tell your individual contributions if you had explored separately. Also in CommNet, we have observed a pattern where agents get together at a point and start exploring together from there which delays the exploration for the enemy/prey. We have updated the paper to reflect this hypothesis and its confirmation using the \u201cattack closest\u201d heuristics baseline in Section 6.2.1.\n\nNote: Macro-actions corresponds to \u201cright click\u201d feature in StarCraft and Dota in which a unit can be called to attack on other unit where units follows the shortest path on map towards the unit to be attacked and once reached starts attacking automatically, this essentially overpowers \u201cattack closest\u201d baseline to easily attack anyone under full-vision without any exploration.\n\nNote 2: You can observe the pattern for CommNet in PP, we just talked about at https://gfycat.com/IllustriousMarvelousKagu . This video has been generated using trained CommNet model on PP-Hard. Red \u2018X\u2019 are predators and \u2018P\u2019 is the prey to be found. We can observe the pattern where the agents get together to find the prey leading to slack eventually", "title": "Thank you for your detailed comments and feedback (1/2)"}, "H1eylm7KCX": {"type": "rebuttal", "replyto": "Skl2aq3q37", "comment": "5. Why do IRIC and IC work worst in the medium in comparison to hard in TJ in table1?\nOur visualizations suggest that this is due to high final add-rate in case of medium version compared to hard version. Collisions happen much more often in medium version leading to less success rate (an episode is considered failure if a collision happens) compared to hard where initial add-rate is low to accommodate curriculum learning for hard version\u2019s big grid size. The final add-rate in case of hard level is comparatively low to make sure that it is possible to pass a junction without a collision as with more entry points it is easy to collide even with a small add-rate.\n\n6. Why is CommNet work worse than IRIC and IC in table 2?\nFirst, we need to notice is that IRIC is better than IC also overall, which points to the fact that individualized reward are better than global rewards in case of exploration. This makes sense because if agents cover more area and know how much they covered through their own contribution (individual reward), it should lead to overall more coverage, compared to global rewards where agents can\u2019t figure out their own coverage but \ninstead overall one. Second, in case of CommNet, it is easy to communicate and get together. We observe this pattern in CommNet where agents first get together at a point and then start exploring from there which leads to slow exploration, but IC is better in this respect because it is hard to gather at single point which inherently leads to faster exploration than CommNet..  Third, the reward structure in mixed scenario doesn\u2019t appreciate searching together which is not directly visible to CommNet and IC due to global rewards.\n\nNote: You can observe the pattern for CommNet we just talked about at https://gfycat.com/IllustriousMarvelousKagu . This video has been generated using trained CommNet model on PP-Hard. Red \u2018X\u2019 are predators and \u2018P\u2019 is the prey to be found. We can observe the pattern where the agents get together to find the prey leading to slack eventually.\n\nWe have updated our paper to reflect the answers in Appendix. Thanks for providing us a detailed review and insightful questions.\n", "title": "Thank you for your detailed comments, suggestions and feedback (2/2)"}, "r1lbXf7FAm": {"type": "rebuttal", "replyto": "BkxEvnt337", "comment": "6. Another observation is on the variance terms that are reported for IC3Net. They are often (not always but definitely in the last table before the conclusion) quite higher when compared to the values associated with the baselines. Can this be explained?\n\nWe agree with the  reviewer about the significant variance in case of IC3Net for StarCraft. We have performed a lot of experiments on StarCraft and can attribute the significant variance to stochasticity in the environment. There are a huge number of possible states in which agents can end up due to millions of possible interactions and their results in StarCraft and we believe it is hard to learn each one of them. This stochasticity variance can even be seen in simple heuristics baselines like \u201cattack closest\u201d  and is in-fact an indicator of how difficult is it to learn real-world scenarios which also have same amount of stochasticity. \n\nAlbeit, one can ask why don\u2019t we see similar variance in CommNet and others. We believe that this might be due to the fact that adding gating action increases the action-state-space combinations which yields better results while being difficult to learn sometimes. Other point to be noted is that this variance is generally seen when the Win % (requires learning more states) is above some particular threshold which is close to  nothing in baselines. We have added a section 6.2.2 on variance to the paper.\n\n7. Another small thing: please add captions to your tables (at least a table number; I think that Table 2 does not have a caption).\n\nWe have updated the paper to better reflect captions for the tables. Thanks for pointing this out. \n", "title": "Thank you for your detailed comments and feedback (2/2)"}, "SygAse7OaX": {"type": "review", "replyto": "rye7knCqK7", "review": "The authors propose a new network architecture for multi-agent reinforcement learning. The new architecture addresses three issues: (1) the applicability of existing algorithms to semi-cooperative or competitive settings; (2) the ability to use local rewards during agent training; (3) the credit assignment problem with global multi-agent rewards. The authors address these issues with a new architecture that is comprised of several LSTM controllers with tied weights that transmit a continuous vector to each other, and that are augment with a gating mechanism that allows them to abstain from communicating.\n\nI think that this paper makes a solid contribution over the existing literature. My main comments are the following:\n* I feel like the paper can be strengthened by comparing to additional baselines. The authors compare mainly to Sukhbataar et al., but I think a more detailed comparison to other approaches (e.g. Foerster et al.)\n* One of the advantages of this method is that it can be used in non-cooperative settings. I am not familiar with this regime, and I would like a better explanation about why we would train competing agent with the same controller, rather than using a different controller for each team.\n* In several experimental results, the proposed method seems to have significantly higher variance than the baselines. I would like to see some discussion about why it is the case.\n* Also, in some places (e.g. Table 1), the method is highlighted in bold, even though it doesn\u2019t actually outperform the baseline. Please correct this and only highlight the best method (if several methods are tied, either highlight them one, or don\u2019t highlight any).\n* Also, in some cases when the error bars contain the previous best result, I am not sure if we can say that the proposed method is obviously better.", "title": "Interesting work", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkxEvnt337": {"type": "review", "replyto": "rye7knCqK7", "review": "From a methodological perspective, this paper describes a simple bu clever learning architecture with individual agents able to decide when to communicate through a learned gating mechanism. Each agent is an LSTM able to decide at each time point which aspects of its internal state should be exposed to other agents through this gating mechanism. The presentation of this method is clear to a level that should allows the reader to implement this him/herself. It would be great if the code associated to this could be released but the presentation allows for reproducibility. \n\nThe experiments are interesting as well. Experimental results are presented on 3 problems and compared with known baselines from the academic community. The obtained results do show the merit of the approach. That being said, while the experimental results are extensive, there are places that could benefit from more clarity. For instance, I have found section 4.2 a bit dry. For instance, I had to read the plots caption and the text several times to map get at the deductions made in 4.2. Given the importance of gating in this work, I recommend expanding on this a bit (if space allows it). Small note: in the caption for Figure 3, on the fourth line, did you mean (f) instead of (d) when arguing that agents stop communicating once they reach the prey ( or am I missing something here)? Also, would it be possible to provide more insights on why IC3Net is doing better than CommNet except for the Combat-10Mv3Ze task (last table before the conclusion, what makes this task harder for IC3Net)? Another observation is on the variance terms that are reported for IC3Net. They are often (not always but definitely in the last table before the conclusion) quite higher when compared to the values associated with the baselines. Can this be explained? Another small thing: please add captions to your tables (at least a table number; I think that Table 2 does not have a caption). \n\n\nOverall, the paper is well written, interesting. Addressing the questions raised above would definitely help me and probably the eventual readers better appreciate its quality. ", "title": "In this work, the authors propose an interesting gating scheme allowing agents to communicate in an multi-agent RL setting. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Skl2aq3q37": {"type": "review", "replyto": "rye7knCqK7", "review": "This work is an extension to the work of Sukbaatar et al. (2016) with two main differences:\n1) Selective communication: agents are able to decide whether they want to communicate.\n2) Individualized reward: Agents receive individual rewards; therefore, agents are aware of their contribution towards the goal.\nThese two new extensions enable their model to work in either cooperative or a mix of competitive and competitive/collaborative settings. The authors also claim these two extensions enable their model to converge faster and better. \nThe paper is well written, easy to follow, and everything has been explained quite well. The experiments are competent in the sense that the authors ran their model in four different environments (predator and prey, traffic junction, StarCraft explore, and StarCraft combat). The comparison between their model with three baselines was extensive; they reported the mean and variance over different runs. I have some concerns regarding their method and the experiments which are brought up in the following:\n \nMethod:\n\nIn a non-fully-cooperative environment, sharing hidden state entirely as the only option for communicate is not very reasonable; I think something like sending a message is a better option and  more realistic (e.g., something like the work of Mordatch & Abbeel, 2017)\n\nExperiment:\n\nThe experiment \"StarCraft explore\" is similar to predator-prey; therefore, instead of explaining StarCraft explore, I would like to see how the model works in StarCraft combat. Right now, the authors explain a bit about the model performance in Starcraft combat, but I found the explanation confusing.\n \nAuthors provide 3 baselines:\n1) no communication, but IR\n2) no communication, no IR\n3) global communication, no IR (commNet)\n\nI think having a baseline that has global communication with IR can show the effect of selective communication better. \n\nThere are some questions in the experiment section that have not been addressed very well. For example:\n Is there any difference between the results of table 1, if we look at the cooperative setup? \nDoes their model outperform a model which has global communication with IR? \nWhy do IRIC and IC work worst in the medium in comparison to hard in TJ in table1? \nWhy is CommNet work worse than IRIC and IC in table 2?", "title": "very well written paper, good experiment section, method of communication can be more motivated", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}