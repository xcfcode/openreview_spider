{"paper": {"title": "CDeepEx: Contrastive Deep Explanations", "authors": ["Amir Feghahati", "Christian R. Shelton", "Michael J. Pazzani", "Kevin Tang"], "authorids": ["sfegh001@ucr.edu", "cshelton@cs.ucr.edu", "pazzani@ucr.edu", "ktang012@ucr.edu"], "summary": "A method to answer \"why not class B?\" for explaining deep networks", "abstract": "We propose a method which can visually explain the classification decision of deep neural networks (DNNs). There are many proposed methods in machine learning and computer vision seeking to clarify the decision of machine learning black boxes, specifically DNNs.  All of these methods try to gain insight into why the network \"chose class A\" as an answer. Humans, when searching for explanations, ask two types of questions. The first question is, \"Why did you choose this answer?\" The second question asks, \"Why did you not choose answer B over A?\" The previously proposed methods are either not able to provide the latter directly or efficiently.\n\nWe introduce a method capable of answering the second question both directly and efficiently. In this work, we limit the inputs to be images. In general, the proposed method generates explanations in the input space of any model capable of efficient evaluation and gradient evaluation. We provide results, showing the superiority of this approach for gaining insight into the inner representation of machine learning models.", "keywords": ["Deep learning", "Explanation", "Network interpretation", "Contrastive explanation"]}, "meta": {"decision": "Reject", "comment": "Paper studies an important problem -- producing contrastive explanations (why did the network predict class B not A?). Two major concerns raised by reviewers -- the use of one learned \"black-box\" method to explain another and lack of human-studies to quantify results -- make it very difficult to accept this manuscript in its current state. We encourage the authors to incorporate reviewer feedback to make this manuscript stronger for a future submission; this is an important research topic. "}, "review": {"rJx5ghGShX": {"type": "review", "replyto": "HyNmRiCqtm", "review": "The idea proposed in this paper is to aid in understanding networks by showing why a network chose class A over class B.  To do so, the goal is to find an example that is close to the original sample, but belongs to the other class. As is mentioned in the paper, it is crucial to stay on the data manifold for this to be meaningful. In the paper, an approach using a GAN to traverse the manifold is proposed and the experimental evaluation is done on MNIST.\n\nIf my understanding is correct the proposed approach requires:\nFinding a noise code z_0 such that the GAN generates an image G(z_0) close to the original input x. As a metric L2 distance is proposed.\nFind a point close to z_b that is close z_0  s.t. Class B is the most likely class and class A is the second most likely prediction. Specifically it is required that\nThe log likelihood of but classified as class B with the same log likelihood of class B for G(z_b) is the same as the log likelihood of class A for the input x.\nSuch that all other classes have a log likelihood that is at least epsilon lower than both the one of class A and class B.\n\nThe proposed approach is compared to a set of other interpretability methods, which were \nGrad-Cam, lime, PDA, xGEM on MNIST AND Fashion MNIST data. The proposed evaluation is all qualitative, i.e. subjective. It must also be noted that in the methods used for comparison are not used as originally intended.\n\n\nCurrently, I do not recommend this paper to be accepted for the following reasons.\nThe idea of using a GAN is to generate images in input space is not novel by itself. Although the application for interpretability by counterfactuals is. It is unclear to me how much of the appealing results come from the GAN model and how much come from truly interpreting the network. I have detailed this below by proposing a very simplistic baseline which could get similar results.\nThe experimental approach is subjective and I am not convinced by the experimental setup.\nOn the other hand, I do really appreciate the ideas of traversing the manifold. \n\nRemarks \nRelated work and limitations of existing interpretability methods are discussed properly. Of course, the list of discussed methods is not exhaustive. The work on the PPGAN and the \u201cSynthesizing the preferred inputs for neurons in neural networks via deep generator networks\u201d is not mentioned although it seems very related to the proposed approach to traverse the manifold. What that work sets apart from the proposed approach is that is could be applied to imaganet and not just MNIST. \n\nTraversing the manifold to generate explanations is certainly a good idea and one that I completely support. One limitation of the proposed approach is that it is unclear to me whether a point on the decision boundary is desirable or that a point that is equally likely is desirable. My reasoning is that the point on the decision boundary is the minimal change and therefore the best explanation. In such a setup, the GAN is still crucial to make sure the sample remains on the data manifold and is not caused by adverarial effects.\n\nThe exact GAN structure and training approach should be detailed in this paper. Now only a reference is provided. \n\nCan you clarify how the constraints are encoded in the optimization problem?\n\nThe grad cam reference has the wrong citation\n\nI do not understand the second paragraph of section 4.1. As mentioned in the paper, these other methods were not designed to generate this type of application. Therefore the comparison could be considered unfair. \n\nI would propose the following baseline. For image x from class A, find image y from class B such that x-y has minimal L2 norm and is correctly classified. Use y instead of the GAN generated image. Is the result much less compelling? Is it actually less efficient that the entire GAN optimization procedure on these relatively small datasets? \n\n\nI do have to say that I like the experiment with the square in the upper corner. It does show that the procedure does not necesarrily exploits adversarial effects. However, the baseline proposed above would also highlight that specific square?\n\n\nFigure 5 shows that multiple descision boundaries are crossed. Is this behaviour desired? It seems very likely to me that it should be possible to move from 9 to 8 while staying on the manifold without passing through 5? Since the method takes a detour through 5\u2019s is this common behaviour?\n\n\nFINAL UPDATE\n--------------------\nUnfortunately, I am not entirely convinced by the additional experiments that we are truly looking into the classifier instead of analyzing the generative model. \nI believe this to be currently the key issue that, even after the revision, needs to be addressed more thoroughly before it can be accepted for publication. ", "title": "Review", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkg0nroYn7": {"type": "review", "replyto": "HyNmRiCqtm", "review": "The paper proposes an approach to provide contrastive visual explanations for deep neural networks -- why the network assigned more confidence to some class A as opposed to some other class B. As opposed to the applicability of previous approaches to this problem -- the approach is designed to directly answer the contrastive explanations question rather adapting other visual saliency techniques for the same. Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.\n\n- Apart from some flaws in the claims made in the paper, the paper is easy to follow and understand.\n- Assuming the availability of a latent model over the images of the input distribution, the proposed approach is directly applicable and faster.\n- The authors clearly highlight the problems associated with existing explanation modalities and approaches; ranging from ones applicable to only specific deep architectures to ones using backpropagation based heuristics.\n- The proposed approach to generate contrastive explanations is simple and is structured along the lines of methods utilizing probe images to explain decisions -- except for the added advantage that the provided explanations are instance-agnostic due to the assumption of a latent model over the input distribution.\n\nComments:\n- One of the problems highlighted in the paper regarding existing explanation modalities is the use of another black-box to explain the decisions of an existing deep network (also somewhat of a black-box) which the authors claim their model does not suffer from. The proposed approach provides explanations by operating in the latent space of a learned generative model of the input distribution. The learned generator in itself is somewhat of a black-box itself -- there has been prior work indicating how much of the input distribution are GANs able to capture. As such, conditioning on a generative model to propose such contrastive explanations is to some extent using another black-box (generator) to explain the decisions of an existing one. Thus, the above claim made in the paper does not seem well-founded. Furthermore, in experiments, the paper does not provide any quantitatively convincing results to suggest the generator in use is a good one.\n- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself. In this light, experiments demonstrating comparisons between GANs and VAEs as the reference generative model for explanations would have made the paper stronger (as the proposed approach relies explicitly on how good the generative model is). \n- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class \u20188\u2019). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.\n- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution. Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading. In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same. In Section 4.1, the use of Gradcam and Lime to generate counterfactual explanations is not very clear and makes it slightly hard to follow. Citations used for Gradcam are wrong -- Sundarajan et al., 2016 should be changed to Selvaraju et al., 2017.\n\nExperimental Issues:\n- Experimental results are provided only on MNIST and Fashion-MNIST. Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient. Additional experiments on at least ImageNet would have made the paper stronger.\nRegarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc. Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a \u2018cat\u2019 to be classified as a \u2018dog\u2019 while there is an instance of the class - \u2018dog\u2019 present in the image itself. Also, section 7 in Gradcam (https://arxiv.org/pdf/1610.02391.pdf) provides a procedure to generate counter-factual explanations using Gradcam. Is there a particular reason the authors did not choose to adopt the above technique as a baseline?\n- Experimental results provided in the paper are only qualitative -- as such, I do not find the comparisons (and improvements) over the existing approaches convincing enough. Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.\n\nThe authors adressed the issues raised/comments made in the review. In light of my comments below to the author responses -- I am not inclined towards increasing my rating and will stick to my original rating for the paper.", "title": "Interesting idea but lacks experimental justification", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJgSW0AK0Q": {"type": "rebuttal", "replyto": "HyxoFlTF0X", "comment": " Could you indicate some concrete examples of scenarios where answering the \"why A and not B?\"  question is critical? \n\n\nLet me elaborate a little bit.\nIn medical diagnosis, what we are doing is called differential diagnosis.  \nOne question is \"what are the symptoms of a flu.\"  In differential diagnosis, one asks how can i tell a flu from a cold?  or why is this image basal cell carcinoma instead of melanomia?\n\nIn the bird watching scenario,  one could ask how can you tell a grebe from a duck (Both are waterbirds, but the grebe has a long neck).   One could also ask why is the a Western Grebe vs. A Clark's Grebe (because the western grebe has a dark patch below its eye).\n\nWe concentrate on MNIST in this paper because the average computer scientist reading it has knowledge of the digits but not necessarily bird or skin cancer.\n", "title": "Examples of why this research is important "}, "H1x6HWaF07": {"type": "rebuttal", "replyto": "BJlWvUII0m", "comment": "\ncomment: You argue, gradient descent on the input image is not providing a satisfactory explanation because of adversarial effects, and I agree. \nHowever even with a VAE or GAN, it is unclear how much of the explanation comes from what the network thinks vs what the generative model thinks. \n\nResponse: We added another section to our supplementary materials, showing the effect of generative models on the explanations.", "title": "Explanations do not come from Generative  models"}, "H1eiGWpt0m": {"type": "rebuttal", "replyto": "BJg3248ICQ", "comment": "\ncomment: While the auto-encoder is not as good as a GAN as a generative model, it is unclear to me \nwhy would this approach would be worse at visualizing what the network thinks.  It is crucial that \nthis aspect can be experimentally validated. \n\nResponse: We have added experiments using VAE, showing the results do not change noticeably. This suggests that the \nexplanations generated by our method come from the discriminator\nand not the generative model itself. Also, we added another section to our supplementary materials,\nshowing the effect of generative models on the explanations.\n\ncomment: Finally, if the L2 distance is small this does not mean that we are close by, especially as we\nincrease the number of dimensions. Consider https://openreview.net/pdf?id=S1xoy3CcYX figure 5. \nThey have visualised two images with similar L2 distance, however \"small\" L2 does not mean are similar.\nTherefore I believe it is crucial how you traverse space and that this path is short (for an appropriate\nmeasure of length) and does not traverse multiple decision boundaries.\n\n\nResponse: We agree. Yet, the L2-norm gives us an smooth transition. Traversing multiple decision boundaries is\nnot necessarily a disadvantage. Also experiments on xGEMs shows that minimizing the length of this optimization\nmay not lead to something sensible in the image space. One may conclude that the network has not learned \nrelated concepts, while using our method shed more light on what concepts network has learned.", "title": "Other generative models"}, "HyxoFlTF0X": {"type": "rebuttal", "replyto": "SJePAMZI07", "comment": "comment: Could you indicate some concrete examples of scenarios where answering the \"why A and not B?\"  question is critical? \n\t\nFor instance, in medical diagnosis contrasting the results helps the doctor to not miss details or explains the\n\toutcome to patient. \n\tOther use can be in training humans. Suppose you have a network trained on classifying birds. It can be used to \n\ttrain amateur bird watchers. For instance, different types of Warblers are very similar to each other for amateur watcher. \n\tThey can provide a picture of a bird to network and ask what needs to be changed in the image to change the bird's label.", "title": "Examples of why this research is important"}, "HklmqfFmAX": {"type": "rebuttal", "replyto": "rJx5ghGShX", "comment": "\n\nComment: I would propose the following baseline. For image x from class\nA, find image y from class B such that x-y has minimal L2 norm and is\ncorrectly classified. Use y instead of the GAN generated image. Is the\nresult much less compelling? Is it actually less efficient that the entire\nGAN optimization procedure on these relatively small datasets?\n\nResponse: This is an interesting baseline but not related to the task.\nThis baseline asks for, \u201dwhat is a good change in input image based\non the training samples which are correctly classified?\u201d This\nis fundamentally different from asking \u201dwhat does the *network*\nthink is a good change to input image?\u201d We are not looking for the\n\"correct\" explanation.  We are looking for what the network thinks\nthat is a correct explanation, which may be right or wrong.  Consider a\nnetwork trained on MNIST with 99% accuracy.  This says that the network\ncorrectly classifies almost all of the images from the probe class.  Then,\nthe baseline reduces to which image from probe class has the minimum L2\ndistance to the input image.  In this procedure, the trained network has\nalmost no role, and therefore it is not an explanation of the network,\njust an explanation of the data.\n\n\nComment: Figure 5 shows that multiple descision boundaries are crossed. Is\nthis behaviour desired? It seems very likely to me that it should be\npossible to move from 9 to 8 while staying on the manifold without\npassing through 5? Since the method takes a detour through 5s is this\ncommon behaviour?\n\nResponse: It is not about desirability. This figure shows that without\nadding constraints, the method may go through other parts of manifold,\nresulting in a wrong speculation image.  That is, xGEMs will not find a\npoint on the boundary between the two desired classes, but somewhere else\n(where the two classes have equal probability, but lower than some third\nclass).  The classification space is complicated enough that \"staying near\"\nthe input is not sufficient.\n\nFigure 5a, top shows that the network knows about the importance of the\ncurves, if we impose our constraints.  If we remove the constraints (like\nin xGEMs), we lose this explanation and revert to something meaningless,\nas the optimization path explores other classes.\n\nNote that these are just paths of the optimizer (only the point at the end\nof the optimization path is the \"answer\").  However, they demonstrate the\ndifficulty with optimization in this complex decision space.  Figure 5b\nshows the problem, where xGEMs (with the constraints) fails to keep the\nanswer in the area of high likelihood for either of the classes.", "title": "Answers part 2"}, "B1g1tMt7AX": {"type": "rebuttal", "replyto": "rJx5ghGShX", "comment": "\nComment: The idea of using a GAN is to generate images in input space\nis not novel by itself.  Although the application for interpretability\nby counterfactuals is.  It is unclear to me how much of the appealing\nresults come from the GAN model and how much come from truly interpreting\nthe network.\n\nResponse: We do not believe we claimed generating images in input space\nas a novel contribution of the work.  We have added results using a VAE\n(instead of a GAN) to demonstrate that the GAN is not providing the\nexplanation power; it is merely serving to keep the changes in the space\nof natural images.\n\n\nComment: One limitation of the proposed approach is that it is unclear\nto me whether a point on the decision boundary is desirable or that a\npoint that is equally likely is desirable.  My reasoning is that the\npoint on the decision boundary is the minimal change and therefore the\nbest explanation. In such a setup, the GAN is still crucial to make sure\nthe sample remains on the data manifold and is not caused by adverarial\neffects.\n\nResponse: Perhaps we have misunderstood the comment.  We are selecting\na point on the boundary between two classes (probe and true).  Among all\nof the points on the boundary between the two classes, we select the one\nof minimal change to the input (image).  Is the suggestion for something\ndifferent?\n\n\nComment: The exact GAN structure and training approach should be detailed\nin this paper. Now only a reference is provided.\n\nResponse: We have added these details to the supplementary materials. Thank\nyou for your suggestion.\n\n\nComment: Can you clarify how the constraints are encoded in the\noptimization problem?\n\nResponse: We encoded them mathematically as in the paper.  We solved them\nas detailed in Bertsekas, D. P. (1999). Nonlinear Programming. Athena\nScientific, second edition, section 4.2.  It is an alternating optimization\nproblem.  One part has a closed form.  The second part can be performed\nby using backpropagation through the network.\n\nComment: The grad cam reference has the wrong citation.\n\nResponse:  Thank you.  We have fixed this.\n\n\nComment: I do not understand the second paragraph of section 4.1. As\nmentioned in the paper, these other methods were not designed to generate\nthis type of application. Therefore the comparison could be considered\nunfair.\n\nResponse:  The only method we found before submitting the paper which was\nable to answer the contrastive explanation was xGems.  However, other\nmethods could be shoe-horned into trying to answer the question of \"why A\nand not B?\" and so we figured we should demonstrate that they were not\nsufficient and that a new method (like ours) was necessary.\n", "title": "Answers part 1"}, "HyxLHfF7RQ": {"type": "rebuttal", "replyto": "ryxtBh5_h7", "comment": "Comment: The proposed method seems to be specifically designed for the\ngeneration of contrastive explanations, i.e.  why the model predicted class\nA and not class B. While the generation of this type of explanations is\nsomewhat novel, from the text it seems that the proposed method may not\nbe able to indicate what part of the image content drove the model to\npredict class A. Is this indeed the case?\n\nResponse: The goal of this paper is not to answer \"why A?\" but rather\n\"why A and not B?\"  The visual answer to the two questions may be similar,\nbut it may not.  We seek to highlight what in the image would need to\nchange to make it a B and not an A, as a way of explaining this contrast.\nThere are other papers that seek to answer the question of \"why A?\" but \nthat is not our focus.\n\n\nComment: Although the idea of generating contrastive explanations is\nquite interesting, it is not that novel. See Kim et al., NIPS\u201916,\nDhurandhar et al., arXiv:1802.07623.\n\nResponse: Dhurandhar et al. does use the term constrastive explanation.\nHowever, they look at the question of \"why A?\"  Contrastive in their case\nrefers to whether something is or is not present that drives the\nclassification of \"A.\"  This is a different constrast than ours that \ncontrasts \"A\" to \"B\" (rather than \"present for A\" to \"absent for A\").\nWe think this is also an interesting form of explanation, but a different\none.\n\nKim et al. also has a different form of model criticism; they look at the\ndataset as a whole for examples that help explain.  We look at a different\nproblem: for a given example (perhaps not even from the training set),\nwhy is it not class B?\n\n\nComment: The work from Samek et al., TNNLS\u201917 and Oramas et al.,\narXiv:1712.06302 seem to display similar properties in their explanations\nwithout the need of explicit constractive pair-wise training/testing. The\nmanuscript would benefit from positioning the proposed method w.r.t. these\nworks.\n\nResponse: The work from Samek et al. is similar to PDA in its essence. We\nwill add the comparison with this method to our work for sake of\ncompleteness.  In the experiment section of Oramas et al., they proposed\na synthetic flowers dataset that can be used for our purpose.  Since it\nis synthetic and fine-grained, we can compare the method qualitatively\nand quantitatively.  We sent a request to the authors for accessing the\ndataset.  If we granted this access we will add quantitative comparisons\nto our paper.\n\n\nComment: In the evaluation section (Sec.4.1) the proposed method is\ncompared against other methods in the literature. Three of these methods,\ni.e. Lime, GradCam, PDA, are not designed for producing contrastive\nexplanations, so I am not sure to what extend this comparison is\nappropriate.\n\nResponse: The only method we found before submitting the paper which was\nable to answer the contrastive explanation was xGems.  However, other\nmethods could be shoe-horned into trying to answer the question of \"why A\nand not B?\" and so we figured we should demonstrate that they were not\nsufficient and that a new method (like ours) was necessary.\n\n\nComment: The reported results are mostly qualitative. I find the set of\nprovided qualitative examples quite reduced. In this regard, I encourage\nthe authors to update the supplementary material in order to show extended\nqualitative results of the explanations produced by their method.\n\n\nResponse: We have added a supplementary section, adding more qualitative\nresults. Thank you for your suggestion.", "title": "Answers"}, "B1xsXfY7RQ": {"type": "rebuttal", "replyto": "rkg0nroYn7", "comment": "Comment: One of the problems highlighted in the paper regarding existing\nexplanation modalities is the use of another black-box to explain the\ndecisions of an existing deep network (also somewhat of a black-box) which\nthe authors claim their model does not suffer from.\n\nResponse: While the GAN is certainly another black box, it is a function\njust of the data (or the data domain), and not a function of the\ndiscriminator from which we want to extract explanations.  Therefore,\ntraining different models, switching prediction tasks on the same\ndomain, or any other similar changes would not require changing the GAN.\n\nComment: Learning such a model of the input space is an overhead in itself.\n\nResponse: Overhead calculations of some form are almost impossible to \navoid.  Whether this is an overly large computational burdon will depend \non the problem, although we believe the GAN or VAE need only be trained once \nper domain.\n\nComment: The paper does not provide any quantitatively convincing results\nto suggest the generator in use is a good one.\n\nResponse: Measuring the reconstruction accuracy (of goodness of the\ngenerator) is difficult, as each measure has its own flaws.  For instance,\nNorm measures are sensitive to translations in the image.\n\n\nComment: Experiments demonstrating comparisons between GANs and VAEs as the\nreference generative model for explanations would have made the paper\nstronger (as the proposed approach relies explicitly on how good the\ngenerative model is)\n\nResponse: This is a good suggestion. We have added experiments using\nwith variational autoencoders (VAEs) instead of GANs.  We believe these\naddress this concern and some of the comments above.  Thank you.\n\n\nComment: The paper proposes an interesting experiment to show that the\nproposed approach is somewhat capable of capturing slightly adversarial\nbiases in the input domain (adding square to the top-left of images of\nclass 8). While I like this experiment, I feel this has not been explored\nto completion in the sense of experimenting with robustness with respect\nto structured as well as unstructured perturbations.\n\nResponse: While we could certainly perform more experiments in this vein,\nwe are uncertain what type of unstructured perturbations would be useful\n(and how then to measure whether the technique captured the correct\nexplanation).\n\n\nComment: typographical errors...\n\nResponse:  Thank you.  We have fixed the typos.\n\n\nComment: Section 7 in Gradcam (https://arxiv.org/pdf/1610.02391.pdf)\nprovides a procedure to generate counter-factual explanations using\nGradcam. Is there a particular reason the authors did not choose to adopt\nthe above technique as a baseline?\n\nResponse: The the proposed counter-factual experiment for GCAM produces\n*any* counter-factual explanation, not a targeted explanation.  It answers\n\"why A?\" and not \"why A and not B?\" as we do in this paper.\n\n", "title": "Answers"}, "S1exWztQR7": {"type": "rebuttal", "replyto": "HyNmRiCqtm", "comment": "\n\nWe thank the reviewers for their comments and reviews.  We address many\nof the comments below, including by adding additional experiments, as\nsuggested.  \n\nWe would like to stress again that the purpose of the method is\nto determine how the neural network is making decisions, *not* necessarily\nto find general distinctions in the data, although the two\nare certainly related.\n\nWe have submitted a revised version of our paper. This revision contains\nthe following:\n\n1 - Experiments with VAE instead of GAN, showing the robustness of our approach.\n2 - Adding comparisions with Layerwise Relevance Propagation (LRP)\n3 - Adding experiments in supplementary section using VAE instead of GAN\nusing xGem method, showing the importance of the constraints regardless\nof the model being used.\n4 - adding more qualitative samples in the supplementary section.\n5 - The structure of the networks we have used.", "title": "General Comment"}, "ryxtBh5_h7": {"type": "review", "replyto": "HyNmRiCqtm", "review": "\nThe paper addresses the problem of providing saliency-based visual explanations of deep models tasked at image classification. More specifically, instead of generating visualizations directly highlighting the image pixels that support the the decision of an image belonging to class A, it generates \"contrastive\" visualizations indicating the pixels that should be added or suppressed in order to support the decision of a image belonging to class A and not to class B.\n\nThe method formulates the generation of these contrastive explanations through a generative adversarial network (GAN), where the discriminator D is the image classification model to be explained and the generator G is a generative model trained to produce images from the dataset used to train D.\n\nExperiments on the MNIST and fashion-MNIST datasets compares the performance of the proposed method w.r.t. some methods from the literature.\n\n\nOverall the manuscript is well written and its content is relatively easy to follow. The idea of generating contrastive explanations through a GAN-based formulation is well motivated and seems novel to me.\n\nMy main concern with the manuscript are the following:\n\ni) The proposed method seems to be specifically designed for the generation of contrastive explanations, i.e. why the model predicted class A and not class B. While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?\n\nii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel. See Kim et al., NIPS'16, Dhurandhar et al., arXiv:1802.07623. Moreover, regarding the presented results on the MNIST dataset (Sec 4.1) where some of the generated explanations highlight gaps to point differences between digit classes. The work from Samek et al., TNNLS'17 and  Oramas et al., arXiv:1712.06302 seem to display similar properties in their explanations without the need of explicit constractive pair-wise training/testing. The manuscript would benefit from positioning the proposed method w.r.t. these works.\n\niii) Very related to the first point, in the evaluation section (Sec.4.1) the proposed method is compared against other methods in the literature. Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.\n\niv) Finally, the reported results are mostly qualitative. I find the set of provided qualitative examples quite reduced. In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.\nIn addition, I recommend complementing the presented qualitative comparisons with quantitative evaluations following protocols proposed in existing work, e.g. a) occlusion analysis (Zeiler et al., ECCV 2014, Samek et al.,2017), a pointing experiment (Zhang et al., ECCV 2016), or c) a measurement of explanation accuracy by feature coverage (Oramas et al. arXiv:1712.06302).\n\n", "title": "interesting idea with potential", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}