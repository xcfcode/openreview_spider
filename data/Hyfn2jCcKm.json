{"paper": {"title": "Solving the Rubik's Cube with Approximate Policy Iteration", "authors": ["Stephen McAleer", "Forest Agostinelli", "Alexander Shmakov", "Pierre Baldi"], "authorids": ["smcaleer@uci.edu", "fagostin@uci.edu", "ashmakov@uci.edu", "pfbaldi@ics.uci.edu"], "summary": "We solve the Rubik's Cube with pure reinforcement learning", "abstract": "Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data. These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network). In these two-player games, a reward is always received at the end of the game. However, the Rubik\u2019s Cube has only a single solved state, and episodes are not guaranteed to terminate. This poses a major problem for these API algorithms since they rely on the reward received at the end of the game. We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away. Autodidactic Iteration is able to learn how to solve the Rubik\u2019s Cube and the 15-puzzle without relying on human data. Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves \u2014 less than or equal to solvers that employ human domain knowledge.", "keywords": ["reinforcement learning", "Rubik's Cube", "approximate policy iteration", "deep learning", "deep reinforcement learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper introduces a version of approximate policy iteration (API), called Autodidactic Iteration (ADI), designed to overcome the problem of sparse rewards.  In particular, the policy evaluation step of ADI is trained on a distribution of states that allows the reward to easily propagate from the goal state to states farther away.  ADI is applied to successfully solve the Rubik's Cube (together with other existing techniques).\n\nThis work is an interesting contribution where the ADI idea may be useful in other scenarios.  A limitation is that the whole empirical study is on the Rubik's Cube; a controlled experiment on other problems (even if simpler) can be useful to understand the pros & cons of ADI compared to others.\n\nMinor: please update the bib entry of Bottou (2011).  It's now published in MLJ 2014."}, "review": {"SJx3iUcc27": {"type": "review", "replyto": "Hyfn2jCcKm", "review": "The authors show how to solve the Rubik cube using reinforcement learning (RL) with Monte-Carlo tree search (MCTS). As common in recent applications like AlphaZero, the RL part learns a deep network for policy and a value function that reduce the breadth (policy) and depth (value function) of the tree searched in MCTS. This basic idea without extensions fails when trying to solve the Rubik cube because there is only one final success state so the early random policies and value functions never reach it. The solution proposed by the authors, called autodidactic iteration (ADI) is to start from the final state, construct a few previous states, and learn value function on this data where in a few moves a good state is reached. The distance to the final state is then increased and the value function learn more and more. This is an interesting idea that solves the Rubik cube, but the paper lacks a more detailed study. What other problems can be solved like this? Would a single successful trajectory be enough to use it in a wider context (as in https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/) ? Is the method to increase distance from final state specific to Rubik cube or general? Is the training stable with respect to this or is it critical to get it right? The lack of analysis and ablations makes the paper weaker.\n\n[Revision] Thanks for the replies. I still believe experiments on more tasks would be great but will be happy to accept this paper.", "title": "Nice idea but little study", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1xDiKEcC7": {"type": "rebuttal", "replyto": "HJlaAp7wn7", "comment": "We would like to thank the reviewer for their helpful comments and for pointing us to the github resource.\n\n> \u201cI am slightly disappointed that the paper does not link to a repository with the code. Is this something the authors are considering in the future?\u201d\n\nWe fully agree with the reviewer that releasing the code is important. We plan to release the code if the paper gets accepted. We have not done so yet to maintain anonymity.\n\n> \u201cI am also curious whether/how redundant positions are handled by the proposed approach...Does the algorithm forbid the reverse of the last action? Is the learned value/policy function good enough that backwards moves are seldom explored? Since the paper mention that BFS is interesting to remove cycles, I assume identical states are not duplicated. Is this correct?\u201d\n\nWe did not strictly forbid reverse moves during the search. However, because we penalize longer solutions, because MCTS attempts many paths simultaneously, and because the virtual loss prevents duplicate exploration, the solver rarely explored repeat states. The BFS expansion of the path was a post-processing step we applied to the resulting path to obtain slightly better solutions. Although this did remove duplicates (if they existed), it more importantly allowed us to find \"shortcuts\" within our path. For example, we can replace say a 7-move sequence with a slightly more efficient 5-move sequence that MCTS didn't find. This effect was minimal but consistent.\n", "title": "Response to Reviewer 3"}, "Bkgout45CX": {"type": "rebuttal", "replyto": "SJx3iUcc27", "comment": "We would like to thank the reviewer for their helpful comments.\n\n> \u201cWhat other problems can be solved like this?\u201d\n\nThis approach can be used in two different types of problems. The first is planning problems in environments with a high number of states. The second type of problem is when you need to find one specific goal but might not know what the goal is. However, if you have examples of solved examples you can train a value function using ADI on these solved examples and hopefully it will transfer to the new problems. For instance, in protein folding, the goal is to find the protein conformation with minimal free energy. We don\u2019t know what the optimal conformation is beforehand, but we can train a value network using ADI on proteins where we know what their optimal conformation is. \n\n\n> \u201cWould a single successful trajectory be enough to use it in a wider context? (as in https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/)\u201d\n\nFor our method to work, all we need is the ability to start from the goal state and take moves in reverse. Therefore, not only is a single successful trajectory sufficient, all that is needed is the final state of that successful trajectory: the goal state. Using only the goal state, it can generate other states by randomly taking actions away from the goal state.\n\n> \u201cIs the method to increase distance from final state specific to Rubik cube or general?\u201d\n\nThe core concept is that the agent uses dynamic programming to propagate knowledge from easier examples to more difficult examples. Therefore, this method is applicable to any scenario in which one can generate a range of states whose difficulty ranges from easy to hard. For our method, we achieved this by randomly scrambling the cube 1 to N times. There has been other work in the field of robotics [1], as well as the work on Montezuma\u2019s Revenge provided by the reviewer, that builds a curriculum starting by first generating states close to the goal and then progressively increasing the difficulty as performance increases. Instead of adaptively changing the state distribution during training, our method fixes the state distribution before training while the targets for the state values change as the agent learns.\n\n> \u201cIs the training stable with respect to this or is it critical to get it right?\u201d\n\nWe found that the value of N, the maximum number of times to scramble the solved cube, was not crucial to the stability of training. It only had an effect on the final performance. If N was too low (e.g. 5), then DeepCube only performed well on cubes close to the solutions, but not on more complicated cubes. If N was too high (e.g. 100), then it took more iterations to learn; nonetheless, the agent would still learn. We found that N=30 resulted in both good value function estimation as well as reasonable training time.\n\n[1] Florensa, C., Held, D., Wulfmeier, M., Zhang, M., & Abbeel, P. (2017). Reverse curriculum generation for reinforcement learning. arXiv preprint arXiv:1707.05300.\n", "title": "Response to Reviewer 2"}, "Skg0UYN5Cm": {"type": "rebuttal", "replyto": "Bye2VTcq27", "comment": "We would like to thank the reviewer for their helpful comments.\n\n> \u201cI am not very clear how to assign the rewards based on the stored states?\u201d\n\nThe environment returns a reward of +1 for the solved state and a reward of -1 for all other states. From this single positive reward given at the solved state, DeepCube learns a value function. Using dynamic programming, DeepCube improves its value estimate by first learning the value of states one move away from the solution and then building off of this knowledge to improve its value estimate for states that get progressively further away from the solution.\n\n> \u201cDo you have solving time comparison between your method and other approximate methods?\u201d\n\nYes, we have improved the efficiency of our solver since we last submitted our paper by optimizing our code. Our method takes, on average, 40 seconds; whereas the fastest optimal solver we could find (implemented by Tomas Rokicki to find \u201cGod\u2019s number\u201d [1]) for the Rubik\u2019s Cube takes 2.7 seconds. These results are summarized in section C of the appendix of the updated paper. While Rokicki\u2019s algorithm is faster, Rokicki\u2019s algorithm also uses knowledge of groups, subgroups, cosets, symmetry, and pattern databases. On the other hand, our algorithm does not exploit any of this knowledge and learns how to solve the Rubik\u2019s Cube given only basic information about the problem. In addition, Rokicki\u2019s solver uses 182GB of memory to run whereas ours uses at most 1GB. These differences are summarized in the updated paper. We are currently making better use of parallel processing and memory to improve the speed of our algorithm.\n\n[1] Rokicki, T., Kociemba, H., Davidson, M., & Dethridge, J. (2014). The diameter of the Rubik's Cube group is twenty. SIAM Review, 56(4), 645-670.\n\n", "title": "Response to Reviewer 1"}, "Bye2VTcq27": {"type": "review", "replyto": "Hyfn2jCcKm", "review": "The authors provide a good idea to solve Rubik\u2019s Cube using an approximate policy iteration method, which they call it as Autodidactic iteration. The method overcomes the problem of sparse rewards by creating its own rewards system. Autodidactic iteration starts with solved cube and then propagate backwards to the state. \n\nThe testing results are very impressive. Their algorithm solves 100% of randomly scrambled(1000 times) cubes and has a median solve length of 30 moves. The God\u2019s number is 26 in the quarter turn metric, while their median moves 30 is only 4 hands away from the God\u2019s number. I appreciate the non-human domain knowledge part most because a more general algorithm can be used to other area without  enough pre-knowledges. \n\nThe training conception to design rewards by starting from solved state to expanded status is smart, but I am not very clear how to assign the rewards based on the stored states? Only pure reinforcement learning method applied sounds simple, but performance is great. The results are good enough with the neural network none-random search guidance. Do you have solving time comparison  between your method and other approximate methods? \n\nPros: -  solved nearly 100% problems with reasonable  moves.\n          -  a more general algorithm solving unknown states value problems.\n\nCons: - the Rubik\u2019s cube problem has been solved with other optimal approaches in the past. This method is not as competitive as other optimal solution solver within similar running time for this particular game.\n           - to solve more dimension cubes, this method might be out of time.  \n", "title": "A good paper ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJlaAp7wn7": {"type": "review", "replyto": "Hyfn2jCcKm", "review": "This paper introduces a deep RL algorithm to solve the Rubik's cube. The particularity of this algorithm is to handle the huge state space and very sparse reward of the Rubik's cube. To do so, a) it ensures each training batch contains states close to the reward by scrambling the solution; b) it computes an approximate value and policy for that state using the current model and c) it weights data points based by the inverse of the number of random moves from the solution used to generate that training point. The resulting model is compared to two non-ML algorithms and shown to be competitive either on computational speed or on the quality of the solution.  \n\nThis paper is well written and clear. To the best of my knowledge, this is the first RL-based approach to handle the Rubik's cube problem so well. The specificities of this problem make it interesting. While the idea of starting from the solution seemed straightforward at first, the paper describes more advanced tricks claimed to be necessary to make the algorithm work. The algorithm seems to be quite successful and competitive with expert algorithms, which I find very nice. Overall, I found the proposed approach interesting and sparsity of reward is an important problem so I would rather be in favor of accepting this paper. \n\nOn the negative side, I am slightly disappointed that the paper does not link to a repository with the code. Is this something the authors are considering in the future? While it does not seem difficult to code, it is still nice to have the experimental setup.\n\nThere has been (unsuccessful) attempts to solve the Rubik's cube using deep RL before. I found some of them here: https://github.com/jasonrute/puzzle_cube . I am not sure whether these can be considered prior art as I could not find associated accepted papers but some are quite detailed. Some could also provide additional baselines for the proposed methods and highlight the challenges of the Rubik's cube.\n\nI am also curious whether/how redundant positions are handled by the proposed approach and wished this would be discussed a bit. Considering the nature of the state space and the dynamics, I would have expected this to be a significant problem, unlike in Go or chess. Does the algorithm forbid the reverse of the last action? Is the learned value/policy function good enough that backwards moves are seldom explored? Since the paper mention that BFS is interesting to remove cycles, I assume identical states are not duplicated. Is this correct?", "title": "interesting deep-RL tweaks to solve problem with sparse reward", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}