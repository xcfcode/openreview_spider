{"paper": {"title": "Machine Solver for Physics Word Problems", "authors": ["Megan Leszczynski", "Jose Moreira"], "authorids": ["mel255@cornell.edu", "jmoreira@us.ibm.com"], "summary": "We build an automated solver for a class of physics word problems, using a combination of neural networks and a numerical integrator.", "abstract": "We build a machine solver for word problems on the physics of a free\nfalling object under constant acceleration of gravity.  Each problem\nconsists of a formulation part, describing the setting, and a question\npart asking for the value of an unknown.  Our solver consists of\ntwo long short-term memory recurrent neural networks and a numerical\nintegrator. The first neural network (the labeler) labels each\nword of the problem, identifying the physical parameters and the\nquestion part of the problem. The second neural network (the \nclassifier) identifies what is being asked in the question. Using\nthe information extracted by both networks, the numerical integrator\ncomputes the solution.  We observe that the classifier is resilient\nto errors made by the labeler, which does a better job of identifying\nthe physics parameters than the question. Training, validation and test\nsets of problems are generated from a grammar, with validation and test\nproblems structurally different from the training problems. The overall\naccuracy of the solver on the test cases is 99.8%.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper explores a model for solving simple physics problems (posed in automatically generated natural language). Whilst this an interesting problem, the reviewers worry that the problem is too simple because all problems are automatically generated. The paper should at least incorporate some reasonable baseline models and/or apply the proposed methodology on real physics problems."}, "review": {"ry6ROpULg": {"type": "rebuttal", "replyto": "Sk0Aqs-Vg", "comment": "Thank you for the useful feedback! We are glad you enjoyed reading the paper! Although our problem generator was based off of physics problems extracted from textbooks, word problems from AMT workers would likely introduce more complexity into our problems. This may also help us better identify the difficulty of the task, particularly if these problems were incorporated into our test sets. Thank you for the suggestion! \n\nAs of now we use two neural networks to be able to perform multiple levels of reasoning as required by the word problem, but the reviewer is correct in that some problems may require more levels of reasoning than seen in these problems. While this is true, a question answering system that can solve problems with only several levels of reasoning would still be highly useful for solving simple problems for K-12 students. Finally, while it would be an interesting problem to replace the numerical integrator, we chose to solve the first part of the challenge (to understand the word problem), as there are not established existing tools to do this, but there are many existing numerical integration tools for computers. ", "title": "Re: An interesting paper to read but could be made better"}, "BJEYOT88l": {"type": "rebuttal", "replyto": "BJDe0lzNl", "comment": "Thank you for the valuable feedback! \n\nThe motivation for the system is to explore new approaches to developing question answering systems. One use case of a question answering system that could solve word problems would be a personal educational assistant. Our overall goal is to develop a way for computers to \"understand\" word problems. We chose the domain of physics to constrain the task as a starting point for this challenge, as the neural network outputs could be readily mapped to a computer-solvable form (dynamic system) for numerical integration. \n\nWe drew inspiration from previous work on natural language processing, such as named entity recognition, in order to apply the tasks to solving physics word problems. The reviewer is absolutely correct that two tasks must occur here: the sequence needs to be labeled and the question classified. We wanted to see if previous successes of neural networks on these tasks could be applied specifically to the problem of reasoning about word problems.  \n\nThank you for the suggestions of further architectural modifications that we can incorporate into future work!", "title": "Re: Rich data generation procedure but system specific and not well motivated"}, "Sk_AR4eEg": {"type": "rebuttal", "replyto": "Hys6iL27x", "comment": "We want to thank the reviewer for the important feedback. We are in the process of assembling a collection of real world problems from published tests and hope to include results from running them through our system in future work.\n\nAs for the fact that we use 10 hidden units in the labeler LSTM, that number was chose after experimenting with different sizes using a grid search. Some of the experiments are reported in our Appendix B.", "title": "Re: A nice approach to this problem, but inputs seem too artificial"}, "rk5apNgNe": {"type": "rebuttal", "replyto": "HyxDQf17e", "comment": "We have uploaded a new version of the paper with measurements of frequency of types of questions.", "title": "Re: How the dataset is generated?"}, "Hkd75bUXl": {"type": "rebuttal", "replyto": "HyxDQf17e", "comment": "Thanks for the comments! \n\n(1) To generate the problems, we build the grammar with Python methods. Each of the nonterminals of the grammar maps to a method where the options for the nonterminals are stored in arrays. The generator begins by building up the formulation and question parts of the problem. First the object is randomly chosen by generating a random number to index into the list of possible objects. Then for the formulation, the action is randomly chosen from an array of possible actions and the assumption is randomly chosen from an array of possible assumptions. We continue this process until we have selected each of the terminals, and the full problem is composed and returned. We will update our paper to show the generation of an example problem. To check for bias, we are currently performing the analysis of the distributions of the types of problems and will update the paper shortly. \n\n(2) We agree that it would be very interesting to replace the numerical integrator by a DNN. We chose to focus on mapping the word problems to a computer solvable format because there are already defined numerical integration tools, but there are not many existing tools for computers to map word problems to a computer-interpretable form. Automating the entire flow and replacing the numerical integrator with DNNs would be interesting to explore in order to test the learning capability of DNNs and remove any manual procedure, and we will add this to our future work section. We appreciate the suggestion!", "title": "Re: How the dataset is generated? "}, "Byl-rCJXe": {"type": "review", "replyto": "HyFkG45gl", "review": "Have the authors tried using a single LSTM to perform both labeling and classification? Also, is the model they use bidirectional?\n\nFor the analysis section, did the authors perform any analysis to determine the importance of each input word for classification?The authors describe a system for solving physics word problems. The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator. On the dataset that the authors synthesize, the full system attains near full performance. Outside of the pipeline, the authors also provide some network activation visualizations.\n\nThe paper is clear, and the data generation procedure/grammar is rich and interesting. However, overall the system is not well motivated. Why did they consider this particular problem domain, and what challenges did they specifically hope to address? Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question? This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks. A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.", "title": "Comparison w/ single network + attention", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJDe0lzNl": {"type": "review", "replyto": "HyFkG45gl", "review": "Have the authors tried using a single LSTM to perform both labeling and classification? Also, is the model they use bidirectional?\n\nFor the analysis section, did the authors perform any analysis to determine the importance of each input word for classification?The authors describe a system for solving physics word problems. The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator. On the dataset that the authors synthesize, the full system attains near full performance. Outside of the pipeline, the authors also provide some network activation visualizations.\n\nThe paper is clear, and the data generation procedure/grammar is rich and interesting. However, overall the system is not well motivated. Why did they consider this particular problem domain, and what challenges did they specifically hope to address? Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question? This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks. A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.", "title": "Comparison w/ single network + attention", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyxDQf17e": {"type": "review", "replyto": "HyFkG45gl", "review": "Could the authors elaborate on how the dataset is generated? Although the grammar is given in appendix, the generation process is not mentioned in details. I wonder whether there is any bias in training/val/test. Authors could show distributions of different types of questions, etc. \n\nBTW, it seems that the problem will be more interesting if the numerical integrator is replaced by a DNN. Currently the core of reasoning is still done by manual designed procedure. I wonder whether the authors have tried on this direction. \nThis paper build a language-based solver for simple physics problems (a free falling object under constant velocity). Given a natural language query sampled from a fixed grammar, the system uses two LSTM models to extract key components, e.g., physical parameters and the type of questions being asked, which are then sent to a numerical integrator for the answer. The overall performance in the test set is almost perfect (99.8%).\n\nOverall I found this paper quite interesting to read (and it is well written). However, it is not clear how hard the problem is and how much this approach could generalize over more realistic (and complicated) situations. The dataset are a bit small and might not cover the query space. It might be better to ask AMT workers to come up with more complicated queries/answers. The physics itself is also quite easy. What happens if we apply the same idea on billiards? In this case, even we have a perfect physics simulator, the question to be asked could be very deep and requires multi-hop reasoning.\n\nFinally, given the same problem setting (physics solver), in my opinion, a more interesting direction is to study how DNN can take the place of numerical integrator and gives rough answers to the question (i.e., intuitive physics). It is a bit disappointing to see that DNN is only used to extract the parameters while still a traditional approach is used for core reasoning part. It would be more interesting to see the other way round.\n", "title": "How the dataset is generated?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Sk0Aqs-Vg": {"type": "review", "replyto": "HyFkG45gl", "review": "Could the authors elaborate on how the dataset is generated? Although the grammar is given in appendix, the generation process is not mentioned in details. I wonder whether there is any bias in training/val/test. Authors could show distributions of different types of questions, etc. \n\nBTW, it seems that the problem will be more interesting if the numerical integrator is replaced by a DNN. Currently the core of reasoning is still done by manual designed procedure. I wonder whether the authors have tried on this direction. \nThis paper build a language-based solver for simple physics problems (a free falling object under constant velocity). Given a natural language query sampled from a fixed grammar, the system uses two LSTM models to extract key components, e.g., physical parameters and the type of questions being asked, which are then sent to a numerical integrator for the answer. The overall performance in the test set is almost perfect (99.8%).\n\nOverall I found this paper quite interesting to read (and it is well written). However, it is not clear how hard the problem is and how much this approach could generalize over more realistic (and complicated) situations. The dataset are a bit small and might not cover the query space. It might be better to ask AMT workers to come up with more complicated queries/answers. The physics itself is also quite easy. What happens if we apply the same idea on billiards? In this case, even we have a perfect physics simulator, the question to be asked could be very deep and requires multi-hop reasoning.\n\nFinally, given the same problem setting (physics solver), in my opinion, a more interesting direction is to study how DNN can take the place of numerical integrator and gives rough answers to the question (i.e., intuitive physics). It is a bit disappointing to see that DNN is only used to extract the parameters while still a traditional approach is used for core reasoning part. It would be more interesting to see the other way round.\n", "title": "How the dataset is generated?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkjeL6tGl": {"type": "review", "replyto": "HyFkG45gl", "review": "From the text, it's hard to understand how hard the task is. \n\nHere are a few ways to answer this:\n- Compare the perplexity of the grammar to real world questions of the same nature.\n- Run the system on real examples.\n- Compare to previous method, or to some simple baseline. The paper uses neural networks to answer falling body physics questions by 1. Resolving the parameters of the problem, and 2. Figure out which quantity is in question, compute it using a numerical integrator and return it as an answer.\nLearning and inference are performed on artificially generated questions using a probabilistic grammar.\nOverall, the paper is clearly written and seems to be novel in its approach.\n\nThe main problems I see with this work are:\n1. The task is artificial, and it's not clear how hard it is. The authors provide no baseline nor do they compare it to any real world problem. Without some measure of difficulty it's hard to tell if a much simple approach will do better, or if the task even makes sense.\n2. The labler LSTM uses only 10 hidden units. This is remarkably small for language modeling problems, and makes one further wonder about the difficulty of the task. The authors provide no reasoning for this choice.\n", "title": "How hard is the task?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hys6iL27x": {"type": "review", "replyto": "HyFkG45gl", "review": "From the text, it's hard to understand how hard the task is. \n\nHere are a few ways to answer this:\n- Compare the perplexity of the grammar to real world questions of the same nature.\n- Run the system on real examples.\n- Compare to previous method, or to some simple baseline. The paper uses neural networks to answer falling body physics questions by 1. Resolving the parameters of the problem, and 2. Figure out which quantity is in question, compute it using a numerical integrator and return it as an answer.\nLearning and inference are performed on artificially generated questions using a probabilistic grammar.\nOverall, the paper is clearly written and seems to be novel in its approach.\n\nThe main problems I see with this work are:\n1. The task is artificial, and it's not clear how hard it is. The authors provide no baseline nor do they compare it to any real world problem. Without some measure of difficulty it's hard to tell if a much simple approach will do better, or if the task even makes sense.\n2. The labler LSTM uses only 10 hidden units. This is remarkably small for language modeling problems, and makes one further wonder about the difficulty of the task. The authors provide no reasoning for this choice.\n", "title": "How hard is the task?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}