{"paper": {"title": "Matrix capsules with EM routing", "authors": ["Geoffrey E Hinton", "Sara Sabour", "Nicholas Frosst"], "authorids": ["geoffhinton@google.com", "sasabour@google.com", "frosst@google.com"], "summary": "Capsule networks with learned pose matrices and EM routing improves state of the art classification on smallNORB, improves generalizability to new view points, and white box adversarial robustness.  ", "abstract": "A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network.", "keywords": ["Computer Vision", "Deep Learning", "Dynamic routing"]}, "meta": {"decision": "Accept (Poster)", "comment": "Authors present a new multi-layered capsule network architecture, implemented an EM routing procedure, and introduced \"Coordinate Addition\".  Capsule architectures are gaining interest because of their ability to achieve equivariance of parts, and employ a new form of pooling called \"routing\" (as opposed to max pooling) which groups parts that make similar predictions of the whole to which they belong, rather than relying on spatial co-locality. New state-of-art performances are being achieved on focused datasets, for which the authors have continued the trend.\n\nPros:\n- New significant improvement to state-of-art performance is obtained on smallNORB, both in comparison to CNN structure as well as the most recent previous implementation of capsule network.\n\nCons:\n- Some concern arose regarding the writing of the paper and the ability to understand the material, which authors have made an effort to address.\n\nGiven the general consensus of the reviewers that this work should be accepted, the general applicability of the technology to multiple domains, and the potential impact that improvements to capsule networks may have on an early field, area chair recommends this work be accepted as a poster presentation. "}, "review": {"Sylh3Mo6EB": {"type": "rebuttal", "replyto": "Skxfz3UaNS", "comment": "Thank you for the implementation and enlightening the challenges. We are looking into it. \n\nWe open sourced our code on January here: \nhttps://github.com/google-research/google-research/commits/master/capsule_em \nwhich provides the checkpoints  for 1.3% test error too. ", "title": "Original implementation"}, "Hyg2bBrLnm": {"type": "rebuttal", "replyto": "S1eo2P1I3Q", "comment": "The gradient flows through EM algorithm. We do not use stop gradient. A routing of 3 is like a 3 layer network where the weights of layers are shared.", "title": "BP through EM"}, "rJgxonoNnm": {"type": "rebuttal", "replyto": "rkgHuO67hQ", "comment": "We use Adam optimizer with default Tensorflow parameters.\n\nWe did not have any special ordering of training batches and we random shuffle. In terms of TF batch:\ncapacity=2000 + 3 * batch_size,\n# Ensures a minimum amount of shuffling of examples.\nmin_after_dequeue=2000.\n\nPlease not that for smallnorb viewpoint generalization test to make sure that the model only sees a fraction of certain directions and the generalization test is strict, we calibrated the semantic of azimuth '0' for every class so that the object at azimuth '0' roughly heads to the right. Therefore, in training the model never sees an object which heads to the left (from any class) while in test it is tested on objects which head to the left as well.\n\nThe gradient indeed is tricky. Almost all the math is done in log-scale to avoid numerical issues. We used truncated_normal_initializer and set the std so that at the start of training half of the capsules in each layer are active and half inactive (for the Primary Capsule layer where the activation is not computed through routing we use different std for activation convolution weights & for pose parameter convolution weights).\n\nRecently we are using a new initialization method: every 4x4 is initialized with I + noise of 0.03: (1 on the diag, random uniform noise in the range +/- 0.03 everywhere else). This new method is more scale able and easier to train. ", "title": "Optimizer & initializer"}, "HJeJrLo4hQ": {"type": "rebuttal", "replyto": "SkxQs3omhm", "comment": "We used 8 sync gpus (batch of 64, 8 on each gpu) to train for ~ a day on small norb, ~10hr on MNIST and ~ 2 day on Cifar10.", "title": "Hardware"}, "rJeQnSsE3X": {"type": "rebuttal", "replyto": "ryxTPFDe2X", "comment": "We use a weight decay loss with a small factor of .0000002 rather than the reconstruction loss.\nWe use an exponential decay with learning rate: 3e-3, decay_steps: 20000, decay rate: 0.96.", "title": "regularizer & learning rate"}, "BkelcSxC47": {"type": "rebuttal", "replyto": "Bkl9bvlpmm", "comment": "the formula we used for lambda is:\nlambda = final_lambda * (1 - tf.pow(0.95, tf.cast(i + 1, tf.float32)))\nwhere 'i' is the routing iteration (range is 0-2). Final_lambda is set to 0.01.\n\nThe margin that we set is: \nmargin = 0.2 + .79 * tf.sigmoid(tf.minimum(10.0, step / 50000.0 - 4))\nwhere step is the training step. We trained with batch size of 64.\n", "title": "Lambda and margin"}, "SkenLVlAEm": {"type": "rebuttal", "replyto": "BJgX7Iy04m", "comment": "The first option is like having 1x1 convolution layers. The second option is what happens if you have kernel size larger than one. Since we have 3x3 convolution capsule layers (32 capsule types each) it means that 9x32 capsules receive the vote of a single capsule in layer bellow. Therefore these 9x32 capsules are competing for its vote (normalize the routing factors over the feedback of these 3x3x32 capsules).  ", "title": "convolution capsule layer"}, "HyvJKULxM": {"type": "rebuttal", "replyto": "ByAqs7VJf", "comment": "The objective function in details is:\n\\sum_c a'_c (-\\beta_a) + a'_c ln(a'_c) + (1-a'_c)ln(1-a'_c)+\\sum_h cost_{ch} + \\sum_i a_i *  r_{ic} * ln(r_{ic})\n\na'_c is the activation for capsule c in layer L+1 and a_i is the activation probability for capsule i in layer L. The rest of the notations follow paper. \n\nPlots showing the decay of objective function and the absolute difference between two routing iterations in the above objective function can be found at:\nhttps://imgur.com/a/eeD2X", "title": "Re: The objective function"}, "Hykw8iKxG": {"type": "review", "replyto": "HJWLfGWRb", "review": "The paper proposes a novel architecture for capsule networks. Each capsule has a logistic unit representing the presence of an entity plus a 4x4 pose matrix representing the entity/viewer relationship. This new representation comes with a novel iterative routing scheme, based on the EM algorithm.\nEvaluated on the SmallNORB dataset, the approach proves to be more accurate than previous work (beating also the recently proposed \"routing-by-agreement\" approach for capsule networks by Sabour et al.). It also generalizes well to new, unseen viewpoints and proves to be more robust to adversarial examples than traditional CNNs.\n\nCapsule networks have recently gained attention from the community. The paper addresses important shortcomings exhibited by previous work (Sabour et al.), introducing a series of valuable technical novelties.\nThere are, however, some weaknesses. The proposed routing scheme is quite complex (involving an EM-based step at each layer); it's not fully clear how efficiently it can be performed / how scalable it is. Evaluation is performed on a small dataset for shape recognition; as noted in Sec. 6, the approach will need to be tested on larger, more challenging datasets. Clarity could be improved in some parts of the paper (e.g.: Sec. 1.1 may not be fully clear if the reader is not already familiar with (Sabour et al., 2017); the authors could give a better intuition about what is kept and what is discarded, and why, from that approach. Sec. 2: the sentence \"this is incorrect because the transformation matrix...\" could be elaborated more. V_{ih} in eq. 1 is defined only a few lines below; perhaps, defining the variables before the equations could improve clarity. Sec. 2.1 could be accompanied by mathematical formulation).\nAll in all, the paper brings an original contribution and will encourage further research / discussion on an important research question (how to effectively leverage knowledge about the part-whole relationships).\n\nOther notes:\n- There are a few typos (e.g. Sec. 1.2 \"(Jaderberg et al. (2015)\",  Sec. 2 \"the the transformation\", Sec. 4 \"cetral crop\" etc.).\n- The authors could discuss in more detail why the approach does not show significant improvement on NORB with respect to the state of the art.\n- The authors could provide more insights about why capsule gradients are smaller than CNN ones.\n- It would be interesting to discuss how the network could potentially be adapted, in the future, to: 1. be more efficient 2. take into account other changes produced by viewpoint changes (pixel intensities, as noted in Sec. 1).\n- In Sec, 4, the authors could provide more details about the network training.\n- In Procedure 1, for indexing tensors and matrices it might be better to use a comma to separate dimensions (e.g. V_{:,c,:} instead of V_{:c:}).", "title": "A novel approach for capsule networks", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ry1nhoKgM": {"type": "review", "replyto": "HJWLfGWRb", "review": "This paper proposes a new kind of capsules for CNN. The capsule contains a 4x4 pose matrix motivated by 3D geometric transformations describing the relationship between the viewer and the object (parts). An EM-type of algorithm is used to compute the routing.\n\nThe authors use the smallNORB dataset as an example. Since the scenes are simulated from different viewer angles, the pose matrix quite fits the motivation. It would be more beneficial to know if this kind of capsules is limited to the motivation or is general. For example, the authors may consider reporting the results of the affNIST dataset where the digits undergo 2D affine transformations (in which case perhaps 3x3 pose matrices are enough?).\n\nMinor: The arguments in line 5 of the procedure RM Routing(a,V) do not match those in line 1 of the procedure E-Step.\n\nSection 2.1 (objective of EM) is unclear. The authors may want to explicitly write down the free energy function.\n\nThe section about robustness against adversarial attacks is interesting.\n\nOverall the idea appears to be useful but needs more empirical validation (affNIST, ImageNet, etc).\n", "title": "Idea is interesting; need more empirical validation than smallNORB", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ByZRu4ClG": {"type": "review", "replyto": "HJWLfGWRb", "review": "The paper describes another instantiation of \"capsules\" which attempt to learn part-whole relationships and the geometric pose transformations between them.  Results are presented on the smallNORB test set obtaining impressive performance.\n\nAlthough I like very much this overall approach, this particular paper is so opaquely written that it is difficult to understand exactly what was done and how the network works.  It sounds like the main innovation here is using a 4x4 matrix for the pose parameters, and an iterative EM algorithm to find the correspondence between capsules (routing by agreement).  But what exactly the pose matrix represents, and how they get transformed from one layer to the next, is left almost entirely to the reader's imagination.  In addition, how EM factors in, what the probabilities P_ih represent, etc. is not clear.  I think the authors could do a much better job explaining this model, the rationale behind it, and how it works.\n\nPerhaps the most interesting and compelling result is Figure 2, which shows how ambiguity in object class assignment is resolved with each iteration.  This is very intriguing, but it would be great to understand what is going on and how this is happening.\n\nAlthough the results are impressive, if one can't understand how this was achieved it is hard to know what to make of it.\n\n", "title": "An extremely opaque paper with a potentially interesting idea and good results", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "HyguZD-Vf": {"type": "rebuttal", "replyto": "ByZRu4ClG", "comment": "Thank you for your comments. upon reflection we agree that the paper was confusing and we have taken several steps to reduce the opacity of our work to the reader. To that end we have done the following: \n- We have added section 2 which gives a general and intuitive explanation of the mechanism of capsule networks, paying close attention to how pose matrices get transformed from one layer to the next.\n- Having identified the EM objective as another source of confusion, we added an extended appendix in which we provide a gentle and approachable explanation for the free energy view of EM and how our routing algorithm builds upon it. \n- We have also added a paragraph to further explain figure 2 in the experiments section. \n- Finally we have made several changes to the language of the paper, focusing in particular on the notation.  \nWe believe that the comprehensibility of the paper has thus improved and appreciate your criticism. ", "title": "Improvements on the clarity of the paper"}, "SJAWbD-4G": {"type": "rebuttal", "replyto": "Hykw8iKxG", "comment": "Thank you for your detailed reading of the paper and suggestions!    \nAs per your comments on the EM routing, we agree that it was not presented as best it could have been, and have added an appendix to present a gentle and thorough introduction to the free energy view of EM and the objective function which our routing operation minimizes. In response to the question about efficiency, we would like to draw your attention to the total number of arithmetic operations required for the routing procedure - each iteration of routing represents fewer arithmetic operations than a single layer feed forward pass, but due to architectural optimization decisions in tensorflow, our current capsule implementation is not as fast as it could be. \n\nWe agree that larger scale testing would ideal, but due to the aforementioned efficiency limitations were not able to include it in this paper. \n\nIn regards to your other comments we have done the following: \n- To increase the clarity of the paper,  we have made several changes to the language used, and improved the mathematical notation.\n- We have added section 2 which provides an intuitive explanation of capsules and makes clear when the routing occurs. We feel that improves the readers' ability to engage with the rest of the presented content. We also defined the variables and notation used in the rest of the paper more explicitly.  \n- We have expanded on the sentence \"this is incorrect because the transformation matrix...\" you mentioned which is now in the appendix. \n- We have also made several changes to the nation and language throughout the paper to make it more comprehensible. \nthank you for your feedback, and hope that we have addressed your comments to your satisfaction. ", "title": "re: A novel approach for capsule networks"}, "HJkSzIZEf": {"type": "rebuttal", "replyto": "ry1nhoKgM", "comment": "thank you for the feedback! To address your comments we have done the following: \n- To clarify the EM objective we have added an extended and thorough appendix which presents a gentle and intuitive explanation of the free energy view of EM, and explicit free energy function, and how our routing algorithm makes use of it.\n- We believe that the benefit of capsules is not limited to smallNORB and will generalize. As suggested, we replicated the affNIST generalization experiment reported in the previous Capsule paper (Sabour et al. 2017). We found that our EM capsule model (the exact architecture used for smallNORB and MNIST in the paper), when trained to 0.8% test error on expanded MNIST (40x40 pixel MNIST images, created by padding and shifting MNIST), achieved 6.9% test error on affNIST. We trained a baseline CNN (with AlexNet architecture, without pooling) to 0.8% test error and it was only able to achieve 14.1% test error on affNIST. Our capsule model was able to half the test error of a CNN when trained on MNIST and tested on affNIST.  Due to time and space constraints these results are not reported in the paper as it is now. \n- finally we address the minor issue raised in line 5 of the routing procedure. \nwe hope this has addressed your concerns, and thank you for your suggestions. ", "title": "affNIST generalization and EM objective "}, "rJUY2VdbM": {"type": "rebuttal", "replyto": "ryTPZJd-f", "comment": "beta_v and beta_a are per capsule type. Therefore, they are vectors for both convolutional capsules and final capsules. For example in terms of the notation in fig.1 beta_a and beta_v for convCaps1 are C dimensional vectors.\n\nThanks! We will revise the paper in regard to these points.", "title": "re: beta_v and beta_a"}, "r17t2UIgf": {"type": "rebuttal", "replyto": "Hy9EvktkG", "comment": "W_{ic} is 4*4 if you flatten the capsule types and grid positions. Therefore i goes over changes in the range of (1, channels * height * width) in this formulation.\n\nHowever, We share the W_ic between different positions of two capsule types as in a convolutional layer with a kernel size k. Therefore, the total number of trainable parameters between two convolutional capsule layer types is 4*4*k*k and for the whole layer is 4*4*k*k*B*C. Where B is the number of different capsule types in layer bellow and C is the number of different capsule types in the next layer.\n\nPlease note that it is 4*4 rather than (4*4)*(4*4). ", "title": "dimensionality of transformation matrix W_{ic} in ConvCaps"}, "BkFS5LLxf": {"type": "rebuttal", "replyto": "ryM_Fi4JM", "comment": "As Jianfei has explained, the primary capsule layer is a convolutional layer with 1x1 kernel. It transforms the A channels in the first layer to B*(4x4+1) channels. Then we split the B*(4x4+1) channels into B*(4x4) as the pose matrices for B capsules and B*1 as the activation logits of B capsules in primary layer. Then we apply sigmoid nonlinearity on the activation logits.", "title": "How to transform conv layer to the primary capsule layer?"}, "ByVzDRDRW": {"type": "rebuttal", "replyto": "S1uPsnwR-", "comment": "They gain a lot by using the meta data at test time. Without using that information (which normally is not available at test time) they get 2.6%. ", "title": "state-of-the-art on \"small NORB\""}}}