{"paper": {"title": "Identifying and Controlling Important Neurons in Neural Machine Translation", "authors": ["Anthony Bau", "Yonatan Belinkov", "Hassan Sajjad", "Nadir Durrani", "Fahim Dalvi", "James Glass"], "authorids": ["abau@mit.edu", "belinkov@mit.edu", "hsajjad@hbku.edu.qa", "ndurrani@qf.org.qa", "faimaduddin@qf.org.qa", "glass@mit.edu"], "summary": "Unsupervised methods for finding, analyzing, and controlling important neurons in NMT", "abstract": "Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.", "keywords": ["neural machine translation", "individual neurons", "unsupervised", "analysis", "correlation", "translation control", "distributivity", "localization"]}, "meta": {"decision": "Accept (Poster)", "comment": "Strong points:\n\n-- Interesting, fairly systematic and novel analyses of recurrent NMT models, revealing individual neurons responsible for specific type of information (e.g., verb tense or gender)\n\n-- Interesting experiments showing how these neurons can be used to manipulate translations in specific ways (e.g., specifying the gender for a pronoun when the source sentence does not reveal it)\n\n-- The paper is well written\n\nWeak points\n\n-- Nothing serious (e.g., maybe interesting to test across multiple runs how stable these findings are).\n\nThere is a consensus among the reviewers that this is a strong paper and should be accepted.\n\n"}, "review": {"BygWvPj7CQ": {"type": "rebuttal", "replyto": "H1z-PsR5KX", "comment": "We have uploaded a revised version incorporating the reviewers's helpful comments. This is the list of changes:\n\n1. Added appendix A.4 with results with different model checkpoints and a footnote referring to the appendix in Section 4. \n2. Added to the Conclusion a potential future work on controlling translations by modifying the decoder. \n3. Mentioned the tense neuron\u2019s mistake on \"Spreads\" in Section 5.3, tense paragraph. \n4. Slightly modified sections 5.2 and 5.3 to emphasize that position is captured by top neurons in some of the unsupervised methods, but not in all. \n5. Clarified that the supervised experiments are used for verifying interpretations, rather than constraining the analysis to a given property that may not be known a priori (Section 3.2, supervised verification). \n6. Added more details on the GMM setup in Section 3.2, supervised verification. \n7. Improved the motivation for using translation control for mitigating model bias in the beginning of Section 6. \n8. Fixed a number of typos and formatting issues. \n", "title": "Updated version incorporating reviewers' comments"}, "rkeX0uhx0Q": {"type": "rebuttal", "replyto": "r1l4JK8ijQ", "comment": "\n7. \u201cwhat insight was gained from the SVCCA analyses\u201d and \u201cthe pros and cons of each of the methods\u201d \nThank you for the feedback. We will improve the discussion in the next revision accordingly. \nThe different methods aim to analyze representations at different levels of localism/distributivity. In particular, while MaxCorr and MinCorr target pairwise neuron correlations, LinReg searches for information that\u2019s distributed in the whole representation in one network, but localized in a different network. SVCCA tries to find a middle ground by projecting representations to a lower-dimensional space and then computing correlations. \nSome of the insights are discussed in Section 5.2, where we observe that the more distributed methods (LinReg and SVCCA) give much importance to identifying specific tokens. This means that information about token identity is distributed among many neurons. The fact that MinCorr cares a lot about position suggests that this kind of information is captured in multiple models in a similar way. Table 10 in the appendix shows that in addition to detecting specific tokens, SVCCA directions may also capture some classes like adjectives and verbs.", "title": "Response to Reviewer 3: part 2/2"}, "ByeWhune0X": {"type": "rebuttal", "replyto": "r1l4JK8ijQ", "comment": "Thank you for your useful feedback and helpful comments. We are glad that you found our methods promising with good intuitions. We would like to clarify a few points based on your comments.\n\n1. \u201cMost of the activations of the important neurons can be explained using sentence position\u201d \nIt is true that many top ranked neurons capture sentence position, especially in the MinCorr method (Table 1). However, other methods reveal important neurons that do not capture position: only 3/10 top ranked neurons by MaxCorr and LinReg are position neurons, and only one top ranked SVCCA direction captures position. These top neurons often capture linguistic properties like morphological features, punctuations, word classes, etc., as analyzed in Section 5.3 and in appendices A and C. We will make this point clearer in the next revision. \n\n2. \u201cThe methods may be able to address the question of *how* localist the representations are (though no numerical measure of localism is proposed)\u201d\nWe also believe that our methods can shed light on the question of how localist the representations are. We would love to try out any suggestions for numerical measures of localism.  \n\n3.\u201dwhy the neurons that track particular properties couldn't be identified using a supervised classifier to begin with\u201d and \u201cmost of the insight in the paper seems to be derived from supervised experiments\u201d\nThe advantage of unsupervised methods is that they are not constrained by available supervision in the form of linguistic annotations. Our working process involved visualizations of the important neurons, which led to forming hypotheses about their function. In order to validate our interpretations, we designed supervised experiments whenever we could. While this may give the impression that most insights are derived from the supervised experiments, in practice it would have been difficult to choose specific properties to target without the unsupervised methods + visualizations. \nIn addition, we have found properties that do not correspond to plausible a priori hypotheses. The neuron detecting item numbers which you mention is one such case. We also found a neuron that activates positively on the first word in a noun phrase and negatively in the rest of the phrase (Figure 5). Other properties that may not be expected to emerge include year and month neurons (Figure 6), a neuron that activates on verbs and their surrounding words (Table 7), and neurons that capture both punctuation and conjunctions (Tables 6+7; note that this would not be captured by standard part-of-speech tag sets). \nWe will improve our presentation according to your feedback. \n\n4. \u201care there neurons that track syntactic dependencies, for example?\u201d \nIn this work we focused mainly on word-level properties. However, we did investigate parentheses, which require longer-range context, and also noun phrases. Still, we agree that it would be interesting to consider more compositional properties such as dependencies or phrase structure. \n\n5. \u201chow the GMMs \u2026 were set up\u201d \nThe GMMs were set up to predict a property from a neuron activation. The number of mixture was chosen as the number of different classes in the prediction task. For instance, for finding parenthesis neurons we used two classes (inside or outside of parentheses/quotes/brackets). We estimated the parameters of the GMMs using the mean and variance of the neuron activation conditioned on each class. We tested the resulting model to see how well it predicts the tag from the neuron activation by computing the posterior probability of each class given an activation using Bayes' rule, and taking the argmax. We will provide more details on the GMM in the next revision. \n\n6. \u201cargument that this technique could be used to reduce gender bias in MT\u201d \nOur reasoning in the control experiments is that some information about sensitive attributes like gender may be available from other sources, such as metadata. If we know that an entity has a specific gender (say, feminine), but that gender is unmarked in the English language (as in the word \u201cdoctor\u201d), then we may encourage the system to output a translation with the correct gender by modifying gender neurons. This is a kind of soft constraint that we may add to the system. We will improve the motivation in the next revision. ", "title": "Response to Reviewer 3: part 1/2"}, "BklYGw3lRQ": {"type": "rebuttal", "replyto": "rklAXqtrhX", "comment": "Thank you for your very positive review. We are glad that you found the choice of methods justified, and the experiments and analysis thorough and well executed. ", "title": "Response to Reviewer 1"}, "B1gWh83lCQ": {"type": "rebuttal", "replyto": "Byl0raGq2X", "comment": "Thank you for your positive and constructive feedback. We are happy that you find our analysis interesting and valuable for understanding the behavior of neural MT models. We answer specific comments below. \n\n1.  \u201cControlling neurons in the decoder\u201d\nWe are also interested in expanding the controlling experiments, both to other properties and to the decoder side, and intend to do so in the future. We will mention this as potential future work. \n\n2. \u201cuse different checkpoints from a single model\u201d \nThank you for bringing this point up. We have compared all checkpoints from a couple of our models and found highly correlated neurons, especially when correlating later checkpoints. This makes sense as the model converges to a solution. We verified that these top correlated neurons are important for the model performance via an erasure experiment similar to Section 5.1. \nMoreover, the top ranked neurons when comparing the last checkpoint to earlier ones are very similar to the ones found when comparing this last checkpoint to different models, including models trained with different target languages. In particular, for the English-Spanish model, we found that 8 out of 10 and 34 out of 50 top ranked neurons are the same in these two rankings. For the English-Arabic model, we found a similar behavior (7 out of 10 and 33 out of 50 top ranked neurons are the same). This indicates that our method may be applied to different checkpoints as well. We will add these results in the next revision. \n\n3. \u201cThe findings in this paper do not lead to immediate translation performance improvements\u201d \nThis is correct. Beyond the scientific value in illuminating how NMT models work, we would also like to mention several potential ideas for improving the systems. First, our experiments for controlling specific characteristics may help mitigate model bias by identifying neurons responsible for sensitive attributes such as gender or politeness. For instance, we might have external knowledge of the gender of person mentioned by an ambiguous profession or title in the source language (e.g., doctor), and may want to encourage the translation to be of the correct gender in the target language (as the Turkish example in Section 6 illustrates). More generally, by identifying neurons that are responsible for common mistakes we may be able to improve the system through similar control experiments. Other directions for improving NMT systems are doing model compression by removing unimportant neurons and guiding neural architecture search by tracking important neurons. \n\n4. \u201cTable 4a, two results\u201d \nThank you. We have fixed this typo.\n\n5. Tense neuron activating on \u201cSpreads\u201d\nIndeed, this is a \u201cmistake\u201d of the neuron. We will mention this. \n\n6. \"Our supervised methods\" \u2192 \"Our unsupervised methods\"\nFixed. Thank you. \n\n7. \u201cCould SVCCA directions be manipulated\u201d\nThis is difficult to do, as it requires changing all the dimensions, rather than a small number of dimensions, and we\u2019ve noticed that modifying more dimensions leads to performance degradation. Moreover, SVCCA directions mostly detect specific tokens rather than a linguistic property (see Table 1) so controlling is not very intuitive in this case. \n\n8. Missing or misplaced parentheses \nWe have fixed those. Thank you. \n", "title": "Response to Reviewer 2"}, "Byl0raGq2X": {"type": "review", "replyto": "H1z-PsR5KX", "review": "This paper presents unsupervised approaches to discover import neurons in\nneural machine translation systems. Some linguistic properties controlled by the\ndiscovered neurons are discussed and analyzed.\n\nStrengths:\n\nThe paper is well-written and provides valuable information to understand the\nbehaviour of neural machine translation models.\n\nThe ability to control characteristics (such as gender) without training\nspecialized models is promising, even if the results are not good enough for\nimmediate use. It would be interesting to see whether controlling neurons\nin the decoder would be more effective.\n\nWeaknesses:\n\nMultiple NMT systems are necessary to discover important neurons. The authors\nmention that it would be possible to use different checkpoints from a single\nmodel, but don't evaluate how well this would work.\n\nThe findings in this paper do not lead to immediate translation performance\nimprovements.\n\nQuestions and other remarks:\n\nIn Table 4a, why are there 2 results for \"-0.25, -0.125, 0\"?\n\nIn section 4.3 (Tense), it may be worthwhile to mention that the neuron is\nhighly activated on the word \"Spreads\", even if it acts as a noun in this\nspecific sentence.\n\nBottom of p. 6: \"Our supervised methods\" -> \"Our unsupervised methods\"\n\nTo control properties, could SVCCA directions or coefficients be manipulated?\n\nSome parentheses around citations are missing or misplaced.\n", "title": "Interesting analysis of the contributions of different neurons in NMT", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rklAXqtrhX": {"type": "review", "replyto": "H1z-PsR5KX", "review": "Strengths:\n- even though the methods for detecting important neurons are not novel (as also stated in the paper), their application to MT is novel\n- the presentation is very clear\n- the choice of methods is well argued and justified\n- the experiments are well executed and analysed\n- thorough and varied analysis of the experimental findings \n\nI recommend this paper for the best paper award.", "title": "This paper presents unsupervised methods for ranking neurons in machine translation. Important neurons are thus identified and used to control the MT output.", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1l4JK8ijQ": {"type": "review", "replyto": "H1z-PsR5KX", "review": "The authors propose a number of methods to identify individual important neurons in a machine translation system. The crucial assumption, drawn from the computer vision literature, is that important neurons are going to be correlated across related models (e.g. models that are trained on different subsets of the data). This hypothesis is validated to some extent: erasing the neurons that scored highly on these measures reduced BLEU score substantially. However, it turns out that most of the activation of the important neurons can be explained using sentence position. Supervised classification experiments on the important neurons revealed neurons that tracked properties such as the span of parentheses or word classes (e.g., auxiliary verbs, plural nouns, etc).\n\nStrengths:\n* The paper is very well written and provides solid intuitions for the methods proposed.\n* The methods seem promising, and the degree of localist representation is striking.\n* The methods may be able to address the question of *how* localist the representations are (though no numerical measure of localism is proposed).\n* There is a correlation between the neuron importance metrics proposed in the paper and the effect on BLEU score of erasing those neurons from the network (of course, it\u2019s not clear what particular linguistic properties are affected by this erasure - the decrease BLEU may reflect inability to track specific word tokens more than any higher-level linguistic property).\n\nWeaknesses:\n* It wasn't clear to me why the neurons that track particular properties (e.g., being inside a parentheses) couldn't be identified using a supervised classifier to begin with, without first identifying \"important\" neurons using the unsupervised methods proposed in the paper. The unsupervised methods do show their strength in the more exploratory visualization-based analyses -- as the authors point out (bottom of p. 6), the neuron that activates on numbers but only at the beginning of the sentence does not correspond to a plausible a-priori hypothesis. Still, most of the insight in the paper seems to be derived from the supervised experiments.\n* The particular linguistic properties that are being investigated in the classification experiments are fairly limited. Are there neurons that track syntactic dependencies, for example?\n* I wasn't sure how the GMMs (Gaussian mixture models) for predicting linguistic properties from neuron activations were set up.\n* It's nice to see that individual neurons function as knobs that can change the gender or tense of the output (with varying accuracy). At the same time, I was unable to follow the authors' argument that this technique could be used to reduce gender bias in MT.\n* I wasn't sure what insight was gained from the SVCCA analyses -- this method seems to be a bit of a distraction given the general focus on localist vs. distributed representation. In general, I didn\u2019t come away with an understanding of the pros and cons of each of the methods.", "title": "Well-written paper applying a method for finding individual influential neurons to MT, but insight is ultimately limited", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}