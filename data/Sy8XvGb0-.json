{"paper": {"title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models", "authors": ["Jesse Engel", "Matthew Hoffman", "Adam Roberts"], "authorids": ["jesseengel@google.com", "mhoffman@google.com", "adarob@google.com"], "summary": "A new approach to conditional generation by constraining the latent space of an unconditional generative model.", "abstract": "Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal \u201crealism\u201d constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function.", "keywords": ["VAE", "GAN", "generative networks", "conditional generation", "latent-variable models"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper clearly surveys a set of methods related to using generative models to produce samples with desired characteristics.  It explores several approaches and extensions to the standard recipe to try to address some weaknesses.  It also demonstrates a wide variety of tasks.  The exposition and figures are well-done."}, "review": {"S1A-vIcgf": {"type": "review", "replyto": "Sy8XvGb0-", "review": "UPDATE: I think the authors' rebuttal and updated draft address my points sufficiently well for me to update my score and align myself with the other reviewers.\n\n-----\n\nORIGINAL REVIEW: The paper proposes a method for learning post-hoc to condition a decoder-based generative model which was trained unconditionally. Starting from a VAE trained with an emphasis on good reconstructions (and at the expense of sample quality, via a small hard-coded standard deviation on the conditional p(x | z)), the authors propose to train two \"critic\" networks on the latent representation:\n\n1. The \"realism\" critic receives either a sample z ~ q(z) (which is implicitly defined as the marginal of q(z | x) over all empirical samples) or a sample z ~ p(z) and must tell them apart.\n2. The \"attribute\" critic receives either a (latent code, attribute) pair from the dataset or a synthetic (latent code, attribute) pair (obtained by passing both the attribute and a prior sample z ~ p(z) through a generator) and must tell them apart.\n\nThe goal is to find a latent code which satisfies both the realism and the attribute-exhibiting criteria, subject to a regularization penalty that encourages it to stay close to its starting point.\n\nIt seems to me that the proposed realism constraint hinges exclusively on the ability to implictly capture the marginal distribution q(z) via a trained discriminator. Because of that, any autoencoder could be used in conjunction with the realism constraint to obtain good-looking samples, including the identity encoder-decoder pair (in which case the problem reduces to generative adversarial training). I fail to see why this observation is VAE-specific. The authors do mention that the VAE semantics allow to provide some weak form of regularization on q(z) during training, but the way in which the choice of decoder standard deviation alters the shape of q(z) is not explained, and there is no justification for choosing one standard deviation value in particular.\n\nWith that in mind, the fact that the generator mapping prior samples to \"realistic\" latent codes works is expected: if the VAE is trained in a way that encourages it to focus almost exclusively on reconstruction, then its prior p(z) and its marginal q(z) have almost nothing to do with each other, and it is more convenient to view the proposed method as a two-step procedure in which an autoencoder is first trained, and an appropriate prior on latent codes is then learned. In other words, the generator represents the true prior by definition.\n\nThe paper is also rather sparse in terms of comparison with existing work. Table 1 does compare with Perarnau et al., but as the caption mentions, the two methods are not directly comparable due to differences in attribute labels.\n\nSome additional comments:\n\n- BiGAN [1] should be cited as concurrent work when citing (Dumoulin et al., 2016).\n- [2] and [3] should be cited as concurrent work when citing (Ulyanov et al., 2016).\n\nOverall, the relative lack of novelty and comparison with previous work make me hesitant to recommend the acceptance of this paper.\n\nReferences:\n\n[1] Donahue, J., Kr\u00e4henb\u00fchl, P., and Darrell, T. (2017). Adversarial feature learning. In Proceedings of the International Conference on Learning Representations.\n[2] Li, C., and Wand, M. (2016). Precomputed real-time texture synthesis with markovian generative adversarial networks. In European Conference on Computer Vision.\n[3] Johnson, J., Alahi, A., and Fei-Fei, L. (2016). Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision.", "title": "Review", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyRPZlYeG": {"type": "review", "replyto": "Sy8XvGb0-", "review": "This paper considers the problem of generating conditional samples from unconditional models, such that one can query the learned model with a particular set of attributes to receive conditional samples.  Key to achieving this is the introduction of a realism constraint that encourages samples to be more realistic without degrading their reconstruction and a critic which identifies regions of the latent space with targeted attributes.   Generating conditional samples then involves finding points in the latent space which satisfy both the realism constraint and the critic.  This is carried out either used gradient-based optimization or using an actor function which tries to amortize this process.\n\nThis paper is clearly on a timely topic and addresses an important problem.  The low-level writing is good and the paper uses figures effectively to explain its points.  The qualitative results presented are compelling and the approaches taken seem reasonable.  On the downside, the quantitative evaluation of method does not seem very thorough and the approach seems quite heuristical at times. Overall though, the paper seems like a solid step in a good direction with some clearly novel ideas.\n\nMy two main criticisms are as follows\n1. The evaluation of the method is generally subjective without clear use of baselines or demonstration of what would do in the absence of this work - it seems like it works, but I feel like I have a very poor grasp of relative gains.  There is little in the way of quantitative results and no indication of timing is given at any point.  Given that the much of the aim of the work is to avoid retraining, I think it is clear to show that the approach can be run sufficiently quickly to justify its approach over naive alternatives.\n\n2. I found the paper rather hard to follow at times, even though the low-level writing is good.  I think a large part of this is my own unfamiliarity with the literature, but I also think that space has been prioritized to showing off the qualitative results at the expense of more careful description of the approach and the evaluation methods.  This is a hard trade-off to juggle but I feel that the balance is not quite right at the moment.  I think this is a paper where it would be reasonable to go over the soft page limit by a page or so to provide more precise descriptions.  Relatedly, I think the authors could do a better job of linking the different components of the paper together as they come across a little disjointed at the moment.", "title": "Seems like a solid paper on a timely topic", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkyYzWcxf": {"type": "review", "replyto": "Sy8XvGb0-", "review": "# Paper overview:\nThis paper presents an analysis of a basket of approaches which together enable one to sample conditionally from a class of \ngenerative models which have been trained to match a joint distribution. Latent space constraints (framed as critics) are learned which confine the generating distribution to lie in a conditional subspace, which when combined with what is termed a 'realism' constraint enables the generation of realistic conditional images from a more-or-less standard VAE trained to match the joint data-distribution.\n\n'Identity preserving' transformations are then introduced within the latent space, which allow the retrospective minimal modification of sample points such that they lie in the conditional set of interest (or not).  Finally, a brief foray into unsupervised techniques for learning these conditional constraints is made, a straightforward extension which I think clouds rather than enlightens the overall exposition.\n\n# Paper discussion:\nI think this is a nicely written paper, which gives a good explanation of the problem and their proposed innovations, however I am curious to see that the more recent \"Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space\" by Nguyen et al. was not cited.  This is an empirically very successful approach for conditional generation at 'test-time'. \n\nOther minor criticisms include:\n* I find the 'realism' constraint a bit weak, but perhaps it is simply a naming issue.  Did you experiment with alternative approaches for encouraging marginal probability mass?\n\n* The regularisation term L_dist, why this and not log(1 + exp(z' - z)) (or many arbitrary others)? \n\n* The claim of identity preservation is (to me) a strong one: it would truly be hard to minimise the trajectory distance wrt. the actual 'identity' of the subject.\n\n* For Figure 6 I would prefer a different colourscheme: the red does not show up well on screen.\n\n* \"Furthermore, CGANs and CVAEs suffer from the same problems of mode-collapse and blurriness as their unconditional cousins\" -> this is debateable, there are many papers which employ various methods to (attempt to) alleviate this issue.\n\n\n# Conclusion:\nI think this is a nice piece of work, if the authors can confirm why \"Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space\" is not placed relative to this work in the paper, I would be happy to see it published.  If stuck for space, I would personally recommend moving the one-shot generation section to the appendix as I do not think it adds a huge amount to the overall exposition.", "title": "Nicely written paper: one key missing reference.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJLvjqOzM": {"type": "rebuttal", "replyto": "S1A-vIcgf", "comment": "Thank you for your time and insight in your review. We've done our best to address your concerns with paper revisions and in the comments below:\n\n\n> \u201dthe way in which the choice of decoder standard deviation alters the shape of q(z) is not explained, and there is no justification for choosing one standard deviation value in particular.\u201d\n\nThis was not made sufficiently clear in the original version. We chose a standard deviation parameter of 0.1 because it maximizes the ELBO. Using ELBO maximization as a hyperparameter selection scheme is a very natural and well-established practice (cf. Bishop's 2006 Pattern Recognition and Machine Learning textbook, for example). We have updated the text to highlight this and added Table 4 to the appendix, which shows the very significant improvement in ELBO from sigma=1.0 to sigma=0.1.\n\n\n> \u201cany autoencoder could be used in conjunction with the realism constraint to obtain good-looking samples\u2026\u201d\n\nThis is true enough, and worth emphasizing\u2014our contributions are not specific to VAEs, but can be used to generate good-looking conditional samples from pretrained classical autoencoders. We have added Figure 15 to the supplement, which explores what happens when the VAE\u2019s sigma parameter goes to 0 (equivalent to a classical autoencoder). We obtain reasonably good conditional samples with high-frequency spatial artifacts.\n\nWe focused on VAEs rather than classical AEs both because they have natural sampling semantics and because they produced slightly better results. We believe this is because the KL divergence term encourages q(z) to fill up as much of the latent space as possible (without sacrificing reconstruction quality). This penalty encourages more of the latent space to map to reasonable-looking images.\n\n\n> \u201c...including the identity encoder-decoder pair (in which case the problem reduces to generative adversarial training).\u201d\n\nThis is an interesting observation, and may be true for the simplest version of our approach (although the identity mapping would stretch the definition of \u201clatent\u201d space). But it breaks down when we regularize the GAN to not move too far from the input z vector, which we found was essential to combat mode collapse and find identity-preserving transformations. In that case, it is essential that Euclidean distance in latent space be more meaningful than distance in pixel space, making the identity \u201cautoencoder\u201d a poor choice.\n\n\n> \u201cthe way in which the choice of decoder standard deviation alters the shape of q(z) is not explained\u201d\n\nSmaller standard deviations will lead to lower-variance posteriors, and therefore a more concentrated q(z). This may not be obvious to all readers, so we updated the text to emphasize it, and added Supplemental Figure 16, which demonstrates the effect experimentally. \n\n\n> \u201cThe paper is also rather sparse in terms of comparison with existing work. Table 1 does compare with Perarnau et al., but as the caption mentions, the two methods are not directly comparable due to differences in attribute labels.\u201d\n\nWe do our best to find work with which to compare, and match experimental conditions, however, there are not well established benchmarks for this type of task. Unfortunately, Perarnau et al. do not list the specific attributes that they selected as most salient, so an exact comparison is not possible. We do our best to match conditions, and provide a list our 10 salient features in supplemental Table 3 for future comparison.\n\n\n> \u201c- BiGAN [1] should be cited as concurrent work when citing (Dumoulin et al., 2016).  [2] and [3] should be cited as concurrent work when citing (Ulyanov et al., 2016).\u201d\n\nThank you for bringing these citations to our attention. They are indeed concurrent work with Dumoulin et al.\u2019s and Ulyanov et al.\u2019s work, and we have cited them as such.\n\n", "title": "Response to AnonReviewer3"}, "Hk-S95OzM": {"type": "rebuttal", "replyto": "HkyYzWcxf", "comment": "Thank you for your review. We've incorporated changes to the paper and respond to your main points below:\n\n\n> \u201cI am curious to see that the more recent \"Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space\" by Nguyen et al. was not cited.  This is an empirically very successful approach for conditional generation at 'test-time'.\u201d \n\n* Thank you for highlighting the paper by Nguyen et al. It is indeed relevant and we have added a citation to the main text. For more context, we highlight several key differences between the papers here below.\n* PPGNs require a high-quality pretrained image-space classifier. This makes them less applicable to domains where very large labeled datasets are unavailable.\n* To generate samples, PPGNs need to apply iterative gradient-based optimization in image space, each step of which requires expensive backpropagation through both a powerful CNN classifier and the generator network. By contrast, our iterative optimization procedure is done entirely in the latent space, which allows our critic networks to be much smaller and dramatically reduces the cost per iteration. Furthermore, our amortized GAN-based sampling approach can generate samples with no iterative optimization at all.\n* PPGNs must backprop through the full generative process, which limits their ability to use non-differentiable generator networks such as the autoregressive VAE we discuss in section 6. Our approach easily handles this non-differentiability, because it operates entirely in the continuous latent space.\n* Finally, we feel that our approach is simpler than PPGNs, in which three DAEs are trained to minimize a stochastic four-term loss.\n\n\n> \u201cI find the 'realism' constraint a bit weak, but perhaps it is simply a naming issue.  Did you experiment with alternative approaches for encouraging marginal probability mass?\u201d\n\nWe considered the name \u201cmarginal posterior constraint\u201d which was more specific, but less concise. Since the marginal posterior, q(z), corresponds to real datapoints, we consider \u201crealism\u201d to be a fair name for an implicit constraint that makes samples more similar to q(z).\n\nAs we note in the future work section, there are other ways to constrain sampling to the marginal posterior, such as learning an explicit autoregressive density model (indeed, van den Oord et al. proposed just such an approach in their very recent paper \u201cNeural Discrete Representation Learning\u201d, although they argued that using discrete latent variables was essential to their success). We feel that combining these approaches is an interesting avenue to consider, but for simplicity we focused on implicit constraints.\n\n\n> \u201cThe regularisation term L_dist, why this and not log(1 + exp(z' - z)) (or many arbitrary others)?\u201d \n\nThe form of L_dist was inspired by the log-density of a student-t distribution, which we chose because it penalizes outliers less than the more obvious MSE regularizer (we found MSE regularization to be quite sensitive to the hyperparameter lambda_dist). There are indeed a number of other similarly heavy-tailed functions that could work; we did not experiment extensively with these, but a better choice may well exist.\n\n\n> \u201cThe claim of identity preservation is (to me) a strong one: it would truly be hard to minimise the trajectory distance wrt. the actual 'identity' of the subject.\u201d\n\nIndeed, without conditioning on explicit labels it is hard to rigorously define identity preservation, let alone enforce it. We use the phrase \u201cidentity preserving\u201d to emphasize that we are trying to match not only attributes, but also whatever other latent structure was discovered by the unconditional generative model. Empirically, we feel this approach produces results that, while not perfect, match intuitive notions of identity preservation much better than models that only attempt to match attributes.\n\n\n> \u201cFor Figure 6 I would prefer a different colourscheme: the red does not show up well on screen.\u201d\n\nPoint well taken. We\u2019ve changed to grey so that it looks like an extension of the black keys on the piano and contrasts more with the red notes which are out of the key of C.", "title": "Response to AnonReviewer1"}, "Sy5Yu9_Mz": {"type": "rebuttal", "replyto": "HyRPZlYeG", "comment": "Thank you for your time and expertise in your review, we've addressed the key points below:\n\n> \u201cGiven that the much of the aim of the work is to avoid retraining, I think it is clear to show that the approach can be run sufficiently quickly to justify its approach over naive alternatives.\u201d\n\nThank you for highlighting that computational efficiency is indeed one of the strengths of this approach. Retraining the whole VAE can be shortcut by training a much smaller and less expensive actor-critic pair on user preferences. While our initial experiments did not focus on computational efficiency, we have since repeated the experiments and found similar results (Supplemental Figure 14 and Table 1), are achievable with a much smaller model (~85x fewer parameters than original generator / discriminator, ~2884x fewer FLOPS/iter than training the VAE). We have updated Table 1 and the main text to emphasize this. \n\n\n> 2. \u201c...I think the authors could do a better job of linking the different components of the paper together as they come across a little disjointed at the moment\u201d\n\nWe agree that there are several interwoven elements to the story. To better summarize and clarify the experimental design we have overhauled and streamlined the visual depiction in Figure 1.\n\n", "title": "Response to AnonReviewer2"}}}