{"paper": {"title": "On the Selection of Initialization and Activation Function for Deep Neural Networks", "authors": ["Soufiane Hayou", "Arnaud Doucet", "Judith Rousseau"], "authorids": ["soufiane.hayou@stats.ox.ac.uk", "doucet@stats.ox.ac.uk", "judith.rousseau@stats.ox.ac.uk"], "summary": "How to effectively choose Initialization and Activation function for deep neural networks", "abstract": "The weight initialization and the activation function of deep neural networks have a crucial impact on the performance of the training procedure. An inappropriate selection can lead to the loss of information of the input during forward propagation and the exponential vanishing/exploding of gradients during back-propagation. Understanding the theoretical properties of untrained random networks is key to identifying which deep networks may be trained successfully as recently demonstrated by Schoenholz et al. (2017) who showed that for deep feedforward neural networks only a specific choice of hyperparameters known as the `edge of chaos' can lead to good performance.\nWe complete this analysis by providing quantitative results showing that, for a class of ReLU-like activation functions, the information propagates indeed deeper for an initialization at the edge of chaos. By further extending this analysis, we identify a class of activation functions that improve the information propagation over ReLU-like functions. This class includes the Swish activation, $\\phi_{swish}(x) = x \\cdot \\text{sigmoid}(x)$, used in Hendrycks & Gimpel (2016),\nElfwing et al. (2017) and Ramachandran et al. (2017). This provides a theoretical grounding for the excellent empirical performance of $\\phi_{swish}$ observed in these contributions. We complement those previous results by illustrating the benefit of using a random initialization on the edge of chaos in this context.", "keywords": ["Deep Neural Networks", "Initialization", "Gaussian Processes"]}, "meta": {"decision": "Reject", "comment": "The paper attempts to extend the recent analysis of random deep networks to alternative activation functions.  Unfortunately, none of the reviewers recommended the paper be accepted.  The current presentation suffers from a lack of clarity and a sufficiently convincing supporting argument/evidence to satisfy the reviewers.  The contribution is perceived as too incremental in light of previous work."}, "review": {"BJlp2oCh2Q": {"type": "review", "replyto": "H1lJws05K7", "review": "The authors prove some theoretical results under the mean field regime and support their conclusions with a small number of experiments. Their central argument is that a correlation curve that leads to sub-exponential correlation convergence (edge of chaos) can still lead to rapid convergence if the rate is e.g. quadratic. They show that this is the case for ReLU and argue that we must ensure not only sub-exponential convergence, but also have a correlation curve that is close to the identity everywhere. They suggest activation functions that attain conditions as laid out in propositions 4/5 as an alternative.\n\nThe paper has many flaws:\n- the value of the theoretical results is unclear\n- the paper contains many statements that are either incorrect or overly sweeping\n- the experimental setup and results are questionnable\n\nTheoretical results:\n**Proposition 1: pretty trivial, not much value in itself\n**Proposition 2: Pretty obvious to the experienced reader, but nonetheless a valuable if narrow result.\n**Proposition 3: Interesting if narrow result. Unfortunately, it is not clear what the ultimate takeaway is. Is quadratic correlation convergence \"fast\"? Is it \"slow\"? Are you implying that we should find activation function where at EOC convergence is slower than quadratic? Do those activation functions exist? It would be good to compare this result against similar results for other activation functions. For example, do swish / SeLU etc. have a convergence rate that is less than quadratic?\n**Proposition 4: The conditions of proposition 4 are highly technical. It is not clear how one should go about verifying these conditions for an arbitrary activation function, let alone how one could generate new activation functions that satisfy these conditions. In fact, for an arbitrary nonlinearity, verifying the conditions of proposition 4 seems harder than verifying f(x) - x \\approx 0 directly. Hence, proposition 4 has little to no value. Further, it is not even clear whether f(x) - x \\approx 0 is actually desirable. For example, the activation function phi(x)=x achieves f(x) = x. But does that mean the identity is a good activation function for deep networks? Clearly not.\n**Proposition 5: The conditions of prop 5 are somewhat simpler than those of prop 4, but since we cannot eliminate the complicated condition (ii) from prop 4, it doesn't help much.\n**Proposition 6: True, but the fact that we have f(x) - x \\approx 0 for swish when q is small is kind of obvious. When q is small, \\phi_swish(x) \\approx 0.5x, and so swish is approximately linear and so its correlation curve is approximately the identity. We don't need to take a detour via propposition 4 to realize this.\n\nPresentation issues:\n- While I understand the point figures 1, 2 and 4b are trying to make, I don't understand what those figures actually depict. They are insufficiently labeled. For example, what does each axis represent?\n- You claim that for ReLU, EOC = {(0,\\sqrt{2})}. But this is not true. By definition 2, EOC is a subset of D_\\phi,var. But {(0,\\sqrt{2})} is not in D_\\phi,var, because it simply leaves all variances unchanged and does not cause them to converge to a single value. You acknowledge this by saying \"For this class of activation functions, we see (Proposition 2) that the variance is unchanged (qal = qa1) on the EOC, so that q does not formally exist in the sense that the limit of qal depends on a. However,this does not impact the analysis of the correlations.\" Section 2 is full of complicated definitions and technical results. If you expect the reader to plow through them all, then you should really stick to those definitions from then on. Declaring that it's fine to ignore your own definitions at the beginning of the very next section is bad presentation. This problem becomes even worse in section 3.2, where it is not clear which definition is actually used for EOC in your main result (prop 4), making prop 4 even harder to parse than it already is.\n\nCorrectness issues:\n- \"In this chaotic regime, it has been observed in Schoenholz et al. (2017) that the correlations converge to some random value c < 1\" Actually, the correlation converges deterministically, so c is not random.\n- \"This means that very close inputs (in terms of correlation) lead to very different outputs. Therefore, in the chaotic phase, the output function of the neural network is non-continuous everywhere.\" Actually, the function computed by a plain tanh network is continuous everywhere. I think you mean something like \"the output can change drastically under small changes to the input\". But this concept is not the same as discontinuity, which has an established formal definition.\n- \"In unreported experiments, we observed that numerical convergence towards 1 for l \u2265 50 on the EOC.\" Covergence of a sequence is a property of the limit of the sequence, and not of the 50th element. This statement makes no sense. Also if you give a subjective interpretation of those experimental results, you should present the actual results first.\n- \"Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions.\" This statement is very vague and sweeping. Also, one could argue that the fact that ReLU is much more popular and tends to give better results than tanh in practice disproves the statement outright.\n- \"Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions. However, these functions suffer from the vanishing gradient problem during back-propagation\" At the edge of chaos, vanishing gradients are impossible! As Schoenholz showed, at the edge of chaos, \\chi_1=1, but \\chi_1 is also the rate of growth of the gradient. Pascanu et al (2013) discussed vanishing gradients in RNNs, which is a different story.\n- \"Other activation functions that have been shown to outperform empirically ReLU such as ELU (Clevert et al. (2016)), SELU (Klambauer et al. (2017)) and Softplus also satisfy the conditions of Proposition 4 (see Supplementary Material for ELU).\" Firstly, SeLU does not satisfy proposition 4. f(x) \\approx x requires \\phi to be close to a linear function in the range where the pre-activations occur. Since SeLU has a kink at 0, it cannot be close to a linear function no matter how small the pre-activations are. Secondly, softplus also doesn't satisfy proposition 4, as \\phi(0) = 0 does not hold. Thirdly, this statement is too sweeping. If ELU / SELU / Softplus \"outperform\" ReLU, why is ReLU still used in practice? At best, those nonlinearities have been shown to outperform in a few scenarios.\n- \"We proved in Section 3.2 that the Tanh activation guarantees better information propagation through the network when initialized on the EOC.\" Prop 4 only applies in the limit as \\sigma_b converges to 0. So you can't claim that you showed tanh as \"better information propagation\" in general.\n- \"However, for deeper networks (L \u2265 40), Tanh is stuck at a very low test accuracy, this is due to the fact that a lot of parameters remain essentially unchanged because the gradient is very small.\" But in figure 6b the accuracy for tanh is decreasing rapidly, so therefore the parameters are not remaining \"essentially unchanged\", as this would also cause the accuracy to remain essentially unchanged. Also, if the parameter changes are too small ... why not increase the learning rate?\n- \"To obtain much richer priors, our results indicate that we need to select not only parameters (\u03c3b , \u03c3w ) on the EOC but also an activation function satisfying Proposition 4.\" Prop 4 only applies when \\sigma_b is small, so you additionally need to make sure \\sigma_b small.\n- \"In the ordered phase, we know that the output converges exponentially to a fixed value (same value for all Xi), thus a small change in w and b will not change significantly the value of the loss function, therefore the gradient is approximately zero and the gradient descent algorithm will be stuck around the initial value.\" But you are using Adam, not gradient descent! Adam explicitly corrects for this kind of gradient vanishing, so a small gradient can't be the reason for the lack of training success.\n\nExperimental issues:\n- \"We use the Adam optimizer with learning rate lr = 0.001.\" You must tune the learning rate independently for each architecture for an ubiased comparison.\n- In figure 6b, why does tanh start with a high accuracy and end up with a low accuracy? I've never seen a training curve like this ... This suggests something is wrong with your setup.\n- You should run more experiments with a larger variety of activation functions.\n\nMinor comments: \n- \"Therefore, it is easy to see that for any (\u03c3b , \u03c3w ) such that F is increasing and admits at least one fixed point,wehaveK\u03c6,corr(\u03c3b,\u03c3w) \u2265 qwhereqistheminimalfixedpoint;i.e. q := min{x : F(x) = x}.\" I believe this statement is true, but I also think it requires more justification.\n- At the end of page 3, I think \\epsilon_r should be \\epsilon_q\n\nThere are some good ideas here, but they need to be developed/refined/polished much further before publication. The above (non-exhaustive) list of issues will hopefully be helpful for this.\n\n\n### Addendum ###\nAfter an in-depth discussion with the authors (see below), my opinion on the paper has not changed. All of my major criticisms remain: (1) There are far easier ways of achieving f(x) ~ x than propositions 4/5/7, i.e. we simply have to choose \\phi(x) approximately linear. (2) The experiments are too narrow, and learning rates are badly chosen. (3) The authors do not discuss the fact that as f(x) gets too close to x, performance actually degrades as \\phi(x) gets too close to a linear function. (Many other criticisms also remain.)\n\nThe one criticism that the authors disputed until the end of the discussion is criticism (1). Their argument seems to hinge on the fact that their paper provides a path to construct activation function that avoid \"structural vanishing gradients\", which they claim 'tanh' suffers from. While they acknowledge that tanh does not necessarily suffer from \"regular\" vanishing gradients (as shown by \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" and \"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks\"), they claim it suffers from structural vanishing gradients. I do not believe that there is such a thing as structural vanishing gradients. However, even if such a concept did exist, it falls on the the authors to provide a clear definition / explanation, which they neither do in the paper nor the rebuttal.", "title": "Some good ideas, many issues", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1gMNNUmAm": {"type": "rebuttal", "replyto": "HJlerbNMCQ", "comment": " I think there is a misunderstanding. We don\u2019t understand what your point is. To sum up the issue :\n\n(1) We want to avoid activation functions that suffer from the vanishing gradient problem (tanh for instance) which is a problem that is independent from the initialization. Avoiding this vanishing gradient problem is one of the main reasons why ReLU has a big success. This \u2018structural\u2019 gradient vanishing problem slows the training and sometimes makes it impossible. This problem was extensively studied, we cite for example :\n-  Krizhevsky et al. \u201cImageNet Classification with Deep Convolutional Neural Networks \u201d \n- Glorot et al. \"Deep Sparse Rectifier Neural Networks\"\nWe also recommend checking this Notebook 'https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html' to understand more this issue.\nIn summary, tanh-like activations are not suited for deep neural networks because of this gradient vanishing problem during back-propagation. \n\n(2) Our point is to find a combination of good initialization and activation function (that does not suffer form the issue above). Swish is a good candidate : first because it doesn\u2019t suffer from the gradient vanishing mentioned above (like ReLU), and second, because it allows better information flow (compared to ReLU) when initialized at the edge of chaos with small sigma_b.\n\nYour proposition is to choose an activation phi such that F is concave so we that we can have sigma_b=0 and q*>0. But, because of what we said in our previous comment, we believe that any activation function phi that satisfies \u2018F is concave\u2019 will suffer either from an exploding gradient near 0 or a \u2018structural\u2019 vanishing gradient (problem mentioned in (1)). As we said, the two cases :\n\n\n- First case : F'(0+) = \\infty will lead to a problem of gradient exploding near 0. To see this, using Gaussian integration by parts, we have \\sigma_w^2 (\\mathbb{E}[\\phi'(\\sqrt{x}Z)^2]+\\mathbb{E}[\\phi''(\\sqrt{x}Z)\\phi(\\sqrt{x}Z)]) = F\u2019(x). By letting x goes to 0 and knowing that \\phi(0)=0, we have \\lim_{y \u2192 0} |\\phi'(y)|=\\infty (an example of such functions is \\phi(y)=\\sqrt{y}(1_{y>0} - 1_{y\\leq 0})). Having \\phi'(0) infinite is problematic as then the gradient explodes (for small values of y, \\phi'(y) will be very large causing instabilities during back-propagation)\n\n\n- Second case : F'(0+) is finite. There is too many cases to discuss here, but let's take the case of an activation such that phi''(x) small for all x (tanh for example). Using the Gaussian integration by parts, we have F'(x) = \\sigma_w^2 (\\mathbb{E}[\\phi'(\\sqrt{x}Z)^2]+\\mathbb{E}[\\phi''(\\sqrt{x}Z)\\phi(\\sqrt{x}Z)]). The second term is usually negligible (since phi'' is very small, this is the case for tanh for example), F'(x) is essentially given by the first term. Having F' decreasing (F concave) would imply that phi' is decreasing and is 'more concentrated' near zero. This would lead to a problem of vanishing gradient as we have said. \n\nYour method of choosing the triplet (sigma_b, sigma_w, phi) will be useful only if phi suffers from problem (1), which we trying to avoid and we made that clear in the introduction of the paper.\n\nTo sum up, we don\u2019t understand your concern with proposition 4, as we have said, it is very easy to verify those conditions numerically, we gave an example for ELU and Swish, and we provided numerical evidence that Swish is better than ReLU.\n\n", "title": "Re : Reviewer 1"}, "SyWiEUnxCQ": {"type": "rebuttal", "replyto": "H1xmIp5lC7", "comment": "Thank you for your comments.\n\n\n** ' I don't understand your argument regarding \"structural\" exploding gradient. Why not have F be concave and differentiable at 0? This is what happens for tanh. Then we don't need \\phi(0) = \\infty.' In our previous response about this issue, our last comment was ' It can be proved more generally for that taking F to be a concave function will lead to a problem of gradient exploding near 0'. Actually, we forgot to mention a second case where this could lead to a 'problem of gradient vanishing'. Apologies for that. Now we explain the full problem in the two cases : \n-- First case : F'(0+) = \\infty will lead to a problem of gradient exploding near 0 as previously discussed\n-- Second case : F'(0) is finite. There is too many cases to discuss here, but let's take the case of an activation such that phi''(x) small for all x (tanh for example).  Using the Gaussian integration by parts, we have F'(x) = \\sigma_w^2 (\\mathbb{E}[\\phi'(\\sqrt{x}Z)^2]+\\mathbb{E}[\\phi''(\\sqrt{x}Z)\\phi(\\sqrt{x}Z)]). The second term is usually negligible (since phi'' is very small, this  is the case for tanh for example), F'(x) is essentially given by the first term. Having F' decreasing (F concave) would imply that phi' is decreasing and is 'more concentrated' near zero. Since the maximum at zero should does not usually exceeds 1 (otherwise, we will have an exploding gradient near 0) This would lead to a problem of vanishing gradient as we have said. For example, Tanh' has a maximum in zero, but the concavity of F forces the gradient of tanh to decreasing. The 'more' concave F is, the more decreasing ph' should be. \nSo, we agree with your approach if you choose a tanh-like activation. However, as we discuss in our paper, we avoid activations that have this gradient vanishing problem (we believe this problem makes the training very slow if not impossible) .Moreove, this will not work with activations that have gradients approx to 1 at least on an subset of R with infinite measure (Swish, Elu etc)\n\n\n** \" this kind of weight sharing is generally regarded as one of the positive features of deep nets.\" We agree that for non polynomial activation this weight sharing is one of the positive features for deep nets. However, with polynomial activation, it is not clear why this would be better than a lasso regularization.. at least the second method would gives us Directly an idea about the features that are contributing the most to the output..\n\n", "title": "Re : Reviewer 1 "}, "Skxk9USxRX": {"type": "rebuttal", "replyto": "HJlWgjtJ0m", "comment": "Thank you for your comments.\n\n** \u201cYes, \\sigma_b=0 and q* > 0 implies exploding gradients. That's why I said this construction puts us in the chaotic regime.\u201d Note that there are two different notions of exploding gradients: exploding gradient linked to the chaotic phase, and the exploding gradient linked to a structural property of phi. It is not clear which one you are mentioning. To make our answer to your earlier comments more precise in this regard:\nIn our previous answer, we showed that taking sigma_b = 0 would leave with two options : if the function F is convex near 0 (which is the case for tanh, swish etc), either you choose sigma_w two small such that F\u2019(0)<1 and in this case q*=0 (since 0 is a an attractive fixed point) or you choose sigma_w such that F\u2019(0)>1 and in this case q^l will most likely diverge. To avoid this problem, you want the function F to be nearly concave near 0 in order to have another stable fixed point (other than zero), but as we showed in our previous response, this would lead to am structural problem in your activation function since it will cause an exploding gradient phenomenon for pre-activations close to 0 (this is different from the exploding gradient linked to the chaotic phase).\nHowever, in general, your way of finding sigma_b and sigma_w that hit the EOC is exactly what we have used in practice.\n\n\n** \u201cBy the way, why are polynomial activation functions bad? I've recently trained some nets with the x^2 activation function and that worked ok.\u201d Because in this case the output of the network is polynomial function of order 2^L where L is the number of layers. So you\u2019re basically doing polynomial regression of order 2^L. Why not doing it directly ? There is a well developed theory for this purpose. Moreover, by doing such regression with a neural network, you are restricting the expressive power of this regression because the output weights are functions of the weights inside the network. Of course, you will have less weights compared to the standard polynomial regression, but this is only because you are assuming such restrictions on the weights, and it is not intuitive why taking such functions would be better than doing e.g. a Lasso regularization to reduce the dimension and select only significant variables.\n\n** \u201cSo? Every network requires a good choice of learning rate. AFAIK, they used a standard training algorithm. Regarding choosing a good initialization ... isn't that one of the messages of your paper?\u201d Yes, choosing a good initialization is crucial for a good training. But, we are not saying it is the only thing that influence the training. In majority of cases, the learning rate and the \u2018structural\u2019 exploding gradient problem have more influence than the initialization. So we agree with you and are essentially saying the same thing\n\n\n** \u201cIsn't one of your core message that ReLU doesn't work for networks beyond a certain depth ... ?? it's precisely because at all fixed points of F, f(x) is far away from x.\u201d Yes it is, but again we are just saying that ReLU suffers \u2018more seriously\u2019 from this problem of Information Propagation compared to Swich, Tanh etc .. However, it is not clear whether this problem would make it IMPOSSIBLE for a very deep ReLU network to be trained, in the same way that the gradient vanishing problem of Tanh (we are not talking about the initialization step, but during the training) didn\u2019t make it impossible to train a very deep Tanh network.", "title": "Re : Reviewer 1"}, "rylzpsv6pX": {"type": "rebuttal", "replyto": "SkeeBUGp6Q", "comment": "Thank you for your comments.\n\n*** Proposition 3 : this prop just gives the exact rate of convergence of the correlation. For experienced reader, this can be compared with the quadratic rate of convergence of the correlation for a residual neural network with ReLU activation.\n\n*** Proposition 4/5/7 : we agree that \u2018given some (\\phi, \\sigma_w, \\sigma_b), I can verify whether f(x) ~ x holds true \u2019. However, the suggested method of constructing such triplet has many issues. You suggest that \u2018For example: pick any \\phi that's close a linear function, initialize \\sigma_b to zero and \\sigma_w to some value for which a q* exists and isn't too large. We immediately get f(x) ~ x. If we also want to be on the edge of chaos, we can e.g. pick a \\phi with \\phi(x) = - \\phi(-x), which ensures we are in the chaotic regime with \\sigma_b=0. \u2019. First, it makes no sense to choose an activation close to a linear function, cause the neural network model will be equivalent to a simple linear regression.. so we assume you meant an activation function close to linear near 0. By initializing sigma_b to 0, 0 will be a fixed point of F, and q=0 for any sigma_w unless F is nearly concave near 0, which would imply (reader can check this) that the activation function has exploding gradients near 0. As an example, To have small q different from 0, you want F to be e.g. \\F(x) = \\sigma_b^2 + x^{\\alpha} where 0<\\alpha<1) .  However, this would lead to a problem of exploding gradient. Indeed, to satisfy this condition, we need to have (assuming \\phi is differentiable a.e. and has a second derivative at least in the distribution sense) \\sigma_w^2 \\frac{1}{\\sqrt{x}} \\mathbb{E}[Z \\phi'(\\sqrt{x}Z)\\phi(\\sqrt{x}Z)] \\approx \\alpha x^{\\alpha -1}, therefore, using Gaussian integration by parts, this yields \\sigma_w^2 (\\mathbb{E}[\\phi'(\\sqrt{x}Z)^2]+\\mathbb{E}[\\phi''(\\sqrt{x}Z)\\phi(\\sqrt{x}Z)]) \\approx \\alpha x^{\\alpha -1}. By letting x goes to 0 and knowing that \\phi(0)=0, we have \\lim_{y \u2192 0} |\\phi'(y)|=\\infty (an example of such functions is \\phi(y)=\\sqrt{y}(1_{y>0} - 1_{y\\leq 0})). Having \\phi'(0) infinite is problematic as then the gradient explodes (for small values of y, \\phi'(y) will be very large causing instabilities during back-propagation): this is why we enforce the condition \\frac{\\phi(x)}{x}|<K in Prop 4. It can be proved more generally for that taking F to be a concave function will lead to a problem of gradient exploding near 0.\nSo your intuition of changing sigma_w until hitting the edge of chaos is wrong, cause you will always have q = 0 for any sigma_w.\n\n\n*** We understand your concern. By saying that we don\u2019t consider activation functions that are trivial, we mean all activation functions that are close to polynomial activations. We will modify the wording.\n\n*** \u2018You comment regarding the fact that the edge of chaos is only achieved for tanh at the beginning of training makes no sense, because the edge of chaos is only defined in the randomly initialized state. A trained network does not follow the mean field model for any activation function. \u2019 By beginning of the training we mean the first step, which is the initialization step, so we think we are saying the same thing.. could you explain why it makes no sense ?\nThe paper \"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks\" showed indeed that we can train CNN of 10000 layers with tanh activation by initializing on the EOC. However, note that here, it is not only the EOC that made it possible to train this very deep networks, it was a combination of a good choice of the learning rate, a good choice of the training algorithm, and finally a good initialization. \n\u2018This is not possible with ReLU. So with proper initialization and learning rate, tanh outperforms ReLU beyond a certain depth. \u2018 Is there a theoritical that proves this ? As far as we know, we are not sure if we can train such very deep networks with ReLU or not. Proving that it is not possible with ReLU needs more theoretical work.\n\n** \u2018Adam is not equivalent to SGD, even at the first step, at least using the algorithm given in the original Adam paper \u2019 Could you explain why ? In the original paper, as we said, the Adam optimizer initializes the first moment and second moments to m_1 = 0 and m_2=0, so the first step of the Adam optimizer is equivalent to an SGD step.\n\u2018And even if it were, my criticism still holds for all other training steps\u2019 Here we were addressing only your concern regarding the first step.\n", "title": "Re : Reviewer 1"}, "SJxDorH5aQ": {"type": "rebuttal", "replyto": "SJgij-iuhX", "comment": "The reviewer suggests that \u2018However, I don't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better.\u2019 In this paper, we only say that information propagation could partly explain why an activation function performs empirically better than another one. Of course, there are many other properties that make one activation better than another (e.g. vanishing gradient). As far as empirical evidence is concerned we refer the reader to Ramachandran et al. (2017). The present paper focuses on understanding/explaining/designing good initialization scheme for the algorithm, which is known to be crucial. \n\n1) \u201cAll experiments are conducted over MNIST with testing accuracy around 96%. The authors should consider using large datasets (at least Cifar10).\u201d: We are currently running more experiments right now. Note that an extensive set of simulations has already been performed in Ramachandran et al. (2017). \n\n\u201cFigure 6(b) seems unconvincing. ReLU network should be trainable with depth>=200\u201d. We do not claim that ReLU is not trainable in such scenarios. Figure 6-b only shows the first steps (40 epochs) of the training, because the information propagation analysis is only valid at the initial step, we are comparing the final accuracies here.\n\n2) We only say that having an activation function that avoids the vanishing/exploding gradient problem is always better than having an activation function that has gradient strictly less than 1 (in absolute value) in deep neural networks. If a Tanh-network can be trained for 1000 layers, it does not mean those networks are not trainable with Swish or Relu. \n\n4) We are currently running more experiments with different learning rates. We note that extensive experiments were already done in in Ramachandran et al. (2017) with different datasets and different learning rates.", "title": "Re : Reviewer 3"}, "ryxeYSB96Q": {"type": "rebuttal", "replyto": "rJe10kLt2m", "comment": "- Major concerns:\n* \u201cIn terms of selection on initialization, the findings seem to be mostly discussed already in Schoenholz et al (2017)\u201d. We respectfully disagree with the reviewer. There are several original results in our manuscript. Among others, our main result -Proposition 4- provides sufficient conditions to ensure deep information propagation. This provides some theoretical grounding explaining the excellent empirical performance of Swish observed in several recent papers. We are not aware of any similar result in the literature.\n\n* We agree with the reviewer that more experiments could be done with different learning rates, which we are working on right now.  However, note that extensive experiments were already carried out in Ramachandran et al. (2017) with different datasets and different learning rates as indicated in our paper. We thus chose not to pursue more simulations in this paper and focused on theoretical properties enlightening the empirical findings of Ramachandran et al. (2017). \n\n- Other concerns:\nFew comments/questions:\n- P3: Is M_{ReLU} = 2 correct, from ReLU EOC, shouldn\u2019t it be \u00bd?: Yes indeed, that was a typo and it has been fixed, thank you for pointing this to us. \n\n- \"For all the works using activation functions satisfying Proposition 4 (ELU/SELU/Softplus/Swish), the initialization scheme close to EOC? Does this work\u2019s analysis actually explain performance boost over ReLU for these activation functions?\": We believe that the fact that these activation functions satisfy Prop4 partially explain the performance boost, since it leads to a better initialization scheme which is known to let the information propagate deeper through the network.\n", "title": "Re : Reviewer 2 "}, "ryguQHBq67": {"type": "rebuttal", "replyto": "BJlp2oCh2Q", "comment": "** \u201cProposition 6: True, but the fact that we have f(x) - x \\approx 0 for swish when q is small is kind of obvious. When q is small, \\phi_swish(x) \\approx 0.5x, and so Swish is approximately linear and so its correlation curve is approximately the identity. We don't need to take a detour via proposition 4 to realize this.\u201d: We believe that \\phi(x) being approximately linear near zero is not good enough for the results of Prop4 to hold and might lead to wrong intuition. An important condition is that we can find a regime where q is small. The condition \u2018q is small\u2019 is not necessarily guaranteed. The condition on the limit of q as sigma_b goes to zero is precisely condition (iii) of Prop4. Having only the property that phi is linear near 0 is not sufficient to ensure that the result of Prop4 holds. For example, the function phi(x) = max(x+1,0) \u2013 1 \\approx x near 0. However, one can check that q does not converge to 0 in this scenario so that the result of prop 4 do not hold. \n\n- Reviewer\u2019s comments on presentation issues:\n* We apologize if fig 1 was not so easy to understand. It displays a draw of an output of a RELU (resp Tanh) network, as such it is a function from [0,1]^2 to \\R (in both cases). We mentioned in the text of page 3 that Figure 1 illustrates this behaviour for d = 2 for ReLU and Tanh using a network of depth L = 10 with Nl = 100 neurons per layer. The axes are x and y (belonging to [0,1]^2) in dimension d=2. Other figures have the same axes.\n\n* We agree that definition of the EOC is not relevant for ReLU. We could make the definition of the EOC more general to include ReLU-like functions. We thought it was an unnecessary complication and this \u2018modification\u2019 was introduced only in the section of ReLU-like activations. We agree that this change of definition is actually confusing and we will address it.\n\n\n- Reviewer\u2019s comments on correctness issues:\n* By \u2018some random value c < 1\u2019: we meant a value of c in the interval (0,1), we will modify the wording.\n\n* \u201cActually, the function computed by a plain Tanh network is continuous everywhere. I think you mean something like \"the output can change drastically under small changes to the input\". We agree with the reviewer that the output is continuous when the network has finite width and depth. However, in the limit of infinite width and depth, it can be proven that the output function is discontinuous everywhere.\n\n* By \u2018we observed that numerical convergence towards 1 for l \u2265 50 on the EOC.\u2019 we meant that output start to be visually constant around l = 50.\n* We disagree that the statement \"Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions\" is very vague and sweeping. We are not claiming that \u201cTanh is better than ReLU\u201d. We are just saying that Tanh verifies conditions of Prop4, therefore it provides better information propagation compared to ReLU which does not enjoy this property. \n\n* \u201cAt the edge of chaos, vanishing gradients are impossible! As Schoenholz showed, at the edge of chaos, \\chi_1=1, but \\chi_1 is also the rate of growth of the gradient.\u201d The result stated from Schoenholz et al. is correct but the vanishing gradient problem is not avoided by initializing the network on the on the edge of chaos. Such an initialization only addresses the vanishing gradient problem at the first step of the learning algorithm. In the subsequent steps of gradient descent, this problem will occur again as the weights are no longer \u2018distributed\u2019 on the edge of chaos. \n* \u201dIn the ordered phase, we know that the output converges exponentially to a fixed value (same value for all Xi), thus a small change in w and b will not change significantly the value of the loss function, therefore the gradient is approximately zero and the gradient descent algorithm will be stuck around the initial value.\" But you are using Adam, not gradient descent! Adam explicitly corrects for this kind of gradient vanishing, so a small gradient can't be the reason for the lack of training success.\u2019   The Adam optimizer initializes the first moment and second moments to m_1 = 0 and m_2=0, so the first step of the Adam optimizer is equivalent to the first step of usual gradient descent. \n\n- Setup issues: \n* Thanks for your comment about figure 6-b. Indeed, after running a second experiment, the drop in the curve of Tanh seems to be due to a learning rate issue, however, the conclusion \u2018ReLU is better than Tanh\u2019 in this case is still numerically true.\n* We are currently running experiments with different learning rates. Note that many experiments were already done in Ramachandran et al. (2017) with different datasets and different learning rates as indicated in the paper. ", "title": "Re : Reviewer 1 (part 2)"}, "H1xr-BSq6Q": {"type": "rebuttal", "replyto": "BJlp2oCh2Q", "comment": "- Reviewer\u2019s comments on theoretical results : \n** \u201cProposition 1: pretty trivial, not much value in itself \u201d We agree that the proposition is trivial and this is stated explicitly after the Proposition. However, we think that after defining the Domains of Convergence, it is logical to give some sufficient conditions for (sigma_b, sigma_w) to be in those domains of convergence.\n\n** \u201cProposition 2: Pretty obvious to the experienced reader, but nonetheless a valuable if narrow result\u201d.  This Proposition gives explicitly the edge of chaos for an important set of activation functions. This result is indeed useful since it provides the parameters (sigma_b, sigma_w) that one should use for ReLU, Leaky-RELU etc.\n\n** \u201cProposition 3: Interesting if narrow result. Unfortunately, it is not clear what the ultimate takeaway is. Is quadratic correlation convergence \"fast\"? Is it \"slow\"? Are you implying that we should find activation function where at EOC convergence is slower than quadratic?\u201d  It is suggested that that \u2018it is not clear what the ultimate takeaway is\u2019. In the text just before Proposition 3, we explain that this proposition establishes that the convergence rate of the correlation of a ReLU network is 1/l^2 instead of the exponential rate exp( -b l). Hence compared to the rate outside the edge of chaos, this is a slow rate, which in practice means that the correlation (the information) propagates deeper inside the network and allows for use of many more layers.\n This proposition indeed only deals with RELU, which makes it seem narrow, RELU activation functions are however widely used in practice. It is expected that similar phenomena hold under RELU-like activation functions. We did not pursue this here since we propose other activation functions which we believe are better behaved \u2013 as explained by our theory (prop 4 and 5) and the empirical evidence provided in Ramachandran et al (2017). \n\n** Proposition 4 : The reviewer suggests that \u201cThe conditions of proposition 4 are highly technical. It is not clear how one should go about verifying these conditions for an arbitrary activation function\u201d. We agree that the conditions can be difficult to verify theoretically, but in many cases, they can be verified numerically; see the Appendix for the ELU activation function. Proposition 5 provides additionally a simpler set of assumptions, which has been verified for important activation functions. \n\n** \u201c\u2026for an arbitrary nonlinearity, verifying the conditions of Proposition 4 seems harder than verifying f(x) - x \\approx 0 directly. Hence, Proposition 4 has little to no value\u2026\u201d We disagree with this comment. First, it is difficult to see how one could verify that  f(x) - x \\approx 0 as sigma_b goes to zero without having any condition on the limit of q. Recall that f depends on q, and q may diverge, hence the use of condition (iii) in Prop4. Moreover, condition (iv) is necessary for uniform convergence of the correlation function f to the identity function. We could have stated a weaker condition, but it be  would more complicated to verify numerically. \n\n** \u201cit is not even clear whether f(x) - x \\approx 0 is actually desirable. For example, the activation function phi(x)=x achieves f(x) = x. But does that mean the identity is a good activation function for deep networks? Clearly not.\u201d: You are quite right the identity function, like any polynomial functions, is not a good activation function. We have explicitly mentioned in the introduction that for phi to be a `good\u2019 activation function then it needs to be non-polynomial, phi should not suffer from the gradient vanishing problem and it should have a good information propagation, this last condition being the focus of our paper. Having  f(x) - x \\approx 0 is a desirable property to obtain a good information propagation, since c^{l+1} = f(c^{l}). Therefore, having f close to the identity slows down the convergence of the correlation to 1, which means the information propagates deeper inside the network. The proof of Prop4 is informative in this respect. \n** \u201cProposition 5: The conditions of prop 5 are somewhat simpler than those of prop 4, but since we cannot eliminate the complicated condition (ii) from prop 4, it doesn't help much.\u201d: In Proposition 7 in the Appendix, we show how one replace condition (ii) of Prop4 by a simpler condition on phi. We will move this proposition to the main paper.", "title": "Re : Reviewer 1 (part 1)"}, "rJe10kLt2m": {"type": "review", "replyto": "H1lJws05K7", "review": "Studying properties of random networks in the infinite width limit, this work suggests guidance for choosing initialization and activation function. \n\nIn my opinion, novel contribution comes for guidance for choosing activation functions and theoretical grounds for superior performance of ```'swish\u2019 activation function. \n\nI have two main concerns :\n\nIn terms of selection on initialization, the findings seem to be mostly discussed already in Schoenholz et al (2017) [1]. In their work, Edge of Chaos is critical line separating different phases and was already shown to have power-law decay rather than exponential decay. As far as I can tell, analysis on EOC on ReLU-like activations are different from Schoenholz et al (2017) [1]. Some of the results for ReLU are already appeared in the literature e.g. Lee et al (2018) [2].\n\nAnother main concern is in the author\u2019s experimental setup. It is hard to draw conclusions when comparison experiments were done with a fixed learning rate.  As we know learning rate is one of the most critical hyperparameter for determining performance and optimal learning rate is often sensitive to architecture choice. Especially for different non-linearity and different depth/width optimal learning rate can change.\n\nPros: \n - Clearly written and easy to understand what authors are trying to say \n - Interesting theoretical support for activation function which recently got attention due to boosting performance in neural networks\n - Nice suggestion of choosing activation function for deep networks (Proposition 4)\n      -- ELU/SELU/Softplus/Swish all satisfy this suggestion\nCons:\n - Novelty may be not strong enough as the standard analysis tool from [1] was mostly used\n - Experimental setup may suffer from some critical flaw \n\nFew comments/questions:\n- P3: Is M_{ReLU} = 2 correct, from ReLU EOC, shouldn\u2019t it be \u00bd?\n- For all the works using activation functions satisfying Proposition 4 (ELU/SELU/Softplus/Swish), the initialization scheme close to EOC? Does this work\u2019s analysis actually explain performance boost over ReLU for these activation functions?\n\n[1] S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 5th International Conference on Learning Representations, 2017.\n[2] J. Lee, Y. Bahri, R. Novak, S.S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. 6th International Conference on Learning Representations, 2018.\n", "title": "Interesting solid work but few concerns remain", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJgij-iuhX": {"type": "review", "replyto": "H1lJws05K7", "review": "Good results; providing some insights on the selection of activation function.  \n\nThis paper builds upon two previous works B.Poole etc. and S.S. Schoenholz etc. who initialized the study of random initialized neural network using a mean field approach (or central limit theorem.)\nThe two principal results of this paper are \n1. Initializing the network critically on the edge of chaos.  \n2. Identifying some conditions on the activation functions which allow good \"information flow\" through the network.   \n\nThe first result is not new in general (already appeared in Schoenholz etc. and many follow up mean field papers). However, the results about ReLU (initializing (weigh_variance, bias_variance)=(2, 0)) seems to be new. The author also shows that the correlations converge to 1 at a polynomial rate (proposition 3), which is interesting. \n\nThe second one is a novel part of this paper (proposition 5). If I understand correctly, the authors are trying to identify a class of activation functions (and suitable hyper-parameters) so that the network can propagate the sample-to-sample correlations (i.e. kernel) almost isometrically (please correct me if I am wrong). This is only possible 1) the activation functions are linear; OR 2) in the regime q->0, where the activation function has small curvature (i.e. almost linear). I think the results (and insights) are quite interesting. However, I don't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better.  \n\n\n\ncons:\n1. I don't think the experimental results are convincing enough for the reasons below:\n    1.1. All experiments are conducted over MNIST with testing accuracy around 96%.  The authors should consider using large datasets (at least Cifar10).\n    1.2 The width (<=80) of the network is too small while the theory of the paper assumes the width approaches infinity. Width>=200 should be a reasonable choice. It should be possible to train a network with depth~200 and width ~200 and batch_size~64 in a single machine.   \n    1.3. Figure 6(b) seems unconvincing. ReLU network should be trainable with depth>=200; see figure 4 of the paper: \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" \n\n\n2. The claim that swish is better than tanh because the latter suffers from vanishing of gradients is unconvincing. It has been shown in Schoenholz etc and many follow-up papers that ultra-deep tanh networks (>=1000 layers) can be trained with critical initialization. \n\n3. Again, I don't think it is convincing to make the conclusion that swish is better than ReLU based on the empirical results on MNIST. \n\n4. Using a constant learning rate (lr=0.001) for all depths (and all widths) is incorrect. I believe the gradients will explode as depth increases. Roughly, the learning rate should decay linearly with depth (and width) when the network is initialized critically.  \n\n\nIn sum, the paper has some interesting theoretical results but the empirical results are not convincing.  \n\n\nOther comments:\n1. The authors should explain the significance and motivation of proposition 4. In particular, explain why we need f(x)~x. \n2. Consider replacing \"Proposition 4\" by  \"Theorem\", since it is the main result of the paper.  \n\n", "title": "Good results; providing some insights on the selection of activation function.  ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}