{"paper": {"title": "Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning", "authors": ["Yichi Zhang", "Zhijian Ou"], "authorids": ["zhangyic17@mails.tsinghua.edu.cn", "ozj@tsinghua.edu.cn"], "summary": "Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with improved performance and significantly reduced cost than traditional methods. ", "abstract": "An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing. \nIn this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.\nIn the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.\nIn this way of learning SSEs with SG-MCMC and pruning, we not only achieve high prediction accuracy since SG-MCMC enhances exploration of the model-parameter space, but also reduce memory and computation cost significantly in both training and testing of NN ensembles.\nThis is thoroughly evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.\nFor example, in LSTM based language modeling (LM), we obtain 21\\% relative reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has only 30\\% of model parameters and 70\\% of computations in total, as compared to the baseline large LSTM LM.\nTo the best of our knowledge, this work represents the first methodology and empirical study of integrating SG-MCMC, group sparse prior and network pruning together for learning NN ensembles.", "keywords": ["ensemble learning", "SG-MCMC", "group sparse prior", "network pruning"]}, "meta": {"decision": "Reject", "comment": "This paper is interesting since it goes to showing the role of model averaging. The clarifications made improve the paper, but the impact of the paper is still not realised: the common confusion on the retraining can be re-examined, clarifications in the methodology and evaluation, and deeper contextulaisation of the wider literature."}, "review": {"B1A7YkceM": {"type": "review", "replyto": "r1uOhfb0W", "review": "The authors propose a procedure to generate an ensemble of sparse structured models. To do this, the authors propose to (1) sample models using SG-MCMC with group sparse prior, (2) prune hidden units with small weights, (3) and retrain weights by optimizing each pruned model. The ensemble is applied to MNIST classification and language modelling on PTB dataset. \n\nI have two major concerns on the paper. First, the proposed procedure is quite empirically designed. So, it is difficult to understand why it works well in some problems. Particularly. the justification on the retraining phase is weak. It seems more like to use SG-MCMC to *initialize* models which will then be *optimized* to find MAP with the sparse-model constraints. The second problem is about the baselines in the MNIST experiments. The FNN-300-100 model without dropout, batch-norm, etc. seems unreasonably weak baseline. So, the results on Table 1 on this small network is not much informative practically. Lastly, I also found a significant effort is also desired to improve the writing. \n\nThe following reference also needs to be discussed in the context of using SG-MCMC in RNN.\n- \"Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling\", Zhe Gan*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin", "title": "Justification for the proposed algorithm is weak + weak experiments.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJt3Bg5gM": {"type": "review", "replyto": "r1uOhfb0W", "review": "In this paper, the authors present a new framework for training ensemble of neural networks. The approach is based on the recent scalable MCMC methods, namely the stochastic gradient Langevin dynamics.\n\nThe paper is overall well-written and ideas are clear. The main contributions of the paper, namely using SG-MCMC methods within deep learning, and then increasing the computational efficiency by group sparsity+pruning are valuable and can have a significant impact in the domain. Besides, the proposed approach is more elegant the competing ones, while still not being theoretically justified completely. \n\nI have the following minor comments:\n\n1) The authors mention that retraining significantly improves the performance, even without pruning. What is the explanation for this? If there is no pruning, I would expect that all the samples would converge to the same minimum after retraining. Therefore, the reason why retraining improves the performance in all cases is not clear to me.\n\n2) The notation |\\theta_g| is confusing, the authors should use a different symbol.\n\n3) After section 4, the language becomes quite informal sometimes, the authors should check the sentences once again.\n\n4) The results with SGD (1 model) + GSP + PR should be added in order to have a better understanding of the improvements provided by the ensemble networks. \n\n5) Why does the performance get worse \"obviously\" when the pruning is 95% and why is it not obvious when the pruning is 90%?\n\n6) There are several typos\n\npg7: drew -> drawn\npg7: detail -> detailed\npg7: changing -> challenging\npg9: is strongly depend on -> depends on\npg9: two curve -> two curves", "title": "A more principled way for training ensemble neural networks, nice idea and experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hy6mmeCgf": {"type": "review", "replyto": "r1uOhfb0W", "review": "The authors note that several recent papers have shown that bayesian model averaging is an effective and universal way to improve hold-out performance, but unfortunately are limited by increased computational costs.   Towards that end, the authors of this manuscript propose several modifications to this procedure to make it computationally feasible and indeed improve performance.\n\nPros:\nThe authors demonstrate an effective procedure for FNN and LSTMs that makes model averaging improve performance.\nEmpirical evidence is convincing on the utility of the approach.\n\nCons:\nNot clear how this approach would be used with convolutional structures\nMuch of the benefit appears to come from the sparse prior, pruning, and retraining (Figure 3).  The model averaging seems to have a smaller contribution.  Due to that, it seems that the nature of the contribution needs to be clarified compared to the large literature on sparsifying neural networks, and the introductory comments of the paper should be rewritten to reflect that reality.", "title": "A useful approach for making model averaging more feasible", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S14r4n5fz": {"type": "rebuttal", "replyto": "r1uOhfb0W", "comment": "Dear Reviewers,\nWe greatly appreciate your helpful and constructive comments on the paper. We have carefully revised the paper to incorporate your comments, adding some new results and polishing the writing for clarification. As a result, we believe that the paper has been substantially improved and strengthened.\nIn the following, we provide our response to your specific points.\nPlease find the updated paper for more details. ", "title": "Updated paper"}, "B1OxK2cMG": {"type": "rebuttal", "replyto": "HysnNFwA-", "comment": "Thanks for your comment.\nAs said in your comment, these previous works apply group Lasso with SGD to learn structurally sparse DNNs. They focus on point estimates and are not in the context of learning ensembles. We have added the discussion in Related Work.", "title": "Response"}, "HkmrOnqzG": {"type": "rebuttal", "replyto": "B1A7YkceM", "comment": "Thank you very much for reviewing the paper.\n\n> Particularly. the justification on the retraining phase is weak. \n\nThanks for your note. As stated in the end of Section 3.2, there are two justifications for the retraining phase: First, theoretically (namely with infinite samples), model averaging does not need retraining. However, the actual number of samples used in practice is rather small for computational efficiency. So retraining essentially compensates for the limited size of samples for model averaging. Second, the MAP estimate is more likely than the network obtained just after pruning but before retraining. Retraining increases the posteriori probabilities of the networks in the ensemble and hopefully improves the prediction performance of the networks in the ensemble.\n\n> The second problem is about the baselines in the MNIST experiments. The FNN-300-100 model without dropout, batch-norm, etc. seems unreasonably weak baseline. So, the results on Table 1 on this small network is not much informative practically. \n\nSuch basic setting in the MNIST FNN experiments allows easy reproduction of the results.\nStrong results are reported on the more challenging LSTM LM task.\n\n> Lastly, I also found a significant effort is also desired to improve the writing.\n\nWe polish the paper and especially rewrite those parts after Sections 4.\n\n> The following reference also needs to be discussed in the context of using SG-MCMC in RNN. - \"Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling\", Zhe Gan*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin\n\nThis work pioneers in applying SG-MCMC to Bayesian learning of RNNs, but without considering model pruning and the cost of model averaging. We have added the discussion in Related Work.", "title": "Response to Reviewer1"}, "BJJWN39zf": {"type": "rebuttal", "replyto": "Hy6mmeCgf", "comment": "Thank you very much for reviewing the paper.\n\n> Not clear how this approach would be used with convolutional structures\n\nIt has been shown in [1] that group Lasso regularization is effective for structured sparsity SGD learning for convolutional structures (filters, channels, filter shapes, and layer depth). It is conceivable that group Lasso used with SGLD can work for convolutional structures, by employing proper groupings like those in [1].\n[1] Wen, Wu, Wang, Chen and Li. Learning structured sparsity in deep neural networks, NIPS 2016.\n\n> The model averaging seems to have a smaller contribution. \n\nIt can be seen from Figure 3(a) that as the training proceeds, more models are averaged, which consistently improves the PPLs. Also, the relationship between the performance of an ensemble and the number of models in an ensemble is examined in Figure 3(b), which clearly shows the contribution of model averaging.\n\n> Due to that, it seems that the nature of the contribution needs to be clarified compared to the large literature on sparsifying neural networks, and the introductory comments of the paper should be rewritten to reflect that reality.\n\nLiterature review with regards to NN sparse structure learning and NN compression is rewritten and presented in Related Work.", "title": "Response to Reviewer2"}, "BkgBDh9GG": {"type": "rebuttal", "replyto": "BJt3Bg5gM", "comment": "Thank you very much for reviewing the paper.\n\n> 1)  \nAs stated in the end of Section 3.2, there are two justifications for the retraining phase: First, theoretically (namely with infinite samples), model averaging does not need retraining. However, the actual number of samples used in practice is rather small for computational efficiency. So retraining essentially compensates for the limited size of samples for model averaging. Second, the MAP estimate is more likely than the network obtained just after pruning but before retraining. Retraining increases the posteriori probabilities of the networks in the ensemble and hopefully improves the prediction performance of the networks in the ensemble.\n\nNote that running SGLD enhances exploration of the model-parameter space, and we take thinned collection of samples so that there are low correlations between the samples. So in contrary to converging to the same minimum after retraining, thinned samples from SGLD would lead to neighbors of different local minima and retraining further fine-tune the paramters to take different minima.\n\n> 2) \nThanks for your suggestion, we have changed the notation to dim(\\theta_g).\n\n> 3)\nWe polish the paper and especially rewrite those parts after Sections 4.\n\n> 4)\nThanks for your suggestion. The results of SGD (1 model) + GSP + PR and SGD (ensemble) + GSP + PR have been added to Table 5, with the discussion in the paragraph before the last paragraph in Section 5.2.\nSGD (1 model)+GSP+PR can reduce the model size but the PPL is much worse than the ensemble, which clearly shows the improvement provided by the ensemble. Additionally, we compare SGLD (4 models)+GSP+PR with SGD (4 models)+GSP+PR. The two ensembles achieve close PPLs. However, SGLD ensemble learning reduces about 30% training time.\n\n> 5)\nWe empirically find that 90% is the highest pruning rate without hurting performance for LSTMs.\n\n> 6)\nTypos have been fixed.", "title": "Response to Reviewer3"}}}