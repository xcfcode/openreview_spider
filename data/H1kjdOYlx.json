{"paper": {"title": "Modular Multitask Reinforcement Learning with Policy Sketches", "authors": ["Jacob Andreas", "Dan Klein", "Sergey Levine"], "authorids": ["jda@cs.berkeley.edu", "klein@cs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "summary": "Learning multitask deep hierarchical policies with guidance from symbolic policy sketches", "abstract": "We describe a framework for multitask deep reinforcement learning guided by\npolicy sketches. Sketches annotate each task with a sequence of named subtasks,\nproviding high-level structural relationships among tasks, but not providing the\ndetailed guidance required by previous work on learning policy abstractions for\nRL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations).\nOur approach associates every subtask with its own modular subpolicy,\nand jointly optimizes over full task-specific policies by tying parameters across\nshared subpolicies. This optimization is accomplished via a simple decoupled\nactor\u2013critic training objective that facilitates learning common behaviors from\ndissimilar reward functions. We evaluate the effectiveness of our approach on a\nmaze navigation game and a 2-D Minecraft-inspired crafting game. Both games\nfeature extremely sparse rewards that can be obtained only after completing a\nnumber of high-level subgoals (e.g. escaping from a sequence of locked rooms or\ncollecting and combining various ingredients in the proper order). Experiments\nillustrate two main advantages of our approach. First, we outperform standard\nbaselines that learn task-specific or shared monolithic policies. Second, our\nmethod naturally induces a library of primitive behaviors that can be recombined\nto rapidly acquire policies for new tasks.", "keywords": ["Reinforcement Learning", "Transfer Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "As per all the reviews, the work is clearly promising, but is seen to need additional discussion / formalization / experimental comparison with related work, and stronger demonstrations of the application of this technique.\n Further back-and-forth with the reviewers would have been useful, but there should be enough to go on in terms of directions. This work would benefit from being part of the workshop track."}, "review": {"rkftobwLl": {"type": "rebuttal", "replyto": "HyaORazEe", "comment": "We have done our best to discuss relationships and provide comparisons with existing work on hierarchical policy learning, and would be happy to do so in more detail if told exactly what in the extremely large family of techniques for policy abstraction we should compare to. We remain somewhat puzzled by references to multiagent RL.", "title": "Author response"}, "SkoH9Zw8l": {"type": "rebuttal", "replyto": "SyJDbsb4e", "comment": "\nTRAINING CONDITION\n\nWe could have done a better job of making this clearer: The reviewer is correct that there is not a natural source of sketch-like training annotations (as there is for e.g. natural language instructions). Our claim is that these sketches are nonetheless extremely easy to produce, contain very few bits of information, but nevertheless result in dramatic improvements in training performance. The extra annotation we use here literally fits in a 10-line text file. We thus think it is reasonable for system designers to take a few minutes to write such a file, and then use a training objective that can efficiently exploit the information contained in it.\n\nNATURAL LANGUAGE\n\nWe also want to emphasize that the learning problem considered here is very different from the one normally encountered in natural language processing. There, the learned model is a policy that conditions on both environment states and text to determine next actions; this policy is completely useless in the absence of instructions. By contrast, our approach induces a collection of options that can later be employed in sketch- or language-free hierarchical RL sections (as shown in the final experiment in our paper), making the approach much more general. We are not aware of any work that uses natural instructions to induce policy fragments that can be executed even when those instructions are not present. We believe the current work is the first step in that direction.", "title": "Author response"}, "ryTgdZvUe": {"type": "rebuttal", "replyto": "SkSAkQlHe", "comment": "NOVELTY\n\nAs described in the related work section, our approach indeed shares a number of similarities in existing work on learning hierarchical policy representations. Structurally the model is quite similar to the Option--Critic approach of Bacon & Precup; this is not intended to be a contribution of the paper. What we claim to be novel are (1) the training condition (using discrete high-level policy representations without an explicit grounding or feature abstraction hierarchy) and (2) the objective (using the small amount of extra structure in the training data to decouple actors from critics across multiple tasks). You mention that symbolic specifications have been explored in other work---as discussed in our earlier comment, we've done our best to describe the differences with nearest neighbors, but are not aware of any previous approaches that learn with as little high-level supervision as we use here. Again, if you can let us know exactly what you have in mind we would greatly appreciate it!\n\nCURRICULA\n\nThe general curriculum learning approach (Algorithm 2) can construct a sampling distribution for any collection of tasks: it relies only on the sketch length and the current empirical performance of the model, both of which can be computed without any task-specific engineering. The collections of tasks in this paper were in fact designed to give rise to particularly challenging curricula (no length-1 tasks, various subpolicies that appear only as constituents of very long tasks, etc.) and demonstrate robustness to decisions about task selection and curriculum design.\n\nTASKS\n\nWe did most of our development on a restricted subset (only length-2 and length-3 sketches) of crafting domain tasks. Generalization to longer crafting tasks, as well as generalization to the maze domain, worked out of the box without any modifications to the initial task design or tuning of hyperparameters. As some evidence that the task design process is indeed reproducible, we are happy to point to follow-up work by Rob Fergus's group, who have already succeeded in reimplementing our tasks and evaluation for a different model architecture (https://uclmr.github.io/nampi/talk_slides/rob-nampi.pdf).", "title": "Author response"}, "BkePOSlre": {"type": "rebuttal", "replyto": "HyaORazEe", "comment": "Thank you for your comments---we'll provide a detailed response soon. Before we do, could you clarify what kind of comparison with MAXQ-based multiagent RL you'd like to see? We have already provided a baseline based on explicit multitask decomposition of the value function. But the present work has nothing at all to do with multiagent RL, so I'm not quite sure what you have in mind. ", "title": "Re: review3"}, "S1HrPSlHl": {"type": "rebuttal", "replyto": "SkSAkQlHe", "comment": "Thanks for the detailed feedback. We'll provide a full response soon. May I start by asking what previous work you're thinking of that uses symbolic policy specifications? We'd like to get started implementing relevant baselines as soon as possible. The main other approach we're currently aware of is the HAM / ALISP family of models, which are based on explicit decomposition of the Q function rather than optimization of a shared policy. We've already implemented a HAM baseline and found that it performs considerably worse than our own approach. If there are other comparisons we should make, please let us know!", "title": "Re: review 1"}, "r1Rf2Qlrg": {"type": "review", "replyto": "H1kjdOYlx", "review": "No questionsThe paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding \"implementations\" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that specifying tasks in simplified language is easy, designing a curriculum may in fact be pretty complicated, depending on the task at hand.  The examples provided are fairly small, and there is no hint of how curriculum can be designed for larger problems. Because the approach is sensitive to the curriculum, this limits the potential utility of the work. It is also unclear if there is a way to provide supervision automatically, instead of doing it based on prior domain knowledge.\nMore minor comments:\n- The experiments are not described in enough detail in the paper. It's great to provide github code, but one needs to explain in the paper why certain choices were made in the task setup (were these optimized? What's this the first thing that worked?) Even with the code, the experiments as described are not reproducible\n- The description of the approach is pretty tangled with the specific algorithmic choices. Can the authors step back and think more generally of how this approach can be formalized?  I think this would help relate it to the prior work more clearly as well.", "title": "No questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkSAkQlHe": {"type": "review", "replyto": "H1kjdOYlx", "review": "No questionsThe paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding \"implementations\" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that specifying tasks in simplified language is easy, designing a curriculum may in fact be pretty complicated, depending on the task at hand.  The examples provided are fairly small, and there is no hint of how curriculum can be designed for larger problems. Because the approach is sensitive to the curriculum, this limits the potential utility of the work. It is also unclear if there is a way to provide supervision automatically, instead of doing it based on prior domain knowledge.\nMore minor comments:\n- The experiments are not described in enough detail in the paper. It's great to provide github code, but one needs to explain in the paper why certain choices were made in the task setup (were these optimized? What's this the first thing that worked?) Even with the code, the experiments as described are not reproducible\n- The description of the approach is pretty tangled with the specific algorithmic choices. Can the authors step back and think more generally of how this approach can be formalized?  I think this would help relate it to the prior work more clearly as well.", "title": "No questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HyP273r7l": {"type": "rebuttal", "replyto": "rJCRXFJme", "comment": "\nSUMMARY\n\nGreat question! Our approach requires no logical representations and no supervision of action traces, and thus relies on considerably less supervision and prior representational structure than existing work on natural language instruction following. Independently, our model architecture is novel: we are not aware of any work that jointly learns separate policy modules for each instruction fragment. To address the question of empirical comparison, we implemented a new baseline corresponding to the closest applicable approach from the NLP literature. Our approach performs considerably better than this baseline (Section 4). We have updated the paper.\n\nDETAILS\n\n1. Related work: we\u2019ve expanded the discussion in Section 2 to discuss related prior work from NLP in more detail. For convenience, we have included the new text here:\n\n\u201cExisting work on instruction following falls into two broad categories: approaches that require a highly structured (typically logical) action and world representations (e.g. Chen & Mooney 2011, Artzi & Zettlemoyer 2013, Andreas & Klein 2015), and approaches that require detailed supervision of action sequences or dense reward signals essentially equivalent to full action traces (e.g. Branavan et al. 2009, Vogel and Jurafsky 2010, Mei et al. 2016). By contrast, the framework we describe here involves no formal or logical language for describing plans, and no supervised action sequences. Additionally, the modular model described in this paper naturally supports adaptation to tasks where no sketches are available, while all existing instruction following models learn a joint policy over instructions and actions, and are unable to function in the absence of instructions.\u201d\n\n2. Evaluation: we have implemented the model used in the Branavan and Vogel papers in order to provide a more direct comparison. The RL approach in those papers (which for the particular class of Q function approximators we\u2019re using turns out to be equivalent to the HAM baseline in the sibling comment) achieves an overall success rate of 28% on the crafting task and 15% on the maze task (Section 4). So in the more challenging data condition we consider in this work, our improved model architecture appears necessary for good performance.", "title": "comparison to NLP methods"}, "Hko-H3rXl": {"type": "rebuttal", "replyto": "rk2GHekmg", "comment": "\nSUMMARY\n\nThanks---the lack of discussion of the HAM / ALISP family was absolutely an oversight. I had intended to include citations to Parr & Russell (1998) and Andre & Russell (2002); we have added these and other suggested citations to Section 2 of the paper. It is certainly possible to think of our sketches as annotations of the kind employed by HAMs; however, there are significant differences in how these sketches are used and what additional side information is required. Below we discuss the differences between our approach and HAMs in more detail, and describe results from a HAM baseline we implemented. Briefly, (1) our approach makes it much easier to learn abstracted state and policy representations; (2) our approach supports automatically inducing sketches for new tasks with shared structure; (3) empirically, the HAM baseline performs considerably worse than our model on both tasks (Section 4). We have updated the paper online.\n\nDETAILS\n\n1. Our approach supports considerably more powerful state abstraction machinery. Parr & Russell (1998) only consider tabular state representations, and the generalization to state abstractions for ALISP (Andre & Russell, 2002) requires hand-engineered abstraction functions that work only under extremely strong theoretical conditions. In this sense, HAMs actually require more detailed supervision than our approach, which can learn to represent policies with deep function approximators and without feature engineering or formally well-behaved abstraction hierarchies.\n\n2. We are not aware of any work on multitask learning of multiple HAM automata with some shared substructure, or on automatically inducing HAM automata from scratch on novel tasks. We have already demonstrated success with our approach in both settings.\n\n3. The implementation details are themselves quite different, both in the choice of the model architecture (our modular subpolicies vs. their common Q function) and the training objective (our actor-critic vs. their critic-only). To provide a direct comparison, we implemented a family of HAMs for our tasks and trained them as in Andre and Russell (2002). Results have been added to Section 4 of the paper. This approach achieves an overall success rate of 28% on the crafting tasks and 15% on the maze tasks---considerably worse than our approach.\n", "title": "relationship with HAMs"}, "rJCRXFJme": {"type": "review", "replyto": "H1kjdOYlx", "review": "Dear authors,\n \n I am a little bit skeptical about the novelty of the approach proposed here which (as you briefly mention) is very close to what is done when learning to map instructions (natural language) to a policy. In that last case, the task is in fact harder than the one you intend to solve in this paper. Many articles are based on that (you cite some of them,but you could cite dozens of articles). Could you please provide a better comparison between what you are doing and these existing works ?  What are the advantages or drawbacks ?  So my only question is about the novelty since the paper is quite clear and well written (and the tasks are nice), and also about the lack of comparison with some baselines that could be extracted from this literature\n\nThe paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch . The learning algorithm is based on an extension of the actor-critic model for that particular case, and also involves curriculum learning techniques when the task to solve is hard. Experimental results are provided on different learning problems and compared to baseline methods. \n\nThe paper is well-written and very easy to follow. I am not really convinced by the impact of such a paper since the problem solved here can be seen as an option-learning problem with a richer supervision (i.e the sequence of option is given). It thus corresponds to an easier problem with a limited impact. Moreover, I do not really understand to which concrete application this setting corresponds. For example, learning from natural langage instructions is clearly more relevant. So since the model proposed in this article is not a major contribution and shares many common ideas with existing hierarchical reinforcement learning methods,  the paper lacks a strong motivation and/or concrete application. So, the paper only has a marginal interest for the RL community\n\n@pros: \n* Original problem with well design experiments\n* Simple adaptation of the actor-critic method to the problem of learning sub policies\n\n\n@cons:\n* Very simple task that can be seen as a simplification of more complex problems like options discovery, hierarchical RL or learning from instructions\n* No strong underlying applications that could help to 'reinforce' the interest of the approach\n", "title": "pre-review question", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyJDbsb4e": {"type": "review", "replyto": "H1kjdOYlx", "review": "Dear authors,\n \n I am a little bit skeptical about the novelty of the approach proposed here which (as you briefly mention) is very close to what is done when learning to map instructions (natural language) to a policy. In that last case, the task is in fact harder than the one you intend to solve in this paper. Many articles are based on that (you cite some of them,but you could cite dozens of articles). Could you please provide a better comparison between what you are doing and these existing works ?  What are the advantages or drawbacks ?  So my only question is about the novelty since the paper is quite clear and well written (and the tasks are nice), and also about the lack of comparison with some baselines that could be extracted from this literature\n\nThe paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch . The learning algorithm is based on an extension of the actor-critic model for that particular case, and also involves curriculum learning techniques when the task to solve is hard. Experimental results are provided on different learning problems and compared to baseline methods. \n\nThe paper is well-written and very easy to follow. I am not really convinced by the impact of such a paper since the problem solved here can be seen as an option-learning problem with a richer supervision (i.e the sequence of option is given). It thus corresponds to an easier problem with a limited impact. Moreover, I do not really understand to which concrete application this setting corresponds. For example, learning from natural langage instructions is clearly more relevant. So since the model proposed in this article is not a major contribution and shares many common ideas with existing hierarchical reinforcement learning methods,  the paper lacks a strong motivation and/or concrete application. So, the paper only has a marginal interest for the RL community\n\n@pros: \n* Original problem with well design experiments\n* Simple adaptation of the actor-critic method to the problem of learning sub policies\n\n\n@cons:\n* Very simple task that can be seen as a simplification of more complex problems like options discovery, hierarchical RL or learning from instructions\n* No strong underlying applications that could help to 'reinforce' the interest of the approach\n", "title": "pre-review question", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rk2GHekmg": {"type": "review", "replyto": "H1kjdOYlx", "review": "Your paper needs a more thorough discussion of related work. In particular, please discuss in more detail work at your own home institution (Berkeley) by David Andre, Bhaskara Marti, Ron Parr and Stuart Russell on programmable HAMs and their variants. These provide exactly what your stated contributions aspire to achieve, namely high level abstract specifications of behavior using constructs that appear far more complex than the ones listed in your paper (subroutines, recursion, concurrency etc.). \n\nThis paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. \n\nSketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. \n\nExperiments are provided through a standard game like domain (maze, minecraft etc.). \n\nThe paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the context of the specific approach proposed here. Second, the experiments are not thorough enough in terms of comparing with all the related work. For example, Ghavamzadeh et al. explored the use of MAXQ like abstractions in the context of mulitagent RL. It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed. ", "title": "Relation to work on programmable HAMs", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HyaORazEe": {"type": "review", "replyto": "H1kjdOYlx", "review": "Your paper needs a more thorough discussion of related work. In particular, please discuss in more detail work at your own home institution (Berkeley) by David Andre, Bhaskara Marti, Ron Parr and Stuart Russell on programmable HAMs and their variants. These provide exactly what your stated contributions aspire to achieve, namely high level abstract specifications of behavior using constructs that appear far more complex than the ones listed in your paper (subroutines, recursion, concurrency etc.). \n\nThis paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. \n\nSketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. \n\nExperiments are provided through a standard game like domain (maze, minecraft etc.). \n\nThe paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the context of the specific approach proposed here. Second, the experiments are not thorough enough in terms of comparing with all the related work. For example, Ghavamzadeh et al. explored the use of MAXQ like abstractions in the context of mulitagent RL. It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed. ", "title": "Relation to work on programmable HAMs", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}