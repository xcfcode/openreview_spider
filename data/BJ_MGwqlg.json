{"paper": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"], "summary": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "keywords": ["Deep learning"]}, "meta": {"decision": "Reject", "comment": "The reviewers feel that this is a well written paper on floating and fixed point representations for inference with several state of the art deep learning architectures. At the same time, in order for results to be more convincing, they recommend using 16-bit floats as a more proper baseline for comparison, and to analyze tradeoffs in overall workload speedup, i.e broader system-level issues surrounding the implementation of custom floating point units."}, "review": {"r16jP1DLx": {"type": "rebuttal", "replyto": "SyusKkUNx", "comment": "We thank you for your valuable time and comments.\n\n===Search method clarifications\nOur search model only requires a small subset (evaluation uses 10) of the validation inputs to predict accuracy by leveraging all of the activations in the last layer, rather than observing only the top-1 (or top-5) results. Last layer activations require fewer inputs compared to top-1 accuracy, since each DNN input produces many scalar activations rather than a single binary correctness value. For example, 10 inputs using top-1 accuracy provides 10 binary values (i.e. each DNN output being either correct or incorrect), while the activations for 10 ImageNet inputs provides 10,000 scalar values (i.e. 10 DNN inputs * 1000 scalar output activations per ImageNet input).\n\n===Batch normalization\nBatch normalization primarily impacts the training process of DNNs rather than inference, so we expect to arrive at very similar conclusions with or without it. Batch normalization during inference applies a fixed linear transformation to each activation, which requires few hardware resources and does not change the shape of the activation distribution.\n\n===Importance of inference performance\nReducing the computational requirements of DNN inference is relevant to the machine learning community. A number of papers on this topic have been published at venues such as ICLR, ICML, and NIPS[1,2,3,4].\n\n[1] Compressing Neural Networks with the Hashing Trick. ICML 2015. Chen, at al.\n[2] Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation. NIPS 2014. Denton, et al.\n[3] Learning both Weights and Connections for Efficient Neural Networks. NIPS 2015. Han, et al.\n[4] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. ICLR 2016. Han, et al.", "title": "Response to AnonReviewer2"}, "S18_vJwUx": {"type": "rebuttal", "replyto": "SkO-TExSl", "comment": "We thank you for your valuable time and comments.\n\n===Denormal floating-point units\nOur evaluated floating-point ALUs do not operate on denormal floating-point representations. Aside from minor differences in empirical results that depend on denormal floating-point arithmetic support, we expect that our conclusions hold. A different design that operates on denormal floating-point representations will, ideally, allow one less bit in the floating-point exponent for the same accuracy, but will be disadvantaged by increased hardware area, circuit delay, and power.\n\n===Frequency scaling of customized-precision units\nWe scale the ALU frequency with the inverse of the hardware circuit delay, so each customized-precision design can have a different frequency.\n\n===Frequency of compute compared to storage\nOur DRAM memory and compute clocks (frequencies) are different, which is common across almost all hardware designs (e.g., modern CPU/GPU designs). DRAM memory fabrication is primarily optimized for higher transistor density (i.e. memory capacity) rather than higher frequency, which is opposite of computational units.\n\n===Validity of MAC results\nThe MAC operations capture the majority of the DNN performance and power breakdown [1]. Other units will scale with the customized-precision design as well, so we expect that a full-system implementation would show benefits very similar to our results. For example, using 84% [1] as the power used by compute, we find our reported 3.4x savings in energy would become 3.15x (time = 0.84/3.4 + 0.16/(32-bit/14-bit from linear scaling) = 0.31 => speedup = 1/0.31 = 3.15x).\n\n===Compute throughput as DNN bottleneck\nThe DRAM memory bandwidth requirements for DNNs is much lower than its computational requirements [2]. This is due to matrix multiplication, the central DNN computational kernel, requiring roughly N^2 memory operations (i.e. from loop tiling [3]) compared to N^3 arithmetic operations.\n\n===Customized-precision memory access\nDRAM memory access can be left unchanged when using customized-precision compute units. Similar to how GPUs do 32-bit computation efficiently when using a 128- to 512-bit memory bus, a single memory access is distributed to multiple compute units. For example, a 128-bit bus provides data to nine 14-bit compute units.\n\n[1] ShiDianNao: Shifting Vision Processing Closer to the Sensor. ISCA 2015. Zidong Du, et al.\n[2] DjiNN and Tonic: DNN as a Service and Its Implications for Future Warehouse Scale Computers. ISCA 2015. Hauswald, et al.\n[3] Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks. FPGA 2015. ", "title": "Response to AnonReviewer5"}, "H1FVw1w8e": {"type": "rebuttal", "replyto": "SJIZu9_Sg", "comment": "We thank you for your valuable time and comments.\n\n===Performance and power methodology\nWe evaluate our results based on the multiply-accumulate (MAC) operations, the key building block of any hardware implementation of a DNN. The MAC operations capture the majority of the DNN performance and power breakdown [1]. Other units will scale with the customized-precision design as well, so we expect that a full-system implementation would show benefits very similar to our results. For example, using 84% [1] as the power used by compute, we find our reported 3.4x savings in energy would become 3.15x (time = 0.84/3.4 + 0.16/(32-bit/14-bit from linear scaling) = 0.31 => speedup = 1/0.31 = 3.15x). Similarly, full-system performance is dictated by MAC performance.\n\n===ASIC design tool details\nOur comparisons are made using designs synthesized with Synopsys Design Compiler and are further verified using Synopsys PrimeTime and SPICE simulations. The specific cell library, provided by the manufacturer, which produces area, power, and timing information, cannot be disclosed. These cell libraries are used for timing verification for chip tape out, hence, they must (and do) accurately model real hardware.\n\n===Importance of search\nOur search method\u2019s 170x speedup mitigates the time-intensive process of emulating customized precision operations, allowing researchers to iteratively adjust DNN topology for optimized hardware efficiency.\n\n===Suggestions for future work and clarifications\nThank you for the suggestions. We will integrate this feedback into future versions of our work.\n\n[1] ShiDianNao: Shifting Vision Processing Closer to the Sensor. ISCA 2015. Zidong Du, et al.", "title": "Response to AnonReviewer3"}, "ryUr1IU7g": {"type": "rebuttal", "replyto": "HkQJ0t17x", "comment": "===Comparison with binary DNNs===\nCustomized-precision DNNs provide transparent, flexible benefits to existing DNN models by removing inconsequential bits. XNOR-Net[1] and similar works[2,3] operate on binary computational primitives that require changes to the DNN architecture and are inflexible (i.e. either on or off). These problems are manifested when applied to large DNNs. For example, the state-of-the-art XNOR-Net technique applied to GoogleNet does not converge (XNOR-Net[1], table 2), while our customized-precision approach yields over 5x speedup with less than 1% loss in accuracy.\n\n\n===Comparison with DNN compression===\nOur customized-precision DNNs use less precision for computation and storage, while previous works that evaluate compressed DNNs[4,5,6] use less precision for storage *without* optimizing the compute units. Computation precision tuning allows quadratic improvement in throughput due to smaller hardware designs (more parallelism) and shorter circuit delay (higher frequency). Smaller numeric representations in storage are limited to a linear increase in throughput, since storage and bandwidth scale linearly with the size of the data.\n\n\n[1] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. ECCV'16. Rastegari, et al.\n[2] BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations. NIPS'15. Courbariaux, et al.\n[3] Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation.  arXiv'15. Cheng, et al.\n[4] Compressing Deep Convolutional Networks using Vector Quantization. arXiv'14. Gong, et al.\n[5] EIE: Efficient Inference Engine on Compressed Deep Neural Network. ISCA'16. Han, et al.\n[6] Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding. ICLR'16. Han, et al.", "title": "Elaboration on prior work comparison"}, "HJRrAH8Xe": {"type": "rebuttal", "replyto": "SkaankJXx", "comment": "We briefly explored training DNNs with customized precision and found that training requires more precision than inference, since training makes minute adjustments to DNN weights until they converge to their final values. A complete sweep of the customized-precision design space for inference already requires a large amount of time (i.e. the product of several DNNs, hundreds of customized-precision design points, millions of input images, and orders-of-magnitude slowdown from hardware emulation). The same study for training would require further order-of-magnitude increase in experiment runtime, since training is an iterative process.", "title": "Customized-precision training"}, "SkaankJXx": {"type": "review", "replyto": "BJ_MGwqlg", "review": "Did you try training these networks using these customized precisions too ? It would be a bonus if the same hardware could be used for training.The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. \n\nThe paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ?\n\nThe results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring.\n\nOverall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. \n\nI am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.", "title": "training ?", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "SyusKkUNx": {"type": "review", "replyto": "BJ_MGwqlg", "review": "Did you try training these networks using these customized precisions too ? It would be a bonus if the same hardware could be used for training.The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. \n\nThe paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ?\n\nThe results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring.\n\nOverall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. \n\nI am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.", "title": "training ?", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}