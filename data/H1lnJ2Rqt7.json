{"paper": {"title": "LARGE BATCH SIZE TRAINING OF NEURAL NETWORKS WITH ADVERSARIAL TRAINING AND SECOND-ORDER INFORMATION", "authors": ["Zhewei Yao", "Amir Gholami", "Kurt Keutzer", "Michael Mahoney"], "authorids": ["zheweiy@berkeley.edu", "amirgh@berkeley.edu", "keutzer@berkeley.edu", "mmahoney@stat.berkeley.edu"], "summary": "Large batch size training using adversarial training and second order information", "abstract": "Stochastic Gradient Descent (SGD) methods using randomly selected batches are widely-used to train neural network (NN) models. Performing design exploration to find the best NN for a particular task often requires extensive training with different models on a large dataset,  which is very computationally expensive. The most straightforward method to accelerate this computation is to distribute the batch of SGD over multiple processors. However, large batch training often times leads to degradation in accuracy, poor generalization, and even poor robustness to adversarial attacks.  Existing solutions for large batch training either do not work or require massive hyper-parameter tuning. To address this issue, we propose a novel large batch training method which combines recent results in adversarial training (to regularize against ``sharp minima'') and second order optimization (to use curvature information to change batch size adaptively during training). We extensively evaluate our method on Cifar-10/100, SVHN, TinyImageNet, and ImageNet datasets, using multiple NNs, including residual networks as well as compressed networks such as SqueezeNext.  Our new approach exceeds the performance of the existing solutions in terms of both accuracy and the number of SGD iterations (up to 1\\% and $3\\times$, respectively). We emphasize that this is achieved without any additional hyper-parameter tuning to tailor our method to any of these experiments.\n", "keywords": ["adversarial training", "large batch size", "neural network"]}, "meta": {"decision": "Reject", "comment": "I would like to commend the authors on their work engaging with the reviewers and for working to improve training time. However, there is not enough support among the reviewers to accept this submission. The reviewers raised several important points about the paper, but I believe there are a few other issues not adequately highlighted in the reviews that prevent this work from being accepted:\n\n1. [premises] It has not been adequately established that \"large batch training often times leads to degradation in accuracy\" inherently which is an important premise of this work. Reports from the literature can largely be explained by other things in the experimental protocol. Even the framing of this issue has become confused since, although it may be possible to achieve the same accuracy at any batch size with careful tuning, this might require using (at worst) the same number of steps as the smaller batch size in some cases and thus result in little to no speedup. For example see https://arxiv.org/abs/1705.08741 and recent work in https://arxiv.org/abs/1811.03600 for more information. Even Keskar et al. reported that data augmentation eliminated the solution quality difference between their larger batch size and their smaller batch size experiments which indicates that even if noisiness from small batches serving to regularize training other regularization techniques can serve just as well.\n\n2. [baseline strength] The appropriate baseline is standard minibatch SGD w/momentum (or ADAM or whatever) algorithm with extremely careful tuning of *all* of the hyperparameters. None of the popular learning rate heuristics will always work and other optimization parameters need to be tuned as well. If learning rate decay is used, it should also be tuned especially if one is trying to measure a speedup. The submission does not provide a sufficiently convincing baseline.\n\n3. [measurement protocol] The protocol for measuring a speedup is not convincing without more information on how the baselines were tuned to achieve the same accuracy in the fewest steps. Approximating the protocols in https://arxiv.org/abs/1811.03600 would be one alternative.\n\nAdditionally there are a variety of framing of issues around hyperparameter tuning, but, because they are easier to fix, they are not as salient for the decision. \n"}, "review": {"Hyg-zATimE": {"type": "rebuttal", "replyto": "HyxnRTvyx4", "comment": "\n>>> I believe there are a few other issues not adequately highlighted in the reviews that prevent this work from being accepted:\n\nResponse: We would like to thank the AC for taking the time to post the meta-review.  Unfortunately, the major concerns are simply not correct.  Below we address the points raised:\n\n>>> [premises] It has not been adequately established that \"large batch training often times leads to degradation in accuracy\" inherently which is an important premise of this work. \n\nResponse: That is incorrect. Multiple (if not tens) of works have shown that large batch size leads to significant degradation of accuracy (arxiv: 1609.04836, 1802.08241, 1706.02677,1811.12941, 1811.03600, etc). This is a well established phenomena and it is surprising that the area chair brings this point. Even the papers that the AC cites do clearly mention this.\n\n>>>Even the framing of this issue has become confused since, although it may be possible to achieve the same accuracy at any batch size with careful tuning, this might require using (at worst) the same number of steps as the smaller batch size in some cases and thus result in little to no speedup. For example see https://arxiv.org/abs/1705.08741 and recent work in https://arxiv.org/abs/1811.03600 for more information.\n\nResponse: This is incorrect. Large batch does not \u201cat worst\u201d need the same number of steps as smaller batch to achieve the same accuracy. It is not clear to us why the AC would even claim this. Large batch training may not be able to recover baseline if a vanilla SGD is used even with longer training (arxiv:1811.03600 Figure 23).\n\n>>> Even Keskar et al. reported that data augmentation eliminated the solution quality difference between their larger batch size and their smaller batch size experiments which indicates that even if noisiness from small batches serving to regularize training other regularization techniques can serve just as well.\n\nResponse: This is completely incorrect and a misunderstanding of Keskar\u2019s paper by the AC. Keskar\u2019s paper (arxiv: 1609.04836) showed that data augmentation can partially alleviate the generation gap between small and large batch training, and it is not possible to completely close this gap with data-augmentation. \n\n>>> [baseline strength] The appropriate baseline is standard minibatch SGD w/momentum (or ADAM or whatever) algorithm with extremely careful tuning of *all* of the hyperparameters. None of the popular learning rate heuristics will always work and other optimization parameters need to be tuned as well. If learning rate decay is used, it should also be tuned especially if one is trying to measure a speedup. The submission does not provide a sufficiently convincing baseline.\n\nResponse: This is not applicable to our work. We do NOT do any hyper-parameter tuning, and we emphasized this in multiple places in the paper. This was one of the main points of the paper, and it is very disappointing that it has been missed. Moreover, we did not change any hyper-parameters for the baseline neural networks that we used.\n\n>>> [measurement protocol] The protocol for measuring a speedup is not convincing without more information on how the baselines were tuned to achieve the same accuracy in the fewest steps. Approximating the protocols in https://arxiv.org/abs/1811.03600 would be one alternative.\n\nResponse: Please see above, we did not tune any hyper-parameters.\n\n\n\nAlso we should mention that Reviewer 1 is pointing to our own code in his review as a reason that our Hessian backpropogation is not novel.  That is, due to the double blind aspect of this publication venue, a reviewer incorrectly thought that our novel Hessian backpropogation procedure was due to someone else and thus not novel.   It was very frustrating for us that we could not clear this due to double blind review policy. Moreover, the reviewer completely missed the supplementary material (thus mentioning non-existent figures). We had hoped that he would at least read our response but unfortunately that did not happen. Combined with AC\u2019s meta review it seems our paper\u2019s points and efforts to clear up confusions were not helpful.\n\nAt the end, we would like to specially thank Reviewer 2 and Reviewer 3 for taking the time to follow up with our response, and providing their valuable feedback.\n", "title": "Response"}, "HJeN-Bjty4": {"type": "rebuttal", "replyto": "r1eMKZgr14", "comment": "We would like to thank the reviewer for the detailed review and feedback. We will update the paper accordingly.\n", "title": "Thanks a lot"}, "Bkl9upa53m": {"type": "review", "replyto": "H1lnJ2Rqt7", "review": "This paper studies the large batch size training of neural networks, and incorporates adversarial training and second-order information to improve the efficiency and effectiveness of the proposed algorithm. In particular, the authors use second-order information to automatically generate the step size and batch size in each iteration, and apply adversarial training as a regularization method to improve the test performance. Finally, the authors demonstrate their algorithm and compare it with the baseline algorithms on a wide range of datasets. This paper is clearly written and has the following strength:\n\n1.\tThis paper proposes an adaptive method for SGD training, which proves its convergence for strongly convex optimization.\n2.\tThis paper incorporates the adversarial training and robust optimization into the adaptive SGD training, and shows that this combination significantly improves the test performance.\n3.\tThe authors perform experiments on different datasets, which show that the proposed method enjoys less training time and higher accuracy when using large batch size.\n\nHowever, this paper also has the following weakness:\n\n1.\tThe theoretical analysis is somewhat trivial, and the assumption on the objective function is rather strong, which is not consistent with the nonconvex loss functions that are widely applied in training neural networks.\n2.\tTheorem 1 provides convergence rate of SGD on strongly convex objective functions. However, the authors do not carefully characterize the learning rate to ensure that the loss function achieve \\epsilon-accuracy. Moreover, in order to make the last term in (5) be smaller than \\epsilon, the learning rate \\eta_0 should be in the order of O(\\epsilon), which is no longer a tuning-free parameter.\n3.\tThe authors mention that the proposed algorithm converges faster than basic SGD, but it is not clearly demonstrated from Theorem 1.\n4.\tI am confused about how to determine the number of iterations for different algorithm as shown in Table 1s and 2? Do you stop each algorithm when they attain the same training error on the training dataset?\n5.\tIt is also confused that the number of iterations for ABS and ABSA are relatively larger than that of BL (Tables 1 and 2), but the training time of BL is longer than those of ABS and ABSA as reported in Table 3?\n6.\tSome minor flaws. In (9) it should be \\|\\nabla L(\\Theta)\\|^2; in Lemma 3, the expectation on the left side should be taken conditioned on \\theta_t.\n", "title": "interesting work, but the theoretical part is not strong enough", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1emuVOcCQ": {"type": "rebuttal", "replyto": "SkxjN0yHsQ", "comment": "We would like to thank the reviewer for taking the time to review our paper. Below we provide response to the points raised:\n\n\n>>> The approach for computing the eigenvalue of the Hessian is not described. Which eigenvalue is computed? How is this done? What is the batch size used in this computation? Is it computed over the full training set? The Limitations section briefly describes this (power iteration to tolerance <= 10^-2), but this should be elaborated on in Section 3.1\n\nResponse: We should have made this clear in one place in the paper which we will fix. We use power iteration to compute the top eigenvalue of the Hessian, which is mentioned on page 3. The batch size for eigenvalue computation on ImageNet is 4096 (mentioned in Appendix C). In Figure 6, we also point out the batch size is 128 for Cifar-10.  We acknowledge that the discussion on batch size used for eigenvalue computation is not discussed clearly in one place. We will correct this. The important point is that we do not need to compute the eigenvalue on the full training dataset, because as empirically observed in (arxiv: 1810.01021), one can get a very accurate estimate of the eigenvalue by considering sub-sampled Hessian (at least for the problems that we have considered).\n\n>>> The introduction makes this even more confusing by claiming the second order information is computed by \u201cbackpropagating the Hessian operator\u201d. \n\nResponse: The reviewer is right. We do mean Hessian-vector product is obtained by differentiating the product of $g\u2019v$. We will correct this in the revision.\n\n>>> In addition, it was not described how the learning rate is changed in the algorithm.\n\nResponse: The learning rate follows the baseline annealing method for each test considered, except that we change it based on the Hessian information. In particular, when the eigenvalue reduces by a factor of two, we increase the learning rate and batch size by a factor of two (to keep total noise the same according to discussion in arxiv: 1711.04623). We provide the values for the hyper-parameters mentioned (duration factor, kappa, etc) at the end of Appendix B on page 15, which were fixed for all of the experiments performed.\n\n>>> If it is indeed the case that the authors are using power iteration to compute the largest eigenvalue, why not use Lanczos method as it typically works better for symmetric matrices? \n\nResponse: It is possible to use Lanczos. However, the reason we do not use Lanczos is that although the Hessian is mathematically symmetric, we are using single precision which results in numerical error.\n\n>>> It is possible that  the algorithm is utilizing negative curvature information rather than positive curvature information (particularly in the earlier epochs), which may contradict their intuition based on flat minima.\n\nResponse: We only use the magnitude of the eigenvalue as a measure of curvature of the landscape. It is certainly possible to have a negative eigenvalue, but we only change batch size based on the magnitude of the eigenvalue. Relatively large eigenvalue means the loss landscape is \u201csharp\u201d, and vice versa.", "title": "Authors' response 1"}, "HkxY5E_9AQ": {"type": "rebuttal", "replyto": "B1emuVOcCQ", "comment": "\n>>> Secondly, there is no explanation as to why increasing the batch size would lead to consistent decrease in the eigenvalues of the Hessian.\n\nResponse: We kindly note that we did not claim increasing the batch size will decrease Hessian eigenvalues. Actually, we use the Hessian eigenvalue to adaptively change the batch size and learning rate. Also, we did not claim that the eigenvalue of the Hessian should necessarily decrease throughout the training procedure.\n\n>>> Even if the flat minima/sharp minima hypothesis is assumed, is it possible for the iterates after increasing the batch size to still tend towards sharper minimizers after being in a flat region?\n\nResponse: Yes this is theoretically possible.\n\n>>> Lastly, why is the duration factor needed to increase the batch size if the eigenvalue condition fails? if the duration factor is removed, how does the batch size evolve? Is it necessary? \n\nResponse: For a simple quadratic problem, the eigenvalue will not change throughout the training. That is why we added the duration factor. It can, of course, be removed.  But if it is removed, then for cases where the training is converging to a landscape that is close to a quadratic minimum, where Hessian spectrum will not change, the batch size will remain constant, which is not desired.\n\n>>> How is the duration factor tuned?\n\nResponse: We did not tune the duration factor (or any other parameters). We used a duration of 10 epochs for all the experiments. One could tune this too and set it to even a smaller number which may further speed up the convergence.\n\n>>> The authors also prove a theorem bounding the expected optimality gap with adaptive batch sizes. On closer look, this is a simple adaptation of the result by Bottou, Curtis, and Nocedal [2].\n\nResponse: We clearly state the limitations of our theoretical work, and never imply that the the novelty of our paper is in its theory. We provide Theorem 1 to show that our ABS method is actually a converging optimization method for the strong convex problem. This is necessary before this method could be applied to the non-convex problems that we have tested. Furthermore, we have actually cited the Bottou, Curtis, and Nocedal paper in our proof (please see Appendix A).\n\n>>> The paper is missing much work done by Nocedal\u2019s group on increasing batch sizes (some of which utilize the L-BFGS approximation to the Hessian); see [1, 3]. Other relevant work by Sagun, Bengio, and others on large batch training, flat minima, and the Hessian in deep learning ought to be included as well; see [4-7]. \n\nResponse: This is certainly an oversight and we will add these references to related work in the revised manuscript.\n\n>>> Lastly, the algorithm demonstrates some significant improvements on the number of iterations. However, efficiency with respect to epochs is not discussed. \n\nResponse:  We are not sure what the reviewer refers to by efficiency. If the question about ``efficiency\u2019\u2019 is related to the training time, then we have actually provided this in Table 3. If it is related to how the accuracy changes during training, we have provided the training and testing accuracy in Figure 2-5 which shows that the proposed method achieves better or same accuracy for same epochs.\n", "title": "Authors' response 2"}, "Syx8xWO5R7": {"type": "rebuttal", "replyto": "Bkl9upa53m", "comment": "We would like to thank the reviewer for taking the time to review our paper. Below we provide response to the points raised:\n\n\n>>> The authors do not carefully characterize the learning rate to ensure that the loss function achieve \\epsilon-accuracy. Moreover, in order to make the last term in (5) be smaller than \\epsilon, the learning rate \\eta_0 should be in the order of O(\\epsilon), which is no longer a tuning-free parameter.\n\nResponse: We kindly note that we have double checked the proof and it is correct. In Theorem 1, we fixed the learning rate for the whole training procedure to show how the error changes as a function of iterations, \\textit{not necessarily for an O(\\epsilon) error}. If you see the last line of our proof (p. 13 before QED), we have the inequality for how far we are in expectation from L^\\star. You can see that to get \\epsilon accuracy the learning rate has to decay. We will clarify this in the text.\nAnother confusion we would like to clarify concerns the nature of our method. We never claim our method is hyper-parameter free. There is no such optimization method found so far. What we state, a number of times in the paper, is that the hyper-parameter setting of our method is exactly the same for all the diverse experiments that we performed. That is, we did not tailor our hyper-parameters for particular tests to boost the performance of our method, even though our tests involved eight state-of-the-art neural network models on four standard datasets, including ImageNet. We were particularly careful about this, because a solution for large batch size training should not add to the list of hyper-parameters that need to be tuned, as this would actually increase the training time which is antithetical to the goal of large batch size training. \n\n>>> The authors mention that the proposed algorithm converges faster than basic SGD, but it is not clearly demonstrated from Theorem 1.\n\nResponse: We discussed this on page 6 right after Theorem 1. This follows from the fact that (1-b_k\\eta_0c_s) <  (1-\\eta_0c_s). We also show a numerical example in Appendix A to illustrate the faster convergence (see Fig. 3).\n\n>>> Do you stop each algorithm when they attain the same training error on the training dataset?\n\nResponse: No, the number of training epochs is fixed, based on the corresponding paper that introduced each neural network. Other criteria such as stopping based on the same training loss or validation could be used as well, but we want to follow exactly the baseline training procedure for each case. Other works on large batch size training have also followed this strategy (please see arxiv: 1711.00489, 1706.02677 and 1708.03888).  Please see Appendix B where we show the detailed training outline for each of the experiments performed. \n\n>>> I am confused about how to determine the number of iterations for different algorithm as shown in Table 1-2? \n\nResponse: Table 1 and 2 report the number of SGD iterations required to finish these fixed number of epochs. Also, we plot how the training loss changes for different methods during training in Figure 2, 4, 5.\n\n>>> It is also confused that the number of iterations for ABS and ABSA are relatively larger than that of BL (Tables 1 and 2), but the training time of BL is longer than those of ABS and ABSA as reported in Table 3?\n\nResponse: This is a very important confusion, and we will make sure to better explain this in the revision. We need to compare apples to apples, that is, we must compare methods that achieve the same accuracy. Therefore, we need to compare the baseline and ABSA for cases where both achieve the same accuracy. Baseline accuracy significantly drops for large batches, whereas ABSA accuracy does not. So for comparison, we should consider baseline with a batch size that achieves similar accuracy as ABSA, and that only occurs for small batch sizes.\nFor instance, the baseline in Table 2 with a batch size of 16K requires 732 SGD iterations but it only achieves 10.21% accuracy, which cannot be compared with our method which achieves >60% accuracy. To properly compare the timings we need to consider baseline with 128 batch size which requires 93K SGD iterations and achieves 60.41% accuracy. This is one of the major points of the paper, and we will make this more clear in the revision.\n\n>>> Some minor flaws. In (9) it should be \\|\\nabla L(\\Theta)\\|^2; in Lemma 3, the expectation on the left side should be taken conditioned on \\theta _t.\n\nResponse: That is correct. We will fix those in the next version.\n", "title": "Authors' response"}, "HyxCSeO5CQ": {"type": "rebuttal", "replyto": "S1xtFQylaQ", "comment": "We would like to thank the reviewer for taking the time to review our paper. Below we provide response to the points raised:\n\n\n>>> The main description of their method is Algorithm 1 box, which suggests to grow batch size when \"eigenvalue\" is much smaller than the previous eigenvalue. Is that the top eigenvalue? How is it estimated? \n\nResponse: Yes, that is correct, we compute the top Hessian eigenvalue using power iteration. We will clarify this in the revised manuscript. We mentioned in Algorithm 1, when the eigenvalue decays by a factor of 2, the batch size increases proportionally.\n\n>>> Why is that [Hessian spectrum] the criterion? \n\nResponse: The main intuition is to increase batch size only in areas where the loss landscape is flatter. That is we have empirically found that using a larger batch size in regions where the loss has a \u201cflatter\u201d landscape, and using a smaller batch size in regions with a \u201csharper\u201d loss landscape, is very effective in avoiding attraction to local minima with poor generalization. To be precise by effective, the SGD iterations can reduce up to 5x as compared to existing state-of-the-art for large batch training.\n\n>>> Note that for stochastic least squares problem one benefits from later batch sizes in later stages of optimization even though Hessian doesn't change.\n\nResponse: The problem of least square is a good point. That is why we introduce the \u201cDuration Time\u201d in our algorithm to overcome this problem, i.e., we adaptively increase the batch size as the Hessian eigenvalue decreases or stays stable for several epochs (fixed to be ten in all of the experiments). However, we are not claiming that our method increases the batch size in the most possible efficient way. Even without the duration factor, the batch size would remain constant. However, note that our method requires significantly less as compared with other state-of-the-art.\n\n>>> In section Section 4.3 they start talking a bit about computing Hessian, referring to non-existent figure 6 for details of block approximation.\n\nResponse: That is not correct. Perhaps the reviewer has missed the appendix. When Figure 6 is used as a reference in section 4.3, we mentioned in the parenthesis that Figure 6 is in Appendix D.\n\n>>> Authors mention that Hessian computation is not supported in major frameworks but don't provide explanation of how they compute it (did they not use a major framework for ImageNet experiments?).\n\nResponse: In Limitation, we mentioned that most of the existing frameworks do not support (memory) efficient backpropagation of the Hessian, not that they do not support it. We have developed a pure python library as well as a PyTorch library and we will add a reference to it after double-blind review is finished. \n\n>>> Note that a single row of Hessian (hence full Hessian) can be computed in all major frameworks by differentiating an element of the gradient. IE, in PyTorch https://gist.github.com/apaszke/226abdf867c4e9d6698bd198f3b45fb7, and also eigenspectrum of Hessian can be approximated -- https://github.com/noahgolmant/pytorch-hessian-eigenthings\n\nResponse: Unfortunately we cannot completely address this question due to double-blind policy. However, to briefly respond to the question, the first link (https://gist.github.com/apaszke/226abdf867c4e9d6698bd198f3b45fb7) computes the Hessian matrix explicitly. For small-scale problem, one can do that. However, for NNs, there are millions of dimensions, and such approach would be infeasible. We will further address this question after the double-blind period is finished.", "title": "Authors' response"}, "S1eDak_5R7": {"type": "rebuttal", "replyto": "H1lnJ2Rqt7", "comment": "We would like to thank all the reviewers for taking the time to review our work and provide their feedback. Below we provide a general response and then a more detailed response to every reviewer.\n\n\n>>> The assumption of the theoretical results are too strong\n\nResponse: There is confusion regarding the purpose of the theoretical portion of the paper. Our goal in Theorem 1 is merely to show that the ABS method is a converging optimization method for a strongly convex problem. Furthermore, we explained in detail in the limitations section of the paper that this is not the focus of our work and that the assumptions are limited. We will further clarify this in the revised manuscript.\n\n>>> It is already clear that increasing batch size during training can avoid \u201cgeneration gap\u201d in previous work.\n\nResponse: The seminal works of arxiv: 1711.00489, 1712.02029, and 1711.0462 illustrated that batch size could be used to anneal the noise during training. However, two new contributions of our work are: (i) the annealing scheduling proposed in prior work is not necessarily an optimal strategy to achieve fewer SGD iterations, and (ii) we have shown that combining the adaptive batch size with robust optimization results in better generalization, as compared to other large batch methods.\n\nOur method does not require extra hyper-parameter tuning, and we have shown this by the most extensive testing so far on multiple datasets (Cifar-10/100, SVHN, TinyImagenet, ImageNet) and multiple NNs (ResNet, Wide ResNet, SqueezeNet etc).  The results in Tables (1-2,4-8) clearly show that the proposed robust Hessian based method achieves a smaller number of SGD updates (up to 5x) as compared to existing state-of-the-art. We emphasized that this is achieved without any hyper-parameter tuning.  A fair evaluation of this work should be viewed in comparison with the current state-of-the-art.\n", "title": "Summary Response"}, "S1xtFQylaQ": {"type": "review", "replyto": "H1lnJ2Rqt7", "review": "The authors propose using information from the Hessian to grow the batch size as the training progresses. It is well-known that larger batch sizes can be used for later stages of optimization (ie, https://arxiv.org/abs/1711.00489, https://arxiv.org/abs/1706.05699), but they are missing motivation as to why use Hessian information for this.\n\nFurthermore, the description of the algorithm is lacking detail and is essentially unreproducible in current form.\n\nThe main description of their method is Algorithm 1 box, which suggests to grow batch size when \"eigenvalue\" is much smaller than previous eigenvalue. Is that the top eigenvalue? How is it estimated? Why is that the criterion? Note that for stochastic least squares problem one benefits from later batch sizes in later stages of optimization even though Hessian doesn't change.\n\nIn section Section 4.3 they start talking a bit about computing Hessian, referring to non-existent figure 6 for details of block approximation.\n\nAuthors mention that Hessian computation is not supported in major frameworks but don't provide explanation of how they compute it (did they not use a major framework for ImageNet experiments?).\n\nNote that a single row of Hessian (hence full Hessian) can be computed in all major frameworks by differentiating an element of the gradient. IE, in PyTorch https://gist.github.com/apaszke/226abdf867c4e9d6698bd198f3b45fb7, and also eigenspectrum of Hessian can be approximated -- https://github.com/noahgolmant/pytorch-hessian-eigenthings", "title": "Clearly an unfinished paper", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkxjN0yHsQ": {"type": "review", "replyto": "H1lnJ2Rqt7", "review": "Based on my understanding, this paper describes a novel approach for addressing the large batch training problem. The authors propose increasing the batch size based on reductions in the largest eigenvalue of the Hessian. This is combined with adversarial training using the fast gradient sign method to reduce the total number of iterations required for training and improve generalization performance. Unfortunately, although the numerical results seem quite promising, the algorithm and its explanation and details are not described clearly in the paper, which makes me lean towards rejection. I describe this more fully below:\n\n1. Description of the Algorithm\n\nThe description of the algorithm in Section 3.1 is simply not clear, and lacks clear exposition motivating why the algorithm ought to work. To add to this confusion, there appear to be some inconsistencies between the (brief) description of the method and the description given in the Main Contributions and Limitations section in the Introduction. \n\nAs an example, in Section 3.1, the approach for computing the eigenvalue of the Hessian is not described. Which eigenvalue is computed? How is this done? What is the batch size used in this computation? Is it computed over the full training set? The Limitations section briefly describes this (power iteration to tolerance <= 10^-2), but this should be elaborated on in Section 3.1. In fact, the limitations should not be discussed until a clear description of the algorithm is given.\n\nThe introduction makes this even more confusing by claiming the second order information is computed by \u201cbackpropagating the Hessian operator\u201d. This seems to imply that the 3rd derivative information is computed for second-order information. Later in the Introduction, the authors claim to use Hessian matvecs to perform the power iteration. I believe that the authors mean that the Hessian-vector product is obtained by differentiating the product g\u2019v (a scalar quantity). \n\nIn addition, it was not described how the learning rate is changed in the algorithm. Later in the experiments, none of the additional hyperparameters in the procedure are given, such as the duration factor, kappa, the hyperparameters in the adversarial training, and more. This all ought to be included for completeness.\n\n2. Questions about Details of the Algorithm\n\nIf it is indeed the case that the authors are using power iteration to compute the largest eigenvalue, why not use Lanczos method as it typically works better for symmetric matrices? In addition, if the intention was to compute the largest eigenvalue of the Hessian, one must be wary that the power iteration/Lanczos method computes the eigenvalue with largest magnitude (the absolute value of lambda), which may mean that it\u2019s possible that the algorithm is utilizing negative curvature information rather than positive curvature information (particularly in the earlier epochs), which may contradict their intuition based on flat minima. This needs to be addressed.\n\nSecondly, there is no explanation as to why increasing the batch size would lead to consistent decrease in the eigenvalues of the Hessian. This is certainly not true for all optimization problems. Even if the flat minima/sharp minima hypothesis is assumed, is it possible for the iterates after increasing the batch size to still tend towards sharper minimizers after being in a flat region? This intuition and explanation needs to be expanded on (and argued for) in order for the algorithm to make any conceptual sense.\n\nLastly, why is the duration factor needed to increase the batch size if the eigenvalue condition fails? if the duration factor is removed, how does the batch size evolve? Is it necessary? How is the duration factor tuned?\n\n3. Inconsequential Theoretical Results\n\nThe authors also prove a theorem bounding the expected optimality gap with adaptive batch sizes. On closer look, this is a simple adaptation of the result by Bottou, Curtis, and Nocedal [2] and does not utilize any of the algorithmic mechanisms described in the paper. Hence, the theoretical result is not novel, does not provide any additional insight on the algorithm, and could be applied to any adaptive/changing batch size SG algorithm. In my opinion, this ought to be removed. (Assumption 2 is also mentioned in the main paper, but is only described in the Appendix.)\n\n4. Additional Considerations\n\nThe paper is missing much work done by Nocedal\u2019s group on increasing batch sizes (some of which utilize the L-BFGS approximation to the Hessian); see [1, 3].\n\nOther relevant work by Sagun, Bengio, and others on large batch training, flat minima, and the Hessian in deep learning ought to be included as well; see [4-7]. \n\nLastly, the algorithm demonstrates some significant improvements on the number of iterations. However, efficiency with respect to epochs is not discussed. It may make sense to plot test loss/error against epochs and batch size against iterations for clarity.\n\nTypos/Grammatical Errors:\n- Page 2: Should not state \u201c(We refer to this method as ABS)\u201d, easier to include by including (ABS) after Adaptive Batch Size in the beginning of the bullet point.\n- Page 6: Section 4: \u201cinformation\u201d not \u201cinformatino\u201d\n- Page 6: Section 4: \u201cthe\u201d not \u201cteh\u201d\n- Page 7: Section 4.1: \u201cconfirms\u201d not \u201cconfirming\u201d\n- Page 7: Section 4.1: no \u201ca\u201d in \u201ca very consistent performance\u201d\n\nSummary:\n\nOverall, although the paper presents some promising numerical results, it lacks a detailed description and explanation of the algorithm to be worthy of publication. It leaves many aspects of the algorithm open to the reader\u2019s interpretation, and I do not believe I could reproduce the results with the information provided. The manuscript needs significant changes to the detail, structure, and writing before it can be considered for publication.\n\nReferences:\n[1] Bollapragada, Raghu, et al. \"A progressive batching L-BFGS method for machine learning.\"\u00a0arXiv preprint arXiv:1802.05374(2018).\n[2] Bottou, L\u00e9on, Frank E. Curtis, and Jorge Nocedal. \"Optimization methods for large-scale machine learning.\"\u00a0SIAM Review\u00a060.2 (2018): 223-311.\n[3] Byrd, Richard H., et al. \"Sample size selection in optimization methods for machine learning.\"\u00a0Mathematical programming134.1 (2012): 127-155.\n[4] Chaudhari, Pratik, et al. \"Entropy-sgd: Biasing gradient descent into wide valleys.\"\u00a0arXiv preprint arXiv:1611.01838(2016).\n[5] Jastrz\u0119bski, Stanis\u0142aw, et al. \"DNN's Sharpest Directions Along the SGD Trajectory.\"\u00a0arXiv preprint arXiv:1807.05031(2018).\n[6] Sagun, Levent, et al. \"Empirical Analysis of the Hessian of Over-Parametrized Neural Networks.\"\u00a0arXiv preprint arXiv:1706.04454\u00a0(2017).\n[7] Zhu, Zhanxing, et al. \"The Regularization Effects of Anisotropic Noise in Stochastic Gradient Descent.\"\u00a0arXiv preprint arXiv:1803.00195\u00a0(2018).", "title": "Promising numerical results, but lacks clear description and explanation of the algorithm", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}