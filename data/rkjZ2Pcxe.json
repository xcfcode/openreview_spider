{"paper": {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "authors": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V. Le", "Lukasz Kaiser", "Karol Kurach", "Ilya Sutskever", "James Martens"], "authorids": ["arvind@cs.umass.edu", "luke@cs.umass.edu", "qvl@google.com", "lukaszkaiser@google.com", "kkurach@google.com", "ilyasu@openai.com", "jmartens@cs.toronto.edu"], "summary": "Adding annealed Gaussian noise to the gradient improves training of neural networks in ways complementary to adaptive learning algorithms and the noise introduced by SGD.", "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper presents a simple heuristic for training deep nets: add noise to the gradients at time t with variance $\\eta/(1+t)^\\gamma$. There is a battery of experimental tests, representing a large amount of computer time. However, these do not compare to any similar ideas, some of which are theoretically motivated. For example the paper identifies SANTA as the closest related work, but there is no comparison.\n \n We encourage the authors to address these outstanding issues and to resubmit."}, "review": {"ryBhueUVe": {"type": "rebuttal", "replyto": "BJB-ZBxNx", "comment": "We thank the reviewer for their thorough review and comments. With respect to the comment that \u201cmany people may also add it to their repertoire of optimization tricks,\u201d we have indeed found that this is already the case. Since our original preprint came out, many practitioners have used this technique for optimizing their models, including at least 2 submissions to ICLR 2017 which we note in our response to Reviewer 1.\n\nWe too are interested in theoretical examinations and arguments for usefulness of this approach, and we cite some recent work in this area in our Related Work section, including Mobahi\u2019s \u201cTraining Recurrent Networks by Diffusion\u201d which analogizes the noise process to a low-pass filter applied to the objective function.\n\nWe agree that the work is quite similar to Langevin dynamics, but note that the combination with adaptive learning algorithms (which we find to be empirically very important in some experiments), makes it not quite equivalent to classic SGLD.\n", "title": "Author Response"}, "S15rQgL4x": {"type": "rebuttal", "replyto": "HyYuOpMVe", "comment": "\nSummary of our Response:\n1. We did accidentally mischaracterize some recent recurrence-compatible normalization methods in our Related Work, and are uploading a fixed version.\n2. We agree that comparing the effects of normalization- and noise-based approaches to optimizing deep networks is a worthy research goal but those experiments are outside of the scope of this work for several reasons:\n           a. We perform many experiments investigating the effectiveness of annealed adaptive gradient noise in concert with other common optimization strategies, and show improvements on difficult tasks. We think this is in and of itself a valuable contribution.\n           b. The paper is already quite long, and as noted by Reviewer3 many of our experiments are large-scale, conducted on many thousands of computers.\n           c. Most of the relevant work on normalization is not published or peer-reviewed, and some is actually in submission to this same conference (without comparing to our method). \n           d. We do not make the argument that this is the only method to improve optimization for complex neural architectures, but that it is a simple and valuable tool to try out.\n3. With regard to whether the method is useful to the ICLR 2017 community: a preprint of this paper has been available for some time, and the method is already being cited and used by practitioners to improve their own models, some of which are quite contemporary and in submission to this same conference.\n\nFull rebuttal:\n\nWe thank the reviewer for pointing out the error in our Related Work section, which stated that batch-norm type techniques have never been shown to be effective for recurrent networks. While much of the paper including related work and fine-grained experiments are new compared to the previous version of our work, this was an oversight. We are updating the draft\u2019s Related Work section to better summarize the most recent (mostly non-peer reviewed) batch-norm style approaches.\n\nAdding experiments with batch normalization is outside of the scope of this work for several reasons. Firstly, we do not claim to make any arguments about whether our method is better or worse than a recurrence-compatible batch normalization, and view them as orthogonal. The purpose of our paper is to examine gradient noise and its interaction with several common (but by no means all) approaches for training deep architectures. In addition to demonstrating raw improvements on several complex architectures, we also perform a set of experiments to contrast the effect of an annealed, adaptive Gaussian noise from the batch-size dependent SGD noise, dropout, weight (non-adaptive) noise, and non-annealed noise. Because many of our experiments involve thousands of computers, it is not possible to easily conduct every experiment we might want. The paper is already quite long and includes many experiments, and we believe it performs a solid examination of the benefits of these various types of noise and the sort of models that can be best improved by them.\n\nWe also must note that although Weight Normalization and Normalization Propagation were published recently in NIPS 2016 and ICML 2016, the other mentioned algorithms such as Layer Normalization and Recurrent Batch Normalization, are not yet published and peer-reviewed parts of the literature. In fact, Recurrent Batch Normalization is in submission to this same conference. It does not contain any experiments comparing to our gradient noise approach, nor do we believe it necessarily should, as those authors demonstrate that their method is broadly useful compared to some reasonable baselines. Further, Normalization Propagation is proposed for non-recurrent architectures, and the only recurrent architecture considered in the Weight Normalization work is the DRAW model, which is quite different from the recurrent models we experiment with in our paper. \n\nConcerns about peer-review aside, we address the question of the method being out-of-date and otherwise not useful. Because, as the reviewer notes, preprint versions of this work have been available for some time, many other researchers have had the opportunity to cite it and use it in their own work, where it has helped them optimize similar architectures to those we explore in this work. A few recent examples include \u201cProgramming with a Differentiable Forth Interpreter,\u201d by Bo\u0161njak et al., and \u201cNeural Functional Programming\u201d by Feser et al., which are both submitted to this same conference.\n\nWe agree that comparison of the effectiveness of gradient noise when combined with recent applicable batch-norm style methods is a very interesting avenue for future work, but we do not feel that such a comparison is necessary to defend the contribution of this work. Our claim is not that this single method is the only way to get state-of-the-art results for every model, nor that it renders all other methods redundant, but that it constitutes a simple and often valuable tool for optimizing complex architectures.\n", "title": "Author Response"}, "H1-3Bx84x": {"type": "rebuttal", "replyto": "rJLknDeEe", "comment": "Hi Tim,\n\nActually, there is a bunch of new content in this paper compared to the previously available draft. However, we agree that we left out some of the recent work on recurrent-compatible normalization methods in our Related Work section, and are updating it. \n\nExperiments combining normalization with noise are a good idea, and we would be interested to see those results. In our current work, we note a negative result combining gradient noise with standard LSTM language modeling, so these experiments would be most instructive if done on more complex architectures for e.g. program induction and algorithm learning. We discuss why those experiments are beyond the scope of this current paper in our response to Reviewer 1.\n\nBest,\nLuke\n", "title": "recurrent batch-norm"}, "rJLknDeEe": {"type": "rebuttal", "replyto": "rkjZ2Pcxe", "comment": "This work was done quite a while ago, and in the meantime optimization of recurrent neural networks has been improved by various batch normalization-like schemes. Have you tried combining the technique with recurrent batch normalization[1], layer normalization[2] or weight normalization[3]? I'd be very curious to see whether the technique still brings improvements for normalized networks.\n\n[1] https://arxiv.org/abs/1603.09025\n[2] https://arxiv.org/abs/1607.06450\n[3] https://arxiv.org/abs/1602.07868", "title": "Use with batch-normalized recurrent networks"}, "Sk8i9axQe": {"type": "rebuttal", "replyto": "rkRV90hMg", "comment": "In general, it is likely that the performance could increase by careful hyperparameter tuning. With respect to the MNIST experiment, the gradient noise technique is helpful if we do not use the analytically-derived ReLU initialization techniques.  When we use good initialization, we did try tuning for the MNIST model and didn't find any benefit over the settings we ended up recommending, so we didn't report that. The negative results are likely because the learning problem for a well initialized net on MNIST is not difficult enough for the noise based exploration to help.", "title": "The negative results are likely because the learning problem for a well initialized net on MNIST is not difficult enough for the noise based exploration to help"}, "rkRV90hMg": {"type": "review", "replyto": "rkjZ2Pcxe", "review": "Couldn't the lack of careful hyperparameter tuning for the annealing scheme explain some of the negative results? This includes the MNIST experiments where gradient noise didn't seem to help.The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.\n\nThe method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn\u2019t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.\n\nThe paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. \n\nThe proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.\n\nDespite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. \n\n\nPros:\n* The idea is easy to implement.\n* The method is evaluated on a variety of tasks and for very different models.\n* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.\n* The paper is well-written.\n\n\nCons:\n* The idea is not very original.\n* There is no clear theoretical motivation of analysis.\n* Not all the results are convincing.\n", "title": "Couldn't the lack of hyperparameter tuning for the annealing scheme explain some of the negative results?", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJB-ZBxNx": {"type": "review", "replyto": "rkjZ2Pcxe", "review": "Couldn't the lack of careful hyperparameter tuning for the annealing scheme explain some of the negative results? This includes the MNIST experiments where gradient noise didn't seem to help.The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.\n\nThe method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn\u2019t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.\n\nThe paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. \n\nThe proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.\n\nDespite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. \n\n\nPros:\n* The idea is easy to implement.\n* The method is evaluated on a variety of tasks and for very different models.\n* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.\n* The paper is well-written.\n\n\nCons:\n* The idea is not very original.\n* There is no clear theoretical motivation of analysis.\n* Not all the results are convincing.\n", "title": "Couldn't the lack of hyperparameter tuning for the annealing scheme explain some of the negative results?", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}