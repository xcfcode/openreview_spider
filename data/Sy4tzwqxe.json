{"paper": {"title": "Two Methods for Wild Variational Inference", "authors": ["Qiang Liu", "Yihao Feng"], "authorids": ["qiang.liu@dartmouth.edu", "yihao.feng.gr@dartmouth.edu"], "summary": "", "abstract": "Variational inference provides a powerful tool for approximate probabilistic inference on complex, structured models. Typical variational inference methods, however, require to use inference networks with computationally tractable probability density functions. This largely limits the design and implementation of variational inference methods. We consider wild variational inference methods that do not require tractable density functions on the inference networks, and hence can be applied in more challenging cases. As an example of application, we treat stochastic gradient Langevin dynamics (SGLD) as an inference network, and use our methods to automatically adjust the step sizes of SGLD to maximize its convergence speed, significantly outperforming the hand-designed step size schemes.", "keywords": ["Theory"]}, "meta": {"decision": "Reject", "comment": "This paper is both time and topical in that it forms part of the growing and important literature of ways of representing approximate posterior distributions for variational inference that do need need a known or tractable density. The reviewers have identified a number of areas that when addressed will improve the paper greatly. These include: more clearer and structured introduction of methods, with the aim of highlighting how this work adds to the existing literature; careful explanation of the term wild variational inference, especially in context of alternative terms and more on relation to existing work; experiments on higher-dimensional data, much greater than 54 dimensions, to help better understand the advantages and disadvantages of this approach. It if for these reasons that the paper at this point is not yet ready for acceptance at the conference."}, "review": {"SkpL3BEvx": {"type": "rebuttal", "replyto": "Sy4tzwqxe", "comment": "We highly appreciate the time and feedback from all the reviewers, all of which we will take into serious consideration in our revision. We agree that this paper needs to be re-structured significantly. We will significantly clarify the presentation and its relation with existing reference and also strengthen the empirical evaluation. Currently, we have shown that our algorithms work well with multi-modal distributions such as GMM and RBM, and is promising when applied on VAE. ", "title": "Thank you for your review and comments"}, "ryL2SRT7g": {"type": "rebuttal", "replyto": "SyecoUuml", "comment": "Because our algorithm aims to train the step sizes over a finite number of iterations of SGLD (and different iteration step learns a different step size), it does not make sense to talk about its asymptotic convergence property. We did find that it is important to initialize our algorithm with small step sizes that converges asymptotically so that our algorithm can make improvement based on it.", "title": "Stepsize of SGLD"}, "SyecoUuml": {"type": "review", "replyto": "Sy4tzwqxe", "review": "Do you have any idea if the learning step size of SGLD satisfies the convergence property of SGLD?The paper proposes two methods for what is called wild variational inference. \nThe goal is to obtain samples from the variational approximate distribution q \nwithout requiring to evaluate the density q(z) by which it becomes possible to \nconsider more flexible family of distributions. The authors apply the proposed \nmethod to the problem of optimizing the hyperparamter of the SGLD sampler. \nThe experiments are performed on a 1-d mixture of gaussian distribution and \nBayesian logistic regression tasks. \n\nThe key contribution seems to connect the previous findings in SVGD and KSD \nto the concept of inference networks, and to use them for hyperparameter \noptimization of SGLD. This can not only be considered as a rather simple \nconnection/extension, but also the toyish experiments are not enough to convince \nreaders on the significance of the proposed model. Particularly, I'm wondering \nhow the particle based methods can deal with the multimodality (not the simple\n1d gaussian mixture case) in general. Also, the method seems still to require to evaluate\nthe true gradient of the target distribution (e.g., the posterior distribution) for \neach z ~ q. This seems to be a computational problem for large dataset settings. \nIn the experiments, the authors compare the methods for the same number of \nupdate steps. But, considering the light computation of SGLD per update, I think \nSGLD can make much more updates per unit time than the proposed methods, \nparticularly for large datasets. The Bayesian logistic regression on 54 dimensions\nseems also a quite simple experiment, considering that its posterior is close to \na Gaussian distribution. Also, including Hamiltonian Monte Carlo (HMC) with \nautomatic hyperparameter tuning mechanism (like, no u-turn sampler) would be\ninteresting.\n\nThe paper is written very unclearly. Especially, it is not clear what is the exact\ncontributions of the paper compared to the other previous works including the\nauthors' works. The main message is quite simple but most of the pages are \nspent to explain previous works. \n\nOverall, I'd like to suggest to have more significant high-dimension, large scale \nexperiments, and to improve the writing. \n", "title": "question", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hyn0iufNe": {"type": "review", "replyto": "Sy4tzwqxe", "review": "Do you have any idea if the learning step size of SGLD satisfies the convergence property of SGLD?The paper proposes two methods for what is called wild variational inference. \nThe goal is to obtain samples from the variational approximate distribution q \nwithout requiring to evaluate the density q(z) by which it becomes possible to \nconsider more flexible family of distributions. The authors apply the proposed \nmethod to the problem of optimizing the hyperparamter of the SGLD sampler. \nThe experiments are performed on a 1-d mixture of gaussian distribution and \nBayesian logistic regression tasks. \n\nThe key contribution seems to connect the previous findings in SVGD and KSD \nto the concept of inference networks, and to use them for hyperparameter \noptimization of SGLD. This can not only be considered as a rather simple \nconnection/extension, but also the toyish experiments are not enough to convince \nreaders on the significance of the proposed model. Particularly, I'm wondering \nhow the particle based methods can deal with the multimodality (not the simple\n1d gaussian mixture case) in general. Also, the method seems still to require to evaluate\nthe true gradient of the target distribution (e.g., the posterior distribution) for \neach z ~ q. This seems to be a computational problem for large dataset settings. \nIn the experiments, the authors compare the methods for the same number of \nupdate steps. But, considering the light computation of SGLD per update, I think \nSGLD can make much more updates per unit time than the proposed methods, \nparticularly for large datasets. The Bayesian logistic regression on 54 dimensions\nseems also a quite simple experiment, considering that its posterior is close to \na Gaussian distribution. Also, including Hamiltonian Monte Carlo (HMC) with \nautomatic hyperparameter tuning mechanism (like, no u-turn sampler) would be\ninteresting.\n\nThe paper is written very unclearly. Especially, it is not clear what is the exact\ncontributions of the paper compared to the other previous works including the\nauthors' works. The main message is quite simple but most of the pages are \nspent to explain previous works. \n\nOverall, I'd like to suggest to have more significant high-dimension, large scale \nexperiments, and to improve the writing. \n", "title": "question", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyRJiEyQe": {"type": "review", "replyto": "Sy4tzwqxe", "review": "I am wondering how the method would perform in more complicated multi-modal posteriors. The 1d mixture model is way too simple to draw serious conclusions and the Bayesian logistic regression model usually has a unimodal posterior distribution that is often very close to Gaussian (that's why the Laplace approximation usually does very well in the logistic regression model).The authors propose methods for wild variational inference, in which the\nvariational approximating distribution may not have a directly accessible\ndensity function. Their approach is based on the Stain's operator, which acts\non a given function and returns a zero mean function with respect to a given\ndensity function which may not be normalized.\n\nQuality:\n\nThe derviations seem to be technically sound. However, my impression is that\nthe authors are not very careful and honest at evaluating both the strengths\nand weaknesses of the proposed work. How does the method perform in cases in\nwhich the distribution to be approximated is high dimensional? The logistic\nregression problem considered only has 54 dimensions. How would this method\nperform in a neural network in which the number of weights is goint to be way\nmuch larger? The logistic regression model is rather simple and its posterior\nwill be likely to be close to Gaussian. How would the method perform in more\ncomplicated posteriors such as the ones of Bayesia neural networks?\n\nClarity:\n\nThe paper is not clearly written. I found it very really hard to follow and not\nfocused. The authors describe way too many methods: 1) Stein's variational\ngradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy\n(KSD), 4) Lavengin inference network, not to mention the introduction to\nStein's discrepancy. I found very difficult to indentify the clear\ncontributions of the paper with so many different techniques.\n\nOriginality:\n\nIt is not clear how original the proposed contributions are. The first of the\nproposed methods is also discussed in\n\nWang, Dilin and Liu, Qiang. Learning to draw samples: With application to\namortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016\n\nHow does this work differ from that one?\n\nSignificance:\n\nIt is very hard to evaluate the importance of proposed methods. The authors\nonly report results on a 1d toy problem with a mixture of Gaussians and on a\nlogistic regression model with dimension 54. In both cases the distributions to\nbe approximated are very simple and of low dimension. In the regression case\nthe posterior is also likely to be close to Gaussian and therefore not clear\nwhat advances the proposed method would provide with respect to other more\nsimple approaches. The authors do not compare with simple variational\napproaches based on Gaussian approximations.", "title": "Performance in multi-modal posteriors", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJsKfve4x": {"type": "review", "replyto": "Sy4tzwqxe", "review": "I am wondering how the method would perform in more complicated multi-modal posteriors. The 1d mixture model is way too simple to draw serious conclusions and the Bayesian logistic regression model usually has a unimodal posterior distribution that is often very close to Gaussian (that's why the Laplace approximation usually does very well in the logistic regression model).The authors propose methods for wild variational inference, in which the\nvariational approximating distribution may not have a directly accessible\ndensity function. Their approach is based on the Stain's operator, which acts\non a given function and returns a zero mean function with respect to a given\ndensity function which may not be normalized.\n\nQuality:\n\nThe derviations seem to be technically sound. However, my impression is that\nthe authors are not very careful and honest at evaluating both the strengths\nand weaknesses of the proposed work. How does the method perform in cases in\nwhich the distribution to be approximated is high dimensional? The logistic\nregression problem considered only has 54 dimensions. How would this method\nperform in a neural network in which the number of weights is goint to be way\nmuch larger? The logistic regression model is rather simple and its posterior\nwill be likely to be close to Gaussian. How would the method perform in more\ncomplicated posteriors such as the ones of Bayesia neural networks?\n\nClarity:\n\nThe paper is not clearly written. I found it very really hard to follow and not\nfocused. The authors describe way too many methods: 1) Stein's variational\ngradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy\n(KSD), 4) Lavengin inference network, not to mention the introduction to\nStein's discrepancy. I found very difficult to indentify the clear\ncontributions of the paper with so many different techniques.\n\nOriginality:\n\nIt is not clear how original the proposed contributions are. The first of the\nproposed methods is also discussed in\n\nWang, Dilin and Liu, Qiang. Learning to draw samples: With application to\namortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016\n\nHow does this work differ from that one?\n\nSignificance:\n\nIt is very hard to evaluate the importance of proposed methods. The authors\nonly report results on a 1d toy problem with a mixture of Gaussians and on a\nlogistic regression model with dimension 54. In both cases the distributions to\nbe approximated are very simple and of low dimension. In the regression case\nthe posterior is also likely to be close to Gaussian and therefore not clear\nwhat advances the proposed method would provide with respect to other more\nsimple approaches. The authors do not compare with simple variational\napproaches based on Gaussian approximations.", "title": "Performance in multi-modal posteriors", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}