{"paper": {"title": "Analyzing Inverse Problems with Invertible Neural Networks", "authors": ["Lynton Ardizzone", "Jakob Kruse", "Carsten Rother", "Ullrich K\u00f6the"], "authorids": ["lynton.ardizzone@iwr.uni-heidelberg.de", "jakob.kruse@iwr.uni-heidelberg.de", "carsten.rother@iwr.uni-heidelberg.de", "ullrich.koethe@iwr.uni-heidelberg.de"], "summary": "To analyze inverse problems with Invertible Neural Networks", "abstract": "For many applications, in particular in natural science, the task is to\ndetermine hidden system parameters from a set of measurements. Often,\nthe forward process from parameter- to measurement-space is well-defined,\nwhereas the inverse problem is ambiguous: multiple parameter sets can\nresult in the same measurement. To fully characterize this ambiguity, the full\nposterior parameter distribution, conditioned on an observed measurement,\nhas to be determined. We argue that a particular class of neural networks\nis well suited for this task \u2013 so-called Invertible Neural Networks (INNs).\nUnlike classical neural networks, which attempt to solve the ambiguous\ninverse problem directly, INNs focus on learning the forward process, using\nadditional latent output variables to capture the information otherwise\nlost. Due to invertibility, a model of the corresponding inverse process is\nlearned implicitly. Given a specific measurement and the distribution of\nthe latent variables, the inverse pass of the INN provides the full posterior\nover parameter space. We prove theoretically and verify experimentally, on\nartificial data and real-world problems from medicine and astrophysics, that\nINNs are a powerful analysis tool to find multi-modalities in parameter space,\nuncover parameter correlations, and identify unrecoverable parameters.", "keywords": ["Inverse problems", "Neural Networks", "Uncertainty", "Invertible Neural Networks"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a framework for using invertible neural networks to study inverse problems, e.g., recover hidden states or parameters of a system from measurements. This is an important and well-motivated topic, and the solution proposed is novel although somewhat incremental. The paper is generally well written. Some theoretical analysis is provided, giving conditions under which the proposed approach recovers the true posterior. Empirically, the approach is tested on synthetic data and real world problems from medicine and astronomy, where it is shown to compared favorably to ABC and conditional VAEs. Adding additional baselines (Bayesian MCMC and Stein methods) would be good. There are some potential issues regarding MMD scalability to high dimensional spaces, but overall the paper makes a solid contribution and all the reviewers agree it should be accepted for publication."}, "review": {"rJeG6QIc3X": {"type": "review", "replyto": "rJed6j0cKX", "review": "1) Summary\n\nThe authors propose to use invertible networks to solve ambiguous inverse problems. This is done by training one group of Real-NVP output variables supervised while training the other group via maximum likelihood under a Gaussian prior as done in the standard Real-NVP. Further, the authors suggest to not only train the forward model, but also the inverse model with an MMD critic, similar to previous works that used a more flexible GAN critic [1].\n\n2) Clarity\n\nThe paper is easy to understand and the main idea is well-motivated. \n\n3) Significance\n\nThe main contribution of this work is of conceptual nature and illustrates how invertible networks are a promising framework for many inverse problems. I really like the main idea and think it is inspiring. However, the experiments and technical contributions are rather limited. \n\nTheoretical / ML contribution: \n\nUsing an MMD to factorize groups of latent variables is well-known and combining flow-based maximum likelihood training in the forward model with GAN-like objectives in the inverse model has been done before as well.\n\nExperimental contribution: \n\nI am not fully convinced by the experiments. \nThe inverse kinematics experiment shows that the posterior collapses from large uncertainty to almost a point for the right-most joint. This seems like a negative result to me. \nThe medical experiment also seems rather limited, because if I understand correctly the tissue data is artificial and the proposed INN only outperforms competitors (despite ABC) on two out of three measurements. Further, the authors should have explained the experimental setup of the tissue experiment better, as it is not a standard task in the field. \nIn the astronomy experiment figure 4 shows strong correlations between some of the z variables, the authors claim that this is a feature of their method, but I argue that they should not be present if training with the factorial prior was successful. It would be good to show the correlation between y and z variables as well if they show high dependencies, learning was not very successful. Simply eyeballing the shape of the posterior is not enough to conclude independence. \n\nIn summary, even though interesting, the significance of the experimental results is hard to judge and I am a bit worried that if the proposed model is making some strange mistakes on artificial toy-data, how well it will perform on challenging realistic problems. \n\n4) Main Concerns\n\nThe authors claim that specifying a prior/posterior distribution in density modeling is complicated and typically the chosen distributions are too simplistic. This argument is, of course, valid, but they also have the same problem and specify z to be factorial Gaussian. So the same \"hen-and-egg\" problem applies here.\n\nThe authors also seem to suggest that they are the first to train flow-based models in forward and inverse direction, but this has already been done in the flow-GAN paper [1].\n\nMMD does not easily scale to high-dimensional problems, this is not a problem here as all artificial problems considered are very low-dimensional. But when applying the proposed algorithm in realistic settings, one will likely need extensions of MMD, like used in MMD GANs, which would introduce min/max games on both sides of the network. This will likely be hard to train and constitutes a fundamental limitation of the approach that needs to be discussed.\n\n5) Minor Concerns\n\n- Some basic citations on normalizing flows seem to be missing, e.g. [2,3].\n- How does one guarantee that padded regions are actually zero on output when padding input with zeros? Small variance in those dimensions could potentially code important information. Is this considered as part of y or z?\n- The authors require the existence of inverse and set this equal to bijectivity, but injectivity would be sufficient.\n- The authors mention that z is conditioned on y, but in their notation, the conditional density p(z|y) never shows up explicitly. It should be made clear, that p(z)=p(z|y) is a consequence of their additional MMD penalty and only holds at convergence.\n\n[1] Grover et al., \"Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models\"\n[2] Tabak and Turner, \"Density estimation by dual ascent of the log-likelihood\"\n[3] Deco and Brauer, \"Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures\"", "title": "An inspiring idea with weaknesses on theoretical and experimental side", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1l9qhhSAm": {"type": "rebuttal", "replyto": "Skx7PaEXCX", "comment": "\n> \u201cit seems you suggest there is no inherent advantage of your method compared to related approaches\u201d\n\nOur previous comment, \u201cDifferences [between INNs and unrestricted architectures] are subtle\u201d, does not refer to INNs applied to inverse problems, but only to the differences in expressive power between these architectures in general.\nWe were hereby explicitly addressing your comment \u201cI was, in fact, referring to practical limitations, related to insufficient expressiveness of the model\u201d.\n\n> \u201cCan you please either confirm or explain in what sense the proposed INNs have a fundamental theoretical advantage over competing conditional generative models with respect to learning high quality (i.e. asymptotically correct) posteriors?\u201d\n\nWe see the following fundamental advantages of our INN-based method:\n- One can learn the forward process and get the inverse for free (in contrast to e.g. cGAN).\n- Posteriors are not restricted to a particular parametric form (in contrast to classical variational methods).\n- Posteriors can be efficiently computed (in contrast to e.g. ABC).\n- Training converges to the true solution (in contrast to dropout inference).\n- One can efficiently calculate the Jacobian of the mapping (which we do not currently take advantage of).\nTo the best of our knowledge, there are no established approaches with the same properties, beyond the ones discussed in the paper, where INNs are superior.\n\nYour question whether there are alternative ways to achieve the same goal, and which method works best, is very interesting and will be the focus of another paper after publication of our present results. Our discussions with you helped us identify promising candidates for such comparisons, but we do not consider these alternatives as established methods for our problem setting, so that confident conclusions cannot yet be drawn. We as a community are just starting to learn how to make best use of INNs, and their trade-offs relative to traditional networks need to be investigated further. Overall, all experiments performed to date were highly encouraging.", "title": "Advantages of INNs"}, "BJgZjv4c37": {"type": "review", "replyto": "rJed6j0cKX", "review": "The authors propose in this paper an approach for learning models with tractable approximate posterior inference. The paper is well motivated (fast and accurate posterior inference) and the construction of the solutions (invertible architecture, appending vectors to input and output, choice of cost function) well described. From my understanding, it seems this method is also to be compatible with other methods of approximate Bayesian Computation (ABC).\nConcerning the experimental section:\n- The Mixture of Gaussians experiment is a good illustration of how the choice of cost functions influences the solution. However, I do not understand how are the *discrete* output y is handled. Is it indeed a discrete output (problem with lack of differentiability)? Softwax probability? Other modelling choice? \n- The inverse kinematics is an interesting illustration of the potential advantage of this method over conditional VAE and how close it is to ABC which can be reasonably computed for this problem.\n- For the medical application, INN outperforms other methods (except sometimes for ABC, which is far more expensive, or direct predictor, which doesn\u2019t provide uncertainty estimates) over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better. I\u2019m not sure I understand what we are supposed to learn from the astrophysics experiments.\nThe method proposed and the general problem it aims at tackling seem interesting enough, the toy experiments demonstrates well the advantage of the method. However, the real-world experiments are not necessarily the easiest to read. \nEDIT: the concerns were mostly addressed in the revision. ", "title": "Constraining models to enable approximate posterior inference", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "HkehGGd36Q": {"type": "rebuttal", "replyto": "HJgl48AlTX", "comment": "We have uploaded a revised version of the paper, and added your paper to the related work section.\nThank you for the suggestion.\n", "title": "Revised Version"}, "H1gn4WuhTX": {"type": "rebuttal", "replyto": "rJeG6QIc3X", "comment": "We have uploaded a revised version of the paper, thank you again for your suggestions.\nThe changes and additions are highlighted in red font for convenience.\nPlease also note that by adding these changes, our page count increased by half a page beyond the recommended 8 pages.\nIf this presents a problem, we can attempt shorten the paper accordingly.", "title": "Revised Version"}, "H1ltbW_naQ": {"type": "rebuttal", "replyto": "BJgZjv4c37", "comment": "We have uploaded a revised version of the paper, thank you again for your suggestions.\nThe changes and additions are highlighted in red font for convenience.\nPlease also note that by adding these changes, our page count increased by half a page beyond the recommended 8 pages.\nIf this presents a problem, we can attempt shorten the paper accordingly.", "title": "Revised Version"}, "H1gIl-_n6m": {"type": "rebuttal", "replyto": "rJxS7urch7", "comment": "We have uploaded a revised version of the paper, thank you again for your suggestions.\nThe changes and additions are highlighted in red font for convenience.\nPlease also note that by adding these changes, our page count increased by half a page beyond the recommended 8 pages.\nIf this presents a problem, we can attempt shorten the paper accordingly.", "title": "Revised Version"}, "H1gxtiJq6m": {"type": "rebuttal", "replyto": "ryeDzcZ867", "comment": "As you suggested, we incorporated the IAF from [1] into our cVAE. That is, we inserted the IAF subnetwork between the existing encoder and decoder, but didn\u2019t use the more complex decoder from [1], as it did not improve results and destabilized the training. \nIntroducing IAF improved results measurably over plain cVAE, at the cost of a larger network. Now, the performance is on par with the INN on the 8 Gaussian mode experiment, but a noticeable gap remains for the inverse kinematics and medical experiments.\nQualitatively, cVAE-IAF exhibits the same shortcomings as the cVAE, but with reduced magnitude.\n\nThe measurements from Table 1 for the cVAE-IAF model are as follows:\nCalibration error: 1.40%\nMAP error s_O2: 0.050 \u00b1 0.002\nMAP error all: 0.74 \u00b1 0.03\nMAP resimulation error: 0.313 \u00b1 0.008\n\nSampled posteriors for each experiment, comparing INN, cVAE and cVAE-IAF:\nhttps://i.imgur.com/s2PECtl.jpg\n\nWe will upload the revised paper later this week.\n\nContrary to intuitive expectations, we (and others) found, that the expressive power of INNs relative to unconstrained networks of comparable size, is not substantially reduced. Differences are subtle, and looking at single experiments in isolation may be misleading.\n\nDefinitive statements should be based on systematic comparisons along various degrees of freedom:\n- INNs (trained bi-directionally) vs. auto-encoders (trained for cycle consistency), each with several subtypes and network sizes\n- different unsupervised losses (adversarial, MMD, maximum likelihood, information theoretical)\n- different applications and problem sizes\n\nIdeally, the experiments should include more traditional Bayesian methods for the prediction of posteriors as well, e.g. accelerated MCMC and Stein point sampling.\nIt will also be interesting to investigate if novel training or prediction schemes enabled by the INNs\u2019 tractable Jacobians can compensate for their potentially reduced expressive power.\n\nWe are currently setting up such experiments and will report about our findings in a future paper. In the present paper, we would like to keep the focus on demonstrating that high-quality posteriors can be learned with bi-directional training as facilitated by INNs.\n\n[1] Kingma, Diederik P., et al. \"Improved variational inference with inverse autoregressive flow.\" Advances in Neural Information Processing Systems. 2016.", "title": "IAF Baseline"}, "rygRUCn4am": {"type": "rebuttal", "replyto": "BJgZjv4c37", "comment": "Thank you very much for your time, and your constructive comments, we are looking forward to further discussions!\nWe answer your questions and concerns in the following. \n\n> \"However, I do not understand how are the *discrete* output y is handled.\"\n\nFor this toy problem, we represent labels y by standard one-hot encoding, and we directly regress one-hot vectors using squared loss instead of softmax. This allows us to input one-hot vectors into the inverted network to generate conditional x-samples.\n\n> \"I\u2019m not sure I understand what we are supposed to learn from the astrophysics experiments.\"\n\nWe included this experiment to demonstrate that we are able to find multi-modal posteriors in a second real-world setting relevant to natural science.\n\n> \"INN outperforms other methods [...] over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better\"\n\nWe indeed consider the calibration errors (reported in Sec. 4.2 (\u201cQuantitative results\u201d) and Appendix Sec. 6) the most meaningful of these comparisons, because they directly measure the quality of the estimated posterior distributions, and INNs have a clear lead here.\nWe will add these numbers to Table 1 to emphasize their importance.\n\n> \"However, the real-world experiments are not necessarily the easiest to read.\"\n\nWe understand, although we tried our best to condense the complicated nature of these applications. For the astrophysics setting, we provide more information in the appendix, Sec. 5, and for the medical application we refer to [1] for full details.\n\n[1] Wirkert et al.: Robust near real-time estimation of physiological parameters from megapixel multispectral images with inverse monte carlo and random forest regression. International Journal of Computer Assisted Radiology and Surgery, 2016.\n(https://link.springer.com/article/10.1007/s11548-016-1376-5 )", "title": "Re: Constraining models to enable approximate posterior inference"}, "SylKBy6Eam": {"type": "rebuttal", "replyto": "rJxS7urch7", "comment": "Thank you very much for your time, and your constructive comments, we are looking forward to further discussions!\nWe answer your questions and concerns in the following. \n\n> \"The advantage of INN is not crystal clear to me versus other generative methods such as GAN and VAE.\"\n\nIt is indeed possible to adapt other network types to the task of predicting conditional posteriors. We are currently setting up experiments for detailed analysis of the respective advantages and disadvantages and will report about these results in a future paper. In the present paper, we focus on demonstrating that high-quality posteriors can actually be learned using bi-directional training as facilitated by INNs.\n\nConcerning the comments/questions:\n1.\n> \"could the authors elaborate on the comparison against cGAN\"\n\ncGAN generators are at an inherent disadvantage relative to INNs, because they never see ground-truth pairs (x,y) directly -- they are only informed about them indirectly via discriminator gradients. This it not a problem for simple relationships, e.g. between images x and attributes y, and cGANs work very well there. However, it makes learning of complicated forward processes much harder and may cause the resulting posteriors to be inaccurate. Moreover, INNs are forced to embed every training point x somewhere in the latent space, whereas cGAN generators may fail to allocate latent space for some x, because this is never explicitly penalized. This can lead to mode collapse and insufficient diversity.\n\n> \"Can cGAN be used to estimate the density of X (posterior or not)?\"\n\ncGANs can in principle do this by choosing a generator architecture with tractable Jacobian (using e.g. coupling layers or autoregressive flow), but we are not aware of published results about this possibility.\n\n2.\n> \"For the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)?\"\n\nYes, the weights of the losses are considered as hyperparameters, because the magnitude of MMD-based losses depends on the chosen kernel function. Hyperparameter optimization suggested an up-weighting of MMD-based losses by a factor of 5, to give them approximately equal impact as the supervised loss.\nFor the iterations, we accumulated gradients over one forward and one inverse network execution before each parameter update. We also tried alternating parameter updates after each forward and backward pass, which resulted in equal accuracy, but was a bit slower. We did not experiment with other ratios than 1:1.\n\n3. \n> \"Is this to effectively increase the intermediate network dimensions?\"\n\nThis is precisely the reason: It improves the representational power of the INN, as mentioned in Sec. 3.2 and discussed in our response to reviewer 1.\nAt present, we find this is only necessary for the toy problem in Fig. 2.\n\n> \"It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z).\"\n\nThis is correct.\nWe explicitly prevent information from being hidden in the padding dimensions in the following way:\nA squared loss ensures that the amplitudes are close to zero.\nIn an additional inverse training pass, we overwrite the padding dimensions with noise of the same amplitude, and minimize their effect via a reconstruction loss.\nWe will add this to the relevant paragraph in the paper.\n\n4.\n> \"I am curious if this model could succeed on higher dimensional data\"\n\nWorks such as [1, 2, 3] (also cited in our paper) have shown that the coupling layer architecture in general works well with images. These works use maximum likelihood training, i.e. exploit the tractable Jacobians to maximize the likelihood of the data embedding in latent space. To scale-up our approach, we may need to replace MMD loss with maximum likelihood as well, and first experiments with this show promising results, see \nhttps://i.imgur.com/ft09Pk9.png .\n\n[1] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv:1605.08803, 2016.\n[2] Diederik P Kingma and Prafulla Dhariwal.  Glow: Generative flow with invertible 1x1 convolutions. arXiv:1807.03039, 2018\n[3] Schirrmeister, Robin Tibor, et al. \"Generative Reversible Networks.\" arXiv:1806.01610, 2018", "title": "Re: Invertible network with observations for posterior probability of complex input distributions with a theoretical valid bidirectional training scheme."}, "HJxI8nnV6m": {"type": "rebuttal", "replyto": "SkgZ2oh4aX", "comment": "> \"The authors claim that specifying a prior/posterior distribution in density modeling is complicated and typically the chosen distributions are too simplistic. This argument is, of course, valid, but they also have the same problem and specify z to be factorial Gaussian. So the same \"hen-and-egg\" problem applies here.\"\n\nWe respectfully disagree with this statement.\nWe only argue that restrictions to the posterior p(x|y) are problematic. In contrast, restricting the latent distribution p(z) to a Gaussian poses no serious limitation, thanks to a theorem in [1]: This paper proves under mild assumptions that any distribution over vectors u can be nonlinearly transformed into a distribution over vectors v, whose elements v_i are independently uniformly distributed in [0,1]^m (\u201cnonlinear independent component analysis\u201d). \nThe uniform distribution can easily be transformed to a Gaussian (or any other desired prior) with standard transformations.\nTherefore, as long as the neural network is powerful enough and assumptions are fulfilled, it can always realize the transformation from Gaussian p(z) to any arbitrary p(x|y) at any desired accuracy. \nNote that these properties are not specific to our INN setup, but apply to all models of \u201cnormalizing flow\u201d-type.\n\n[1] A. Hyv\u00e4rinen and P. Pajunen. Nonlinear Independent Component Analysis: Existence and Uniqueness results. Neural Networks 12(3): 429--439, 1999.\n(https://www.cs.helsinki.fi/u/ahyvarin/papers/NN99.pdf )\n\n> \"MMD does not easily scale to high-dimensional problems, this is not a problem here as all artificial problems considered are very low-dimensional. But when applying the proposed algorithm in realistic settings, one will likely need extensions of MMD, like used in MMD GANs, which would introduce min/max games on both sides of the network.\"\n\nOur paper intentionally includes two real-world examples in order to demonstrate that there are plenty of low-dimensional applications, which will directly profit from our MMD-based solution. \nScaling MMD to high dimensions is indeed not easy, and other losses (maximum likelihood, adversarial) may be superior.\nThe following figure shows preliminary results of a forthcoming paper on this subject, where we train using maximum likelihood in conjunction with a supervised classification loss, to enable conditional generation by INNs:\nhttps://i.imgur.com/ft09Pk9.png\n\n> \"- Some basic citations on normalizing flows seem to be missing, e.g. [2,3].\"\n\nThank you for pointing these out. It is fascinating to see that some key ideas were already invented 25 years ago. We will add these references.\n\n> \"- How does one guarantee that padded regions are actually zero on output when padding input with zeros? Small variance in those dimensions could potentially code important information. Is this considered as part of y or z? \"\n\nWe explicitly prevent information from being hidden in the padding dimensions in the following way:\n* A squared loss ensures that the amplitudes are close to zero.\n* In an additional inverse training pass, we overwrite the padding dimensions with noise of the same amplitude, and minimize their effect via a reconstruction loss.\nNote that zero padding of the input is only necessary for the toy problem in Fig. 2, because the width of the resulting network would be too small otherwise.\nWe consider the padding part of y, as it has a supervised loss.\nWe will add this to the relevant paragraph in the paper.\n\n> \"- The authors require the existence of inverse and set this equal to bijectivity, but injectivity would be sufficient.\"\n\nWe think that bijectivity is required for bi-directional training to be well-defined.\nSince the coupling architecture is bijective by construction, the distinction has no practical implications for our method.\n\n> \"- The authors mention that z is conditioned on y, but in their notation, the conditional density p(z|y) never shows up explicitly. It should be made clear, that p(z)=p(z|y) is a consequence of their additional MMD penalty and only holds at convergence.\"\n\nYou are right, we will make this clear in our revised text.\n\n> \"[...] I am a bit worried that if the proposed model is making some strange mistakes on artificial toy-data, how well it will perform on challenging realistic problems.\"\n\nWe feel that this statement might be due to the misunderstandings discussed in the answers above.\nThere is no indication, quantitatively or otherwise, that our model is behaving incorrectly or unexpectedly in any of the experiments.\nIf this does not answer your concerns, we will be happy to provide further clarifications and additional data.", "title": "Re: An inspiring idea with weaknesses on theoretical and experimental side (Part 2)"}, "SkgZ2oh4aX": {"type": "rebuttal", "replyto": "rJeG6QIc3X", "comment": "Thank you very much for your time, and your constructive comments, we are looking forward to further discussions!\nWe answer your questions and concerns in the following. \nNote that we split the response into two comments, due to the 5000 character limit.\n\n> \"The inverse kinematics experiment shows that the posterior collapses from large uncertainty to almost a point for the right-most joint. This seems like a negative result to me.\"\n\nThis comment made us realize that the description/illustration of experiment 2 may not have been clear enough.\nThe rightmost circle marker is not a joint, but the end effector (\u2018hand\u2019) of the arm.\nThe conditioning variable y is the position of this hand.\nTherefore, having the hand located on or near the gray cross is the desired outcome of the experiment, not a failure.\nThe thick contour line does not represent the posterior p(x|y), but indicates the re-simulation error: It is the 97%-confidence region of the model\u2019s end-point distribution p(y|y_target) = integral p(y|x) p(x|y_target) dx and should be as small as possible (ideally, a delta(y - y_target) is desired).\nThe ABC result (leftmost panel) is essentially the ground truth posterior. \nWe will replace Fig. 3 with the following improved illustration, to clarify the setup and show what the arm\u2019s degrees of freedom are:\nhttps://i.imgur.com/nNMdwPA.png\n\n> \"The medical experiment also seems rather limited, because if I understand correctly the tissue data is artificial and the proposed INN only outperforms competitors (despite ABC) on two out of three measurements. \"\n\nConcerning the artificial nature of the medical experiment:\nMedical researchers must resort to simulation, because so far there is no way to create real training data from living tissue.\nThese simulations are sufficiently realistic that they are currently used in clinical trials during actual surgery, albeit only with point estimate methods. \nThe medical scientists consider our approach a major leap forward, because our full posteriors allow them to quantify uncertainty reliably and efficiently for the first time, especially regarding possible ambiguities arising from multi-modal posteriors.\n\nConcerning the performance measures:\nTo compare posteriors, the calibration errors reported in Sec. 4.2 (\u201cQuantitative results\u201d) and Appendix Sec. 6 are the most meaningful performance metrics, and the INN has a clear lead here.\nWe will add these numbers to Table 1 to emphasize their importance.\nThe numbers in the current Table 1 refer to MAP estimate accuracy, where alternative methods may be competitive, even if their estimated posteriors or uncertainties are inferior.\n\n> \"In the astronomy experiment figure 4 shows strong correlations between some of the z variables, the authors claim that this is a feature of their method, but I argue that they should not be present if training with the factorial prior was successful. It would be good to show the correlation between y and z variables as well if they show high dependencies, learning was not very successful.\"\n\nThere seems to be a misunderstanding, the paper does not show the correlation matrix of the latent z variables. \nInstead, the matrices in Figs. 4 and 5 (right) show the correlation of the x-variables for some fixed y.\nIt is a distinguishing feature of our method that we can uncover correlations in the posterior p(x|y), which are not visible in the marginals p(x_i|y) or a mean-field approximation.\nWe verify correctness of the correlations in Fig. 4 via comparison to (expensive) ABC.\n\n> \"The authors also seem to suggest that they are the first to train flow-based models in forward and inverse direction, but this has already been done in the flow-GAN paper [1]. \"\n\nThank you for pointing out that their \u2018hybrid\u2019 strategy is equivalent to bi-directional training. We will change the related work and Sec. 3.3, to properly appreciate their pioneering contributions. Note that we did not make any claims to be the first to use bi-directional training.\n", "title": "Re: An inspiring idea with weaknesses on theoretical and experimental side"}, "rJxS7urch7": {"type": "review", "replyto": "rJed6j0cKX", "review": "While the invertible model structure itself is essentially the same as Real-NVP, the use of observation variables in the framework with theoretically sound bidirectional training for safe use of the seemingly na\u00efve inclusion of y (i.e., y and z can be independent). Its abilities to model the posterior distributions of the inputs are supported by both quantitative and qualitative experiments. The demonstration on practical examples is a plus. \n\nThe advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE. This is an interesting paper overall, so I am looking forward for further discussions.\n\nPros:\n1.\tExtensive analyses of the possibility of modeling posterior distributions with an INN have been shown. Detailed experiment setups are provided in the appendix.\n\n2.\tThe theoretical guarantee (with some assumptions) of the true posterior might be beneficial in practice for relatively low-dimensional or less complex tasks.\n\nComments/Questions:\n1.\tFrom the generative model point of view, could the authors elaborate on the comparison against cGAN (aside from the descriptions in Appendix 2)? It is quoted \u201ccGAN\u2026often lack satisfactory diversity in practice\u201d. Also, can cGAN be used estimate the density of X (posterior or not)?\n\n2.\tFor the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)? This question comes from my observation that the nature of the losses, especially for L_y vs. L_y,L_x (i.e., SL vs. USL) seem to be different.\n\n3.\t\u201cwe find it advantageous to pad both the in- and output of the network with equal number of zeros\u201d: Is this to effectively increase the intermediate network dimensions? Also, does this imply that for both forward and inverse process those zero-padded entries always come out to be zero? It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z).\n\n4.\tIt seems that most of the experiments are done in relatively small dimensional data. This is not necessarily a drawback, I am curious if this model could succeed on higher dimensional data (e.g., image), especially with the observation y.\n", "title": "Invertible network with observations for posterior probability of complex input distributions with a theoretical valid bidirectional training scheme.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hkx3D2TaoX": {"type": "rebuttal", "replyto": "H1xOO9K9sX", "comment": "Thank you for your interest and your comment. We address your questions in order:\n\n* We are not sure what is meant by this question. We simply concatenate y and z into a single vector, and compute the derivatives with respect to this.\n\n* We use u and v to generically denote the in- and output of each coupling block. For instance, u = x for the first coupling block, and v = [y,z] for the last.\n\n* This is correct. However, as illustrated in the image below, each coupling block consists of two affine transformations. The first of these has an upper triangular Jacobian, and the second has a lower triangular Jacobian. The argument concerning the triangular Jacobians applies to each affine transformation separately. A more in-depth look at the Jacobians of affine coupling layers can be found in Dinh et al. (https://openreview.net/forum?id=HkpbnH9lx Sec. 3.2 and 3.3).\n\nSchematic illustration of coupling block:\nhttps://i.imgur.com/XdccxeA.png\n\n* As far as we know, we are the first to apply loss functions on both ends of the same network. Our ablations in Fig. 2 and Table 1 show that the method works best when making full use of that. On the practical side, we perform a parameter update once gradients from all loss terms have been accumulated -- an approach also known from GAN training. In our experiments, we found that alternating forward and inverse parameter updates did not affect training results, but increased training time by ~5%.  \n\n* L_z is defined as the MMD between the network outputs q(y, z), and the target distribution p(y, z). In our case, y and z are explicitly independent in the target distribution: p(y, z) = p(y)p(z).  When the MMD converges to zero, q is necessarily equal to p, therefore the y and z outputs are asymptotically independent. At present, we do not explicitly differentiate between residual dependency of y and z, and other types of mismatch between the distributions in the case of non-zero loss.  \n\n* The network architecture depends on two problem characteristics: Problem dimensionality dictates the width of the layers, and the complexity of the forward process we wish to learn determines the required depth.  We did a coarse grid search to roughly determine the smallest network needed for each application. We will supply ablation studies showing the effect of a larger or smaller number of coupling layers for each of our applications in the following days.\n\n* This is true, the influence of L_x is felt on finite training sets. We meant to say that it plays a smaller role in Table 1 than e.g. in Fig. 2. We will correct our wording in the relevant sections.", "title": "Re: Several questions about claims, text clarifications"}}}