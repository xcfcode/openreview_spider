{"paper": {"title": "Self-Supervised Learning of Compressed Video Representations", "authors": ["Youngjae Yu", "Sangho Lee", "Gunhee Kim", "Yale Song"], "authorids": ["~Youngjae_Yu1", "~Sangho_Lee1", "~Gunhee_Kim1", "~Yale_Song1"], "summary": "We propose a self-supervised approach to learning compressed video representations.", "abstract": "Self-supervised learning of video representations has received great attention. Existing methods typically require frames to be decoded before being processed, which increases compute and storage requirements and ultimately hinders large-scale training. In this work, we propose an efficient self-supervised approach to learn video representations by eliminating the expensive decoding step. We use a three-stream video architecture that encodes I-frames and P-frames of a compressed video. Unlike existing approaches that encode I-frames and P-frames individually, we propose to jointly encode them by establishing bidirectional dynamic connections across streams. To enable self-supervised learning, we propose two pretext tasks that leverage the multimodal nature (RGB, motion vector, residuals) and the internal GOP structure of compressed videos. The first task asks our network to predict zeroth-order motion statistics in a spatio-temporal pyramid; the second task asks correspondence types between I-frames and P-frames after applying temporal transformations. We show that our approach achieves competitive performance on compressed video recognition both in supervised and self-supervised regimes. \n", "keywords": ["Compressed videos", "self-supervised learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "Reviewers liked the self-supervised learning of compressed videos, noting that it is an \"exciting topic\" and an \"important problem\", although they found the proposed methods (PMSP andCTP) less exciting. Reviewers were satisfied with the execution and the extensive experimental studies. AC felt the community may benefit from the paper's intuitive integration of self-supervised learning and the compressed video's signals (I and P frames, residuals, motion vectors, etc). "}, "review": {"56ZnZxJmAh0": {"type": "rebuttal", "replyto": "7R3cLWxnCPp", "comment": "Thank you for the thoughtful comments and a recommendation to accept our paper! We understand somewhat lukewarm sentiment towards our algorithmic treatments to exploit the inherent structure of compressed videos. As the first attempt to self-supervised compressed video understanding, we wanted to use simple and intuitive techniques to clearly show the benefit of our approach. We appreciate the comment that the findings of our work will be \u201cvaluable to the broad video community and thus will have a good impact.\u201d We do hope that our results will encourage the community to follow up with effective and more novel algorithms to bring further improvements.\n", "title": "Appreciate the positive (even though somewhat lukewarm) feedback!"}, "iVW-Upf470R": {"type": "review", "replyto": "jMPcEkJpdD", "review": "Summary:\nThe authors study a new problem \u2013 self-supervised learning for compressed video understanding. It can dramatically save the compute and storage requirements for action recognition. The experiment results on several datasets demonstrate that the proposed approach can achieve the best results over the baselines.\n\nStrength:\n1. Self-supervised compressed video understanding is an exciting topic. It can save computation and storage for online video understanding.\n2. The two pretext tasks are efficient and straightforward for compressed video understanding.\n \nQuestions:\n\n1. Figure 2 is misleading, T_I is k times smaller than T_M,3D Deconv should be on the top and 3D Conv should be on the bottom; the image cubes are also incorrect -- they are supposed to be placed in H-W-C plane. Please re-organize the text and the figures to make the main text clear.\n2. The authors design two pretext tasks according to the heuristic: pyramidal motion statistics prediction and correspondence type prediction. I want to say the authors have done a fair evaluation, but the approach's novelty is limited. I would like to see more discussion about the intuition of the proposed method.\n3. Why not compare with the contrastive learning-based approaches for self-supervised learning. We can still get the positive pairs from the same video, though it is difficult to augment the  videos give the compressed videos.", "title": "Good topic but the approach lacks novelty", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SYycWLkU9b9": {"type": "rebuttal", "replyto": "Uq4FdrVF9aD", "comment": "Thanks for further clarification! Now we see where the confusion comes from -- we changed where the T dimension appears between the input cubes and the feature cubes. We did this to visually emphasize how C changes at different stages of inference. We have revised Fig 2 and caption to clarify that the T-HW-C coordinate system is for the \"feature tensors (blue and orange cubes)\" to avoid further confusion. ", "title": "We revised Fig. 2 and caption again (ever so slightly)"}, "iUlcrolsEJp": {"type": "rebuttal", "replyto": "WjWSlMJEuDP", "comment": "We appreciate your positive comments and thoughtful feedback. \n\n* **Missing references in Table 3:** We have added the three SoTA results in Table 3, indicating that they all use an additional audio modality for learning and are based on decoded RGB frames. While we feel that these baselines wouldn\u2019t necessarily be the most direct comparison to ours, the results show that there is still room for improvements.\n\n* **Missing details**\n\n    * We pretrained our model for 20 epochs including the warm-up period of 5 epochs. We then finetuned our model for 500 epochs on UCF-101 and for 300 epochs on HMDB-51, including a warm-up period of 30 epochs.\n\n    * Table 2 reports FLOPs and per-frame speed of both the preprocessing step and the inference step, following the convention in the compressed video understanding literature, e.g., CoViAR and DMC. As we can see, the biggest speed difference comes from the preprocessing step (which measures how long it takes to load the video data onto the memory): the two non-compressed models, i.e., ResNet152 and R(2+1)D, take 26x longer than the compressed-based models. This is because the non-compressed models need to decode RGB frames on-the-fly. \n\n* **AssembleNet and AssembleNet++:** We have added both references to our revised paper.", "title": "Thank you! We incorporated your thoughtful suggestions to our revision"}, "CycdFCNvxkw": {"type": "rebuttal", "replyto": "iVW-Upf470R", "comment": "We appreciate your constructive comments and an acknowledgement that our work studies \u201ca new problem\u201d and \u201can exciting topic\u201d of self-supervised compressed video understanding. \n\n* **Figure 2:** \n\n    * Thanks for pointing out that the Deconv and 3D Conv blocks are misplaced. This was our mistake and we have fixed it in the revision.\n\n    * The image cubes are actually correct; we had a coordinate sign specifying that the cubes are in the T-HW-C plane. The updated figure now highlights the coordinate system in red. \n\n* **Novelty & intuition:** Our novelty mainly comes from the way we exploit the inherent structure of compressed videos for self-supervised representation learning. As our response to R4 below indicated, we wanted to use simple and intuitive techniques to clearly show the benefit of our ideas. The reviewer is making a great point that providing a discussion about intuition is important for this. Below we provide more insights about the PMSP and CTP tasks.\n\n    * The PMSP task exploits the videographer bias -- i.e., videos are purposely recorded to highlight important objects and their movements. The PMSP task encourages our network to learn discriminative representations that help capture the most vibrant regions in the spatio-temporal space. We note that Sec. B in the Appendix provides extended discussion about the intuition of PMSP with empirical evidence; we kindly ask the reviewer to revisit Sec.B and C in the Appendix, especially the captions of Figures 5-12, as they provide comprehensive insights about the task.\n\n    * The CTP task largely follows the intuition of correspondence prediction often used in self-supervised learning, including contrastive learning approaches (we also mentioned this in the paper). However, our formulation is slightly nuanced in that the negative P-frames not only come from different videos, i.e., Fig. 4(b) Random -- as is typically done in the literature, e.g., [Korbar et al., 2018, Owens & Efros 2018] among others -- but also come from the same clip in a different frame order, i.e., Fig. 4(c) Shuffle and 4(d) Shift. The intuition is that providing negative P-frames of the same clip will encourage the network to learn more discriminative representations based on local (frame-level) correspondence, while the first type only provides global (clip-level) correspondence signals. We believe that the use of \u201cShuffle\u201d and \u201cShift\u201d negatives has not been explored before (though its technical novelty is not overwhelmingly high). We have added this discussion in the paper to emphasize the nuanced difference. \n\n* **Contrastive baseline:** Thank you for this great suggestion! We evaluated the contrastive baseline using the vanilla InfoNCE loss used in SimCLR [Chen et al., 2020], taking negatives by randomly shuffling the embeddings of the I-frames and the P-frames within the same batch. This achieved 73.9% on UCF-101 and 43.7% in HMDB51 (we have added the new results in Table 3).  We note that this baseline is somewhat similar to the CTP (Binary) baseline (in Table 3) in that both methods use negative P-frames from a different clip; the only meaningful difference is that one is multi-class classification and another is a binary classification. Our new result confirms the importance of adding the \u201cShuffle\u201d and \u201cShift\u201d negatives (Fig 4).", "title": "Thanks! We clarified novelty & provided more discussion about the intuition"}, "WjWSlMJEuDP": {"type": "review", "replyto": "jMPcEkJpdD", "review": "This paper proposes an approach to self-supervised learning from videos. The approach takes advantage of compressed videos, using the encoded residuals and motion vectors within the video codec. Using encoded videos has been shown to reduce computation time required by decoding videos. Previous works have explored compressed videos for supervised recognition, showing the potential, while this paper introduces a way to leverage compressed videos for self-supervised learning.\n\nThe paper is well written and easy to follow. The proposed IMRNet shows benefit over previous supervised compressed video approaches. The paper proposes two self-supervised tasks related to compressed videos: one to predict the spatial locations with movement and one to predict the temporal matching of the inputs (Correspondence Type Prediction).\n\nThe Correspondence Type Prediction tasks are fairly standard, and have been explored by many previous works (which are cited). The main difference being the use of the compressed video features instead of rgb frame or audio.\n\nAn interesting observation is that the two tasks when combined don't seem to be any better, roughly 0.1%-0.3%, which is likely within noise on smaller datasets like HMDB and UCF101.\n\nThe comparisons in Table 3 are missing comparisons to state-of-the-art approaches. For example,\n- \"Cooperative learning of audio and video models from self-supervised synchronization\", NeurIPS'18 \n- \"Audio-visual scene analysis with self-supervised multisensory features\", ECCV'18\n- \"Evolving Losses for Unsupervised Video Representation Learning\", CVPR'20\n\nall outperform this approach. While those approaches use audio features and are not specifically focused on compressed videos, there's nothing about them making them incompatible with compressed videos using say only I-frames. I think Table 3 could be improved by comparing to existing approaches for non-compressed videos as well as using those approaches on compressed videos. This would help show the benefit of the proposed tasks.\n\nThe paper has some missing details. One of the claims is that compressed videos is faster, which is shown in Table 2. But there are some missing details, for example, the number of training epochs is not reported (only that 30 are used for warmup). How many does this use? How much faster is this method than non-compressed method?\n\nThere are also some other related works, e.g.\n\"AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures\", ICLR'20\nproposes a similar idea to the bidirectional connections and\n\"AssembleNet++: Assembling Modality Representations via Attention Connections\", ECCV'20\nproposes ideas similar to the multimodal gated attention.\n\n\nOverall, the paper is interesting. I think some of the experiments could be strengthen to better compare to existing self-supervised approaches.", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "7R3cLWxnCPp": {"type": "review", "replyto": "jMPcEkJpdD", "review": "This paper is not bad in the sense that:\n1. important problem -- how to leverage compressed video such a natural and efficient format is very important while under-explored by the community\n2. extensive experiments -- clear gains on standard benchmarks demonstrate the proposed method's gains. good ablation studies in like table 3 demonstrate the effectiveness of each proposed pretext task. \n3. the paper explored the how to better fuse multiple streams and two pretext ssl tasks i.e. PMSP+CTP, which all turn out to be successful\n\nYet, the way fusing multiple streams using techniques like multi-modal attention and the pretext tasks i.e PMSP and CTP -- they look fair while do not surprise me. It is quite natural to think of these techniques to improve the current models. For example, correspondence has been used by UCB researchers to do SSL in video yet for other task. \n\nBut I do acknowledge it takes significant work to validate these findings and conduct such comprehensive explorations and comparisons. I do believe these findings are valuable to the broad video community and thus will have good impact. Thus, I lean to accept this paper. ", "title": "a fair paper yet not surprisedly interesting", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}