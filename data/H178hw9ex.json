{"paper": {"title": "Dynamic Steerable Frame Networks", "authors": ["J\u00f6rn-Henrik Jacobsen", "Bert De Brabandere", "Arnold W.M. Smeulders"], "authorids": ["j.jacobsen@uva.nl", "bert.debrabandere@esat.kuleuven.be", "a.w.m.smeulders@uva.nl"], "summary": "Introducing non-orthogonal and overcomplete bases for ConvNets and derive Dynamic Steerable Frame Networks, a hybrid of Dynamic Filter Networks and Spatial Transformers.", "abstract": "Filters in a convolutional network are typically parametrized in a pixel basis. As an orthonormal basis, pixels may represent any arbitrary vector in Rn. In this paper, we relax this orthonormality requirement and extend the set of viable bases to the generalized notion of frames. When applying suitable frame bases to ResNets on Cifar-10+ we demonstrate improved error rates by substitution only. By exploiting the transformation properties of such generalized bases, we arrive at steerable frames, that allow to continuously transform CNN filters under arbitrary Lie-groups. Further allowing us to locally separate pose from canonical appearance. We implement this in the Dynamic Steerable Frame Network, that dynamically estimates the transformations of filters, conditioned on its input. The derived method presents a hybrid of Dynamic Filter Networks and Spatial Transformer Networks that can be implemented in any convolutional architecture, as we illustrate in two examples. First, we illustrate estimation properties of steerable frames with a Dynamic Steerable Frame Network, compared to a Dynamic Filter Network on the task of edge detection, where we show clear advantages of the derived steerable frames. Lastly, we insert the Dynamic Steerable Frame Network as a module in a convolutional LSTM on the task of limited-data hand-gesture recognition from video and illustrate effective dynamic regularization and show clear advantages over Spatial Transformer Networks. In this paper, we have laid out the foundations of Frame-based convolutional networks and Dynamic Steerable Frame Networks while illustrating their advantages for continuously transforming features and data-efficient learning.", "keywords": ["Computer vision", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "This paper studies how to incorporate local invariance to geometric transformations into a CNN pipeline. It proposes steerable filter banks as the ground-bed to measure and produce such local invariance, building on previous work from the same authors as well as the Spatial Transformer Networks. Preliminary experiments on several tasks requiring different levels of local invariance are presented. \n \n The reviewers had varying opinions about this work; all acknowledged the potential benefits of the approach, while some of them raised questions about the significance and usefulness of the approach. The authors were very responsive during the rebuttal phase and took into account all the feedback. \n \n Based on the technical content of the paper and the reviewers opinion, the AC recommends rejection. Since this decision is not consensual among all reviewers, please let me explain it in more detail.\n \n - The current manuscript does not provide a clear description of the new model in the context of the related works it builds upon (the so-called dynamic filter networks and the spatial transformer networks). The paper spends almost 4 pages with a technical exposition on steerable frames, covering basic material from signal processing. While this might indeed be a good introduction to readers not familiar with the concept of steerable filters, the fact is that it obfuscates the real contributions of the paper. which are not clearly stated. In fact, the model is presented between equations (10) and (11), but it is not clear from these equations what specifically differentiates the dsfn from the other two models -- the reader has to do some digging in order to uncover the differences (which are important). \n \n Besides this clarity issue, the paper does not offer any insight as to how the 'Pose generating network' Psi is supposed to estimate the pose parameters. Which architecture? what is the underlying estimation problem it is trying to solve, and why do we expect this problem to be efficiently estimated with a neural network? when are the pose parameters uniquely determined? how does this network deal with the aperture effects (i.e. the situations where there is no unicity in determining a specific pose) ?\n Currently, the reader has no access to these questions, which are to some extent at the core of the proposed technique."}, "review": {"HJVovnD_e": {"type": "rebuttal", "replyto": "rJhj2fUOx", "comment": "- Also, my impression is that the steerable filter bank model has important properties and consequences that are not at all discussed in this paper. The main property is indeed that fact that a steerable filter bank linearizes group actions. In other words, given an input x and its frame coefficients Wx = {w_1...w_L}, a group transformation T[g] x will have coefficients W T[g] x of the form B_g W x. The authors then consider models where a 'registration' transformation g*(x) can be applied to the frame coefficients prior to sending it to the higher layers, in order to improve the local invariance of the model. The simplest model that comes to mind is the bilinear model, where B_g is now a linear with respect to x, say B_g = C(x), so Y = C(x) W x becomes a bilinear model. One can then build on top of that and consider piecewise bilinear models, which leads naturally to the residual networks, in which features are obtained by combining intermediate representations together with linear and multiplicative interactions. This baseline model which simply builds a residual network on top of a steerable filter bank should be the basis to compare any fancier model such as the ones suggested by the authors.   \n\n- Another important question (that has been addressed in the comments thread) is how to build steerable representations in all the layers, and not only in the first one. This is a hard open problem, but in the context of this work, it should be discussed. Otherwise, the reader is left questioning why the first layer is so special.\n\n- Cifar experiments: If I understood correctly, the first numerical experiment does not involve the proposed model, but only studies the effect of using the steerable filter bank instead of a generic filter bank as the first layer. Whereas the numerical results are encouraging, the reader needs to know whether they are statistically significant or not: the best way to do that is to produce error bars in all classification experiments! \n\n- Edge detection experiment: This experiment highlights the benefits of the proposed approach versus previous neural network approaches. However, I am wondering why the authors did not mention the obvious baseline for edge detection, namely Canny edge detectors? Also, see for example https://arxiv.org/pdf/1511.04166v2.pdf for a recent treatment of this problem. \n\n- Finally, with respect to the comparison with the Spatial Transformer Network, what is the impediment to make it work locally instead of globally? Perhaps this should also be included as a baseline for comparisons.\n\nOverall, my assessment of this work is that it is on the right track to provide better, deeper understanding of the geometrical properties of Convnets, with the potential also to improve its numerical performance on specific tasks. However, in its current form this work does not present the ideas in a well crafted, clear way, and its numerical experiments are hard to relate to the basic properties of the model. ", "title": "(meta-review, continued; it was cut from the dialog character limitation) "}, "B1-TSl0Ug": {"type": "rebuttal", "replyto": "H178hw9ex", "comment": "Dear Reviewers,\n\nThank you once again for your helpful comments, suggestions and time.\n\nWe have revised the manuscript based on your reviews.\n\n1. We have reworked the text according to your suggestions. We revised the whole introduction to put more focus on high-level arguments for our proposed method and to get rid of unnecessary technicality. We have rewritten the experimental section to focus more on comparison with standard methods. And we have updated other parts, including the abstract accordingly.\n\n2. We have added multiple experiments to highlight the benefit of our method and to show a wider comparison to other methods.\n\n- We have added additional experiments with SOTA Densenets to the Cifar10+ table, with additional results on the \"naive frame\", where the \"image frame\" outperforms the Densenet baseline as well as it did outperform the ResNets, highlighting that our observations generalize across multiple architectures.\n\n- We have added a Spatial Transformer Network to the hand-gesture recognition task in two settings, a full-affine STN, and a restricted scale-rotation STN. In both cases, the STN fails to learn meaningful global warps that benefit the classification. However, restricting the full-affine to scale-rotation increases performance by 10%, indicating that less global invariance is better in a task where most class-specific information is encoded in global appearance and movement and only local invariance is desirable. Our proposed DSFN increases performance above all baselines by at least 22%.\n\n- We have added parameter counts for the models evaluated on the hand-gesture recognition task, showing that the DSFN comes at a very small parameter cost.\n\n- To facilitate model insight, we have added visualizations of the transformations learned by the Spatial Transformer Networks and the Dynamic Steerable Frame Networks to the appendix. Showing, that the DSFN learns local invariance, while the STN zooms and rotates the videos in what seems to be arbitrary ways.\n\n- On the edge-detection task, we have added an experiment where we provide a frame as an input to a Dynamic Filter Network, to assess if the DFN benefits from a steerable function space as input. The DFNs performance increases substantially if we do so, indicating, that the DFN was not able to learn a continuously transforming frame on its own. Our proposed DSFN again cuts the error of the strongest DFN result by half, illustrating that contiuously transforming input and output spaces are superior when precise local adaption is needed.\n\n- As requested by reviewer 2, we have also added an additional Autoencoder baseline to the results on the edge-detection task. As expected, it illustrates that location invariant filtering performs poorly on such a task.\n\n3. We have updated figure 3 as suggested by reviewer 3, removed redundancy and moved it as close as possible to the actual computational flow of the proposed method.\n\nWe would like to thank the reviewers once again for their insightful reviews and hope that we were able to answer all their questions.\n\nLet us know if you have any further questions or remarks.", "title": "Revision Summary"}, "HJVAWf38x": {"type": "rebuttal", "replyto": "SyYDPcR4x", "comment": "Thank you very much for your comments and questions.\n\nThe main goal of the Cifar10 experiments was to show, that one can replace the pixel-basis with a frame that has additional desirable properties like the steerability without decreasing performance. Additionally, we were able to show that steerable frames can consistently increase the performance of SOTA networks given they are suitable for natural image data. Note that we have added SOTA Densenet models to table 2, in which we were able to improve performance by substituting the pixel-basis with frames as well as in the Resnet models, substantiating the generality of our observations.\n\n(1) Do such transformations constitute real challenges in practice? What such components would add to the state of the art?\n\nBeing able to replace the pixel-basis by steerable frames leads us to our proposed Dynamic Steerable Frame Networks (DSFNs), a method that continuously transforms filters conditioned on the input. DSFNs are locally adaptive, interpretable and data-efficient. We compare our DSFNs to other adaptive methods. In this domain Dynamic Filter Networks (DFNs) and Spatial Transformer Networks (STNs) constitute the state of the art. \n\nDynamic Filter Networks learn location varying filters in an unconstrained manner and generate any type of filter kernel for each location in the input, this approach is very data-inefficient as it has many unconstrained parameters and the transformations have no clear geometrical meaning. Spatial Transformer Networks transform the whole feature stack globally under predefined geometrical parameters. The method regularizes the additional parameters introduced by it effectively and achieves global transformation invariance.\n\nSTNs are not locally adaptive, thus they fail in many cases where it is not beneficial to transform the image globally as it would destroy discriminative information (deformable objects, many objects, dynamic movements) or where global registration is anyway performed as a standard preprocessing step (medical imaging data). The hand-gesture experiment is an example of a whole range of tasks where STNs fail, as they are not suitable for moving deformable objects.\n\nDFNs are black boxes and not data-efficient. They introduce many unconstrained parameters and are thus not suited for limited-data scenarios. At the same time, it is not possible to check if the model converges or does something meaningful. Such a behavior is undesirable when data is limited and interpretability is key, like in medical imaging. Further, they fail to generate very fine-grained local adaption as can be achieved with a well-regularized DSFN, illustrated with the edge-detection experiment. This makes them inferior for tasks like segmentation, even in unlimited data scenarios.\n\nOur proposed DSFNs transform filters locally in a continuous manner. This allows the model to precisely adapt filters to local features in the image. It can do so in a data-efficient manner. One learned filter can be applied to all its transformations, alleviating the necessity to learn one filter for each orientation or each different scale, effectively representing infinitely many geometrical variants of the same filter. \n\nIn both experiments, our proposed method substantially outperform DFNs and STNs and we are able to explain why this is the case. In conclusion, our proposed Dynamic Steerable Frame Networks are able to fill the gap between adaptive state-of-the-art Dynamic Filter Networks and Spatial Transformer Networks.\n\n(2) The computational cost is not discussed.\n\nFrame-based CNNs have the same runtime as vanilla CNNs. The Dynamic Steerable Frame Networks have the same runtime as vanilla Dynamic Filter Networks. Besides that, there is a whole body of research on how to speed up convolution with analytic frame functions, with strategies like recursive filtering, showing promise to potentially decrease runtime.\n\nThank you once again for your review, we have updated the manuscript to include our answers.\n\nLet us know if this answers your questions!", "title": "Response"}, "rk5E_aoUe": {"type": "rebuttal", "replyto": "HkR6G4vNe", "comment": "Thank you very much for the insightful comments and questions. You asked:\n\nWhat are the feature transformations learned by the algorithm?\n\nThe algorithm learns location varying filters that transform under continuous transformation groups, such as rotations, changes in size, shearing, changes of color. In fact, it provides a framework to transform filters under arbitrary local transformations that can be formulated as a Lie group.\n\nLocally adaptive filters are advantageous in many tasks. In natural images and video, many things change locally. For instance, the leaves of a tree can all move, but the tree remains a tree. Humans and animals are changing locally all the time, even though their overall appearance remains largely the same. Medical imaging is almost purely concerned with local features that don\u2019t necessarily have a preferred orientation. In all of these cases, data-augmentation and global methods like Spatial Transformer Networks can only get rid of global variabilities. Ideally the amount of local invariance should not be pre-defined, but learned to preserve all important information for the task at hand. Our proposed Dynamic Steerable Frame Networks (DSFNs) provide a framework to do so.\n\nDSFNs transform filters locally in a continuous manner. This allows the model to precisely adapt filters to local features in the image. It can do so in a data-efficient manner. One learned filter can be applied to all its transformations, alleviating the necessity to learn one filter for each orientation or each different scale, effectively representing infinitely many geometrical variants of the same filter. \n\nWhen precise local adaption is necessary, the usefulness of continuously transformable filters is illustrated by the edge-detection experiment. The Dynamic Filter Network (DFN) fails to learn a continuously transforming basis, while the DSFN outperforms it by an order of magnitude. The DFN improves performance when it receives a steerable frame as input, but the error it makes is still double compared to the well-regularized DSFN, see figure 4. We visualize the pose space of the DSFN to highlight that it indeed learns rotation invariant filters in the same figure. Note, that the DSFN yields an interpretable pose space, while the DFN acts as an uninterpretable black box.\n\nAnother example of local adaption are the hand-gesture recognition experiments as an example where local invariance is key. The DSFN allows us to learn locally varying filters in a data-efficient and interpretable manner, as substantiated by 18% higher accuracy than the baseline. We also evaluated Spatial Transformer Networks on this task in two settings, one that allows the full-affine transformations and one that only allows rotation and uniform scalings. The affine STN fails to find meaningful warps of the inputs and achieves almost random performance, only classifying one static class correctly. The rotation scaling STN still decreases performance compared to the baseline, but it performance substantially better than the affine STN, indicating, that partially removing global invariances increases preserved information content. We are also adding visualizations of the warps and filters learned by STN and DSFN to the appendix to increase insight into the trained models. The STN learns more or less random warps, while the DSFN indeed discovers locally invariant filters that move with the boundaries of the object in a continuous manner, whereas the STN approach fails. \n\nThe paper could benefit from some high-level arguments.\n\nOur proposed Dynamic Steerable Frame Networks fill the gap in-between STNs and DFNs. DSFNs are locally adaptive, interpretable and data-efficient. They can be applied to a range of problems where neither STNs nor DFNs are a good fit, but one would rather like to use a hybrid approach of the two.\n\nDFNs learn location varying filters in an unconstrained manner and generate any type of filter kernel for each location in the input, this approach is very data-inefficient as it has many unconstrained parameters and the transformations have no clear geometrical meaning. \n\nSTNs transform the whole feature stack globally under predefined geometrical parameters. The method regularizes the additional parameters introduced by it effectively and achieves global transformation invariance.\n\nSTNs are not locally adaptive, thus they fail in many cases where it is not beneficial to transform the image globally as it would destroy discriminative information (deformable objects, many objects, dynamic movements) or where global registration is anyway performed as a standard preprocessing step (medical imaging data). The hand-gesture experiment is an example of a whole range of tasks where STNs fail, as they are not suitable for moving deformable objects.\n\nDFNs are black boxes and not data-efficient. They introduce many unconstrained parameters and are thus not suited for limited-data scenarios. At the same time, it is not possible to check if the model converges or does something meaningful. Such a behavior is undesirable when data is limited and interpretability is key, like in medical imaging. Further, they fail to generate very fine-grained local adaption as can be achieved with a well-regularized DSFN, illustrated with the edge-detection experiment. This makes them inferior for tasks like segmentation, even in unlimited data scenarios.\n\nWhy are certain frames more suitable than others?\n\nSuitable frames are the ones have been designed with general image properties in mind. Each frame performs a change of basis that can be seen as a rotation of the inputs basis. Natural image statistics based frames are thus a good idea (like the Gauss-Frame) while rotating to an arbitrary frame (like polynomials) can hurt performance, but depending on the task at hand a decrease in 1% of classification performance can also be acceptable, given that different frames exhibit properties the pixel-basis does not have, for example steerability.\n\nWe have made the Cifar10 experiments section clearer in this regard and added the SOTA Densenet to the table. We show that it behaves similar to the ResNets, thus our observations generalize to different models and model sizes. Our main purpose of this experiment is to show that suitable frames can outperform the standard pixel-basis, while they also exhibit multiple beneficial properties like steerability. We thus do agree with the reviewer, that the contribution associated with this insight has to be restated. We have changed it to: \u201cSuitable choices of frames lead to improved performance.\u201d\n\nWhat are runtime considerations?\n\nFrame-based CNNs have the same runtime as vanilla CNNs. The Dynamic Steerable Frame Networks have the same runtime as vanilla Dynamic Filter Networks. However, due to separability and strategies like recursive filtering, frame-bases show much promise to be faster than standard filter formulations, but we leave this for future work.\n\nFigure 3\n\nWe have simplified figure 3 based on the reviewer's suggestions. We believe merging different blocks and staying very close to the main equation of the paper (eq.7) and further simplification helped to increase the readability of it significantly, we thank the reviewer for the suggestions.\n\nThank you once again. We have reworked the manuscript according to your comments and believe they were instrumental for improving it substantially. \n\nLet us know if we have answered all your questions!", "title": "Response"}, "Hk7fUW2Lx": {"type": "rebuttal", "replyto": "BJrAJ6ZEx", "comment": "Thank you very much for your positive review and for acknowledging the usefulness of the approach.\n\nWe have included your comments into the revision of the paper.\n\n* For the LSTM experiment, in order to be more exact, it would be useful to include information about total number of parameters, as the network which estimates the pose also increases the number of parameters.\n\nWe have added the number of parameters to the table. The 1-Layer convLSTM has 905k parameters and the DSFN convLSTM has 907k parameters, while the 2-Layer convLSTM has 913k parameters. Thus, our proposed method comes with a very small parameter cost, as it mainly consists of 1x1 convolutions performed on the output of a convolution with fixed frame functions.\n\n* Would it be possible to provide more details about how the back-propagation is done through the steerable filters?\n\nWe will add an example for the backpropagation to the appendix.\n\n* For the Edge Detection experiment, it would be useful to provide results for some standard baseline - e.g. CNN with a similar number of parameters. Simply to see how useful it is to have location-variant filters for this task.\n\nWe have added an Autoencoder baseline, it performance substantially worse than the locally adaptive methods. Further, we have added a Dynamic Filter Network who's input is a steerable frame, doing so improves the performance of the DFN significantly, illustrating that the DFN does not seem to be capable of discovering a continuously steerable frame by itself. Most effective, however, is the full DSFN, cutting the error of the frame-based DFN again in half.\n\n* The last sentence in second paragraph on page 1 is missing a verb. Also it is maybe unnecessary.\n\nThank you very much for pointing this out. We have rewritten this paragraph incorporating the comments of reviewer 3 and thus the sentence has been removed.\n\n* The hyphenation for ConvNet is incorrect on multiple places (probably `\\hyphenation{Conv-Net}` would fix it).\n\nWe have removed the term ConvNet and replaced it with CNN.\n\nThank you once again for your constructive and positive feedback.\n\nLet us know if this answers all your questions!", "title": "Response"}, "rJyDk82Qg": {"type": "rebuttal", "replyto": "BymraOJQx", "comment": "Dear Reviewer,\n\nThank you for acknowledging the usefulness of our approach.\n\nApplying it to higher layers is certainly possible. In this case, the input I(x,y) of the Dynamic Steerable Frame Network (see Figure 3) will be a 3D tensor containing all input feature maps and the output will be the locally transformed and filtered inputs. The network will learn the relationship between all local transformations and how to mix channels etc., such that the transformations \"make sense\" for the task at hand. \nThe shortcomings of our method are that you will not have guaranteed invariance, at least not without extending the approach slightly. However, this is also an advantage, as all local transformation relationships will be learned from data and if full rotation invariance is useful, the Dynamic Steerable Frame Network can easily discover this fact and use it. \n\nAnother way of using the proposed method when global invariance is desired is to restrict it to a special case, where each feature map gets a set of pose parameters assigned to it, rather than each location in each feature map. This way, we maintain the benefit of dynamically estimating the poses from the equivariant function space, while enforcing consistent behavior across locations. This is likely to be useful in layers, where effective receptive field sizes already contain full objects.\n\nWe hope this clarifies your questions.\n\nKind regards", "title": "Answer AnonReviewer2"}, "BymraOJQx": {"type": "review", "replyto": "H178hw9ex", "review": "- The local steerability seems to be an useful tool, especially when invariance to the selected group of transformations is needed in the low level image features (e.g. for the edge detection). Do you think that this concept would work also for the higher network layers, e.g. for rotation invariance for object classification? Do you think that the higher layers would be steerable? And what would be the shortcomings of your method for this case? \nPlease take this question more as an open question to clarify the details and the context of your method.This works applies steerable frames for various tasks where convolutional neural networks with location invariant operators are traditionally applied. Authors provide a detailed overview of steerable frames followed with an experimental section which applies dynamic steerable network to small machine learning problems where the steerability is conceptually useful.\n\nEven though the evaluation is performed only on few small tasks, the reason why more tasks were not evaluated is that piece-wise pose invariance is needed only for a subset of tasks. The fact, that simply using overcomplete bases as a sort of \"feature pre-processing\" improves the results for already highly optimized ResNet and DenseNet architectures is quite interesting achievement.\n\nFor the edge detection, a relatively hard baseline is selected - the Dynamic Filter Networks, which already attempts to achieve position invariant filters. The fact that DSFN improves the performance on this task verifies that regressing the parametrization of the steerable filters yields better results than regressing the filters directly.\n\nIn the last experiment authors apply the network to video classification using LSTMs and they show that the improved performance is not due to increased capacity of the network.\n\nIn general, it is quite interesting work. Even though it does not offer ground-breaking results (mainly in a sense of not performing experiments on larger tasks), it is theoretically interesting and shows promising results.\n\nThere are few minor issues and suggestions related to the paper:\n* For the LSTM experiment, in order to be more exact, it would be useful to include information about total number of parameters, as the network which estimates the pose also increases the number of parameters.\n* Would it be possible to provide more details about how the back-propagation is done through the steerable filters?\n* For the Edge Detection experiment, it would be useful to provide results for some standard baseline - e.g. CNN with a similar number of parameters. Simply to see how useful it is to have location-variant filters for this task.\n* The last sentence in second paragraph on page 1 is missing a verb. Also it is maybe unnecessary.\n* The hyphenation for ConvNet is incorrect on multiple places (probably `\\hyphenation{Conv-Net}` would fix it).\n", "title": "Pre-Review Questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJrAJ6ZEx": {"type": "review", "replyto": "H178hw9ex", "review": "- The local steerability seems to be an useful tool, especially when invariance to the selected group of transformations is needed in the low level image features (e.g. for the edge detection). Do you think that this concept would work also for the higher network layers, e.g. for rotation invariance for object classification? Do you think that the higher layers would be steerable? And what would be the shortcomings of your method for this case? \nPlease take this question more as an open question to clarify the details and the context of your method.This works applies steerable frames for various tasks where convolutional neural networks with location invariant operators are traditionally applied. Authors provide a detailed overview of steerable frames followed with an experimental section which applies dynamic steerable network to small machine learning problems where the steerability is conceptually useful.\n\nEven though the evaluation is performed only on few small tasks, the reason why more tasks were not evaluated is that piece-wise pose invariance is needed only for a subset of tasks. The fact, that simply using overcomplete bases as a sort of \"feature pre-processing\" improves the results for already highly optimized ResNet and DenseNet architectures is quite interesting achievement.\n\nFor the edge detection, a relatively hard baseline is selected - the Dynamic Filter Networks, which already attempts to achieve position invariant filters. The fact that DSFN improves the performance on this task verifies that regressing the parametrization of the steerable filters yields better results than regressing the filters directly.\n\nIn the last experiment authors apply the network to video classification using LSTMs and they show that the improved performance is not due to increased capacity of the network.\n\nIn general, it is quite interesting work. Even though it does not offer ground-breaking results (mainly in a sense of not performing experiments on larger tasks), it is theoretically interesting and shows promising results.\n\nThere are few minor issues and suggestions related to the paper:\n* For the LSTM experiment, in order to be more exact, it would be useful to include information about total number of parameters, as the network which estimates the pose also increases the number of parameters.\n* Would it be possible to provide more details about how the back-propagation is done through the steerable filters?\n* For the Edge Detection experiment, it would be useful to provide results for some standard baseline - e.g. CNN with a similar number of parameters. Simply to see how useful it is to have location-variant filters for this task.\n* The last sentence in second paragraph on page 1 is missing a verb. Also it is maybe unnecessary.\n* The hyphenation for ConvNet is incorrect on multiple places (probably `\\hyphenation{Conv-Net}` would fix it).\n", "title": "Pre-Review Questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}