{"paper": {"title": "Privacy Preserving Recalibration under Domain Shift", "authors": ["Rachel Luo", "Shengjia Zhao", "Jiaming Song", "Jonathan Kuck", "Stefano Ermon", "Silvio Savarese"], "authorids": ["~Rachel_Luo1", "~Shengjia_Zhao1", "~Jiaming_Song1", "~Jonathan_Kuck1", "~Stefano_Ermon1", "~Silvio_Savarese1"], "summary": "We introduce a framework that provides abstractions for performing recalibration under differential privacy constraints, design a novel recalibration algorithm that works well in this setting, and extensively validate our method experimentally. ", "abstract": "Classifiers deployed in high-stakes applications must output calibrated confidence scores, i.e. their predicted probabilities should reflect empirical frequencies. Typically this is achieved with recalibration algorithms that adjust probability estimates based on the real-world data; however, existing algorithms are not applicable in real-world situations where the test data follows a different distribution from the training data, and privacy preservation is paramount (e.g. protecting patient records). We introduce a framework that provides abstractions for performing recalibration under differential privacy constraints. This framework allows us to adapt existing recalibration algorithms to satisfy differential privacy while remaining effective for domain-shift situations. Guided by our framework, we also design a novel recalibration algorithm, accuracy temperature scaling, that is tailored to the requirements of differential privacy. In an extensive empirical study, we find that our algorithm improves calibration on domain-shift benchmarks under the constraints of differential privacy. On the 15 highest severity perturbations of the ImageNet-C dataset, our method achieves a median ECE of 0.029, over 2x better than the next best recalibration method and almost 5x better than without recalibration.", "keywords": ["uncertainty calibration", "differential privacy"]}, "meta": {"decision": "Reject", "comment": "This work considers the problem of calibrating a multi-class classifier while preserving differential privacy. It proposes a method Accuracy Temperature Scaling, that aims to achieve consistency rather than calibration. The method is particularly easy to implement under the constraint of DP. The paper then evaluates  the calibration algorithm in the context of domain perturbation/shift and, as the authors demonstrate it outperforms adaptations of other technques to DP.\n\nThe strong sides of this work are \n* the first work to study calibration in this setting (albeit that is also a result of the setting being of a relatively narrow interest)\n* proposes a new algorithm\n* evaluation on multiple benchmarks\n\nThe weaknesses\n* The method is not justified either by theoretical analysis or clear intuition\n* Evaluation of performance in the context of domain shift makes the the presentation somewhat confusing and experiments much more involved but is largely orthogonal to the problem of calibration\n\nOverall the work has merits but also significant issues."}, "review": {"5qkJNqWJnK": {"type": "rebuttal", "replyto": "211VMxAg_6v", "comment": "We are grateful for the detailed review and thank the reviewer for their constructive comments. \n\n*\u201cI don't see how the algorithm addresses the domain shift problem.\u201d*\n\nWe address the domain shift point in the general response above. \n\n*\u201cAccording to Tables 3 and 4 in the appendix, the Acc-T works well compared to others without privacy constraints. As it is expected to have higher biases, I would appreciate it if the authors could provide more details on how they evaluated the error and explanations.\u201d*\n\nAcc-T has **slightly higher bias but much lower variance** than the other methods, which leads to better performance unless the recalibration dataset is very large. We note that a similar observation has been made before for temperature scaling compared to histogram binning in [1]. [1] showed that for typical recalibration datasets, temperature scaling achieves lower calibration error than histogram binning, even though histogram binning is unbiased and guaranteed to be perfectly calibrated in the limit of infinite data. \n\nFor evaluating the ECE error, we use exactly the same approximation method as [1], with 15 equally spaced bins. Therefore, the results should be comparable and standardized. \n\n[1] On Calibration of Modern Neural Networks, Guo et. al. \n\n*\u201cThe framework seems like federated learning with differential privacy, where a central server only gets private local updates from users and takes the average to optimize the parameters. The framework doesn't seem to be novel, but it can be a novel use of this setting for recalibration. It would be good to add a few sentences discussing the connections to federated learning.\u201d*\n\nGood point, this is a valuable connection to make. Our problem setup can be framed as differentially private federated learning for recalibration. We have added this context to our problem statement. \n\n*\u201cTruncating the log likelihood function can also lower the sensitivity. It would be interesting to see the comparison.\u201d*\n\nThank you for pointing this out! In the original paper, we used  source domain data to pick a truncation threshold for the NLL-T method. It\u2019s true that this threshold may not result in optimal recalibration, but it does guarantee differential privacy since it does not access private data from the target domain. We also experimented with using private target domain data  to find the optimal truncation thresholds. This artificially improves the result of NLL-T by violating differential privacy. However, even with the most favorable threshold, NLL-T performs significantly worse than Acc-T. In our experiments, using the optimal threshold for NLL-T never improves its performance by more than 10% over the reported results, and this improvement comes at the cost of privacy violations. We have added an additional plot (Figure 52) in Appendix E.3, showing the ECE achieved by NLL-T at different clipping thresholds for an exemplar dataset. \n\n*\u201cMinor comment: In algorithm 1, the query functions for all d data sources are the same. But the general framework states that they can be different. I just wonder how to design a customized query function for each data source. Or maybe it would be more clear to remove the subscript.\u201d*\n\nThank you for pointing this out. We have removed the subscript. \n", "title": "Response to AnonReviewer2"}, "drwXt4KMgv": {"type": "rebuttal", "replyto": "dkbicLSuEuq", "comment": "We thank the reviewer for their thoughtful feedback and positive comments. \n\n*\u201cIn terms of methodology, the paper is only tangentially related to domain shift, since there is a form of fine-tuning on the target dataset (using the overall accuracy) (see \u00a72.2).\u201d*\n\nWe address the domain shift point in the general response above. \n\n*\u201cHowever, there must also be an interplay between the degree of shift, the effectiveness of transfer learning (e.g. fine-tuning) and calibration.\u201d*\n\nYes, there is an interplay between the severity of the domain shift, the effectiveness of fine-tuning, and calibration. We have performed additional experiments with different domain shift severity levels (where higher severity \u2192 bigger distribution shift) for CIFAR-100. The results are shown in Table 7 of Appendix E.3. To give a high level overview of our experimental results: 1. Higher severity corruption generally leads to worse calibration performance under differential privacy. 2. Even at the lowest severity level, Acc-T achieves a median ECE of 0.0766, a 26% lower calibration error compared to the second best baseline across the board. \n", "title": "Response to AnonReviewer1"}, "ytCmxCax5Kh": {"type": "rebuttal", "replyto": "MBsIQ8ykjII", "comment": "We appreciate the constructive comments and thank the reviewer for their feedback. \n\n*\u201cThe proposed algorithm is not described very clearly in section 3 and section 4.\u201d*\n\nWe have rewritten parts of Sections 3 and 4, and we will also add a new figure to improve clarity. We highlight a few new writing changes: \n\nWe have added an introduction to Section 3 that clarifies how our problem setup **falls within the context of federated learning**. Multiple parties experience the same domain shift (e.g. they live in the same changing world). Each party would benefit from access to additional data, but each party also wants to keep their own data private. We propose an algorithm that allows all parties to react to domain shifts more quickly by pooling their data (so each individual party needs less labeled data from the new distribution), while maintaining the privacy of each party. \n\nTo clarify the examples in Section 3.1, let us consider Example 1 in the context of federated learning. In this case, the hospitals are the parties that wish to keep their data (patient info) private. The novel strain of the virus represents a domain shift. The hospitals each have only a few data points (images of patients with the novel virus), so they want to aggregate their data in order to improve their classifier\u2019s calibration while still respecting patient privacy. \n\nOur proposed algorithm is in Section 4.1 of the paper. On Line 2 we select initial temperature values (the recalibration parameter).  Line 3 specifies a query function that the hospitals use to pool their data while respecting differential privacy. Lines 4-12 implement differentially private golden section search over the recalibration temperature parameter. The algorithm outputs a temperature value that improves the classifier\u2019s calibration on the new domain. \n\n*\u201cI still feel hard to follow how they address the domain-shift issues.\u201d*\n\nWe address the domain shift point in the general response above. \n\n*\u201cThe privacy part seems like a plug-and-play of the Laplace mechanism.\u201d*\n\nWe would like to highlight that our goal is to propose the problem setup, a general framework for addressing the problem, and show the surprising empirical effectiveness of one novel algorithm (accuracy temperature scaling). Because this is the first time that this problem setup has been proposed, we use the Laplace mechanism, which works to preserve privacy. We agree that results with more advanced privacy-preserving techniques are an interesting area for future work!\n\n*\u201cI would suggest the authors use recently advanced composition [1,2] for better privacy and utility tradeoffs.\u201d*\n\nWe use pure differential privacy as our notion of privacy. These advanced compositions are applicable to more relaxed definitions of privacy. In principle our framework should also work in a plug-and-play manner with these relaxed definitions of privacy and advanced compositions, and we leave that for future work. \n\n*\u201cAs the authors claim in Section C.2.1 in the appendix, the sensitivity is technically infinite.\u201d*\n\nWe use a clipping threshold to preserve privacy when the sensitivity is infinite. Please see \u201cBounding the sensitivity of $\\Delta f$ for NLL-T\u201d in the general response above. \n", "title": "Response to AnonReviewer4"}, "UcLKzMZB0P": {"type": "rebuttal", "replyto": "TgGh-KYSNi7", "comment": "We thank the reviewer for their positive feedback and constructive comments. \n\n*\u201cThe sensitivity of f cannot be bounded. What the authors propose (Section C.2.1) to address this --- i.e., using a sufficiently large value based on the empirical values --- is not generally accepted.\u201d* \n\nWe use a clipping threshold to preserve privacy when the loss function is unbounded. Please see \"Bounding the sensitivity of $\\Delta f$ for NLL-T\" in the general response above. ", "title": "Response to AnonReviewer3"}, "rGEvlab4Rq": {"type": "rebuttal", "replyto": "m08OHhXxl-5", "comment": "We thank all reviewers for their thoughtful feedback on our work. We would like to clarify a few points below: \n\n**Bounding the sensitivity of $\\Delta f$ for NLL-T:**\n\nOur framework is guaranteed to preserve privacy even when the loss function is unbounded (as in the case of NLL-T). During deployment, if the loss function is unbounded, a clipping threshold should be selected ***before*** the (private) recalibration data is observed, i.e. loss_clipped = min(clipping_threshold, loss). The Laplace mechanism then guarantees differential privacy, using standard analysis. In our experiments, we selected good clipping thresholds based on the public training data (rather than the private recalibration data) for the NLL-T baseline. Note that this discussion only applies to the NLL-T baseline; our recommended algorithm, Acc-T, does not need any clipping. \n\n**Relationship to domain shift:**\n\nOur framework should not be confused with unsupervised domain adaptation. Instead, our recalibration framework requires a small amount of labeled data from the target domain. Specifically, we take as input a classification model trained on data from some source domain. We then fine-tune the model using a small amount of labeled data from the target domain while preserving differential privacy. Our framework returns a model with improved calibration on the target domain.\n\nOur method (Acc-T) is indeed **applicable outside the context of domain shift** (e.g. it improves calibration on the source domain as well, see the experiments in Appendix E.3). However, it is particularly suitable for the domain shift setup because our method assumes access to very little information about the new domain (simply the average overall accuracy). Thus, **very little target domain data is needed** to estimate the average accuracy, and it is easy to preserve privacy. ", "title": "General Response"}, "211VMxAg_6v": {"type": "review", "replyto": "m08OHhXxl-5", "review": "Summary:\n\n \nThe paper studies the problem of classifier recalibration under differential privacy constraints. They propose a framework with a calibrator and several private data sources, and it works as follows. At each iteration, the calibrator queries each source, and the data source sends back the private answer, which will be used to optimize the calibration. They also provide a recalibration technique, accuracy temperature scaling, which is effective under the privacy constraint for the reason of low sensitivity. Rigorous experimental results are provided.\n\n\nReasons for score: \n\n Overall, I am positive about this paper, but I have a few concerns.  I've listed the strengths and weaknesses below. Hopefully, the authors can address my concern in the rebuttal period. I'd be happy to raise my score if I am wrong.\n\nStrengths: \n\n\n1. The problem is well-motivated, giving the rising privacy concern and the importance of recalibration.\n\n2. The choice of the query function is novel for privacy constraint, as it has lower sensitivity compared to the log-likelihood function.\n\n3. They provide extensive experimental results to demonstrate the effectiveness of the proposed method.\n\n \n Weaknesses:\n\n1. I don't see how the algorithm addresses the domain shift problem. But they claim that ''We also fine-tune on the target domain'' in section 2.2.\n\n2. According to Tables 3 and 4 in the appendix, the Acc-T works well compared to others without privacy constraints. As it is expected to have higher biases, I would appreciate it if the authors could provide more details on how they evaluated the error and explanations.\n\n3. The framework seems like federated learning with differential privacy, where a central server only gets private local updates from users and takes the average to optimize the parameters. The framework doesn't seem to be novel, but it can be a novel use of this setting for recalibration. It would be good to add a few sentences discussing the connections to federated learning.\n\n4.. Truncating the log likelihood function can also lower the sensitivity. It would be interesting to see the comparison.\n\nMinor comments:\n\n1. In algorithm 1, the query functions for all d data sources are the same. But the general framework states that they can be different. I just wonder how to design a customized query function for each data source. Or maybe it would be more clear to remove the subscript.\n\n", "title": "Review", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "dkbicLSuEuq": {"type": "review", "replyto": "m08OHhXxl-5", "review": "The paper tackles the problem of privacy preserving calibration under domain shift, which is an interesting combination of 3 separate problems that may often occur together. The main contribution is the use of accuracy temperature scaling for calibration, which is a good match for differential privacy. Extensive experimental validation is given.\n\nStrengths:\n- The use of accuracy temperature scaling is neat in combination with DP. Whilst not a startling novelty, it is a nice observation that their method Accuracy Temperature scaling (Acc-T) combines so well with differential privacy, and yet does not lose out in terms of utility (and in fact does better under more stringent privacy settings, since fewer DP noise iterations are required.\n- Strong experimental section. Although only on image data, the experiments cover a multitude of shift types, and the baselines are apt for the task.\n\nWeaknesses:\n- In terms of methodology, the paper is only tangentially related to domain shift, since there is a form of fine-tuning on the target dataset (using the overall accuracy) (see \u00a72.2). There are extensive experiments on recalibration under domain shift (using perturbed image datasets). However, there must also be an interplay between the degree of shift, the effectiveness of transfer learning (e.g. fine-tuning) and calibration. This is not really covered\n", "title": "Neat use of accuracy temperature scaling for differential privacy", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "MBsIQ8ykjII": {"type": "review", "replyto": "m08OHhXxl-5", "review": "This paper studies the problem of privacy-preserving calibration under the domain shift. The authors propose ''accuracy temperature scaling'' with privacy guarantees.  \n \nThe empirical results seem complete. I still have several concerns about the technical part.\n\n\u00a0\n1. The proposed algorithm is not described very clearly in section 3 and section 4. After spending considerable time reading sections 3 and 4, I still feel hard to follow how they address the domain-shift issues.\n\u00a0\n2. The privacy part seems like a plug-and-play of the Laplace mechanism. Hence, the technical novelty might be limited. Note that the privacy computation in section 3 based on a naive composition --- ` each $M_i$ satisfies $\\epsilon/k$, the total privacy cost follows $\\epsilon$. I would suggest the authors use recently advanced composition [1,2] for better privacy and utility tradeoffs. Moreover, the calculation of sensitivity seems to be wrong. As the authors claim in Section C.2.1 in the appendix, the sensitivity is technically infinite. They set $\\triangle_f=10$ based on empirical observation, which violates the privacy definition.\n\n[1] The Composition Theorem for Differential Privacy.\n[2] Renyi Differential Privacy.", "title": "The empirical results seem complete. I have several concerns about the technical part.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "TgGh-KYSNi7": {"type": "review", "replyto": "m08OHhXxl-5", "review": "Summary:\n\nThis paper studies the problem of recalibrating a classifier under the presence of domain shift and the constraints of differential privacy.  They show how to adapt several algorithms for dealing with domain shift to the paradigm of differential privacy, giving mechanisms that achieve both goals.\n\nStrong Points:\n\n1. Addresses a new, interesting, and practically relevant problem.\n2. Technically strong, good insights and clearly bridges the gap between two distinct areas.  \n3. Well organized and written, very clear for the most part. \n\nWeak Points:\n\n1. The sensitivity of f cannot be bounded.  What the authors propose (Section C.2.1) to address this --- i.e., using a sufficiently large value based on the empirical values --- is not generally accepted.\n\nOther Notes:\n\nI think the ideas are cool.  The reduction to a 1-dimensional minimization problem over T makes sense.  The technical insight to use golden search to reduce queries/noise is clever.  Experiments show that the proposed approach actually works as it is expected to.  My one weak point is a bit concerning however.  Hopefully authors can address it adequately in the rebuttal.  \n", "title": "This paper studies the problem of recalibrating a classifier under the presence of domain shift and the constraints of differential privacy.  They show how to adapt several algorithms for dealing with domain shift to the paradigm of differential privacy, giving mechanisms that achieve both goals.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}