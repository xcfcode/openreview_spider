{"paper": {"title": "Lifelong Perceptual Programming By Example", "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "authorids": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com", "dtarlow@microsoft.com"], "summary": "Combination of differentiable interpreters and neural networks for lifelong learning of a model composed of neural and source code functions", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "keywords": ["Deep learning", "Supervised Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The reviewers were quite divided on this submission, which proposes a method for lifelong learning in the context of program generation. While a novel idea, the experiments and baselines are simply not clear enough or convincing enough, and the method itself is not clearly conveyed. The paper makes strong claims that are not substantiated with more compelling or challenging experiments. Since this is an interesting, novel idea in a relevant area, however, I think it should be invited as a workshop paper."}, "review": {"ByEUXjyPl": {"type": "rebuttal", "replyto": "HysiJjZEe", "comment": "\nThanks for the thoughtful review.\n\nThe preliminary tasks are less about individual task complexity, and more about the ability to learn modular knowledge that can be transferred to more complex tasks. The MATH task is on a level of complexity comparable to a failure case of an existing system in the literature that aims to learn algorithms [1]. \n\nWe have now added Progressive Neural Network and Neural GPU baselines which are strong baselines for transfer learning and learning algorithms respectively. Our approach outperforms these baselines and the qualitative conclusions of the paper are unchanged.\n\n[1] Eric Price, Wojciech Zaremba & Ilya Sutskever; Extensions and Limitations of the Neural GPU (submitted to ICLR 2017)\n", "title": "Response"}, "Bk30Jj1De": {"type": "rebuttal", "replyto": "Hkw5qHb4x", "comment": "\nIn addition to responding in the general response comment above, we have now addressed many of the main issues raised by this review:\n- Added a Neural GPU baseline for the MATH task.\n- Modified baselines for MATH to train on up to 5-digit expressions, showing the conclusions are unchanged.\n- Add a comparison to Progressive Neural Networks (PNNs), showing that our method substantially outperforms PNNs.\n- Added a response (see the thread above under \"General Response\") to the new claim that we are \"mostly measuring overfitting.\" Our comment argues why we do not believe this to be the case.", "title": "Response"}, "S1ceCcJwg": {"type": "rebuttal", "replyto": "Sy5VLKT8x", "comment": "\nThere are some errors in this comment:\n\n1. \"a powerful model with 40M parameters will overfit 5-digit training much easier than a less-powerful LSTM with half that many, and, clearly, a 32-parameter model will not overfit.\"\n\nIt is not generally valid to compare model capacity just by counting parameters, especially when dealing with very different model classes. It is also not valid in the specific case being discussed here.  In [1], the Neural GPU model has 200k parameters while the baseline LSTM has 30k parameters, but the LSTM still \"overfits\" (in the strong generalization sense) on the Long Binary Addition task while the Neural GPU does not. Further, [2] states \"It is not self-evident that larger [Neural GPU] model would generalize better, but the empirical results are clear.\"\n\nTo confirm this in our specific setting, we ran additional experiments reducing the number of parameters for the Neural GPU, and the trend is that generalization performance gets worse as the number of parameters decreases (see the generalization performance on 8-digit expressions below). Thus, the issue with the Neural GPU is not overfitting, and explaining generalization performance in terms of number of parameters is overly simplistic and does not match experimental results.\n\n# filters | 32 | 64 | 128 | 256 | 512\n# parameters | 169k | 671k | 2.7M | 10.7M | 43.8M\n8-digit accuracy | 22.5% | 40.1% | 62.7% | 63.3% | 68.6%\n\n\n\n2. \"it was clearly established that Neural GPU generalizes much better than a LSTM baseline on arithmetic tasks, while you show the opposite\"\n\nWe are aware of only two experiments where Neural GPU was compared to LSTM. These are the ones in [1] showing that Neural GPU outperforms LSTM+attention on long addition and long multiplication when:\n(a) data are represented in binary,\n(b) expressions are always of length 2, and\n(c) a separate model is trained for each operator (addition vs multiplication).\n\nFurther, it does not appear that the experimental protocol used in [1] is applicable to the case where there are only short data sequences available for training. Specifically, the results rely upon access to the longer test data in order to select which setting from the grid should be used, and the text reports that generalization performance is sensitive to the choice of seed.\n\nIn contrast, in our experiment:\n(a) data are represented in decimal\n(b) expressions are of variable length,\n(c) a single model is trained on expressions that have a mix of operators, and\n(d) we do not assume access to longer validation data.\n\nOur setting is more challenging and defines a stricter definition of strong generalization than [1] (namely, generalizing beyond the length of data used for training *or validation*). In our setting, there is no evidence that Neural GPU generalizes better than an LSTM baseline. \n\nRegarding (a), the literature [1, 2] reports that Neural GPU performance degrades when numbers are represented in decimal, but there are no experiments showing how the LSTM baseline is affected.\n\nRegarding (b) & (c), [2] experiments with evaluating variable-length expressions with mixed mathematical operators (albeit on binary numbers). They report that a Neural GPU is not able to generalize well to longer sequences, which is consistent with our experiments. There is no LSTM baseline in [2].\n\nIn summary, there is no contradiction between our experiments and existing work, and our experiments are not \"mostly measuring overfitting\".\n\nOur paper is about supplying a very different type of inductive bias than models like an LSTM or Neural GPU. There are different assumptions being made in NTPT (the key one being that there is a component of the model that is well described using source code). We don't claim that this is the right assumption for all tasks (e.g., image classification), but there is a large space of tasks (e.g., evaluating a mathematical expression, navigation, complex policies in RL) where this assumption is valid, and in these cases we believe NTPT-like models will generalize more strongly from less data than models like LSTM or Neural GPU.\n\n\n[1] \u0141ukasz Kaiser & Ilya Sutskever; Neural GPUs Learn Algorithms (ICLR 2016)\n[2] Eric Price, Wojciech Zaremba & Ilya Sutskever; Extensions and Limitations of the Neural GPU (submitted to ICLR 2017)\n", "title": "Clarifications"}, "S1RNuo1Lg": {"type": "rebuttal", "replyto": "HJStZKqel", "comment": "\nWe thank the reviewers again for their suggestions. We have now\u00a0rewritten the experiment section, and believe that it is considerably clearer and more complete thanks to their suggestions. The main\u00a0changes that address\u00a0the reviewers' concerns are:\n\n(1) We have\u00a0improved the description of the baseline models and included a Progressive Neural Network baseline in the 2x2 tasks as requested. These updates can be seen in section 4, figure 6c and figure 7.\n\n(2) We additionally study a Neural GPU baseline for the MATH task as an example of a more modern neural architecture for learning algorithms. In addition, we have pointed out that the MATH task is similar in complexity to tasks in contemporary literature [1]. These updates can be seen in section 5.2 and figure 8a.\n\n(3) We have extended the training of the LSTM\u00a0baseline in the MATH\u00a0task beyond two digit expressions and ensured that we have trained all models to convergence. We\u00a0also now test on\u00a0longer expressions up to\u00a016 digits. These updates can be seen in section 5.2 and figure 8b.\n\nThe qualitative conclusions of the paper are unchanged: \n\n(1) We demonstrate simultaneous learning of source code and neural networks to solve algorithmic tasks with perceptual inputs.\n\n(2) In a lifetime of learning, NTPT outperforms all baselines, and is the only model to demonstrate reverse transfer.\n\n(3) The source code representation of the algorithm for the MATH task leads to better generalization performance than baseline models on long expressions.\n\n[1] Eric Price, Wojciech Zaremba & Ilya Sutskever; Extensions and Limitations of the Neural GPU (submitted to ICLR 2017)", "title": "Revision in response to reviewers"}, "SyqFvQaLx": {"type": "rebuttal", "replyto": "B1rWG728e", "comment": "\nYes, the NTPT model for MATH contains 32 independent floats that define distributions over 19 discrete values. These discrete values define a combinatorially large space of around 10^6 different programs in the MATH model (and in other cases the space is larger, such as fig 3b having a program space of over 10^12 syntactically distinct programs). In addition to these parameters, there are 0.5M parameters in the \"perceptual\" neural networks that define the mappings from images to distributions over discrete values in the programs.\n\nWhile it may seem that 32 values is small, there are a few things to consider:\n- Each joint configuration of the discrete variables gives rise to a different program; a small number of variables can parameterize a very large space of functions\n- NTPT separates the parameterization of the \"algorithmic\" and \"perceptual\" components of the model. There are many tasks where the algorithmic component is relatively simple given a good mapping from perceptual to discrete representations (e.g., navigation). If we can compactly parameterize the algorithmic component and do a good job of jointly learning the perceptual and algorithmic components (as NTPT allows), then we expect stronger generalization, which is what we see in experiments.\n- Representationally it is easy to specify a Turing machine or assembly code-like model for the algorithmic component of a NTPT model (see [1]), so there is no ceiling on the representational capacity of the algorithmic component. The challenge currently holding us back from scaling up further is not the representational capacity of the model; it's in the optimization.\n\n[1] Alexander L. Gaunt et al. Terpret: A probabilistic programming language for program induction. (http://arxiv.org/abs/1608.04428)\n", "title": "Parameter count"}, "r1QtCYt8x": {"type": "rebuttal", "replyto": "SJlPfk7Ue", "comment": "\nThank you for your comment. In light of your questions we have made three further improvements to the generalization section (5.2) and Fig 8 (updates shown in red in the pdf). \n\n------------------------------------\n(1) Dropout \n\n_Summary_: We improved the performance of both the LSTM baseline and NTPT by training for longer. \n\n_Details_: We find that dropout significantly slows down learning, but ultimately, by using dropout we were able to improve the baseline performance by increasing the baseline model size to a 3 layer LSTM with 1024 units in each layer (details of this architecture are now included at the end of section 4). This configuration required 8M training instances to reach convergence.\n\nTo be fair, we also extended the training of the NTPT model to 8M instances. The source code learned by NTPT converges rapidly (after 30k instances), but we find that extended training on the MATH task continues to improve digit classifier (net_0) in the library of neural functions from 95% to 97.5% (converged) accuracy on a single digit classification task. The performance of the NTPT model is very sensitive to the performance of the classifiers for long arithmetic expressions (e.g. consider the ratio 0.975^16/0.95^16 = 1.5 for the 16 digit case). Therefore, this small boost in classifier performance has a significant effect on the generalization accuracy. \n\n_Numerical values_: Below we provide the requested numerical values for the accuracies plotted in Fig. 8b:\n\n# Digits | 2 | 3 | 4| 5| 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16\nLSTM (2-digit) | 96.2 | 19.6 | 18.2 | 19.0 | 18.0 | 17.6 | 18.3 | 17.7 | 18.4 | 18.9 | 18.9 | 18.5 | 18.7 | 17.8 | 19.1\nLSTM (5-digit) | 94.2 | 92.1 | 90.8 | 89.3 | 88.4 | 88.0 | 87.0 | 85.7 | 85.2 | 85.0 | 84.4 | 84.4 | 83.9 | 82.9 | 82.8\nNTPT (2-digit) | 96.2 | 94.3 | 93.3 | 92.1 | 91.2 | 91.3 | 90.1 | 89.6 | 88.8 | 88.8 | 88.7 | 88.1 | 88.3 | 87.3 | 87.1\n\nThe numbers in parentheses indicate the maximum expression length observed during training. The conclusion remains unchanged: NTPT trained on only 2-digit expressions outperforms the best LSTM baseline even when the LSTM is provided with up to 5-digit expressions during training.\n\n_Changes to paper_: We have updated Fig 8b with the results from this extended training (and we include the numerical values for the 16 digit test accuracy). We also include a comment on the sensitivity of NTPT to the classifier accuracies in section 5.2.\n\n------------------------------------\n(2) Neural GPU baseline\n\n_Summary_: We performed an investigation into a Neural GPU baseline on the MATH task as requested, but find that it shows worse generalization than the LSTM. Independent findings from [1] support our conclusion.\n\n_Details_: We obtained an implementation of the Neural GPU from the original authors [2] (which includes all training tricks highlighted in [3]). To assess this model, we first evaluated the relative performance of the Neural GPU, the LSTM and TerpreT models on a simpler, perception-free version of the MATH task where digits and operators are supplied as one-hot vectors and there is not the additional complication of decoding handwritten symbols. Following [1] we tried Neural GPUs with 128, 256, or 512 filters, and found best performance with the largest model. We find that the Neural GPU can perfectly fit the training data (expressions of length <=5 digits), but generalizes very poorly to longer expressions. The LSTM on the other hand is seen to be a very strong baseline (see the numerical values below and Fig. 8a).\n\nOur findings are supported by [1] where the authors consider a task similar to our MATH problem. There are two main differences between our experiment and [1] which affect the difficulty of the task in different ways \u2013 these are:\n(a) we use a decimal representation of numbers while [1] uses binary (both [3] and [1] suggest that decimal representation tends to increase the difficulty of the task for the Neural GPU)\n(b) we consider shorter test cases: up to 32 tokens (digits + operators) on the input vs. 201 tokens in [1] (this reduces the difficulty of our task).\n\nUsing a 512-filter Neural GPU we achieve a 25% accuracy on our 32 token decimal MATH task, and argue that this seems compatible with the results from [1] where a 512-filter Neural GPU achieves a similarly low ~30% accuracy on the 201 token binary MATH task. \n\nOur experimental findings (& [1]) indicate that the LSTM is a much stronger baseline than the Neural GPU for our task. Since the gap in performance is so large on this simplified, perception-free version of the MATH task, we drop the Neural GPU when we re-introduce the perceptual component into the MATH experiments (Fig 8b). \n\n_Numerical values_: Below are the numerical values for the accuracies of all models on the perception-free MATH task (plotted in Fig 8a).\n\n# Digits | 2 | 3 | 4| 5| 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16\nNeural GPU (5-digit) | 100 | 100 | 100 | 100 | 99.56 | 92.82 | 68.55 | 54.35 | 47.36 | 44.82 | 42.72 | 38.18 | 34.33 | 29.0 | 25.0\nLSTM (5-digit) | 100 | 100 | 100 | 99.9 | 99.2 | 98.7 | 98.2 | 96.4 | 96.2 | 95.6 | 94.7 | 93.2 | 93.0 | 91.1 | 92.8\nNTPT (2-digit) | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 \n\n_Changes to the paper_: To justify our belief that the LSTM is a strong baseline, we include the results for the performance of the Neural GPU, LSTM and TerpreT on the simplified, perception-free version of the MATH task in the paper (Fig 8a). A short description of the results in Fig 8a is also included in section 5.2.\n\n------------------------------------\n(3) Parameter counts \n\n_Summary_: The algorithmic part of NTPT has six orders of magnitude fewer parameters than the LSTM baseline. \n\n_Numerical values_: Below we report the number of parameters in each model excluding parameters in the perceptual networks (net_0/1):\nNTPT: 32\nLSTM: 21.1M\nNeural GPU (512 filters): 43.8M\nThe perceptual nets themselves contribute an additional 0.5M parameters to each model. \n\n_Changes to the paper_: These parameter counts are included in Fig 8a.\n\n[1] Eric Price, Wojciech Zaremba & Ilya Sutskever; Extensions and Limitations of the Neural GPU (submitted to ICLR 2017)\n[2] https://github.com/tensorflow/models/tree/master/neural_gpu\n[3] \u0141ukasz Kaiser & Ilya Sutskever; Neural GPUs Learn Algorithms (ICLR 2016)\n", "title": "further improvements to LSTM and additional Neural GPU baseline"}, "H1RMiiJUx": {"type": "rebuttal", "replyto": "HySL-VEHx", "comment": "\nThank you for your comment. We have now included the PNN baseline in the current revision of the paper, and we have improved the LSTM baseline in the MATH task.\n\nWe believe that the new LSTM results now make for a convincing and natural baseline, but to ensure that we have satisfied your concerns, we would like to ask for clarification on the request for more complex fully supervised models for MATH (e.g. the Neural Turing Machine).\nspecifically: \n(i) We are not sure what is meant by 'fully supervised': All of our models are weakly supervised with input-output examples.\n(ii) The Neural Turing Machine\u00a0has been shown to be effective at learning\u00a0copy/recall/sorting algorithms\u00a0which require shifting patterns around in memory. It is not clear that NTMs are more effective than LSTMs\u00a0at learning to perform arithmetic operations, nor is it clear\u00a0how to include perceptual inputs to a NTM. We believe that solving these issues with NTMs might be considered a separate line of work rather than a baseline for the present work.", "title": "Clarification on NTMs"}, "ryNroAMre": {"type": "rebuttal", "replyto": "HJStZKqel", "comment": "We thank all the reviewers for their time and comments on our work. To summarize, two reviewers state that this is an interesting line of research, and that this paper could be a valuable contribution to the emerging field of learning programs from data. However the reviewers raise a concern that the tasks presented in the experimental section are too simple and the baselines may be weak.\n\nIn particular, AnonReviewer1 notes that if strong supervision were available the ADD2x2 task could be solved by a baseline consisting of 2 consecutive linear models. However, in this paper we are concerned entirely with the LPPBE paradigm, where weak supervision and lifelong learning are key. We argue that the ADD2x2 and APPLY2x2 tasks in the experimental section were not chosen for complexity, but instead chosen to cleanly illustrate different benefits of our method relative to a purely neural baseline subject to these LPPBE restrictions. AnonReviewer5 agrees that our choice of tasks allows for a clean illustration of the key qualitative conclusions: success with weak supervision (Figure 5) and avoidance of catastrophic forgetting / demonstration of reverse transfer (Figure 6).\n\nHaving progressed through the illustrative early tasks, the lifetime of learning culminates in the MATH task, which is a non-trivial task of some practical use (evaluation of handwritten arithmetic expressions of arbitrary length). We argue that AnonReviewer1\u2019s analysis concentrates on a linear model for our simplest ADD2x2 task and ignores this more complex MATH task (which even with strong supervision would require at least a recurrent non-linear model).\n\nWe thank the AnonReviewer3 for the suggestion to include a Progressive Neural Network (PNN) as a more modern baseline model. We will include results using PNNs in a final version of the paper. Note that this baseline does not affect Figure 5 (which shows the success of our model over all weakly supervised baselines on a single task), and while PNNs may prevent catastrophic forgetting in Figure 6, they will not exhibit reverse transfer. We therefore feel that the main qualitative messages of the paper are not significantly changed with the addition of this baseline.\n\nUltimately, as we mention in the paper, the complexity of the tasks that we consider is limited by the known difficulty in training differentiable interpreters. The tasks we present in this paper require search over program spaces of size up to 10^12 syntactically distinct programs. Solution to problems of this scale is comparable to the state of the art for methods using weakly supervised differentiable interpreters. It is an open area of research to improve the performance of these models, and this work aims to provide motivation for this line of research by producing the first demonstration of the value of differentiable interpreters as a glue for combining symbolic programming with neural networks.\n", "title": "General response"}, "BJZjpf-4e": {"type": "rebuttal", "replyto": "Hkb8KkxNl", "comment": "Thank you for the question and the nice feedback.\n\nTo answer the last question first, we are unaware of any formal answer to the question. What is learnable depends not only on the supervision but also on the assumptions and priors that are encoded into the model (either explicitly or implicitly). What we infer based upon seeing examples [2, 1, 4]->[1, 2, 4] and [4, 3]->[3, 4] depends on the priors and inductive bias of the model. Thus in some sense, this question boils down to \"what inductive bias can/should be encoded over program space, and how powerful is it to disambiguate a 'correct' program (one that generalizes strongly) from an 'incorrect' program (one that does not)?\"\n\nWe don't have a clear answer to this right now and it is beyond the scope of this paper, but our long-term view is the following: (1) the explicit source code representation provides strong inductive bias towards learning an algorithm that generalizes in a strong way; (2) this inductive bias could be strengthened further by studying how people use programming languages in practice and incorporating this as a prior over source code representations; and (3) learning in a lifelong setting can achieve many of the benefits of a curriculum and thus help towards building up to more complicated programs from simpler ones.\n\nFinally, we believe that an ultimate lifelong learning system should be able to learn from many types of supervision, ranging from very strong to very weak, and I/O examples are one important point on that spectrum that we should be studying.", "title": "It remains to be seen, but we think the future is bright"}, "Hkb8KkxNl": {"type": "review", "replyto": "HJStZKqel", "review": "\nThis is a wonderful paper on program induction, with a very nice framework for combining code with neural networks.\n\nI'd like to ask the authors a simple question, which in no way attempts to diminish the great value of this paper, but rather prompts for clarification.\n\nCan you learn more complex programs such as sorting with weak supervision? (and without curriculum, which amounts to strong supervision of hierarchy). Reed et al learn sorting with only 8 demonstrations, but imitation is of course a stronger form of supervision.\n\nCan the family of tasks that can be learned with weak supervision be easily characterised? Does classical complexity theory offer an answer?\nI think the paper is a bit more solid now and I still stand by my positive review. I do however agree with other reviewers that the tasks are very simple. While NPI is trained with stronger supervision, it is able to learn quicksort perfectly as shown by Dawn Song and colleagues in this conference. Reed et al had already demonstrated it for bubblesort. If the programs are much shorter, it becomes easy to marginalise over latent variables (pointers) and solve the task end to end. The failure to attack much longer combinatorial problems is my main complaint about this paper, because it makes one feel that it is over-claiming.\n\nIn relation to the comments concerning NPI,  Reed et al freeze the weights of the core LSTM to then show that an LSTM with fixed weights can continue learning new programs that re-use the existing programs (ie the trained model can create new programs). \n\nHowever, despite this criticism, I still think this is an excellent paper, illustrating the power of combining traditional programming with neural networks. It is very promising and I would love to see it appear at ICLR.\n\n===========\nThis paper makes a valuable contribution to the emerging research area of learning programs from data.\n\nThe authors mix their TerpreT framework, which enables them to compile programs with finite integer variables to a (differentiable) TensorFlow graph,  and neural networks for perceiving simple images. This is made possible through the use of simple tapes and arithmetic tasks.  In these arithmetic tasks, two networks are re-used, one for digits and one for arithmetic operations.  This clean setup enables the authors to demonstrate not only the avoidance of catastrophic interference, but in fact some reverse transfer. \n\nOverall, this is a very elegant and potentially very useful way to combine symbolic programming with neural networks. As a full-fledged tool, it could become very useful. Thus far it has only been demonstrated on very simple examples. It would be nice for instance to see it demonstrated in all the tasks introduced in other approaches to neural programming and induction: sorting, image manipulation, semantic parsing, question answering. Hopefully, the authors will release neural TerpreT to further advance research in this domain.\n\n ", "title": "How far can one push the notion of weak supervision in program induction", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJJjahzEe": {"type": "review", "replyto": "HJStZKqel", "review": "\nThis is a wonderful paper on program induction, with a very nice framework for combining code with neural networks.\n\nI'd like to ask the authors a simple question, which in no way attempts to diminish the great value of this paper, but rather prompts for clarification.\n\nCan you learn more complex programs such as sorting with weak supervision? (and without curriculum, which amounts to strong supervision of hierarchy). Reed et al learn sorting with only 8 demonstrations, but imitation is of course a stronger form of supervision.\n\nCan the family of tasks that can be learned with weak supervision be easily characterised? Does classical complexity theory offer an answer?\nI think the paper is a bit more solid now and I still stand by my positive review. I do however agree with other reviewers that the tasks are very simple. While NPI is trained with stronger supervision, it is able to learn quicksort perfectly as shown by Dawn Song and colleagues in this conference. Reed et al had already demonstrated it for bubblesort. If the programs are much shorter, it becomes easy to marginalise over latent variables (pointers) and solve the task end to end. The failure to attack much longer combinatorial problems is my main complaint about this paper, because it makes one feel that it is over-claiming.\n\nIn relation to the comments concerning NPI,  Reed et al freeze the weights of the core LSTM to then show that an LSTM with fixed weights can continue learning new programs that re-use the existing programs (ie the trained model can create new programs). \n\nHowever, despite this criticism, I still think this is an excellent paper, illustrating the power of combining traditional programming with neural networks. It is very promising and I would love to see it appear at ICLR.\n\n===========\nThis paper makes a valuable contribution to the emerging research area of learning programs from data.\n\nThe authors mix their TerpreT framework, which enables them to compile programs with finite integer variables to a (differentiable) TensorFlow graph,  and neural networks for perceiving simple images. This is made possible through the use of simple tapes and arithmetic tasks.  In these arithmetic tasks, two networks are re-used, one for digits and one for arithmetic operations.  This clean setup enables the authors to demonstrate not only the avoidance of catastrophic interference, but in fact some reverse transfer. \n\nOverall, this is a very elegant and potentially very useful way to combine symbolic programming with neural networks. As a full-fledged tool, it could become very useful. Thus far it has only been demonstrated on very simple examples. It would be nice for instance to see it demonstrated in all the tasks introduced in other approaches to neural programming and induction: sorting, image manipulation, semantic parsing, question answering. Hopefully, the authors will release neural TerpreT to further advance research in this domain.\n\n ", "title": "How far can one push the notion of weak supervision in program induction", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}