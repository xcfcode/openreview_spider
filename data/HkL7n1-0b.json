{"paper": {"title": "Wasserstein Auto-Encoders", "authors": ["Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf"], "authorids": ["iliya.tolstikhin@gmail.com", "obousquet@gmail.com", "sylvain.gelly@gmail.com", "bs@tuebingen.mpg.de"], "summary": "We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.", "keywords": ["auto-encoder", "generative models", "GAN", "VAE", "unsupervised learning"]}, "meta": {"decision": "Accept (Oral)", "comment": "This paper proposes a new generative model that has the stability of variational autoencoders (VAE) while producing better samples. The authors clearly compare their work to previous efforts that combine VAEs and Generative Adversarial Networks with similar goals.  Authors show that the proposed algorithm is a generalization of Adversarial Autoencoder (AAE) and minimizes Wasserstein distance between model and target distribution. The paper is well written with convincing results. Reviewers agree that the algorithm is novel and practical; and close connections of the algorithm to related approaches are clearly discussed with useful insights.  Overall, the paper is strong and I recommend acceptance."}, "review": {"Bkv3jj_sM": {"type": "rebuttal", "replyto": "BJD-D8Ioz", "comment": "Thank you for the question!\n\nIndeed, we were not completely clear in the text. The argument applies only to the version of WAE-GAN, where the KL divergence is used instead of the JS entropy (which can b estimated in a very similar way using the adversarial training).", "title": "On dropping the mutual information"}, "BysNAdlOf": {"type": "rebuttal", "replyto": "SJOMQOguf", "comment": "Yes, in Theorem 1 the encoder Q(Z|X) is allowed to be Dirac as long as it satisfies the constrain. Of course, it is not always the case that deterministic encoders can match the prior: consider the case when the intrinsic dimensionality of the data is less than the latent space dimensionality (as discussed on page 7 and in more details in [1]).\n\nRegarding Gamma(Y|X)---the conditional part of the coupling---we assume you are referring to the sentence \"we can consider Gamma(Y|X) as a non-deterministic mapping from X to Y\" appearing in the proof of Theorem 1 on page 13. The proof never argues that Gamma(Y|X) should be necessarily random or deterministic, and does not use any of these two assumptions.\n\n[1] Rubenstein, P., Scholkopf, B., Tolstikhin, I. On the Latent Space of Wasserstein Auto-Encoders. https://arxiv.org/pdf/1802.03761.pdf\n ", "title": "Encoders can indeed be Dirac"}, "r1FUJDldG": {"type": "rebuttal", "replyto": "r1kaoIgdf", "comment": "None of WAE implementations in our paper uses Gaussian encoders. Theorem 1 does not constrain encoders in any way and WAEs can be used with any form of random (or deterministic) encoders, including flexible implicit random encoders induced by generative architectures of GANs (and for instance used in [1]).\n\n[1] Mescheder, L. and Nowozin, S. and Geiger, A. Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks. ICML 2017.", "title": "There are no constraints on encoders"}, "rkSWN2rvf": {"type": "rebuttal", "replyto": "SkGcPcZ-z", "comment": "Thank you for this comment. Indeed, this observation provides one more intuitively clear way to explain a difference between VAEs and WAEs. We will use your suggestion in the camera-ready version of the paper.", "title": "WAEs drop the mutual information term in the VAE regularizer"}, "HyBIaDXBM": {"type": "rebuttal", "replyto": "rJSDX-xSG", "comment": "Thank you for the question.\n\nUnfortunately, we did not quite get the point of your Markov chain example. But we would like to make it clear that the paper does not assume anything specific about the encoder Q(Z|X). As long as the aggregated posterior Qz matches the prior Pz, the encoder can be either deterministic or random. The same holds true for the WAE algorithm. We will try to emphasize it better in the updated version of the paper.\n\nThe decoder is indeed a different story: for Theorem 1 we need it to be deterministic, but a very similar result holds also for the random decoder (Supplementary B).", "title": "Regarding random / deterministic encoders in WAE"}, "SJQzLO_gM": {"type": "review", "replyto": "HkL7n1-0b", "review": "This paper satisfies the following necessary conditions for\nacceptance. The writing is clear and I was able to understand the\npresented method (and its motivation) despite not being too familiar\nwith the relevant literature. Explicitly writing the auto-encoder(s)\nas pseudo-code algorithms was particular helpful. I found no technical\nerrors. The problem addressed is one worth solving - building a\ngenerative model of observed data. There is some empirical testing\nwhich show the presented method in a good light.\n\nThe authors are careful to relate the presented method with existing\nones, most notably VAE and AAE. I suppose one could argue that the\nclose connection to existing methods means that this paper is not\ninnovative enough. I think that would be unfair - most new methods\nhave close relations with existing ones - it is just that sometimes\nthe authors do not flag this up as they should.\n\nWAE is a bit oversold. The authors state that WAE generates \"samples\nof better quality\" (than VAE) without any condition being put on when\nit does this. There is no proof that it is always better, and I can't\nsee how there could be. Any method of inferring a generative model\nfrom data must make some 'inductive' assumptions. Surely one could\ndevise situations where VAE outperforms WAE. I think this issue should\nhave been examined in more depth.\n\nI found no typo or grammatical errors which is unusual - good careful\njob!\n\n", "title": "This is a well-written paper which provides a useful generalisation of some existing methods for inferring generative models.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hk2dO8ngz": {"type": "review", "replyto": "HkL7n1-0b", "review": "This very well written paper covers the span between W-GAN and VAE. For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations. The appendix are very useful, and tutorial paper material (especially A). \n\nWhile I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work. There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous. One that could be improved is P(P_X, P_G) where one loses the fact that the second random variable is Y.\n\n\nThis work contains plenty of novel material, which is clearly compared to previous work:\n- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1. I could not verify its novelty, but this seems to be a great contribution.\n- Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max\n- The use of MMD in the context of GANs has also been tried. The authors claim that their use in the latent space makes it more practival\n\nThe experiments are very convincing, both numerically and visually.\n\nSource of confusion: in algorithm 1 and 2, \\tilde{z} is \"sampled\" from Q_TH(Z|xi), some one is lead to believe that this is the sampling process as in VAEs, while in reality Q_TH(Z|xi) is deterministic in the experiments.", "title": "Excellent tutorial papers with novel contributions and convincing results", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJncf2gWz": {"type": "review", "replyto": "HkL7n1-0b", "review": "This paper provides a reasonably comprehensive generalization to VAEs and Adversarial Auto-encoders through the lens of the Wasserstein metric. By posing the auto-encoder design as a dual formulation of optimal transport, the proposed work supports the use of both deterministic and random decoders under a common framework. In my opinion, this is one of the crucial contributions of this paper. While the existing properties of auto-encoders are preserved, stability characteristics of W-GANs are also observed in the proposed architecture. The results from MNIST and CelebA datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward MMD metric and potentially discuss their pros and cons. In some sense, given the challenges in evaluating and comparing closely related auto-encoder solutions, the authors could design demonstrative experiments for cases where Wassersterin distance helps and may be  its potential limitations.\n\nThe closest work to this paper is the adversarial variational bayes framework by Mescheder et.al. which also attempts at unifying VAEs and GANs. While the authors describe the conceptual differences and advantages over that approach, it will be beneficial to actually include some comparisons in the results section.", "title": "A well-written paper that generalizes Wasserstein distance to VAEs ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkU7vv8ff": {"type": "rebuttal", "replyto": "SkxfpL8GG", "comment": "Dear Mathieu,\n\nthank you for the suggestion. We will update the paper accordingly.", "title": "Assumptions clarified"}, "BJ2VpnZff": {"type": "rebuttal", "replyto": "Hk2dO8ngz", "comment": "We thank the reviewer for the positive feedback and the kind words regarding the overview part of the paper.\n\nWe will make sure to make notations clearer and include all the details of architectures used in experiments in the updated version of the paper. Of course we will also open source the code.", "title": "Answer to AnonReviewer1"}, "BkrqpnbGG": {"type": "rebuttal", "replyto": "SJQzLO_gM", "comment": "We are pleased that the reviewer found the paper well written. \n\nWe tried to be modest in our claims, in particular we never implied that WAEs produce better samples for *all data distributions*. As noticed by the reviewer this would be indeed impossible to prove, especially because the question of how to evaluate and compare sample qualities of unsupervised generative models is still open. We will double-check that there are no bold and unsupported statements in the final version of the paper.", "title": "Answer to AnonReviewer2"}, "H1bGp3bfz": {"type": "rebuttal", "replyto": "SJncf2gWz", "comment": "We thank the reviewer for the positive feedback. \n\nComparing properties of WAE-MMD and WAE-GAN is indeed an intriguing direction and we intend to look into the details in our future research. In this paper we only report initial empirical observations, which can be concluded by saying that WAE-MMD enjoys a stable training but does not match Pz and Qz perfectly, while the training of WAE-GAN is not so stable but leads to much better matches once succeeded. \n\nIn this paper we decided that comparing to VAE was sufficient for our purposes: both VAE and AVB follow the same objective of maximizing the marginal log likelihood in contrast to the minimization of the optimal transport studied in our work. However, we do agree that in future it would be interesting to compute the FID scores of the AVB samples. ", "title": "Answer to AnonReviewer3"}}}