{"paper": {"title": "Predictive Coding Approximates Backprop along Arbitrary Computation Graphs", "authors": ["Beren Millidge", "Alexander Tschantz", "Christopher Buckley"], "authorids": ["~Beren_Millidge1", "~Alexander_Tschantz1", "~Christopher_Buckley1"], "summary": "We show that predictive coding algorithms from neuroscience can be setup to approximate the backpropagation of error algorithm on any computational graph.", "abstract": "The backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in multilayer-perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies solely on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures.", "keywords": ["Predictive Coding", "Backprop", "Biological plausibility", "neural networks"]}, "meta": {"decision": "Reject", "comment": "This paper extends recent work (Whittington & Bogacz, 2017, Neural computation, 29(5), 1229-1262) by showing that predictive coding (Rao & Ballard, 1999, Nature neuroscience 2(1), 79-87) as an implementation of backpropagation can be extended to arbitrary network structures. Specifically, the original paper by Whittington & Bogacz (2017) demonstrated that for MLPs, predictive coding converges to backpropagation using local learning rules. These results were important/interesting as predictive coding has been shown to match a number of experimental results in neuroscience and locality is an important feature of biologically plausible learning algorithms.\n\nThe reviews were mixed. Three out of four reviews were above threshold for acceptance, but two of those were just above. Meanwhile, the fourth review gave a score of clear reject. There was general agreement that the paper was interesting and technically valid. But, the central criticisms of the paper were:\n\n1) Lack of biological plausibility\nThe reviewers pointed to a few biologically implausible components to this work. For example, the algorithm uses local learning rules in the same sense that backpropagation does, i.e., if we assume that there exist feedback pathways with symmetric weights to feedforward pathways then the algorithm is local. Similarly, it is assumed that there paired error neurons, which is biologically questionable.\n\n2) Speed of convergence\nThe reviewers noted that this model requires many more iterations to converge on the correct errors, and questioned the utility of a model that involves this much additional computational overhead.\n\nThe authors included some new text regarding biological plausibility and speed of convergence. They also included some new results to address some of the other concerns. However, there is still a core concern about the importance of this work relative to the original Whittington & Bogacz (2017) paper. It is nice to see those original results extended to arbitrary graphs, but is that enough of a major contribution for acceptance at ICLR? Given that there are still major issues related to (1) in the model, it is not clear that this extension to arbitrary graphs is a major contribution for neuroscience. And, given the issues related to (2) above, it is not clear that this contribution is important for ML. Altogether, given these considerations, and the high bar for acceptance at ICLR, a \"reject\" decision was recommended. However, the AC notes that this was a borderline case."}, "review": {"lWegAXK_uVJ": {"type": "review", "replyto": "PdauS7wZBfC", "review": "### Update after author responses: \nWhile the author address some of my comments, I would have still liked to see a more detailed discussion of how the algorithm compares in terms of algorithmic scaling, which I think is relevant because it is a fundamental property of the algorithm, even if it is targeted towards understanding biology. So my score remains the same.\n\nSummary:\n\nThe authors extend recent work on MLPs to show that predictive coding converges asymptotically to exact backprop gradients on arbitrary computation graphs. They construct predictive coding networks for common architectures and show that it works well.\n\nOverall, I vote for an accept because I think the generalisation is quite useful and interesting for training deep networks with local learning rules. The authors demonstrate that this method works, but haven't demonstrated its computational advantages clearly enough. There are some issues of clarity that I have also outlined below.\n\nStrengths:\n+ The generalisation of the earlier MLP results to arbitrary computational graphs is quite powerful esp. since it can be applied to most deep learning architectures.\n+ The experimental evaluation includes all popular deep learning architecture, and it's impressive that this works on all of them. The experimental evaluation is also extensive.\n  \nWeaknesses:\n- The increase in computational cost (of 100x) is mentioned quite late and seems to be glossed over a bit.\n- Due to the potential for parallelisation in the predictive coding network, a comparison of wall-clock time for training on highly parallel setups might have been very interesting.\n- For RNNs and LSTMs, the equivalent predictive coding network is generated after unrolling the network, which means that the predictive coding network has no memory advantage over BPTT and a huge performance penalty. This is related to the previous point, where the utility of the predictive coding network is not demonstrated sufficiently.\n\nClarity:\n- Many of the figures are almost unreadable on paper. E.g. Fig. 3.\n- Algorithm 1 does't seem to be referenced anywhere.\n- In fig. 1 bottom, $\\delta$ missing in the denominators\n- Eqn. 3 is a bit sloppy, where derivative w.r.t $\\theta$ is suddenly equated to a derivative w.r.t $\\theta_i$.\n- If $\\epsilon_i = v_i - \\hat{v}_i$ then eqn. 7 is inconsistent with this, since $\\frac{d\\epsilon^*_i}{d\\theta}$ would be $-\\frac{d\\hat{v}_i}{d\\theta_i}$ (missing -ve sign). Unless I misunderstood something.", "title": "Generalisation of predictive coding approach to arbitrary computational graphs could be quite powerful", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "qV5M0UvC5gZ": {"type": "rebuttal", "replyto": "0wY69t2aaQ_", "comment": "We agree with the reviewer that demonstrating that the algorithm works with alternative loss functions (such as crossentrpy) is important to show the generality of the method. In the revised version (now up), we have added experiments comparing predictive coding and backprop with the crossentropy with the CNN architecture (Figure 7) and shown that performance closely matches that of the backprop network.", "title": "response to reviewer 4 continued"}, "PCLCBLO5JJY": {"type": "rebuttal", "replyto": "l9pURe5Ldmw", "comment": "\nThanks for your detailed comments -- this is all really helpful to improve the manuscript. I appreciate the time it's taking you to go through this -- so thank you.\n\nTo start out with the notation -- I think I understand your confusion now. $p(\\\\{v_i\\\\})$ is a full generative model including the prior. Perhaps I should write it out like $p(v_0 \u2026 v_n) = p(v_0) \\prod_i p(v_i | par(v_i))$ to make it clear it is a generative model. Now importantly, this is a generative model in that it can assign a probability to any assignment of the v_is and can be used to generate the data -- i.e. \u201clabels\u201d from the data. I.e. suppose we clamp the prior to some data item $p(v_0) = \\delta(v_0 - D_i)$ then $p(v_1 | v_0) = N(v_1; f(v_0),1)$ etc. If we want to know the max likelihood assignment of the labels $v_L$ given data clamped to v_0, then it is just $f(f(f(....v_0)))$ -- i.e. a standard feedforward pass through the networks. This is the inference step.\n\nI will also write out the variational posterior more clearly as $q(v_i | Par(v_i), Chi(v_i))$ to make clear that this is a conditional distribution.\n\nYou can also use this approach to reconstruct the input from the labels by clamping the labels and running the inference steps multiple times. My understanding of the Orchard and Sun paper is not that it is impossible to run the network backwards to generate an input -- and thus the model is not actually a generative model -- but rather simply that the inverse problem of trying to generate data from the labels is fundamentally ill-posed (as there are loads of potential images for a given label) and thus naively reconstructing it will find a poor solution, which they propose to tackle with additional regularisation. This does not mean, however, that the model is not generative.  I haven't actually tried it with the more complex networks -- i.e. the CNNs etc, and I suspect generation will be poor due to the ill-posedness of the problem as found in the Orchard and Sun paper and it is kind of orthogonal to the point of this paper, but am happy to play around with this if you think it would be valuable.\n\nI am not following why you think that the posterior is known. Importantly we are not just looking at the posterior over the labels (which is known in training since we know the labels), but the posterior distribution over all the nodes in the graph, which is not known. I.e. suppose we take a three layer network with the generative model $p(v_0, v_1, v_2)=p(v_0)p(v_1 | v_0)p(v_2 | v_1)$ and then clamp the prior $v_0$ to the data and $v_2$ to the targets, then the true posterior is $p(v_1 | v_0, v_2)$ which is not immediately known since it depends on both the data and the labels together. This is why we need to approximate this quantity with an iterative variational inference scheme. \n", "title": "Response to comment"}, "iTdHmF0DKih": {"type": "rebuttal", "replyto": "lOigDgNZdv", "comment": "We thank the reviewer for their insightful comments. Regarding the originality, we still believe that the extension to arbitrary graphs (including CNNs and RNNs) instead of just a MLP network is not completely trivial and has not been shown before in the literature. We agree that it is unfortunate that the derivations for the CNN, LSTM etc are in the appendix, which is done primarily for space constraints. If space permits, we will be happy to move some of this material into the main text.", "title": "Response to reviewer 3"}, "G2ELhvNMUIp": {"type": "rebuttal", "replyto": "JW3A19WCM38", "comment": "Continued from before...\n\n4.) The reason that the inner-loop optimisation problem is not trivial given the fixed prediction assumption, is that the update rule for the $v_i$ depends not only on the current layer (which would be trivial) but also the layer above. This means that information and dependencies flows between layers, so that the total (across all layers) sum of prediction errors is minimized rather than just each layer\u2019s prediction error  independently. Crucially, at the output layer the real labels are fed into the network and generates a prediction error which cannot be trivially minimized. Over the course of the optimization, this output prediction error is slowly spread across all nodes in the network due to the operation of the inner-loop optimisation rule until it becomes equivalent to the backpropagation gradients at convergence.\n\nAs far as the authors are aware, this does not relate to the generative/discriminative model distinction. Importantly, the predictive coding network, although tested in a discriminative setting, is actually still a generative model. Simply one where the \"discriminative\" direction -- generating labels from data is straightforward. It is also possible to run the network \u201cbackwards\u201d so that the labels are fixed and the inner-loop will optimize a prediction of the data (i.e. reconstruct the image). \n\n5.) The inference process in our network is just a simple feedforward pass. In the inference phase, the predictive coding network is equivalent to a standard ANN. The key difference is that it can be trained with a local learning algorithm instead of backprop. If the network is run \u201cbackwards\u201d to generate data from the labels, then the inference would be done through the E-step of an EM scheme.\n\nSpecific comments on typos and suggestions:\n1.) Regarding the point that the generative model does not include the prior p(v_N) -- this is technically not true since the generative model description is defined as p(v_i | parents(v_i)) for every node, including the prior. It is just that the parents of the prior is the empty set, giving us just p(v_N). Nevertheless, we shall definitely clarify this in the revised version of the paper. Moreover, in derivations in Appendix D, we do explicitly include the prior.\n\n2.) The derivation of line 2 of Equation 1 from line 1 is provided in Appendix D, as well as the derivation of the update rules (Equations 2 and 3)\n\n3.) We do consider the posterior to be a conditional distribution. We denote the variational distributions Q(x) as not an explicit conditional distributions, since they only depend on the data through an optimization process, not directly (as in an amortised encoder). If this causes confusion, we are happy to change the notation. We always denote the true posterior ( p(v_{1...N | v_0, v_N) ) as a conditional distribution. Perhaps the confusion has arisen from the fact that the first equality in Equation 1 is the ELBO and we represent the joint as p({v_i}) which looks like a marginal distribution?\n4.) You make a really good point about the organization of the appendix. We shall reorder the appendix in the revised version to reflect the order in which the sections are cited in the main text.\n\nWe hope that this response addresses some of your concerns -- thanks again for the really detailed response. It will definitely help us improve the manuscript going forwards.", "title": "Response to reviewer 4 -- continued"}, "JW3A19WCM38": {"type": "rebuttal", "replyto": "9evPd1ch91R", "comment": "We thank the reviewer for their extremely detailed and thorough review, which will undoubtedly help us improve the manuscript, and also for the many typos and minor mistakes they have highlighted which we are happy to fix.\n\nBelow are responses to the detailed comments:\n\n1.) The gaussian generative model does assume a mean-square-error loss function. This does not mean that other loss functions cannot be used. If you want to use another loss function that can be represented as the log of a distribution, then the final output distribution p(v_N | v_N-1) can be set to the distribution necessary to represent the other loss function. If the loss function cannot be represented as the log of a conditional distribution, then the predictive coding algorithm (i.e. equations 2 and 3) will still work, except with the algorithm as a whole will no longer have the elegant interpretation as variational inference on a specific generative model. It is important to note that all the other nodes except the output node can still be gaussian and optimize a mean-square-error regardless of the choice of loss function at the output layer. Nevertheless, this is a good point and we have added a footnote to this effect in the revised version of the paper.\n\n2.) The reviewer is correct that this current proposal of using predictive coding does not address the weight transport problem -- only the issues of nonlocality and sequential computation inherent in backprop. In the literature there are already several proposed remedies for the weight transport problem such as Feedback Alignment (Lillicrap 2016) and learning the backwards weights (Amit 2019) which could equally well be used in the predictive coding algorithm as compared to backprop networks where they have currently been tested. Indeed, more recent work (https://arxiv.org/pdf/2010.01047.pdf), has shown that the weight transport problem can be addressed in the predictive coding framework through a set of independent learnable backwards weights which can be trained with a Hebbian learning rule. In a revised draft (to follow soon), we shall include a section discussing the overall biological plausibility of the algorithm and linking to these papers.\n\nWe have added the following paragraph discussing this into the related work section of the revised version of the paper (to follow shortly):\n\n\"\"\"\"\"\"\nIt is important to note that predictive coding, as advanced here, still retains some biologically implausible features. Although using only local and Hebbian updates, the predictive coding algorithm still requires identical forward and backwards weights, as well as mandating a very precise one-to-one connectivity structure between value neurons $v_i$ and error neurons $\\epsilon_i$. However, recent work () has begun to show that these implausibilities can be relaxed using learnable backwards weights instead of requiring weight symmetry, allowing for learnable dense connectivity between value and error neurons, without harm to performance in simple MLP settings.\n\"\"\"\"\"\"\n\n3.) The main advantage of the locality of the algorithm is theoretical -- that it allows for a parallel implementation and, crucially, is a step closer to the kind of biologically plausible credit assignment and learning that could take place in the brain. We believe that this is of intrinsic intellectual interest, although it may also prove useful computationally to aid in the construction and design of neuromorphic hardware, or in training deep neural networks in highly parallel settings. In theory the parallel properties of this algorithm could allow for it to be faster than backprop in a sufficiently parallel setup, potentially on neuromorphic hardware. However, this is an avenue for future work and out of scope for this paper.\n\n", "title": "Response to reviewer 4"}, "zl9hqELu5Us": {"type": "rebuttal", "replyto": "lWegAXK_uVJ", "comment": "We thank the reviewer for their helpful and insightful review, and also thank them for their comments on clarity and alerting us to the various typos. Regarding Eqn 3 and 7, this was indeed an oversight on our part -- all the $\\theta$s should be $\\theta_i$s and the -ve sign is missing. We will fix this in the revised version (to follow soon), and also increase the size of the figures, given the extra page, especially Figure 3.\n\nRegarding the additional computational cost, 100x provides an upper bound. This figure is for (almost) complete convergence with a relatively low learning rate. Preliminary experiments indicate (as can also be seen by eyeballing figure 2 (left) that convergence can require fewer iterations with higher learning rates -- often about 10-20 iterations in practice. Nevertheless, we do not envisage the contribution of this paper to be a practical algorithm directly competitive with backprop in terms of computational cost. Instead, we have presented a fully parallelizable and local approximation to backprop that has a number of interesting and biologically plausible properties. We hope that demonstrating these properties inspires the development of more efficient algorithms in the future, and helps contextualise research into backpropagation and the brain. However, we do agree that testing the inherent parallelism of the algorithm both in a cluster environment as well as on neuromorphic hardware would be a very interesting project, although out of scope for the current paper.\n\nWe also agree about predictive coding on recurrent networks like RNNs or LSTMs not being directly useful in terms of memory or particularly in terms of biological plausibility either. The aim in showcasing these networks was primarily a proof-of-concept to show that predictive coding could be applied on very complex graphs like that of an unrolled LSTM rather than as a competitive algorithm.\n", "title": "First response to reviewer 2"}, "KZd1YE1ytn": {"type": "rebuttal", "replyto": "b9oWwaQTTlJ", "comment": "We thank the reviewer for their detailed review and positive appraisal. With regards to softening  the claim of biological plausibility to \u201cpotentially biologically plausible\u201d, we agree with this point and intend to include a more detailed discussion of this in the revised paper (to follow shortly). Specifically, we will discuss the status of the dedicated error nodes, the symmetric backwards and forward weights, and the fixed-prediction assumption (which is likely a limitation of biological plausibility). Also note that there has been recent work showing that some of the less biologically plausible aspects of predictive coding -- such as symmetric weights and  one-to-one error neuron to value neuron connectivity can be removed without significantly  harming the performance of the algorithm (https://arxiv.org/pdf/2010.01047.pdf) in simple MLP architectures. We have added a short discussion of this in the related work section of the revised version of the paper.\n\nSpecific responses:\n1.)  The fixed-prediction assumption as stated is a limitation of biological plausibility. However, it is also important to note that the fixed-prediction assumption is only required for exact convergence to the gradients computed by backprop. Predictive coding without this assumption has been shown to attain very good classification performance (Whittington and Bogacz 2017) without this assumption, but no longer converges exactly to backprop. We anticipate that a good avenue for future work would lie in figuring out whether this assumption can be relaxed, or the design of predictive coding inspired algorithms which do not require it. The fixed-prediction assumption is required for exact convergence to the backpropagated gradients because for the gradients to match the output of the network must match the forward pass of the corresponding ANN upon which backprop is performed. Without the fixed-prediction assumption, the outputs of the network are allowed to evolve during convergence, thus leading to different gradients being computed.\n\n2.)  Above 0.1 the update rules can become unstable (0.1 is already very high as a euler-integration time-step for an ODE). This happens especially in the long computation graphs of the LSTM. CNNs can typically tolerate learning rates up to about 0.2-0.3 without serious danger of instability.\n\n3.) For the experiments in this paper we did not use a convergence condition but ran the predictive coding update for 100 iterations,which we found empirically to yield very good convergence in almost all cases. This was further verified by comparing the estimated predictive coding gradients against the true backprop gradients (as in Figure 5).  In practice, a convergence condition can be constructed in a principled manner by using the variational free energy (i.e. the sum of prediction errors). However, the aim of the paper was to establish and verify the connection between predictive coding and backprop, rather than optimize for computational overhead.\n\n4.) We investigated the number of iterations required to reach a certain level of convergence and found a roughly sublinear relationship between graph size and iterations, similar to Figure 9. We have included this graph in the revised version of the manuscript. We found the relationship to be somewhat sublinear, so that the number of iterations required to reach a specific convergence threshold does not scale linearly with computation graph size but asymptotes at about 200-300 iterations.\n\nWe hope these points help address and clarify the reviewer's comments and concerns\n", "title": "First response to reviewer 1"}, "9evPd1ch91R": {"type": "review", "replyto": "PdauS7wZBfC", "review": "#### Summary of the paper\nIn their paper, the authors demonstrate that Predictive Coding (PC) is a local approximation of back-propagation and could then be interpreted with Hebbian learning rule (a neuro-plausible learning rule). This result has been first demonstrated by [1] with MLP network (on the MNIST dataset) and the presented paper extend this finding to CNNs (on CIFAR10, CIFAR100 and SVHN), RNN and LSTM. \n\n#### Pros\n* The authors provided experimental evidences on a wide variety of networks' type and databases.\n* The link between neuro-plausible learning rule and back propagation is interesting.\n* The paper is well situated in the literature.\n* The authors are providing the code for clean reproducibility\n\n#### Cons\n* The mathematical definition and notation of the paper are not rigorous enough. It makes the paper unclear and hard to follow.\n* Some crucial points would have deserved in-depth discussion and are just ignored (see below)\n* The paper is not well enough motivated: what\u2019s the point of such a local approximation beside the neuro-plausibility (faster ? Consume less resources ? \u2026)\n* The demonstration seems to include only one kind of loss function, which does not match the claim of the paper\n\n#### Recommendation\nGiven the limited impact and the lack of clarity of the paper, I would tend to reject the article.\n\n#### Detailed comments:\n* The gaussian parametrization used by the authors constrains the comparison between PC and backprop to networks with L2 loss function. One cannot claim to approximate arbitrary computational graph if one demonstrates the approximation on a specific loss function (which is known to poorly perform on classification problem). So could your framework be generalized to more effective loss function like cross entropy ? If yes, what would be the underlying probabilistic hypothesis ? This should be included in the paper, as it will strongly strengthen your claim.\n\n* The PC framework proposed by the authors propose a solution to the \u2019non locality\u2019 of the back propagation (to be a bio-plausible mechanism). However the authors also raised the weight transport problem. On my understanding the proposed framework is still suffering from weight transport as the backward connection weights are the transpose of the feedforward one (due to the derivation of the forward operator). The paper would deserve an in-depth discussion concerning this point.\n\n* What is the computational advantage of local approximations ? Is it saving computational resources (computational time, memory\u2026) ? A comparison of the algorithmic complexity between PC and back-propagation would be valuable to support your claim. In the discussion, the authors mention that their framework, being substantially more expensive than back-prop network, could be deeply parallelized across layer. The authors should provide experimental or theoretical evidences that such parallelization is enough to mitigate the higher number of inference steps (i.e. 100-200) needed by their PC framework.\n\n* The concept of \u2018fixed-prediction assumption\u2019 introduced by authors in the paper considers that each (v_i) are fixed to their feedforward value. Then what is the point of the Eq. 2, as you already know the value of the activity vector? On my mind this is here a crucial point, as this is dealing with the core principle of PC : an inner-loop (i.e. the expectation step) that find the most likely v_i, and an outer loop (i.e. the maximization step) that update the parameters. I have the intuition that this problem arises because the authors are tackling a discriminative problem (i.e. finding a mapping between inputs and labels) using a generative model (PC is a generative generative model as described by [2, 3]). Can you please clarify this point ?\n\n* What is the testing procedure of your network ? Is it a simple feedforward pass (which I suspect) or is it an Expectation-Maximization scheme. Your algorithm 1 shows only the training procedure (as you need the label information to perform the computation). If this is a simple feedforward pass, what would be the advantages of the inference process (more robustness ? Better prediction ?)\n\n## Typos and suggestions to improve the paper :\n* The authors state (3rd paragraph page 3) that they are considering a generative model. If it is the case, the formula p(v_i) = product(p(v_i | parent(v_i)) is inaccurate as the authors forgot the prior p(v_N).\n* Eq 1 : The derivation between the first and the second line of Eq.1 has to be demonstrated or referenced (at least in annex)\u2026\n* The authors consider the posterior is a marginal probability (see eq1, and subsequent paragraph). In general the posterior is a conditional probability (this specific point makes your equation 1 hard to grasp because readers are not making the link with the classical ELBO, i.e. the negative free energy). In general, the probabilistic notations are not rigorous enough, and it makes the rest of the mathematical derivation complicated to follow. \n* The authors should reorganize the Annex to make sure it follows the reading order (Appendix D is cited first)\n* Caption of Figure 1 : backwawrds \u2014> backwards\n* Page 3, 2 lines below eq 1 : as as \u2014> as \n* Page 4, 4 lines below eq 3 : forwards \u2014> forward\n* Figure 3, which is on my mind the most import one, is shown but not cited in the core text.\n\n[1] Whittington, J. C., & Bogacz, R. (2017). An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5), 1229-1262.\n\n[2] Rao, Rajesh PN, and Dana H. Ballard. \"Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.\" Nature neuroscience 2.1 (1999): 79-87.\n\n[3]Friston, Karl. \"A theory of cortical responses.\" Philosophical transactions of the Royal Society B: Biological sciences360.1456 (2005): 815-836.", "title": "The paper is not clear enough", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "b9oWwaQTTlJ": {"type": "review", "replyto": "PdauS7wZBfC", "review": "The paper extends prior work on equivalence between predictive coding and backprop in layered neural networks to arbitrary computation graphs. This is empirically tested first on a simple nonlinear scalar function, and then on a few commonly used architectures (CNNs, RNNs, LSTMs), confirming the theoretical results. The importance of this advance is highlighted by noting that the demonstrated equivalence shows how in principle modern architectures could be implemented in biological neural systems, and that the highly parallel nature of predictive coding could lead to efficient implementations in neuromorphic hardware.\n\nThe paper is very well written, easy to read, and includes a nice introduction section with a fairly comprehensive overview of backprop, and the problems related to its potential implementations in biological systems. I also appreciated the \"tension in a chain\" metaphor illustrating the dynamics of backprop and predictive coding. That the exact backprop gradients are computable in a fully local system with Hebbian plasticity for an arbitrary graph is an interesting and promising result.\n\nThroughout the text, predictive coding is quoted as biologically plausible. This isn't strictly true as noted already in (Whittington & Bogacz, 2017), as e.g. dedicated error nodes are not known to exist for every cell in the brain. I'd suggest calling this \"potentially biologically plausible\", and including a short discussion on how these plausibility concerns could be addressed.\n\nAll in all, the results are interesting, open up interesting directions for future work, and I recommend the acceptance of the paper.\n\nAdditional questions/suggestions:\n- Is the fixed-prediction assumption a limitation to biological plausibility?\n- Fig. 2 shows the model converging at high inference learning rates for the case of the scalar function. Is the 0.1 rate used for CNNs the max that was stable, or could higher values be used to reduce the computational overhead?\n- What convergence condition was used?\n- How does convergence speed depend on the size/diameter of the computational graph? This is similar to Fig. 9, but asking a slightly different question -- i.e. how many iterations are needed to reach convergence as a function of graph size.\n\nTypos:\nFig. 1 caption: \"backwawrds\"\n", "title": "Prective coding shown to converge to backprop gradients for abritrary computational graphs", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "lOigDgNZdv": {"type": "review", "replyto": "PdauS7wZBfC", "review": "##########################################################################\n\nSummary:\n \nAuthors propose that predictive coding gives similar convergence as backprop algorithms by extending the work of (Whittington &Bogacz 2017,  https://www.mrcbndu.ox.ac.uk/sites/default/files/pdf_files/Whittington%20Bogacz%202017_Neural%20Comput.pdf) to arbitrary graphs. This is an important topic both for the application of classical deep nets to neuromorphic hardware, but also for our understanding of computations in biological tissues.\n\n1. The work presented in this paper follows similar results by Amit (2019) or Lilicrap, and provides with numerical simulations comforting the theoretical predictions. \n2. Authors present an extension of the previous work to arbitrary graphs, and they apply their claims to LSTM and RNN models.\n3. It provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework. This makes the point of the paper more convincing and complementary to similar works.  \n\n##########################################################################\n\nConcern:\n \nA major issue of this paper is to not identify its originality. The extension to \u00ab arbitrary graphs \u00bb or to CNNs is straightforward in theory, While it is not explicitly stated in  (Whittington & Bogacz, 2017), an unwrapped RNN is by definition a feed-forward graph and is therefore a direct application of their work. \n\nConcerning novelty, the actual derivations (eg for the LSTM) are original and the simulations clearly support that original contributions. However this material is at the end of the paper or in appendices. Recentered on this original contributions and how this makes a suitable contribution to the community would make the paper acceptable to be accepted at ICLR.\n\n##########################################################################\nminor:\np.2 \u00ab\u00a0backwawrds\u00a0\u00bb\np7 \u00ab\u00a0dataest\u00a0\u00bb & p 14", "title": "Clear paper - marginal originality", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}