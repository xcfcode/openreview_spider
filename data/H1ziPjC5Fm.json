{"paper": {"title": "Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks", "authors": ["Jose Oramas", "Kaili Wang", "Tinne Tuytelaars"], "authorids": ["jose.oramas@esat.kuleuven.be", "kaili.wang@esat.kuleuven.be", "tinne.tuytelaars@esat.kuleuven.be"], "summary": "Interpretation by Identifying model-learned features that serve as indicators for the task of interest. Explain model decisions by highlighting the response of these features in test data. Evaluate explanations objectively with a controlled dataset.", "abstract": "Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations. Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation. Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest.", "keywords": ["model explanation", "model interpretation", "explainable ai", "evaluation"]}, "meta": {"decision": "Accept (Poster)", "comment": "This was a difficult decision to converge to. R2 strongly champions this work, R1 is strongly critical, and R3 did not participate in the discussions (or take a stand). On the one hand, the AC can sympathize with R1's concerns -- insights developed on synthetic datasets may fail to generalize and fundamentally, the burden is not on a reviewer to be able to provide to authors a realistic dataset for the paper to experiment on. Having said that, a carefully constructed synthetic dataset is often *exactly* what the community needs as the first step to studying a difficult problem. Moreover, it is better for a proceeding to include works that generate vigorous discussions than the routine bland incremental works that typically dominate. Welcome to ICLR19. "}, "review": {"Byx6M0cC2Q": {"type": "review", "replyto": "H1ziPjC5Fm", "review": "Pros:\n\nThis paper\n - Proposes a method for producing visual explanations for deep neural network outputs,\n - Improves quality of the guided backprop approach for strided layers by converting stride 2 layers to stride 1 and resampling inputs (improving on a longstanding difficulty with such approaches),\n - Shows fairly rigorous experimentation demonstrating the applicability and properties of the proposed approach, and\n - Releases a new synthetic dataset and benchmark for visual explanation methods.\n\nAlthough producing visual explanations is a task fraught with difficulty for many reasons, including that explanations for complex decisions may not necessarily be communicable via one or a small number of saliency maps over the image pixels, this paper strives valiantly in this admittedly difficult direction.\n\nThe experimentation is fairly rigorous, which is a welcome departure from and improvement on the norm for this type of paper. I hope such more quantitative evaluation will become more common in papers evaluating visual explanations.\n\nCons:\n\nWhat about features that are very important but not linearly predictive on their own? This approach (and many others) would not work in that case; recognizing this, extending the an8Flower dataset to include such images and labels may be motivating for the field. For example, flowers where the class is determined not by a specific single color or feature (thorns or spots) but by the combination. In these cases, it\u2019s not clear what the right answer would even be in the form of a saliency map, so the first task for researchers would be to determine in what format the answer should even be provided! So: less a benchmark than a motivating open question.\n\n\nSmaller notes:\n\nI found the presentation of the stride 1 resampling approach a little confusing. When performing the backward pass through the network from, say, layer 20, is the approach followed at every stride 2 layer on the way back? If so, I don\u2019t think I saw this mentioned. If not, wouldn\u2019t artifacts be introduced and compounded at any stride 2 layer during the backward pass?\n\n\n====== Update 12/12/18 ======\n\nThanks for your notes in reply. I'll just add that if the dataset can be extended to slightly greater complexity either for this version or for submission to a subsequent venue, it would be impactful. Simple extensions could include scenes with multiple flowers and classes where the explanatory factor is tricker to uncover. For example, a dataset could be created with scenes of three flowers: two of one color and one of another color, with the class determined by the color of the lone flower. The correct explanation (the color of the lone flower) is still clear, and it would be great to see if the proposed LASSO approach (or a future approach) could correctly identify those pixels.", "title": "A great contribution to the visual explanation literature!", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJgSnw53nQ": {"type": "review", "replyto": "H1ziPjC5Fm", "review": "Summary: the paper proposes a method for Deep Neural Networks (DNN) that identifies automatically relevant features of the set of the classes, enriching the predictions made with the visual features that contributed to that class, supporting, thus, interpretation (understanding what the model has learned) and explanation (justification of the predictions/classifications made by the model). This scheme does not rely on additional annotations, like earlier techniques do.\n\nThe contributions of this paper are relevant to, I would say, a large segment of the AI community, since interpretability and explainability of AI (XAI) is the focus of many current works in the area, and there are still many unresolved issues. I consider this paper suitable for ICLR 2019, in particular, it fits the call for papers topic \u201cvisualization or interpretation of learned representations\u201d.\n\nThe authors also present a new dataset (am8Flower) that can be used by the community for future evaluations of explanation methods for DNN. From my point of view, this is a significant contribution, since there is a lack of datasets that can be used for evaluation.\n\nThe authors motivate properly the need for this research/study, addressing the main weakness of the two more common strategies for interpreting DNN, (1) manually inspecting visualizations of every single filter or (2) comparing the internal activations produced by a given model w.r.t. a dataset with pixel-wise annotations of possibly relevant concepts.\n\nI would encourage the authors to write the limitations and weakness of their proposal w.r.t. similar approaches they reviewed. I am aware that the space is limited, but in p.8, section 4.3, when Table 1 is introduced and the authors confirm that their proposal has higher IoU than other methods, the authors could explain, in brief, what are the weaknesses of their method w.r.t. the other approaches analyzed.\n\nAnother clarification concerns the initialization of input parameters, such as sparsity; e.g., p.6 sparsity is initialized with 10 for all datasets, why? How has this value been selected and how sensitive is the performance regarding variations of this value?\n\nOnce again, I know that the space is limited, but I would like to be able to see some of the figures better (since this is an essential part of the paper). The additional material complements very well the paper and shows larger figures, but I think that the paper itself should be self-sufficient, and figures like Fig. 5 should be enlarged so it is easier to see some details.\n\nJust a concern or something that I quite did not understand about one of the arguments the authors use to justify the evaluation carried out: the authors claim that they want to avoid the subjectivity introduced by humans (citing Gonzalez-Garcia et al. 2017), and prefer to avoid user studies, presenting a more objective approach in their evaluation. Ok, but then, the analysis presented in, for example, page 7, is based mainly in their interpretation of the results, a qualitative analysis of the images (we can see fur patterns, this and that, etc.). So aren\u2019t they interpreting the results obtained as users? So after all, aren\u2019t the visual explanations and feedback intended for users? Why should we claim that we want to avoid the subjectivity introduced by humans in the evaluation when the method proposed here is actually going to be used by users \u2013with their inherent subjectivity? I do not mean that the evaluation carried out is not interesting per se, but it could be motivated differently, or it could be complemented later on with future user studies (that would make an interesting addition to the paper). Moreover, I also wonder whom the authors see as intended users for the proposed scheme.\n\nSmall comments:\nP.1 \u201cuseful insights on the internal representations\u201d \uf0e0 insights into the internal representations.\nP. 2: space needed in \u201cback-propagation methods.Third,\u201d\nP. 3: Remove \u201cs\u201d in verb (plural authors): \u201cSimilarly, Bach et al. (2015) decomposes the classification\u201d \uf0e0 decompose or decomposed\nP.3: n needed \u201cChattopadhyay et al. (2018) exteded\u201d \uf0e0 extended\nP.3: \u201cThis saliency-based protocol assume that\u201d \uf0e0 protocol assumes\nP.3: \u201chighlighted by the the explanations\u201d \uf0e0 remove one \u201cthe\u201d\nP. 5: \u201cspace. As as result we get\u201d \uf0e0 remove one \u201cas\u201d\nP. 5: \u201cand compensate this change\u201d \uf0e0 compensate for this change\nP. 6: \u201cIn this experiment we verify\u201d \uf0e0 In this experiment, we verify\nP. 6: \u201cTo this end, given a set of identified features we\u201d \uf0e0 To this end, given a set of identified features, we\nP. 6: \u201cNote that the OnlyConv method, makes the assumption\u201d \uf0e0 remove \u201c,\u201d after method\nP. 7: \u201cIn order to get a qualitative insight of the type of\u201d \uf0e0 insight into the\nP. 7: I would write siamese and persian cat with capital \u201cS\u201d and \u201cP\u201d (Siamese, Persian)\nP. 7: others/ upper \u201cSome focus on legs, covered and uncovered, while other focus on the upped body part.\u201d \uf0e0 while others focus on the upper body part\nP. 7: \u201cThese visualizations answers the question\u201d \uf0e0 answer\nP. 7:  \u201cIn this section we assess\u201d \uf0e0 In this section, we\nP. 7: Plural \u201cWe show these visualization for different\u201d \uf0e0 these visualizations\nP. 7: In \u201cHere our method reaches a mean difference on prediction confidence\u201d \uf0e0 difference in prediction \u2026\nP. 7: \u201cThis suggest that our method is able\u201d \uf0e0 This suggests that\nP. 8: state-of-the-art\nP. 8: \u201chas higher mean IoU\u201d \uf0e0 has a higher mean IoU\nWhole document: when using \u201ci.e.\u201d add \u201c,\u201d after: i.e.,\n\nReferences: Some of the references in the list have very little information to be able to find it/proper academic citation, e.g. , Yosinski et al. 2015; Vedaldi and Lenc, 2015:\n\nJason Yosinski, Jeff Clune, Anh Mai Nguyen, Thomas J. Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. 2015.\n\nA. Vedaldi and K. Lenc. Matconvnet: Convolutional neural networks for matlab. In MM, 2015.\n\nRef Doersch et al.: What makes paris look like paris? \uf0e0 Paris\n", "title": "A good paper with interesting contributions that requires improved motivations and some clarifications", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Bkg5s3Op2m": {"type": "review", "replyto": "H1ziPjC5Fm", "review": "In this paper, the authors proposed a novel scheme to interpret deep neural networks\u2019 prediction by identifying the most important neurons/activations for each category using a Lasso algorithm.\n\nFirstly, the authors produce a 1-dimensional descriptor for each filter in each convolutional layer for each image. Then these descriptors are concatenated as a new feature vector for this image. A feature selection algorithm (u-Lasso) is then trained to minimize the classification loss between the prediction from the new feature vector and the original prediction from DNN (formula (1)). Finally, the importance of each filter is identified by the weights of the lasso for each category.\n\nThe authors also improved the visual feedback quality over the deconvolution+guided back-propagation methods, and release a new synthetic dataset for benchmarking model explanation.\n\nThe paper is well-written, however, I have several concerns about this paper:\n\n1.      How to verify the importance of the identified relevant features is a problem. In the experiments, the authors removed features in the network by setting their corresponding layer/filter to zero. The authors only compared their method with randomly removing features. And in Fig 4, the differences seem small for ImageNet. The results are not convincing enough to me. It is a bit baffling randomly removing features did almost as well as the proposed approach.\n\n2.      I don't think one should get away with only showing some results from the synthetic dataset without showing any quantitative results on any real datasets. I like the idea of having a synthetic dataset where all the parameters are controllable. However in this case it is very simple and maybe lacking enough distracting features that can really test the capability of the algorithm. I would believe quantitative results on a realistic dataset are still necessary for the pubilcation of this paper.\n\n3.      Recently several papers pointed out some significant issues in Guided BP, \n\nXie et al. A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations. ICML 2018\nAdebayo et al. Sanity Checks for Saliency Maps. NIPS 2018\nKindermans et al. The (Un)reliability of saliency methods. NIPS workshop 2017\n\ncan the authors comment on that? Based on those papers I don't seem to think Guided BP is actually doing anything that is relevant to the classification, but is just finding prominent gradients. This, unfortunately would lead to reasonably good behavior on the synthetic dataset created by the authors. ", "title": "Some issues", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJxqomaeCX": {"type": "rebuttal", "replyto": "HJgSnw53nQ", "comment": "\nWe consider as potential users of our method,\n \n- For dataset debugging\nOur method can assist other researchers on verifying whether a top-performing model has indeed learned a general representation from the data or this top performance is caused by any bias on the data itself, e.g. dataset bias.\n\n- For accountability\nIt can serve students, researchers and other individuals working with deep models identify relatively not apparent causes for the high performance of a given model. For instance, our method has provided a set of good insights of why the models trained in Wang et al., WACV'18 have such a good performance when compared with human subjects.\n\n- To enforce fairness\nVery related to the previous case, individuals tasked with assessing the \"fairness\" of models making decisions about other individuals (e.g Gender Shades, Buolamwini et al., PMLR'18). Our method can help to verify whether these models have any bias related to genre, ethnicity, etc.\n\n- Alternatively, our visual explanations can serve to indicate regions of interest that can serve as input to other automatic systems/methods aiming at distilling information from the model being explained with the goal of producing lighter models.\n\n\nWe thank the reviewer for the time invested in providing the detailed feedback at the end of the review (small comments and references). Likewise, we will invest time to meticulously integrate this feedback on a revised version of our manuscript.\n\nWe have revised the manuscript in order to integrate the provided feedback. \n(New content is coloured in green)", "title": "(2|2) : RE: A good paper with interesting contributions that requires improved motivations and some clarifications "}, "rJeWC7TgAX": {"type": "rebuttal", "replyto": "HJgSnw53nQ", "comment": "Thanks for the feedback.\n\nWe appreciate the reviewer recognizes the relevance that our work can have in the field in general.\nWe agree with the reviewer on the significance of the contribution given by the proposed an8Flower dataset.\nAlthough very recently few works (Adebayo et al.,NIPS'18, Nie et al., ICML'18) have proposed means to assess the sanity/reliability of visual explanation methods, no method has been proposed to objectively evaluate the generated explanations themselves. \nMoreover, the proposed an8Flower dataset can be further extended to evaluate different settings of interest, e.g. occurrence of distracting objects, object classes driven by contextual information, fine-grained differences classes, etc., and can be used as a sanity check itself to verify whether a proposed explanation method can accurately explain a specific setting of interest.\n\nRegarding the suggestion of providing a discussion covering the limitations/weaknesses of the proposed method w.r.t. similar compared methods, e.g. those from Table 1.\nIndeed, there are space limitations in place, as was pointed out by the reviewer. \nHowever, this is a good suggestion and we believe that adding such discussion would provide further insights on the proposed method, and strengthen the manuscript at the same time.\nHere is a summary of limitations that our method has w.r.t. similar compared methods:\n- Our method requires an additional process, i.e. feature selection via u-lasso, at training time (Sec. 3.1).\n- There is the need to define an additional parameter, i.e \\mu, for the feature selection process (Sec. 3.1).\n\nRegarding the sparsity parameter (\\mu) used for the feature selection process (u-lasso): increasing the sparsity value \\mu in the u-lasso formulation will increase the number of selected features.\nThis will allow to the selected filters to focus on more specific/specialized features that can help to handle better outlier/rare instances of the classes of interest. Please see, Sec.8 and Fig.9 from the supplementary material for an extra analysis on the effect that the \\mu value has on the capability of the selected features to serve as indicators of the classes of interest.\nWe decided to start from a relatively low value, i.e. \\mu=10, in order to focus on a small set of relevant features that can generalize to the classes of interest while, at the same time, keeping the u-lasso optimization with a low computational cost.\n\nRegarding the size of the figures (Fig.5 especially), we totally agree with the reviewer that figures like Fig. 5 should be enlarged so it is easier to see some details. Despite the space limitations, we are aware that the 8-page length for the manuscript (content only) is not strict, and that authors are allowed to go up to 10 pages. Having said this, if reviewers and ACs agree on the need for larger figures, we would like to cross the 8-page length and include larger versions of some of the figures that are currently too small to visualize details.\n\nRegarding the question of whether human inspection (or user studies) are necessary for model interpretation/explanation.\nWe agree with the observation made by the reviewer regarding the fact that our method still requires some level of human intervention. Furthermore, we agree that since the proposed method is meant to be used by users, an indication of how \"understandable\" an explanation is for end users is required. Having said this, the main goal of our method is to reduce the load on the user side which can introduce bias and noise. By reducing (and separating) the number of visualizations (i.e. number of features in the explanation visualizations and the relevant set of features learned by the model [interpretation]) to be inspected, we aim at reducing exhaustive inspections that are used in previous works to achieve model interpretation/explanation.\nWe admit that in our manuscript, the need for human inspection is understated. Moreover, we agree that our objective evaluation should be complemented with relatively simpler user studies in order to ensure that the produced explanations are meaningful to the individuals they aim to serve. We will update the motivation behind our method in order to emphasize further the need of reduced used inspection and the complementary between our evaluation and user studies.\n", "title": "(1|2) : A good paper with interesting contributions that requires improved motivations and some clarifications "}, "SJx10L6lRQ": {"type": "rebuttal", "replyto": "Bkg5s3Op2m", "comment": "\nRegarding (3), thanks for the pointers towards those works. Indeed there are some interesting insights there that we can address from the perspective of our method.\n\nAs pointed by the reviewer, our method identified important features for each of the classes modeled by the network using a u-Lasso optimization. Then, at test time, we explain the class predicted by the model by, first, looking for the response (on the test image) of the subset of features identified as relevant for such class, and then, generating heatmaps highlighting the top responding features via our variant of DeconvNet with Guided Backpropagation. Each of these feature visualizations is generated by using the DeconvNet with Guided Backpropagation method to highlight the image regions that produce the activations observed for the relevant features.\nAs such our method is composed by two main components: a) the feature selection component, and b) the visualization component. At test time, these two components are linked by the class predicted by the model.\n\nKindermans et al., NIPS'17 (ws), propose a shift-test in which the explanations produced by a image-model pair should match that of its shifted counterpart.\nIn our case, relevant features are identified by applying the u-Lasso optimization on the internal activations. If these activations remain constant (as enforced by the shift-test) no difference in the selected features (filter/layers) is to be expected.\n\nNie et al.,ICML'18 and Adebayo et al., NIPS'18 suggest that explanations from DeconvNet and Guided-Backpropagation methods are not determined by the predicted class, but by the filters of the first layer and the edge-like structures in the input images.\nRegarding the question of whether this is the reason why our method has good performance on the proposed dataset, it can be noted that the explanations generated by our method in the proposed dataset go beyond regions with prominent gradients (edge-like regions). In fact, in classes where color is a discriminative feature uniform regions are highlighted. \nMoreover, in our method, we use DeconvNet with Guided-Backpropagation as means to highlight the image regions that justify the identified relevant features, not the predicted classes themselves.\n\nRegardless of the observations made in the referred works, the adopted DeconvNet+GBP method is just the means we use for visualization. This visualization method does not influence the way in which relevant features are selected but in the way they are visualized. So if a better, more robust/principled, visualization method is proposed in the literature it can be integrated into an upgraded version of our method. We don't see the current visualization mechanism as a major weakness but as a point that can be improved as new insights related to visualization of internal features of DNNs are obtained by the community.\n\nWe thank the reviewer for motivating the discussion in the direction of these aspects. We believe the discussion above helps to get an insight on strengths and potential points for improvement of the proposed method. If the other reviewers and ACs agree, we would like to add the discussion above in a revised version of the manuscript. Additionally, depending on the time (and space) constrains, we will try to add some of the tests presented in the referred papers from above in a revised version of our submission.\n\nWe have revised the manuscript in order to integrate the provided feedback. \n(New content is coloured in green)", "title": "(2|2) : RE: Some issues "}, "Sye4lwalA7": {"type": "rebuttal", "replyto": "Bkg5s3Op2m", "comment": "Thanks for the feedback.\n\nRegarding (1), the ablation of features labeled as \"random\" refers to settings where features were removed by setting to zero the response of randomly selected filters from layers that were indicated to host important features by the u-lasso optimization. \nAs such, these features (filter responses) are not 100% random per se. To verify this aspect, we have conducted an experiment on the full imageNet dataset where we ablated completely randomly selected features  (i.e  both layers and filter locations). We computed the mean performance after 5 runs and obtained a classification accuracy of 0.33, which is  10% higher than that when the selected relevant features are ablated (0.23).\nIn addition, different from the other datasets tested with a VGG-based method, the setting of the full imageNet dataset has the highest ratio between classes of interest and features. At this higher ratio, features internally modeled by the network are more likely to be shared between the classes. As such, ablating one feature may have a side effect on another class as well.\n\n\nRegarding (2), we respectfully disagree. Our synthetic dataset may look simple and artificial, but that's on purpose to make it clear beyond discussion what elements are crucial to explain a decision. To the best of our knowledge there isn't any realistic dataset with such annotation and in fact, we have no idea how one would go about to create one. Nor is there any other unbiased quantitative evaluation setup using realistic data, as far as we know. For instance, using semantic labels as done in (Zhang et al, CVPR'18 / arXiv:1710.00935 ignores the validity of any context cues that fall outside of the object boundaries. In our synthetic dataset, the regions to be highlighted are controlled by design, therefore providing an objective means of evaluation. If however the reviewer can point us towards a realistic dataset which such level of annotation, we would be happy to try it out, to further strengthen our manuscript.\n\nRegarding the presence of distracting features mentioned in (2), we are conducting experiments on the classification task of Pascal VOC'07. In this dataset there are several distracting instances/objects per image. Initial results show that despite the presence of these distracting objects/elements, our method is able to highlight image regions related to the prediction made by the model. If requested by the reviewers, we will revise the manuscript in order to include some of these new results.\nIn addition, adding distracting objects/features could be an interesting way to extend our current synthetic dataset. We will work towards having an additional variant of our dataset that includes distracting elements for the moment of its official release.", "title": "(1|2) : RE : Some issues "}, "B1eE1fpxCm": {"type": "rebuttal", "replyto": "Byx6M0cC2Q", "comment": "\nWe thank you for the motivating feedback. As you mentioned visual explanation is a task fraught with difficulty for many reasons, and indeed here we try to push efforts addressing this task, with important neuron selection, better visualization and especially new complementary objective evaluation protocols.\n\nRegarding the comment on the features that are very important but not linearly predictive: we tried to cover that scenario to some extent with the an8flower-double-12c variant of our dataset (please see Fig. 10 from the supplementary material). There the classes are not just defined by the color, but by the part/location where those colors are applied. Yet, this is just one scenario; there are many others that might be interesting to investigate. In its current form, an8Flower is just an initial step towards more objective evaluation. Taking into account the feedback from reviewers and from the community, we hope to turn it into a fully developed benchmark for visual explanations.\n\nRegarding the comment \"...so the first task for researchers would be to determine in what format the answer should even be provided!\". Indeed that is a very good point and an interesting research question. We hope to be able to tackle such questions in future work.\n\nRegarding the smaller note on the stride 2 resampling approach: yes, you are right, this adjustment is applied to every layer during the backward pass. Otherwise, as accurately noted, artifacts produced at top layers would propagate towards the lower ones. \nWe thank the reviewer for pointing this out. We will revise the manuscript to make sure this aspect is clear.\n\nWe have revised the manuscript in order to integrate the provided feedback. \n(New content is coloured in green)", "title": "RE: A great contribution to the visual explanation literature!"}}}