{"paper": {"title": "Learning to Remember Rare Events", "authors": ["Lukasz Kaiser", "Ofir Nachum", "Aurko Roy", "Samy Bengio"], "authorids": ["lukaszkaiser@google.com", "ofirnachum@google.com", "aurko@gatech.edu", "bengio@google.com"], "summary": "We introduce a memory module for life-long learning that adds one-shot learning capability to any supervised neural network.", "abstract": "Despite recent advances, memory-augmented deep neural networks are still limited\nwhen it comes to life-long and one-shot learning, especially in remembering rare events.\nWe present a large-scale life-long memory module for use in deep learning.\nThe module exploits fast nearest-neighbor algorithms for efficiency and\nthus scales to large memory sizes.\nExcept for the nearest-neighbor query, the module is fully differentiable\nand trained end-to-end with no extra supervision.  It operates in\na life-long manner, i.e., without the need to reset it during training.\n\nOur memory module can be easily added to any part of a supervised neural network.\nTo show its versatility we add it to a number of networks, from simple\nconvolutional ones tested on image classification to deep sequence-to-sequence\nand recurrent-convolutional models.\nIn all cases, the enhanced network gains the ability to remember\nand do life-long one-shot learning.\nOur module remembers training examples shown many thousands\nof steps in the past and it can successfully generalize from them.\nWe set new state-of-the-art for one-shot learning on the Omniglot dataset\nand demonstrate, for the first time, life-long one-shot learning in\nrecurrent neural networks on a large-scale machine translation task.", "keywords": ["Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The primary contribution of this paper is showing that k-nearest-neighbor method based memory can be usefully incorporated in a variety of architectures and supervised learning tasks. The presentation is clear, and results are good. I like the synthetic task and analysis. For the Omniglot task, running and reporting results on the original splits used by Lake would be good, as the splits used in matching nets are considerably easier and result in ceiling effects."}, "review": {"r1I5Em2Ix": {"type": "rebuttal", "replyto": "r1SMgxjUx", "comment": "When writing to a new location in memory we consider the set of locations i where\n\nA[i] > argmax_i A[i] - C\n\nIn other words, we consider the oldest elements in memory, upto a constant C. We then pick one of those locations at random.\n\nThe reason to use this randomness instead of just picking one of the oldest elements is that we're training in a fully asynchronous mode. When N replicas are training concurrently, it would be possible that they'll all try to write to the same location in memory, thus over-writing each other. In practice, we picked C = 3 * N (i.e., C = 24 for 8-replica training) and used this setting for our experiments.", "title": "Clarification of randomness use in asynchronous training."}, "r1SMgxjUx": {"type": "rebuttal", "replyto": "SJTQLdqlg", "comment": "what does \"introduce some randomness in the choice so as to avoid race conditions in asynchronous multi-replica training\" mean specifically?\nIs there any reference paper to let the readers to get better understanding of it? Thank you very much. ", "title": "Question about update memory during training"}, "ryEMSeqVg": {"type": "rebuttal", "replyto": "ryD867U4g", "comment": "Dear Reviewer,\n\nWe are grateful for your review. We will release the source code for the memory module class in TensorFlow that will make it easy to add it to any model, and the code to replicate our results, at least on Omniglot (the GNMT model code is not open-source, so we cannot promise to release code for these experiments, but the module can be added to other TF seq2seq models easily and we might give example code for that).\n\nWe will also add [R1] to our related work section.\n\nAre there any other concerns or questions that we could address to improve the rating?", "title": "Reply to Review by AnonReviewer2"}, "BkB0QN2Qx": {"type": "rebuttal", "replyto": "SJTQLdqlg", "comment": "We are very grateful for the reviewers' questions, and we uploaded a new revision that clarifies the paper to answer them.\n\nIn addition to the answers, we updated the definition of the memory module for the cases when no positive neighbour is found in the top-k. We now get any vector from memory instead of using all 0s, which prevents the loss from jumping in such cases. This improved our results on Omniglot, they are state-of-the-art now.", "title": "New Revision"}, "BJBmm4hQx": {"type": "rebuttal", "replyto": "H1ThNbJXl", "comment": "In our experiments, the values in the memory module V[n] are indeed integer scalars, and during training they are updated using the supervised value v which is also an integer scalar -- the label to be produced by the network at this time-step.\nThis holds both for RNNs and for the Convolutional Network, exactly as you wrote.\n\nIn GNMT with memory, the output y_{m+1} is created as in the pure GNMT, by a softmax layer, but the input to the softmax is created by an additional linear layer that gets two concatenated tensors as input: the output of the last LSTM layer (as in pure GNMT) and the embedded output of the memory module. To embed the scalar, we use the same embedding as the decoder.\n\nFor translation experiments with context: the memory is not updated during evaluation, only during training. But when context is given, we want to update the memory but not the weights. This is equivalent to a training step with learning-rate of 0.", "title": "Reply to Questions from AnonReviewer3"}, "ByIYZNn7x": {"type": "rebuttal", "replyto": "SykeHVkmg", "comment": "The test set is composed of a few paragraph with sentences coming in order, e.g., say the first 6 sentences are from the same newspaper article in the order they appear in the article.\n\nSo when we give odd sentences as context and test the model on even sentences, it will get sentences 1, 3, 5 as context and will be tested on 2, 4, 6. Since they come from the same paragraph, the memory updates from 1, 3, 5 actually help with translating 2, 4, 6.\n\nWe re-wrote this paragraph of the experimental section in the new revision and hope that it is clearer now.", "title": "Reply to Questions from AnonReviewer1"}, "rJAre43Xl": {"type": "rebuttal", "replyto": "HyTlsSJXe", "comment": "Great thanks for your questions, we included the answers in the text of the paper in the newly uploaded revision.\n\n1. What is the memory size used in all the three tasks?\n\nWe used memory of half a million.\n\n2. For the synthetic task, what is the size of the training data?\n\nIt is 40K, so each from the 16K random examples is there 2 or 3 times.\n\n3. In the synthetic task, the big performance gap between baseline extended neural GPU with and without memory module is surprising. It would be interesting if authors can also report results on seq2seq+attention with and without memory module.\n\nWe trained a 2-layer 256-unit LSTM seq2seq with attention on this task. It overfits the data, memorizing the traning set but with < 1% accuracy on the test set. The same model with a memory module does much better, accuracy is > 30%. We will train a few more models to see how lower sizes affect the results. But from our experience with the Extended Neural GPU models on this task, we'd expect the small models to under-fit and not get good results. We'll report a more thorough model-size-vs-accuracy in the final version. In all cases, the models with memory module get better results.\n\n4. Last few lines in Translation section is not clear. What do you mean by providing the whole test context? Model sees all the test sentences and translations and then asked to translate the same?\n\nThis part was re-written to clarify. Yes - the model sees all test sentences, but cannot update its weights, only memory, and is then asked to translate the same. We use it only to assess the headroom that the memory can bring.\n\n5. I do not agree with the comment in the related work section that hard access queries have limited success. See for example [1] \n\nWe were not aware of [1] before, the related work section was changed accordingly.", "title": "Reply to Questions from AnonReviewer2"}, "HyTlsSJXe": {"type": "review", "replyto": "SJTQLdqlg", "review": "1. What is the memory size used in all the three tasks?\n2. For the synthetic task, what is the size of the training data?\n3. In the synthetic task, the big performance gap between baseline extended neural GPU with and without memory module is surprising. It would be interesting if authors can also report results on seq2seq+attention with and without memory module.\n4. Last few lines in Translation section is not clear. What do you mean by providing the whole test context? Model sees all the test sentences and translations and then asked to translate the same?\n5. I do not agree with the comment in the related work section that hard access queries have limited success. See for example [1] \n\n[1] Gulcehre, Caglar, Chandar, Sarath, Cho, Kyunghyun, and Bengio, Yoshua. Dynamic neural turing\nmachines with soft and hard addressing schemes. CoRR, abs/1607.00036, 2016. URL http:\n//arxiv.org/abs/1607.00036.This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.\n\nUsing k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.\n\nAuthors have addressed all my pre-review questions and I am ok with their response.\n\nAre the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?\n\nReferences:\n\n[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)\n", "title": "Few questions", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryD867U4g": {"type": "review", "replyto": "SJTQLdqlg", "review": "1. What is the memory size used in all the three tasks?\n2. For the synthetic task, what is the size of the training data?\n3. In the synthetic task, the big performance gap between baseline extended neural GPU with and without memory module is surprising. It would be interesting if authors can also report results on seq2seq+attention with and without memory module.\n4. Last few lines in Translation section is not clear. What do you mean by providing the whole test context? Model sees all the test sentences and translations and then asked to translate the same?\n5. I do not agree with the comment in the related work section that hard access queries have limited success. See for example [1] \n\n[1] Gulcehre, Caglar, Chandar, Sarath, Cho, Kyunghyun, and Bengio, Yoshua. Dynamic neural turing\nmachines with soft and hard addressing schemes. CoRR, abs/1607.00036, 2016. URL http:\n//arxiv.org/abs/1607.00036.This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.\n\nUsing k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.\n\nAuthors have addressed all my pre-review questions and I am ok with their response.\n\nAre the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?\n\nReferences:\n\n[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)\n", "title": "Few questions", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SykeHVkmg": {"type": "review", "replyto": "SJTQLdqlg", "review": "Thank you for a very interesting paper.\n\nI am not sure if I understand the translation experiment correctly.\nCan you briefly elaborate on the purpose of even and odd splitting and why it improves performance?A new memory module based on k-NN is presented.\nThe paper is very well written and the results are convincing. \n\nOmniglot is a good sanity test and the performance is surprisingly good.\nThe artificial task shows us that the authors claims hold and highlight the need for better benchmarks in this domain.\nAnd the translation task eventually makes a very strong point on practical usefulness of the proposed model.\n\nI am not a specialist in memory networks so I trust the authors to double-check if all relevant references have been included (another reviewer mentioned associative LSTM). But besides that I think this is a very nice and useful paper. I hope the authors will publish their code.", "title": "Context Set", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJHEwdBEl": {"type": "review", "replyto": "SJTQLdqlg", "review": "Thank you for a very interesting paper.\n\nI am not sure if I understand the translation experiment correctly.\nCan you briefly elaborate on the purpose of even and odd splitting and why it improves performance?A new memory module based on k-NN is presented.\nThe paper is very well written and the results are convincing. \n\nOmniglot is a good sanity test and the performance is surprisingly good.\nThe artificial task shows us that the authors claims hold and highlight the need for better benchmarks in this domain.\nAnd the translation task eventually makes a very strong point on practical usefulness of the proposed model.\n\nI am not a specialist in memory networks so I trust the authors to double-check if all relevant references have been included (another reviewer mentioned associative LSTM). But besides that I think this is a very nice and useful paper. I hope the authors will publish their code.", "title": "Context Set", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1ThNbJXl": {"type": "review", "replyto": "SJTQLdqlg", "review": "As I understand it, the value $v$ is the supervised desired target value which you use during training.\nThen every memory operation will store this supervised value $v$?\nSo memory updates are only done during training? Or what value $v$ do you use during evaluation / inference?\n\nThe output of the memory module $V[n]$ is always only a scalar, or even an scalar integer?\nAs well as $v$?\n\nIn Convolutional Network with Memory, the output is the nearest neighbor from the memory module, i.e. the output of the network is also just a single scalar?\n\nIn GNMT with added memory module, how exactly is $\\hat{y}_{m+1}$ defined?\nFor translation if you don't know the target sequence, how would you update and use the memory module?\nThe paper proposes a new memory module to be used as an addition to existing neural network models.\n\nPros:\n* Clearly written and original idea.\n* Useful memory module, shows nice improvements.\n* Tested on some big tasks.\n\nCons:\n* No comparisons to other memory modules such as associative LSTMs etc.\n", "title": "memory module update", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rk1PUS-Vx": {"type": "review", "replyto": "SJTQLdqlg", "review": "As I understand it, the value $v$ is the supervised desired target value which you use during training.\nThen every memory operation will store this supervised value $v$?\nSo memory updates are only done during training? Or what value $v$ do you use during evaluation / inference?\n\nThe output of the memory module $V[n]$ is always only a scalar, or even an scalar integer?\nAs well as $v$?\n\nIn Convolutional Network with Memory, the output is the nearest neighbor from the memory module, i.e. the output of the network is also just a single scalar?\n\nIn GNMT with added memory module, how exactly is $\\hat{y}_{m+1}$ defined?\nFor translation if you don't know the target sequence, how would you update and use the memory module?\nThe paper proposes a new memory module to be used as an addition to existing neural network models.\n\nPros:\n* Clearly written and original idea.\n* Useful memory module, shows nice improvements.\n* Tested on some big tasks.\n\nCons:\n* No comparisons to other memory modules such as associative LSTMs etc.\n", "title": "memory module update", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}