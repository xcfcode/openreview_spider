{"paper": {"title": "Locally Linear Unsupervised Feature Selection", "authors": ["Guillaume DOQUET", "Mich\u00e8le SEBAG"], "authorids": ["doquet@lri.fr", "sebag@lri.fr"], "summary": "Unsupervised feature selection through capturing the local linear structure of the data", "abstract": "The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data. The proposed approach, called Locally Linear Unsupervised Feature Selection, relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000). The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art.", "keywords": ["Unsupervised Learning", "Feature Selection", "Dimension Reduction"]}, "meta": {"decision": "Reject", "comment": "This  paper presents an LLE-based unsupervised feature selection approach. While one of the reviewers has acknowledged that the paper is well-written with clear mathematical explanations of the key ideas, it also lacks a sufficiently strong theoretical foundation as the authors have acknowledged in their responses; as well as novelty in its tight connection to LLE. When theoretical backbone is weak, the role of empirical results is paramount, but the paper is not convincing in that regard."}, "review": {"HygWE6dxAm": {"type": "rebuttal", "replyto": "BJgZhgKDhm", "comment": "Thank you for your review.\n\n\nQ1: \"First, the result of the dimensionality reduction drastically depend on the method used.\nIt is well known that every DR method focuses on preserving certain properties of the data.\nFor instance, PCA preserves the global structure while t-SNE works locally, maximizing the recall [1].\nThe choice of the DR method should justify the underlying assumption of the approach.\nI expect that the results of the experiments to change drastically by changing the DR method.\"\n\nA1: You are right, the result depends on the DR method. However:\ni) a linear DR does not work (as shown on the toy XOR example)\nii) non-linear DR methods (Isomap, t-SNE, MDS, LLE) rely on the local Euclidean distance in the original space, that might be arbitrarily corrupted by random features.\niii) The non-linear DR method in LLUFS (denoising auto-encoder with D-D/2-D/4-D/8-D/4-D/2-D neural architecture) yields stable results (e.g. 98% same features are in the top 100 selected features for all datasets w.r.t. different initialisations).\n\nQ2: \"The LLE method is based on the assumption that if the high-dimensional data is locally linear,\nit can be projected on a low-dimensional embedding which is also locally linear.\nTransitioning from a locally linear high-dimensional data to a lower dimension makes sense because there exists higher degree of freedom\nin the higher dimension. However, making this assumption in the opposite direction is not very intuitive.\nWhy would the features that do not conform to the local linearity of the low-dimensional structure (which itself is obtained via a non-linear mapping) are insignificant?\"\n\nA2: The approach assumes that\n* X (the initial data in D dimensions) can be mapped onto Z (latent space in D/8 dimensions) with no or little loss of information;\n* From Z, the idea is to find among X_sub (all datasets defined from X by selecting a subset of features) the best mapping in the LLE sense. From Z (dimension D/8) to X_sub, the decrease in dimensionality is still high (the evaluation considers the selected top-100 features, with 100 << D/8 except for Madelon). \n\n\nQ3: \"Finally, there are no theoretical guarantees on the performance of the method. Is there any guarantee that, e.g. given one noisy feature in high dimension, the method will find that feature, etc.?\"\n \nA3: You are right, there is no theoretical guarantees for LLUFS. To our best knowledge, the evaluation of all unsupervised FS methods (including ours) is based on a supervised setting.  \n\nQ4 : \"Minor: what is the complexity of the method compared to the competing methods? What is the runtime? Is this a practical approach on large datasets?\"\n\nA4 : Theoretically: Besides the complexity of learning the Auto-Encoder, the time complexity of the prior agglomerative hierarchical clustering is O(D**2) with D the number of features (up to logarithmic terms). This complexity motivates the extension proposed in the paper, to use the feature correlation within the Auto-Encoder loss to deal with redundancy (section 3.3).\nThe time complexity of the nearest neighbor search is O(D/8 n**2) with D/8 the dimension of Z and n the number of points. \nThe time complexity of computing the W matrix is  O(D/8 n k**3) with k the number of neighbors set to 6 in all problems. \nEmpirically, LLUFS is slower than LAP, SPEC, MCFS and faster than NDFS. On dataset lung (203 points, 3312 features), the respective runtimes (on a single 2.67 Ghz CPU core) are :\n* 0.2 seconds for LAP.\n* 1.6 seconds for SPEC.\n* 24.5 seconds for MCFS.\n* 114.4 seconds for LLUFS (*)\n* 131.0 seconds for NDFS.\n(*) 24.4 seconds for the agglomerative clustering; 77.7 seconds for training the AutoEncoder; 12.3 seconds for the distorsion step.\n\nOn dataset pixraw10P (100 points, 10 000 features) :\n*0.3 seconds for LAP.\n*1.8 seconds for SP\u00cbC.\n*258 seconds for MCFS.\n*930 for LLUFS (*) \n*1646 seconds for NDFS.\n(*) 614.6 seconds for the agglomerative clustering + 300 seconds for training the AutoEncoder + 15.9 seconds for the distorsion step.", "title": "Response to Reviewer1"}, "B1giAnOxCQ": {"type": "rebuttal", "replyto": "rJxCdB-dnm", "comment": "Thank you for your review.\n\nQ1 : \"This work basically assumes that the dataset is (well) clustered. This might be true for most real world datasets, \nbut I believe the degree of clustered-ness may vary by dataset. It will be nice to discuss effect of this. \nFor example, if most data points are concentrated on a particular area not being well clustered, \nhow much this approach get affected? If possible, it will be great to formulate it mathematically, but qualitative discussion is still useful.\"\n\nA1 : Given the absence of label information, unsupervised FS algorithms rely on the assumption that there is some sort of \"intrisic structure\" to the data. \nUnsupervised approaches [1,2,3,4,5] assume that there are some clusters, which can be well-separated by an appropriate feature subset. \nAs these clusters are defined in the initial feature space, they depend on the Euclidean distance which is arbitrarily corrupted from irrelevant features \n(except for [5] , which iteratively learns a new distance during selection).\n\nLLUFS proposes another strategy: \n* An auto-encoder achieves the non-linear dimensionality reduction and constructs features, defining a compressed version Z of the initial data X;\n* We now search for the subset of initial features, defining X_sub such that, if we applied LLE dimensionality reduction on Z, X_sub would be a perfect candidate (in the sense of preserving the local structure defined from W, with Z ~= WZ). \n* The gain is that the combinatorial optimization problem of finding the best subset of features of size d can be solved in a straightforward way as the score of each feature is its distorsion: ranking the initial features by increasing distorsion, the optimal set of features is the top d features.\n\n\nQ2 : \"For the dimension reduction, the authors used autoencoder neural network only. What about other techniques like PCA or SVD?\"\n\nA2 : Non separable clusters (the XOR problem) cannot be captured from a linear dimensionality reduction (PCA, SVD) method.\nIt is true that we could have used other non-linear dimensionality reduction methods (Isomap or MDS) to define a latent representation, instead of Auto-Encoder. However, Isomap and MDS depend on the Euclidean distance in the initial feature space, thus with same weakness as said in A1. \n\n\n\"It seems x_j is missing in Johnson-Lindenstrauss Lemma formula.\"\n\nYou are right. Thank you. We fixed the typo. \n\n[1] Cai et al. (2010) \"Unsupervised Feature Selection for Multi-Cluster Data\",\n[2] Li et al. (2012) \"Unsupervised feature selection using non-negative spectral analysis\"\n[3] Li et al. (2014) \"Clustering-guided sparse structural learning for unsupervised feature selection\",\n[4] Shi et al. (2014) \"Robust spectral learning for unsupervised feature selection\"\n[5] Nie et al. (2016) \"Unsupervised Feature Selection with Structured Graph Optimization\"", "title": "Response to Reviewer2"}, "SkerO3_x07": {"type": "rebuttal", "replyto": "Hkxcj4-lTQ", "comment": "Thank you for your review and for the reference.\n\nQ1: \"This paper seems to directly use one existing dimensionality reduction method, i.e., LLE, to explore the local structure of data\"\n\nA1: Actually, LLUFS uses two dimensionality reduction approaches in complementary ways, along a 2-step process:\n* An auto-encoder achieves the non-linear dimensionality reduction and constructs features, defining a compressed version Z of the initial data X;\n* We now search for the subset of initial features, defining X_sub such that, if we applied LLE dimensionality reduction on Z, X_sub would be a perfect candidate (in the sense of preserving the local structure defined from W, with Z ~= WZ). \n* The gain is that the combinatorial optimization problem of finding the best subset of features of size d can be solved in a straightforward way as the score of each feature is its distorsion: ranking the initial features by increasing distorsion, the optimal set of features is the top d features.\n\n\nQ2:  \"Why uses LLE rather than other methods such as LE? What are the advantages?\"\n\nA2: Linear embedding can hardly be used in the first step if we want to capture non-separable patterns (e.g. XOR) in the initial representation. \nAs for the second step, prior work such as [1,2,3] indicate that in order to be efficient, feature scoring must reflect data structure on a local scale. \nThis observation motivates using the proposed distorsion score over global-scale methods such as PCA.\n\nQ3: \"Authors state that the method might be biased due to the redundancy of the initial features. \nTo my knowledge, there are some unsupervised feature selection to explore the redundancy of the initial features, such as the extended work of Li et al. (2012) \"Unsupervised Feature Selection via Nonnegative Spectral Analysis and Redundancy Control\".\"\n\nA3: The authors of (Li et al., 2015) improve on NDFS [4] through an additional term on the feature importance matrix, penalizing the selection of correlated features. \nAt the moment, feature redundancy is taken into account in LLUFS prior to launching the Auto-Encoder: using the feature correlation within the Auto-Encoder loss is a perspective for further work (section 3.3). \n\n\nQ4: \"How about the computational complexity of the proposed method?\"\n\nA4: Theoretically: Besides the complexity of learning the Auto-Encoder, the time complexity of the prior agglomerative hierarchical clustering is O(D**2) with D the number of features (up to logarithmic terms). This complexity motivates the proposed extension (Q3). \nThe time complexity of the nearest neighbor search is O(D/8 n**2) with D/8 the dimension of Z and n the number of points. \nThe time complexity of computing the W matrix is  O(D/8 n k**3) with k the number of neighbors set to 6 in all problems. \nEmpirically, LLUFS is slower than LAP, SPEC, MCFS and faster than NDFS. On dataset lung (203 points, 3312 features), the respective runtimes (on a single 2.67 Ghz CPU core) are :\n* 0.2 seconds for LAP.\n* 1.6 seconds for SPEC.\n* 24.5 seconds for MCFS.\n* 114.4 seconds for LLUFS (*)\n* 131.0 seconds for NDFS.\n\n(*) 24.4 seconds for the agglomerative clustering; 77.7 seconds for training the AutoEncoder; 12.3 seconds for the distorsion step.\n\nOn dataset pixraw10P (100 points, 10 000 features) :\n*0.3 seconds for LAP.\n*1.8 seconds for SP\u00cbC.\n*258 seconds for MCFS.\n*930 for LLUFS (*) \n*1646 seconds for NDFS.\n(*) 614.6 seconds for the agglomerative clustering + 300 seconds for training the AutoEncoder + 15.9 seconds for the distorsion step.\n\nQ5 \"Finally, the equation above Eq. 8 may be wrong.\"\nYou are right. Thank you. We fixed the typo. \n\n[1] Cai et al. (2010) \"Unsupervised Feature Selection for Multi-Cluster Data\"\n[2] Qian and Zhai (2013) \"Robust Unsupervised Feature Selection\"\n[3] Liu et al. (2014) \"Global and local structure preservation for feature selection\"\n[4] Li et al. (2012) \"Unsupervised feature selection using non-negative spectral analysis\"", "title": "Response to Reviewer3"}, "Hkxcj4-lTQ": {"type": "review", "replyto": "ByxF-nAqYX", "review": "This paper focuses on the problem of unsupervised feature selection, and proposes a method by exploring the locally linear embedding. Experiments are conducted to show the performance of the proposed locally linear unsupervised feature selection method. There are some concerns to be addressed.\n\nFirst, the novelty and motivation of this paper is not clear. This paper seems to directly use one existing dimensionality reduction method, i.e., LLE, to explore the local structure of data. Why uses LLE rather than other methods such as LE? What are the advantages?\n\nSecond, in Section 3.3, authors state that the method might be biased due to the redundancy of the initial features. To my knowledge, there are some unsupervised feature selection to explore the redundancy of the initial features, such as the extended work of f Li et al. (2012) \"Unsupervised Feature Selection via Nonnegative Spectral Analysis and Redundancy Control\". \n\nThird, how about the computational complexity of the proposed method? It is better to analyze it theoretically and empirically.\n\nFinally, the equation above Eq. 8 may be wrong.\n", "title": "Locally Linear Unsupervised Feature Selection", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJxCdB-dnm": {"type": "review", "replyto": "ByxF-nAqYX", "review": "In this paper, the authors presented Locally Linear Unsupervised Feature Selection (LLUFS), where a dimensionality reduction is first performed to extract data patterns, which are used to evaluate compliance of features to the patterns, applying the idea of Locally Linear Embedding.\n\n1. This work basically assumes that the dataset is (well) clustered. This might be true for most real world dataset, but I believe the degree of clustered-ness may vary by dataset. It will be nice to discuss effect of this. For example, if most data points are concentrated on a particular area not being well clustered, how much this approach get affected? If possible, it will be great to formulate it mathematically, but qualitative discussion is still useful.\n\n2. For the dimension reduction, the authors used autoencoder neural network only. What about other techniques like PCA or SVD? Theoretical and experimental comparison should be interesting and useful.\n\n3. This paper is well-written, clearly explaining the idea mathematically. It is also good to mention limitation and future direction of this work. It is also good to cover a corner case (XOR problem) in details.\n\n4. Minor comments:\n - Bold face is recommended for vectors and matrices. For instance, 1 = [1, 1, ..., 1]^T, where we usually denote the left-hand 1 in bold-face.\n - It seems x_j is missing in Johnson-Lindenstrauss Lemma formula. As it is, \\sum_j W_{i,j} is subject to be 1, so the formula does not make sense.", "title": "Locally Linear Unsupervised Feature Selection", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "BJgZhgKDhm": {"type": "review", "replyto": "ByxF-nAqYX", "review": "Summary: The paper proposes the LLUFS method for feature selection. The idea is to first apply a dimensionality reduction method on the input data X to find a low-dimensional representation Z. Next, each point in Z is represented by a linear combination of its nearest neighbors by finding a matrix W which minimizes || Z  - WZ||. Finally, these weights are used to asses the distortion of every feature in X by considering the reconstruction loss in the original space.\n\nComments: There are multiple shortcomings in the motivation of the approach. First, the result of the dimensionality reduction drastically depend on the method used. It is well known that every DR method focuses on preserving certain properties of the data. For instance, PCA preserves the global structure while t-SNE works locally, maximizing the recall [1]. The choice of the DR method should justify the underlying assumption of the approach. I expect that the results of the experiments to change drastically by changing the DR method.\n\nSecond, the LLE method is based on the assumption that if the high-dimensional data is locally linear, it can be projected on a low-dimensional embedding which is also locally linear. Transitioning from a locally linear high-dimensional data to a lower dimension makes sense because there exists higher degree of freedom in the higher dimension. However, making this assumption in the opposite direction is not very intuitive. Why would the features that do not conform to the local linearity of the low-dimensional structure (which itself is obtained via a non-linear mapping) are insignificant?\n\nFinally, there are no theoretical guarantees on the performance of the method. Is there any guarantee that, e.g. given one noisy feature in high dimension, the method will find that feature, etc.?\n\nMinor: what is the complexity of the method compared to the competing methods? What is the runtime? Is this a practical approach on large datasets?\n\nOverall, I do not agree with the assumptions of the paper nor convinced with the experimental study. Therefore, I vote for reject.\n\n[1] Venna et al. \"Information retrieval perspective to nonlinear dimensionality reduction for data visualization.\" Journal of Machine Learning Research 11, no. Feb (2010): 451-490.", "title": "The paper lacks a solid motivation", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HyeU4d1UnQ": {"type": "rebuttal", "replyto": "ByxF-nAqYX", "comment": "We spotted three typos that could hinder the reader's comprehension :\n\n-In the notations section , the unnormalized Laplacian should read \"L = Delta - S\" (instead of \"L = M - S\")\n\n-At the beginning of page 3, in the formal background section, the first eigenvector should read : \"Xi_0 = Delta^(1/2) 1\" (instead of \"Xi_0 = M^(1/2) 1 \")\n\n-In appendix 1, the Laplacian score should read : \"L_j = (1/sigma_j) * Sum_(i,k) (X[i,j] - X[k,j])S_(i,k)\"\n\nWe apologize for these errors and hope those clarifications prove useful.", "title": "Clarifying typos"}, "HJgBSRh3cQ": {"type": "rebuttal", "replyto": "S1l1Zfv6FX", "comment": "Thank you for bringing this blunder to our attention.\nThe paper is now properly anonymized.", "title": "Anonymity issue fixed"}}}