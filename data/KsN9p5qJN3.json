{"paper": {"title": "Energy-based Out-of-distribution Detection for Multi-label Classification", "authors": ["Haoran Wang", "Weitang Liu", "Alex Bocchieri", "Yixuan Li"], "authorids": ["~Haoran_Wang5", "~Weitang_Liu1", "~Alex_Bocchieri1", "~Yixuan_Li1"], "summary": "We investigate OOD detection for multi-label classification networks, and propose an energy-based method which is both theoretically meaningful and empirically effective, establishing state-of-the-art performance on common benchmarks. ", "abstract": "Out-of-distribution (OOD) detection is essential to prevent anomalous inputs from causing a model to fail during deployment. Improved methods for OOD detection in multi-class classification have emerged, while OOD detection methods for multi-label classification remain underexplored and use rudimentary techniques. We propose SumEnergy, a simple and effective method, which estimates the OOD indicator scores by aggregating energy scores from multiple labels. We show that SumEnergy can be mathematically interpreted from a joint likelihood perspective. Our results show consistent improvement over previous methods that are based on the maximum-valued scores, which fail to capture joint information from multiple labels. We demonstrate the effectiveness of our method on three common multi-label classification benchmarks, including MS-COCO, PASCAL-VOC, and NUS-WIDE. We show that SumEnergy reduces the FPR95 by up to 10.05% compared to the previous best baseline, establishing state-of-the-art performance. ", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper aims to do out-of-distribution (OOD) detection in multi-label classification. However,  the challenges of extending energy-based  OOD methods in multiclass to multi-label setting is not big. This paper just defines the label-wise free energy. The key challenging issues in MLC is the label dependency. The paper did not consider modeling the label dependency and devide it to several binary classification issues. And the paper did not provide the theory gurantee. The paper is far below the bar of top conferences."}, "review": {"PvaAWb6ZFKl": {"type": "rebuttal", "replyto": "ftelJrlZe3T", "comment": "Thanks for the response. We are glad to hear that **the reviewer agrees on the significance of our proposed SumEnergy**, which aggregates information over the labels and is better than methods that only consider the maximum scores. We have addressed the concerns regarding math clarity. \n\n1. [notation] As per suggestion, we\u2019ve updated the notations throughout the paper for clarity, using $y_i$ to indicate when an input $\\textbf{x}$ is associated with the $i$-th label. **See changes made in Equations (1) - (16) in Section 2,  as well as notations in Section 3, Page 7-8**.  \n\n2. We expanded our draft by adding formal derivations in forms of joint conditional likelihood  $p(\\textbf{x}\\mid y_1=1,y_2=1,...,y_K=1)$. **See detailed proof in Section 2.2, Equation (12-16) on Page 4**.  To provide a brief proof here for your convenience, we can apply the Bayes' theorem in each term of $\\log p(\\textbf{x} \\mid y_i)$ in $E_\\text{sum}$, which yields:\n\n\\begin{align}\n   E_\\text{sum}(\\textbf{x})&= \\log \\prod_{i=1}^K  \\frac{p(y_i \\mid \\textbf{x}) \\cdot p(\\textbf{x})}{p(y_i)} + Z\\\\\n    = \\log \\prod_{i=1}^K p(y_i \\mid \\textbf{x})  + K\\cdot \\log p(\\textbf{x}) + \\underbrace{(Z - \\log \\prod_{i=1}^K p(y_i))}_{\\text{C: constant for all  $\\textbf{x}$}} \n\\end{align}\n \nGiven all label $y_i$ are conditionally independent given $\\textbf{x}$, we have $\\prod_{i=1}^K p(y_i \\mid \\textbf{x}) = p(y_1,y_2,...,y_K\\mid \\textbf{x})$. Therefore, Equation above is equivalent to:\n \n$$  E_\\text{sum}(\\textbf{x})= \\log p(y_1,y_2, \\ldots, y_K \\mid \\textbf{x}) + K\\cdot \\log p(\\textbf{x}) + C$$\n$$= \\log \\frac{p(\\textbf{x} \\mid y_1,y_2,...,y_K)\\cdot \\prod_{i=1}^K p(y_i)}{p(\\textbf{x})} + K\\cdot \\log p(\\textbf{x})  + C\\text{       \n     (Note: first term is via the Bayes' theorem)}$$\n$$=\\underbrace{\\log p(\\textbf{x} \\mid y_1, y_2,...,y_K)}_{\\text{joint conditional log likelihood}}+ (K-1) \\cdot \\log p(\\textbf{x}) + Z$$\n \nOur proposed SumEnergy therefore captures the underlying data density and the joint information across labels, which is desirable for OOD detection in a multi-label setting. The first term is precisely joint conditional log likelihood, and also highlights the novelty and importance to our work (which is to take into account the joint estimation in a multi-label setting). This also differentiates our method from the multi-class setting which does not consider this term otherwise. \n\n3. Reference to Wu et al. has been added. See Page 8, related work. \n\nIn summary, **we greatly appreciate the feedback and have sufficiently addressed all the concerns of the reviewer**. We hope the reviewer can evaluate our work by considering the importance of the research problem (OOD detection in multi-label setting), technical solution (OOD score estimation jointly from multiple labels through energy), as well as the significance of the results (better by a large margin, with up to 10.05% FPR reduction over current SOTA).\n\n", "title": "We have addressed all the concerns and believe the math is clear; please see the updated draft for formal proof"}, "plK9NhLgtY3": {"type": "rebuttal", "replyto": "KsN9p5qJN3", "comment": "We thank all the reviewers for their insightful feedback. We have additionally responded to each reviewer in detail. We are encouraged that they find our idea to be _interesting_ (R4), _novel_ (R4), _motivated by a very important and underexplored problem_ (R2, R4), and _clearly presented and written_ (R1, R3). We are equally glad they found our results and evaluations _extensive_ (R2), _promising_ (R1), _convincing_ (R3), and _outperforming the state-of-the-art_ (R3, R4). Moreover, we are pleased that (R3) appreciated our mathematical insights on why SumEnergy is suitable for OOD detection in a multi-label setting. **We have incorporated the feedback from all reviewers in our updated draft**. Below we summarize our changes. \n\n[-->R1, R2] We added AUROC curves in Figure 2 (Page6), which shows the effect of threshold $\\tau$. We follow the common practice in literature and select $\\tau$ based on 95% of true positive rate (fraction of in-distribution images that are correctly classified). Threshold-independent metric AUROC was also provided in Table 1. \n\n[-->R1] We expanded the mathematical interpretation and proof in form of joint conditional likelihood (see Page 4 Equation 12-16, Section 2.2). Notations throughout the paper have been revised as suggested.\n\n[-->R2] We have highlighted the connections to attention-based and graph-based multi-label classification methods  (see Page 4 in Section 2.2). \n\n[-->R3] We expanded the discussion on SumEnergy vs. SumProb, highlight the merits of our method both empirically and theoretically (see Page 7).\n\n[-->R3, R4] We expanded our related work, added missing references on energy-based learning (see Page 8-9). \n\n[-->R3] We have added clear citations to the existing baseline methods in Table 1.\n\nIn summary, we believe our work contributes to the field by studying an **underexplored problem** (OOD detection for multi-label classification), proposing an **unexplored and effective solution space** (OOD scoring function estimated jointly from multiple labels, with up to 10.05% FPR reduction compared to the previous best method) that establishes state-of-the-art performance. We also provide theoretical interpretations. Furthermore, our work opens up an **interesting and promising** future direction for OOD detection to consider information jointly from across semantics.\n\nWe thank the reviewers again for the constructive comments which helped greatly improve our work!\n", "title": "Revision summary "}, "nkAh_8bzxj": {"type": "rebuttal", "replyto": "FDatSD9S3N0", "comment": "We thank the reviewer for the positive feedback. We are glad to hear that our paper is well written, with convincing and strong results that outperform baseline methods. Moreover, we are pleased that R3 appreciated our mathematical insights on why SumEnergy is well-suited for the multi-label classification problem. \n\n1 Re: Most baselines are designed by authors.\n\nWe\u2019d like to clarify that MaxLogit, LOF, and IForest are existing methods, which were evaluated in [1] for the OOD detection in multi-label classification networks. Our evaluation framework builds on [1], while expanding with more competitive baselines in literature. We are not inventing things new here and would like to give credit to prior works. **We\u2019ve updated our draft, added clear citations to each baseline method in Table 1**. \n\n[1] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and\nDawn Song. A benchmark for anomaly segmentation, 2019.\n\n\n2 Re: SumEnergy vs SumProb\n\nWe highlight the advantage of SumEnergy over SumProb both empirically and theoretically. **See expanded discussion on Page 7-8**.\n\n**Empirically**: The performance difference between SumEnergy and SumProb is substantial, as evidenced in Table 2. In particular, on MS-COCO, our method outperforms SumProb by 11.56% (FPR95). For threshold independent metric AUROC, SumEnergy consistently outperforms SumProb by 3.38% (MS-COCO), 4.57% (PASCAL), and 4.48% (NUS-WIDE). \n\n**Theoretically**: SumEnergy is a mathematically meaningful measurement and can be interpreted from a joint likelihood perspective (see Section 2.2), whereas SumProb does not. In fact, one can show that the probability score for each individual label is not aligned with the conditioned data density function. To see this, we can derive the probability for each binary logistic classifier as:\n\\begin{align*}\n\t\\log p(y \\mid \\textbf{x}) &=  \\log \\frac{e^{f_y(\\textbf{x})}}{1+ e^{f_{y}(\\textbf{x})}} = f_y(\\textbf{x}) + E_y(\\textbf{x}).\n\\end{align*}\nIn fact, the first term $f_y(\\textbf{x})$ is larger for in-distribution data with label $y$, whereas the second term is smaller for in-distribution data with $E_y(\\textbf{x}) \\propto -\\log p(\\textbf{x}\\mid y)$. This leads to a biased scoring distribution that is no longer proportional to the label-conditional likelihood $\\log p(\\textbf{x}\\mid y)$. In other words:\n\\begin{align*}\n\t\\log p(y \\mid \\textbf{x}) \\not \\propto \\text{data density for label}~y\n\\end{align*}\n\nSumProb, as a result, inherits this weakness theoretically and performs worse than SumEnergy. \n\n3 Re: Missing references for energy-based learning\nThank you - all added in our new draft (**see Page 8-9**)!\n\nIn summary, we believe our work contributes to the field by studying an **underexplored problem** (OOD detection for multi-label classification), proposing an **unexplored and effective solution space** (OOD scoring function estimated jointly from multiple labels, with up to 10.05% FPR reduction compared to the previous best method) that establishes state-of-the-art performance. We also provide theoretical interpretations. Furthermore, our work opens up an **interesting and promising** future direction for OOD detection to consider information jointly from across semantics.\n\nWe thank the reviewer again for the constructive comments which helped greatly improve our work!\n", "title": "[draft updated] Thank you for the positive feedback!"}, "cHccVslskFT": {"type": "rebuttal", "replyto": "wqUMaHy89P6", "comment": "We thank the reviewer for the insightful and helpful comments! We are glad to hear that our paper is well written with superior and promising results. Below we address all 3 comments in detail.\n\n1. Mathematical explanation:\n\nTo clarify this, the interpretation from a joint likelihood can be seen from Equation 11, where $E_\\text{sum}(\\textbf{x}) = \\sum_{y=1}^K \\log p(\\textbf{x}|y) + Z = \\log \\prod_{y=1}^K p(\\textbf{x}| y) + Z$. In this equation, $\\prod_{y=1}^K p(\\textbf{x}| y)$ measures the joint conditioned likelihood for the sample x to be in each class. Equation 13 shows a more direct meaning behind our method, which is equivalent to the probability for the data to be associated with all labels. Intuitively, OOD data should have a low SumEnergy score since $p(\\textbf{x},y)$ will be low for all labels. \n\nOur derivation is based on the assumption that all labels are conditionally independent, in which case reduces the chain probability to $\\prod_{y=1}^K p(\\textbf{x},y)$. This assumption is consistent with standard multi-label classification loss, which is multiple independent binary classification problems. We consider this setting for its simplicity and generality. Our work also opens up an interesting future direction of OOD detection by considering the structural dependency among labels. **We have further clarified this in our updated draft (see Page 4, Section 2.2)**.\n\n2. Threshold:\n\nWe follow the common practice in literature and select $\\tau$ based on 95% of true positive rate (fraction of in-distribution images that are correctly classified). Performance reported in Table 1 shows the average FPR when TPR is 95%. **To see the sensitivity of the threshold, we additionally provide AUROC curves in Figure 2**. This shows how the FPR changes (in x-axis) as we sweep over all thresholds $\\tau$ (which translates into different TPR accordingly). In general, our method produces high AUROC consistently across three in-distribution datasets considered. \n\n\n3. Novelty:\n\nWhile the energy score was used for OOD detection in the multi-class setting (Liu et al., 2020), this method does not directly generalize to a multi-label classification setting. A multi-label setting in fact exacerbates the difficulty of OOD detection since labels are not mutually exclusive. The key technical challenge and contribution of our work are to derive statistics by considering the joint information across labels (which is indeed reflected in our formulation as clarified above). This joint estimation of the OOD scoring function was previously unexplored in literature, hence not incremental but new to our work. \n\nIn particular, we showed in Section 2.2 the novel definition of SumEnergy derived from a multi-label classification network, as well as its mathematical interpretations from a joint likelihood view---both of which are new contributions to this work, and are distinct from prior works. \n\nOur empirical results in Table 1 support precisely the importance of taking into account joint information from across labels, which was not considered in previous baselines.\n\nIn summary, our work contributes to the field by both studying an **underexplored problem** (OOD detection for multi-label classification) and proposing an **unexplored solution space** (OOD scoring function estimated jointly from multiple labels). Furthermore, it opens up an **interesting and promising** future direction for OOD detection to consider information jointly from across semantics.\n", "title": "[draft updated] clarification on math explanation, threshold, and novelty. Thank you for the insightful comments!"}, "LKg-l95bTDr": {"type": "rebuttal", "replyto": "wqUMaHy89P6", "comment": "Dear reviewer, \n\nWe believe all three comments raised have been addressed in our response and updated draft. We'd like to follow up and clarify any remainder confusion.\n\nYour feedback has been very important for us to improve the work!\n\nThank you,\nAuthors of Paper1862", "title": "follow up"}, "lyCsjO2FJPT": {"type": "rebuttal", "replyto": "mk0vLqT-w1G", "comment": "We are really glad to hear the concerns have been addressed.\n\nOnce again thank you for the great comments which helped us improve the work!", "title": "thank you!"}, "OgJ_8dXohjq": {"type": "rebuttal", "replyto": "Ir96jTCaFL9", "comment": "We appreciate the reviewer for finding our work interesting and pointing out the missing references! We\u2019ve updated our draft, added citation [1], and several subsequent works in the development of EBMs. **See Page 8-9 for changes**. \n\nRe: Novelty\n\nWhile the energy score was used for OOD detection in the multi-class setting (Liu et al., 2020), this method unfortunately does not directly generalize to a multi-label classification setting. A multi-label setting in fact exacerbates the difficulty of OOD detection since labels are not mutually exclusive. The key technical challenge and contribution of our work are to derive statistics by considering the joint information across labels. This joint estimation of the OOD score from multiple labels was previously unexplored in literature, hence not incremental but new to our work. \n\nIn particular, we showed in Section 2.2 the novel definition of SumEnergy derived from a multi-label classification network, as well as its mathematical interpretations from a joint likelihood view---both of which are new contributions to this work, and are methodologically distinct from prior works. \n\nOur empirical results in Table 1 support precisely the importance of taking into account joint information from across labels, which was not considered in any of the previous baselines (Hendrycks et al. 2019, Liang et al. 2018, Lee et al. 2018) relying on maximum scores. \n\nIn summary, our work contributes to the field by both studying an **underexplored problem** (OOD detection for multi-label classification) and proposing an **unexplored solution space** (OOD scoring function estimated jointly from multiple labels). Furthermore, it opens up an **interesting and promising** future direction for OOD detection to consider information jointly from across semantics.\n", "title": "[draft updated] Missing references have been added; novelty recap"}, "Ir96jTCaFL9": {"type": "review", "replyto": "KsN9p5qJN3", "review": "======== original feedback ===================\n\nReview: This paper studies out-of-distribution (OOD) detection for multi-label classification with energy-based models. Specifically, the paper proposes to use SumEnergy, which aggregates energy scores from multiple labels, to estimates the OOD. Empirical studies are performed to validate the proposed framework on several benchmarks.\n\nStrength: \n+ The paper studies out-of-distribution detection for multi-label classification by using an energy-based framework, which is a real and practical problem. \n+ The paper provides comprehensive experiments to show the effectiveness of the proposed energy-based framework for out-of-distribution detection with multi-label classification. The method also establishes the state-of-the-art performance, which is good.\n\nConcerns:\n+ since the paper for energy-based out-of-distribution detection has existed. Generalizing it to multi-label context is incremental. \n+ missing some important references:  the current paper didn\u2019t discuss and cite [1], which is the first paper to show that a discriminative classifier can be interpreted from an energy-based perspective. \n+ the related work about EBM is not comprehensive. Even though the current paper is about discriminative EBM, a discussion about the development of the generative EBMs is desirable. \n\n[1] A Theory of Generative ConvNet (ICML 2016)\n\n======== score changed ==============\n\nMy major concern has been addressed by the reply from the authors.  The revised paper has been improved.   \n\n", "title": "an interesting paper studying an underexplored problem", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rEa0pUIn4wR": {"type": "rebuttal", "replyto": "3bLBcp0nXV", "comment": "We thank the reviewer for the constructive feedback! We are encouraged to hear that our motivation and idea to be appealing and that our analysis and evaluations are extensive and strong. We absolutely agree with your view that OOD detection in multi-label classification deserves research further. Below we address the comments in detail. \n\nRe: Effect of energy threshold $\\tau$\n\nGreat question! We follow the common practice in literature and select $\\tau$ based on 95% of true positive rate (fraction of in-distribution images that are correctly classified). To see the effect of $\\tau$, **we have updated the draft with the AUROC curves in Figure 2, Page 6**. The curves show how the performance changes (FPR, x-axis) as we vary the TPR (that determines $\\tau$) on the y-axis. We report AUROC performance in Table 1, which is a threshold-independent measurement for OOD detection. In general, our method produces high AUROC consistently across three in-distribution datasets considered. \n\nRe: Attention-based & graph-based methods for multi-label classification\n\nVery interesting suggestions! Our main focus in the paper is to derive OOD scoring statistics from the output of a pre-trained multi-label classification model, rather than improving the multi-label classification model itself. We opt for the standard multi-label classification network for its generality and simplicity. However, we concur that improved training mechanisms (including suggested graph-based & attention-based methods) for the classification network can be beneficial for improving the downstream OOD scoring estimation. **We have highlighted these works on Page 4 in Section 2.2**, and are excited to explore this direction in future work. \n\nRe: few-shot learning vs. OOD detection\n\nWe'd like to highlight both the difference and connections between these two learning problems. \n\n**Difference**: In few-shot/zero-shot learning, a model may be given very few or no examples of a specific class in its training data. This class is nonetheless assumed to belong to the training data, and the model is expected to generalize to this class.  In contrast, OOD detection attempts to detect what the model cannot generalize to (e.g., a completely unseen or unknown semantics). From a learning problem perspective, the formulations and objectives are different. \n\n**Connection**: In open-world learning, one can use OOD detection for identifying novel categories as a precursor. Few-shot learning can be useful for subsequent training by incorporating the novel data detected. Though interesting to us, such a learning framework is out of the scope of current work. \n", "title": "[draft updated] Thank you for the constructive feedback!"}, "FDatSD9S3N0": {"type": "review", "replyto": "KsN9p5qJN3", "review": "Summary: In this work, SumEnergy is proposed for out-of-distribution detection for multi-label classification. According to the results of the experiment, SumEnergy performs better than MaxEnergy in several datasets of multi-label classification. \n \n+ves: \n1. This paper is written well. The main part is clear.  With the mathematical equation, it shows that SumEnergy is linearly aligned with the log of joint likelihood. The aggregated include more likelihood information over all labels. \n\n2. The results in Tables 1 and 2 show a clear comparison with other baselines. It also indicates that SumEnergy performs best among all methods. The analysis and Qualitative case study are convincing. \n\nConcerns\n1. Most baselines are designed by the authors. Unfortunately, there are no other baselines that are done by other papers. The contribution of this work could be a little questionable.  \n\n2. The SumEnergy seems to has a minor difference with Sum Prob.  Maybe more reason on why still use energy score could be discussed. \n\nQuestions during the rebuttal period: \n\n\nSome related work on energy-based learning could be mentioned.:\n \n[1] Structured Prediction Energy Networks, ICML 2016\n\n[2] Learning Approximate Inference Networks for Structured Prediction. ICLR 2018\n\n[3] Residual Energy-Based Models for Text Generation. ICLR 2020\n", "title": "clear work, however, the contribution could a little limited. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "wqUMaHy89P6": {"type": "review", "replyto": "KsN9p5qJN3", "review": "This paper deals with the out-of-distribution detection task in the multi-label classification (MLC) setting. It proposes an energy-based method called SumEnergy, which estimates the OOD indicator scores by aggregating energy scores from multiple labels. Experimental results illustrate its superiority compared with other baselines.\n\n\n##############################################################################################\npros:\n1. Overall, this paper is well-written and organized.\n2. The proposed method achieves promising experimental results.\n\n##############################################################################################\ncons:\n1. The explanation for why SumEnergy works is confusing. Specifically,  in Eq.(13), authors explain $E_{sum}(\\mathcal{x})$ is linearly aligned with the log of joint likelihood, which is confusing, because $p(\\mathcal{x}, \\mathcal{y}) = p(\\mathcal{x}, y_1) p( y_2 \\| \\mathcal{x}, y_1)  ...  p( y_K \\| \\mathcal{x}, y_1, ... , y_{K-1})$ while Eq.(13) has $\\prod_{j=1}^{K} p(\\mathcal{x}, y_j)$. (For clarity, I have changed some notations.) \n2. What is the effect of the hyperparameter $\\tau$ for these two methods (i.e. MaxEnergy and SumEnergy)? Please give more comparisons and clarify how to tune $\\tau$.\n3. The novelty is incremental. Although authors highlight the non-triviality to extend from the multi-class setting (Liu et al., 2020) to multi-label setting lies in leveraging information between different labels,  the proposed method SumEnergy may not reflect this point due to 1.", "title": "The explanation is confusing", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "3bLBcp0nXV": {"type": "review", "replyto": "KsN9p5qJN3", "review": "####################\n\nSummary: \n\nThe paper proposes a SumEnergy method to estimate the out-of-distribution indicator scores for multi-label classification.\n\n####################\n\nReason for score:\n\nOverall, the paper is above the borderline. I like the idea of utilizing SumEnergy operation to address the out-of-distribution problem in the task of multi-label classification. My major concern is about some unclear parts in the paper and insufficient experimental comparison (see cons below). Hopefully, it would be grateful that the authors could address my concerns during the rebuttal period. \n\n\n####################\n\nPros:\n\n(1) The motivation of the paper, i.e., out-of-distribution detection in multi-label classification is very important and deserves research further.\n\n(2) Extensive experimental results demonstrate that the SumEnergy based on the aggregation over label-wise energy scores achieves better performance than that of MaxEnergy.\n\n(3) The comparative analysis and ablation study in the paper are convinced and detailed, which can help better understand the applicability in the multi-label setting. \n\n####################\n\nCons:\n\n(1) Even though the experimental results demonstrate that the proposed SumEnergy outperforms MaxEnergy and MaxLogit, it will be better to conduct additional comparisons with other methods that also consider the information of all the labels, such as attention-based methods [1][2] and graph-based [3][4] multi-label classification approaches.\n\n[1] Recurrent attentional reinforcement learning for multi-label image recognition. AAAI 2018\n[2] Decoupling category-wise independence and relevance with self-attention for multi-label image classification. Arxiv 2019\n[3] Learning semantic-specific graph representation for multi-label image recognition. Arxiv 2019\n[4] Multi-label image recognition with graph convolutional networks. CVPR 2019\n\n(2) According to the description in the paper, the OOD problem is similar to the few-shot or zero-shot issue. Could the authors explain more about the difference between OOD and few-shot/zero-shot learning, please? Moreover, more experimental details and results on few-shot/zero-shot problems will be welcome.\n\n(3) How to determine the energy threshold seems very critical to the task, because the scores always various on different dataset and domains with the proposed SumEnergy method. More discussion and the impact for experimental results should be added in the paper.\n\n####################\nQuestions during the rebuttal period: \n\nPlease address and clarify the cons above. Thank you!", "title": "The paper proposes a SumEnergy method to estimate the out-of-distribution indicator scores for multi-label classification.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}