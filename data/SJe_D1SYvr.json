{"paper": {"title": "Partial Simulation for Imitation Learning", "authors": ["Nir Baram", "Shie Mannor"], "authorids": ["nirb@campus.technion.ac.il", "shie@ee.technion.ac.il"], "summary": "A formulation of solving imitation problems using RL, without requiring full knowledge about the state dynamics", "abstract": "Model-based imitation learning methods require full knowledge of the transition kernel for policy evaluation. In this work, we introduce the Expert Induced Markov Decision Process (eMDP) model as a formulation of solving imitation problems using Reinforcement Learning (RL), when only partial knowledge about the transition kernel is available. The idea of eMDP is to replace the unknown transition kernel with a synthetic kernel that: a) simulate the transition of state components for which the transition kernel is known (s_r), and b) extract from demonstrations the state components for which the kernel is unknown (s_u). The next state is then stitched from the two components: s={s_r,s_u}. We describe in detail the recipe for building an eMDP and analyze the errors caused by its synthetic kernel. Our experiments include imitation tasks in multiplayer games, where the agent has to imitate one expert in the presence of other experts for whom we cannot provide a transition model. We show that combining a policy gradient algorithm with our model achieves superior performance compared to the simulation-free alternative.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Behavior Cloning", "Partial Simulation"]}, "meta": {"decision": "Reject", "comment": "The paper introduces the concept of an Expert Induced MDP (eMDP) to address imitation learning settings where environment dynamics are part known / part unknown. Based on the formulation a model-based imitation learning approach is derived and the authors obtain theoretical guarantees. Empirical validation focuses on comparison to behavior cloning. Reviewers raised concerns about the size of the contribution. For example, it is unclear to what degree the assumptions made here would hold in practical settings."}, "review": {"r1xJuNzPYH": {"type": "review", "replyto": "SJe_D1SYvr", "review": "* Paper summary.\nThe paper considers an IL problem where partial knowledge about the transition probability of the MDP is available. To use of this knowledge, the paper proposes an expert induced MDP (eMDP) model where the unknown part of transition probability is modeled as-is from demonstrations. Based on eMDP, the paper proposes a reward function based on an integral probability metric between a state distribution of expert demonstrations under the target MDP and a state distribution of the agent under the eMDP. Using this reward function, the paper proposes an IL method that maximizes this reward function by RL. The main theoretical result of the paper is that the error between value function of the target MDP and that of the eMDP can be upper-bounded. Empirical comparisons against behavior cloning on discrete control tasks show that the proposed method performs better. \n\n* Rating.  \nThe main contribution of the paper is the eMDP model, which enables utilizing prior knowledge about the target MDP for IL. While this idea is interesting, eMDP is too restrictive and its practical usefulness is unclear. Moreover, there are other issues that should be addressed such as clarity and experiments. I vote for weak rejection. \n\n* Major comments:\n- Limited practicality due to an assumption on the unresponsive transition kernel.\nIn eMDP, the unresponsive transition kernel is modeled from demonstrations by directive using the observed next states (in demonstrations) as a next state of the agent. This modeling implicitly assumes that the agent cannot influence the unresponsive part of the state space. This is too restrictive, since actions of the agent usually influence other parts of the state space such as opponents and objects. For instance, in pong, actions of the agent indeed influence trajectories of the ball. Due to this restrictive assumption, I think the practicality of eMDP is too limited.\n\nAlso, it is unclear what happens when the transition of unresponsive state space is stochastic. In this case, the unresponsive transitions of eMDP would be incorrect since eMDP assumes deterministic transitions as-is from demonstrations. Though I am not sure about this. \n\n- The equality in Eq. (3) should be an upper-bound. The IPM is defined by the absolute difference between summations (expectations), but the right-hand side of Eq. (3) is summations (expectations) of the absolute difference. These two are not equal, and the right-hand side should be an upper-bound. \n\n- The paper is difficult to follow. There are many skipping contents in the paper. Especially, in the description of eMDP and its solution, where the paper describes the optimal solution of eMDP (Section 2) before describing the eMDP model (Section 4). Also, it is unclear from my first reading pass what is the actual IL procedure. Including a pseudo-code would help. \n\n- The state-value function\u2019s error bound ignores a policy. The proof utilizes the state distribution P(s\u2019|s). However, this distribution should depend on a policy. It is unclear to me what is the policy function used in this error bound. Does this error-bound hold for any policy or only for the optimal policy? In the case that it only holds for the optimal policy, this result still does not provide useful guarantees when the optimal policy is not learned, similarly to the result in Proposition 2.1.\n\n- Empirical evaluation lacks strong baseline methods. The paper compares the proposed method against behavior cloning, which is known to perform poorly under limited data. The paper requires stronger baseline methods such as those mentioned in Section 3. Also, it is strange that the vertical axis in Figure 2 represents the reward function of eMDP, which is an artificial quantity in Eq. (5). The results should present the reward function of the target MDP, which is the ground-truth reward of the task.\n\n* Besides the major comments, I have few minor comments and questions:\n- Does the reward function in Eq. (5) correspond to the Wasserstein distance for any metric d_s? If it is not, then the value function\u2019s error-bound for the Wasserstein distance is not applicable to this reward function. \n- What is the reward function r(s) in the value function\u2019s error-bound that we can control the Lipschitz constant? We do not know the ground-truth reward r(s,a) so it cannot be controlled. The reward r(s,s\u2019) of eMDP is given by the metric d_s of the state space so again we cannot control it. \n- What is the metric d_s used in the experiments? \n- How do you perform BC without expert actions in the experiments? \n- Confusing notations. E.g., the function r is used for r(s,a) (MDP\u2019s reward), r(s, s\u2019) (eMDP\u2019s reward), and r(s) (reward in the error bound\u2019s proof); these r\u2019s have different meaning and should be denoted differently.\n- Typos. E.g., \u201cLemma 2.1\u201d in Section 5 should be \u201cProposition 2.1\u201d, \u201cLemma A.1\u201d should be \u201cProposition 2.1\u201d. \"tildeF\". etc.\n\n*** After authors' response.\nI read the other reviews and the response. My major concern was the impracticality of eMDP. While I can see that the eMDP model provides advantages over BC, I still think that it is impractical since it assumes the agent cannot influence the unresponsive part. Moreover, extracting next-unresponsive states as-is from demonstrations is problematic with stochastic transitions, since actual next-unresponsive states can be different from next-unresponsive states in demonstrations even when the sames state and actions are provided. Given that the major concern remains, I still vote for weak rejection. \n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "SygLpncfjS": {"type": "rebuttal", "replyto": "r1xJuNzPYH", "comment": "We thank the reviewer for his detailed response. Below is our response to the main issues raised. Other issues will be fixed or handled in the PDF.\n\n\u2022\tQ: \u201c\u2026This modeling implicitly assumes that the agent cannot influence the unresponsive part of the state space. This is too restrictive\u2026\u201d\n\u2022\tA: The eMDP model bridges between BC and contemporary imitation methods by using the BC kernel for features we cant simulate, and the responsive kernel for features we can. While with BC the agent cant influence any part of the state space, our model allows some interaction to occur. Our experiments show that even partial interaction is better than zero interaction.\n\n\u2022\tQ: I think the practicality of eMDP is too limited.\n\u2022\tA: To the best of our knowledge, this is the first and only method that allows imitation in a multiplayer setup.\n\n\u2022\tQ: Also, it is unclear what happens when the transition of unresponsive state space is stochastic. In this case, the unresponsive transitions of eMDP would be incorrect since eMDP assumes deterministic transitions as-is from demonstrations. Though I am not sure about this.\n\u2022\tA: eMDP does not assume deterministic transitions.\n\n\u2022\tQ: The state-value function\u2019s error bound ignores a policy. The proof utilizes the state distribution P(s\u2019|s). However, this distribution should depend on a policy. It is unclear to me what is the policy function used in this error bound. Does this error-bound hold for any policy or only for the optimal policy?\n\u2022\tA: The value function error is true for any policy and not just for the optimal one, thus it was omitted. A comment about it should be added to the text.\n\n\u2022\tQ: Empirical evaluation lacks strong baseline methods. The paper compares the proposed method against behavior cloning, which is known to perform poorly under limited data. The paper requires stronger baseline methods such as those mentioned in Section 3.\n\u2022\tA: Unfortunately, this is not possible. Stronger baselines require full knowledge of F, which does not apply in our setting. For example, we cannot provide rules for the behavior of the opponent human player.\n\n\n\u2022\tQ: Does the reward function in Eq. (5) correspond to the Wasserstein distance for any metric d_s? If it is not, then the value function\u2019s error-bound for the Wasserstein distance is not applicable to this reward function.\n\u2022\tA: The reward is defined over the metric space S while the Wasserstein distance is defined between probability distributions. In Section 5, we relate the metric distance to the distribution distance. Please see footnote 2 at the bottom of page 6.\n\n\u2022\tQ: What is the reward function r(s) in the value function\u2019s error-bound that we can control the Lipschitz constant? We do not know the ground-truth reward r(s,a) so it cannot be controlled. The reward r(s,s\u2019) of eMDP is given by the metric d_s of the state space so again we cannot control it.\n\u2022\tA: Yes. This is the reward from Eq. 5. It is determined by the state space and cannot be controlled.\n\n\u2022\tQ: What is the metric d_s used in the experiments?\n\u2022\tA: This is the reward from Eq. 5. We assume no knowledge about the state space and use a Dirac delta function at the event s_r=s_{r,e}. In some of the environments (that involve a shooting action) we implemented the Dirac function asymmetrically (agent shoots and experts don\u2019t is rewarded differently than expert shoots and agents don\u2019t).\n\n\u2022\tQ: How do you perform BC without expert actions in the experiments?\n\u2022\tA: The BC baseline \u201ccheats\u201d by looking at expert actions, while the eMDP model doesn't access A.\n", "title": "response to review #3"}, "Syeg935GoS": {"type": "rebuttal", "replyto": "rkgvO5taFB", "comment": "We thank the reviewer for his positive feedback. Below is our response to the issues raised.\n\n\u2022\tQ: For the experimental part, BC baselines for multiplayer games just imitates the target player's behavior (player1), completely ignoring the states/actions of player2 (ignoring the states of bricks and balls for breakout-v0)?\n\u2022\tA: The BC baseline receives the original (un-factored) state as an input. In Breakout it is the complete frame including the bricks, ball and expert racket. The baseline is also granted access to expert action which we do not provide to the eMDP model. I.e., the BC baseline \u201ccheats\u201d.\n", "title": "Response to review #2"}, "H1eDzhcfiB": {"type": "rebuttal", "replyto": "SkeQKHjaYH", "comment": "We thank the reviewer for his important comments and apologies for the issue with the link anonymization. Below is our response to the issues raised.\n\n\u2022\tQ: Isn't it problematic that the imitation agent is trained in an environment where unresponsive state features from the demonstrations are replayed and not affected by the agent's actions?\n\u2022\tA: Yes. Ideally, you would like to use a \u201cfull\u201d simulation to eliminate the problem. However, most real-world problems do not have a simulator yet. Our experiments show that even partial interaction of the agent with the environment is better than zero interaction that happens with simulation free methods such as BC. Moreover, our transition kernel becomes less biased as the agent improves.\n\n\u2022\tQ: It would be nice to expand Section 4 to explain why it makes sense to reflect the unresponsive component in the transition kernel.\n\u2022\tA: Yes, we agree. The reflection operator is the transition kernel of BC where the prediction of the agent on a state does not affect the next state. The eMDP model bridges between BC and contemporary imitation methods by using the BC kernel for features we cant simulate, and the responsive kernel for features we can simulate. As stated before, the reflection operator also possesses the nice property of being unbiased when pi=pi_E and it requires zero calculations, thus dramatically accelerating training time. \n\n\u2022\tQ: What is the purpose of observing unresponsive features, if the reward function for the imitation agent is only defined in terms of the responsive features?\n\u2022\tA: The reward is defined over a) s_r: the responsive features of the agent which are observed in the state space, and b) s_{r,e}: the responsive features of the expert which are unobserved. How can the agent maximize the reward if it depends on unobserved features? It does so by observing s_u: the unresponsive features, which define (or at least have high mutual information with) s_{r,e}.\n\n\u2022\tQ: Is it that the imitation policy is conditioned on the unresponsive features?\n\u2022\tA: Yes, of course, the unresponsive features are a part of the state space and the agent and expert share the same state space. Think of Pong for example where the unresponsive features represent the opponent racket and the ball. The agent can't take actions without having this info.\n\n\u2022\tQ: What distance metric was used to define the reward function?\n\u2022\tA: We report the reward defined in Eq.5 (the optimization problem we wish to solve). In words, the reward is a Dirac delta function on the event s_r=s_{r,e}. It is the most restrictive form of similarity and requires zero prior knowledge. Calculating it requires extracting s_r and s_{r,e} at each step as explained in section 4. In the Atari games, we do the extraction manually from frame pixels. However, with access to the game\u2019s ROM, this can be done much easier.\n", "title": "Response to Review #1"}, "rkgvO5taFB": {"type": "review", "replyto": "SJe_D1SYvr", "review": "This paper provides a theoretical basis called expert-induced MDP (eMDP) for formulating imitation learning through RL when only partial information on the transition kernel is given. Examples include imitating one player in multiplayer games where the learner cannot explicitly have the transition kernel for other players. eMDP just uses the available part of transition kernel (e.g. the target player's state-action transitions) for learning, but substitute the next state from demonstrations 'as is' into the unavailable part. In this situation, using IPM (Wasserstein-like) divergence, it is easily guaranteed that eMDP is consistent with the optimal solution in the limit. However, this does not guarantee anything when the optimum is not reached. The paper's main contributions would be to provides error bounds (Lemma 5.2 in particular) for the convergence rate in terms of Lipschitz constants and the maximum error between eMDP and the demonstration set. This roughly means that when transition functions are continuous and smooth and the sample demonstration set well characterize the underlying MDPs, then eMDP is theoretically guaranteed to work well. If these factors (Lipschitz continuity and the max deviation of state distributions conditioned on the given demonstration set) are expected to be small, eMDP can nicely perform learning with partial knowledge. Some demonstrations confirm eMDP works when simple BC fails in multiplayer games.\n\nOverall, the paper is worth accepting. The proposed eMDP would be a technically simple solution, but providing a solid theoretical basis for that solution to these commonly seen situations of imitation learning with partial knowledge would be quite beneficial. The paper is well written with intuitive examples and nice overview summaries for the theoretically complicated parts. Though the implications of the experimental part are not clear, the paper fairly states this point and also provides a video for visual assessment with displaying ghost imitations. \n\nOne minor question: For the experimental part, BC baselines for multiplayer games just imitates the target player's behavior (player1), completely ignoring the states/actions of player2 (ignoring the states of bricks and balls for breakout-v0)? ", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}, "SkeQKHjaYH": {"type": "review", "replyto": "SJe_D1SYvr", "review": "The paper proposes an imitation learning algorithm that learns from state-only demonstrations and assumes partial knowledge of the transition dynamics. The demonstrated states are decomposed into \"responsive\" and \"unresponsive\" features. The imitation policy is trained in an environment where the responsive state features are simulated and controllable by the agent, and the unresponsive state features are replayed from the demonstrations. The agent is rewarded for tracking the responsive state features in the demonstrations.\n\nOverall, I was confused by this paper. Isn't it problematic that the imitation agent is trained in an environment where unresponsive state features from the demonstrations are replayed and not affected by the agent's actions? It would be nice to expand Section 4 to explain why it makes sense to reflect the unresponsive component in the transition kernel. It would also be helpful to include some of this information in Section 1.\n\nI am also confused by the method. What is the purpose of observing unresponsive features, if the reward function for the imitation agent is only defined in terms of the responsive features? Is it that the imitation policy is conditioned on the unresponsive features?\n\nThere are several important experimental details missing from Sections 4 and 6. What distance metric was used to define the reward function? For example, was it Euclidean distance in pixel space for the Atari games? The main experimental results in Figure 2 are difficult to interpret without knowing the how the reward function is defined in the eMDP. Could the authors either provide definitions of the reward functions of the eMDPs in the experiments, or measure performance of the imitation agents using more interpretable or standardized metrics (e.g., game score) for the chosen environments?\n\nIt is also concerning that the Google Drive links to code and supplementary materials aren't anonymized.", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 2}}}