{"paper": {"title": "Paleo: A Performance Model for Deep Neural Networks", "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"], "summary": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "keywords": ["Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The reviewers were consistent in their praise of the paper. They asked for newer architectures, e.g. ResNet, DenseNet. The authors released an update with a Caffe converter which provides access to a wide range of CNNs and residual networks (ResNet-50 and DenseNet examples are provided). This seems like an incredibly useful tool and very glad it is open source. Paper is a clear accept."}, "review": {"BJgvHw4Ug": {"type": "rebuttal", "replyto": "SyVVJ85lg", "comment": "We thank all the reviewers for reading and commenting on the paper! \n\nAnonReviewer1: \u201cIt would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet\u201d\n\nResponse: \nWe released a converter to port Caffe model specs to Paleo, and thus Paleo now supports a wide range of CNNs and residual networks.  Our GitHub repository provides several examples, including ResNet-50 and DenseNet via the Caffe converter. Details can be found in our open source repository (https://github.com/TalwalkarLab/paleo) and online UI (https://talwalkarlab.github.io/paleo/).\n\nAnonReviewer2: \n\u201c...tests are only performed on a few networks of very similar type...\u201d\n\u201cMuch broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model.\u201d\n\nResponse: \nWe agree with the reviewers and it is our goal to make Paleo support a wide variety of models. Based on the observation that neural networks are assembled with a set of commonly reused operators, we take a bottom-up approach to estimating scaling properties of neural networks by analyzing these operators. Since the vast majority of scalability efforts have thus far focused on convolutional neural networks, we initially focused on CNNs. However, we are actively working on generalizing our work and have already extended Paleo to support GAN models, e.g., see our additional experimental result for a GAN model in Section 4.3.2 of the revised version of our paper (1/11/2017). Paleo can naturally extend to other types of models (e.g. RNNs) following the same modeling principles, and we will continue working on such extensions in further releases of Paleo. \n\nIn terms of the variety of settings, although the NiN, AlexNet, Inception are all CNNs, they includes very different layer sizes and configurations. In the case studies we presented in the paper we already consider a variety of batch sizes. In our third case study with a hybrid model, we evaluate model parallelism and node placement on up to eight devices. Remarkably, our simple model can make accurate predictions for all of these experimental setups, and we view the simplicity (and associated transparency / interpretability) of our model to be a key virtue of Paleo. ", "title": "Rebuttal"}, "Hy-p02CQe": {"type": "review", "replyto": "SyVVJ85lg", "review": "I found the paper clear enough, had no questions, but want this task to go away.In PALEO the authors propose a simple model of execution of deep neural networks. It turns out that even this simple model allows to quite accurately predict the computation time for image recognition networks both in single-machine and distributed settings.\n\nThe ability to predict network running time is very useful, and the paper shows that even a simple model does it reasonably, which is a strength. But the tests are only performed on a few networks of very similar type (AlexNet, Inception, NiN) and only in a few settings. Much broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model. This is why this reviewer considers this paper borderline -- it's a first step, but a very basic one and without sufficiently large experimental underpinning.\n\nMore experiments were added, so I'm updating my score.", "title": "No question", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1Y403RQe": {"type": "review", "replyto": "SyVVJ85lg", "review": "I found the paper clear enough, had no questions, but want this task to go away.In PALEO the authors propose a simple model of execution of deep neural networks. It turns out that even this simple model allows to quite accurately predict the computation time for image recognition networks both in single-machine and distributed settings.\n\nThe ability to predict network running time is very useful, and the paper shows that even a simple model does it reasonably, which is a strength. But the tests are only performed on a few networks of very similar type (AlexNet, Inception, NiN) and only in a few settings. Much broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model. This is why this reviewer considers this paper borderline -- it's a first step, but a very basic one and without sufficiently large experimental underpinning.\n\nMore experiments were added, so I'm updating my score.", "title": "No question", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkiotF5Xg": {"type": "rebuttal", "replyto": "SyzvzN7Qx", "comment": "As a follow-up to the previous response, we have release a live demo at https://talwalkarlab.github.io/paleo/.\nNotably, we added a cost estimation feature that predicts dollar costs on AWS instances.\n\nThe current interface provides a predefined set of configurations and works with data parallelism. We will allow users to upload customized networks and model splits in the coming releases of the interface; although core implementations are already include in the open-sourced repository.", "title": "Live demo is online now"}, "BJJqle4Ql": {"type": "rebuttal", "replyto": "SyzvzN7Qx", "comment": "Thank you for the comment. As an update, we have pushed our initial open-source release at https://github.com/TalwalkarLab/paleo.\n\nIn addition, we are also working on a web interface based on this code. We will post the link here when it becomes available by the end of this week.", "title": "Open-source release"}, "HJMp1G-Qx": {"type": "rebuttal", "replyto": "Syhu4rRfx", "comment": "Thanks for the comments! Here are the clarifications:\n\n(1) The definition of \u201cone step time\u201d is the total time of forward propagation, backward propagation, and parameter update for one mini-batch on one worker.  We will add a sentence in the text to clarify this in our next revision.\n\n(2) We did not include the reported times in Table 2 for various reasons: for case studies 1 and 3 the original publications do not mention these times explicitly; for case study 2 no run time information is provided whatsoever; and since case study 4 is a hypothetical setup, there is no number to report.\n\nHowever, for case studies 1 and 3, we can in fact derived this number approximately from the available information in the papers, and the numbers are as follows: \n- Case 1:  Paleo: 1918.33 ms*; FireCaffe: 2274.59 ms.\n- Case 3:  Paleo: 402 ms; OneWeirdTrick: 418.35 ms.\n\n* We just noticed a typo in Paleo\u2019s one step time estimate for Case 1 our v2 draft (11/15). We fixed a minor bug in our Case 1 results between v1 and v2 (as seen Table 3), but we didn\u2019t update our \u2018one step time\u2019 number in Table 2. We apologize for this oversight and it will be fixed shortly in our next revision.", "title": "Clarifications"}, "Syhu4rRfx": {"type": "review", "replyto": "SyVVJ85lg", "review": "Can you explain what is \"one step time\" in Table 2? Also again in Table 2, there are estimates by PALEO, can you also add the reported results from the publications for these estimations? This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies. \nThe paper is very clear.  The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature.\nFurthermore, their code is open-source and the live demo is looking good. \nThe authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful.\nIt would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.\n\n", "title": "Clarification - Table 2", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1GUJz-Ne": {"type": "review", "replyto": "SyVVJ85lg", "review": "Can you explain what is \"one step time\" in Table 2? Also again in Table 2, there are estimates by PALEO, can you also add the reported results from the publications for these estimations? This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies. \nThe paper is very clear.  The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature.\nFurthermore, their code is open-source and the live demo is looking good. \nThe authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful.\nIt would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.\n\n", "title": "Clarification - Table 2", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJyhqG5Ge": {"type": "rebuttal", "replyto": "S1hB5J9Gg", "comment": "Thanks for the comment! Yes, we do have plans to make it publicly available soon. And indeed as one of applications/future directions, it can be used by software frameworks for informed model splitting and device placement. Although for this paper we emphasize our model is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies.", "title": "Yes"}, "S1hB5J9Gg": {"type": "review", "replyto": "SyVVJ85lg", "review": "Or better: incorporate it into TensorFlow so that the framework is able to make better decisions.This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.\n\nIn terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.\n\nIn the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.\n\nUPDATED: code is now available. Revised review accordingly.", "title": "Any plans to open-source the model?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyzvzN7Qx": {"type": "review", "replyto": "SyVVJ85lg", "review": "Or better: incorporate it into TensorFlow so that the framework is able to make better decisions.This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.\n\nIn terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.\n\nIn the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.\n\nUPDATED: code is now available. Revised review accordingly.", "title": "Any plans to open-source the model?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}