{"paper": {"title": "Sequence to Sequence Transduction with Hard Monotonic Attention", "authors": ["Roee Aharoni", "Yoav Goldberg"], "authorids": ["roee.aharoni@gmail.com", "yoav.goldberg@gmail.com"], "summary": "Sequence to sequence learning with a hard attention mechanism that works better than soft attention models on monotonically aligned sequences", "abstract": "We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task.", "keywords": ["Natural language processing", "Applications"]}, "meta": {"decision": "Reject", "comment": "While this area chair disagrees with some reviewers about (1) the narrowness of the approach's applicability and hence lack of relevance to ICLR, and also (2) the fairness of the methodology, it is nonetheless clear that a stronger case needs to be made for novelty and applicability."}, "review": {"SJXlNaGXg": {"type": "rebuttal", "replyto": "Sysdg2Mmx", "comment": "Thanks for the question! \n\nTo make it clear, in all experiments both our model and the soft attention model are trained with the exact same training set and development set, which consists solely of parallel input and output sequences. The difference between the models is the following: \n\nIn our approach, we decouple the training phase: we first learn an alignment over the the training set (the fixed alignments you mentioned), which is then used to guide the training of the neural network. In the soft attention approach, the network jointly learns both the alignment and transduction, without decoupling those steps.\n\nIn any case, both models are trained with the exact same training data without additional information given to any side,  so we find the predicted alignment comparison fair.", "title": "Re. attention pre-training"}, "Sysdg2Mmx": {"type": "review", "replyto": "HkyYqU9lx", "review": "Did you try to pretrain the soft-attention model to predict the same fixed alignments that were used to train the hard attention? If not, then I am not sure that the comparison between the soft and hard attention was fair, especially when a very small dataset was used.The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it. \n", "title": "Attention pretraining", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "B12qxxMVl": {"type": "review", "replyto": "HkyYqU9lx", "review": "Did you try to pretrain the soft-attention model to predict the same fixed alignments that were used to train the hard attention? If not, then I am not sure that the comparison between the soft and hard attention was fair, especially when a very small dataset was used.The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it. \n", "title": "Attention pretraining", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1XLspbQe": {"type": "rebuttal", "replyto": "rkuiqIsMx", "comment": "Thanks for pointing it out! The latest revision of the paper uses the plain version for the subscripts as you proposed.", "title": "Fixed typos in 2.3"}, "BJmvcpWXx": {"type": "rebuttal", "replyto": "HyKae52fx", "comment": "Thanks for bringing our attention to those works! They are all indeed very relevant and are now addressed in the latest revision of the paper, in the introduction section and the related work section.  \n\nOur approach is strongly related to the first two works, but differs from them by allowing direct dependency between the encoder and decoder networks through the hard attention mechanism, while in those works the two RNNs are kept independent to allow tractable inference. We also compare our results to the reported results in Yu et. al. and show that our approach provides better overall accuracy over the Wiktionary inflection generation dataset.\n\nWe also added reference to the works by Chorowski and Bahdanau et. al. in https://arxiv.org/abs/1508.04395 and https://arxiv.org/abs/1506.07503, which adapted the soft attention model to perform more focused, local attention over the long input sequences in speech recognition. We suggest that for shorter input sequences, like the characters of a word in natural language, our hard attention model may be sufficient.\n \n\n  \n\n", "title": "Added missing prior work"}, "BJM5EaZXe": {"type": "rebuttal", "replyto": "H18-f8JXl", "comment": "Thanks for the questions! \n\nWe used a 5 model ensemble as it showed significant improvements in the soft attention model of Kann & Sch\u00fctze (https://www.aclweb.org/anthology/P/P16/P16-2090.pdf) and in the encoder-decoder model of Faruqui et. al. (https://arxiv.org/abs/1512.06110) which we used as baselines in our experiments.  Also clarified this in the latest revision of the paper.\n\nRegarding beam search, we did not use it in our implementation of the soft attention model. We left it this way as we found it performed better than the soft attention model of Kann & Sch\u00fctze, which is based on the Groundhog implementation which includes beam search - pointing on a small improvement potential from beam search in this case while making decoding significantly slower.\n", "title": "Clarification re. ensembling and beam search"}, "H18-f8JXl": {"type": "review", "replyto": "HkyYqU9lx", "review": "In your appendix, you write that you report all results with a 5 model ensemble? Was there a specific reason for doing this?\n\nAlso you write that you did not do beamsearch with your model because it was slower and did not improve performance which seems sensible. Did you also not do beamsearch with your soft-attention model? This paper proposes a sequence transduction model that first uses a traditional statistical alignment methods to provide alignments for an encoder-decoder type model. The paper provides experiments on a number of morphological inflection generation datasets. They shows an improvement over other models, although they have much smaller improvements over a soft attention model on some of their tasks. \n\nI found this paper to be well-written and to have very thorough experiments/analysis, but I have concerns that this work isn't particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input (the authors \"suggest\" that their approach is sufficient for shorter sequences, but don't compare against the approach of Chorowski et al. 2015 or Jailty el at 2016).\n\nIn summary, I found this paper to be well-executed/well-written, but it's novelty and scope too small. That said, I feel this work would make a very good short paper. ", "title": "Clarifications for choice to ensemble + not do beamsearch", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ryb4JjfEe": {"type": "review", "replyto": "HkyYqU9lx", "review": "In your appendix, you write that you report all results with a 5 model ensemble? Was there a specific reason for doing this?\n\nAlso you write that you did not do beamsearch with your model because it was slower and did not improve performance which seems sensible. Did you also not do beamsearch with your soft-attention model? This paper proposes a sequence transduction model that first uses a traditional statistical alignment methods to provide alignments for an encoder-decoder type model. The paper provides experiments on a number of morphological inflection generation datasets. They shows an improvement over other models, although they have much smaller improvements over a soft attention model on some of their tasks. \n\nI found this paper to be well-written and to have very thorough experiments/analysis, but I have concerns that this work isn't particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input (the authors \"suggest\" that their approach is sufficient for shorter sequences, but don't compare against the approach of Chorowski et al. 2015 or Jailty el at 2016).\n\nIn summary, I found this paper to be well-executed/well-written, but it's novelty and scope too small. That said, I feel this work would make a very good short paper. ", "title": "Clarifications for choice to ensemble + not do beamsearch", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyKae52fx": {"type": "review", "replyto": "HkyYqU9lx", "review": "The proposed alignment model is virtually the same as https://arxiv.org/abs/1211.3711, which additionally can be trained without prior alignments by using a forward-backward like algorithm to marginalize  over all alignments. The idea was later extended to support hard alignments in https://arxiv.org/abs/1609.08194. Please clarify the relation. \n\nVery similar models were alos explored for speech recognition, please see: https://arxiv.org/abs/1511.04868 https://arxiv.org/abs/1603.00223\n\nAlso, it is not true that the soft-alignment model of Bahadanu et al must attend at each step to all of input sequence. Please see https://arxiv.org/abs/1508.04395 and https://arxiv.org/abs/1506.07503 for the use of location-based priors in a soft-attention model and the use of a windowing that constrains the attention to the vicinity of the previous selection.The paper describes a recurrent transducer that uses hard monotonic alignments: at each step a discrete decision is taken either to emit the next symbol or to consume the next input token. \n\nThe model is moderately novel - similar architecture was proposed for speech recognition (https://arxiv.org/pdf/1608.01281v1.pdf). Soft monotonic alignemts are also enforced by A. Graves in https://arxiv.org/abs/1308.0850.\n\nThe difficult part in training the proposed model is backpropagation through the discrete decisions. Typically, reinforcement learning techniques are used. In this contribution, the authors side-step the issue by using a problem-dependent aligner to generate optimal decisions for which they train the model.\n\nThe results indicate that such specially supervised model is better than the generic soft-attention model that doesn't require any problem-dependent external supervision. However the authors did not attempt to work on regularizing the soft-attention model, which is not fair - the extra supervision by using the ground-truth alignment is a form of regularization and it could be used as e.g. an extra signal to the soft-attention model for a better comparison. That being said the authors reash state-of-the-art results against other domain specific methods.\n\nI believe the paper would more suit a NLP venue - it sound and properly written, but its applicability is limited to the considered NLP problem.\n\n", "title": "Missing prior work", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkVEm2z4g": {"type": "review", "replyto": "HkyYqU9lx", "review": "The proposed alignment model is virtually the same as https://arxiv.org/abs/1211.3711, which additionally can be trained without prior alignments by using a forward-backward like algorithm to marginalize  over all alignments. The idea was later extended to support hard alignments in https://arxiv.org/abs/1609.08194. Please clarify the relation. \n\nVery similar models were alos explored for speech recognition, please see: https://arxiv.org/abs/1511.04868 https://arxiv.org/abs/1603.00223\n\nAlso, it is not true that the soft-alignment model of Bahadanu et al must attend at each step to all of input sequence. Please see https://arxiv.org/abs/1508.04395 and https://arxiv.org/abs/1506.07503 for the use of location-based priors in a soft-attention model and the use of a windowing that constrains the attention to the vicinity of the previous selection.The paper describes a recurrent transducer that uses hard monotonic alignments: at each step a discrete decision is taken either to emit the next symbol or to consume the next input token. \n\nThe model is moderately novel - similar architecture was proposed for speech recognition (https://arxiv.org/pdf/1608.01281v1.pdf). Soft monotonic alignemts are also enforced by A. Graves in https://arxiv.org/abs/1308.0850.\n\nThe difficult part in training the proposed model is backpropagation through the discrete decisions. Typically, reinforcement learning techniques are used. In this contribution, the authors side-step the issue by using a problem-dependent aligner to generate optimal decisions for which they train the model.\n\nThe results indicate that such specially supervised model is better than the generic soft-attention model that doesn't require any problem-dependent external supervision. However the authors did not attempt to work on regularizing the soft-attention model, which is not fair - the extra supervision by using the ground-truth alignment is a form of regularization and it could be used as e.g. an extra signal to the soft-attention model for a better comparison. That being said the authors reash state-of-the-art results against other domain specific methods.\n\nI believe the paper would more suit a NLP venue - it sound and properly written, but its applicability is limited to the considered NLP problem.\n\n", "title": "Missing prior work", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkuiqIsMx": {"type": "rebuttal", "replyto": "HkyYqU9lx", "comment": "In the \"Encoder\" section, the authors appear to use plain math italic x_i for input elements (characters) and boldface x_i for biLSTM encodings.\n\nI believe the subscript in e_{x_i} should be the plain version since here x_i represents an input element.\nAlso, the definition of boldface x_i should take e_{x_i} and not boldface x_i as input; otherwise the definition is circular.\n\n(The numeric subscripts i should not be boldfaced either since they are also not vectors; but that typo is less confusing.)\n\nCorrect?\n", "title": "typos in section 2.3 \"Encoder\""}}}