{"paper": {"title": "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data", "authors": ["Nicolas Papernot", "Mart\u00edn Abadi", "\u00dalfar Erlingsson", "Ian Goodfellow", "Kunal Talwar"], "authorids": ["ngp5056@cse.psu.edu", "abadi@google.com", "ulfar@google.com", "ian@openai.com", "kunal@google.com"], "summary": "Semi-supervised learning of a privacy-preserving student model with GANs by knowledge transfer from an ensemble of teachers trained on partitions of private data.", "abstract": "Some machine learning applications involve training data that is sensitive, such\nas the medical histories of patients in a clinical trial. A model may\ninadvertently and implicitly store some of its training data; careful analysis\nof the model may therefore reveal sensitive information.\n\nTo address this problem, we demonstrate a generally applicable approach to\nproviding strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in\na black-box fashion, multiple models trained with disjoint datasets, such as\nrecords from different subsets of users. Because they rely directly on sensitive\ndata, these models are not published, but instead used as ''teachers'' for a ''student'' model. \nThe student learns to predict an output chosen by noisy voting\namong all of the teachers, and cannot directly access an individual teacher or\nthe underlying data or parameters. The student's privacy properties can be\nunderstood both intuitively (since no single teacher and thus no single dataset\ndictates the student's training) and formally, in terms of differential privacy.\n These properties hold even if an adversary can not only query the student but\nalso inspect its internal workings.\n\nCompared with previous work, the approach imposes only weak assumptions on how\nteachers are trained: it applies to any model, including non-convex models like\nDNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN\nthanks to an improved privacy analysis and semi-supervised learning.\n", "keywords": []}, "meta": {"decision": "Accept (Oral)", "comment": "The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written."}, "review": {"rJdDo_KLl": {"type": "rebuttal", "replyto": "rJ6AW94Ug", "comment": "Auxiliary input in definition 2 represents any input other than the database that goes into the mechanism. E.g. in our setting, the aux input may be a point that the student wants labeled by the teachers in a differentially private way.\n\nIn general, such inputs could be taken to be part of the definition of the mechanism M, but in cases when the algorithm is adaptive, a formal privacy analysis requires that it be part of the input. This is so for a technical reason: to analyze the total privacy cost, we need the mechanism definition to not depend on the database. In our setting, the student needs a new point to be labeled, and in general, this point may depend on the labels the student received on the previous points (e.g. if using low confidence points using the student in Section C.2). Since our moments analysis allows for an auxiliary input, we can handle this adaptivity easily.", "title": "response on \"auxiliary input\""}, "B1vg7xwLl": {"type": "rebuttal", "replyto": "ryhZ3-M4l", "comment": "Thank you for your encouraging review. In the revised version of our paper posted today, we added more context to the bound on the moment generating function provided in Theorem 1. Our analysis is rather conservative in that it pessimistically assumes that, even if just one example in the training set for one teacher changes, the classifier produced by that teacher may change arbitrarily. Therefore, our analysis does not require any assumptions about the workings of the teachers; we regard this point as an advantage of our approach. Nevertheless, we expect that stronger privacy guarantees may perhaps be established. We will add a discussion of this subject to the paper.\n", "title": "response to review 1"}, "ryHAfgw8l": {"type": "rebuttal", "replyto": "HJyf86bNx", "comment": "Thank you for your time and for your review. \n\nCurrent deep learning models do not come with any accuracy guarantees, unfortunately. In particular, for tasks such as MNIST and SVHN, none of the state-of-the-art methods, even those not concerned about privacy, provide accuracy guarantees. With our current level of theoretical understanding, performance in terms of accuracy has to be established empirically, as is current practice in ML research, and as we have done here.  \n\nIn order to further demonstrate the general applicability of our approach, we performed experiments on two additional datasets. The UCI Adult dataset is made up of census data, and the task is to predict when individuals make over $50k per year. The UCI Diabetes dataset includes de-identified records of diabetic patients and corresponding hospital outcomes, which we use to predict whether diabetic patients where readmitted less than 30 days after their hospital release. We looked at the UCI datasets because some of them appear in previous papers on privacy; among the many UCI datasets, in the limited time available, we quickly decided to study two that looked interesting. These UCI datasets seemed at least reasonable, and not random, as they had been used for similar ML applications, so they were the first (and only) datasets that we tried for our new experiments. They turned out to be well-suited to our techniques, even though we had no idea whether this was the case before running our experiments.  While our experiments on MNIST and SVHN used convolutional neural networks and generative adversarial networks, we instead used random forests to train our teacher and student models for both of the UCI datasets. Our new results on these datasets show that despite the differing data types and architectures, we are able to provide meaningful privacy guarantees---with epsilon bounds below 2.7---at modest costs in accuracy---with reductions of at most 2% between a non-private model and our privacy-preserving student. We added them to the revised draft of our paper posted today.\n\nIn a real deployment, noise needs to be added to the differential privacy guarantees themselves before releasing them. In order to allow for a meaningful comparison with related work, we provided these additional values in our updated draft. We note that doing so does not impact our results\u2019 competitiveness with prior work.\n\nWe added a discussion of prior work by Jagannathan et al. in the revision of our paper. A key difference is that their approach is tailored to decision trees. Our approach is applicable to any machine learning algorithm, including more complex ones. In the specific case of decision trees, our approach can also work well, as demonstrated by our new, added results for the Adult and Diabetes datasets performed with random forests. Another key difference is that their approach modifies the classic model of a decision tree to include the Laplace mechanism. Thus, the privacy guarantee does not come from the disjoint sets of training data analyzed by different decision trees in the random forest, but rather from the modified architecture. In contrast, partitioning is essential for our approach's privacy guarantees. We know of no other paper that is similarly comparable to our work, and could not find any via a reverse citation search based on Jagannathan et al.\n\nRegarding the comparison of Appendix C with GANs, we made it clearer that our goal was to reduce the number of labels required from the teacher ensemble. This makes the comparison more straightforward. \n\nWe thank you for all remaining comments not discussed in this response: we agree with all of them and modified the draft accordingly.", "title": "response to review 2"}, "ByX2zew8g": {"type": "rebuttal", "replyto": "HJNWD6Z4l", "comment": "Thank you for your time and for your review. \n\nAs in other work on differential privacy, we provide probabilistic guarantees, and these are parameterized to make trade-offs explicit. Our method provides a lower probability of privacy failure than other methods at comparable utility. (You may find a detailed explanation of the different parameters of our analysis in our answer dated from November 22nd on OpenReview.)\n\nAn important advantage of MNIST and SVHN, at this stage of development of the field, is that they enable rapid experimentation and benchmarking---no dataset that contains sensitive data or that imitates sensitive data seems to be a standard \"fruit fly\" to the same degree. However, to further demonstrate the applicability of our approach to other natural data types, like the ones encountered in medical applications, we have followed your excellent suggestion of looking at specific applications using medical data. For this purpose we have performed additional experiments on the UCI Diabetes dataset, and have added the results to the new, revised version of our submission. The results show that our approach can apply well to medical data, and provide strong, meaningful privacy guarantees without reducing the accuracy of key medical-domain tasks, such as diagnostic classification and relapse prediction [https://arxiv.org/pdf/1602.04257.pdf]. (You may find additional details in our response to another review.)\n", "title": "response to review 3"}, "Bys_WirLx": {"type": "rebuttal", "replyto": "r15xhezLx", "comment": "1. The first three questions in your comment refer to the attack model that we protect against. This is an important and subtle question. Differential privacy is the strongest existing definition of privacy for which we know of any methods of getting utility. The precise semantics say that no matter what the adversary knows (e.g., all of the training data except one example), she does not learn anything about any specific example that she could not have learnt if we omitted this example from the training data. This has strong implications. In particular, it protects against the second attacker model in your first question: the attacker cannot test whether a particular sample is in the training data. Moreover, even if the attacker has partial training data, they cannot reconstruct the rest. The precise implications of differential privacy are well-understood and exhaustively discussed in the privacy community. A full discussion of this is beyond the scope of this work and we refer interested readers to https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf .\n\n2. Differential privacy protects against either possible attacker in your second question.\n\n3. We are not currently aware of an attack method that reliably recovers training data (exact or approximate) for all models and datasets. We cite some work that does recovery in some cases; we believe that it is possible to do more in this direction, and it does not seem prudent to assume that we know of all future attack ideas. As far as defenses against such (approximate or exact extraction) attacks are concerned, techniques that guarantee differential privacy---such as ours---are in our opinion important because they thwart such attacks in the future. However, as regards the second part of your question, it is important to note that machine learning models will, by their nature, reveal information about population statistics. Unavoidably, this may give attackers some information about the training data; for instance, this will be the case if the attackers previously had no information about the statistics of data such as the training data. But, intuitively, revealing such aggregate statistics need not compromise privacy, and the training data will still benefit from the guarantees of differential privacy. \n\n4. We do not understand this question. (Please feel free to try again.)\n\n5. The construction of an adversarial example preserves most of the information about the original example, so training only on adversarial examples does not guarantee privacy. For example, suppose the data is stored with 8 bit pixels, and fed to the model using 32 bit pixels. A common practice is to create adversarial examples that modify only the smallest 24 bits. The model thus sees the original version of the largest 8 bits. Even for adversarial examples with very large perturbations, the model will generally see the largest 5 bits of the true value for every pixel. Even the bits that are modified may leak some information if the adversarial example construction algorithm has some statistical regularities. See the other ICLR submission \u201cAdversarial machine learning at scale\u201d by Kurakin et al: adversarial examples can leak information about the true label, so presumably they can also leak information about the original value of the input pixels.\n\n6. The privacy preservation of an algorithm cannot be measured empirically. It has to be established by a theoretical guarantee. Because there is no theoretical argument that adversarial training preserves privacy, we cannot compare the amount of privacy given by adversarial training and the amount of privacy given by our method.\n", "title": "reply"}, "rJ6AW94Ug": {"type": "rebuttal", "replyto": "HkwoSDPgg", "comment": "I think this paper has great impact.\n\nMy question is what is the \"auxiliary input\" in Definition 2.\n\nCould you explain this term in theoretical view and what is that in your paper?", "title": "Simple question related with privacy loss"}, "Hyc2E5Q8x": {"type": "rebuttal", "replyto": "HkwoSDPgg", "comment": "Thank you for providing an interesting paper.\n\nIn the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016).\n\nAs far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning.\n\nSo, my question is \"Where is the generator ?\".\n\nThe aggregation of teacher network is treated as the generator in GAN framework? ", "title": "Question for student GAN training"}, "SktdDgVIx": {"type": "rebuttal", "replyto": "Hyc2E5Q8x", "comment": "The generator network is a separately parameterized model. It is created and trained at the same time as the student network.\n\nThe teachers can't be the generator. The teachers take an image as input and produce a label as output. The generator needs to take noise as input and produce an image as output.", "title": "reply"}, "r15xhezLx": {"type": "rebuttal", "replyto": "HkwoSDPgg", "comment": "Hi,\n\nI have few questions about the paper.\n\n1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? \n\n2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK?\n\n3- Talking about neural networks:\n- Do you think there is any attack method to recover an exact training data from the learning model?\n- Do you think there is any defense method to prevent an attacker from recovering even an approximate training data?\n\n4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism?\n\n5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. \n- How can the model \"memorize\" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs?\n\n6- How do you compare the performance of your method with adversarial training?\n\nThanks.", "title": "Attacker's Model and Goal?"}, "rklFyHJVx": {"type": "rebuttal", "replyto": "BkyQluAmx", "comment": "Thank you for your question.\n\nIn our approach, the sensitive and nonsensitive datasets have the same set of input features and output classes. Therefore, the knowledge learned by teachers can be transferred to the student by having the teachers provide labels for inputs on which the student is trained.\n\nIn some applications, it might be hard to find non-sensitive data that is drawn from the same distribution as the sensitive data. For the current work, we used the same source of data for both settings. In an application where this is not possible, the models would need to be trained with an algorithm that is capable of domain adaptation.", "title": "re: input of student model"}, "BkyQluAmx": {"type": "rebuttal", "replyto": "HkwoSDPgg", "comment": "Thanks for interesting and well-organized papers. I have a question about teacher-student model. \n\nTeachers are trained on sensitive data, and students are trained on non-sensitive data.\nI wonder how students work on the outputs of teachers.\nSensitive and non-sensitive are different attributes, so I think there are no correlation between teachers and students. \n\nPlease give me some more details. Thanks.  \n", "title": "Input of student model"}, "BkOE60Wmg": {"type": "rebuttal", "replyto": "Hk0_ioy7l", "comment": "In our view, the most important high-level challenge is to develop ML models that learn while protecting the privacy of training data, without sacrificing accuracy, performance, or software-engineering simplicity, and with a satisfactory definition of \"privacy\". This is particularly important because deep learning often relies on vast quantities of data for training, and this data is often sensitive. It is particularly difficult for deep learning because we cannot make some convenient simplifying assumptions (e.g., convexity). This is part of why in previous works, the accuracy losses to guarantee privacy were significant. \nIn addition, deep learning is a \"moving target\"---the field is so active that we cannot make very strong bets on which approaches will be most successful in the long run and deserve the biggest attention. At a lower level, these difficulties result in technical problems, such as the need for accurate tracking of privacy losses (which our moments account addresses) and of algorithmic and architectural ideas (of which our paper is an example), sometimes specialized to particular learning tasks or models.\n", "title": "Challenges in privacy-preserving Deep Learning"}, "SylS2iyml": {"type": "rebuttal", "replyto": "BJyXHYkml", "comment": "Thank you for the insightful question. The short answer is that it's\nboth empirical and theoretical work. Here's the long answer:\n\nSuppose we have input images x and class labels y. Purely supervised\nlearning consists of learning p(y|x). A common strategy for\nsemi-supervised learning is to learn a generative model p(x,y). The\nconditional distribution p(y|x) in this generative model often solves\nthe semi-supervised learning task with less overfitting than would\noccur with direct supervised learning. This common strategy does not\nyet have a clear theoretical justification. The limitations of\ntheoretical understanding of this approach are described in the Deep\nLearning textbook, section 15.1.1, \"When and why does unsupervised\npretraining work?\":\nhttp://www.deeplearningbook.org/contents/representation.html\n\nOur work has exposed something unusual: our semi-supervised student\nnetwork learned a p(y|x) that obtained lower test error than the\nteacher ensemble's p(y|x).\n\nOur usual understanding of generative semi-supervised learning would\npredict that the teacher network would obtain some test error rate\nE_teacher. A model trained using purely supervised learning to mimic\nthe teacher would overfit somewhat, and obtain a test error rate\nE_supervised, with E_supervised > E_teacher. Finally, generative\nsemi-supervised learning would obtain less overfitting, and would\nobtain some test error rate E_generative, with E_teacher <\nE_generative < E_supervised.\n\nInstead, to our surprise, we found that E_generative < E_teacher in many cases.\n\nThis means that the unsupervised component of the learning algorithm is\nnot merely reducing overfitting. It actually causes the student\nnetwork to become *better than the labels it was trained on*. This\nresult goes beyond the predictions made by our usual understanding of\ngenerative semi-supervised learning.\n\nWe suspect that this means that semi-supervised learning via\ngenerative modeling is not fully generic. Instead, generative modeling\nbiases the classifier toward recognizing categories that are highly\nsalient in input space. For tasks like MNIST and SVHN, the inputs are\nimages of digits, which are written symbols. Humans designed written\nsymbols to be very visually salient and distinct from each other, so\nthat the most obvious feature of the input is the identity of the\nsymbol. This suggests that optical character recognition is a task\nthat is especially likely to benefit from regularization by generative\nmodeling. A model that uses the same input to predict a different\noutput (e.g., author identity, rather than digit identity) may not\nbenefit nearly as much from generative training. Likewise, generative\ntraining might be less effective on a different input domain, where\nthe input features have not been intentionally designed to make the\nmost important aspect of the data be highly salient. For example,\ngenerative training might not be as useful for analyzing gene\nexpression data.\n\nA full resolution of this anomaly would require:\n- Empirical work testing the effectiveness of generative\nsemi-supervised learning in different settings (e.g., predicting\nauthor identities of MNIST inputs, classifying gene expression data)\n- Theoretical work to explain more concretely how generative\nsemi-supervised learning is able to reduce the test error\n\nWhile such additional empirical and theoretical work would be\ninteresting, such an in-depth investigation of the underpinnings of\ngenerative semi-supervised learning would be beyond the scope of our\ncurrent paper.", "title": "The challenge is to further our understanding of how generative semi-supervised learning works"}, "Hk0_ioy7l": {"type": "review", "replyto": "HkwoSDPgg", "review": "Nice, well-written, and interesting paper. Very general question: what are the most important challenges in the context of privacy preserving specifically in deep learning that you perceive? This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.", "title": "Clarifications", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryhZ3-M4l": {"type": "review", "replyto": "HkwoSDPgg", "review": "Nice, well-written, and interesting paper. Very general question: what are the most important challenges in the context of privacy preserving specifically in deep learning that you perceive? This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.", "title": "Clarifications", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJyXHYkml": {"type": "review", "replyto": "HkwoSDPgg", "review": "You mentioned future work on tasks in which \"the discrete output categories are not as distinctly defined by the salient input space features.\"\n\nIs this primarily a matter of empirical experimentation, or are there also some technical/theoretical challenges that would need to be addressed? If so, please elaborate.Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.", "title": "What are the challenges involved in your proposed future work?", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJyf86bNx": {"type": "review", "replyto": "HkwoSDPgg", "review": "You mentioned future work on tasks in which \"the discrete output categories are not as distinctly defined by the salient input space features.\"\n\nIs this primarily a matter of empirical experimentation, or are there also some technical/theoretical challenges that would need to be addressed? If so, please elaborate.Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.", "title": "What are the challenges involved in your proposed future work?", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B14FEx-zx": {"type": "rebuttal", "replyto": "B11Bz9hbl", "comment": "[Note: This response was crafted with the original version of the pre-review question in mind. We later noticed that the question had been modified and extended our reply in the comment below to take into account the modifications made to the pre-review question.]\n\n1) The Fredrikson et al. 2015 paper on model inversion (the one that we reference) addresses a very different question than ours, so we cannot provide an apples-to-apples comparison. Indeed, it is difficult to see how differential privacy can apply to the problem domain of their work (identifying individuals' faces). This difficulty is highlighted by the problem's incompatibility with our learning approach. Teachers trained on disjoint sets of faces could never agree, or generalize, in a manner that would allow the student to learn.\n\nPlease let us know if we misunderstood your question on Fredrikson et al.\u2019s paper, though. In particular, perhaps your question was referring to a different 2015 paper by Wu, Fredrikson, et al. https://arxiv.org/abs/1512.06388 ? This work applies output perturbation and considers strongly convex models, for which the sensitivity is small and one can therefore hope to get good differential privacy guarantees. In a non-convex setting such as ours, the sensitivity of the optimization function is very large, and adding noise proportional to it would yield a model whose accuracy is no better than a random predictor.\n\nCompared to Hamm et al.\u2019s work, our most significant contribution is that we broaden the scope within which the method may be applied, in part because our privacy analysis, based on the moments accountant, makes fewer assumptions and can provide privacy guarantees for non-convex models such as deep neural networks. We do not have a direct comparison for the performance of our method and that of Hamm et al. because they focus on logistic regression while we consider more sophisticated models, with different datasets. We believe that the moments account would also lead to improved results on logistic regression, however. \n\n2) In our setting, with distillation, the student would learn only from data that is labeled by the teachers (making no use of purely unlabeled data). In contrast, with semi-supervised learning techniques, the student can learn both from labeled data and also from completely unlabeled data\u2014which may be more abundant, and more easily accessible.  \n\nOne way to do semi-supervised learning is using GANs, as in our work. As indicated in Appendix C.1, we have observed that distillation does not perform sufficiently well when training with very few queries\u2014as is required to ensure tight privacy bounds, particularly when each answer includes the entire probability vector output by the ensemble of teachers. Thus, we believe that, at equal levels of privacy, semi-supervised learning with GANs will provide higher utility than distillation. We hope that the further details in the appendix provide a satisfactory explanation; please let us know if you have further questions, or if you would recommend that we move some of those details into the body of the paper.", "title": "Response to question by AnonReviewer3 "}, "SJq7WmMze": {"type": "rebuttal", "replyto": "B14FEx-zx", "comment": "The newer comments from the reviewer relate to the parameters of our work, and their evaluation.\n\nOur analysis follows the established framework of differential privacy. It defines a probabilistic bound, with parameters (epsilon and delta) on changes in the output of our algorithm when it is ran on differing inputs. Smaller values of the parameters yield stronger guarantees. As is often the case with security, achieving 100% privacy (represented by epsilon = delta = 0) is unfortunately impossible while releasing a useful model. However, we follow standard practices and guidelines, setting epsilon in the single-digit range (as is often done, for instance, by Google's Rappor system and in Apple's use of differential privacy) and setting delta so that its order of magnitude is inverse to the number of training samples. The other \"empirical\" parameters of our method (e.g., number of teachers and level of noise) do not have to be considered separately: they can be accurately judged by their impact on privacy (as represented by epsilon and delta), and on utility (the accuracy on test data). We believe that our results provide a validation for these \"empirical\" parameters, in the sense that we can take epsilon and delta as explained above while maintaining competitive accuracy. In other words, to evaluate how \u201csecure the method is,\u201d using the standard metrics of the security and privacy community, it suffices to look at the privacy parameters epsilon and delta; it is not necessary to consider details of the algorithm, the internal parameters, or the experimental setup.\n\nWe would welcome input on whether adding an explanation along these lines to the paper would be helpful. Also, if we are speaking to a different notion of security than what the reviewer had in mind, we would be happy to continue this discussion.", "title": "Response to the updated question by AnonReviewer3"}, "B11Bz9hbl": {"type": "review", "replyto": "HkwoSDPgg", "review": "Thanks for the nice paper with inspiring intuitions. The theory analysis is cool, however, the probabilistic bound has quite a number of  empirical parameters, which makes it difficult to understand whether the absolute 100% security is  guaranteed. Thus we have to check the experiments to understand how secure the method is, unfortunately it is not straightforward to evaluate either. \n\nMore specifically, I have two questions related to the experiments.\n\n1) It is not clear from the experiments that how much better the proposed method is compared with the previous works, e.g., [Fredikson et al, 2015] and [Hamm et al, 2016].\n\n2)  I am quite interested in the comparison between GANs vs distillation for semisupervised learning. From the paper my impression is that GAN-based approach is more secure while distillation-based approach may have higher performance. If it is correct, please give more details and analyze why. \nThis paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. \n\nThe theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.\n\nThe experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. ", "title": "theory and experiment", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJNWD6Z4l": {"type": "review", "replyto": "HkwoSDPgg", "review": "Thanks for the nice paper with inspiring intuitions. The theory analysis is cool, however, the probabilistic bound has quite a number of  empirical parameters, which makes it difficult to understand whether the absolute 100% security is  guaranteed. Thus we have to check the experiments to understand how secure the method is, unfortunately it is not straightforward to evaluate either. \n\nMore specifically, I have two questions related to the experiments.\n\n1) It is not clear from the experiments that how much better the proposed method is compared with the previous works, e.g., [Fredikson et al, 2015] and [Hamm et al, 2016].\n\n2)  I am quite interested in the comparison between GANs vs distillation for semisupervised learning. From the paper my impression is that GAN-based approach is more secure while distillation-based approach may have higher performance. If it is correct, please give more details and analyze why. \nThis paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. \n\nThe theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.\n\nThe experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. ", "title": "theory and experiment", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}