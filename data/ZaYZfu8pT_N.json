{"paper": {"title": "Non-iterative Parallel Text Generation via Glancing Transformer", "authors": ["Lihua Qian", "Hao Zhou", "Yu Bao", "Mingxuan Wang", "Lin Qiu", "Weinan Zhang", "Yong Yu", "Lei Li"], "authorids": ["~Lihua_Qian1", "zhouhao.nlp@bytedance.com", "~Yu_Bao1", "~Mingxuan_Wang1", "lqiu@apex.sjtu.edu.cn", "~Weinan_Zhang1", "~Yong_Yu1", "~Lei_Li11"], "summary": "", "abstract": "Although non-autoregressive models with one-iteration generation achieve remarkable inference speed-up, they still fall behind their autoregressive counterparts in prediction accuracy. The non-autoregressive models with the best accuracy currently rely on multiple decoding iterations, which largely sacrifice the inference speed of non-autoregressive models.  Inspired by the way of learning word dependencies in autoregressive and iterative-decoding models, we propose Glancing Transformer (GLAT) with a glancing language model (GLM), which learns to capture the word dependency gradually. Experiments on three benchmarks demonstrate that our approach can significantly improve the accuracy of non-autoregressive models without multiple decoding iterations. In particular, GLAT achieves state-of-the-art results among non-iterative models and even outperforms top iterative counterparts in some specific benchmarks.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This work raised quite a few questions, and left the reviewers somewhat divided. The authors have done their best to answer these questions, conducting additional experiments where needed.\n\nThe close relation of this work to Mask-Predict (Ghazvininejad et al. 2019) was noted by several reviewers. Although the current version of the manuscript addresses this, the introduction still frames Mask-Predict as an iterative model, and does not explicitly make the connection between GLAT and single-iteration Mask-Predict. My impression is that this understates the relationship between these models somewhat.\n\nTaking single-iteration Mask-Predict as a baseline, the proposed extension is fairly simple, and seemingly effective, which is a potentially impactful combination. However, the manuscript is still held back by presentation issues (including but not limited to spelling and choice of words), and I concur with Reviewer 2 that the connection with curriculum learning should be elucidated not just in words, but with supporting experimental analysis.\n\nRegarding training cost, given that training for GLAT seems to be more costly for the same number of training iterations, a comparison where the total compute budget is held constant could be interesting -- though I appreciate that this is not a key point of the paper, as the authors point out (whereas inference cost is).\n\nI believe the changes made by the authors in response to the reviewers' comments are substantial enough that they merit a further review cycle, and may still fall short of the reviewers' expectations in some aspects. Therefore, I will not recommend acceptance, though I want to add that this was a tough call to make. I would also like to encourage the authors to resubmit their updated manuscript."}, "review": {"NRuEyxn8p5": {"type": "review", "replyto": "ZaYZfu8pT_N", "review": "This paper proposes a non-autoregressive neural machine translation model that does not require multiple iterations to achieve a good translation quality. The key difference to previous models is that during training it uses decoding to estimate the number of words to randomly sample (proportionally to the error) as opposed to random sampling a fixed number of them and uses this sampling strategy to mask tokens. \n\n**Strengths** \n\nThe idea of using the model to come up with the number of words to sample before predicting them looks new and it appears to be helping a lot the model to do better with a single iteration. \n\nThe evaluation shows that the proposed model outperforms previous non-iterative ones and performs on par with iterative ones in some cases while being significantly faster which is promising (but it still relies on reranking based on an autoregressive model). \n\nMoreover, the method performs much better than the non-autoregressive baseline on longer sequences (a trait that is observed in autoregressive models) and performs better than the autoregressive baseline on very short sequences (up to 20 tokens). \n\n**Weaknesses** \n\n(1) The writing requires some more effort because it has some grammatical/spelling errors and was not clear in several parts. It would help simplify wording and make more clear statements. For instance, there is a lot of time spent describing the glancing sampling strategy while it could be described in one paragraph (random sampling strategy paragraph seems redundant). \n\n(2) The proposed idea has some merits and works well judging from the results but it seems somewhat incremental and not very clearly explained (see below). The connection to curriculum learning was a bit hand-wavy and hard to follow. The model does not seem to be trained on targets that are of increasing difficulty but rather the number of incorrect targets simply defines the number of *random* samples to be used for training. Was this the intended connection to this line of work? Having many errors in the prediction during training does not necessarily mean that the example is difficult. \n\n(3) In Section 3.3, how is the function f_{ratio} actually implemented and trained (if applicable)? It's not clear from the provided description of how this is done. How are the gradients computed through the sampling process if it's trainable? This is an essential component of the model and it hasn't been explained well. \n\n(4) Another setback for the reader is the lack of discussion or acknowledgment of the computational cost that is required by the method during training. Performing decoding twice during training looks interesting from the modeling perspective but what is its effect on training speed? The evaluation focus is mainly on inference time but the training speed factor should also play a role too when deciding which method to use.\n\n(5) Related to the above, the non-autoregressive models rely on knowledge distillation and re-ranking based on a pretrained autoregressive model. This already has its own training cost, so increasing it more could lead to a situation where the benefits during inference are overshadowed by the computational cost from training.\n\n\nOther comments:\n\nAre the models trained until convergence or for a fixed number of steps?  I am wondering what is the impact of the glancing sampling strategy on the convergence. \n\nHow did the authors come up with the number of reranking candidates for each model? It looks like the number is different for each model; this should affect the quality of the model.  \n\nCould the authors elaborate on what do they mean by \"the formulation of our proposed glancing language model *could* be maximizing the likelihood of remaining words...\"? Does it actually do that? This was unclear and it leads the reader to make guesses. \n\nIn Section 4.2, what do you mean by \"BLEU and speed-up are more or less contradictory\"?\n\nIn Figure 1 and 2, the replaced inputs use the notation h_2, h_3 which points to the encoded inputs but in the textual description the replaced tokens seemed to be coming from the embedding on the decoder side (Section 3.2, paragraph 2): E_{y_t \\in GS(Y,\\hat{Y} }(y_t). There seems to be this inconsistency between the notation in the diagrams and the notation in text, which is very confusing. \n ", "title": "Promising results, unsatisfactory delivery", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "EvCks-wCMTS": {"type": "rebuttal", "replyto": "ZaYZfu8pT_N", "comment": "We thank all the reviewers for their valuable comments and suggestions. \n\nWe have individually responded to issues mentioned by reviewers and uploaded a revised draft with some major modifications as follows:  \n1. Fix grammatical errors and polish the paper for better delivery\n2. Add experiments for ablation study (Section 4.4)\n > a. To explore more strategies for selecting reference words to replace the decoder inputs, we add four other reference word selection strategies in addition to the default random strategy for comparison  \n > b. For exploring more metrics to compare the predictions and references, we add results of GLAT with Levenshtein distance along with the results of the Hamming distance  \n > c. Add experiments for comparing GLAT and Mask-Predict: \n    >>1. replace the decoder inputs of GLAT with [MASK] tokens\n    >>2. replace the sampling strategy of GLAT with the uniform sampling strategy of Mask-Predict \n3. Add discussions for  the training speed of GLAT (Section 4.3)\n4. Provide more insights for the designing of GLM\n\nWe highly appreciate all reviewers for your efforts and time spent reviewing our paper and the constructive, critical, and meaningful suggestions for our method. Revisions are made in our submission to clarify our approach further and discuss these thoughtful suggestions. \nWe hope the above reply could address your concerns. Welcome to any further comments/questions!", "title": "Paper Updates"}, "s63qxCdmCyA": {"type": "rebuttal", "replyto": "_N8E3DFF4SK", "comment": "Thanks for your constructive suggestions.   \nTo address your concern that *\"I believe that more exploration of strategies to sample number of masked tokens, and sample words from the reference sentence is necessary to make the paper publishable.\"*, we added several results to compare more strategies of sampling number and more strategies of selecting/sampling words from the reference sentence: a) besides the default random reference selection strategy, we add four other strategies that leverage the prediction of first decoding; b) besides the Hamming distance, we include the Levenshtein distance in GLAT for comparison. And we also added ablation studies for comparisons between GLAT and Mask-Predict.\n\n**Question1: Novelty compared with mask-predict**  \n\nThanks for your kind notes. Intuitively, GLAT is quite different from Mask-Predict in following aspects:\n1) GLAT is a non-iterative generation model while Mask-Predict relies on multiple decoding iterations.\n2) The difference between GLAT and Mask-Predict seems to mainly lie in the sampling strategy, but the insight behind is quite different. The initial motivation of GLAT is to boost the performance of one-iteration generation. We find that explicit word dependency modeling is crucial for NAT, while the language model for one-iteration generation has not been well designed. Besides, we also find the training of NAT is very steep, and we finally turn to the idea of curriculum learning by first learning the generation of fragments and gradually moving to whole sequences. However, intuitions of Mask-Predict come from different ways, they want to train an iterative editing model. They find mask language model is suitable for leveraging partially generated outputs, and finally apply MLM in the iterative editing setting, which gives very promising results.\n3) Although the adaptive glancing sampling is very simple, it is very effective, which leads to significant improvements compared to uniform sampling (about 6 BLEU on WMT EN-DE & DE-EN).\nWe hope the above reply could address your concerns about the novelty of GLAT, and any further comments/questions are welcome!\n\n**Question2: Experiments with more sampling strategies. E.g. One possible strategy to exploit this could be selecting tokens that the model was not able to predict correctly based on the hamming distance comparison.**\n\nThank you for the constructive advice. Besides the default random sampling strategy for reference word selection, we add two strategies related to the prediction probability of the words in reference: $p_\\text{ref}$ and $1-p_\\text{ref}$. Note that the two sampling approaches are used after the adaptive sampling number N is obtained.   \nFor $p_\\text{ref}$, the sampling probability of each reference word is proportional to the word output probability $p_\\text{ref}$ on the corresponding position. For $1-p_\\text{ref}$, the sampling probability is proportional to $1-p_\\text{ref}$.    \nSimilar to the word selection strategy for masking words during inference in Mask-Predict, we also add two strategies related to the prediction confidence: \"most certain\", \"most uncertain\". For \"most certain\", we sample words at these positions with higher prediction confidence. For \"most uncertain\", we choose those with the lower prediction confidence.   \nThe results are list as follows:\n\n|\t\t|\t|random\t||\t$p_\\text{ref}$\t||\t$1-p_\\text{ref}$||\tmost certain\t||\tmost uncertain|\n|-----------|---|:-----------:|---------------|:----------------:||:---------:||:------------:||:-----------------:|\n|GLAT\t|\t|25.21\t||\t24.87\t||\t25.37\t\t|| 24.99\t\t||\t    24.86\t|\n|GLAT(with reranking)|\t|26.55\t||\t25.83||\t\t26.52\t||\t26.22\t||\t26.13|\t\n|\n\nThe results show that GLAT can significantly improve the performance with all these strategies for reference word selection.  \nIn comparisons, the model with the selection strategy $1-p_\\text{ref}$ outperforms the one with $p_\\text{ref}$, indicating that words hard to predict are more important for glancing in the training process.   \nAnd we find that the random strategy performs a little better than the two confidence-based strategies. We think this indicates that introducing more randomness in the glancing sampling could enable GLAT to learn to explore more dependencies among target words. We adopt the random strategy for its simplicity and good performance.\n", "title": "We have compared more strategies for the sampling number of reference words, and the selection of words sampled from references"}, "6tR3AMfh-af": {"type": "rebuttal", "replyto": "s63qxCdmCyA", "comment": "**Question3: Experiments with more distances for comparing the model's prediction and reference sentence**\n\nThanks, we add experiments of GLAT with Levenshtein distance and list the results of both Hamming distance and Levenshtein distance as follow:\n\n|\t\t\t|\t|WMT14 EN-DE|\t\t\t|\t|\tWMT14 DE-EN|\t\t\t\t|\n|---------------|-----|:--------------|:-------------------|---|:---------------------|:----------------|\n|\t\t\t|\t|Hamming\t|\tLevenshtein\t|\t|\tHamming\t|  Levenshtein|\n|GLAT\t\t|\t|25.21\t\t|\t24.56\t\t|\t|\t29.84\t\t|\t28.96\t|\n|GLAT(with reranking)|\t|26.55\t|\t\t26.21\t|\t|\t\t31.02\t|\t\t30.85|\n|\n\nBoth distances can be used in GLAT for obtaining improvements by one-iteration generation and we find that the GLAT with Hamming distance performs better than the Levenshtein counterpart. Intuitively, the Hamming distance is a more strict metric compared to the Levenshtein distance. Specifically, only correctly predicting the same words at the corresponding position of the reference is regarded as matching with the reference (the Levenshtein distance does not punish position errors a lot).  Note that such a strict distance metric is more consistent with the MLE objective used in the training of GLAT.\n\n**Question4:  I think the authors should include experiments to compare use of [MASK] tokens and their approach empirically.**\n\nThanks for your suggestion. We did a quick run and compare our proposed GLAT with Mask-Predict by:\n1. We replace the glancing sampling with the uniform random sampling used by mask-predict\n2. We use the [MASK] token as our decoder inputs but keep all other aspects of training the same\n\n|\t\t\t\t\t\t\t\t\t\t|   |WMT14 EN-DE\t|\tWMT14 DE-EN |\n|---------------------------------------------------|---|:----------------------:|:---------------------:|\n|GLAT w/ sampling strategy of Mask-Predict\t|\t|\t19.16\t\t|\t\t23.56\t|\n|GLAT w/ decoder inputs of Mask-Predict \t|\t|\t24.99\t\t|\t\t29.48\t|\n|GLAT\t\t\t\t\t\t\t\t\t|\t|\t25.21\t\t|\t\t29.84\t|\n|\n\nThe results show:\n1. the proposed glancing  sampling is crucial, without which the results will drop significantly (5 ~ 6 BLEU scores)  \n2. Replacing the decoder inputs with the MASK token will also lead to a small drop (0.2 ~ 0.3 BLEU). Note that although the performance drop is not significant, feeding representations from the encoder as the decoder input could still moderately improve the strong baseline after adopting glancing sampling (23.56 ->29.48 -> 29.84 on DE-EN)\n\nTo sum up, the adaptive glancing sampling approach contributes the most to the final improvement and the use of the representations from the encoder also helps a bit.\n\n**Question5: How is attentional copy implemented**\n\nWe use the positional embedding of the decoder as the query in the attention mechanism and the source representation as the key and the value. With the positional embedding and the source representation, we can compute a distribution over the source representation and the result of attention is the weighted sum of source representation with the distribution.\n\n**Question6: Grammatical errors**\n\nThanks, we fixed grammatical errors and polished the paper in the revised submission.\n", "title": "We have compared more strategies for the sampling number of reference words, and the selection of words sampled from references (2)"}, "Fkus6oZuEHi": {"type": "rebuttal", "replyto": "rpagn3N4s9B", "comment": "**Question8: Could the authors elaborate on what do they mean by \"the formulation of our proposed glancing language model could be maximizing the likelihood of remaining words...\"? Does it actually do that?**\n\nSorry for the confusion, we should replace \"could be\" with \"is\" here. We have fixed it in the paper. Thanks for your kind reminder.\n\n**Question9: In Figure 1 and 2, the replaced inputs use the notation h_2, h_4 which points to the encoded inputs but in the textual description the replaced tokens seemed to be coming from the embedding on the decoder side (Section 3.2, paragraph 2): E_{y_t \\in GS(Y,\\hat{Y} }(y_t).**\n\nThe replaced inputs in GLAT are word embeddings of sampled words by glancing sampling, here \"replace\" refers to replacing the original encoded inputs with such word embeddings at responding positions. Specifically, the notation h_2, h_4 represent copied representations from source rather than the replaced tokens, and the replaced tokens from the target embeddings are represented by y_1, y_3 and y_5.  \nWe think the misunderstandings here may be caused by the symbol colors used in Figure 1 and Figure 2, we have modified the two figures to avoid misunderstanding. ", "title": "We have improved the delivery and given more discussions as requested (2)"}, "rpagn3N4s9B": {"type": "rebuttal", "replyto": "NRuEyxn8p5", "comment": "Thanks very much for your comments. We will first clarify the connection between our method and curriculum learning, then give discussions about the training speed, and finally answer questions about the detailed implementation of GLAT.\n\n**Question1: Having many errors in the prediction during training does not necessarily mean that the example is difficult, the connection with curriculum learning**\n\nActually, the \"difficulty\" mentioned in our paper means hard to fit in training, and such \"difficulty\" can be in proportion to the prediction errors in the first decoding pass of training.   \nThe intuitive connection between GLAT and curriculum learning is that it is very hard to learn \"difficult\" data points in the initial phase of training while we can make the model learn these \"less difficult\" data points. Along with the training process, the model gets better tuned and then turns to learn these \"difficult\" data points. Such an idea of curriculum learning could guide the model to learn better parameters in training.   \nSpecifically, to reduce the difficulty of learning,  GLAT samples more target words as decoder inputs and the model learns to fit the remaining fragments when the model is not well tuned. As the model is trained better along the training process, the model glances fewer target words and gradually learns to fit the whole sequence. We have made it clearer in our revised submission (See Section 1). \n\n**Question2: Performing decoding twice during training looks interesting from the modeling perspective but what is its effect on training speed?**\n\nThanks for pointing out this. We have added the discussion for training speed in the updated submission (See Section 4.3). By training on the same device (8 V100 GPUs) for WMT14 EN-DE, NAT-base takes about 45 hours while GLAT takes about 56 hours. Compared to NAT-base, the training of GLAT is about 1.2 times slower, which is acceptable for achieving significant accuracy improvements. \n\n**Question3: Reranking and KD already have their own training cost, increasing it more could lead to a situation where the benefits during inference are overshadowed by the computational cost from training.**\n\nYes, KD indeed introduces extra training costs, however, we mainly focus on accelerating the inference speeds of text generation, which is also the main purpose of most NAT works.  Reducing the training cost introduced by KD is not in the scope of this work.\nAdditionally, as mentioned in last answer, we think the 1.2 times slower training speed is acceptable considering the significant improvement.\n\n**Question4: Writing: grammatical errors and simplification**\n\nThanks for your kind notes, we have fixed the grammatical errors and simplified the redundant parts in our updated paper. Especially, we have rephrased the description of the glancing sampling strategy section (Please see Section 3.2).\n\n**Question5: How is the function f_{ratio} actually implemented and trained (if applicable)?**\n\nSorry for the misleading. We have rephrased the description of f_{ratio} in Section 4.1 to make it clear. The ratio function is a hyper-parameter for better controlling the sampling number, which is not trainable. We adopt linear annealing from 0.5 to 0.3 for WMT datasets and a fixed value of 0.5 for IWSLT16.\n\n**Question6: Are the models trained until convergence or for a fixed number of steps?**\n\nWe train the models for 300K steps, which is the same as in Mask-Predict.\nRegarding the impact of the glancing sampling strategy on the convergence, we find that NAT-base and GLAT give similar converge trends in practice, which indicates that the glancing sampling does not slow down the convergence of GLAT while predicting partial sentences.\n\n**Question7: How did the authors come up with the number of reranking candidates for each model?**\n\nBecause both 9 candidates and 7 candidates had been used for target length reranking in previous work, all the results of GLAT with reranking are reported by using 7 reranking candidates for fair comparisons. But empirically, we find  results of  7  and  9 reranking candidates are similar. The reranking results with 7 and 9 candidates are: \n\n|                   | EN-DE | DE-EN | EN-RO | RO-EN |\n|----------------------------|:-------:|:-------:|:-------:|:-------:|\n|GLAT(reranking 7 candidates)| 26.55 | 31.02 | 32.87 | 33.51 |\n|GLAT(reranking 9 candidates)| 26.67 | 31.13 | 33.11 | 33.57 |\n||\n\n\n\n", "title": "We have improved the delivery and given more discussions as requested (1)"}, "jxlhNj8z_c1": {"type": "rebuttal", "replyto": "WewrV5bc_Bl", "comment": "**Question5: What do you mean by \"ratio function\" ? What is the input to this function? Steps?**\n\nSorry for the confusion with \"ratio function\". The ratio function is a hyper-parameter for better controlling the sampling number, and the input to this function is the training step. We adopt linear annealing from 0.5 to 0.3 for WMT datasets and a fixed value of 0.5 for IWSLT16. We have made it clear in the revised submission (Please see Section 4.1)\n\n**Question6: How does the model handle the length?**\n\nWe follow the implementation in Mask-predict for length prediction. Similar to the [CLS] token in BERT(Devlin et al., 2019), a special [LENGTH] token is added to the source input of the encoder. And the encoder output of the additional [LENGTH] token is used to predict the target length for the decoder.   \nWe have clarified this in our updated version of our paper (Section 4.1). \n\n**Question7: Add discussion and comparison with Imputer**\n\nThanks, we added the discussion with Imputer in our updated paper. Imputer and GLAT are orthogonal approaches for boosting the performances of NAT without iterative inference.   \n\nAdditionally, the two models use different methods to determine the best target length. Based on CTC (Graves et al., 2016), Imputer sets the max target length twice the length of source input and determines the best length by removing blanks and contiguous repetitive words after generation. Thus,  it is non-trivial to apply the length reranking trick in Imputer, while GLAT can be further improved from 25.2 to 26.5 with AT reranking (Note that GLAT can also employ itself instead of AT in length reranking, still achives 26.0 ) on WMT EN-DE, which outperforms the Imputer model.  \n\nBesides, GLAT and CTC use quite orthogonal techniques to improve generation quality,  thus the combination of them would also be a promising direction for future work. GLAT starts from learning to generate sequence fragments and gradually learns to generate the whole sequence in one iteration. CTC learns to align the predictions with the targets extended by predefined rules.\nWe added the comparison with Imputer in the updated version (Section 4.2).\n\n&nbsp;  \n &nbsp;\n\n* Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.  Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pp. 4171\u20134186, 2019.  \n* Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In ICML, pp. 369\u2013376, 2006", "title": "We have added more discussions about the method and more comparisons with Mask-Predict (2)"}, "qzDERpvzvm": {"type": "rebuttal", "replyto": "Fu91yWWeXmU", "comment": "Thanks very much for your insightful suggestions.\nAs you mentioned in your questions, we add ablation studies to attribute causes to the improvements. Besides, we add discussions on training speed and answer all the minor questions.\n\n**Question 1: \"What happens if you do not forward the decoder inputs to the second decoding pass, but instead set them to a unique MASK token as in prior work (but keep all other aspects of training the same)?\"**  \n\nWe did a quick run and compare our proposed GLAT with Mask-Predict by:   \n1. We replace the glancing sampling with the uniform random sampling used by mask-predict\n2. We use the MASK token as our decoder inputs but keep all other aspects of training the same \n                              \n|\t\t\t\t\t\t\t\t\t\t|   |WMT14 EN-DE\t|\tWMT14 DE-EN |\n|--------------------------------------|---|:--------------------:|:---------------------:|\n|GLAT w/ sampling strategy of Mask-Predict\t|\t|\t19.16\t\t|\t\t23.56\t|\n|GLAT w/ decoder inputs of Mask-Predict \t|\t|\t24.99\t\t|\t\t29.48\t|\n|GLAT\t\t\t\t\t\t\t\t\t|\t|\t25.21\t\t|\t\t29.84\t|\n|  |\n\nThe results show:  \n1. the proposed glancing  sampling is crucial, without which the results will drop significantly (5 ~ 6 BLEU scores)\n2. Replacing the decoder inputs with the MASK token will also lead to a small drop (0.2 ~ 0.3 BLEU). Although the performance drop is marginal, feeding representations from the encoder as the decoder input could still moderately improve the strong baseline after adopting the glancing sampling (23.56 ->29.48 -> 29.84 on DE-EN)\n\nTo sum up, the adaptive glancing sampling approach contributes the most to the final improvement of GLAT, while the use of representations from the encoder also helps a bit.\n\n**Question 2: \"What happens if you use mask-predict's probability-based sampling criterion to pick which tokens to sample, instead of the default random strategy.\"**  \n\nThanks for your kind notes.  We add experiments on WMT14 EN-DE, comparing mask-predict's probability-based sampling with the random sampling (the one used in our paper) to select reference words for glancing. Note that the two sampling strategies are used after the adaptive sampling number N is obtained by computing the Hamming distance.  \nHere, \"most certain\" means we sample words at these positions with higher prediction confidence, and \"most uncertain\" means we choose those with the lower prediction confidence. Results are shown as follows:  \n\n| \t\t|\t|random\t||\tmost certain\t||\tmost uncertain\t|\n|------------|---|:----------:|----------|:-------------:||:---------------:|\n|GLAT\t\t||25.21\t||\t24.99\t\t||\t    24.86\t|\n|GLAT(with reranking)||\t26.55||\t\t26.22\t||\t\t\t26.13|\n||\n\nWe find that all strategies give similar results and the random strategy performs a little better than the two probability-based strategies. We adopt the random strategy in GLAT because it is simple yet effective. Additionally, we think introducing randomness in the glancing sampling could enable GLAT to learn to explore dependencies among target words.\n\n**Question 3: \"GLAT incorporates several changes from the NAT-base architecture they primarily compare against, without clear ablations to attribute cause to the improvement. \"  \nQuestion 4: \"What is the exact architecture of NAT-base? You show the encoder of GLAT works better than NAT-base - is NAT-base trained with the attention (instead of UniformCopy or SoftCopy)? Could that be the reason for the improvement?\"** \n \nThe two questions are related, we answer them as follows:  \n\nTo Question3:  \nSorry for the confusion. The proposed GLAT and NAT-base only have a difference in training, they have the same model architecture. We have provided clear ablations in Question 1 to attribute causes to the final improvement.\n\nTo Question4:  \n1. The model architecture of NAT-base is the same as Gu et al., (2018), except that a) we use attention to copy encoded representation as the decoder inputs instead of uniform copy. b) we do not use their fertility strategy; \nNote that NAT-base is also trained with attention copy. *Neither UniformCopy or SoftCopy is used in our implementation.*\n2. Because NAT-base and GLAT are both trained with attention copy, the attention copy is not the reason that why the encoder of GLAT works better than NAT-base's. We think the improvements come from that after using the proposed glancing sampling strategy, parameters of GLAT is better trained than NAT-base. This leads to a better encoder in GLAT.\n\n **Question 5: \"Discussion on training speed\"**  \n\nThanks for your kind suggestions. We added the discussion for training speed in our updated paper (Section 4.3). By training on the same device (8 V100 GPUs) for WMT14 EN-DE, NAT-base takes about 45 hours while GLAT takes about 56 hours. Compared to NAT-base, the training of GLAT is about 1.2 times slower, which is acceptable for achieving significant accuracy improvements.\n\n* Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and Richard Socher. Non-autoregressive neural machine translation. In ICLR, 2018", "title": "We have added ablation studies. (1)"}, "WewrV5bc_Bl": {"type": "rebuttal", "replyto": "feTdiFsKjcF", "comment": "Thanks for your positive and insightful comments. We have added more discussions about a) why GLAT does not get stuck at predicting only parts of words, and b) the mismatch between two decoding passes in training. We also provided more experiments for comparisons: a) ablation studies for comparisons between GLAT and Mask-Predict; b) besides the hamming distance, we include the Levenshtein distance in GLAT for comparison.  \nAdditionally, we also clarify the details of ratio function and length prediction, as well as provide discussions about the Imputer model.\n\n**Question 1: Why wouldn't the model get stuck at predicting only part of the words correctly? Will the model only learn to predict easy words and give up learning the difficult words? I guess the random sampling strategy might be the key but it would be good to know answers from the authors.**\n\nYes, the randomness introduced by the random sampling strategy enables GLAT not to be stuck in training. With such a random sampling strategy, all the words in the target sentence have equal chances to  be set as the learning target. Additionally, GLAT obtains extra randomness in training, because the model gets different batch samples during training, which makes the model parameters keep varying along with the training phase. This prevents GLAT from getting stuck at predicting only parts of the words. \n\n**Question2: There will be a clear mismatch between the second pass and the first pass. Why can the first model be sure to improve while training the second pass? Only by sharing parameters between two decoding passes?**\n\nThanks for your question. Because the first pass shares a lot of parts with the second pass, not only the parameters but also the encoder inputs and part of the decoder inputs, the first pass could still be improved though there is a mismatch between the second pass and the first pass.  \nAdditionally, the mismatch will be reduced along with the training process. Because during the training process, the model will be better tuned and the adaptive glancing sampling will glance at fewer and fewer target words. As a result, such a mismatch will be addressed to some extent. \n\n**Question3: Will the proposed method also apply to other distance such as Levenshtein distances or a learnable distance?**  \n\nThanks for your kind suggestions, we add experiments of GLAT with Levenshtein distance and list the results of both Hamming distance and Levenshtein distance as follows:\n\n|\t\t\t|\t|WMT14 EN-DE\t    |\t\t|\t|\tWMT14 DE-EN\t|  |\n|-----------------|-----|:--------------|:-------------------|---|:---------------------|:----------------|\n|\t\t\t|\t|Hamming\t    |\tLevenshtein\t|\t|\tHamming\t|  Levenshtein|\n|GLAT\t\t|\t|25.21\t\t|\t24.56\t\t|\t|\t29.84\t\t|\t28.96\t|\n|GLAT(with reranking)|\t|26.55\t|\t26.21      |\t|\t\t31.02\t|\t\t30.85|\n|\n\nBoth distances can be used in GLAT for obtaining improvements by one-iteration generation and we find that the GLAT with Hamming distance performs better than the Levenshtein counterpart. Intuitively, the Hamming distance is a more strict metric compared to the Levenshtein distance. Specifically, only correctly predicting the same words at the corresponding position of the reference is regarded as matching with the reference (the Levenshtein distance does not punish position errors a lot).  Note that such a strict distance metric is more consistent with the MLE objective used in the training of GLAT.\n\n**Question4: Ablation study compared with mask-predict**\n\nThanks for pointing out this. We did a quick run and compare our proposed GLAT with Mask-Predict by:\n1. We replace the glancing sampling with the uniform random sampling used by mask-predict\n2. We use the MASK token as our decoder inputs but keep all other aspects of training the same\n\n|\t\t\t\t\t\t\t\t\t\t|   |WMT14 EN-DE\t|\tWMT14 DE-EN |\n|---------------------------------------------------|---|:----------------------:|:---------------------:|\n|GLAT w/ sampling strategy of Mask-Predict\t|\t|\t19.16\t\t|\t\t23.56\t|\n|GLAT w/ decoder inputs of Mask-Predict \t|\t|\t24.99\t\t|\t\t29.48\t|\n|GLAT\t\t\t\t\t\t\t\t\t|\t|\t25.21\t\t|\t\t29.84\t|\n|\n\nThe results show:\n1. the proposed glancing  sampling is crucial, without which the results will drop significantly (5 ~ 6 BLEU scores)\n2. Replacing the decoder inputs with the MASK token will also lead to a small drop (0.2 ~ 0.3 BLEU). Although the performance drop is marginal, feeding representations from the encoder as the decoder input could still moderately improve the strong baseline after adopting the glancing sampling (23.56 ->29.48 -> 29.84 on DE-EN) \n \nTo sum up, the adaptive glancing sampling approach contributes the most to the final improvement of GLAT, while the use of representations from the encoder also helps a bit.", "title": "We have added more discussions about the method and more comparisons with Mask-Predict (1)"}, "c2g8AxLn2Nq": {"type": "rebuttal", "replyto": "qzDERpvzvm", "comment": "**Question 6: \" Some of the writing is poor and overly-colloquial\"**   \nThanks, we have proofread the draft carefully and fixed these writing issues in the updated paper.\n\n**Question 7: \"You mention your final model is based on averaging the 5 best checkpoints, how are you measuring best?\"**  \nThe best checkpoints are chosen by BLEU scores on the validation set. We made it clear in the updated version.\n\n**Question8: \"You make a quite strong claim that \"We think GLAT is superior to AT to some extent\", which I think needs either more qualification or more focus.\"**   \nSorry for the confusion, We have fixed it in the updated version.  \nEmpirically, we find that GLAT outperforms AT on WMT14 DE-EN, when the length of the source input is less than 20. We speculate that it is because the modeling of target word dependencies is bidirectional in GLAT while AT is unidirectional (left-to-right). The bidirectional word dependency modeling introduces the great potential to parallel text generation models (similar to the differences between GPT and BERT for language understanding).\n\n**Question9: \"I struggled with understanding the paragraph starting with \"Adaptive Sampling Number\", I think it could use some clean-up.\"**  \nThank you for your advice, we revised this part in the updated paper to make it easy to follow (See Section 3.2).", "title": "We have added ablation studies. (2)"}, "_N8E3DFF4SK": {"type": "review", "replyto": "ZaYZfu8pT_N", "review": "The authors propose Glancing Transformer for single step parallel text generation. The approach is inspired from curriculum learning i.e. the training task is adaptively controlled based on the model's current performance. Specifically, the paper proposes a glancing strategy which compares the model's generation and reference sentence, and forms a sequence which is partially masked. The number of masked tokens in this sequence depends on the similarity between model's generation and reference sentence. The model is then trained to complete the partially masked sequence. \n\nThe model achieves strong improvements on standard non-autoregressive MT baselines while not modifying inference process, thus not compromising on inference speed over vanilla non-autoregressive models.\n\n+ves : \n- Limitations of current non-autoregressive MT models (NAT) are well explained, and the approach is nicely motivated. The paper is well written (although there are many grammatical mistakes that can be revised by the authors) and is easy to follow. The experimental details are well documented, many ablation studies are reported apart from comparison using standard metrics.  \n\n- The results on standard benchmarks are strong. The paper improves over vanilla NAT by approx. 5 BLEU points on average, and is only 1 BLEU point less than baseline autoregressive models while being ~7x faster in inference.\n\nConcerns : \n\n- While the use of curriculum learning inspired techniques to augment NAT model training is new and interesting, their specific technique does not seem to have sufficient novelty according to me. If I understood their technique correctly, the only difference in the training algorithm between Mask-Predict (Ghazvininejad et al., 2019) and their method is the selection of \"number of tokens to mask (`mask-num`)\" in the decoder's input. While Mask-Predict uses a uniform distribution to sample `mask-num` , they use a glancing sampler that decides this number based on the hamming distance between model's prediction and reference sentence.\n\n- Although the reported results and ablation studies show a significant impact of this simple change, I think more exploration of this technique is possible and should be done in the paper. E.g. A simple hamming distance may not be a good strategy to compare the model's prediction and reference sentence. A related finding has been described in detail by Ghazvininejad et al., 2020 (https://arxiv.org/abs/2004.01655). So, I believe that authors can explore more strategies to compare reference and generated sequence.\n\n- The authors use Random Sampling like Mask-Predict as their sampling strategy. The authors argue that random sampling may not be the most efficient strategy, but it's the easiest one, and has been shown to be powerful in models like BERT. While I totally agree with their argument, I believe that there is some possibility here to exploit the fact that we have access to model's prediction and reference sentence.  E.g. one possible strategy to exploit this could be selecting tokens that the model was not able to predict correctly based on the hamming distance comparison.  \n\n- In the Introduction section, it is mentioned that - \"Second, GLM replaces the [MASK] token in the MLM with signals from the encoder, thus not only\nthe target word dependency, but also the dependencies between the input and target are strengthened.\" While this is an interesting argument, I didn't find any experiment to validate this. I think the authors should include experiments to compare use of [MASK] tokens and their approach empirically. \n\nBased on these concerns, I am currently inclined to recommend rejection. While I find the idea of incorporating curriculum learning very interesting (together with strong results as demonstrated by the paper), I believe that more exploration of strategies to sample number of masked tokens, and sample words from the reference sentence is necessary to make the paper publishable. I have described this point in more detail in the Concerns above. \n\nMinor Comments : \n- There are many grammatical errors in the current version of the paper that the authors might want to revise. (E.g. in abstract - falls -> fall, achieves -> achieve). \n- In Section 3.1 authors mention this - \"Note that we use the attention mechanism to form the decoder inputs with the input X.\" I think it might be helpful to elaborate more on what this exactly means. ", "title": "curriculum learning based technique to improve single step parallel generation for MT", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "feTdiFsKjcF": {"type": "review", "replyto": "ZaYZfu8pT_N", "review": "- Overall Comment\nThis submission improves non-autoregressive translation (NAT) by proposing a non-iterative parallel text generation model called Glancing Transformer (GLAT), which includes the explicit word dependency modeling in NAT via a proposed Glancing Language Model (GLM). Compared to previous work, biggest contribution of the proposed method is that it improves the training of NAT model with a similar idea of curriculum learning, while keeping the inference time unchanged, setting a significant improvement for non-iterative NAT without reranking. It would be a good baseline for future research on non-iterative NAT models. \nHowever, I still have the following comments and questions:\n- Methods\n(1) Why wouldn't the model gets stuck at predicting only part of the words correctly? \n    As described by the algorithm, the model will sample more reference words as the inputs of the decoder if the prediction is incorrect. However, the loss is only calculated for the remaining words.\nWill the model only learn to predict easy words and give up learning the difficult words? I guess the random sampling strategy might be the key but it would be good to know answers from the authors.\n(2) Since the model only updates in the second pass where the inputs are always mixed with reference words and source embeddings. There will be a clear mismatch between the second pass and the first pass. Why can the first model be sure to improve while training the second pass? Only by sharing parameters between two decoding passes?\n(3) Hamming distance is quite weak. Will the proposed method also apply to other distance such as Levenshtein distance or a learnable distance?\n(4) What do you mean by \"ratio function\" $f_\\text{ratio}$? What is the input to this function? Steps?\n- Experiments\n(1) The proposed training process is in fact very similar to Mask-Predict except the inputs (Encoder hidden states instead of [MASK]) and adjusting the number of reference during training.  It would be nice to have a fair comparison with Mask-Predict for at least two settings (and combined):\n(A) Mask-Predict with encoder hidden state inputs (with uniform copy or attention)\n(B) Mask-Predict with 1 decoding iteration.\n(2) I did not see how the model handle the length. With another network for length prediction? Or decoding with multiple lengths? For the latter case, how to decide these lengths?\n- Missing Reference\nThis paper had an even higher one-iteration NAT results (**25.8** with Imputer compared to **25.21** in the submission on WMT En-De). Although the methods are different and the difference is to be honest marginal, it is important to include discussions on that or combine them in future work.\n *Saharia, Chitwan, et al. \"Non-autoregressive machine translation with latent alignments.\" arXiv preprint arXiv:2004.07437 (2020).*\n", "title": "Inspiring idea for training \"one-iteration\" NAT", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Fu91yWWeXmU": {"type": "review", "replyto": "ZaYZfu8pT_N", "review": "*Paper Contributions*\n\nThe paper proposes the Glancing Transformer (GLAT), a model for non-autoregressive generative model of language (focusing on the MT domain). GLAT incorporates several changes which result in a model which is state-of-the-art in several MT categories for non-autoregressive models and without requiring iterative sampling.\n\n*Strong points of the paper*\n\n* The paper clearly lays out the suggested model changes, and how they related to previous work.\n* Some of the results are clearly state of the art for the task.\n* The model and changes are described well and clearly.\n\n*Weak points of the paper*\n\n* GLAT incorporates several changes from the NAT-base architecture they primarily compare against, without clear ablations to attribute cause to the improvement.\n* Most of the paper discusses the impact of architectures on inference/sampling speed, but doesn't discuss training speed, which seems relevant because my understanding of the model means that each step requires an additional decoder pass.\n* Some of the writing is poor and overly-colloquial.\n\n*Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.*\n\nI believe this paper is marginally above the acceptance threshold, though I think there is a clear avenue to improve the paper and change my rating to accept.\n\n*Supporting arguments for your recommendation.*\n\nThe proposed GLAT model essentially consists of two changes from prior work:\n1) Using an unconditional forward pass of the decoder to determine which tokens to mask, and then running the decoder a second time\n2) Forwarding of encoding outputs via attention instead of mask tokens in the second decoder.\n\nMy main criticism of the paper is that is it unclear how each of the two changes contribute to the overall improvement being seen by the model. It might be the case that it is exclusively one of them, and I think the paper would clearly benefit from ablations discussing the two and showing how they each contribute.\n\nAs a secondary criticism, the paper proposes using a simple random sampling strategy to determine which tokens to mask (and which to glance at). While the authors defend this as the simple choice (and it certainly is), it feels to me that there is an obvious second alternative, which is to use the probability of the tokens generated by the decoder without glancing at anything (as in mask-predict's iterative sampling) to pick which tokens to glance at and which not to. It feels to me that this alternative is substantially obvious (seeing as it is exactly how mask tokens have been predicted in the past) that it should be attempted.\n\n*Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.*\n\nI would ask that two questions be answered, both ablations trying to get at the heart of the reason for improvement:\n\n1) What happens if you do not forward the decoder inputs to the second decoding pass, but instead set them to a unique MASK token as in prior work (but keep all other aspects of training the same)?\n2) What happens if you use mask-predict's probability-based sampling criterion to pick which tokens to sample, instead of the default random strategy.\n\nAlso, I have two questions which I think also should be answered, but don't require any further experimentation:\n\n3) What is the impact on glancing sampling to training time?\n4) What is the *exact* architecture of NAT-base? You show the encoder of GLAT works better than NAT-base - is NAT-base trained with the attention (instead of UniformCopy or SoftCopy)? Could that be the reason for the improvement?\n\nI feel that if the authors answer these questions (ideally with WMT benchmark data for the first two), along with addressing the more minor points I list below, that I would feel comfortable increasing my rating to \"accept\".\n\n*Additional feedback with the aim to improve the paper.*\n\nI had a few minor questions while reading that I think would be useful to address:\n* You mention your final model is based on averaging the 5 best checkpoints, how are you measuring best?\n* You make a quite strong claim that \"We think GLAT is superior to AT to some extent\", which I think needs either more qualification or more focus.\n* I struggled with understanding the paragraph starting with \"Adaptive Sampling Number\", I think it could use some clean-up.\n\nFinally, I think most of the writing is clear, but there are a few points where it becomes quite colloquial and difficult to follow, I think an additional pass cleaning up some of the structure could be quite beneficial!", "title": "Official Review of Non-iterative Parallel Text Generation via Glancing Transformer", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}