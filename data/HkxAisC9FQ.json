{"paper": {"title": "Improved robustness to adversarial examples using Lipschitz regularization of the loss", "authors": ["Chris Finlay", "Adam M. Oberman", "Bilal Abbasi"], "authorids": ["christopher.finlay@mail.mcgill.ca", "adam.oberman@mcgill.ca", "bilal.abbasi@mail.mcgill.ca"], "summary": "Improvements to adversarial robustness, as well as provable robustness guarantees, are obtained by augmenting adversarial training with a tractable Lipschitz regularization", "abstract": "We augment adversarial training (AT) with worst case adversarial training\n(WCAT) which improves adversarial robustness by 11% over the current state-\nof-the-art result in the `2-norm on CIFAR-10. We interpret adversarial training as\nTotal Variation Regularization, which is a fundamental tool in mathematical im-\nage processing, and WCAT as Lipschitz regularization, which appears in Image\nInpainting. We obtain verifiable worst and average case robustness guarantees,\nbased on the expected and maximum values of the norm of the gradient of the\nloss.", "keywords": ["Adversarial training", "adversarial examples", "deep neural networks", "regularization", "Lipschitz constant"]}, "meta": {"decision": "Reject", "comment": "This paper suggests augmenting adversarial training with a Lipschitz regularization of the loss, and suggests that this improves the adversarial robustness of deep neural networks. The idea of using such regularization seems novel. However, several reviewers were seriously concerned with the quality of the writing. In particular, the paper contains claims that not only are not needed but also are incorrect. Also, the Reviewer 2 in particular was also concerned with the presentation of prior work on Lipschitz regularization. \n\nSuch poor quality of the presentation makes it impossible to properly evaluate the actual paper contribution. "}, "review": {"BJgh0eTL1N": {"type": "rebuttal", "replyto": "HkePeI0SyV", "comment": "Hi,\n\nThanks for your interest. You're correct, a mixed derivative -- in both x (image) and theta (parameters) is computed. Our implementation was easily done in PyTorch, simply by running autograd twice - first in x to get the norm gradient, then in theta. In practice we found that networks trained with this Lipschitz penalty took no longer than four times the training time of an unregularized network. Which means that if you are doing many steps of PGD adversarial training (for example in Madry et al uses 7 steps), Lipschitz regularization can be faster. The comparison depends on how many PGD steps you take.", "title": "training details"}, "HJxhfMpp0m": {"type": "rebuttal", "replyto": "rJejui5aCX", "comment": "I see there is a discussion of how our results compare.  Some interesting points are raised by the poster and by the authors of the other paper.   \n\nWe stand by our statement in our paper:\nWe implemented the attacks correctly and our robustness results are stronger than the other paper, and any other published results for L2 attacks.", "title": "thanks for the pointer"}, "H1xFcKa_hX": {"type": "review", "replyto": "HkxAisC9FQ", "review": "This paper explores augmenting the training loss with an additional gradient regularization term to improve the robustness of models against adversarial examples. The authors show that this training loss can be interpreted as a form of adversarial training against optimal L2 and L_infinity adversarial perturbations. This augmented training effectively reduces the Lipschitz constant of the network, leading to improved robustness against a wide variety of attack algorithms.\n\nWhile I believe the results are correct and possibly significant, the paper is poorly written (especially for a 10 page submission) and comparison with prior work on reducing the Lipschitz constant of the network is lacking. The authors also made little to no effort in writing to ensure the clarity of their paper. I would like to see a completely reworked draft before opening to the idea of recommending acceptance.\n\nPros:\n- Theoretically intuitive method for improving the model's robustness.\n- Evaluation against a wide variety of attacks.\n- Empirically demonstrated improvement over traditional adversarial training.\n\nCons:\n- Lack of comparison to prior work. The authors are aware of numerous techniques for controlling the Lipschitz constant of the network for improved robustness, but did not compare to them at all.\n- Poorly written. The paper contains multiple missing figure references, has a duplicated table (Tables 1 and 3), and the method is not explained well. I am confused as to how the 2-Lip loss is minimized. Also, the paper organization seems very chaotic and incoherent, e.g., the introduction section contains many technical details that would better belong in related works or methods sections.\n\n--------------------------------------------\n\nRevision:\n\nI thank the authors for incorporating my suggestions and reworking the draft, and I have updated my rating in response to the revision. While I believe the organization is much cleaner and easier to follow, there is still much room for improvement. In particular, the paper does not introduce concepts in a logical order for a non-expert to follow (e.g. Reviewer 1) and leaps into the paper's core idea too quickly. I am strongly in favor of exceeding the suggested page limit of 8 pages and using that space to address these concerns.\n\nA more pressing concern is the evaluation of prior work. The authors added a short section (Section 5.4) comparing their method to that of (Qian and Wegman, 2018). This is certainly a reasonable comparison and the results seem promising, the evaluation lacks an important dimension -- varying the value of epsilon and observing the change in robustness. This is an important aspect for defenses against adversarial examples as certain defense may be less robust but are insensitive to the adversary's strength. Showing the robustness across different adversary strengths gives a more informative view of the authors' proposed method in comparison to others. The evaluation is also lacking in breadth, ignoring other similar defenses such as (Cisse et al., 2017) and (Gouk et al., 2018).", "title": "Interesting idea but poorly written", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hye2u2xPCm": {"type": "rebuttal", "replyto": "H1e9KG-ch7", "comment": "Thank you for your review. We have reworked our draft, and we hope that our new version addresses your points.\n\nWe would like to make a comment regarding Big-O notation. In the context used in the paper, which corresponds to https://en.wikipedia.org/wiki/Big_O_notation ,  Big-O notation is a rigorous result, not experimental. \n\nIn many areas of scientific computing, engineering and statistics it is accepted that results need only be shown up to Big-O of some error term. For example in polynomial interpolation it typically it suffices to show a particular method has error epsilon^(n+1) with a n-th degree polynomial. We have shown that adversarial training is equivalent to Total Variation minimization, up to order epsilon^2, where epsilon is the size of the adversarial perturbation. This means that when epsilon is small, as is typical (our epsilon is 0.01),  the two methods are nearly equivalent.  By equivalent,  lossely speaking, we mean that replacing one term with another should lead to results which are very close.   However the Big-O notation has a rigorous meaning in the limit as \\epsilon goes to zero. \n\nPlease also see our general reply to all reviewers, above.", "title": "Reply to AnonReviewer3"}, "BJgKHaxwAX": {"type": "rebuttal", "replyto": "H1xFcKa_hX", "comment": "Thank you for your comments. Following your suggestion, we have completely reworked our submission, with an eye towards clarity and the page limit.  In fact we have reduced the page count to just over seven pages.  We hope that you find the paper better organized and easier to read.\n\nRegarding comparison to other results, we have re-analyzed our experimental results and have now included a direct comparison with state-of-the-art results. We find that when attacks are measured in the 2-norm, our method is state-of-the-art, improving on the previous state-of-the-art by 11%. When measured in the max-norm, our results are comparable to the state-of-the-art (Madry et al (2017)), however we use only one-step adversarial training, whereas in Madry et al seven step adversarial training is used.\n\nWe have also now included a section explicitly comparing our methods with prior methods, which can be summarized as follows. Prior work has focused on controlling the estimate of the Lipschitz constant using the product of norms of weight matrices. We argue that for deep networks this estimate is inaccurate, since its error grows exponentially in the number of layers. In our work we propose an alternative method for estimating the Lipschitz constant, which is an underestimate, and is estimated from the training data. This is a novel approach.\n\nPlease also see the general reply to all reviewers, above.", "title": "Reply to AnonReviewer2"}, "BklZ42xPRm": {"type": "rebuttal", "replyto": "rkgx_ajThm", "comment": "We have posted a new version of our paper. We have re-written the paper to be as accessible as possible to someone not directly familiar with adversarial robustness. We have also pushed the heavier math to the appendix, and reinterpreted our Lipschitz regularization as worst-case adversarial training, which is an interpretation of a more familiar idea in the area. \n\nWe would greatly appreciate your comments on the new draft. We hope that you will find it easier to read.\n\nPlease also see our general reply to all reviewers above.", "title": "Reply to AnonReviewer1"}, "rkxOCoewAQ": {"type": "rebuttal", "replyto": "HkxAisC9FQ", "comment": "Thank you to the reviewers for your comments. We have posted a new draft of the manuscript. Following AnonReviewer2\u2019s comments, we have completely reworked the draft. We have attempted to communicate our ideas and results as clearly as possible. We hope that the paper is accessible to persons outside the field of adversarial attacks as well, such as AnonReviewer1.\n\nWe would like to highlight the merits of our results. \n\n1. We achieve state-of-the-art results when measuring attacks in the 2-norm, improving by 11% over the previous state-of-the-art on CIFAR-10. Following off line conversations with some of our colleagues, we have also analyzed our results in the max-norm as well. In the max-norm, we are on par with the current state-of-the-art (Madry et al (2017)). However we did not focus our efforts on the max-norm, so we believe with a bit more effort our results could possibly be improved, for example if we had used multi-step adversarial training like in Madry et al (we only used one-step adversarial training).\n\n2. Our implementation of Lipschitz regularization is novel and an improvement to existing results in terms of both accuracy (by orders of magnitude) and efficiency (we can leverage the gradients already used in adversarial training). Training networks by penalizing with estimates of the Lipschitz constant have in the past used the product of weight matrix norms. However this estimate of the Lipschitz constant grows exponentially in the number of layers. As such the estimate is intractable for deep networks. In contrast, our method provides a more accurate estimate, which we demonstrate is closer in magnitude to the true value of the Lipschitz constant.\n\n3. We believe that our interpretation of adversarial training will be interesting and useful to the community. We show that adversarial training is a form of Total Variation regularization, which has been used successfully outside the deep learning community in image preprocessing to denoise images. We believe that this is a useful insight that could be leveraged further in the future. \n\n4. We obtain novel average case and worst case robustness bounds, which we verify empirically.  These bounds allow use to predict adversarial robustness based on the statistics of quantities we can read off of the trained model.  ", "title": "General reply to reviewers"}, "B1xzNWKERm": {"type": "rebuttal", "replyto": "BJg8q4ZuTm", "comment": "Hi,\nThanks for your comment.  We had in mind a result which would be true under additional assumptions.  But, in fact, we don't use the lemma anywhere - it was just to illustrate upper bounds on the Lipschitz constant coming from the architecture.  \n\nWe are removing it from the revision.", "title": "The lemma was not needed, it was redundant, so it has been removed"}, "rkgx_ajThm": {"type": "review", "replyto": "HkxAisC9FQ", "review": "The authors propose a novel method of training neural networks for robustness of adversarial attacks based on 2-norm and Lipschitz regularization. Unfortunately I'm not at all familiar with the literature on adversarial attacks so it is difficult for me to judge the quality and significance of this work. The theoretical results look plausible and clearly stated. The experiments show improvements over existing methods but I can't tell whether the right baselines were used. Overall the writing is reasonably clear but not very accessible for someone not already familiar with the area.", "title": "Possibly a good paper but not my area of expertise at all", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "H1e9KG-ch7": {"type": "review", "replyto": "HkxAisC9FQ", "review": "Summary: this paper uses a trick to simplify the adversarial loss by one in which the adversarial perturbation appears in closed form.\n\npros:\n\n- interesting idea\n- experiments are interesting\n\ncons:\n\n- formal results are either trivial or could be improved in their statements \n- experimental guarantees only, up to what is hidden in the Big-Oh notations of Theorem 2.2, 2.3.\n\ndetails:\n\n* In Theorem 2.2, you need to remove the $O(epsilon^2)$, unless you point to the Taylor theorem that guarantees that for the identity you claim before (5). The closest one I see is that the O(||a||^2) is in fact $||a|| u(||a||)$ with $\\lim u(x) = 0$ as $x \\rightarrow 0$, which does not guarantee the $O$ notation for any $a$.\n\n* In Theorem 2.2, how do you pass from the solution of (5) (which is indeed a vector) to the solution of the following equation, which, without constraint, gives a dim > 1 subspace in the general case ?\n\n* In all cases, you do not get Theorem 2.3 in its form as the $O$ notation just guarantees you an upperbound. You need to rephrase.\n\n* Figure ?? (twice) before Section 3\n\n* Define the \u201cgroup norm\u201d notation appearing with the max in (8) (isn\u2019t one redundant ?)\n\n* Section 3.4 is interesting. Have you looked at generalising your observation in the last identity to more losses  = f-divergences (hence, proper losses modulo assumptions) ? \n\n* Section 4: many Figure ??", "title": "Interesting idea -- could be significantly strengthened", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkxQ77mUiX": {"type": "rebuttal", "replyto": "HkxAisC9FQ", "comment": "Several people have suggested that it would be helpful if we also reported measurements of adversarial distance in the L-infinity norm (to complement L2). Following this suggestion, we have re-generated all the tables and figures in L-infinity.\n\nFor example, our main results are presented in Table 1, where we report median adversarial distance and percent error at a fixed adversarial distance. Here is Table 1 with distances in L-infinity. We report percent misclassified at adversiarial distance 1/16 (rather than 0.1) to more easily compare with other literature's results.\n\nDataset     defense method     median distance    % err at eps=1/16\n\nCIFAR-10    J0 (baseline)                            1.02e-2                          99.92\n                    J1 (AT, FGSM)                          2.12e-2                          96.06\n                    J2 (AT, L2)                                3.45e-2                          84.76\n                   J2-Lip & tanh                            6.00e-2                          51.64\n\nCIFAR-100   J0 (baseline)                           5.83e-3                          99.61\n                     J1 (AT, FGSM)                         1.07e-2                          98.46\n                     J2 (AT, L2)                               1.06e-2                           98.03\n                    J2-Lip & tanh                           1.60e-2                           93.73\n\nWe hope this updated table is useful during the review process. ", "title": "Results measured in L-infinity"}}}