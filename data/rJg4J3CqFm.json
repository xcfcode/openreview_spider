{"paper": {"title": "Learning Embeddings into Entropic Wasserstein Spaces", "authors": ["Charlie Frogner", "Farzaneh Mirzazadeh", "Justin Solomon"], "authorids": ["frogner@mit.edu", "farzaneh@ibm.com", "jsolomon@mit.edu"], "summary": "We show that Wasserstein spaces are good targets for embedding data with complex semantic structure.", "abstract": "Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions. Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric. Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures. We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions. We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding. We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space. This obviates the need for dimensionality reduction techniques such as t-SNE for visualization.", "keywords": ["Embedding", "Wasserstein", "Sinkhorn", "Optimal Transport"]}, "meta": {"decision": "Accept (Poster)", "comment": "\n+ An interesting and original idea of embedding words into the (very low dimensional) Wasserstein space, i.e. clouds of points in a low-dimensional space\n+ As the space is low-dimensional (2D), it can be directly visualized. \n+ I could imagine the technique to be useful in social / human science for data visualization, the visualization is more faithful to what the model is doing than t-SNE plots of high-dimensional embeddings\n+ Though not the first method to embed words as densities but seemingly the first one which shows that multi-modality  / multiple senses are captured (except for models which capture discrete senses)\n+ The paper is very well written\n\n-  The results are not very convincing but show that embeddings do capture word similarity (even when training the model on a small dataset)\n-  The approach is not very scalable (hence evaluation on 17M corpus)\n-  The method cannot be used to deal with data sparsity, though (very) interesting for visualization\n-  This is mostly an empirical paper (i.e. an interesting application of an existing method)\n\nThe reviewers are split. One reviewer is negative as they are unclear what the technical contribution is (but seems a bit biased against empirical papers). Another two find the paper very interesting. \n\n\n\n\n"}, "review": {"S1xCCk9GfN": {"type": "rebuttal", "replyto": "HJe5LctXW4", "comment": "Thanks for the comment.\n\n- The input to each branch of our Siamese architecture is a one-hot encoding of a word from the vocabulary. Next, each of the branches goes through a fully-connected linear layer, followed by the point cloud layer, which is a fully-connected linear layer whose output is normalized to lie within the unit ball. The Sinkhorn distance is then applied to the outputs of the two branches to compute the distances used within the contrastive loss.\n\n- You\u2019re right, thanks!", "title": "Siamese architecture"}, "rJgNxurnR7": {"type": "rebuttal", "replyto": "BkgLLSSjC7", "comment": "Thank you for responding. We are significantly confused by two of your statements:\n\n\u201c... given that your axis are probability values, plotting the data in an orthogonal axis is probably not the best idea. The correct way of representing the data would be on a simplex.\u201d\n\nThis statement is false. As noted by Reviewer 2, our axes are **not** probability values. This is indeed the central concept of the paper. We are representing inputs as sums of Diracs and optimizing the locations of these Diracs (Section 3.2). As the Diracs are weighted uniformly, they can be visualized as point clouds in the ground metric space (which, in Section 4.2.1, is R^2). Again, this is fundamental to understanding what we did; it is the central concept of the paper.\n\n\u201c... you can do the same visualization in a Euclidean space.\u201d\n\nThis statement is also false. A high-dimensional Euclidean embedding **cannot** be visualized without dimensionality reduction. A point cloud, meanwhile, can be directly visualized since it consists of (multiple) points in two or three dimensions.\n\nWhile we appreciate your willingness to respond, the issues above appear to indicate a fundamental misunderstanding of our submission.  While we still hope you might consider revising your score or confidence level, we also respect your suggestion to defer to the Area Chair.", "title": "Some confusion"}, "rkg2CiWi0Q": {"type": "rebuttal", "replyto": "ryl-6sbsAX", "comment": "*** DIRECT VISUALIZATION ***\n\nRegarding the meaning of direct visualization: Keep in mind that an embedding into a discrete probability distribution is high-dimensional (i.e. it has many parameters, being the point locations), even when the ground space is low-dimensional. If a discrete distribution is supported at 8 points in a 2-D ground space, it has 8x2=16 parameters, so the most direct comparison would be to a 16-D Euclidean embedding, which cannot be visualized without dimensionality reduction. Yet in our case we do not need to apply dimensionality reduction, because the embedding is interpretable as a point cloud in the low-dimensional ground space. We can directly visualize this point cloud. This is the sense of the word \u201cexact:\u201d We need not apply any dimensionality reduction before visualizing the point cloud.\n\n*** UTILITY ***\n\nRegarding utility of the embedding: Please see Section 4.2, in which we show performance on a set of semantic similarity benchmarks, which require predicting human semantic similarity judgments. Note that in Section 4.1 we use the mean distortion as it is the standard measure for metric embeddings and directly characterizes the representational capacity of the embedding.\n\n*** RUNTIME ***\n\nRegarding runtime: Complexity of the Sinkhorn divergence computation is discussed in (Cuturi 2013) and (Soules 1991), with the latter showing linear convergence of the Sinkhorn iteration. Automatic differentiation of the Sinkhorn iteration has the same complexity. The runtime complexity of our proposed method is identical to that of (Genevay 2018).\n\n\n(Nickel & Kiela 2017) Maximilian Nickel and Douwe Kiela. Poincare embeddings for learning hierarchical representations. In NIPS (2017).\n(Muzellec & Cuturi 2018) Boris Muzellec and Marco Cuturi. Generalizing point embeddings using the Wasserstein space of elliptical distributions. In NIPS (2018).\n(Vilnis & McCallum 2014), Luke Vilnis and Andrew McCallum. Word representations via Gaussian embedding. In ICLR (2015).\n(Cuturi 2013) Marco Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In NIPS (2013).\n(Soules 1991) George Soules. The rate of convergence of Sinkhorn balancing. Linear Algebra and its Applications 150 (1991).\n(Genevay 2018) Aude Genevay, Gabriel Peyr\u00e9, Marco Cuturi. Learning Generative Models with Sinkhorn Divergences. In AISTATS (2018).", "title": "Thank you for clarifying! (2/2)"}, "ryl-6sbsAX": {"type": "rebuttal", "replyto": "B1lPtGDXCm", "comment": "Thank you for clarifying your remaining questions.  We are convinced of the novelty and interest of our contribution and are still hoping to demonstrate it to you.\n\n*** NOVELTY ***\n\nThere seems to be some confusion regarding the particular contributions of our paper. We would characterize our main contributions as follows:\n\n1. NEW REPRESENTATION:  We propose to learn embeddings of arbitrary inputs, such as images or words, as discrete probability distributions under an optimal transport metric. Notably, embedding into non-Euclidean spaces is currently an active area of research in the representation learning community (see for example (Nickel & Kiela 2017) and (Muzellec & Cuturi 2018)). To our knowledge, the idea of learning an embedding into a discrete transport metric has never been explored.\n\n2.  EMBEDDING CAPACITY:  We show empirically that such learned embeddings can preserve a wide variety of metrics (Section 4.1), with lower distortion than baseline metrics. This is interesting both from the pragmatic perspective, that our learned embeddings are more flexible than the baselines, and also from the theoretical perspective, suggesting that the Sinkhorn divergence might inherit the favorable embedding properties of the Wasserstein distance identified in Section 2.3.\n\n3.  WORD EMBEDDING:  We show empirically that useful Wasserstein embeddings can be learned for words (Section 4.2), achieving scores comparable to the state of the art on a set of semantic similarity benchmarks, which require predicting human similarity judgments.  This contribution extends a recent line of work on word embeddings that are more general than \u201ca single point in a vector space.\u201d See for example word2gauss (Vilnis & McCallum 2014) and Poincare embedding (Nickel & Kiela 2017), for other work in this direction. \n\n4.  VISUALIZATION:  We demonstrate that Wasserstein embeddings over low-dimensional ground spaces can be directly visualized, unlike other high-dimensional embeddings (Section 4.2.1), and that, in the case of word embeddings, these visualizations reveal useful information for interpreting and debugging the embedding.\n\nAgain, we are confident that these are novel and interesting ideas. Our ideas are well-tested empirically, are linked in the discussion to relevant existing theoretical results, and suggest new theoretical challenges that will inspire follow-on work. For recent papers with a similar empirical approach to proposing embeddings into alternative metric spaces, see for example (Nickel & Kiela 2017) and (Muzellec & Cuturi 2018).\n\n*** LOSS FUNCTION ***\n\nRegarding designing a loss function that uses the Wasserstein structure: It is not clear to us what this would entail. The message of our paper is that learning an embedding into an entropic Wasserstein space is both possible and useful, so our focus is on the impact of the Wasserstein metric and not the outer loss (which is problem-specific and agnostic to the particular divergence used, see Equation 7). This focus on the metric over the loss is very much in the vein of recent work, such as (Nickel & Kiela 2017) and (Muzellec & Cuturi 2018). Moreover, separating the target divergence from the loss function can be advantageous for learning embeddings, as it allows greater flexibility in designing the learning system than tailoring an ad hoc loss function for each task-representation pair. If you provide additional detail or an example as to what you mean by a loss function that \u201cuses the Wasserstein structure,\u201d we are happy to provide additional clarification.\n\n*** INTERPRETATION OF PHI(X) ***\n\nRegarding interpretation of phi(x) for word embeddings: Each word is embedded as a discrete distribution in an entropic Wasserstein space, so its interpretation requires thinking about the notion of nearness induced by the transport metric. Two distributions are near under a transport metric only if they are similar both in shape and location -- i.e. their supports are close and their densities are similar -- as differences in either can require transporting some of the probability mass over a large distance. This is what makes Wasserstein spaces flexible: We can encode semantics in both the shape and location of the distribution, providing an enormous embedding capacity even when the ground space (i.e. the support) is low-dimensional. Unlike a traditional mixture model (such as LDA), in which each location in the ground space (i.e. each component of the mixture) has a fixed semantic interpretation, in our case interpreting phi(x) requires looking at the entire embedded distribution, in the context of other embedded distributions. We illustrate how to do so in Section 4.2.1.", "title": "Thank you for clarifying! (1/2)"}, "SkxUwq-jCX": {"type": "rebuttal", "replyto": "rkegohvX0X", "comment": "Thanks for getting back to us!  We certainly do not want to disappoint and are enthusiastic to change your mind before the discussion period ends.\n\nThe note about generalization in Section 4.1 is specific to the experiments in that section; other parts of the paper address generalization directly. Generalization of the learned models to unseen input pairs and new tasks is certainly possible, and this is in fact one purpose of the discussion in Section 4.2. In particular, we demonstrate generalization of the learned word embeddings on an independent prediction task, in which we successfully predict human semantic similarity judgments. The positive results in Table 2 indicate that such generalization is indeed possible. \n\nIn general, the question of generalization of learned embeddings is an interesting one and we expect this to be an important area for application and further development of the proposed method.  If there are experiments you wish for us to include in our camera-ready draft to demonstrate generalization in our existing framework, we would be more than happy to include them.\n\nWe appreciate your feedback.  Please let us know if you have further concerns!", "title": "Regarding generalization"}, "rkgjrO0Z0Q": {"type": "rebuttal", "replyto": "rJg9pF6g0m", "comment": "Thanks for commenting!\n\n1.  Thanks for noticing the typo in Eq. (9). It\u2019s been corrected.\n2.  Each word is embedded as a point cloud (i.e. a uniformly weighted discrete distribution). In Section 4.2.1, we apply KDE to each point cloud (i.e. word), separate from the others.\n\nLet us know if you have any other questions.", "title": "Thanks!"}, "ByxV7nuupQ": {"type": "rebuttal", "replyto": "SkgboCbq37", "comment": "Thank you for your helpful comments! We made several small revisions, according to your suggestions.\n\nTo make it more clear that we are using regularized transport, we have made four changes:\n1.  The title is now \u201cLearning Entropic Wasserstein Embeddings.\u201d\n2.  In the introduction, we expanded the existing discussion of the Sinkhorn divergence and moved it to a separate paragraph (now the fifth paragraph).\n3.  In Section 2.3, we note that we are using the Sinkhorn divergence, and briefly discuss its theoretical properties.\n4.  In the conclusion, we state that our empirical results are for the Sinkhorn divergence.\n\nWe have expanded the third paragraph of Section 2.3, to provide more detail on embedding capacity results for Wasserstein spaces.\n\nOur intent with including \u201cnice\u201d in the visualization section was to show first an error (Figure 3c), which is that \u201cnice\u201d is visibly distant from \u201ckind\u201d (a synonym), then to show the explanation for the error (Figure 3d), which is that the network learned that \u201cnice\u201d is a city in France, while ignoring its second meaning. To make this clearer, we have changed the caption for Figure 3d to \u201cExplaining a failed association.\u201d Please let us know if this addresses your comment.\n\nThe idea to look at semantic hierarchies is very interesting, and we did investigate this briefly. We observed that the minimal level sets of the parent and child in the hierarchy were often partially overlapping -- for instance the embedding of \u201ccity\u201d has two major modes, one of which overlaps with a number of cities, including \u201cnice.\u201d As you point out, however, it is not immediately clear how hierarchies should manifest in this type of embedding, particularly as the parent can have semantics not shared by the children (e.g. \u201cstate,\u201d which has multiple non-geographic meanings). Perhaps the parent (or one of its modes) should be near to the barycenter of the children? This would be interesting to investigate further.\n\nThank you again for your feedback!", "title": "Thank you for your feedback!"}, "r1eUJa_d6m": {"type": "rebuttal", "replyto": "r1gPBWZ8hm", "comment": "Thank you for your thoughtful comments! We have made several small edits, according to your suggestions.\n\n(Brancolini 2009) and (Kloeckner 2012) show that, when using a weighted point cloud to approximate an absolutely continuous, compactly-supported measure, the order of convergence in p-Wasserstein distance when allowing non-uniform weights is O(n^(-1/d)), which is the same rate as when restricted to uniform weights (Dudley 1969). Non-uniform weights might buy you an improved constant term, but the rate is the same. We have expanded the description of this fact in Section 3.2, for clarity.\n\nThe embedding capacity of W^\\lambda_p(R^d) is unknown, so far as we are aware, except in the weak sense that the approximation error with respect to the p-Wasserstein distance vanishes as the regularizer is taken zero (Carlier 2017; Genevay 2018). We have added a discussion of this distinction between W_p and W^\\lambda_p to Section 2.3.\n\nInverting the mapping and following geodesics in Wasserstein space would definitely be interesting. We have added this to the suggested future work in the conclusion. An approach such as (Seguy 2015) might be useful here.\n\nWe have updated the notation in eq. (9) to highlight the fact that entropic regularization is used for learning word embeddings.\n\nWe have added a comment on generalization performance to Section 4.1, paragraph 3.\n\nThank you again for your feedback!\n\n(Brancolini 2009) Alessio Brancolini, Giuseppe Buttazzo, Filippo Santambrogio, Eugene Stepanov. Long-term planning versus short-term planning in the asymptotical location problem. ESAIM: Control, Optimisation, and Calculus of Variations 15, no. 3 (2009).\n(Kloeckner 2012) Benoit Kloeckner. Approximation by Finitely Supported Measures. ESAIM: Control, Optimisation, and Calculus of Variations 18, no. 2 (2012).\n(Dudley 1969) Richard Dudley. The Speed of Mean Glivenko-Cantelli Convergence. Annals of Mathematical Statistics 40, no. 1 (1969).\n(Carlier 2017) Guillaume Carlier, Vincent Duval, Gabriel Peyr\u00e9, Bernhard Schmitzer. Convergence of Entropic Schemes for Optimal Transport and Gradient Flows. SIAM Journal on Mathematical Analysis 49, no. 2 (2017).\n(Genevay 2018) Aude Genevay, L\u00e9naic Chizat, Francis Bach, Marco Cuturi, Gabriel Peyr\u00e9. Sample Complexity of Sinkhorn Divergences. arXiv:1810.02733 (2018).\n(Seguy 2015) Vivien Seguy and Marco Cuturi. Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric. NIPS 2015.", "title": "Thank you for your comments!"}, "BkxbM0_d6X": {"type": "rebuttal", "replyto": "B1gutHkNnX", "comment": "Thank you for your comments! We are confident that the contributions of our work are both novel and interesting to the ICLR community.  We are eager to address your concerns, which can be addressed in a minor revision to our text or experiments.  Some responses are provided here, and we are happy to continue the conversation and/or provide additional information at your request. \n\nMost importantly, although you mention automatic differentiation as a negative, its use for approximating gradients of optimal transport-based divergences is non-obvious and was very recently a primary contribution of the AISTATS paper (Genevay 2018), which provides a way to differentiate transport for use in a neural network that is both stable and efficient. In a sense, our work is among the first to apply these new developments to a representation learning/embedding problem. \n\nMore generally, efficient evaluation of optimal transport distances and their derivatives is a well-known and long-standing challenge in optimization. We leverage the current state of the art, in terms of efficiency, by using the Sinkhorn approximation to the Wasserstein distance (Section 2.2).\n\nWhile we appreciate the suggestion to show t-SNE plots and can add them to the paper if acceptance to ICLR is contingent on this change, it is worth noting that they will communicate different, more limited information in comparison to our visualizations. As is well-known in the data science community, it is easy to ascribe signal to noise when interpreting the locations of the t-SNE points, as they are not intended to capture locations or distances in the original embedding space. Of course, we are happy to generate the figures and see if they are useful, at your request.\n\nWe respectfully highlight a few instances in which your questions are partially addressed in the existing text. We will gladly revise and/or augment the text for clarity, with guidance on the most effective ways to communicate the ideas below.\n\n1. The interpretation of phi(x) for word embeddings is discussed in Section 4.2.1, where we demonstrate direct visualization of the embedding. \n2. The Euclidean and hyperbolic embeddings are computed nearly identically to the Wasserstein embedding -- same loss function, same optimizer (Adam), different learning rate. This was mentioned in Section 4.1, fifth paragraph; we have also edited this to clarify that the optimizer is the same, and would be happy to add additional detail to this description.\n3. The loss function for the first problem (complex networks) is dictated by the problem statement itself: We are establishing that one can learn Wasserstein embeddings that achieve low mean distortion, so our loss is the mean distortion. This is stated in Section 4.1, paragraph 2. Note that distortion is the standard criterion for metric embeddings.\n4. The utility of the word embeddings for scoring semantic similarity of words is described in Section 4.2, paragraph 4, and in Table 2, where we evaluate the word embeddings on several benchmarks. Note also that we compare to five alternative methods.\n\nThank you again for your feedback!  We appreciate your time and look forward to the possibility of sharing our work in ICLR soon.\n\n(Genevay 2018) Aude Genevay, Gabriel Peyr\u00e9, Marco Cuturi. Learning Generative Models with Sinkhorn Divergences. AISTATS 2018.\f", "title": "Thank you for your feedback!"}, "SkgboCbq37": {"type": "review", "replyto": "rJg4J3CqFm", "review": "This paper learns embeddings in a discrete space of probability distributions, endowed with a (regularized) Wasserstein distance.\n\npros:\n\n- interesting idea, nice results, mostly readable presentation.\n- the paper is mostly experimental but the message delivers clearly the paper\u2019s objective\n- the direct visualisation is interesting\n- the paper suggests interesting problems related to the technique\n\ncons:\n\n- to be fair with the technique, the title should mention the fact that the paper minimises a regularised version of Wasserstein distances (Wasserstein -> Sinkhorn ? put \u201cregularised\" ?)\n- and to be fair, the paper should put some warnings related to regularisation -- this is not a distance anymore, sparsity is affected by regularisation (which may affect visualisation). Put some reminders in the conclusion, reword at least the third paragraph in the introduction.\n- the paper could have been a little bit more detailed on Section 2.3, in particular for its third paragraph. Even when it is an experimental paper.\n- the direct visualisation is interesting in the general case but has in fact a problem when distributions are highly multimodal, which can be the case in NLP. This blurs the interpretation.\n- the paper delivers a superficial message on the representation: I do not consider that nice having modes near physical locations (Paris, France) is wrong. It is also a city. However, it would have been interesting to see the modes of \u201ccity\u201d (or similar) to check whether the system indeed did something semantically wrong.\n\nQuestions:\n\n- beyond that last remark comes the problem as to whether one can ensure that semantic hierarchies appear in the plot: for example if Nice was only a city, would we observe a minimal intersection with the support of word \u201ccity\u201d ? (intersection to be understood at minimal level set, not necessarily 0).\n\n", "title": "A simple and interesting idea on how to map data in a discrete (Wasserstein) space", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1gPBWZ8hm": {"type": "review", "replyto": "rJg4J3CqFm", "review": "The paper \u2018Learning Discrete Wasserstein Embeddings' describes a new embedding method that,\ncontrary to usual embedding approaches, does not try to embed (complex, structured) data into an \nHilbertian space where Euclidean distance is used, but rather to the space of probability measures\nendowed with the Wasserstein distance. As such, data are embed on an empirical \ndistribution supported by Diracs, which locations can be determined by a map that is learnt from data.\nInterestingly, authors note a 'potential universality' for W_p(R^3) (from a result of Andoni et al., 2015), \nsuggesting that having Diracs in R^3 could embed potentially any kind of metric on symbolic data. \n\nExperimental validations are presented on graph and word embedding, and a discussion on visualization of \nthe embedding is also proposed (since the Diracs are located in a low dimensional space).   \n\nAll in all the paper is very clear and interesting. The idea of embedding in a Wasserstein space is \noriginal (up to my knowledge) and well described. I definitely believe that this work should be presented\nat ICLR. I have a couple of questions and remarks for the authors:\n - It is noted in section 3.2 that both Diracs location and associated weights could be optimized. Yet the authors \n   chose to only optimize locations. Why not only optimizing the weights (as in an Eulerian view of probability\n   distributions) ? The sentence involving works of Brancolini and Claici 2018 is not clear to me. Why weighting \n   does not improve asymptotically the approximation quality ? \n - Introducing the entropic regularization is mainly done for being able to differentiate the Wasserstein \n   distance. However, few is said on the embeddability of metrics in W^\\lambda_p(R). Is using an entropic \n   version of W moderating the capacity of embedding ? At least experimentally, a discussion could be made  \n   on the choice of the regularization parameter, at least in section 4.1. In eq. (9), it seems that it is not \n   the regularized version of W. ? \n - I assume that the mapping is hard to invert, but did the authors tried to experiment reconstructing an object \n   of interest by following a geodesic in the Wasserstein space ?  \n - It seems to me that authors never give generalization results. What is the performance of the metric approximation \n   when tested on unseen graphs or words ? This point should be clarified in the experiment.        \n\n", "title": "A very nice and original work.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1gutHkNnX": {"type": "review", "replyto": "rJg4J3CqFm", "review": "The paper proposes embedding the data into low-dimensional Wasserstein spaces. These spaces are larger and more flexible than Euclidean spaces and thus, can capture the underlying structure of the data more accurately. However, the paper simply uses the automatic differentiation to calculate the gradients. Thus, it offers almost no theoretical contribution (for instance, how to calculate these embeddings more efficiently (e.g. faster or more efficient calculation of the Sinkhorn divergence), how to motivate loss functions than can benefit the structure of the Wasserstein spaces, what the interpretation of phi(x) is for each problem, e. g. word embedding, etc.). Additionally, the experiments are unclear and need further improvement. For instance, which method is used to find the Euclidean embedding of the datasets? Have you tried any alternative loss functions for the discussed problems? Are these embeddings useful (classification accuracy, other distortion measure)? How does the 2-D visualizations compare to DR methods such as t-SNE? What is complexity of the method? How does the runtime compare to similar methods?", "title": "Very Limited Contribution", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}