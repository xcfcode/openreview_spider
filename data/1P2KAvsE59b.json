{"paper": {"title": "Robustness to Pruning Predicts Generalization in Deep Neural Networks", "authors": ["Lorenz Kuhn", "Clare Lyle", "Aidan Gomez", "Jonas Rothfuss", "Yarin Gal"], "authorids": ["~Lorenz_Kuhn1", "~Clare_Lyle1", "~Aidan_Gomez1", "~Jonas_Rothfuss1", "~Yarin_Gal1"], "summary": "We demonstrate empirically that a neural network's robustness to pruning is highly predictive of its generalization performance.", "abstract": "Why over-parameterized neural networks generalize as well as they do is a central concern of theoretical analysis in machine learning today. Following Occam's razor, it has long been suggested that simpler networks generalize better than more complex ones. Successfully quantifying this principle has proved difficult given that many measures of simplicity, such as parameter norms, grow with the size of the network and thus fail to capture the observation that larger networks tend to generalize better in practice.\nIn this paper, we introduce a new, theoretically motivated measure of a network's simplicity: the smallest fraction of the network's parameters that can be kept while pruning without adversely affecting its training loss. We show that this measure is highly predictive of a model's generalization performance across a large set of convolutional networks trained on CIFAR-10. Lastly, we study the mutual information between the predictions of our new measure and strong existing measures based on models' margin, flatness of minima and optimization speed. We show that our new measure is similar to -- but more predictive than -- existing flatness-based measures.", "keywords": ["Generalization", "Pruning", "Generalization Measures"]}, "meta": {"decision": "Reject", "comment": "Summary: \nThe authors propose to predict a neural network classifier's\ngeneralization performance by measuring the proportion of parameters\nthat can be pruned to produce an equivalent network (in terms of\ntraining error). Experimental and theoretical evaluation are provided.\n\n\nDiscussion:\nThe overall opinion in reviews was that the idea is\npotentially interesting, but needs to be pursued further before\npublication, and that the empirical evaluation in particular was\nlacking. That was followed by a detailed discussion, in which authors\nwere able to address a number of concerns, and have provided helpful\nadditional experiments.\n\nRecommendation:\nThis is a potentially interesting paper that is not quite there\nyet. Although reviewers have raised scores in discussion, the case for\nacceptance would still be hard to make. I recommend to reject.\n\nIt looks like a reasonable  amount of additional work will turn this\nfrom what is now on the weak end of borderline into a potentially strong\nsubmission, especially given the thoughtful and thorough feedback from\nreviewers. The next top-tier\nconference deadline is not far away, and I encourage\nthe authors to incorporate the feedback fully and resubmit soon.\nThat being said, I agree with reviewers that the theory provided is,\nat present, not strong. Also, a point that still seems to require work\nis the relation between prunability and the use of dropout.\n\nNote to authors and chairs:\nAnonReviewer3 explicitly stated in\ndiscussion that they would raise their score from 5 to 6, but the\nchange was not recorded in the system. My recommendation assumes their score\nis 6."}, "review": {"GTL93KJdg_y": {"type": "review", "replyto": "1P2KAvsE59b", "review": "The paper proposes a novel generalisation measure, i.e., measurement that indicates how well the network generalises, based on pruning. The idea is to measure the fraction of the weights that can be pruned (either randomly, or based on the norms) without hurting the training loss of the model. The paper provides thorough discussion of the related methods and motivates the measure in multiple ways. Further, the authors show empirical evidence for the correlation of the pruning robustness to the generalisation ability of networks, based on the paper by Jiang et al., 2019 and dataset (updated with additional models) provided in the paper.\n\nThe paper is well written and states a clear goal of introducing and proving a generalisation measure based on pruning. The authors provide a nice discussion and lots of empirical evidence. Nonetheless, several points of motivation for the measure seem unclear to me:\n\n- the authors refer to Jiang et al., 2019 saying that there are lots of generalisation measures that correlate with performance, but that they fail to explain the test performance - which calls for another measure. If the experiments shown in Jiang et al. are demonstrating failure of explaining test performance, then the presented paper also does not provide many more evidences that the proposed measure is not failing.\n- the motivation for pruning as a way to improve generalisation is connected to the training procedures - dropout and lottery ticket hypothesis. Nevertheless, none of these methods improve generalisation when applied on top of already trained network. I would not say that lottery ticket retraining can be classified as integration of pruning into optimisation, as well as dropout improves training only when all the network is used afterwards.\n\nThere are several failures, that make me believe that more work can improve the paper:\n- The goal of the paper is to show a measure that will perfectly predict generalisation - but according to the experiments it can be outperformed by other measures on the presented dataset.\n- The theoretical justification seem unclear to me: what is the goal of introducing the generalisation bound (moreover even Appendix does not have details of derivation of the presented formula) if the authors notify themselves right away that it is vacuous. The justification is to give an intuition of how pruning connects to generalisation. It is unclear to me though, how it can be concluded based on a vacuous bound?\n- The idea to check the measures behaviour in double descent setup is very interesting, but only one measure is checked there and in a different experimental setup, without proper motivation for such change.\n- Section5.4.3 attempts to analyse casual connection between existing measures, that seems to me unclear by motivation as well - one wants to see causal connection to generalization, not other measures. Especially, that the table3 is discussed only for the random perturbations measurement - still not providing an answer what type of the connection is there between two.\n\nI would suggest to reject the paper, since the idea feels not being worked through enough. \n\nMinor comments:\n\n1 - it would be nice to have a discussion on the type of pruning used - does it somehow change the measurements in a predictable way?\n\n2 - typo in the first sentence of section3 (twice \u201cdenote\u201d) \n\n3 - typo in the first sentence of the second paragraph of section3 (letter denoting data distribution)\n\n4 - table1 misses highlights of the \u201cwinning\u201d approaches\n\n***\n\nI would like to thank authors for accurate answers and a lot of work put on reworking the paper. Unfortunately, I still find my concerns about motivation for the metric valid, which together with the rather weak performance creates a problem for this paper. I highly encourage authors to continue the work and try to explain the reasons for this correlation and find justifications for usage of the metric. ", "title": "Pruning as Generalisation Measure", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "leRSh2_60Nk": {"type": "review", "replyto": "1P2KAvsE59b", "review": "The authors consider the problem of estimating generalization in deep neural networks, and propose a measure based on the ability to set a fraction of the neural network weights to zero (prunability). The authors introduce some theoretical motivation based on the PAC-Bayesian framework, and perform an empirical evaluation based on a set of convolutional networks trained on the CIFAR-10 dataset.\n\nThe problem of understanding generalization in deep neural networks is a fundamental problem of deep learning, and the results obtained by the authors present a potentially interesting perspective on the study of generalization in deep neural networks. The paper is well-written, and presents some interesting empirical results. However, the theoretical contribution is not  particularly novel, and is not complete enough to justify all the measures evaluated in paper. Due to this issue, I feel that the paper only has limited impact. I detail my comments on the theoretical and empirical results of the paper below.\n\nOn the theoretical side, the contribution for the case of \u201crandom pruning\u201d is minor and is a straightforward extension of known results. Indeed, \u201crandom pruning\u201d is similar to many other well-studied random perturbation schemes, and corresponds exactly to the case of dropout, a well-understood method. On the other hand, there does not appear to be any bound provided for the case of \u201cmagnitude pruning\u201d, and it is not immediately obvious why such a measure should lead to a generalization bound. Indeed, existing work on using pruning and compression for measuring generalization (Arora et al. 2018, Zhou et al. 2019) establish bounds on the *pruned* network, and not the original network. Establishing a bound in terms of the \u201cmagnitude pruning\u201d measure would be an interesting contribution, as handling such non-random modifications has been a challenge in the community. However, the authors do not seem to make any attempt at providing such a bound or a heuristic argument for such a bound to hold. Finally, the choice of measuring the prunability of a network as a proportion requires more careful justification in the context of magnitude pruning. Indeed, for \u201crandom pruning\u201d, the proportion is easily interpreted as a magnitude of noise injected. However, in the \u201cmagnitude pruning\u201d setup, with the parallels the authors draw to Occam\u2019s razor and compression ideas, it is the absolute number of parameters which is more theoretically relevant than the proportion of parameters which can be eliminated: having 1 million parameters where half can be eliminated is still more complex than only having 100,000 parameters (where none can be eliminated).\n\nOn the empirical side, I found that the methodology was clear and well-adapted for the \u201crandom pruning\u201d method, which is closely related to drop-out and other random perturbation ideas. With the addition of networks of different depth in the set of networks used for evaluation, the empirical methodology also seems appropriate for the \u201cmagnitude pruning\u201d method and the parallels the authors draw to compression. However, it is not obvious to me that the empirical results support that connection, as we know that in architectures which vary substantially in size (e.g. MobileNet or EfficientNet), the compressibility (i.e. the proportion of parameters which can be pruned) is directly related to the size of the network (see e.g. Gupta et al. 2017), and I feel that this is a further indication that the choice of using the proportion of pruned parameters should be more carefully discussed.\n\nThe presentation of the empirical result could be improved by including (either in the main text or the appendix) the standard errors for the measured correlation for all measures, and ensuring that the tables are formatted similarly to the ICLR template for better readability (the latex package booktabs can be used for this purpose).\n\nOther notes: please ensure that you cite published versions of papers when they are available (instead of the arxiv pre-prints). For example, Arora et al. 2018 appeared at ICML 2018, and Zhou et al. 2018 [sic] appeared at ICLR 2019 (there are many other such cases in the references).\n\n=====================\n\nEdited after author response: I thank the authors for their considerate responses. Overall, my opinion remains mostly unchanged, and I share similar opinions to reviewer 3 and 4 that although the proposed idea is interesting and intriguing, the paper is not quite ready at this point. I would like to see the authors present either: 1) stronger empirical evidence for the importance of their metric or 2) a more solid theoretical foundation of the measure they propose.\n", "title": "Interesting ideas with potential but weak theoretical justifications", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "zU2Y5GK5N1K": {"type": "review", "replyto": "1P2KAvsE59b", "review": "## Paper summary\n\nIn order to understand why deep networks generalize well, this paper proposes \"prunability\" as an empirical measure that can be predictive of the generalization. Prunability is roughly the smallest _fraction_ (i.e., $\\in [0,1]$) of parameters that can be retained, while zeroing out everything else, without increasing the model's training loss by too much. The authors experimentally demonstrate the predictive ability of this measure in three ways.\n\n1. They consider **the large-scale empirical framework of Jiang et al' 2019** where one computes different statistics about \"generalization vs. the measure\" from a large pool of trained models (viz., DEMOGEN dataset from Jiang et al' 2018). They compare prunability against four other existing measures:  Frobenius norms, random perturbation robustness (a flatness-based measure), Normalized margins (a layer-wise margin measure), the average loss across training (a speed-of-optimization based measure). \n\n Then they compute three different previously-proposed statistics:\n   - a) Kendall's rank correlation coefficient (Jiang et al' 2019) which tells us how well the measure can rank the models: **here prunability performs better than the flatness measure, and much better than the norm and speed measures.** (However, normalized margins outperform everything)\n   - b) adjusted $R^2$: here prunability performs just as well as other measures (although normalized margins outperform again.)\n   - c) a conditional mutual information (CMI) term Jiang et al 2019 that tells us whether the measure has a causal role in the generalization behavior: prunability's CMI is pretty low revealing poor causal connections.\n   \n2. They conduct Maddox et al., 2020's experiment where one evaluates the measure for varying widths to see whether it can **capture the double descent behavior of the test loss**. They observe that prunability does show a double descent behavior, while Maddox et al., 2020's flatness-based \"effective dimensionality\" only shows an \"ascent-descent\" behavior. \n\n3. Finally they demonstrate that **prunability captures something different from all the other measures.** They do this by showing that there is little causal connection between prunability and all other measures except the flatness-based random-perturbation-robustness measure. Then they go on to show that pruning and random perturbations affect models differently. Notably, pruning can lower the test loss of the network while random perturbations always only increases the test loss.\n\n\n## Strengths\n\n1. The idea that compressibility relates to generalization is not new and has been theoretically quantified via generalization bounds. However, such bounds are still parameter count dependent and/or they're computed for a handful of models and it's not clear how well they correlate with generalization.  The paper takes an orthogonal direction towards relating compressibility and generalization: it pins down an empirical measure of compressibility, and then \nprovides three sufficiently different kinds of arguments to demonstrate the usefulness of that metric. Although the experiments used within these arguments in themselves are not novel, I think the fact the metric holds up in all these tests is interesting.\n\n2. The way this paper quantifies prunability --- in terms of the fraction of parameters --- is simple and also somewhat thought-provoking (why should generalization be related to the fraction of parameters?).\n\n\n3. The paper is honest and rigorous in terms of the values it reports: prunability is not the best of all metrics, and the paper is transparent about it. The paper also gives sufficient credit to work that it builds upon. The writing is smooth.\n\n## Weaknesses\n\n4. Given that this is a purely empirical paper, I'd have appreciated if the observations made in the double-descent experiment and \"the effect of pruning-vs-perturbation on test loss\" experiment, were also shown in at least one other dataset/architecture.\n\n\n## Overall opinion\n\nThis paper provides multiple different pieces of evidence backing its claim that prunability predicts generalization. These empirical observations are rigorous and would be valuable in understanding the generalization puzzle. This way of quantifying prunability might also open up new theoretical questions. Hence, I think this is a good paper worth publishing.\n\n## Clarification questions\n\n5. Could you explain why the last few experiments are reported in terms of the cross-entropy loss? Is it because the corresponding plots for the 01 error do not show as much different between the lines? Nevertheless, I feel that it'd be nice to have those plots in the paper too (no pressure to produce those plots during rebuttal). \n\n6. Could you clarify the claims in the first paragraph of 5.4.1? Specifically \n>\"prunability is highly informative of generalization across all of our evaluation metrics  outperforming random perturbation robustness, the training loss itself and the Frobenius norm measures\" \n\nseems to contradict later observations that on some evaluation metrics they are all just as good as each other on adjusted $R^2$.\n\n### Minor suggestions\n\n- Under Table 1 and 2 would be nice to remind the reader as to whether larger values indicate better predictivity or not.\n- Is the usage of adjusted $R^2$ inspired by Jiang et al., 2018? If so, would help to cite them appropriately.\n- Page 12 typo \"Vargins\" --> \"Margins\"\n\n### Suggested citation\n\n\"On the importance of single directions for generalization\" Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, Matthew Botvinick, ICLR 2018 https://arxiv.org/abs/1803.06959   -- they empirically study how generalization is related to how many hidden units you can zero out. (Certainly not the same as what the submission suggests, but I think worth citing.)\n\n### References \n- Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic\ngeneralization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.\n\n- Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization gap\nin deep networks with margin distributions. arXiv preprint arXiv:1810.00113, 2018.\n\n- Wesley J. Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking parameter counting\nin deep models: Effective dimensionality revisited, 2020.\n\n\n**Update:** The authors clarified all my questions very well. They also added an extra plot for the double descent experiment on a different architecture (ResNet). Although I feel a bit lukewarm about the added plot (in that the double descent phenomenon is only somewhat weakly reflected by their empirical measure), I've increased my confidence score from 3 to 4 to appreciate their efforts in addressing my concerns. Good luck to the authors!", "title": "Solid empirical observations connecting prunability to generalization in deep learning", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "iljRiYSZOg": {"type": "rebuttal", "replyto": "8sm3oGpNGuB", "comment": "Dear reviewer, \n\nWe are happy to hear that you find our additional experiments convincing, that we were able to address your concerns and that you will raise your score to a 6. Thank you very much.\n\nTo communicate our current understanding of the relation between prunability and dropout more clearly, we've added an additional comment at the end of the Related Work section in our latest revision. We agree that studying this relation more closely would be an interesting direction for future research. \n\nThank you, \n\nAuthors of Paper3241", "title": "Response to AnonReviewer3"}, "AE-po61X3en": {"type": "rebuttal", "replyto": "ZDbmqBI0qcq", "comment": "Dear reviewer, \n\nThank you very much for your comment.\n\nWe very much agree with the points you raise. To make our position with regards to our theoretical contribution even more clear, we extended the conclusion to state \u201cGiven the strong empirical performance of prunability \u2013 and given that the theory behind its success is not yet well understood \u2013 this measure of model complexity may be of use to the construction of future generalization bounds\u201d. \n\nThank you for the suggestion,\n\nAuthors of paper 3241\n", "title": "Response to AnonReviewer2's comment on AnonReviewer1's review"}, "j1IPfFNabpC": {"type": "rebuttal", "replyto": "jQffr-73GYN", "comment": "Dear reviewer, \n\nThank you for your response!\n\nIn addition to the large scale study and the double descent experiment included in the first version of our paper, we\u2019ve now also evaluated prunability in an additional double descent setting and evaluated additional strong baselines in these settings (see Figure 1 and Appendix B). The additional experiments seem to be consistent with the results of our large scale experiments: prunability is highly predictive of generalization and is competitive with some of the strongest known generalization measures.\n\nWe agree that additional theoretical work will be required to properly understand prunability and its relation to other phenomena observed in deep neural networks. We further agree with the reviewer that our empirical results are not a corollary of known properties of dropout and iterative magnitude pruning; we will clarify in future revisions that while prior work on these techniques suggests that pruning and generalization are connected, our work presents a distinct and novel contribution in evaluating prunability in its own right. We strongly believe that our empirical results in their current form are valuable as a stepping stone and lay necessary groundwork for further theoretical work. \n\nWe do hope that in light of our additional results and clarification of our narrative you would consider raising your score as we feel our work presents a valuable addition to the field.\n\nThank you, \n\nAuthors of Paper3241\n", "title": "Response to AnonReviewer4"}, "m-N4Gc2tfI9": {"type": "rebuttal", "replyto": "T80MQ_y-98P", "comment": "Dear reviewer, \n\nThank you very much for your feedback and for increasing your confidence score!\n\nIt seems indeed like we accidentally didn\u2019t include the correction for the phrase you quote. We have now changed it to: \u201cIn particular, it outperforms random perturbation robustness, the training loss itself and the Frobenius norm measures in terms of Kendall's rank correlation coefficient.\u201d\n\nWe added the additional experiments AnonReviewer4 suggested to the latest revision of the paper (see Figure 1 and Appendix B). We would be very interested in hearing your thoughts on them.\n\nThank you, \n\nAuthors of Paper 3241\n", "title": "Thank you for your comment"}, "6Y3S5UFEKs4": {"type": "rebuttal", "replyto": "9OoJTTq-7u", "comment": "[see the previous comment for the first part of our response]\n\nYou write: \n* \u201cHowever, in the \u201cmagnitude pruning\u201d setup, with the parallels the authors draw to Occam\u2019s razor and compression ideas, it is the absolute number of parameters which is more theoretically relevant than the proportion of parameters which can be eliminated: having 1 million parameters where half can be eliminated is still more complex than only having 100,000 parameters (where none can be eliminated)\u201d\n\nGenerally, it seems difficult to reason about model complexity in terms of the numbers of their parameters, given that parameter counting has been shown to be not very informative of model\u2019s generalization power (see Jiang et al., 2020, for instance). Furthermore, we do study model\u2019s of different sizes (we compare models of different depth and width) and across these the fraction of parameters that can be kept is indeed predictive whereas the absolute number of parameters is not. Lastly, even if we assume that looking at the number of parameters is informative, it is not self-evident that a larger but highly sparse model would be more complex than a smaller, dense model according to many learning-theoretic measures. For example, in the random-pruning setting, it is not obvious to us that the set of functions which can be computed by a given architecture with $n$ parameters is necessarily smaller than the set of functions that can be computed by an architecture with $2n$ parameters, of which any $n$ may be removed without affecting the function\u2019s loss.\n\t\nWe have updated our citations to refer to the published versions of resources where applicable. Additionally, for the reader\u2019s convenience, we\u2019ve added URLs for all resources.\n\nThank you for the suggestions of including the standard errors, we\u2019ll adapt the next revision of the paper accordingly.\n\nThanks again, \n\nAuthors of Paper3241\n\nYiding Jiang*, Behnam Neyshabur*, Hossein Mobahi, Dilip Krishnan, and Samy Bengio.  Fantastic generalization measures and where to find them. In International Conference on Learning Representations, 2020\n", "title": "Response to AnonReviewer1 \u2013 Part II"}, "9OoJTTq-7u": {"type": "rebuttal", "replyto": "leRSh2_60Nk", "comment": "Dear reviewer, \n\nThank you for your review and your thoughtful comments. We\u2019d like to address some of the points you raise. Please let us know whether our replies make sense to you or whether you would like further clarifications. \n\nWith regards to our theoretical justification/motivation you write:\n* \u201cOn the theoretical side, the contribution for the case of \u201crandom pruning\u201d is minor and is a straightforward extension of known results. [...] Establishing a bound in terms of the \u201cmagnitude pruning\u201d measure would be an interesting contribution, as handling such non-random modifications has been a challenge in the community. However, the authors do not seem to make any attempt at providing such a bound or a heuristic argument for such a bound to hold.\u201d\n\nOur view on this is the following:\n* We agree that our theoretical justification for robustness to pruning is not our main contribution. We do, however, strongly believe that it is highly valuable to empirically identify new aspects of deep learning models that relate to generalization. We think that the explanation AnonReviewer2 gives under 1) in the Strengths section nicely summarizes our position on this.\n\nFurthermore, you write:\n*  \u201cFinally, the choice of measuring the prunability of a network as a proportion requires more careful justification in the context of magnitude pruning. Indeed, for \u201crandom pruning\u201d, the proportion is easily interpreted as a magnitude of noise injected.\u201d\n\nOur view on this is the following:\n* We agree with you that we could have more clearly motivated the step from studying random pruning to magnitude pruning. Magnitude pruning can remove a greater number of model weights without affecting performance, and can therefore be considered a more effective pruning strategy. We hypothesized that using a more effective pruning method might yield a measure of prunability that better predicts generalization. We will add this to a revision of the paper, thank you for the suggestion.\n\nWith regards to our empirical contribution you write:\n* \u201cOn the empirical side, I found that the methodology was clear and well-adapted for the \u201crandom pruning\u201d method, which is closely related to drop-out and other random perturbation ideas. With the addition of networks of different depth in the set of networks used for evaluation, the empirical methodology also seems appropriate for the \u201cmagnitude pruning\u201d method and the parallels the authors draw to compression. However, it is not obvious to me that the empirical results support that connection, as we know that in architectures which vary substantially in size (e.g. MobileNet or EfficientNet), the compressibility (i.e. the proportion of parameters which can be pruned) is directly related to the size of the network (see e.g. Gupta et al. 2017), and I feel that this is a further indication that the choice of using the proportion of pruned parameters should be more carefully discussed.\u201d\n\nOur view on this is the following:\n* We agree that the compressibility of models of different sizes varies a lot. While we do not compare different architectures of different sizes, we do compare models of the same architecture with widely different numbers of parameters by varying both the depth and the width. Across these models of different sizes prunability does indeed seem predictive of generalization.\n\n[we continue our response below]\n", "title": "Response to AnonReviewer1 \u2013 Part I"}, "jBtVsLYk-_Q": {"type": "rebuttal", "replyto": "BMB5_bN5Qm7", "comment": "[see the previous comment for the first part of our response]\n\nWith regards to our experiments studying the conditional mutual information you write:\n* \u201cSection 5.4.3 attempts to analyse casual connection between existing measures, that seems to me unclear by motivation as well - one wants to see causal connection to generalization, not other measures. Especially, that the table3 is discussed only for the random perturbations measurement - still not providing an answer what type of the connection is there between two.\u201d\n\nOur motivation for studying the conditional mutual information between prunability and the baseline generalization measures is to what extent prunability can already be explained by the baselines. \n\nIndeed, we do not provide a conclusive answer about the connection between prunability and random perturbation robustness. Rather, we point out that even though they seem related,  pruning and random weight perturbations seem to impact models\u2019 losses quite differently. Based on this, we suggest that prunability deserves to be studied as a novel generalization measure \u2013 particularly given that it seems to outperform random perturbation robustness.\n\nWe are working on the minor changes you suggest and will integrate them into a later revision of our paper, thank you!\n\nThank you,\n\nAuthors of Paper3241\n", "title": "Response to AnonReviewer4 \u2013 Part II"}, "BMB5_bN5Qm7": {"type": "rebuttal", "replyto": "GTL93KJdg_y", "comment": "Dear reviewer, \n\nThank you for taking the time to write a thorough review and the encouraging feedback. We are glad that you enjoyed our empirical contributions and we\u2019d like to address the issues you mention. Please do let us know what you think about our replies below.\n\nWith regards to the goals and results of our empirical study you write:\n* \u201cIf the experiments shown in Jiang et al. are demonstrating failure of explaining test performance, then the presented paper also does not provide many more evidences that the proposed measure is not failing.\u201d\n* \u201cThe goal of the paper is to show a measure that will perfectly predict generalisation - but according to the experiments it can be outperformed by other measures on the presented dataset.\u201d\n\nWe don\u2019t make the claim that our generalization measure is the single best generalization measure, nor is it our goal to find a generalization measure that perfectly explains generalization.\n\n We do, however, strongly believe that it is highly valuable to identify new aspects of deep learning models that seem related to generalization, even if they don\u2019t explain generalization entirely by themselves. We see our study of prunability as just such a contribution. We think that the explanation AnonReviewer2 gives under 1) in the Strengths section summarizes this idea very nicely.\n\nWith regards to the motivation we provide in the paper you write:\n* \u201cthe motivation for pruning as a way to improve generalisation is connected to the training procedures - dropout and lottery ticket hypothesis. Nevertheless, none of these methods improve generalisation when applied on top of already trained network. I would not say that lottery ticket retraining can be classified as integration of pruning into optimisation, as well as dropout improves training only when all the network is used afterwards.\u201d\n\nIt is true that we do not provide a way of improving the generalization of models once they are trained but this is also not the ambition of our paper, as described above, nor is it a goal of existing work on generalization of neural networks.  Further, it is true that random pruning  \u2013 in the form of dropout \u2013  is already widely used in training schemes. However, randomly dropping out a fixed percentage of a model\u2019s weights during training only indirectly optimizes prunability, leaving open the possibility of more effective approaches. For example, two training heuristics which more directly optimize prunability would be to drop out the highest-magnitude weights, or to increase the percentage of dropped out weights over the course of training. \n\nWith regards to the theoretical justification for studying pruning you write:\n* \u201cThe theoretical justification seem unclear to me: what is the goal of introducing the generalisation bound (moreover even Appendix does not have details of derivation of the presented formula) if the authors notify themselves right away that it is vacuous. The justification is to give an intuition of how pruning connects to generalisation. It is unclear to me though, how it can be concluded based on a vacuous bound?\u201d\n\nThe motivation behind our inclusion of the bound is to demonstrate that prunability is consistent with existing generalization theory: networks which are more prunable obtain lower PAC-Bayesian generalization bounds. So while the bound is, like most generalization bounds, likely to be vacuous in neural networks, it predicts that prunability should be predictive of generalization in terms of how it ranks models with different generalization performance. This ranking, then, is what we test in our experiments. We do agree that it would have been useful to include this explanation in the paper, so we added it to our latest revision in the paragraph below equation 1).\n\nWith regards to our double descent experiments you write:\n* \u201cThe idea to check the measures behaviour in double descent setup is very interesting, but only one measure is checked there and in a different experimental setup, without proper motivation for such change.\u201d\n\nThis is a good point, we will evaluate additional baselines in the double descent setting and add them to the paper in the coming week. Thank you for the suggestion.\nWe use two different experimental set ups that were designed for different purposes. One of them was designed to obtain a range of realistic models with a wide range of generalization performances by varying a number of relevant hyperparameters. The other setup was particularly chosen to obtain a test loss double descent which is only observed for a few particular configurations. The motivation then is: 1.) Prunability is informative of generalization in a general case and 2.) prunability is predictive of generalization even in a particularly challenging setting.\n\n[we continue our response below]", "title": "Response to AnonReviewer4 \u2013 Part I"}, "7KHC3NsCiL8": {"type": "rebuttal", "replyto": "zU2Y5GK5N1K", "comment": "Dear reviewer,\n\nThank you for your review \u2013 we are happy to hear that you enjoyed our paper. We very much agree with your assessment of the strengths and weaknesses of our paper.\n\nWe\u2019d like to briefly address the questions you raise. We would be very interested to hear what you think about our responses.\n\nYou write:\n* \u201cGiven that this is a purely empirical paper, I'd have appreciated if the observations made in the double-descent experiment and \"the effect of pruning-vs-perturbation on test loss\" experiment, were also shown in at least one other dataset/architecture.\u201d\n\nYes, we agree that it makes sense to extend the double descent and pruning-vs-random-perturbation experiments. We have added one additional experiment for each of these in Appendix B of the paper and we find that the results of the additional experiments are consistent with the results of the previous experiments. Thank you for the suggestion.\n\nYou ask:\n* Could you explain why the last few experiments are reported in terms of the cross-entropy loss? Is it because the corresponding plots for the 01 error do not show as much different between the lines? Nevertheless, I feel that it'd be nice to have those plots in the paper too (no pressure to produce those plots during rebuttal).\n\nThe experimental set up of our double descent experiment is directly taken from Maddox et al., 2020. In this setting, we study the cross-entropy rather than the 1-0 loss because that is the metric for which the double descent has been observed. While this is not the main focus in these experiments, we also report the correlation between the generalization measures and the test error in Figure 1b).\n\nYou ask:\n* Could you clarify the claims in the first paragraph of 5.4.1? Specifically \"prunability is highly informative of generalization across all of our evaluation metrics outperforming random perturbation robustness, the training loss itself and the Frobenius norm measures\" seems to contradict later observations that on some evaluation metrics they are all just as good as each other on adjusted R2\n\nThanks for pointing out the imprecision of our statement with regards to the performance in terms of Adjusted R2 of the different measures. Indeed, the norm-based measure seems to outperform the other mentioned measures in terms of Adjusted R2. We\u2019ve added a clarifying comment to the paper, thanks.\n\nDoes this make things clearer to you?\n\nThank you, we will add the citation you suggest and do the minor fixes in the next revision of the paper.\n\nThanks again, \n\nAuthors of Paper3241\n\nWesley J. Maddox, Gregory Benton, and Andrew Gordon Wilson.  Rethinking parameter counting in deep models: Effective dimensionality revisited, 2020. \n", "title": "Response to AnonReviewer2"}, "ZXvr2DKKPj4": {"type": "rebuttal", "replyto": "hFd_ZzW8e0k", "comment": "Dear reviewer,\n\nThank you for taking the time to engage with our paper and the encouraging comments. We are happy to hear that you think we\u2019re pursuing a promising research direction.\n\nTo summarize our response: we believe that our proposed generalization measure is worth including in the generalization measure debate precisely because it is competitive with some of the few generalization measures that actually *work well* and because it is sufficiently different from existing measures, thus adding another piece to the puzzle of model generalisation.\n\nIn the following, we would like to address the questions you raise. We would greatly appreciate to hear your thoughts on our replies!\n\nParticularly, you write:\n* \u201cIn my opinion, the idea behind this complexity measure makes a lot of sense, but: 1) it is already used explicitly in dropout, so I don't see how it could inspire the development of better training heuristics. 2) It is not easier to measure that most other complexity measures (in the random version). 3) It is closely related to pre-existing metrics. 4) More importantly, it is not more predictive of generalization than some alternatives already discussed in the literature. Therefore, I think that this work lacks at least one strong point that could motivate the inclusion of this latest complexity measure into the generalization debate.\u201d\n\nWe are thinking about these points as follows:\n1) It\u2019s true that random pruning  \u2013 in the form of dropout \u2013  is already being used during training. However, randomly dropping out a fixed percentage of a model\u2019s weights during training only indirectly optimizes prunability, leaving open the possibility of more effective approaches. For example, two training heuristics which more directly optimize prunability would be to drop out the highest-magnitude weights, or to increase the percentage of dropped out weights over the course of training. While this is not the main focus of our work, we do see our contribution as a stepping stone towards further research on such methods.\n2) We agree with your point that prunability is similarly expensive to measure as some of the other generalization measures we evaluate. It is however, much less expensive than the best-performing, margins-based generalization measure which requires the training of a large number of models to train the linear regression on which this measure is based.\n3) AnonReviewer2, nicely summarizes our view on this. We would argue that our measure is novel in a number of ways: first, we establish a particular connection between pruning and generalization that has not been described before; second, we establish a way of formulating a generalization measure based on a model\u2019s number of parameters \u2013 which has proved difficult so far; third, we do show some preliminary evidence that our measure does seem to measure something different -- and more useful -- than the seemingly related random perturbation robustness. \n4) Lastly, we agree that prunability is not the single best known generalization measure. Finding the most predictive generalization measure is one of the goals of the generalization literature broadly. Another important goal (one which is instrumental to the first, but also interesting as a scientific question in its own right) is to _understand_ generalization. It is possible to contribute to the latter without aiming for the first of these goals. We see our study of prunability as just such a contribution.  Prunability is a generalization measure that is competitive with some of the strongest known generalization measures \u2013 and seems different from them. It clearly outperforms random perturbation robustness, for instance, which in turn was one of the strongest out of the 40 or so studied in the Fantastic Generalization measures paper. Note that while the margin-based generalization measure is the most predictive baseline measure we study, it also has its disadvantages. It is, for instance, data set specific and would thus have to be retrained to be used on a new data set. See also AnonReviewer2\u2019s framing of topic, with which we very much agree. \n\nTo summarize, we believe that prunability is in fact a very strong (although not the strongest), novel generalization measure and that it would be valuable to add to the discussion around generalization measures. \n\nThanks again, \nAuthors of Paper3241\n", "title": "Response to AnonReviewer3"}, "hFd_ZzW8e0k": {"type": "review", "replyto": "1P2KAvsE59b", "review": "In the present work, the authors tackle the highly debated (and sometimes confusing) problem of finding a good simplicity/complexity measure able to predict generalization performance of deep networks. A novel measure called 'prunability' is introduced and compared with some of the many alternatives in the literature. This property measures how networks are able to retain low training loss when a fraction of the weights is set to zero, and is clearly related to common training practices (e.g. dropout) that seems to yield better generalization performance in practice. The experimental settings and the evaluation methods for this new metric are inspired by recent extensive studies on deep networks performance. The authors are able to show that prunability is in fact associated with good generalization and seems able to capture some non-trivial phenomena (double-descent), but they also find it to be inferior to pre-existing (margin based) measures. Moreover, the close relationship to perturbation robustness and flatness measures is investigated, but the results are not fully conclusive.\nIn my opinion, the idea behind this complexity measure makes a lot of sense, but: 1) it is already used explicitly in dropout, so I don't see how it could inspire the development of better training heuristics. 2) It is not easier to measure that most other complexity measures (in the random version). 3) It is closely related to pre-existing metrics. 4) More importantly, it is not more predictive of generalization than some alternatives already discussed in the literature. Therefore, I think that this works lacks at least one strong point that could motivate the inclusion of this latest complexity measure into the generalization debate.", "title": "I think that the authors propose a sensible research direction, but the presented results might be insufficient for justifying the introduction of an nth complexity measure in the generalization debate. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}