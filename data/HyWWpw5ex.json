{"paper": {"title": "Recurrent Coevolutionary Feature Embedding Processes for Recommendation", "authors": ["Hanjun Dai*", "Yichen Wang*", "Rakshit Trivedi", "Le Song"], "authorids": ["hanjundai@gatech.edu", "yichen.wang@gatech.edu", "rstrivedi@gatech.edu", "lsong@cc.gatech.edu"], "summary": "Our work combines recurrent neural network with point process models for recommendation, which captures the co-evolution nature of users' and items' latent features.", "abstract": "Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multi-dimensional point process model. The RNN learns a nonlinear representation of user and item embeddings which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning parameters. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts. ", "keywords": ["Deep learning", "Applications"]}, "meta": {"decision": "Reject", "comment": "A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method."}, "review": {"Sy3Ek1w8x": {"type": "rebuttal", "replyto": "HyWWpw5ex", "comment": "Dear reviewers, we have revised our paper according to your insightful suggestions and comments.\n\n1) We highlight the importance and difficulty of modeling the nonlinearity in the point process models in the introduction part. \n\n2) We added the discussion with Chen et.al ICML 2013 and a detailed comparison with Wang et.al NIPS 2016.\n \n3) We added the experiment on the large Yelp dataset, which contains 1,005 users and 47,924 items. We run our algorithm and all the baselines on this one. The result of our algorithm is consistently better than alternatives. \n\n4) We added the section 6.4.1 explaining the quantitative results we get in the experiment. Specifically, we studied the performance of different history length of users, and how the diversity of tastes of users affect the results by visualizing the user-item interaction graph. \n\n5) We also added the section 6.4.2 that quantitatively compare the effect of different history length on Yelp dataset. \n", "title": "Paper Revision"}, "ryu48_BHe": {"type": "rebuttal", "replyto": "r10fdSBNe", "comment": "Thank you very much for your reviews.\n\nA. Comparison between (Wang et.al 2016) \n\t\nOur work is significantly different in both modeling and learning: \n\n1. (Wang et.al 2016) uses a specific and simple process (Hawkes process) to model the evolution of features, which corresponds to a *linear* summation of history features. Our work is more expressive and general since we use the *nonlinear* recurrent framework. Specifically, we use a latent state to capture the histories, and update it using RNN. The RNN aspect makes our work able to capture the nonlinearity in the data hence our work has consistent improvements over (Wang et al 2016) in the prediction tasks.\n\nA recent work (Recurrent Marked Temporal Point Process, Nan et.al 2016)  shows that the RNN is quite flexible to approximate many point process models. This work systematically compared the RNN with many other point process models, such as Hawkes and self-correcting processes. Thus using RNN to capture the coevolutionary feature process gives us a powerful and flexible model.\n\n2. Regarding the # parameters, In (Wang et.al 2016), # parameters are O( #user \\times #item). However, our work only has O(#user + #item) regardless of RNN related parameters.\n\n3. In terms of learning: (Wang et.al 2016) uses a convex optimization which is a batch method. However, we develop a novel stochastic training technique which is *never* explored in this co-evolution dynamic graph scenario before. Our method can also be potentially applied to online setting. \n\n4. In terms of the time prediciton task: (Wang et.al 2016) needs to simulate future events to predict the next event time, which more sampling computation and less accurate. However, we assume the history is encoded in the hidden states of RNN and we can use a parametric distribution which has the closed form of the expected value of next event time.\n\nAccording to the universal approximation theorem, the network with sigmoid (or tanh) function used in our paper is already powerful enough [Cybenko., G. (1989)]. And also, it is straightforward to incorporate more complicated RNN cells, such as GRU or LSTM, to further strengthen the representation power. We also tried different parameterizations of the intensity functions (e.g., the version used in Nan et.al 2016), different dependency of multidimensional point process (e.g., user-centric multidimensional one used in (Wang et.al 2016), versus our symmetric NxM dimensional one),  different activation functions (ReLU has poor performance in this case), and used the best paramertrization choice in our paper.\n\nPlease check http://www.cc.gatech.edu/~ywang/papers/WanDuTriSon16.pdf for the NIPS paper (Wang et al 2016). \n\nB.  Evaluation metric:\n1. To test the item prediction performance, given a triple of the upcoming test event (user U, item I, time T), we ask the question: what is the item the user U will interact at time T? Since we know the true answer is item I, we evaluate the model\u2019s prediction against this groundtruth.\n\n2. Accurate prediction of the returning time of a customer to a specific service is also very useful for the service provider. Although the following important applications are from different domains, they can be well captured by the proposed model: \n\nI. As a web company, like Google and Facebook, time-sensitive recommendations can have potential impact first to display ads. If we can predict when our users will come back next, we can make the existing ads bidding much more economical, allowing marketers to bid on time slots. After all, marketers do not need to blindly bid all time slots indiscriminately. \n\nII. For most online stores, accurate prediction of the returning time of customers can help to improve stock management and products display and arrangement. \n\nIII. For mainstream personal assistants, like Google Now, because people tend to have different activities dependent on the temporal/spatial contexts like morning vs. evening, weekdays vs. weekend, making recommendations on the right thing and at the right moment can make such services more relevant and usable. \n\nIV. In modern electronic health record data, patients may have several diseases that have complicated dependencies on each other. The occurrence of one disease can trigger the progression of others. Predicting the returning time on certain disease can effectively help to take proactive steps to reduce the potential risks.\n\nC. complexity\n1. # parameters: If we use one-hot representation of user and items, then the basic feature embedding would take (#user + #item) \\times embedding_size parameters; the interaction features are independent to #user or #item (e.g., bag of words features); other model parameters are independent of the dataset size, so we won\u2019t have many more parameters than basic MF models. \n\n2. Training complexity: we use stochastic training to handle large datasets, each time we focus on consecutive K samples. When a new event happens, two embeddings gets updated, and #user + #item dimensions will update their constant intensity functions. So ideally we need O((#user + #item) \\times K) to forward the intensity and its integration (survival probability, which has closed form of integration).  However, as we mentioned in the paper, we use NCE to sample C \u2018\u2019negative\u2019\u2019 (i.e., the dimensions survives from last event to current event) dimensions, in order to further reduce the terms of survival probabilities, analogous to NCE used in language model to deal with large vocabulary size. So finally, we update 2K embeddings, and correspondingly compute C*K survival terms.  \n\n3. Testing task complexity: a) for item recommendation, we need O(N) time to compare with each item for the current user; this is unimprovable. b) for time prediction, it takes O(1) since we have closed form of expectation calculation; c) we use RNN to update two embeddings corresponds to the current user and item. It still takes O(1). \n\nIn summary, our method is scalable in both training and testing. During testing, it has same complexity as traditional matrix factorization based methods. \n", "title": "RE: no title"}, "rJasNOSBx": {"type": "rebuttal", "replyto": "r1IyyRbNg", "comment": "Thank you very much for your reviews.\nYour suggestions on the new metric are very interesting, and we will definitely incorporate them in our experiments. \n\nThe first switching time metric is actually similar to our time prediction task. To do this, we can predict the time for each item, then report the one with earliest time as next event.\n\nAs for the second metric, since P(item, time | history) can be factorized as P(item | history) P(time | item, history), we can make the joint prediction according to this formula.", "title": "RE: review for Recurrent Coevolutionary Feature Embedding Processes for Recommendation"}, "HkMOVuSrx": {"type": "rebuttal", "replyto": "HJbEIfxEg", "comment": "Thank you very much for your reviews.\n\nOur work focus on the implicit feedback problem. It is motivated by the real world applications where users watch TV programs or visit different restaurants multiple times. Hence it is nature that we require each *user* would have multiple events, but we do not require that each user-item pair has multiple events. We are now doing experiments on a larger Yelp dataset, and will further improve the experiment section. ", "title": "RE: review"}, "ryHc1tumg": {"type": "rebuttal", "replyto": "SkcCZ7dmg", "comment": "Thank you for pointing out this! [1] is the version we published in a Workshop (DLRS) of Recsys 2016. A workshop paper doesn\u2019t violate the dual submission policy. However, we\u2019ll cite this workshop version if it is more appropriate. \n\nThough [1] and our current submission share the same idea, we improved it over several aspects: \n1) In [1], we use (#item) - dimensional point process for each individual user. This is an asymmetric model. In current submission, we use (# user \\times # item) - dimensional point process, which models user/item interactions symmetrically.\n2) The current submission uses a specific form of intensity function, which corresponds to Rayleigh distribution assumption. This is easier to train, and has closed form of expectation. [1] used a different form, which needs to do numerical integration when calculating the expectation. \n3) In this paper, we incorporate the noise contrastive estimation to speed up the training. \n4) We improved the experiment section by evaluating on more datasets.\n", "title": "RE: Already published?"}, "SJ27JYuQl": {"type": "rebuttal", "replyto": "HJmoAtUme", "comment": "Thank you very much for pointing out the important paper. However, several major differences are listed here:\n\nFor the task: \nChen et.al mainly focused on static rating prediction. However, their work is not capable of predicting the future time point (i.e., when will a user interact with a specific item). \n\nIn our work, we both do the time dependent item recommendation (i.e., predict the right item at right time), as well as the prediction of time point of future interactions.\n\nFor the model:\nChen et.al offered a nice way to model the time-dependent change of user features. However, the item features are static over time. Though it is possible to model evolution of item feature in a similar way, there is no way to incorporate the **coevolve** property between user and item. The user and item will evolve in their own pace. Also, the number of pieces in their piecewise constant function is fixed.\n\nIn our work, we model both the evolution and coevolution of users\u2019 and items\u2019 latent features. The coevolution part allows the embeddings to update with the latest user/item embeddings so far. By representing the embeddings with shared parameters in a recurrent way, we are able to make this nonlinear evolution tractable. In our work, we also assume the piecewise constant property of user/item embedding function, but the no. of pieces depends on the no. of related events. \n\nWe\u2019ll include the discussion in our paper. \n", "title": "RE: relation with general function matrix factorization"}, "SkcCZ7dmg": {"type": "rebuttal", "replyto": "HyWWpw5ex", "comment": "Nice work, but is seems to me that it was already published at DLRS 2016 in September (http://dl.acm.org/citation.cfm?id=2988451). Can you please elaborate how this paper is different from [1]? I don't seem to find any mention of this earlier paper in your submission. Thanks!\n\n[1] H. Dai, Y. Wang, R. Trivedi, L. Song: Coevolutionary Latent Feature Processes for Continuous-Time User-Item Interactions", "title": "Already published?"}, "HJmoAtUme": {"type": "review", "replyto": "HyWWpw5ex", "review": "It would be nice to discuss the relation with General Functional Matrix factorization (Chen et.al ICML 13), which also models the time change with piecewise constant functionThis paper proposes a method to model time changing dynamics in collaborative filtering.\nComments:\n1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN\n2) The author describes a BPTT technique to train the model  \n3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair.\n4) It would be interesting to consider other metrics, for example\n- The switching time where a user changes his/her to another item \n- Jointly predict the next item and switching time. \nIn summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).\n\n \n", "title": "relation with general function matrix factorization", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1IyyRbNg": {"type": "review", "replyto": "HyWWpw5ex", "review": "It would be nice to discuss the relation with General Functional Matrix factorization (Chen et.al ICML 13), which also models the time change with piecewise constant functionThis paper proposes a method to model time changing dynamics in collaborative filtering.\nComments:\n1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN\n2) The author describes a BPTT technique to train the model  \n3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair.\n4) It would be interesting to consider other metrics, for example\n- The switching time where a user changes his/her to another item \n- Jointly predict the next item and switching time. \nIn summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).\n\n \n", "title": "relation with general function matrix factorization", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bye2pC-me": {"type": "rebuttal", "replyto": "ByfA8o17x", "comment": "Thank you very much for detailed comments, which helps improve the paper.\n\nThe novelty of our work comes from modeling coevolutionary latent feature of users and items by interacting multi-dimensional point processes, and we use RNN to capture the nonlinearity of the embedding. It is fundamental both in insights and methodology.\n\nAs for the insight, designing model in point process framework allows us to predict not just potential items, but also the *precise* timings of user item interactions. Precise time prediction is especially novel and not possible by most prior state-of-arts . The fundamental insight is that our framework models time as a *random variable* and learns the generative pattern of events. However, most prior session-based works discretize time into epochs, which needs the tuning of epoch size and prediction accuracy is limited by epoch length.\n\nAs for the methodology, our model also leads to technical challenges since user embeddings and item embeddings are *interdependent* and intertwined over time. The embedding of latent features are further captured by the RNN. Another key contribution is the efficient optimization algorithm that solves the stochastic training over nested dependent samples (non iid) and makes the BTPP tractable in our co-evolving case. ( the sentence after \u2018and\u2019\u2018 might be repetition)\n\nIn Yelp dataset, most of the users only have one interaction event per interacted item, ie, a user typically posts his comment on a business only once. For the point process framework, it requires at least two events to learn the model and predict the next event time. Thus we select active users with more than hundreds of events. We will adopt your comments and continue refining our experiment section. ", "title": "RE: pre-review questions"}, "ByfA8o17x": {"type": "review", "replyto": "HyWWpw5ex", "review": "I had trouble digesting the difference with existing work on Deep RS, especially in Section 2. The text argues that the work is difference due to the idea of co-evolution and the use of multidimensional point-processes, but is this a difference in fundamental insights or a difference in methodology? At the end of the day, aren't all these methods (incl. some work by the same authors) trying to capture non-linearities in recommendation at the end of the day? Can you spend some time elaborating on the fundamentally new insight here?\n\nThe Yelp dataset used here is a *tiny* subset of the complete Yelp data. Dataset size doesn't seem to be an issue as the IPTV dataset is much larger. Why is the dataset subsampled in this way, and what are the results if you don't subsample it?The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows:\n(a) the paper models the co-evolutionary process of users' preferences toward items\n(b) the paper is able to incorporate external sources of information, such as user and item features\n(c) the process proposed is generative, so is able to estimate specific time-points at which events occur\n(d) the model is able to account for non-linearities in the above\n\nFollowing the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is).\n\nOther than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar.\n\nI hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with \"hundreds\" of events means that you're left with a very biased sample of the user base.\n\nOther than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.\n", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJbEIfxEg": {"type": "review", "replyto": "HyWWpw5ex", "review": "I had trouble digesting the difference with existing work on Deep RS, especially in Section 2. The text argues that the work is difference due to the idea of co-evolution and the use of multidimensional point-processes, but is this a difference in fundamental insights or a difference in methodology? At the end of the day, aren't all these methods (incl. some work by the same authors) trying to capture non-linearities in recommendation at the end of the day? Can you spend some time elaborating on the fundamentally new insight here?\n\nThe Yelp dataset used here is a *tiny* subset of the complete Yelp data. Dataset size doesn't seem to be an issue as the IPTV dataset is much larger. Why is the dataset subsampled in this way, and what are the results if you don't subsample it?The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows:\n(a) the paper models the co-evolutionary process of users' preferences toward items\n(b) the paper is able to incorporate external sources of information, such as user and item features\n(c) the process proposed is generative, so is able to estimate specific time-points at which events occur\n(d) the model is able to account for non-linearities in the above\n\nFollowing the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is).\n\nOther than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar.\n\nI hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with \"hundreds\" of events means that you're left with a very biased sample of the user base.\n\nOther than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.\n", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}