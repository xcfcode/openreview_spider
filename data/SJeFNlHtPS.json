{"paper": {"title": "Hidden incentives for self-induced distributional shift", "authors": ["David Scott Krueger", "Tegan Maharaj", "Shane Legg", "Jan Leike"], "authorids": ["davidscottkrueger@gmail.com", "tegan.jrm@gmail.com", "legg@google.com", "leike@google.com"], "summary": "Performance metrics are incomplete specifications; the ends don't always justify the means.", "abstract": "Decisions made by machine learning systems have increasing influence on the world. Yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in online learning for applications such as content recommendation, where the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. Generally speaking, it is possible for an algorithm to change the distribution of its own inputs. We introduce the term self-induced distributional shift (SIDS) to describe this phenomenon. A large body of work in reinforcement learning and causal machine learning aims to deal with distributional shift caused by deploying learning systems previously trained offline. Our goal is similar, but distinct: we point out that changes to the learning algorithm, such as the introduction of meta-learning, can reveal hidden incentives for distributional shift (HIDS), and aim to diagnose and prevent problems associated with hidden incentives. We design a simple \u00a0environment as a \"unit test\" for HIDS, as well as a content recommendation environment which allows us to disentangle different types of SIDS.\u00a0 We demonstrate the potential for HIDS to cause unexpected or undesirable behavior in these environments, and propose and test a mitigation strategy.\u00a0", "keywords": ["distributional shift", "safety", "incentives", "specification", "content recommendation", "reinforcement learning", "online learning", "ethics"]}, "meta": {"decision": "Reject", "comment": "The paper shows how meta-learning contains hidden incentives for distributional shift and how a technique called context swapping can help deal with this. Overall, distributional shift is an important problem, but the contributions made by this paper to deal with this, such as the introduction of unit-tests and context-swapping, is not sufficiently clear. Therefore, my recommendation is a reject."}, "review": {"rJxhKoSaFB": {"type": "review", "replyto": "SJeFNlHtPS", "review": "The authors study the phenomena of self-introduced distributional shift. They define the term along with the term hidden incentives for distributional shift. The latter describes factors that motivate the learner to change the distribution in order to achieve a higher performance. The authors study both phenomena in two domains (one being a prisoner dilemma and the other a recommender system) and show how meta-learning reveals the hidden incentives for distributional shift. They then propose an approach based on swapping learners between environments to reduce self introduced distributional shift.\n\nIn my opinion this paper should be (weak) rejected for several reasons.\n\n-\tThe paper is poorly written. Some sentences are hard to comprehend, even after repeated reading. It feels hastily written and should be carefully proofread with the help of a proficient English speaker.\n-\tThe very first paragraph is not a helpful example. While the authors provide an example that that illustrates a distributional shift, they don\u2019t describe what the shift is and why that shift in a negative consequence for learning. Therefore, the example is not helpful, but rather confusing.\n-\tIn point 3 of the contributions, the authors state \u2018that meta-learning reveals HIDS in these environments\u2019. Is this true for all meta-learning approaches? The current statement overclaims their findings. If the authors decide to keep it, they should provide a description of the types of meta-learning approaches that reveal HIDS and which don\u2019t or evaluate all different meta-learning approaches.\n-\tSection 4.1 is hard to follow. The authors state that the meta-learner is used to tune the learning rate \u2013 they fail to clearly explain how exactly the learning rate tuning results in a non-myopic behavior for a myopic learner.\n-\tIn the Q-learning example in Section 4.1, is the effect of \\epsilon considered? The discovery of non-myopic strategies might simply be based on chance. I would like the authors to include an investigation into the effect of this parameter.\n\nGenerally speaking, I appreciate the author\u2019s thoughts on SIDS and how they approach revealing hidden incentives. Although I vote to reject this paper, I strongly encourage the authors to rewrite the paper, address all other issues that are noted during this peer review and resubmit.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 4}, "HkgaesWFsr": {"type": "rebuttal", "replyto": "H1e8RBdx9r", "comment": "We appreciate the feedback and would love to hear more detailed suggestions for improvements along the lines of your final paragraph.  Highlighting any parts of the experimental set-up that you found unclear, would be especially useful, since we believe we\u2019ve already described it in sufficient detail.  We\u2019ll be happy to make all of the specific changes you\u2019ve suggested.\n\nRegarding \u201crestrict[ing] the claims to PBT and PBT-like methods\u201d, we\u2019re sorry this wasn\u2019t clear.  I assume you\u2019re referring to the bottom half of page 6, where we discuss Q-learning and REINFORCE.  I\u2019ll attempt to clarify this now, and request that you please let us know:\n1) what if anything is still unclear to you,\n2) whether you still think we shouldn\u2019t have made such claims\n3) what you think could be done to make this easier to follow.\nWe included these additional experiments at the request of a previous reviewer, and we believe they shed some light on our results using PBT.  We chose these experiments precisely because we view these other algorithms as similar to PBT in important (but different) ways, and we believe these experiments serve as controls to support our hypotheses as to why PBT has this effect.  \nWe certainly believe that future work should aim for a more conclusive and general understanding of how choice of learning algorithms influence which incentives are pursued.  The connection we draw with meta-learning is just one example; including what we have already observed and hypothesized in these control experiments seems likely to help future researchers develop such understanding.", "title": "Authors' response: requesting more feedback, and explaining why we included control experiments"}, "SJx3qIZtoB": {"type": "rebuttal", "replyto": "HJg5_UWFsB", "comment": "\nRegarding the final paragraph of your review on the relevance of our work:\nThere are already well-known issues related to SIDS, and works addressing them.  If I understand correctly, we are on the same page about SIDS, but you are skeptical that:\n1) HIDS presents new challenges that require new approaches.\nand/or\n2) The challenges HIDS raised have (or will have) significant practical relevance.\nand/or\n3) Our work makes useful progress in addressing challenges of HIDS. \nIt would help us to know which of the above describes your position!  Our primary aim is to explain HIDS, the kinds of problems we imagine it might lead to, and some ideas for how they could be addressed.  We hope that a clear exposition of the problems related to HIDS will motivate other researchers to come up with their own ideas for how to diagnose and address these problems in practice.\n\nWhile we agree that it\u2019s unclear how significant our work is for current issues with real-world systems, we believe we provide important insights!  Two in particular are:\n1) Viewing the learning algorithm (and not just the loss/reward function) as an important aspect of specification.\n2) Learners trained with supervised learning + meta-learning can pursue incentives for SIDS (similarly to RL agents).\nIn general, we believe that understanding when and why incentives are hidden or revealed is an important and interesting scientific question that deserves attention, and hope to clearly communicate this question and our motivation for studying it.", "title": "Authors' response: Answering questions and explaining relevance and relation to alternative approaches (...continued)"}, "HJg5_UWFsB": {"type": "rebuttal", "replyto": "S1xlW-zf9S", "comment": "Thank you for your thoughtful review.\nWe\u2019re glad that you find the topic important, and hope to clarify what we see as the relevance of our work.\nWe\u2019re striving to make our paper as clear as possible, and would greatly appreciate any further help in doing so, or identifying other ways to address the challenges of SIDS and especially HIDS.\n\nAddressing your questions:\n\n\nQ1: I am not very familiar with this series of research but I am wondering why the paper focuses on meta-learning and its connection to HIDS. Does it use meta-learning as a tool to identify HIDS? \nA1: We believe we are the first to study HIDS.   We think meta-learning provides a clear illustration of why HIDS might lead to easily-overlooked problems; meta-learning is often framed as a method of finding a better solution to a given problem, but in fact it can also change what counts as a good solution, and our impression is that many researchers find that surprising (sentence 1 of final paragraph of section 1).\n\nQ2: Does [we] use meta-learning as a tool to identify HIDS?\nA2: Yes, you can view it this way.  But the point is that one should be aware of which incentives are hidden/visible, and also be aware that seemingly innocuous changes in the learning algorithm can change that (sentence 3 of section 3.2).\n \nQ3: It is well known normally an interactive system that can change its inputs have distributional shift. What other information does this \"unit-test\" inform us?\nA3: We agree this is well known.   The unit test tells us whether a given learner is indifferent to such changes, or will actively seek to induce them.  Consider our example of content recommendation.  Content recommendation can change user interests whether or not the learner is seeking to induce such a shift, but we should be more concerned about algorithms that view changing user interests as a legitimate strategy to improve performance than those that are indifferent to such changes.\n\nQ4: Similarly, how does \"context swapping\" mitigate distributional shift? From the experiments, it dumbs the meta-learning algorithms and make it pass the \"unit-test\", but I am not sure what other practical benefits it can bring to improve real systems.\nA4: TODO: an example\nContext swapping doesn\u2019t make the meta-learning algorithms less smart, it merely changes their incentives, aiming to remove incentives for distributional shift.  However, as we noticed in the content recommendation experiments, it doesn\u2019t work well in situations where learners are unable to track any distributional shift which does occur.  In other words, it\u2019s only a starting point for managing learners\u2019 incentives, not a complete solution.\n\n\n\nRegarding your 2nd to last paragraph:\nFirst, I'm not precisely sure what you are suggesting as an alternative. Can you be more concrete, or provide an example?\nIt seems like you are suggesting that RL algorithms can learn to model distributional shift in the environment and account for it.\nWhile this is true, this does not address the issue of whether/when an RL agent should view SIDS as a legitimate part of a solution strategy.  By default, RL algorithms aim to maximize returns by any means, viewing any form of SIDS as something which should be leveraged to drive up performance.\nWe could try to address this by providing a reward function that penalizes only those SIDS we think are undesirable. For example, in the content recommendation setting, instead of seeking learners indifferent to changes in user preferences, we could attempt to provide the learner with a specification that would distinguish between good changes (e.g. based on informing users) and bad changes (e.g. based on manipulating or misinforming users).  \nHowever, we have concerns about the scalability and tractability of this as a fully general approach, since it seems to rely on the learner having thorough knowledge of human preferences over different outcomes.  Such an approach may be impractical and error-prone, since it may require the reward to be a function of the entire history of interaction.  It may also be undesirably value-laden, since different users may have different ideas about what forms of influence are (il)legitimate.\nThis is discussed briefly in paragraph 3 of the introduction.\n\n\n\n", "title": "Authors' response: Answering questions and explaining relevance and relation to alternative approaches"}, "rkxvzVbKoS": {"type": "rebuttal", "replyto": "rJxhKoSaFB", "comment": "Thanks for you encouragement and detailed comments.\nWe\u2019d greatly appreciate further input on how to improve our submission!\n\nFirst, to clarify: context swapping is meant to remove *incentives* to induce (or prevent) distributional shift, but not to prevent or reduce SIDS, which may occur regardless of the learner\u2019s incentives.  For example, Pennycook et al.[1] find evidence that the \u201cillusory truth effect\u201d can lead users to believe in \u201cfake news\u201d; this would happen regardless of whether an intelligent content recommendation system was trying to induce such an effect, or merely showed a user fake news articles because that\u2019s what it predicted they would click on.\n\n\nAddressing your bullet points:\n- We really want our paper to be as clear as possible, and would love to know more specifically which sentences you found awkward or difficult to parse.\n- The distribution of \u201cwhen the owner will wake up today\u201d is shifted; the robot wakes them up in order to ensure that they will want coffee.  We can make this more explicit.  To be clear, our point is not that the robot will have difficulty learning in the presence of such a distributional shift; our point is that the robot having an incentive to produce such a shift is an alignment problem.\n- We agree this is overstated, and will soften the claim.  We\u2019ve demonstrated this for PBT and REINFORCE (when considered as a meta-learning algorithm), only.  We believe it will hold true for a wide variety of meta-learning algorithms, but probably not all of them.  \n- In fact, it is not the tuning of the learning rate that results in non-myopic behavior.  Rather, the EXPLOIT step of PBT is the main mechanism by which non-myopia is incentivized.  Appendix 3.1.2 walks through the mechanism.\n- Indeed, the choice of epsilon is important.  We use epsilon=0.1, and for much larger values of epsilon, non-myopic strategies are unstable and do not persist.  We will include more discussion and exploration of this choice.\nWhile there's certainly an element of chance (as our experiments demonstrate) as to whether the learner learns a stable non-myopic policy, we don\u2019t think that invalidates the result; we think it is significant and surprising that Q-learning can yield a sub-optimal policy in 10/30 experiments.  Can you please explain why this is a concern for you?  Or elaborate on what you mean by \u201cbased on chance\u201d?\n\n\n[1] Gordon Pennycook, Tyrone D Cannon, and David G. Rand. Prior exposure increases perceived accuracy of fake news. Journal of Experimental Psychology (forthcoming), 2019. ", "title": "Authors' response: clarifying a few points and requesting elaboration"}, "H1e8RBdx9r": {"type": "review", "replyto": "SJeFNlHtPS", "review": "The main idea of the paper: When using meta-learning there is an inherent incentive for the learner to win by making the task easier. The authors generalise this effect to a larger class of problems where the learning framework induces a set of Hidden Incentive for Distributional Shift (HIDS) and introduce Context Swapping, a HIDS mitigation technique. In the experimental section, the authors propos a HIDS unit test which then they employ to show that PBT (Population Based-Trainng), a popular meta-learning algorithm exhibits HIDS ant that context swapping helps fixing it. \n\nOverall, I found the idea of the paper interesting, but the attempt to generalise the effect from meta-learning to general learning setups hard to follow and detracting from the overall value. I think the authors should have restricted their claims to PBT and PBT-like methods and follow-up with something more general in future work. \n\nFurthermore, the notation and formaliation of the problem are incomplete:\n    * the concept of \u2018trajectory\u2019 is introduced without being properly defined, though its crucial in the definition of the proposed HIDS mitigation approach\n    * the context swapping algorithm description is not clearly motivated and explained, a diagram showing the learner shuffling would be quite helpful\n    * in the HIDS unit-test section, the game theoretical setup is only partially explained, the defection and cooperation actions are not clearly linked to the HIDS \n    * In Section 4.1.1 HIDS UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION Figure2 is refered for results without ever stating the task and the Figure itself does not mention it\n\n       In terms of suggestions, I think the paper needs to go through a careful refactoring with an attention to technical details (careful concept definition, introduction of notation, clarity on experimental setup)\u2028", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 2}, "S1xlW-zf9S": {"type": "review", "replyto": "SJeFNlHtPS", "review": "The paper discusses concepts of self-induced distributional shift (SIDS) and the hidden incentives when using meta-learning algorithms. It then prescribes a unit-test to check whether there is hidden incentive for distributional shift (HIDS) in the algorithm and proposes to use context swapping to mitigate such phenomenon.\n\nI am not very familiar with this series of research but I am wondering why the paper focuses on meta-learning and its connection to HIDS. Does it use meta-learning as a tool to identify HIDS? But from the description and experiments, it seems the paper is talking about how meta-learning itself leads to HIDS, for example, by comparing different hyper-parameter setting for meta-learning and PBT, it shows the unit-test is failing. So it seems like meta-learning itself leads to distributional shift?\n\nAlso I cannot fully appreciate the utility of this \"unit-test\". It is well known normally an interactive system that can change its inputs have distributional shift. What other information does this \"unit-test\" inform us? Similarly, how does \"context swapping\" mitigate distributional shift? From the experiments, it dumbs the meta-learning algorithms and make it pass the \"unit-test\", but I am not sure what other practical benefits it can bring to improve real systems.\n\nUsually, a reinforcement learning algorithm can meaningfully mitigates the adverse effects of distributional shift by explicitly modeling this interactive process and evaluating rewards with considerations to distributional shift caused by different policies. It is difficult to see how the concepts discussed in the paper provide meaningful approaches to address the issue.\n\nOverall, the paper touches the important question of distributional shift for machine learning systems but I find the concepts discussed in the paper, such as the focus on meta-learning, the \"unit-test\", and \"context-swapping\", less relevant to how we can really mitigate the issues in real systems or how it can provide additional insights about the problem.", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 1}}}