{"paper": {"title": "Meta-Model-Based Meta-Policy Optimization", "authors": ["Takuya Hiraoka", "Takahisa Imagawa", "Voot Tangkaratt", "Takayuki Osa", "Takashi Onishi", "Yoshimasa Tsuruoka"], "authorids": ["takuya-h1@nec.com", "~Takahisa_Imagawa1", "~Voot_Tangkaratt1", "~Takayuki_Osa1", "takashi.onishi@nec.com", "~Yoshimasa_Tsuruoka1"], "summary": "", "abstract": "Model-based reinforcement learning (MBRL) has been applied to meta-learning settings and has demonstrated its high sample efficiency. \nHowever, in previous MBRL for meta-learning settings, policies are optimized via rollouts that fully rely on a predictive model of an environment. \nThus, its performance in a real environment tends to degrade when the predictive model is inaccurate. \nIn this paper, we prove that performance degradation can be suppressed by using branched meta-rollouts. \nOn the basis of this theoretical analysis, we propose Meta-Model-based Meta-Policy Optimization (M3PO), in which the branched meta-rollouts are used for policy optimization. \nWe demonstrate that M3PO outperforms existing meta reinforcement learning methods in continuous-control benchmarks. ", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper presents a meta-learning for Model-based RL that introduces branched rollouts to improve sample efficiency of the learned model.\n\nWhile the paper addresses an important topic of sample efficiency in RL, and provides theoretical analysis, the reviewers raised concerns with the novelty and clarity. The extension to POMDP setting is certainly important technological contribution, albeit a straightforward. To be suitable for publication the work needs to make stronger case for the significance of the method.  "}, "review": {"YSvrP0ImDA3": {"type": "rebuttal", "replyto": "4UIsjvTZfEt", "comment": "Thank you very much for your comments. \nWe posted our response to common concerns on our work, at the top of this board. \n\nHere is our response to your comments: \n\n> Both Theorem 1 and 2 seem to be a straightforward combination of the results in [1] and the fact that POMDPs can be cast as MDPs with history-states. Thus the theoretical contribution/novelty is quite limited.\n\nAlthough it is relatively straightforward to derive Theorem 1, we would like to argue that Theorem 2 is not a trivial and straightforward extension of [1]. \nThis is because the proof of our theorems requires a refined version of the theorems in [1] that bounds the performance of branched rollouts more properly (that's what we derived in A.7). \nTherefore, our contribution is providing not only the bound of the branched rollouts performance for POMDPs, but also the proper bound of the branched rollout performance.\n(see also our reply to Reviewer 5) \n\n\n> Appendix A.1 briefly discusses the differences to [1], and claims to derive similar Theorems under more appropriate assumptions/premises in Appendix A.7. How do the theorems in A.7. differ from the ones in Section 5? If the theorems build on more appropriate assumptions, why not use them in the main part of the paper?\n\nTheorem 4 in A.7 is a theorem for MDPs, and Theorem 2 in Section 5 is its extension for POMDPs. \nWe did not include theorems in A.7 in the main part due to space limitation. \nWe will include them into the main part to improve the clarity. \n\n> Adding a simple simulation study that shows how much smaller the gap/discrepancy is, when branched instead of full rollouts are used, would require little effort and strengthen the claim that branched rollouts are preferable.\n\nWe had conducted a simulated study, in which the trend of discrepancy values in the full-rollout case (Theorem 1) and that in branched-rollout case (Theorems 2 and 3) are evaluated. \nThe trend of the performance discrepancy values in Theorem 1 is shown in Figure 9 and that in Theorems 2 and 3 is shown in Figure 5. \nGiven appropriate values for $k$ for Theorem 2 and 3, the results in those figures indicate that the discrepancy in the branched-rollout case is smaller than that in the full-rollout case. \n\nIn addition, from the viewpoint of empirical performance discrepancy (i.e., the discrepancy between a training performance in the meta-model and an actual performance in the true environment), we will evaluate M3PO against the variant of M3PO that uses full rollouts. \nWe expect that the empirical performance discrepancy of M3PO is smaller than that of the variant. \n\n\n\n> I found section 5.1. to be very confusing \u2013 It is stated that the gap can be \u201cexpressed as the function of two error quantities of the meta-model: generalization error due to sampling and distribution shift due to the updated meta-policy\u201d. Indeed Def. 2 is probably the generalization error due to sampling. However, Def. 1 is the expected TV distance of the estimated model from the true model. Then, how should this be the \u201cdistribution shift due to the updated meta-policy\u201d?\n\nDef. 1 is the expected TV distance between the true environment and the model. Therefore, it measures the error occurs by sampling from the model. \nDef. 2 is the TV distance between the data collection policy and the learning policy. Therefor it measures error due to the distributional shift. \n\nWe will revise this part as follows:  \n\"The discrepancy between these returns, $C$, can be expressed as the function of two error quantities of the meta-model: generalization error due to sampling and distribution shift due to the updated meta-policy.\"  \n->  \n\"The discrepancy between these returns, $C$, can be expressed as the function of two error quantities: the generalization error of the meta-policy and the distribution shift due to the updated meta-policy.\" \n", "title": "Our response to Reviewer 4's review"}, "TWMUzMJyjZA": {"type": "rebuttal", "replyto": "QPFhxk3hokp", "comment": "Thank you very much for your comments. \nWe posted our response to common concerns on our work, at the top of this board. \n\nHere is our response to your comments: \n\n\n> Maybe it's because I don't work on meta-learning, but I think that you are really missing a clear description of the algorithm. Where are the task switches happening, how does the initial task distribution come into play? This should be clear from algorithm 2 in my opinion.\n\nThe task switches in accordance with the transition function of the task when the trajectory is sampled from the environment in line 5 in Algorithm 2. \nAlso, in line 5, the task is sampled from the initial task distribution at the beginning of an episode. \nNote that since the task information is assumed to be unobservable from the agent in our setting (see Section 4), the sampled task is used only in the environment and is not fed to the agent (algorithm). \n\nWe will revise the paper to improve the clarity on this part. \n\n\n> In Algorithm 2. you could introduce a loop over tasks maybe? I realize that in the environments that you used, task switches happen during the episode, but I wouldn't rely on that in the algorithm description because it's a bit confusing, looking at the the algorithm itself, I don't see a meta-learning algorithm but rather a model-based RL algorithm for POMDPs.\u3000\n\nWe answer your question, assuming that you are referring to the intra-task loop such as lines 3--9 in Algorithm 1 of the PEARL [3] paper as \"a loop over tasks.\"  \nWe have not introduced such a task loop into Algorithm 2, since task information is assumed to be unobservable from the agent. \nNamely, in our algorithm description (M3PO), the trajectories sampled from the environments are not stored into a replay buffer for each task as in the original PEARL algorithm, but stored together into a single replay buffer. \n\n\n> Are the results for L2A correct? They seem to be too bad to be true based on the results reported in the original paper. \n\nWe used the public source code of L2A published by the authors and tuned its hyper-parameters to our experimental setting. \n\nThe results of L2A are different probably because we use a different (and more difficult) experimental setup. \nFor example, in our \"Ant-crippled-leg\" environment, we introduce early termination (terminating an episode when the ant falls down) and allows the part of ant's crippled leg to change. \n\n\n\n\n> In Fig. 1, by number of training samples it is meant number of time steps in the environment?\n\nYes. \n\n> The variance of the performance is quite high for the ant fwd-bwd, much higher that the other methods. Any explanation for that?\n\nHere is our hypothesis based on the observation of the meta-policies' behaviour during the learning:  \nIt is quite difficult to make the ant correctly move forward and backward in accordance with the given task, and it makes the meta-policy unstable. \nIn Ant-fwd-bwd, the learning agent tends to at first learn to move forward correctly when the task is \"forward\" and stop when the task is \"backward\" (or vice versa). At this first step, learning is stable (so the variance of performance is relatively small until around the first 100 episodes). After that, the agent starts to learn to correctly move forward and backward in accordance with the task, but it is difficult to learn and makes the learning unstable. \nNote that the performance of other methods is more stable because their policy could not learn this task at all. \n\n> In Fig. 1, the legend should appear for the whole figure and not just for the upper-left graph.\n\nThank you for the suggestion. We will modify the paper as you suggested. \n\n> The performance of PEARL with long-run is better in 3/6 experiments and seems comparable in the others, does this mean that M3PO doesn't use new data efficiently? \n\nThis is primarily due to unstable learning incurred by meta-model rather than inefficient data leverage. \nSince M3PO uses the fictitious data generated by the meta-model rollout to train the meta-policy (and value function), the performance improvement seems to be more sensitive to meta-model ambiguity and distributional shifts due to meta-model updates as the training progresses.  \nTo reduce the reliance on meta-model, we modified M3PO to use a mixture of real and fictitious data. \nThe evaluation results of the modified M3PO are shown in the revised paper A.16. \nThe results indicated that the long-term performance of M3PO with the mixed data is comparable to PEARL. \n\n[3] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen.  Efficient off-policy meta-reinforcement learning via probabilistic context variables. ICML, 2019.\n\n", "title": "Our response to Reviewer 3's review (1/2)"}, "cXoVlZ8-htT": {"type": "rebuttal", "replyto": "R9qcFIlPDS", "comment": "Thank you very much for your comments. \nWe posted our response to common concerns on our work, at the top of this board. \n\nHere is our response to your comments: \n\n\n> The theory part of this paper is quite similar to Janner et al. (2019). It seems they just apply similar theorems to the POMDP setting. For example, theorem 1 in this paper is an immediate result when combining Theorem 4.1 in Janner et al. (2019) and Lemma 1 in Silver & Veness (2010). And the idea of using branched rollout to control model errors is not first proposed by this paper either. \n\nAs for the derivation part of Theorem 1, as you pointed out, we simply applied the known theorem and lemma.  \nHowever, as for the derivation part of Theorems 2 and 3, we use the refined version of the theorems in Janner et al. (2019) that bound the performance of branched rollouts more properly (Theorems 4 and 5 in A.7). \nIn this respect, we do more than just apply the known theorem and lemma. \n(See also our reply to Reviewer 5)\n\n\n> The proposed algorithm (M3PO) is also very similar to algorithm 2 in Janner et al. (2019), and they just use this algorithm in POMDPs. From an implementation perspective, there are only two differences: 1. using GRU as network architectures, 2. use PEARL in policy optimization. So I feel the novelty in the proposed algorithm is limited. \n> Why MBPO ( Janner et al. (2019) ) does not support POMDP? Just because it does not use RNN?\n\nMBPO does not support POMDPs because of more algorithmic-level differences: \n1. MBPO performs the model rollout based on (MDP) states, not the history of (POMDP) observations. More specifically, MBPO randomly samples a (MDP) state contained in D_{\\text{env}} during the model rollout (line 8 in their Algorithm 2), and, in this, the sequential information is not taken into account. \n2. The type of input to the policy and transition model is not the history. MBPO treats the (MDP) state as the type of their input, which is different from the history of observations (i.e., h in our paper). \nThe use of RNNs (for meta-policy and meta-model) is an instance of approaches to address the second point. (i.e., there are other approaches. For example, a gradient-based MAML architecture can be used for the meta-policy and meta-model instead.) \nAlso, to address the first point, modification of D_{\\text{env}} is necessary. \n\nAs for the implementation, note that, in the last paragraph of Section 6, we explain only the main extensions from the MBPO implementation described, and we have made other extensions. \nFor example, we introduce gradient clipping to deal with training instability derived from the nature of the sequential model, and dynamic automated handling to deal with the breakdown of meta-model training.\n\n> In the experiments, it seems M3PO (their algorithm) only has significant advantages over PEARL on Halfcheetah-fwd-bwd and Ant-fwd-bwd. In the other environments, the performance gap seems not very clear. \n\nIn terms of sample efficiency, M3PO has significant advantages over PEARL on Halfcheetah-fwd-bwd, Walker2D-randomparams, Ant-fwd-bwd, Ant-crippled-leg and Humanoid-direc, because, in Figure 1, the learning curves of M3PO are plotted at upper left parts than those of PEARL. \n(i.e., In these environments, M3PO achieve a certain level of performance significantly faster than PEARL.)\n\n\n> And if we compare M3PO-long and PEARL-long, it seems M3PO-long is overall worse than PEARL-long.\n\nSince M3PO only uses fictitious data generated by the meta-model rollout, it tends to be defeated by the model-free RL in the long-term training.  \nAs an additional experiment, we have evaluated a modified version of M3PO that uses a mixture of the fictitious and real data for meta-policy update. \nThe results of this experiment are given in Appendix A.16 of the revised version. The results show that the long term performance of M3PO with a mixture data is comparable to that of PEARL. \n", "title": "Our response to Reviewer 2's review"}, "fLGHy_p55IR": {"type": "rebuttal", "replyto": "OglSMOcOSnG", "comment": "Thank you very much for your comments. \nWe posted our response to common concerns on our work, at the top of this board. \n\nHere is our response to your comments: \n\n> A bit too similar to (Janner, 2019). Novelty-wise, it seems like a small (but effective) modification of the work in (Janner, 2019) from model-based optimization in the single MDP setting to the meta-learning setting, by including a recurrent/RNN model. The main contribution seems to be this modification, but all other techniques of the paper (e.g. defining the correct terms such as a branched rollout, generating the concepts such as C(eps, eps'), etc.) are exactly the same in (Janner 2019). I think the paper overemphasizes the portions that have already been written in (Janner, 2019).\n\nWe would like to highlight the following theoretical analysis as other main contribution:  \nWe improve the theorems proposed in Janner et al. (2019) to bound the performance of branched rollouts more properly. \nSpecifically, in Theorems 4.2 and 4.3 proposed in Janner et al. (2019), some important premises (e.g., multiple model-based rollouts factors) are not taken into account. \nThe oversights of those important premises induce a large mismatch between those for their theorems and those made for the actual implementation of branched rollouts. \nThe mismatch becomes larger especially when the model-rollout lengths (k) is large (more precisely, the mismatch is incurred in all cases where $k>1$); namely, Theorems 4.2 and 4.3 do not properly bound the performance of the branched rollouts in large $k$. \nProperly bounding the performance of the branched rollouts in large $k$ is important for correctly comparing with other rollouts (e.g., full rollouts) or estimating the optimal $k$ for example. \n\nIn this paper, we take the premises into account and propose refined theorems (Theorems 4 and 5 in A.7 in our paper). The refined theorems do not have the issue of Janner et al. (2019), and enable us to bound the performance in large k more properly. \nIn addition, we extend our Theorems 4 and 5 rather than Theorems 4.2 and 4.3 in Janner et al. (2019) when we derive the theorems for the POMDP case (Theorems 2 and 3 in our paper).  \n\n\n> A bit hard to read at times, due to the denseness of the notation. This is especially encountered in page 6, where there are large chunks of text + math without breathing room. I suggest that the authors provide a high level summary in the main body, especially because most terms have already been defined in (Janner, 2019).\n\nThank you for the suggestion. We will improve the clarity of the paper as suggested. \n", "title": "Our response to Reviewer 5's review"}, "sBU3NuMwZo1": {"type": "rebuttal", "replyto": "TWMUzMJyjZA", "comment": "> Also, I would be interested in knowing how the variance looks like for those longer runs. \n\nAs for PEARL-long, the variances in Halfcheetah-pier and Ant-fwd-bwd are large compared to those in rest of the environments. In Halfcheetah-pier and Ant-fwd-bwd, the learning progresses in each trial are quite different depending on initial random-seed. \nAs for M3PO-long, the variances are smaller than those in PEARL-long in most of the environments because the best scores in each trial are nearly the same. \nFigure 13 provides the results in  each trial for both PEARL-long and M3PO-long.\n\n> I would be interested in seeing how the method generalizes across OOD unseen tasks (not seen on train), such that the training and test distributions are different and then plot training vs test performance. \n\nWe will conduct an additional experiment for this. We hypothesize that M3PO is able to generalize across the OOD tasks proposed in the PEARL paper since the PEARL architecture is used in our algorithmic implementation. \n\n\n> For one, the idea behind meta-learning is generalization across different tasks/environments. While this was introduced well in sections 3 and 4, the connection is missing in the experiment section and algorithm section.  \n> Is the meta-learning formalism even needed for this kind of algorithm and evaluation?\n\nThe algorithms and evaluation do not require the meta-learning formalism. However, our research focuses on meta-RL, and we believe that the meta-learning formulation in Section 4 is necessary in order to show that the theorems we derived in Section 5 can be applied to it. \n", "title": "Our response to Reviewer 3's review (2/2)"}, "CUUWcqU3pEm": {"type": "rebuttal", "replyto": "KOtxfjpQsq", "comment": "Dear reviewers,  \nThank you very much for your thoughtful review comments. \n\nWe understand that the reviewers are mainly concerned about novelty and clarity. \nTo improve clarity, we will revise the paper according to the review comments. \nIn this reply and the first revision, we focus on clarifying our novelty and contributions and answering the questions raised in the reviews. \n\nWe understand that our work can be seen as a POMDP extension of known theories and algorithm. \nHowever, we would like to argue that this is not a critical weakness of the paper since there are a number of prominent and important works that have extended the theory and algorithms known in MDP to the ones for POMDP (e.g., [1] and [2]). \nWe hope that the reviewers will re-evaluate the strengths and weaknesses of our work by taking this fact into account. \n\nFurther, we would like to emphasize our contribution on the meta-RL frontier: \"Our work is the first attempt to enable sample efficient meta-RL using the model-based framework with a theoretical guarantee.\"  \nAs we introduced in Section 1, in the previous model-based meta-RL literature, it has not been clear how learning the meta-model relates to real environment performance. \nWe provide relations between them (Theorems 1, 2 and 3) and build the model-based meta-RL framework on the basis of the theories. \nWe also show that M3PO achieves better sample efficiency than existing meta-RL methods. \n\nIn addition, in Section 4, we provide a discussion about how existing meta-RL settings can be recovered in our setting. This discussion indicates that our framework and theories can be flexibly applied to a variety of existing meta-RL settings. \nTherefore, we believe that our framework and theory is useful in the meta-RL community. (For example, one may extend our framework and theories by introducing additional assumptions on their target meta-RL setting.)\n\n\n[1] David Silver, Joel Veness, Monte-Carlo Planning in Large POMDPs, NeurIPS, 2010.  \n[2] Matthew Hausknecht, Peter Stone, Deep Recurrent Q-Learning for Partially Observable MDPs. AAAI, 2015\n", "title": "Our response to common concerns on our work"}, "4UIsjvTZfEt": {"type": "review", "replyto": "KOtxfjpQsq", "review": "=== Summary ===\n\nThe paper concerns model-based meta-RL. It exploits the fact that meta-RL can be formulated as POMDP in which the task indicator is part of the (unobserved) hidden state. Thus, the paper effectively analyzes and proposes model-based algorithms for POMDPs. The paper bounds the gap between the expected reward of a policy in the actual POMDP and the estimated model and then theoretically shows that this gap can be reduced when using dyna-style / branched rollouts instead of full rollouts under the learned model. Motivated by this finding, the paper proposes a Dyna-like algorithm for POMDPs. In the experimental evaluation, the paper compares its proposed method, M3PO, to two recent meta-RL approaches in a range of meta-RL environments for continuous control.\n\n=== Merits of the paper ===\n\nOverall, the paper contributes a new algorithm for model-based meta-RL which is neatly motivated by the paper\u2019s theoretical analysis. Both the theoretical analysis and the proposed algorithm are sound. The experimental evaluations demonstrate that the algorithm performs similar or better than previous model-based meta-RL algorithms and thus could be a relevant contribution to the field of meta-RL.\n\n=== Strenghts ===\n- The experiments include relevant meta-learning environments and algorithms to compare to.\n- The proposed algorithm is sound and motivated by theory.\n- I have done a very simple simulation study, being able to confirm that the gap in Theorem 2 is indeed (much) smaller than the one Theorem 1.\n\n=== Weaknesses / Concerns ===\n- Both Theorem 1 and 2 seem to be a straightforward combination of the results in [1] and the fact that POMDPs can be cast as MDPs with history-states. Thus the theoretical contribution/novelty is quite limited.\n- As expected, from my simulation study it seems that bounds are vacuous and their usefulness beyond motivating branched rollouts is questionable.\n- The only difference to original dyna-like approaches is the fact that a recurrent model with internal/hidden state is used \u2013 thus the algorithmic contribution is small as well.\n- Overall, the clarity of the paper could probably be improved a lot \u2013 Many paragraphs and sentences are hard read and lack vital explanations. For examples, see below.\n\n=== Overall Assessment ===\n\nIn my opinion, the paper is a borderline case. Currently, I see the paper slightly below the acceptance threshold. Both the theoretical and the algorithmic contribution of the paper are small. Overall, the paper is a straightforward extension of [1] to POMDPs with meta-RL experiments. Yet, due to a lack of clarity, it is hard to read. Nonetheless, the proposed algorithm seems to be practically relevant for meta-RL - thus I am happy to hear other opinions and open to being convinced to increase my score.\n\n=== Questions & tips for improvement ===\n\nAppendix A.1 briefly discusses the differences to [1], and claims to derive similar Theorems under more appropriate assumptions / premises in Appendix A.7. How do the theorems in A.7. differ from the ones in Section 5? If the theorems build on more appropriate assumptions, why not use them in the main part of the paper?\n\nAdding a simple simulation study that shows how much smaller the gap/discrepancy is, when branched instead of full rollouts are used, would require little effort and strengthen the claim that branched rollouts are preferable.\n\nI found section 5.1. to be very confusing \u2013 It is stated that the gap can be \u201cexpressed as the function of two error quantities of the meta-model: generalization error due to sampling and distribution shift due to the updated meta-policy\u201d. Indeed Def. 2 is probably the generalization error due to sampling. However, Def. 1 is the expected TV distance of the estimated model from the true model. Then, how should this be the \u201cdistribution shift due to the updated meta-policy\u201d?\n\n=== Minor comments ===\n\n- Section 4, 3rd paragraph: It should probably be \"$r_{t}$ and $o_{t+1}$ are assumed to be conditionally independent\n- Section 5.1.: It would be good to have a proper definition if $\\pi_{\\mathcal{D}}$, i.e. the data-collection policy\n- Theorem 1: The definition of $\\epsilon_m$ is duplicate whereas the $\\epsilon_\\pi$ is not included\n- Section 7, 4th paragraph: Should be \u201cIn Humanoid-direct, the performance\u201d\n\n\n[1] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. NeurIPS 2019\n", "title": "Fair paper - Straightforward extension of Janner et al. (2019) to POMDPs", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "QPFhxk3hokp": {"type": "review", "replyto": "KOtxfjpQsq", "review": "\n\n\n## General Summary\n\nThe contributions of this paper are two-fold. For one, the authors propose a meta model-based RL algorithm working with a meta-policy and a meta-model. In addition, the authors provide a lower bound of the meta-policy performance.\nThe algorithm is compared against PEARL and L2A, there is a further comparison against M2PO in the appendix.\n\n## Writeup\n\nI find the writeup quite dense and difficult to follow. For one, the idea behind meta-learning is generalization across different tasks/environments. While this was introduced well in sections 3 and 4, the connection is missing in the experiment section and algorithm section. The notation can be better introduced.\n\n## Pros\n\nThe theoretical analysis is the greatest asset of this paper, it is useful to have a bound on the actual performance of the policy that depends on the length of the model rollout, model error and policy distribution shift. Also, it is a valid extension of MBPO by Jenner et al. (2019). \n\n## Cons\n\nThe policy distribution shift and model error based bounds have been addressed in the model-based RL literature. What separates this paper is the extension to POMDPs and meta-learning formalism. The writeup is dense, although the idea behind is simple (branched model rollouts in POMDPs).\n\n## Comments\n\nMaybe it's because I don't work on meta-learning, but I think that you are really missing a clear description of the algorithm. Where are the task switches happening, how does the initial task distribution come into play? This should be clear from algorithm 2 in my opinion.\n\nIn Algorithm 2. you could introduce a loop over tasks maybe? I realize that in the environments that you used, task switches happen  during the episode, but I wouldn't rely on that in the algorithm description because it's a bit confusing, looking at the the algorithm itself, I don't see a meta-learning algorithm but rather a model-based RL algorithm for POMDPs. \n\nThe result that performance degrades with increasing model rollout horizon is not surprising and is the consequence of error compounding. This is a well-known result coming from standard model-based RL and MPC literature.\n\nBranched rollouts (although meta-branched as the authors name them) are also also not a novel addition, as the authors note in the paper. The improvement was introduced in earlier literature (Dyna, MBPO).\n\nCouple of questions for the experiment section:\n\n* Are the results for L2A correct? They seem to be too bad to be true based on the results reported in the original paper.\n* In Fig. 1, by number of training samples it is meant number of time steps in the environment? \n* The variance of the performance is quite high for the ant fwd-bwd, much higher that the other methods. Any explanation for that?\n* In Fig. 1, the legend should appear for the whole figure and not just for the upper-left graph.\n* The performance of PEARL with long-run is better in 3/6 experiments and seems comparable in the others, does this mean that M3PO doesn't use new data efficiently? Also, I would be interested in knowing how the variance looks like for those longer runs.\n* In Fig. 2, it is not really surprising that the performance degrades so much with the rollout length. What is surprising to me is that oftentimes the performance is best for k=1, indicating that there is not much use of the model except for 1 time step, this is in contrast to Janner et al. (2019) in the non-meta setting.\n* I would be interested in seeing how the method generalizes across  OOD unseen tasks (not seen on train), such that the training and test distributions are different and then plot training vs test performance.\n\nIt's good that the authors make a clear comparison between this work and Jenner et al. (2019) because there is a lot of overlap and as I understand, code was reused. Nevertheless, I am concerned about the amount of novelty, \nthis is an extension to POMDPs with a correction to the theory. \n\nIs the meta-learning formalism even needed for this kind of algorithm and evaluation? \n\nText-intricacies:\n* check that the references are correct, p. 10 you have a reference of the form:  MBPO. [link]\n* sec. 3 par. 2 polity->policy\n\n\n", "title": "Theory improvement and small extension of existing work", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "R9qcFIlPDS": {"type": "review", "replyto": "KOtxfjpQsq", "review": "### Paper summary\n\nThis paper focus on model-based RL on a POMDP setting (they call it \"meta RL\"), where the policy and model need to infer the current hidden state according to history. It provides a theoretical relation between true environment returns and the returns from learned models in a POMDP setting. And it also provides a practical algorithm called M3PO and shows this algorithm is more sample efficient than some meta-RL baselines in some continuous control tasks.\n\n### Pros\n\n- Extend an existing work ( Janner et al. (2019) ) to a POMDP setting\n- Refine some theorems in Janner et al. (2019), by taking more important premises into consideration\n\n### Cons\n\n- The theory part of this paper is quite similar to Janner et al. (2019). It seems they just apply similar theorems to the POMDP setting. For example, theorem 1 in this paper is an immediate result when combining Theorem 4.1 in Janner et al. (2019) and Lemma 1 in Silver & Veness (2010). And the idea of using branched rollout to control model errors is not first proposed by this paper either.\n- The proposed algorithm (M3PO) is also very similar to algorithm 2 in Janner et al. (2019), and they just use this algorithm in POMDPs. From an implementation perspective, there are only two differences: 1. using GRU as network architectures, 2. use PEARL in policy optimization. So I feel the novelty in the proposed algorithm is limited.\n- In the experiments, it seems M3PO (their algorithm) only has significant advantages over PEARL on Halfcheetah-fwd-bwd and Ant-fwd-bwd. In the other environments, the performance gap seems not very clear. Also, in Fig 1, they only show the horizon of 0.2 M steps, which is a relatively short training time for these difficult tasks. And if we compare M3PO-long and PEARL-long, it seems M3PO-long is overall worse than PEARL-long.\n\n### Questions\n\n- Why MBPO ( Janner et al. (2019) ) does not support POMDP? Just because it does not use RNN?\n\n### Minor points:\n\n- The figures are pretty hard to read, especially fig 2.\n\n### References\n\n- Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Proc. NeurIPS, 2019.\n- David Silver and Joel Veness. Monte-Carlo planning in large POMDPs. In Proc. NIPS, pp. 2164\u2013 2172, 2010.", "title": "Limited novelty compared to the existing works", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "OglSMOcOSnG": {"type": "review", "replyto": "KOtxfjpQsq", "review": "### **Summary and Contributions of Paper** \nThis paper modifies the work \"When to Trust Your Model: Model-Based Policy Optimization\" (Janner, 2019) to handle the meta-learning case. The definitions, paper structure, and algorithm flows are all very similar to (Janner, 2019). The main difference is that the model architecture is an RNN (and hence conceptually, a new hidden state vector h_{t} is introduced) to handle adaptation for different given tasks, whereas (Janner, 2019) used a probabilistic feed-forward network as the model.\n\nExperiments are conducted on standard RL meta-learning benchmarks (e.g. ForwardBackward Halfcheetah), to compare sample efficiencies between different model-based techniques.\n\n### **Strengths**\n- Contributes to the literature in model-based techniques in the meta-learning setting, which is generally lacking. \n- Experimentation seems to show the desired result - i.e. the paper's method is more sample efficient than both PEARL and L2A.\n- Paper shows that a small modification of (Janner, 2019) using an RNN can solve meta-learning problems in RL.\n\n### **Weaknesses**\n- A bit _too_ similar to (Janner, 2019). Novelty-wise, it seems like a small (but effective) modification of the work in (Janner, 2019) from model-based optimization in the single MDP setting to the meta-learning setting, by including a recurrent/RNN model. The main contribution seems to be this modification, but all other techniques of the paper (e.g. defining the correct terms such as a _branched rollout_, generating the concepts such as C(eps, eps'), etc.) are exactly the same in (Janner 2019). I think the paper overemphasizes the portions that have already been written in (Janner, 2019).\n\n- A bit hard to read at times, due to the denseness of the notation. This is especially encountered in page 6, where there are large chunks of text + math without breathing room. I suggest that the authors provide a high level summary in the main body, especially because most terms have already been defined in (Janner, 2019).\n\n### **Clarity Comments**\n- Since the paper is very similar to (Janner, 2019), it may be useful to highlight the main differences (e.g. different coloring of changed lines in the Algorithmic boxes), rather than redefining everything already mentioned - it was difficult to parse which sections were novel/changed and which were from (Janner, 2019). \n\nOverall, the main benefit seems to be this RNN modification and relevant meta-learning experiments (which is why I give a marginal accept), but I do think that the authors can re-write the paper in a much simpler fashion.", "title": "Seems like the TL;DR is a modification of (Janner, 2019)'s method to use an RNN model for meta-learning.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}