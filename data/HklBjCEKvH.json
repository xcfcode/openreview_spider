{"paper": {"title": "Generalization through Memorization: Nearest Neighbor Language Models", "authors": ["Urvashi Khandelwal", "Omer Levy", "Dan Jurafsky", "Luke Zettlemoyer", "Mike Lewis"], "authorids": ["urvashik@stanford.edu", "omerlevy@gmail.com", "jurafsky@stanford.edu", "lsz@fb.com", "mikelewis@fb.com"], "summary": "We extend a pre-trained neural language model by linearly interpolating it with a k-nearest neighbors model, achieving new state-of-the-art results on Wikitext-103 with no additional training.", "abstract": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this transformation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 -- a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.", "keywords": ["language models", "k-nearest neighbors"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes an idea of using a pre-trained language model on a potentially smaller set of text, and interpolating it with a k-nearest neighbor model over a large datastore. The authors provide extensive evaluation and insightful results. Two reviewers vote for accepting the paper, and one reviewer is negative. After considering the points made by reviewers, the AC decided that the paper carries value for the community and should be accepted."}, "review": {"Bke5gwsosr": {"type": "rebuttal", "replyto": "Hke8y2cPjH", "comment": "Hello Reveiwer1, we just wanted to follow up on our previous comment about generating results for a CNN-based model. Our experiments are currently running, but unfortunately we won\u2019t have the results before the end of the discussion period. \n", "title": "Follow-up"}, "Hke8y2cPjH": {"type": "rebuttal", "replyto": "H1eELYG-cS", "comment": "Hello Reviewer1,\n\nThanks for your comments. We\u2019re glad you enjoyed the paper!\n\nEfficiency:\nBuilding the datastore: A single epoch of training over the Wikitext-103 data takes ~5 hours on a single GPU. In comparison, a single forward pass over the same dataset to save keys/values took ~4 hours. Then, creating the datastore using FAISS took two hours on a single CPU. Hence, building the datastore is is comparable to a single epoch of training. In addition, the saving of keys/values as well as creating the datastore are trivial to parallelize.\n\nInference: We measured the decoding speed of kNN-LM and found that it can sample roughly 60 tokens per second on one GPU, which is easily fast enough for most applications (albeit slower than the vanilla LM, which can sample roughly 500 tokens per second). Improving the efficiency is not a focus of this work, but it is likely that it could be significantly improved - for example, by downsampling frequent words from the datastore.\n\nOther architectures:\nWe are in the process of evaluating the model on a CNN-based LM as well! Thanks for the suggestion!\n\nFuture work:\nWe also agree that applying kNN-LM to translation would be an exciting next step which we hope to pursue in followup work!\n\nThanks!\n", "title": "Response to Reviewer1"}, "S1xWjs9woH": {"type": "rebuttal", "replyto": "ryghQUjTYH", "comment": "Hello Reviewer2,\n\nThanks for your comments. \n\nAs per your suggestion, we\u2019ll add more details comparing our method against related work! The difference between kNN-LM and prior work is certainly larger than just operating at the token vs. the sentence level: prior work uses training examples very differently. Guu et al. (2018) sample a training example at random and edit it into a new sentence. Gu et al. (2018) look up training examples using edit distance against the test string that needs to be translated. Both Gu et al. (2018) and Weston et al. (2018) train their models with the retriever and use the embeddings retrieved as inputs to the model. In contrast, our kNN module requires no training and uses retrieved examples as the model\u2019s prediction directly. \n\nThis model highlights the effectiveness of the similarity function that is learned by the LM. In fact, our work shows that instead of using large models trained on large datasets, we may be able to use smaller models that learn effective similarity functions to generalize to larger datasets as well as to other domains, without any additional training necessary. This sets us on an exciting path of thinking about using kNN to make our models more effective without necessarily scaling them up!\n\nThanks for your notes on the memory networks literature. We\u2019re working on adding a contrast there as well as a note on work from Walter Daelemans on pre-neural memory based language processing (2005)!\n\nThanks!\n", "title": "Response to Reviewer2"}, "Hylodjcvor": {"type": "rebuttal", "replyto": "H1gQzS6XjS", "comment": "Hello Reviewer3,\n\nThanks for your comments.\n\nConvergence and Evaluation:\nRegarding the concerns about convergence and evaluation, we note that the autoregressive LM used in this work has been trained to convergence for every experiment, i.e. the dev loss is seen going up at the end of training. For such LMs (vastly different from bi-directional masked LMs like BERT), it is standard practice to evaluate performance using perplexity (Shannon, 1951; Brown et al., 1992; Goodman, 2001; Bengio et al., 2003; Mikolov et al., 2010; Baevski and Auli, 2019, Dai et al., 2019 etc.), as also noted in Felix\u2019s comment. Wikitext-103 is one of the standard benchmarks used for this task and all prior work has evaluated using perplexity as well.\n\nLarge training sets:\nRegarding large training sets, we note that two of our experiments used models trained to convergence on 3-billion tokens of Wikipedia, (1) Table 3 where we show training on so much data is perhaps unnecessary and a kNN-LM can be far more effective instead, and (2) Table 4 where we show even a model trained on this much data is not very effective at domain adaptation, while a kNN-LM makes this model useful in multiple domains and hence better at generalization.\n\nCost:\nWhile the kNN component does require storage it is not GPU based, which makes this storage very cheap. This is possible because the kNN component does not add any trainable parameters and requires no further parameter updates on the GPU, unlike larger models. Querying this module is also fast using the FAISS library, which allows a reasonable decoding speed of 60 tokens per second using the Wikitext-103 datastore containing 100-million entries.\n\nTrain/test distributions:\nFor cases where the test set distribution is vastly different from the LM training set distribution, our domain adaptation results have shown how kNN-LM helps improve generalization (shown in Table 4). For cases where the test set distribution is vastly different from both the LM training set and the datastore, we expect model generalization to be no worse than that of the base LM already. \n\nWe do agree that applying this model to tasks such as translation and summarization, as well as further research into reducing the size of the kNN datastore all make for exciting next steps.\n\nThanks!\n", "title": "Response to Reviewer3 comments"}, "BygRmOwXjB": {"type": "rebuttal", "replyto": "SJeUqRdlsH", "comment": "Thanks for your clarification, Felix!! We absolutely agree with your points and will be sure to add an explicit note to the paper highlighting the fact that this work is not related to learning better contextual representations. Thanks to you and Reviewer3, we were able to catch the ambiguity and can fix it early! \n\nWe also agree the approach could naturally be applied to summarization and translation, and think this would be an exciting direction for future work.", "title": "Thanks, Felix!"}, "rkluvmvXjr": {"type": "rebuttal", "replyto": "B1ejI7Rgor", "comment": "Hi Aurko,\n\nThanks for your comments!\n\nThere is no averaging! We save every context-target pair. So the same word appears many times in the datastore each with different keys corresponding to the different contexts it appeared in. To compute the final kNN probability of a word, we aggregate over all occurrences of that word retrieved in the k-nearest neighbors set.\n\nThanks!\n", "title": "Datastore"}, "B1gMnYs8tS": {"type": "review", "replyto": "HklBjCEKvH", "review": "This work utilizes the kNN method on dense vectors to augment the LMs. The method is simple and straightforward, meanwhile, the performance seems great if only in terms of PPL.\nThree of my most concerns:\n1)\tIt seems that this approach heavily relies on the similarity of context distribution between the training and test set. Intuitively, higher performance will be achieved with more similar examples between training and test set. This question should be discussed more in this work. This similarity cannot always satisfied in practice, I thus quite doubt the proposed method can work for general case.\n2)\tThe evaluation is only done for PPL, I notice the LM was trained in a corpus scale as pre-trained BERT, though none of real downstream tasks were evaluated like BERT. Expect to see some MRC or NLI results with the proposed LM.\n3)\tFurthermore, though FAISS is very fast, it is hard to get great results with only a small datastore which makes the retrieving slow. So it seems not suitable for tasks such as generations but maybe open-domain QA can be the scene for this method. It would be great if there are some experiments on such tasks, and also combining with models such as BERT could be much better and convincing.\n\nQuestions\nWhat about other distance functions such as cosine distance? The author only said L2 is better but there is no analysis on it.\n", "title": "Official Blind Review #1318", "rating": "3: Weak Reject", "confidence": 3}, "ryghQUjTYH": {"type": "review", "replyto": "HklBjCEKvH", "review": "\nSummary:\nThe authors extend a pretrained LM by interpolating its next word distribution with a KNN model. The authors show retrieving nearest neighbor from corpus achieve quite large perplexity decrease in several language modeling benchmarks.\n\nDecision:\nOverall, the idea seems simple but is quite effective. Even with some discussions on the related work with cache based LM and the work that use training examples explicitly, I feel it is a simple extension/usage of previous approaches. Hence I am borderline with my decision.\n\nSupporting argument:\n1. The proposed idea uses KNN to look up training examples for interpolating the prediction. As discussed by the authors, this approach is effective in factual knowledge, names, and near-duplicate sentences.\n2. There are several experiments and ablation study in showing the effectiveness of the approach.\n3. The related work that uses training examples explicitly is quite similar to the proposed approach, though the authors claim that one is at the level of individual tokens and the other is the whole training sentences.\n\nAdditional feedback:\n1. In reference, \u2018Bert\u2019 -> \u2018BERT\u2019\n2. Missing reference: Yogatama et al., Memory Architectures in Recurrent Neural Network Language Models, 2018, https://arxiv.org/abs/1410.3916, https://arxiv.org/abs/1803.02400", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "H1eELYG-cS": {"type": "review", "replyto": "HklBjCEKvH", "review": "[Overview]\n\nIn this paper, the authors proposed a simple but effective way to augmentation the language model through memorization. Specifically, after obtaining a language model on a dataset, the model further uses the dataset to build a lookup table and then a k-nearest neighbor is used to searching the closest tokens for a token during inference. Based on this, the output distribution of a target token during the inference time would be modified accordingly. Through a comprehensive experiments and ablation studies, the authors showed that the proposed strategy can improve the performance of language models significantly for both the in-domain and out-domain testing scenarios. This is very insightful considering recently a lot of language models are focusing on increasing the size of model and training data.\n\n[Pros]:\n\nOverall I think the paper is well-written and presents clearly. Detailed points below:\n\n1. the authors proposed a simple but effective method for increasing the generalization ability of language model through a memorization strategy. Specifically, the authors proposed to build a lookup table which memorizes the representation and output token pairs which are then used for the inference of language model. Different from conventional way, the proposed strategy does not introduce any more parameters in the model and also does not need any more training or fine-tuning on the target dataset.\n\n2. The authors showed that the proposed strategy can improve the performance of language generation model (i.e., transformer) without any extra training or data, as shown in Table 1. Also, using the continuous caches  with KNN-LM further improve the performance.\n\n3. Besides the main results shown in Table 1 and Table 2, the authors also showed using kNN-LM can probably outperforms the model which is directly trained on it. Also, it also supports domain adaptation from one language domain to another domain.\n\n4. Finally, the authors presented a number of ablation studies to investigate how the performance is affected by the method of building datastore, including the size of nearest neighbor, the interpolation parameter, etc. These results are also insightful and meaningful for the readers to understand the method.\n\n[Cons]:\n\nI think this paper is a solid paper. So I would have some suggestions below:\n\n1. The first concern about the method is the efficiency. At page 3, the authors mentioned that the proposed strategy will bring more time cost. It would be good if the authors can perform more systematical analysis on the time cost of building the datastore and inference for the proposed model. \n\n2. Second, the authors should not only evaluate the proposed method based on transformers. It would be good to test on various language models to verify the generalization ability across different models, including the old-fashioned one like RNN and CNN.\n\n3. Also, the authors should try to extend the proposed model to other language tasks, such as translation.\n\n[Summary]\n\nIn this paper, the authors introduced a simple but effective method to augment the pertained language model through memorizations. Though this is not absolutely new and relatively simple , the authors successfully demonstrate that it can be applied to improve the generation of language model much. The. thorough ablation studies help to understand the property of the proposed strategy. I think this paper overall is insightful and thoughtful. It would be good to see the authors add more analysis on the computational complexity and also evaluate on more type of language models.\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}, "Hke_6-MAqB": {"type": "rebuttal", "replyto": "Hkg-ndcpcS", "comment": "Hi Jack,\n\nThanks for your comments!\n\nYou raise an interesting question! We actually included a version of this experiment in the submission: by turning off dropout in our base (16 layer) model, it reached a training perplexity of 1 (see Figure 8). This result shows that the 16 layer model does have the capacity to memorize the entire training set in its parameters. \n\nHowever, ensembling the overfit model with the original LM only improved the validation perplexity by 0.1 (see discussion in Section 6). This suggests that while the Transformer certainly has the capacity to memorize the training set, doing so does not result in context representations that generalize well enough to mimic kNN-LM\u2019s explicit nearest neighbors mechanism.\n\nThanks!\n", "title": "Model ensembles"}, "Bkxs8w5adr": {"type": "rebuttal", "replyto": "HklBjCEKvH", "comment": "Hello Reviewers,\n\nSince submission, we realized that the Books-1B dataset contained duplicate books in train/test, which our model is particularly effective at exploiting. To mitigate this effect, we re-ran these experiments on a de-duplicated version of Books-1B, and present updated tables below. Results on other datasets are unchanged because those were already de-duplicated, and all conclusions still hold. \n\nTable 2 Books-1B:\nBaevski & Auli (2019) \t                Dev = 14.75 \t\tTest = 11.89\nkNN-LM \t\t\t                Dev = 14.20 \t\tTest = 10.89\n\nTable 4 Domain adaptation for Books-1B:\nWiki-3B\t\t\t\t                Dev = 37.13\t\tTest = 34.84\nBooks-1B\t\t\t\t        Dev = 14.75\t\tTest = 11.89\nWiki-3B + Books-1B datastore\tDev = 24.85\t\tTest = 20.47\n\nGiven the above results, our conclusions still follow. Table 2 shows that kNN-LM helps in domains other than Wikipedia. Table 4 shows that while an in-domain LM trained on Books-1B has relatively low perplexity (11.89), an LM trained on Wiki-3B and evaluated on Books-1B performs considerably worse (34.84). Adding a datastore containing Books-1B training examples to the Wiki-3B model reduces perplexity by 14 points (down to 20.47) demonstrating that kNN-LM allows a single model to be useful in multiple domains without additional training. \n\nThanks!", "title": "Updated Results"}}}