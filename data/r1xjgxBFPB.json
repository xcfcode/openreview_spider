{"paper": {"title": "Continual Deep Learning by Functional Regularisation of Memorable Past", "authors": ["Pingbo Pan", "Alexander Immer", "Siddharth Swaroop", "Runa Eschenhagen", "Richard E Turner", "Mohammad Emtiyaz Khan"], "authorids": ["pingbo.pan@student.uts.edu.au", "alexander.immer@epfl.ch", "ss2163@cam.ac.uk", "reschenhagen@uni-osnabrueck.de", "ret26@cam.ac.uk", "emtiyaz.khan@riken.jp"], "summary": "This paper introduces a scalable functional-regularisation approach for continual learning that uses a GP formulation of neural networks to identify and regularise over a memorable past.", "abstract": "Continually learning new skills without forgetting old ones is an important quality for an intelligent system, yet most deep learning methods suffer from catastrophic forgetting of the past. Recent works have addressed this by regularising the network weights, but it is challenging to identify weights crucial to avoid forgetting. A better approach is to directly regularise the network outputs at past inputs, e.g., by using Gaussian processes (GPs), but this is usually computationally challenging. In this paper, we propose a scalable functional-regularisation approach where we regularise only over a few memorable past examples that are crucial to avoid forgetting. Our key idea is to use a GP formulation of deep networks, enabling us to both identify the memorable past and regularise over them. Our method achieves state-of-the-art performance on standard benchmarks and opens a new direction for life-long learning where regularisation methods are naturally combined with memory-based methods.", "keywords": ["Continual learning", "deep learning", "functional regularisation"]}, "meta": {"decision": "Reject", "comment": "This work tackles the problem of catastrophic forgetting by using Gaussian processes to identify \"memory samples\" to regularize learning.\n\nAlthough the approach seems promising and well-motivated, the reviewers ultimately felt that some claims, such as scalability, need stronger justifications. These justifications could come, for example, from further experiments, including ablation studies to gain insights. Making the paper more convincing in this way is particularly desirable since the directions taken by this paper largely overlap with recent literature (as argued by reviewers).\n"}, "review": {"SkxrQ_HCKS": {"type": "review", "replyto": "r1xjgxBFPB", "review": "Summary: The paper uses a Gaussian Processes framework previously introduced in [1] to identify the most important samples from the past for functional regularization. For evaluation authors report their average accuracy on Permuted MNIST, Split-MNIST, and CIFAR10-100 and achieve superior performance over EWC, DLP, SI, VCL-Coreset, and FRCL.\n\nPros:\n(+): The paper is well-written, addressed the prior work quite well despite missing a few important work from the past (more on this later)\n(+): The paper is well motivated\n\nCons that significantly affected my score and resulted in rejecting the paper are as follows:\n\n1- lack of support for \u201cscalability\u201d:\nAuthors claim their method is scalable in several parts of the paper (abstract in line 7, Section 3 in the 1st paragraph, and Section 5 in Discussion). However, this claim is not supported in the experimental setting as the benchmark used are only toy datasets (Permuted MNIST, Split MNIST, and CIFAR10 followed by CIFAR100) where the maximum # of task considered is 10 and the maximum size of the datasets is 60K which is not convincing for ability to scale. There is also no time complexity provided. \n\n2- Incremental novelty over the prior work (FRCL by Titsias et al 2019):\nThis baseline is the closest prior work to this work which according to the experiments shown in Table 2 are slightly outperformed by the proposed method. (for example for P-MNIST the gain is 0.6%+-0.1) where there is a lack of complete discussion on how the two methods are different. Particularly I suggest that the authors elaborate more on their claimed differences stated on page 4, paragraph 5 such as \u201ctractability of the objective function only when we assume independence across tasks\u201d. Do authors mean assuming clear task boundary between tasks? If so, have they considered a \u201cno-task\u201d or an \"overlapping\u201d task boundary in their experiment? Isn't it necessary to back up this if it is stated as a shortcoming of FRCL? Also, how are these methods differ in their computational expenses?\n\n3- Lack of measuring forgetting: \nThis is the most important drawback in the experimental setting. Authors indicate on page 3 \u201cOur goal in this paper is to design methods that can avoid such catastrophic forgetting.\u201d and reiterate on this on other parts of the paper yet there is no forgetting evaluation to support this claim. Authors can simply report the initial performance of the model on each task so that readers can compare it with the reported accuracy after being done with all tasks. Having a method with high average accuracy does not necessarily mean it has minimum forgetting. You can use forgetting measurements such as Backward Transfer (BWT) introduced in [1] or forgetting ratio defined in [4] for this assessment. \n\n4- Ambiguous claims about prior work:\n(a) On page 1, paragraph 3, when authors mention that methods such as GEM or iCaRL use random selection to pick previous samples, I think the line of follow-up work on these methods should be mentioned as well that have explored different techniques for sample selection and have provided benchmark comparisons (ex. [2,3]). In fact it would be beneficial if authors could compare the samples selected by their method versus other sampling techniques. \n(b) On page 1, paragraph 3, they mention some prior work such as GEM and iCaRL \u201cdo not take uncertainty of the output into account\u201d. While it is true, there have been methods proposed that use uncertainty of the output for parameter regularization [5]. It appears to be a parallel work to this but it\u2019s worth mentioning to prevent false claims.\n\n5- Claim on the state of the art should be double-checked:\n\tAlthough the results shown for the experiments are superior to the provided baselines, there is an important baseline missing which has achieved higher performance than the reported ones. Also missed to be cited in the prior work list. Serra et al [4] proposed a method at ICML 2018 called HAT, which is a regularization technique with no memory usage that learns an attention mask over parameters and was shown to be very effective on small and long sequence of significantly different tasks. They do not use samples from previous task but yet achieved good average ACC as well as minimum forgetting ratio. Note that 5-Split MNIST is not reported in [4], but a recent work has reported HAT\u2019s performance on this dataset (https://openreview.net/forum?id=HklUCCVKDB) that achieves 99.59%. I recommend authors provide comparison of their own on the given benchmarks with the original HAT\u2019s implementation (https://github.com/joansj/hat) before claiming to be SoTA. In my opinion, it is not an issue if a novel method achieves a slightly lower performance to the sota because I think it still adds value and proposes a new direction. However, a false claim should not be stated.\n\nLess major (only to help, and not necessarily part of my decision assessment):\n\n1- Providing upper bound?\nIt is common to show an upper bound for any continual learning algorithm by showing joint training performance which is considered to be the maximum achievable performance. I also recommend showing the naive baseline of fine-tuning for the proposed method  which often can give insight to maximum forgetting ratio.\n\n2- Forward transfer?\nRegularization techniques combined with memory might have an ability to perform zero-shot transfer or so called FWT. I recommend authors provide such metric to further support their method.\n\n3- Hyper parameter tuning?\nIt is also worth mentioning how the tuning process was performed. In continual learning we cannot assume that we have access to all tasks' data, hence authors might want to shed some light on this.\n\nReferences: \n[1] Khan, Mohammad Emtiyaz, et al. \"Approximate Inference Turns Deep Networks into Gaussian Processes.\" arXiv preprint arXiv:1906.01930 (2019).\n\n[2] Chaudhry, Arslan, et al. \"Continual Learning with Tiny Episodic Memories.\" arXiv preprint arXiv:1902.10486 (2019). (https://arxiv.org/abs/1902.10486)\n\n[3] Aljundi, Rahaf, et al. \"Gradient based sample selection for online continual learning.\" arXiv preprint arXiv:1903.08671 (2019). (https://arxiv.org/abs/1903.08671)\n\n[4] Serr\u00e0, J., Sur\u00eds, D., Miron, M. & Karatzoglou, A.. (2018). Overcoming Catastrophic Forgetting with Hard Attention to the Task. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:4548-4557\n\n[5] Ebrahimi, Sayna, et al. \"Uncertainty-guided Continual Learning with Bayesian Neural Networks.\" arXiv preprint arXiv:1906.02425 (2019).\n\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nPOST-REBUTTAL Response from R1:\n\nThank you for taking the time and replying to comments. Here are my responses to authors' replies:\n\n[Authors' response:] 1. Scalability: Our algorithm only adds a small computational overhead on top of Adam on a standard neural network. This is what we mean by scalable. The additional complexity scales cubically in M, the coreset size. This is due to the inversion of the kernel in fr_grad. Another overhead is the computation of Jacobian which is order PKM, where K is the dimensionality of the output and P is the number of network parameters. Both of these additional costs are small for small coreset sizes M. We will add these details to make these points clear in the paper.\n\n[Reviewer's response:] I still insist on the fact that simply explaining the overhead of a method is not a support for scalability claim versus showing the performance on a large scale dataset and comparing it with other CL methods that also have high scalability given the fact that authors only use MNIST and CIFAR datasets.\n\n[Authors' response:] 4. Prior work: We discuss other works in Section 1 (\u201ctwo separate methods are usually used for regularisation and memory-building\u201d), and we will expand upon this sentence, going into more detail, and also referencing iCaRL and other works (including [3]). Note that our method of choosing a memorable past follows directly from the theory in Section 3.1, and is achieved with a single forward-pass through the trained network (as mentioned in the paper). Other techniques for sample selection do not integrate so naturally with the framework, and are not as straightforward to understand or implement either.\n\n[Reviewer's response:] I disagree with authors on this because GEM, its faster version (A-GEM (Chaudhry et al. 2018)), and all other methods explored in the recent study which I mentioned in my review (Ref#2) use the single epoch protocol and are perfect match to be compared with this method but there is no memory-based baseline except for VCL with coreset and FRCL (only for MNIST variations) which makes it difficult to measure this method's capabilities (performance, memory size, and computational time) against methods which only require one epoch to be trained.\n\nAuthors have provided FWT for their method as 6% which is unbelievably large for this metric (see GEM paper) and hence does not make sense to me. Please double check whether you computed this value right. \n\nWhile I accept the response for the remaining questions from authors but I am still concerned about the weak experiments and an issue brought up by R3 regarding lack of enough comparisons with FRCL on any other datasets besides split MNIST and P-MNIST. Also in  CIFAR experiment, what is the architecture used across the baselines? More importantly in results reported for VCL on CIFAR, it is not clear to me how authors obtained this results. Did they use a conv net? VCL was originally shown on MLPs only and it is one of the downside of this method that was never shown to be working in convolutional networks. Therefor, it is important to mention how they are obtained. This might explain the reason for the huge forgetting reported for VCL with coreset (\u22129.2 \u00b1 1.8) as opposed to \u22122.3 \u00b1 1.4 for EWC which is really strange as VCL even without coreset (on permuted mnist for example) is reported superior to EWC by a large margin (6%) in the original VCL paper. Overall I am concerned about the experimental setup and some of the reported results and hence intend to keep my score.", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 3}, "r1lb8nRosB": {"type": "rebuttal", "replyto": "r1xjgxBFPB", "comment": "We have update the paper in line with the reviewers' feedback. \n\nSummary of changes:\n- We added backward transfer and forward transfer metrics on split CIFAR for continual learning. They show how FROMP outperforms the baselines. We also added an upper bound of a model jointly trained on all tasks. FROMP performs close to this model, especially on tasks 4-6.\n- We added a visualisation of memorable past vs random examples for split MNIST. The memorable past examples are harder to distinguish from other classes, in line with the toy example in Figure 1.\n- We added a paragraph discussing the time complexity of our algorithm. It is small for small memorable past size M.\n- We added a detailed discussion regarding FRCL.\n- We added detailed hyperparameters for our experiments.\n- We added more references and expanded upon some previous work (as suggested by AnonReviewer1).\n- We cleaned up some notation and explanation in Section 3.3 and the Algorithm (and Appendix A). Please note that nothing technical has changed, and the overall algorithm is exactly the same.\n\nMany thanks for your time.", "title": "Paper improvements"}, "Hkev9olmiS": {"type": "rebuttal", "replyto": "Skgef0KaYH", "comment": "Thank you for your review. Our response is given below.\n\nRegarding the use of deeper networks: This paper develops a new method for continual learning. In line with previous literature in the area, we first evaluate the method on standard continual learning benchmarks established to evaluate new methods including EWC, SI, VCL and FRCL. In terms of scaling up, our current implementation requires Jacobian computation, which requires additional implementation to speed up computation. We hope to do this in the future.\n\nRegarding the dependance on the number of the coreset: We believe there is a misunderstanding about our experiments. Coresets are an important part of FROMP, and are the only way in which past information is propagated. Using very few coresets is not meaningful since there is very little to \u201cremember\u201d from the past. Comparing such small-size coreset cases to methods \u201cwithout coresets\u201d is not meaningful as these are complementary approaches, not direct competitors. As the coreset size gets very large, the selection strategy is not expected to matter. The purpose of Fig. 3c and 4b is to show that increasing coreset size improves results as expected, and using selected coresets rather than random is useful when the size of coreset is small. For example, selectively choosing a coreset of size 10 is about the same as randomly choosing 30 (on split CIFAR, Figure 4b). The ultimate number of coreset examples depends on the problem (e.g. data and network size). We are happy to discuss this further if this is unclear. Thanks!\n\nTraining time: We will add a discussion in the paper. Our algorithm only adds a small computational overhead on top of Adam on a standard neural network. The additional complexity scales cubically in M, the coreset size. This is due to the inversion of the kernel in fr_grad. Another overhead is the computation of Jacobian which is order PKM, where K is the dimensionality of the output and P is the number of parameters. Both of these additional costs are small for small coreset sizes M.", "title": "Response to AnonReviewer2"}, "rJgGDoemsr": {"type": "rebuttal", "replyto": "SJezZcn6FS", "comment": "Thanks for your comments about the strengths and weaknesses of our work. Our response is given below.\n\nWe agree regarding the comparison with FRCL, but this is a very recent work and there is no available code. We will try to add this in the camera-ready but it will depend on the reproducibility of the FRCL paper (e.g. if they provide all the details necessary to reproduce results).\n\nWe also agree on your comment about experimental details. We shall add them. For permuted MNIST, we used 10 tasks.\n\nRegarding your comment about EWC, could you provide a reference regarding this? We have reported the results from [1]. It is also possible that 97% is obtained with a much larger network than ours.\n\n[1] Nguyen, Cuong V et al. Variational continual learning. ICLR, 2018.", "title": "Response to AnonReviewer3"}, "SylF4jgXjr": {"type": "rebuttal", "replyto": "SkxrQ_HCKS", "comment": "Thank you for your long and useful review. We will first provide a short summary of our response, before going into more detail. \n- In terms of scalability, we test on standard small to medium size benchmarks, with complexity above Adam (on a standard neural network) dependent on M, the coreset size. \n- We will add more details comparing our method to FRCL [1], and provide a short summary below. \n- We will provide more metrics for measuring forgetting. \n- We will add a more detailed review of the literature on other coreset selection strategies, but unlike our strategy, these do not naturally fit within our framework. \n- We respectfully disagree with you on our claim about our method being state-of-the-art being false.\n\n1. Scalability: Our algorithm only adds a small computational overhead on top of Adam on a standard neural network. This is what we mean by scalable. The additional complexity scales cubically in M, the coreset size. This is due to the inversion of the kernel in fr_grad. Another overhead is the computation of Jacobian which is order PKM, where K is the dimensionality of the output and P is the number of network parameters. Both of these additional costs are small for small coreset sizes M. We will add these details to make these points clear in the paper.\n\n2. Comparison to FRCL [1]: \n(a) Thank you for raising this point. FRCL proposes using the last layer of the neural network as kernel features. This is limiting as it does not use the whole network\u2019s weights, unlike what we do. A more important issue is with the difficulty of optimising inducing points; they are usually obtained by an ad-hoc procedure. In comparison, we provide a simple, effective way that is naturally consistent with our GP formulation. As per your suggestions, we will add a more detailed discussion explaining this.\n(b) There is a misunderstanding about our statement on \u201ctractability of the objective function only when we assume independence across tasks\u201d. This is not about the task boundaries. We mean that the GP used in FRCL defines separate kernels for each task, since otherwise the kernel is too big.\n\n3. Measuring forgetting: Thank you for raising this point. We agree and will provide these. We are trying our best, but these may not be available by the end of rebuttal, in which case we will add them in the next version of the paper.\n\n4. Prior work: We discuss other works in Section 1 (\u201ctwo separate methods are usually used for regularisation and memory-building\u201d), and we will expand upon this sentence, going into more detail, and also referencing iCaRL and other works (including [3]). Note that our method of choosing a memorable past follows directly from the theory in Section 3.1, and is achieved with a single forward-pass through the trained network (as mentioned in the paper). Other techniques for sample selection do not integrate so naturally with the framework, and are not as straightforward to understand or implement either.\n\n5. Claim on state-of-the-art: We respectfully disagree with you on our claim being false. The 99.59% accuracy of HAT [2] on split MNIST is achieved with a much larger network. On the network we use, HAT achieves 91.6% on permuted MNIST, significantly lower than FROMP (94.9%), FRCL (94.3%) and VCL (93%). The openreview link you provided also uses a much larger network size (1200 units per hidden layer, as opposed to 256). We will add a reference to this work.\n\n[1] Titsias, Michalis K et al. Functional regularisation for continual learning using gaussian processes. arXiv preprint arXiv:1901.11356, 2019.\n[2] Serr\u00e0, J., Sur\u00eds, D., Miron, M. & Karatzoglou, A.. (2018). Overcoming Catastrophic Forgetting with Hard Attention to the Task. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:4548-4557.\n[3] Ebrahimi, Sayna, et al. \"Uncertainty-guided Continual Learning with Bayesian Neural Networks.\" arXiv preprint arXiv:1906.02425 (2019).", "title": "Response to AnonReviewer1"}, "BygTT5g7sr": {"type": "rebuttal", "replyto": "r1xjgxBFPB", "comment": "We would like to thank all the reviewers for their reviews, and the time they put into providing feedback. We will update the paper incorporating their feedback. We are in the process of obtaining some further metrics and visualisations as suggested by the reviewers, and will report them once we have them.\n\nWe will now address the points made by each reviewer in turn.", "title": "Thanks to reviewers"}, "Skgef0KaYH": {"type": "review", "replyto": "r1xjgxBFPB", "review": "The paper proposed a new functional regularization method with gaussian process which has similar direction with recent two works (khan et al, titsias et al).\nTo perform functional regularization, they introduce small coreset which are selected from previous dataset instances, called memorable past. They select most memorable samples depends on eigenvalue. The model FROMP outperforms baselines and their ablations. However, the experiments are only performed on shallow networks, it is required to apply on much deeper networks, such as ResNet. Also, in the experiment results, I feel the performance of the FROMP largely depends on the number of the coreset, while 'important' selection just shows marginal effects even on split CIFAR. \nFROMP show higher performance than FRORP with only a few of examples, but it isn't meaningful results that anyway the performances are too poor that are even worse than old baseline, EWC. \n\nI have several wonderings on the paper.\n\n- How about of training time on FROMP? I wonder if utilizing or selecting memorable pasts requires much time for training.\n\n- Is there an analysis like figure 1 on real dataset, such as MNIST or CIFAR?\n\n\n\n", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 3}, "SJezZcn6FS": {"type": "review", "replyto": "r1xjgxBFPB", "review": "Summary\nThe paper proposes a method for continuous learning called Functional Regularization of Memorable Past (FROMP) which maintains the output distribution of models on memory samples. FROMP uses the Laplace approximation and Gaussian process with neural tangent kernel (NTK) to approximate the output distribution. According to the leverage score strategy, the sample to be stored is selected. The leverage score strategy tends to select the sample of highest variances. \n \nStrengths\nTo some extent, I think the proposed method is novel, although there is a similar work named as Functional Regularisation for Continual Learning (FRCL). FROMP first uses NTK in Gaussian process for continual learning and proposes a new strategy of selecting memory samples.\nThe strategy of selecting samples to be stored is simple and effective.\nThe method achieves a good performance.\nThe paper is clearly written and easy to follow.\n \nWeaknesses\nIt needs more experimental comparisons between FROMP and FRCL, like adding comparison results of FROMP and FRCL for Split-Cifar. Currently, this paper only shows the performance on Permuted MNIST and Split MNIST but those two benchmark are quite simple and also the improvement is limited.  \nThe experimental section needs more detailed analysis. At least, in current version, it is not clear how many tasks in Permuted MNIST. The setting of hype-parameters for dropout are not provided.\n \nOther comments \nIn this paper, for Split MNIST experiment with multi-head, it shows that the method of EWC achieves worse results than SI. However, in my experiment, the precision of EWC is at least larger than 97%. In theory, I think they should have the similar performance and at least the discrepancy of accuracy between them is not as big as shown in this paper. I expect authors could explain this point.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}}}