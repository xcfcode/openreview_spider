{"paper": {"title": "AdvCodec: Towards A Unified Framework for Adversarial Text Generation", "authors": ["Boxin Wang", "Hengzhi Pei", "Han Liu", "Bo Li"], "authorids": ["boxinw2@illinois.edu", "hzpei16@fudan.edu.cn", "hanliu@northwestern.edu", "lbo@illinois.edu"], "summary": "we propose a novel framework AdvCodec to generate adversarial text agaist general NLP tasks based on tree-autoencoder, and we show that AdvCodec outperforms other baselines and achieves high performance in human evaluation.", "abstract": "Machine learning (ML) especially deep neural networks (DNNs) have been widely applied to real-world applications. However, recent studies show that DNNs are vulnerable to carefully crafted \\emph{adversarial examples} which  only deviate from the original data by a small magnitude of perturbation. \nWhile there has been great interest on generating imperceptible adversarial examples in continuous data domain (e.g. image and audio) to explore the model vulnerabilities, generating \\emph{adversarial text} in the discrete domain is still challenging. \nThe main contribution of this paper is to propose a general targeted attack framework \\advcodec for adversarial text generation which addresses the challenge of discrete input space and be easily adapted to general natural language processing (NLP) tasks. \nIn particular, we propose a tree based autoencoder to encode discrete text data into continuous vector space, upon which we optimize the adversarial perturbation. With the tree based decoder, it is possible to ensure the grammar correctness of the generated text; and the tree based encoder enables flexibility of making manipulations on different levels of text, such as sentence (\\advcodecsent) and word (\\advcodecword) levels. We consider multiple attacking scenarios, including appending an adversarial sentence or adding unnoticeable words to a given paragraph, to achieve arbitrary \\emph{targeted attack}. To demonstrate the effectiveness of the proposed method, we consider two most representative NLP tasks: sentiment analysis and question answering (QA). Extensive experimental results show that \\advcodec has successfully attacked both tasks. In particular, our attack causes a BERT-based sentiment classifier accuracy to drop from $0.703$ to $0.006$, and a BERT-based QA model's F1 score to drop from $88.62$ to $33.21$ (with best targeted attack F1 score as $46.54$). Furthermore, we show that the white-box generated adversarial texts can transfer across other black-box models, shedding light on an effective way to examine the robustness of existing NLP models.", "keywords": ["adversarial text generation", "tree-autoencoder", "human evaluation"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a method for generating text examples that are adversarial against a known text model, based on modifying the internal representations of a tree-structured autoencoder.\n\nI side with the two more confident reviewers, and argue that this paper doesn't offer sufficient evidence that this method is useful in the proposed setting. I'm particularly swayed by R1, who raises some fairly basic concerns about the value of adversarial example work of this kind, where the generated examples look unnatural in most cases, and where label preservation is not guaranteed. I'm also concerned by the fact, which came up repeatedly in the reviews, that the authors claimed that using a tree-structured decoder encourages the model to generate grammatical sentences\u2014I see no reason why this should be the case in the setting described here, and the paper doesn't seem to offer evidence to back this up."}, "review": {"rygOacSKjH": {"type": "rebuttal", "replyto": "rkeeoeHYvr", "comment": "General Responses\nWe thank the reviewers for their valuable comments and suggestions. Based on the review comments, we have revised Section 3 and Section 4 to make the presentation clearer. We also added 3 sections in the appendix and conducted additional experiments following the reviews\u2019 suggestions.\n\nSpecifically, we made the following revisions:\n1. We updated Section 1 to clarify our technical innovation and contributions.\n2. We added more explanation on AdvCodec(Word) in Section 3.\n3. We moved the scatter attack results from the appendix to Section 4.\n4. We added a section to discuss the AdvCodec training details in Appendix A, including how to select a good autoencoder, how to train our tree autoencoder, how the attack is performed. We also added additional experiments on how to select the good initial seed for QA, and showed the untargeted scatter attack results for QA.\n5. We added a section in Appendix B to discuss how classification models and QA models are trained along with their hyperparameter settings for the baseline attack methods.\n6. We added a section in Appendix C to show more adversarial examples generated by our AdvCodec framework.\n7. We fixed the typos and minor errors pointed out by the reviewers.\n\nPlease don\u2019t hesitate to let us know if you have any additional comments.", "title": "General Response"}, "HJeWfvBFjH": {"type": "rebuttal", "replyto": "S1lpcqm6YS", "comment": "Q1: \u201cThe paper achieves good success rate based on its experimental results but doesn't convince me that 2) (the generated texts are reasonable (e.g. syntactically correct) and are not contradictory to the original texts) is also guaranteed.\u201d\nA1: \nWe totally agree with reviewer #2 on the challenges of generating good adversarial texts. So we evaluate our adversarial sentences based on two metrics: \n1) the linguistic quality;\n2) human accuracy comparison based on benign and adversarial texts, as illustrated in Section 5.\nFor 1) we calculate the ratio of the generated adversarial texts that can be recognized as \u201cnatural\u201d by human to evaluate the linguistic quality.\nFor 2) we record the accuracy of human performance on tasks (e.g. classification and QA) based on both benign and adversarial texts as shown in Table 10 and 11.\nSo far the above metrics are what we can come up with and they are also standard to validate the adversarial examples for NLP domains, which have also been used in other state-of-the-art adversarial text generation work [2][3][4].\n\nEmpirically, we first confirm AdvCodec(Sent) under the tree constraints are syntactically correct, which is demonstrated by the human study in Section 5.1. Then we verify that \u201cour adversarial sentences do not contradict the original texts\u201d via human evaluation in Section 5.2 and show that our adversarial datasets do not significantly affect human judgment. We can also evaluate the generated adversarial text quality by looking at the samples in table 1 and the updated Appendix C. \n\u2014\n\nQ2: \u201cThe paper mentioned that human can ignore irrelevant tokens added by the proposed scatter attack method but it is an extra assumption added to the grammatical correctness.\u201d\nA2:\nThank you for pointing it out and we are sorry to make the confusion. This is actually our another interesting discovery based on human evaluation: we find that by adding scatter words the human performance on these generated texts will not be largely affected. We admit that the scatter attack cannot ensure grammatical correctness since it does not consider the global syntactic constraints and only manipulates on the word level, and it is just another discovery and we have made this clear in the revision. \nTo ensure better grammatical correctness, we suggest using AdvCodec(Sent) whose language quality is confirmed by human readers.\n\u2014\n\nQ3: \u201cIs scatter attack not effective to attack QA task?\u201d\nA3: \nThank you for the interesting question. Based on the suggestion, we conducted additional experiments by performing the scatter attack on QA. Indeed, we find that the targeted attack success rate is not satisfactory. It turns out QA systems highly rely on the relationship between questions and contextual clues, which is hard to break when setting an arbitrary token to a target answer. This is also why we use some heuristics to creating a similar fake context when initializing QA appended sentence. We have made it clear in Section 4.2.\n\nWe also performed the untargeted scatter attack on QA. The results are shown in Table 13 in Appendix A.3. We insert 30 random tokens (but no more than 1/3 the total words of the paragraph) over the paragraph, and optimize the adversarial tokens to mislead the model. We observe that the untargeted scatter attack can achieve a higher untargeted attack success rate (adversarial F1 of 49.7) than Jia & Liang (adversarial F1 of 52.6) [4]\n\u2014\n\nQ4: \u201cthe paper reports the human evaluation on adversarial texts which shows accuracy degradation and low votes. Ideally, the human accuracy on adversarial texts should also be compared to justify 2). More examples can be added to reduce \"noise\" mentioned in the paper.\u201d\nA4: \nThanks for the suggestions. \nWe compare the human accuracy on both benign and adversarial texts for both tasks (QA and classification) in the revision section 5.2 with more samples in Appendix C.\nThe human performance drops a bit on adversarial texts.\nIn particular, it drops around 10% for both QA and classification tasks based on AdvCodec as shown in Tables 10 and 11. We believe this performance drop is tolerable and the stoa generic based QA attack algorithm experienced around 14% performance drop for human performance [4].\nIn addition, we also discuss other reasons for the human performance drop in the appendix B.4. Possible factors include the (majority vote) aggregation noise, length of paragraph and sampling randomness.\n\u2014", "title": "Response to Reviewer #2 (Part 1)"}, "BJeL0LHtiB": {"type": "rebuttal", "replyto": "S1lpcqm6YS", "comment": "Q5: \u201cin figure 1, will you encode the original text along with the appended sentence into one vector? then, how do you guarantee that the perturbation only applies to the appended sentence but not the original text for the ADVCodec(sent)? or the original text will be reproduced due to the autoencoder?\u201d\nA5: \nThank you for the interesting question and sorry for the confusion. We have made it clear in the revision that we will not encode the original text. Original text will not be perturbed or modified under any circumstances: we only add perturbation to the appended sentence for the concat attack; and we only manipulate on the scattered words for scatter attack while keeping original tokens unperturbed by masking out the perturbation on them. We have added more details in Section 3.3.\n\u2014\n\nQ6: \u201cit will be helpful to add more details on training and optimization. For example, is the autoencoder trained by the authors or is from the existing model? what does the confidence score in (5) means empirically and how to choose its value?\u201d\nA6: \nThanks for the suggestions. We have added more details on training and optimization in Appendix A.1 and A.2. The tree autoencoder is trained by us because the tree autoencoder is based on the novel tree decoder proposed by us. The confidence score is chosen via binary search to search for the optimal tradeoff-constant between the target perturbation magnitude and the attack confidence, which follows the optimization-based attack [1].\n\n[1] Carlini, Nicholas and David A. Wagner. \u201cTowards Evaluating the Robustness of Neural Networks.\u201d 2017 IEEE Symposium on Security and Privacy (SP) (2016): 39-57.\n[2] Iyyer, Mohit, John Wieting, Kevin Gimpel and Luke S. Zettlemoyer. \u201cAdversarial Example Generation with Syntactically Controlled Paraphrase Networks.\u201d NAACL-HLT (2018).\n[3] Jin, Di, Zhijing Jin, Joey Tianyi Zhou and Peter Szolovits. \u201cIs BERT Really Robust? Natural Language Attack on Text Classification and Entailment.\u201d ArXiv abs/1907.11932 (2019): n. pag.\n[4] Jia, Robin and Percy Liang. \u201cAdversarial Examples for Evaluating Reading Comprehension Systems.\u201d EMNLP (2017).\n", "title": "Response to Reviewer #2 (Part 2)"}, "Bkg4HIBtsH": {"type": "rebuttal", "replyto": "HyeN5T9AtS", "comment": "Thank you for recognizing the novelty and contribution of our paper.\nQ 1.1: \u201cit is not clear to me why the proposed method would not change the ground truth answer for QA.\u201d \nA 1.1: \n1) Thanks for the interesting question. In fact, we only append an adversarial sentence/ scattering adv tokens into the original text without editing any original words. When searching for the optimal adversarial sentence, we keep the optimization steps until the adversarial sentence and context sentence are disjoint. So ideally the adversarial dataset has the same answers with the original dataset. And our human evaluation in Section 5.2 also confirms that human readers can still find the correct answers (ground truth) even with adversarial sentences appended.\n\nQ 1.2: \u201cthe authors claim to achieve this by carefully choosing the initial sentence as the initial point of optimization, which seems a bit heuristic.\u201d\nA 1.2:\nWe conducted additional experiments by using different initial sentences based on the suggestion and added more discussion on how we select the initial seed to attack QA in Appendix A.4. The conclusion is we observe using different initialization sentences will greatly affect the attack success rates. Therefore, the initial sentence selection is indeed important to help reduce the number of optimization iterations and guarantee to converge to the optimal  $z^*$ efficiently. \n\nWe also would like to emphasize this heuristic step is the very first step of our framework followed by a series of optimization steps to ensure the ground truth is not changed. In this paper, we ensure our appended adversarial sentences are not contradictory to the ground truth by a) choosing an initial sentence as the initial seed of optimization, b) adding perturbation to the sentence, c) searching for the optimal adversarial sentence, d) ensuring that the adversarial sentence and context sentence are disjoint, otherwise keep the iteration steps. If the maximum steps are reached, the optimization is regarded as a failure. \n\nQ 1.3: \u201cmore experimental results to justify this claim.\u201d\nA 1.3: Thank you for the suggestion, and we have added more experiments in Appendix A.4 to discuss the initial seed selection. To support that our appended adversarial sentences/ scattered tokens are not contradictory to the ground truth, we conduct the human evaluation in Section 5.2, which verifies our adversarial dataset is compatible with the original answers and barely affects human judgments.\n", "title": "Response to Reviewer #3"}, "HJglbUStoB": {"type": "rebuttal", "replyto": "Byg5OXlgcr", "comment": "Q5: \u201cit is unclear that the majority answers on the adversarial text will, respectively, match the majority answers on the original text\u2026 whether the proposed framework can generate legitimate adversarial text to human readers or not.\u201d\nA5: \nThanks for pointing this out and we have added the corresponding discussion in revision Section 5.2.\nThe human performance drops around 10% for both QA and classification tasks in Table 10 and 11. We believe this performance drop is tolerable and the stoa generic based QA attack algorithm experienced around 14% performance drop for human performance [4].\nIn addition, we also discuss other reasons for the human performance drop in the appendix B.4: Possible factors include the (majority vote) aggregation noise, length of paragraph and sampling randomness.\n\u2014\n\nQ6: \u201cput examples in the appendix.\u201d\nA6: We have put more generated adversarial examples in Appendix C, and thank you for the helpful suggestions.\n\u2014\n\nQ7: \u201cMissing training details: It is unclear how the model architectures are chosen, and learning rate, optimizer, training epochs etc. are also missing. All these training details should be included in the appendix.\u201d\nA7: We have added model settings and training details in Appendix B and thanks for the suggestion.\n\u2014\n\nQ8: \u201cminor errors: \"Append an initial sentence...\",  section 3: \"map discrete text into a high dimensional...\",  section 3.2.2: \"Different from attacking sentiment analysis...\" ....\u201d\nA8: Thank you for pointing these out and we have fixed the typos in the revision.\n\n[1] Welleck, Sean, Kiant\u00e9 Brantley, Hal Daum\u00e9 and Kyunghyun Cho. \u201cNon-Monotonic Sequential Text Generation.\u201d ICML (2019).\n[2] Iyyer, Mohit, John Wieting, Kevin Gimpel and Luke S. Zettlemoyer. \u201cAdversarial Example Generation with Syntactically Controlled Paraphrase Networks.\u201d NAACL-HLT (2018).\n[3] Jin, Di, Zhijing Jin, Joey Tianyi Zhou and Peter Szolovits. \u201cIs BERT Really Robust? Natural Language Attack on Text Classification and Entailment.\u201d ArXiv abs/1907.11932 (2019): n. pag.\n[4] Jia, Robin and Percy Liang. \u201cAdversarial Examples for Evaluating Reading Comprehension Systems.\u201d EMNLP (2017).\n[5] Sutskever, Ilya, Oriol Vinyals and Quoc V. Le. \u201cSequence to Sequence Learning with Neural Networks.\u201d NIPS (2014).\n", "title": "Response to Reviewer #1 (Part 2)"}, "Skla6rHKsB": {"type": "rebuttal", "replyto": "Byg5OXlgcr", "comment": "Q1: \u201cAlthough the studied problem in this paper is interesting, the technical innovation is very limited. All the techniques are standard or known. \u201d\nA1:  \nThank you for pointing this out, and we will make our contribution clear in the revision. We would like to emphasize our main technical innovations as below:\n1) We design a novel tree **decoder** to decode latent vectors into natural languages which can not only guarantee the syntax correctness, but also achieves the property of non-monotonic order which is also discussed in [1].\n2) We also design a novel framework to generate adversarial text on different levels (e.g. word and sentence) by combining a tree LSTM encoder with the proposed tree based decoder. In particular, we automatically leverage the tree autoencoder to map the discrete text into latent space, generate adversarial perturbation on selected instances, and decode it with our tree based decoder to ensure grammatical correctness. (This novelty is also mentioned by reviewer #2.)\n3) We also propose and explore novel adversarial settings, including scatter attack for classification and targeted attack for QA, which provides diverse ways to evaluate the robustness of existing NLP models. We believe with our general framework which will be open-source soon, it will help the community to further understand the vulnerabilities of current NLP models.\n4) In addition, we have conducted extensive experiments, including adversarial attacks on QA which has not been evaluated by efficient optimization algorithms, and novel BERT based classifier and QA models. Our novel observations such as BERT is less robust than BiDAF and self-attentive models can provide more insights towards evaluating the robustness of various models. \n--\n\nQ2: \u201clacking a rigorous metric of human unnoticeability\u201d\nA2: \nThank you for the comment and we will describe our evaluation metrics clear in revision.\nIn particular, we conduct two types of human evaluation to measure the human sensitivity to our adversarial examples in terms of 1) the linguistic quality and 2) human accuracy comparison based on benign and adversarial texts, as illustrated in Section 5.\nFor 1) we calculate the ratio of the generated adversarial texts that can be recognized as \u201cnatural\u201d by human to evaluate the linguistic quality.\nFor 2) we record the accuracy of human performance on tasks (e.g. classification and QA) based on both benign and adversarial texts as shown in Table 10 and 11.\nSo far the above metrics are what we can come up with and they are also standard to validate the adversarial examples for NLP domains, which have also been used in other state-of-the-art adversarial text generation work [2][3][4]. \n--\n\nQ3: \u201clacking justification of the advantage of the tree-based autoencoder\u2026 unclear why tree-structured LSTM instead of a standard LSTM/GRU should be chosen in this framework for adversarial text generation. If this architecture is preferred, sufficient ablation studies should be conducted.\u201d\nA3:  \nThank you for the helpful suggestion, we will clarify the advantages of the tree-LSTM first and we have also conducted the suggested ablation studies.\n1) The advantages of the tree-based autoencoder are:\na) grammar rules are integrated directly based on the tree structures, thus it can intrinsically guarantee the grammar correctness of generated texts. This is also confirmed by the human study in Section 5.1 that AdvCodec(Sent) generated adversarial text has higher language quality and ensures syntactically correctness;\nb) The tree structure allows us to flexibly modify the node embedding at different node levels in order to generate controllable perturbation on words or sentences. \n\n2) In addition, we conducted the suggested ablation studies: we leverage the standard LSTM architecture [5] and generate adversarial perturbation. We add the ablation study results in appendix A in revision. The experimental results show that LSTM based autoencoder can neither achieve high attack efficiency (The adversarial F1 score is 57.5 with LSTM on BiDAF, compared with 17.6 by AdvCodec -- lower the better) nor guarantee the correct syntactic structures. \n\u2014\n\nQ4: \u201cthe description about adversarial attacks at word level is unclear. More detailed loss function and algorithms along with equations should be provided.\u201d\nA4: \nThanks for the suggestion, and we have added more details and corresponding notations/equations in the revision Section 3.3 along with a pseudo-code in Appendix A.2.\nIn particular, the difference between word level and sentence level manipulation is the meaning of context vector z (in figure 1). For the word-level attack, the context vector $z$ are the concatenation of leaf node embedding (which corresponds to each word):\n$z = [z_1, z_2, \u2026, z_n]$\nAdvCodec(Word) has the same optimization function against QA and classification tasks by manipulating the latent representation z. \n\u2014\n", "title": "Response to Reviewer #1 (Part 1)"}, "S1lpcqm6YS": {"type": "review", "replyto": "rkeeoeHYvr", "review": "The paper proposed a new adversarial text generation framework based on tree-structured LSTM. Compared with two existing methods, the proposed method gives better successfully attacking rates. The tree-structured LSTM model is an existing work but applying it to generate adversarial text is new. \n\nThe difficulty of generating good adversarial text lies 1) high success rate and 2) the generated texts are reasonable (e.g. syntactically correct) and are not contradictory to the original texts. The paper achieves good success rate based on its experimental results but doesn't convince me that 2) is also guaranteed. The paper mentioned that human can ignore irrelevant tokens added by the proposed scatter attack method but it is an extra assumption added to the grammatical correctness. The classification model was trained on texts without these randomly added tokens or typos. In the results, I saw the scatter attack was applied to sentiment analysis but not QA tasks. Is this method not effective to attack QA task?\nAlso, the paper reports the human evaluation on adversarial texts which shows accuracy degradation and low votes. Ideally, the human accuracy on adversarial texts should also be compared to justify 2). More examples can be added to reduce \"noise\" mentioned in the paper. And, the paper can be improved by adding more details on training and optimization.\n\nSome extra questions and comments\n1. in figure 1, will you encode the original text along with the appended sentence into one vector? then, how do you guarantee that the perturbation only applies to the appended sentence but not the original text for the ADVCodec(sent)? or the original text will be reproduced due to the autoencoder?\n2. it will be helpful to add more details on training and optimization. For example, is the autoencoder trained by the authors or is from the existing model? what does the confidence score in (5) means empirically and how to choose its value? ", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}, "HyeN5T9AtS": {"type": "review", "replyto": "rkeeoeHYvr", "review": "This paper proposes a new attack framework AdvCodec for adversarial text generation. The main idea is to use a tree-based autoencoder to embed text data into the continuous vector space and then optimize to find the adversarial perturbation in the vector space. The authors consider two types of attacks: concat attack and scatter attack. Experimental results on sentiment analysis and question answering, together with human evaluation on the generated adversarial text, are provided. \n\nOverall, this paper has a nice idea: use tree autocoders to embed text into vector space and perform optimization in the vector space. On the other hand, it is not clear to me why the proposed method would not change the ground truth answer for QA. Currently the authors claim to achieve this by carefully choosing the initial sentence as the initial point of optimization, which seems a bit heuristic. The authors could add more discussion on this and more experimental results to justify this claim. ", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 1}, "Byg5OXlgcr": {"type": "review", "replyto": "rkeeoeHYvr", "review": "Motivated by recent development of attack/defense methods addressing the vulnerability of deep CNN classifiers for images, this paper proposes an attack framework for adversarial text generation, in which an autoencoder is employed to map discrete text to a high-dimensional continuous latent space, standard iterative optimization based attack method is performed in the continuous latent space to generate adversarial latent embeddings, and a decoder generates adversarial text from the adversarial embeddings.  Different generation strategies of perturbing latent embeddings at sentence level or masked word level are both explored. Adversarial text generation can take either a form of appending an adversarial sentence or a form of scattering adversarial words into different specified positions. Experiments on both sentiment classification and question answering show that the proposed attack framework outperforms some baselines. Human evaluations are also conducted.\n\nPros: \n\nThis paper is well-written overall. Extensive experiments are performed.\n\nMany human studies comparing different adversarial text generation strategies and evaluating adversarial text for sentiment classification/question answering are conducted.\n\nCons:\n\n1) Although the studied problem in this paper is interesting, the technical innovation is very limited. All the techniques are standard or known. \n\n2) There are two major issues: lacking a rigorous metric of human unnoticeability and lacking justification of the advantage of the tree-based autoencoder. I think the first issue is a major problem that renders all the claims in this paper questionable. The metrics used to define adversarial images for deep CNN classifiers are indeed valid and produce unnoticeable images for human observers. But in this paper, the adversarial attack is performed in the latent embedding space, and there is no explicit constraint enforced on the output text. It\u2019s unconvincing that this approach will generate adversarial text that seems negligible to humans. Therefore, the studied problem in this paper has a completely different nature from the one for CNN image classifiers and it is hard to convince readers that the proposed  framework generates adversarial text legitimate to human readers. \n\n3) It is unclear why tree-structured LSTM instead of a standard LSTM/GRU should be chosen in this framework for adversarial text generation. If this architecture is preferred, sufficient ablation studies should be conducted.\n\n4) In section 3.3, the description about adversarial attacks at word level is unclear. More detailed loss function and algorithms along with equations should be provided.\n\n5) In section 5.2, it is unclear that the majority answers on the adversarial text will, respectively, match the majority answers on the original text. Moreover, it seems that there is a large performance drop from original text to adversarial text. Therefore, it is valid to argue that whether the proposed framework can generate legitimate adversarial text to human readers or not.\n\n6) It\u2019s better to include many examples of generated adversarial text in the appendix.\n\n7) Missing training details: It is unclear how the model architectures are chosen, and learning rate, optimizer, training epochs etc. are also missing. All these training details should be included in the appendix.\n\n8) Minor: Figure 1: \"Append an initial sentence...\",  section 3: \"map discrete text into a high dimensional...\",  section 3.2.2: \"Different from attacking sentiment analysis...\" ....\n\nIn summary, the research direction of adversarial text generation studied in this paper is interesting and promising. However, some technical details are questionable, and the produced results without rigorous metrics seem to be unconvincing. \n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 2}, "BJlwnxYpYr": {"type": "rebuttal", "replyto": "BJgacDp3FB", "comment": "We thank the commenter for the interesting observations and comments.\n---\nQ1: \u201cin the provided several examples the grammar of the sentences is violated by scattered word.\u201d\n\nA: Since the scatter attack is performed on the word level (leaf node of the tree autoencoder) and the word level manipulation does not take global (sentence-level) grammatical constraints into consideration, it is expected to observe more \u201cfree\u201d manipulation than AdvCodec(Sent) and achieves a higher attack success rate at the expense of grammatical correctness. In the scatter word attack, we assume that adding sparse word level manipulation will retain the attack subtlety and we evaluate its effects with human studies. We will make this clear in our revision. \n\nIn addition, to ensure better grammatical correctness, we would suggest using AdvCodec(Sent) that takes the whole sentence grammatical structures into consideration when generating the adversarial sentence. However, as we point out in the paper, there is a trade-off between the language quality and adversarial attack success rate.\n---\nQ2: \u201cAnd concatenated sentences are also imperfect (\"chickens and chickens\").\u201d\n\nA: In this case, the initial seed of this sentence is \u201cI kept expecting to see donkeys and chickens walking around.\u201d and the concatenated adversarial sentence simply changes \u201cdonkeys\u201d to \u201cchickens\u201d, and it surprisingly attacks the model output to the opposite answer. \nNote that here the original and adversarial sentences have the same dependency structure (based on the Stanford CoreNLP Dependency Parsing), which confirms that our adversarial tree decoder can largely maintain the original semantics and linguistic structures. Although our model cannot guarantee the adversarial sentence is perfect and natural, we aim to ensure the grammatical and linguistic structural similarity with the proposed tree autoencoder. To guarantee the generated adversarial sentences to be natural, we can add the language model such as GPT2 as a query reference model, which would be interesting future work.\n---\nQ3:  \u201c I find it suspicious that these \"noisy\" samples forms about 10-15% \u2026 the proposed adversarials do change the meaning in fact in more than 10% of all cases.\u201d\n\nA: Comparing the adversarial QA and original QA dataset, we can find an 8.09 drop of majority vote F1 score from 90.987 to 82.897. In the failure cases, most human raters did not choose our fake answers generated by AdvCodec, which confirms that their errors are not necessarily caused by our concatenated adversarial sentence. It is also confirmed in Jia and Liang [1], which claims similar observations in their human evaluation. Moreover, the aggregation process of the human evaluation will inject certain noise to the evaluation results. Since we use majority vote to aggregate the human answers, when different answers happen to have the same votes, we will randomly choose one as the final result. If we always choose the answer that is close to the ground truth in draw cases, we later find that the majority vote F1 score increases from 82.897 to 89.167, which indicates that such randomness contributes to the noisy results largely, instead of the adversarial manipulation. We will make this clear in our revision. \n\n--\nQ4: \u201cOne option where adversarials will change the meaning I can think of is the insertion of \"not\" word in appropriate positions. Have you noticed such situations?\u201d\n\nA: Thank you for the interesting observation. \nWe expect that adding the strong sentiment related words as suggested (e.g. \u201cnot\u201d for negatives) would be able to attack the model. However, here we hope the manipulation would be subtle so that the adversarial sentence will not easily fool humans.\nFor instance, in our setting we use \u201cthe\u201d as initial seeds to *randomly* scatter over the paragraph, so it would be quite rare to manipulate the token to be \u201cnot\u201d in the appropriate positions. \n\nIn addition, we just ran a small experiment to explore this case as suggested. Over 600 successful adversarial (under scatter attack) paragraphs, we find that there is *only one paragraph* where the human *made a mistake* which indeed a \u201cnot\u201d is appended to an Adverb. \n\n\nReferences:\n[1] Jia, Robin and Percy Liang. \u201cAdversarial Examples for Evaluating Reading Comprehension Systems.\u201d EMNLP (2017).", "title": "Response to the grammar violations"}}}