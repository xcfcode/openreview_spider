{"paper": {"title": "Learning with Reflective Likelihoods", "authors": ["Adji B. Dieng", "Kyunghyun Cho", "David M. Blei", "Yann LeCun"], "authorids": ["abd2141@columbia.edu", "kyunghyun.cho@nyu.edu", "david.blei@columbia.edu", "yann@fb.com"], "summary": "Training deep probabilistic models with maximum likelihood often leads to \"input forgetting\". We identify a potential cause and propose a new learning criterion to alleviate the issue.", "abstract": "Models parameterized by deep neural networks have achieved state-of-the-art results  in  many  domains.  These  models  are  usually  trained  using  the  maximum likelihood principle with a finite set of observations. However, training deep probabilistic models with maximum likelihood can lead to the issue we refer to as input forgetting. In deep generative latent-variable models, input forgetting corresponds to posterior collapse---a phenomenon in which the latent variables are driven independent from the observations. However input forgetting can happen even in the absence of latent variables.  We attribute input forgetting in deep probabilistic models to the finite sample dilemma of maximum likelihood.  We formalize this problem and propose a learning criterion---termed reflective likelihood---that explicitly prevents input forgetting. We empirically observe that the proposed criterion significantly outperforms the maximum likelihood objective when used in classification under a skewed class distribution.  Furthermore, the reflective likelihood objective prevents posterior collapse when used to train stochastic auto-encoders with amortized inference.  For example in a neural topic modeling experiment, the reflective likelihood objective leads to better quantitative and qualitative results than the variational auto-encoder and the importance-weighted auto-encoder.", "keywords": ["new learning criterion", "penalized maximum likelihood", "posterior inference in deep generative models", "input forgetting issue", "latent variable collapse issue"]}, "meta": {"decision": "Reject", "comment": "The proposed \u201cinput forgetting\u201d problem is interesting, and the reflective likelihood can come to be seen as a natural solution, however the reviewers overall are concerned about the rigor of the paper. Reviewer 2 pointed out a technical flaw and this was addressed, however the reviewers remain unconvinced about the theoretical justification for the approach. One suggestion made by reviewer 1 is to focus on simpler models that can be studied more rigorously. Alternatively, it could be useful to focus on stronger empirical results. The method works in the experiments given, but for example in the imbalanced data experiments, only MLE is compared to as a baseline. I think it would be more convincing to compare against stronger baselines from the literature. If they are orthogonal to the choice of estimator, then it would be even better to show that these baselines + RLL outperforms the baselines + MLE. Alternatively, you mention some challenging tasks like seq2seq, where a convincing demonstration would greatly strengthen the paper. While the paper is not yet ready in its current form, it seems like a promising approach that is worth further exploration."}, "review": {"HkebgVQXk4": {"type": "rebuttal", "replyto": "Sye3Xh1sR7", "comment": "Dear Reviewer 3,\n\nThanks again for your review. We were wondering if we have addressed all your concerns and if you have further comments.\n\n", "title": "Have we addressed your concerns?"}, "rJxOVW7714": {"type": "rebuttal", "replyto": "BkeaOAeg1E", "comment": "Thank you Reviewer 1 for your insightful comments. We answer your questions below.\n\n1--\"I don't find the motivations and logic behind the derivations to be rigorous\".\n\nWe explain the motivation behind the derivations as follows. Consider supervised learning with input/output pairs (x, y). Input forgetting happens--by definition--when the input x is not being taken into account by the neural network to predict the output y. This led us to consider looking at the maximum likelihood objective for a conditional model p_{\\theta}(y | x) but with an independent assumption on x and y. This leads to Eq. 5 in the draft. However fitting Eq. 5 corresponds to fitting a marginal distribution over y. This is input forgetting. We then relate the objective in Eq. 5 to the maximum likelihood objective of interest. This leads to Eq. 6 which shows that performing maximum likelihood corresponds to a tradeoff between fitting the marginal (the first term in Eq. 6) or minimizing the second term (this second term is the only term that contains information that relates x and y). However a learning algorithm might find it easier to fit the first term (the marginal over y) than to minimize the second term. We make sure to prevent fitting the marginal by regularizing maximum likelihood .\n\n2--\"Overall, it's very hard for me to swallow that there is a deficiency in maximum likelihood learning and that Equation 7 fixes this deficiency in just two pages of exposition.\"\n\nWe are proposing to regularize maximum likelihood learning to impose a stringer dependence between variables. We are not throwing away maximizing log likelihood completely. The RLL objective is the difference between the log likelihood and the log reflective probability of the outputs y. Our work is about noticing a common problem when fitting deep models with log likelihood and proposing a potential solution to fix the problem. There have been many methods proposed to regularize maximum likelihood. These \"penalized maximum likelihood\" objectives include Lasso (which regularizes maximum likelihood by penalizing the L1 norm of the parameters) and Ridge also known as weight decay in the deep learning literature (which regularizes maximum likelihood by penalizing the L2 norm of the parameters). The method we propose is a data-dependent regularization method that regularizes maximum likelihood by minimizing the log reflective probability of the outputs. There are even more alternatives to maximum likelihood in the statistics literature. See for example Generalized Estimating Equations (GEE) for longitudinal data. \n\nWe now provide further evidence that the proposed RLL objective promotes a stronger dependence between inputs and outputs. Consider a fixed \\alpha schedule \\alpha_n = \\alpha_0 for all n and 0 < \\alpha_0 < 1, we can show the RLL objective in Eq. 8 can be rewritten as:\n\nL_{RLL} = 1/N \\sum_{1}^{N} [ (1 - \\alpha_0) * \\log p_{\\theta}(y_n | x_n) + \\alpha_0 * PMI(x_n, y_n) ]\n\nwhere PMI(x_n, y_n) = \\log p_{\\theta}(y_n | x_n) - \\log p_{\\theta}^{refl}(y_n)\n\nEssentially this shows that RLL regularizes maximum likelihood by maximizing the pointwise mutual information between individual pairs (x_n, y_n). This is the reason why it promotes a stronger dependence between inputs and outputs.\n\n3--\"Moreover, there are still no theorems, clear mathematical definitions, or some simulations.\"\n\nWe did not think a theorem was needed. For example the consistency of the objective in Eq. 8 is a simple consequence of applying the law of large numbers and invoking the continuity of logarithm. \n\nHowever we would like to hear about what type of theorem you were expecting. \n\n4--\"For instance, can you show how the reflective likelihood changes analytically tractable / closed-form solutions?  What would the 'reflective OLS estimator' be?  These simpler, classical cases need addressed before I could be convinced that it fixes the problem.\"\n\nWe looked into this as well. In fact there is no closed form formula for the RLL objective on the usual linear model. One has to solve an estimating equation to find the RLL solution. This is expected because RLL uses a data-dependent regularizer.\n\nPlease let us know if our response answers your comments above. We are looking forward to your reply. ", "title": "Thank you for your insightful feedback."}, "BJlhz2qAAQ": {"type": "rebuttal", "replyto": "SJlHDE5ARQ", "comment": "Dear Reviewer 2,\n\nWe are glad that you want to get to the bottom of the problem we are trying to address in the paper.\n\nWe are still addressing a peculiarity in maximum likelihood that causes \"input forgetting\". This peculiarity was wrongly stated in subsection 2.1 of version 1 of the paper. We are now highlighting this peculiarity in the introduction in the paragraph titled \"the finite sample dilemma of maximum likelihood\". \n\nThe main story of the paper is to say \"look there is this problem that keeps happening when fitting deep models with the maximum likelihood objective. This problem manifests itself in deep latent variable models as posterior collapse. This problem manifests itself in Seq2Seq conversation models as production of generic responses by the decoder. This problem manifests itself as failure to predict rare classes in classification under imbalance. All these issues can be nailed down to one common issue: the variables being conditioned upon are not taken into account by the neural network. In deep latent variable models the conditioning variable is the latent variable. In SeqSeq it is the output of the encoder. In classification, it is the input covariate x. We propose this objective that ties outputs to conditioning variables by subtracting a marginal to the original maximum likelihood objective. We think this objective should fix the problem. How does this objective fix the problem? We suggest looking at the KL divergence formulation of maximum likelihood (the problematic subsection 2.1 of version 1). We define the marginal in the proposed objective in supervised learning. We define the marginal in unsupervised learning as well. We now look at how this objective compares to maximum likelihood in a supervised learning problem and an unsupervised learning problem. We see that the objective outperforms maximum likelihood.\"\n\nThis is the same story both in version 1 and in the revision. What changed is the answer to the question \"How does this objective fix the problem?\". The response we provided in subsection 2.1 of version 1 was not correct. We fix this by using the \"finite sample dilemma of maximum likelihood\" argument in the introduction. \n\nFurther evidence of why the RLL objective works is its relationship to pointwise mutual information, KL divergences, and ranking.", "title": "We are addressing the same problem. "}, "Hyg41uYRRQ": {"type": "rebuttal", "replyto": "r1lJnt8AAQ", "comment": "Dear Reviewer 2,\n\nThank you for replying to the rebuttal.\n\n1- Your concern---your whole review---was only about subsection 2.1 of the first version of the paper titled \"A peculiarity of maximum likelihood learning\". We would like to point out that this subsection was not the \"main theory of the paper\" as you suggest. The main theory of the paper was and is still to propose a new objective function that mitigates the \"input forgetting\" issue of maximum likelihood. We proposed this objective for supervised learning and for unsupervised learning with deep latent variable models. We then ran an empirical study which showed the RLL objective outperforms maximum likelihood in a classification under imbalance study and in a neural topic modeling experiment. \n\nSubsection 2.1 of the first version was about justifying why we propose the objective as a potential fix for the \"input forgetting\" issue of maximum likelihood. \n\n2- We addressed your concern by providing another justification for why the objective we propose makes sense. We do this in the revision in the paragraph of the introduction titled \"the finite sample dilemma of maximum likelihood\". \n\nIf there is one thing to get out of that paragraph it is this: Eq. 6 shows that a learning algorithm may find it easier to increase the left hand side of the equality---the maximum likelihood objective---by increasing the first term rather than decreasing the second term. The problem is that the second term is the only term with information regarding how outputs y relate to inputs x. A natural thing to do to avoid this is to use the proposed RLL objective which penalizes maximization of the first term in the right hand side of Eq. 6.\n\n3- The reason why we restructured the paper is that the other reviewers complained that our paper as is was confusing. However this restructuring did not change \"the main theory of the paper\" which is (1) the new objective, (2) what it is for supervised learning, (3) what it is for unsupervised learning, and (4) its comparison to maximum likelihood in an empirical study.\n\nIf you are concerned that it is not clear why the RLL objective works, we explain this in the introduction of the revision. We also point you out to our message above regarding the connection of RLL to pointwise mutual information (PMI). \n\nThanks for your reply.", "title": "You complained about subsection 2.1 and we addressed it. The main theory of the paper was not subsection 2.1 of version 1. The main theory of the paper has not changed."}, "S1eOz28sCm": {"type": "rebuttal", "replyto": "SJlh2jR9FX", "comment": "We extended section 4 with new findings. We unfortunately cannot post a new revision at the moment. We will add the new revision once it is allowed to upload revisions again. \n\nWe found the RLL objective in Eq. 8 can be rewritten as a convex combination between the log likelihood \\log p_{\\theta}(y_n | x_n) and the pointwise mutual information between x_n and y_n for each data pair (x_n, y_n) when the schedule for alpha is \\alpha_n = \\alpha_0 < 1. This new perspective is yet another proof that RLL promotes a stronger dependence between inputs and outputs. The pointwise mutual information term forces each output y_n to strongly depend on its corresponding input x_n. This justifies the huge gain in performance in classification under imbalance. Note pointwise mutual information is stronger than mutual information when it comes to dependence. This is because pointwise mutual information acts at the datapoint level whereas mutual information is an average. \n\nWe also found that for a fixed \\alpha schedule the RLL objective can be written as a difference of KL divergences: \\alpha_0 KL(p_{data}(y_n | x_n) || p_{\\theta}^{refl}(y_n)) - KL(p_{data}(y_n | x_n) || p_{\\theta}(y_n | x_n)). Maximizing the RLL is equivalent to  fitting the conditional model p_{\\theta}(y_n | x_n) on the data while \"unfitting\" the unconditional model defined by the reflective probability  p_{\\theta}^{refl}(y_n) on the data. ", "title": "New insights on the RLL objective: relationship to pointwise mutual information and KL divergences"}, "BkeqBtejAm": {"type": "rebuttal", "replyto": "rJlp3F0d2Q", "comment": "Thank you for your in depth review. We are very happy that you enjoyed the connections we made to ranking losses. We think this was a cool finding as well! In fact there might be more connections: the coefficient \\alpha can be data-dependent and in that sense it induces a family of regularizers. The step-schedule for alpha (as defined in Eq. 11 of the paper) corresponds to ranking loss. We conjecture that there might be other connections when carefully choosing other schedules for \\alpha. \n\nWe think the \u201cfinite sample dilemma\u201d derivations address your remark about formalizing the fact that subtracting the marginal will make the optimization focus on capturing the dependencies. Please let us know if you think otherwise. \n\nWe agree with you on the calibration remark. We also found classification under imbalance to be a natural fit for testing the RLL objective. The results suggest RLL is doing what it is supposed to be doing i.e. strengthen the dependence between x and y.\n\nRegarding your remarks on terminology and lack of rigor in the discussion: we clarified the exposition of the paper and formalized the explanation as to why the proposed objective works. Please see the general rebuttal above for details on what changed in this new version. We also hope you will find the time to read the revision. \n\nRegarding your comment on connections: thank you for bringing this up! Although it might seem that there is a connection to maximum entropy methods, there is no such connection unfortunately. We would like to point out that in your derivation, the quantity you define as entropy is not the entropy of q(y). This is because the expectation is not taken under q(y) but under p*(y | x)---the conditional distribution of y given x under the population distribution---which is different from q(y). This is why we did not mention connections to maximum entropy methods. Thank you for the reference! We have not looked into connections to Bayesian loss calibration. \n\nWe hope we have addressed your concerns. Please let us know if you have other remarks. We would appreciate it if you could read the revision and let us know if we have addressed all your concerns.\n", "title": "Response to Reviewer 1"}, "Sye3Xh1sR7": {"type": "rebuttal", "replyto": "HklxkbeU3X", "comment": "Thank you for reviewing our paper. We are very glad you find the idea potentially interesting and that the experimental results are encouraging. \n\nIn the first version of our paper, we made a wrong statement when justifying our proposed objective. This made the paper very confusing as you mentioned in your review. In light of this feedback, we refactored the paper with a new perspective on why regularizing maximum likelihood with the log reflective probability is a good thing to do. The results did not change because the method is the same. It is the explanation behind why our proposed method works that changed. We replaced the section 2 of version 1 with the paragraph titled \u201cthe finite sample dilemma of maximum likelihood\u201d in the introduction. We chose to add this paragraph in the introduction for sake of clarity and because we want the reader to grasp the intuition behind the proposed objective early on. \n\nWe hope these changes address your earlier concerns. \n\nRegarding your question on connections to mutual information: our objective increases the dependence between inputs and outputs as evidenced in the empirical study section and as motivated by the finite sample dilemma of maximum likelihood. In this sense the RLL objective is implicitly related to mutual information which is a measure of dependence. However, there is no mathematical equation that directly relates RLL and mutual information. \n\nRegarding your question on asymptotics: for infinite data the criterion in Eq. 8 (which is the RLL...the one we use) converges in probability to the difference between the true maximum likelihood objective (the one using expectations under the population distribution) and the log reflective-probability (we define this in the paper...it is some marginal over the output y). In this sense our finite-data objective in Eq. 8 is a consistent estimator of the true objective. You get this result using the law of large numbers and continuity of logarithm. \n\nWe hope we have addressed your concerns. Please let us know if you have further remarks. \n", "title": "Response to Reviewer 3"}, "SJlhsHJsAX": {"type": "rebuttal", "replyto": "rJl5OkRLnm", "comment": "Thank you for taking the time to review the paper.  We corrected the statement and refactored the paper to reflect this. We hope you will be able to read the revision and verify that your concerns have been addressed. Please let us know if you have any further questions. \n\n", "title": "Response to Reviewer 2"}, "rJeJhKMcRX": {"type": "rebuttal", "replyto": "SJlh2jR9FX", "comment": "We thank all the reviewers for taking time to review our paper. Your feedback has greatly helped us revise the paper. The two main concerns from all three reviewers were that (1) the equivalence statement made in section 2 of the first version of the paper was incorrect and (2) the paper is very confusing.  We rewrote the paper to address these two issues. We corrected the equivalence statement and refactored the paper to further clarify the intuitions and technical details behind our proposed idea.  We hope the reviewers will read the revision. We apologize for taking time to post the revision. We wanted to make sure we addressed all the concerns. \n\nWe want to draw attention on the importance of the issue tackled by the paper. The most ubiquitous learning objective for deep models is maximum likelihood. It works very well in practice in most cases. However there are cases where maximum likelihood leads to poor behavior. For example it leads to posterior collapse in deep latent-variable models. Furthermore, it causes lack of diversity in generated responses in Seq2Seq conversation models. Finally, it struggles to learn useful features for rare classes in classification when the class distribution is highly skewed. All these issues can be summarized into one main behavior: the variables being conditioned upon are not taken into consideration by the deep network. We call this problem \"input forgetting\". We identify a potential cause of this issue and propose the RLL objective to alleviate it. \n\nThe new structure of the paper is as follows:\n\n1- We state the \"finite sample dilemma\" of maximum likelihood in the introduction. This replaces the section 2 of the first version of the paper.  We mention our contributions and related work also in the introduction.\n\n2- In section 2 of this current version we derive the RLL objective for supervised learning and propose a practical stochastic approximation of it.\n\n3- In section 3 we extend RLL to unsupervised learning using the auto-encoding framework...this leads us to proposing reflective auto-encoders (RAEs)---a new family of stochastic auto-encoders that do not suffer from posterior collapse.\n\n4- We discuss connections to ranking losses in a new section 4.\n\n5- We finally present empirical findings in section 5. To save space, we added the table listing the learned topics to the appendix.\n", "title": "Rebuttal: In response to reviewer feedback we refactored the paper. The idea is the same, the results are the same, the exposition is different."}, "rJlp3F0d2Q": {"type": "review", "replyto": "SJlh2jR9FX", "review": "Summary:\n\nThis paper proposes maximizing the \u201creflective likelihood,\u201d which the authors define as: E_x E_y [log q(y|x) - \\alpha log q(y)] where the expectations are taken over the data, q is the classifier, and \\alpha is a weight on the log q(y) term.  The paper derives the reflective likelihood for classification models and unsupervised latent variable models.  Choices for \\alpha are also discussed, and connections are made to ranking losses.  Results show superior F1 and perplexity in MNIST classification and 20NewsGroups modeling.\n\nPros:\n\nI like how the paper frames the reflective likelihood as a ranking loss.  It does seem like subtracting off the marginal probability of y from the conditional likelihood should indeed \u2018focus\u2019 the model on the dependent relationship y|x.  Can this be further formalized?  I would be very interested in seeing a derivation of this kind.    \n\nI like that the authors test under class imbalance and report F1 metrics in the experiments as it does seem the proposed method operates through better calibration.\n\nCons:\n\nMy biggest issue with the paper is that I find much of the discussion lacks rigor.  I followed the argument through to Equation 3, but then I became confused when the discussion turned to \u2018dependence paths\u2019: \u201cwe want our learning procedure to follow the dependence path\u2014the subspace in \u0398 for which inputs and outputs are dependent. However this dependence path is unknown to us; there is nothing in Eq. 1 that guides learning to follow this dependence path instead of following Eq. 3\u2014the independence path\u201d (p 3).  What are these dependence paths?  Can they be defined mathematically in a way that is more direct than switching around the KLD directions in Equations 1-3?  Surely any conditional model x-->y has a \u2018dependence path\u2019 flowing from y to x, so it seems the paper is trying to make some stronger statement about the conditional structure?\n\nMoving on to the proposed reflective likelihood in Equation 4, I could see some connections to Equations 1-3, but I\u2019m not sure how exactly that final form was settled upon.  There seems to be a connection to maximum entropy methods?  That is,   E_x E_y [log q(y|x) - \\alpha log q(y)] = E_x E_y [log q(y|x)] + \\alpha E_y [ -log q(y)] \\approx E_x E_y [log q(y|x)] + \\alpha H[y], if we assume q(y) approximates the empirical distribution of y well.  Thus, the objective can be thought of as maximizing the traditional log model probability plus an estimate of the entropy.  As there is a long history of maximum entropy methods / classifiers, I\u2019m surprised there were no mentions or references to this literature.  Also, I believe there might be some connections to Bayesian loss calibration / risk by viewing \\alpha as a utility function (which is easy to do when it is defined to be data dependent).  I\u2019m less sure about this connection though; see Cobb et al. (2018) (https://arxiv.org/abs/1805.03901) and its citations for references.   \n\nThe data sets used in the experiments are also somewhat dissatisfying as MNIST and 20NewsGroups are fairly easy to get high-performing models for.  I would have liked to have seen more direct analysis / simulation of what we expect from the reflective likelihood.  As I mentioned above, I suspect its really providing gains through better calibration---which the authors may recognize as F1 scores are reported and class imbalance tested---but the word \u2018calibration\u2019 is never mentioned.  More direction comparison against calibration methods such as Platt scaling would be make the experiments have better focus.  It would be great to show that this method provides good calibration directly during optimization and doesn\u2019t need the post-hoc calibration steps that most methods require. \n\nEvaluation:  While the paper has some interesting ideas, they are not well defined, making the paper unready for publication.  Discussion of the connections to calibration and maximum entropy seems like a large piece missing from the paper\u2019s argument.  ", "title": "Interesting ideas that need further refinement", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJl5OkRLnm": {"type": "review", "replyto": "SJlh2jR9FX", "review": "This paper is technically flawed. Here are three key equations from Section 2. The notations are simplified for textual presentation:  d \u2013 p_data; d(y|x) \u2013 p_d(y|x); m(y|x) \u2013 p_theta(y|x)\n\nmax E_x~d E_y~d(y|x) [ log m (y|x) ]                 \t\t\t\t               (1) \nmax E_x~d { E_y~d(y|x) ) [ log d(y|x) ]}  -  E_y~d(y|x) [ log m (y|x) ]}        (2)\nmax { E_y~d [  log  (y) ]  -  E_y~d  log E_x~d(x|y) [ m (y|x) ]}                        (3)\n\nFirst error is that the \u201cmax\u201d in (2) and (3) should be \u201cmin\u201d. I will assume this minor error is corrected in the following.\nThe equivalence between (1) and (2) is correct and well-known. The reason is that the first entropy term  in (2) does not depend on model.  The MAJOR ERROR is that (1) is NOT equivalent to (3). Instead, it is equivalent to the following:\n\n min { E_y~d [  log d (y) ]  -  E_y~d  E_x~d(x|y) [ log m (y|x) ]}                     (3\u2019)\n\nNotice the swap of \u201cE_x\u201d and \u201clog\u201d. By Jensen\u2019s nequality, we have \n\n log E_x~d(x|y)  m (y|x) ]  > E_x~d(x|y) [ log m (y|x)\n -  E_y~d  log E_x~d(x|y)  [ m (y|x) ]    < -  E_y~d  E_x~d(x|y) [ log m (y|x) ]                    \n\nSo, minimizing (3) amounts to minimizing a lower bound of the correct objective (3\u2019). It does not make sense at all.\n", "title": "the paper is technically flawed", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HklxkbeU3X": {"type": "review", "replyto": "SJlh2jR9FX", "review": "The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x).\n\nThere is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments.\n\n- Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information.\n\n- Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models?", "title": "potentially interesting idea, but very confusing in current form", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1e43-v127": {"type": "rebuttal", "replyto": "r1xoJGzy3X", "comment": "Thank you AnonReviewer3 for your feedback and for giving us the opportunity to clarify things before you make your decision! There are several points raised in your comment that we would like to provide an answer to.\n\n1. \"... this statement is false. It's true that the true conditional model p(y|x) is a solution to eq 1,2 and 3, but the converse does not hold...\"\n\nWe agree with you that  statement is misleading and wrong as formulated. It was not meant as a statement on the maxima (clearly Eq. 1 and Eq. 3 may not have the same global maxima; there are simple counterexamples to show this), but as a way to distinguish which paths are followed to maximize Eq. 1. Let us clarify what we mean by this as follows: \n\nConsider a supervised learning setting. We have observations (x, y) and our goal is to fit a conditional model p_{\\theta}(y | x). We can fit this model by maximizing either (1) or (2) as they are equivalent; they correspond to maximum likelihood estimation. However, in practice, when we learn a conditional model p_{\\theta}(y | x) that is arbitrarily flexible (e.g., parameterized by a deep neural network) by optimizing (1) (or equivalently (2)) using data, the resulting model often has the issue that it ignores the inputs x. In this case, the resulting value of the parameters is analogous as if we had optimized (3) instead. In other words, when optimizing (1) with data, nothing guarantees that (3) is not being optimized. This behavior is undesirable. To prevent that, we want to promote a strong dependency between y and x. That is, we propose to avoid the \"marginal path\", as induced by (3).\n\nWe will edit out the statement in the revision and make that part of the paper more clear.  \n\n2. \"You are basically claiming that maximum likelihood is not a consistent estimation method, contradicting all of the statistical literature.\"\n\nIf by \"consistent estimation method\" you mean consistency in the statistical sense (i.e. convergence in probability of the estimator to the true parameter as sample size goes to infinity) then no we are not studying consistency/inconsistency of maximum likelihood in the paper. \n\nHowever we would like to point out that maximum likelihood does not always lead to consistent estimators. Consider the counterexample of Bahadur, 1958 (see [1] for the reference) showing an example where maximum likelihood is inconsistent. \n\n3. \"Please clarify your motivation for the proposed method, and let me know if I'm misunderstanding.\"\n\nThank you for giving us the opportunity to make things more clear. Our motivation for the paper is this: there is this common behavior we call \u201cinput forgetting\u201d in the paper that happens quite often with models parameterized by deep neural networks. This is manifested in deep latent variable models as the phenomenon known as \u201cposterior collapse\u201d or \u201clatent variable collapse\u201d in the literature. This also happens in RBMs (see [2]). However the problem can happen even without latent variables. Some other examples we have not mentioned in the paper include Seq2Seq models where the decoder does not account for the input. A good manifestation of this is in neural conversation models where the decoder provides very generic responses such as \u201cI don\u2019t know\u201d or \u201cok\u201d no matter what the query/input is. See for example [3] for more details on generic answers in conversation models. \n\nOne common denominator of all these examples is that the variable being conditioned upon is ignored by the deep network. Our paper proposes a regularization approach to mitigate this problem. We add a regularizer termed the \u201creflective likelihood\u201d that is basically a marginal distribution over the output variable. We define this marginal in the paper for both supervised and unsupervised learning. The resulting objective is the difference between the usual maximum likelihood objective and this reflective likelihood. Subtracting the reflective likelihood forces the optimization to favor parameter settings that promote usage of the variable being conditioned upon. We validate this hypothesis through our empirical studies where we notice an improvement in terms of latent variable collapse and classification performance for rare classes. \n\nIn summary: for applications where you care about promoting a stronger dependence between inputs and outputs (e.g. in deep latent variable models or in classification under imbalance) then we propose to use the objective proposed in this paper instead of vanilla MLE.\n\nWe hope our answer clarifies things. Thank you for bringing these points up. We will add these clarifications in the revision.\n\n[1] R. R. Bahadur. Examples of Inconsistency of Maximum Likelihood Estimates. The Indian Journal of Statistics, 1958.\n[2] K. Cho et al. Enhanced Gradient and Adaptive Learning Rate for Training Restricted Boltzmann Machines. In ICML,2011.\n[3] J. Li et al. A Diversity-Promoting Objective Function for Neural Conversation Models. In NAACL, 2016.", "title": "Clarifications"}}}