{"paper": {"title": "Neural Machine Translation with Latent Semantic of Image and Text", "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "summary": "", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The area chair agrees with the reviewers that this paper is not of sufficient quality for ICLR. The experimental results are weak (there might be even be some issues with the experimental methodology) and it is not at all clear whether the translation model benefits from the image data. The authors did not address the final reviews."}, "review": {"BkiFO-Gml": {"type": "rebuttal", "replyto": "Hk0_CEbQe", "comment": "C means \"Constrained\", i.e. the model was only trained with the given training corpus and not without some other resources (that would be a U: Unconstrained)", "title": "C is for Constrained"}, "rJLKBSbmg": {"type": "rebuttal", "replyto": "rkklCqe7g", "comment": "We are not participating the competition, therefore it is not a competition submission of course, and we just evaluate our models with published dataset. So we thought it is not a problem to do further analysis to the model which scores the best in the test dataset.\n\nI hope this can be the answer for your question. ", "title": "Thanks for your further comment."}, "SyB7NBW7e": {"type": "rebuttal", "replyto": "HkrDj-1Xe", "comment": "Thanks for your question. \nWe do not have the obvious evidence which shows our model is actually capturing useful semantics but we have some clues. We conclude that our model is capturing the semantic of the sentence because of the fact that our model scores the higher METEOR score compared to baselines, and some translation examples in qualitative analysis (such as Figure 6). ", "title": "Thanks for your comment."}, "Hk0_CEbQe": {"type": "rebuttal", "replyto": "BJUyn9gmg", "comment": "According to 1, we know this paper and it came about a week after we submitted our paper in openreview.\n\nAccording to 2, we should correct the reference as you commented. We will revise the paper. Thanks for advice.\n\nAccording to 3, we denote NMT as the monomodal translation using dl4mt baseline and we trained it by ourself. \nWe are sure the score reported by Huang et al is with -norm. There is also their scores reported in (http://www.statmt.org/wmt16/pdf/W16-2346.pdf), which is without norm and the scores reported in Huang et al are higher than those reported in (http://www.statmt.org/wmt16/pdf/W16-2346.pdf), which indicates that the report in Huang et al is with -norm. The reason why we did not put the score without norm is that we were not sure what \"CMU_1 MNMT_C\" denotes (We were not sure what \"C\" indicates. We may be missing the part explaining about it though). \nAbout your claim that we should put other competitors score such as Moses, what we compare our model to is limited to only end-to-end neural machine models because we want to show that the neural machine translation can actually benefit from images in our way, but as you told, we should have put other competitor's score for richer comparison.\n\nThanks for your valuable comment!!", "title": "Thanks for your comment."}, "rkklCqe7g": {"type": "rebuttal", "replyto": "Syh3nLemx", "comment": "Regarding this,\n\nIf you were participating to the challenge where the test set ground-truth sentences were not accessible except for official evaluators, you would have selected G+O-TXT for your best system and not G. So I think it is not good to further base your qualitative analysis on G vs VNMT and not G+O-TXT vs VNMT. Again if that was a competition submission, you wouldn't have access to your test scores and you wouldn't do your qualitative analysis using 'G'..", "title": "Small addition"}, "BJUyn9gmg": {"type": "rebuttal", "replyto": "B1G9tvcgx", "comment": "1. Just for information, regarding your claim \"We also present the first translation task with which one uses a parallel corpus and images in training, while using a source corpus in translating\", there's a recent paper called \"Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot\" which also proposes the very same idea (https://arxiv.org/pdf/1611.04503v1.pdf) but I don't know which one comes earlier.\n\n2. It would be nice to clarify that when you refer to Multi30k and the WMT16 challenge results, you actually refer to multimodal machine translation task (Task 1) where you have 1 English and 1 German descriptions and not the second one where you have 5 for each.\n\n3. Your results table is not very clear. What is NMT? Is it a monomodal(textual) dl4mt baseline that you trained yourself? You report the METEOR scores with '-norm' parameter but are you sure that Huang et al' reported with -norm as well? -norm gives substantially higher METEOR scores and the primary competition metric was METEOR without -norm. Also you may want to include the competition winner and Moses baseline in your results for a richer comparison.\n\n4. At the end of section 4.1, BLUE -> BLEU\n\n\n", "title": "General comments"}, "Syh3nLemx": {"type": "rebuttal", "replyto": "Sy4luWyQx", "comment": "In our paper, we propose 4 different architectures, G, G+O-AVG, G+O-RNN, and G+O-TXT. Their difference is how the image information is integrated into a latent variable. \n\nIn the validation, we tune the best hyper parameters for each architecture with the validation dataset. The selection of architecture is not a part of hyper parameters setting.Then, we evaluate each architecture with test data.\n\nAccording to your comment that we should select only G+O-TXT for test, we thought it is not a problem to show all architecture's results since model selection is not part of the parameter tuning. Figure 4 suggests that G model's validation scores during the training iteration is higher than others in most cases. It can be the explanation why G scores higher than G+O-TXT in the test.\n\nI hope this can be the answer for your question. If you have further questions, it is welcomed.", "title": "Thanks for your comment."}, "HkrDj-1Xe": {"type": "review", "replyto": "B1G9tvcgx", "review": "I like your underlying motivation of using \"a latent\nvariable containing an underlying semantic extracted from texts and images\".  Do you have evidence that your particular model is indeed capturing useful semantics, for example with some qualitative demonstrations?This paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images. Considering way to improve translation with visual information seems like a sensible thing to do when such data is available. \n\nAs pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper. This makes the gains reported by the authors very marginal. In addition, as the author's also said in their question response, it is not clear if the model is really learning to capture useful image semantics. As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.\n\n\n\n", "title": "Latent representations", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJNO3xMVg": {"type": "review", "replyto": "B1G9tvcgx", "review": "I like your underlying motivation of using \"a latent\nvariable containing an underlying semantic extracted from texts and images\".  Do you have evidence that your particular model is indeed capturing useful semantics, for example with some qualitative demonstrations?This paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images. Considering way to improve translation with visual information seems like a sensible thing to do when such data is available. \n\nAs pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper. This makes the gains reported by the authors very marginal. In addition, as the author's also said in their question response, it is not clear if the model is really learning to capture useful image semantics. As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.\n\n\n\n", "title": "Latent representations", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Sy4luWyQx": {"type": "review", "replyto": "B1G9tvcgx", "review": "Can you please tell how exactly you performed model selection? If you tuned the hyperparameters using the development (validation) set, then you should only use the model that performed best on it. From Table 1 it seems like this model is G+O+TXT, and the test scores of this model are only marginally better than the baseline (0.6 METEOR, 0.2 BLEU).The paper proposes an approach to the task of multimodal machine translation, namely to the case when an image is available that corresponds to both source and target sentences. \n\nThe idea seems to be to use a latent variable model and condition it on the image. In practice from Equation 3 and Figure 3 one can see that the image is only used during training to do inference. That said, the approach appears flawed, because the image is not really used for translation.\n\nExperimental results are weak. If the model selection was done properly, that is using the validation set, the considered model would only bring 0.6 METEOR and 0.2 BLEU advantage over the baseline. In the view of the overall variance of the results, these improvements can not be considered significant. \n\nThe qualitative analysis in Subsection 4.4 appears inconclusive and unconvincing.\n\nOverall, there are major issues with both the approach and the execution of the paper.\n\n", "title": "Development and test set scores", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Bkgf7ZeM4e": {"type": "review", "replyto": "B1G9tvcgx", "review": "Can you please tell how exactly you performed model selection? If you tuned the hyperparameters using the development (validation) set, then you should only use the model that performed best on it. From Table 1 it seems like this model is G+O+TXT, and the test scores of this model are only marginally better than the baseline (0.6 METEOR, 0.2 BLEU).The paper proposes an approach to the task of multimodal machine translation, namely to the case when an image is available that corresponds to both source and target sentences. \n\nThe idea seems to be to use a latent variable model and condition it on the image. In practice from Equation 3 and Figure 3 one can see that the image is only used during training to do inference. That said, the approach appears flawed, because the image is not really used for translation.\n\nExperimental results are weak. If the model selection was done properly, that is using the validation set, the considered model would only bring 0.6 METEOR and 0.2 BLEU advantage over the baseline. In the view of the overall variance of the results, these improvements can not be considered significant. \n\nThe qualitative analysis in Subsection 4.4 appears inconclusive and unconvincing.\n\nOverall, there are major issues with both the approach and the execution of the paper.\n\n", "title": "Development and test set scores", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}