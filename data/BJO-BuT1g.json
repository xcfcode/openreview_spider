{"paper": {"title": "A Learned Representation For Artistic Style", "authors": ["Vincent Dumoulin", "Jonathon Shlens", "Manjunath Kudlur"], "authorids": ["vi.dumoulin@gmail.com", "shlens@google.com", "keveman@google.com"], "summary": "A deep neural network to learn and combine artistic styles.", "abstract": "The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.", "keywords": ["Computer vision", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The reviewers (two of whom stated maximum confidence) are in consensus that this is a high-quality paper. It also attracted some public feedback which was also positive. The authors have already incorporated much of the feedback into their revised paper. This seems to be a clear accept in my opinion."}, "review": {"HkcU8vbUl": {"type": "rebuttal", "replyto": "H12u3mRBe", "comment": "Thank you for your comments and pointing us at recent work from your group. We will read your papers and get back to you over email.", "title": "Re: Discussions about some related work"}, "H12u3mRBe": {"type": "rebuttal", "replyto": "BJO-BuT1g", "comment": "This is very interesting and inspiring work about neural style transfer. Here we want to share some discussions about our two works, which are closely related to this paper.\n\n1. Our recent work \"Demystifying Neural Style Transfer\"[a] proposed a new interpretation of neural style transfer to explain a fundamental problem why Gram matrix can represent artistic style. In [a], we demonstrate that neural style transfer is indeed a domain adaptation problem. Neural style transfer is intrinsically to a process of feature distribution alignment.\n\n2. Our another domain adaptation paper AdaBN [b] shares the similar idea with this paper to deal with domain adaptation problem. Differently, we use different mean and variance statistics in BN layers for different domains. This paper use learned slope and bias terms in BN layers for different 'domains' (if we consider a target style image is an individual domain). We also tried the idea of AdaBN for neural style transfer in [a] and achieves comparable results with those of original neural style transfer. In terms of testing, both these two methods can absorb the scaling and shifting factors into one operation, which results in the same operation.\n\nWe think combining these three paper together: this paper, [a] and [b] can help use understand the neural style transfer in a more holistic and systematical perspective. Neural style transfer is indeed a domain adaptation problem [a]. Modulating the BN parameter is suitable for aligning distributions, which can be applied both in domain adaptation and neural style transfer.\n\n\n[a] Demystifying Neural Style Transfer, (https://arxiv.org/abs/1701.01036)\n[b] Revisiting Batch Normalization For Practical Domain Adaptation, (https://openreview.net/forum?id=BJuysoFeg)\n", "title": "Discussions about some related work"}, "ryIVKNXHx": {"type": "rebuttal", "replyto": "BJO-BuT1g", "comment": "The link to the manuscript has been updated and a new PDF may be downloaded. Changes addressed all of the reviewer comments.\n", "title": "Updated manuscript now available."}, "HJ-YM_xSe": {"type": "rebuttal", "replyto": "BJ35IjNVx", "comment": "We had not been aware of the relationship to Kirkpatrick et al. Thank you for the reference and comment!", "title": "Re: Learning task-specific batch-norm params"}, "BkZkfOeHx": {"type": "rebuttal", "replyto": "HJ3OH8PEl", "comment": "Correct. The typo in Equation 3 will been fixed in the next revision.\n", "title": "Re: Typo in Eq. 3"}, "SkbTb_gre": {"type": "rebuttal", "replyto": "HkvBZFQVl", "comment": "Thank you for your comments. We found them very useful in improving the quality of the manuscript. Following your suggestions, we have made the following changes to the manuscript:\n\n+ Subsection heading 2.1 was removed to group all prior art review in the same place.\n\n+ Work related to feedforward networks is now cited in the prior art review section and in Fig 2.\n\n+ We appreciate the reviewer\u2019s suggestion to combine Figures 2 and 3. After some thought we decided to keep these figures separate because (a) the two figures convey distinct points and (b) Figure 2 recapitulates previous work while Figure 3 highlights our unique contribution. The latter point is quite important as we would not want to confuse a reader about what is unique to this paper. If the reviewer feels strongly otherwise, please do let us know.\n\n+ For Equation 5, we have added additional text to clarify x and z.\n\n+ For Equation 4, we felt that the equations were already quite heavy so rather then add a subscript s, we added explicit, additional text to highlight that the style transfer network is specific to a painting style.\n\n+ In Fig 5 (left), an explanation for the curve colors has been added to the caption.\n\n+ The \u201cnewtork\u201d typo has been corrected in page 2.\n\n+ \u201cmuch less\u201d has been replaced with \u201cfewer\u201d after Eq 5.\n\nRegarding the mention of VGG-19 in page 1: This is intentional. Gatys et al. use VGG-19, whereas we use VGG-16 like Johnson et al.", "title": "Response to AnonReviewer1"}, "BJ98-ulHx": {"type": "rebuttal", "replyto": "S1CreA-Ne", "comment": "Thank you for your comments. We agree with you that the interpretability of the learned style representation is an important area of investigation.\n\nThis is something we are currently working on. The results are very preliminary, and we do not feel like they should be included in the manuscript yet, but following another reviewer\u2019s point we tried breaking the network into 3 sections (encoder, residual and decoder) and independently carrying out the style interpolation for the 3 sections. Broadly speaking we found the \"encoder\" did almost nothing; the \"residual\" performed most of the interesting spatial decomposition of the image and the \"decoder\" performed the colorization of the image.\n\nLike was said above, more work is needed in this direction, but our goal would be to provide a more detailed analysis in the camera-ready version of the paper.", "title": "Response to AnonReviewer3"}, "HyRWWdeSl": {"type": "rebuttal", "replyto": "HyA-m--Nx", "comment": "Thank you for your feedback. We like the idea of adding new styles to a mobile application by only sending over a small number of style-specific parameters. We will be adding it to our discussion and credit you for the suggestion in our next revision.\n\nWe will be adding a reference to Li and Wand alongside other feedforward style transfer citations. Thank you for pointing it out.\n\nWe also acknowledged Justin Johnson\u2019s GitHub repository as prior work on optimization-based style blending in the paper. One interesting difference that we would like to point out is that our approach to style blending produces homogeneous pastiches, whereas the optimization-based approach produces heterogeneous pastiches with some areas exhibiting one set of style features and other areas exhibiting a different set of style features.\n\nWe agree that the style and content losses are not always well correlated with the perceptual quality of the results. In the paper, we use these metrics to back the statement that the optimization of the N-styles version of the feedforward style transfer network is not significantly impacted when compared with single-style networks. One potential pitfall is that the N-styles network could behave very differently than single-style networks despite having comparable losses, which we account for by qualitatively comparing pastiches. We welcome suggestions to improve the reliability of the quantitative evaluation. ", "title": "Response to AnonReviewer2"}, "HJ3OH8PEl": {"type": "rebuttal", "replyto": "BJO-BuT1g", "comment": "If I understand correctly, Eq.3 should be comparing \"p\" with \"c\" (content) rather than \"s\" (style). ", "title": "Typo in Eq.3 "}, "BJ35IjNVx": {"type": "rebuttal", "replyto": "BJO-BuT1g", "comment": "I think this is a great and very general method for multi-task learning.\nI view it as selecting the features which are relevant (scale) or intrinsic (shift) to each task.\nNote that DeepMind use the same technique for their \"overcoming catastrophic forgetting\" paper (Kirkpatrick et al.)  \nYou guys should ask them to cite you!", "title": "Learning task-specific batch-norm params (\"feature selection\") is a great idea!"}, "HkBKWKo7l": {"type": "rebuttal", "replyto": "BJO-BuT1g", "comment": "The link to the manuscript has been updated and a new PDF may be downloaded. Changes:\n - Updated Figure 6 to show the stylized rendering based on 'fine-tuning' or 'from scratch' after a small number of training steps (i.e. 5k steps).\n - Added more details about the training algorithm per the reviewer request.\n - Updated caption to Figure 5 to provide the identities of the painting styles.\n - Some minor text changes per the reviewer comments.\n", "title": "Updated manuscript now available."}, "SyeDFKPXx": {"type": "rebuttal", "replyto": "B1r4HLHml", "comment": "Thank you for the comments and suggestions. We will be posting a revised version of the manuscript shortly to address these reviewer issues.\n\n> - In Fig. 6, as I understand, stylisations are compared between the fine-tuned network after 5k \n> training iterations and a network trained from scratch after 40k iterations. What does the \n> stylisation from the trained-from-scratch network after 5k iterations look like? Is it also  \n> comparable? It would be informative to include that comparison in the plot.\n\nAfter 5k iterations, training from scratch produces poorly stylized images. Conversely, after 5k iterations, \u2018fine-tuning\u2019 produces images that are qualitatively similar to the final stylized image after 40k iterations. This is consistent with the performance in the loss curves. We have updated Figure 6 to additionally show sample stylized images that highlight this qualitative difference.\n\n> - The paper shows nicely that a large number of styles can be encoded in a condensed \n> feature space of shift and scaling parameters for the feature maps. Interpolation results are \n> shown where the style parameters are linearly interpolated between up to 4 styles. My \n> questions is: Did the authors try other interpolation schemes than just linear interpolation? For \n> example to have different style parameters at different layers (eg. all layers apart from one \n> have style A and one layer has style B), or different style parameters for different features in \n> the same layer? In that way one might be able to visualise the specific contribution of single \n> layers or units to the stylisation outcome.\n\nWe appreciate the author\u2019s suggestion as we think this is an interesting direction to explore - particularly the suggestion of mixing interpolation across layers or units. We can qualitatively compare the rendered images but if the reviewer has any suggestions for how to quantitatively measure the effects of various layers or units, we would be very excited to explore. Also, note that in Figure 7 (right) one sees that the style loss does not grow linearly in spite of the fact that we use a linear interpolation. Other colleagues have suggested trying to interpolate in a manner that provides linear, uniform spacing of the style loss, but linear interpolation is the only method that has been tried so far.\n\n> - Finally, in the discussion of the paper it says that \"it could be that the network architecture is \n> overspecified for the task. We see evidence for this behavior in that pruning the architecture \n> leads to qualitatively similar results\". Nevertheless, the stylisation results presented in this \n> paper and with feed-forward networks in general are still substantially behind the quality that is \n> achieved by the iterative procedure of the Gatys et al. algorithm (which is also not perfect). \n> Thus it sounds strange to me that the network is 'overspecified' (as in 'too expressive') for the \n> task, because the it does not solve the task conclusively. So shouldn't one further qualify that \n> statement by adding that just for the given architecture of the network, adding more \n> parameters does not improve stylisation performance?\n\nWe agree with the reviewer\u2019s point and have adjusted the text to the following: \u201cThat said, in the case of art stylization when posed as a feedforward network, it could be that the specific network architecture is unable to take full advantage of its capacity.\u201d\n\n> Small comment: \n> - It would be nice to have a legend for the first plot in Fig. 5\n\nWe have amended the caption with the identities of the 2 paintings indicated in the dark curves (Vetheuil = teal and Water Lillies = purple) . The other light curves correspond to the other Monet paintings listed in the Appendix.", "title": "Re: Comments and questions"}, "HyD4_Yw7l": {"type": "rebuttal", "replyto": "SyLVGsIme", "comment": "Thank you for your question. We will be adding several of these extra details in our revised manuscript that we will post shortly. We use the training method introduced in [1, 2] and illustrated in Figure 2 in the manuscript. In an online setting, we proceed as follows:\n\n\n1. Draw a style image \u201cs\u201d and its corresponding index \u201ci\u201d at random from the set of \u201cN\u201d style images.\n2. Draw a content image \u201cc\u201d at random from the training set.\n3. Compute the pastiche image \u201cp\u201d as \u201cp = T(c, i)\u201d, where \u201cT\u201d is the style transfer network. Conditional instance normalization layers in the network are implemented following Equation 5 in the paper.\n4. Feed \u201cp\u201d, \u201cs\u201d and \u201cc\u201d to the pre-trained VGG16 network and compute the style and content losses according to Equation 1 in the paper.\n5 . Compute the gradient of the loss (defined as the weighted sum of the style and content losses) with respect to \u201cT\u201d\u2019s parameters and update \u201cT\u201d\u2019s parameters in that direction.\n\nThe minibatch version is a straightforward extension of the procedure described above. We employed the ImageNet dataset [3] as our set of content images for training.\n\nYou can also examine a complete open-source implementation for yourself at: https://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization\n\n[1] Ulyanov, Dmitry, et al. \"Texture Networks: Feed-forward Synthesis of Textures and Stylized Images.\" arXiv preprint arXiv:1603.03417 (2016).\n[2] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. \"Perceptual losses for real-time style transfer and super-resolution.\" arXiv preprint arXiv:1603.08155 (2016).\n[3] Deng, Jia et al, \u201cImageNet: A Large-Scale Hierarchical Image Database, CVPR (2009)", "title": "Re: How to train this network."}, "SyLVGsIme": {"type": "rebuttal", "replyto": "BJO-BuT1g", "comment": "I found that the authors does not describe in detail how to train this network.\n", "title": "How to train this network."}, "B1r4HLHml": {"type": "review", "replyto": "BJO-BuT1g", "review": "Questions:\n- In Fig. 6, as I understand, stylisations are compared between the fine-tuned network after 5k training iterations and a network trained from scratch after 40k iterations. What does the stylisation from the trained-from-scratch network after 5k iterations look like? Is it also comparable? It would be informative to include that comparison in the plot.\n\n- The paper shows nicely that a large number of styles can be encoded in a condensed feature space of shift and scaling parameters for the feature maps. Interpolation results are shown where the style parameters are linearly interpolated between up to 4 styles. My questions is: Did the authors try other interpolation schemes than just linear interpolation? For example to have different style parameters at different layers (eg. all layers apart from one have style A and one layer has style B), or different style parameters for different features in the same layer? In that way one might be able to visualise the specific contribution of single layers or units to the stylisation outcome.\n\n- Finally, in the discussion of the paper it says that \"it could be that the network architecture is overspecified for the task. We see evidence for this behavior in that pruning the architecture leads to qualitatively similar results\". \nNevertheless, the stylisation results presented in this paper and with feed-forward networks in general are still substantially behind the quality that is achieved by the iterative procedure of the Gatys et al. algorithm (which is also not perfect). Thus it sounds strange to me that the network is 'overspecified' (as in 'too expressive') for the task, because the it does not solve the task conclusively. So shouldn't one further qualify that statement by adding that just for the given architecture of the network, adding more parameters does not improve stylisation performance?\n\nSmall comment: \n- It would be nice to have a legend for the first plot in Fig. 5\nThe paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. \n\nThis enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer.  \n\nFinally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al. and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space.\nHere I see the most potential for improvement of the paper: The parameterisation of style in terms of scale and shift parameters of individual features seems like a promising basis to achieve interpretable style features. It would be a great addition to explore to what extend statements such as \u201cThe parameters of neuron N in layer L encodes e.g. the colour or brush-strokes of the styles\u201d can be made. I agree that this is a potentially laborious endeavour, but even just qualitative statements of this kind that are demonstrated with the respective manipulations in the stylisation would be very interesting.\n\nIn conclusion, this is a good paper presenting an elegant and valuable contribution that will have considerable impact on the design of feed-forward stylisation networks.\n", "title": "Comments and questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1CreA-Ne": {"type": "review", "replyto": "BJO-BuT1g", "review": "Questions:\n- In Fig. 6, as I understand, stylisations are compared between the fine-tuned network after 5k training iterations and a network trained from scratch after 40k iterations. What does the stylisation from the trained-from-scratch network after 5k iterations look like? Is it also comparable? It would be informative to include that comparison in the plot.\n\n- The paper shows nicely that a large number of styles can be encoded in a condensed feature space of shift and scaling parameters for the feature maps. Interpolation results are shown where the style parameters are linearly interpolated between up to 4 styles. My questions is: Did the authors try other interpolation schemes than just linear interpolation? For example to have different style parameters at different layers (eg. all layers apart from one have style A and one layer has style B), or different style parameters for different features in the same layer? In that way one might be able to visualise the specific contribution of single layers or units to the stylisation outcome.\n\n- Finally, in the discussion of the paper it says that \"it could be that the network architecture is overspecified for the task. We see evidence for this behavior in that pruning the architecture leads to qualitatively similar results\". \nNevertheless, the stylisation results presented in this paper and with feed-forward networks in general are still substantially behind the quality that is achieved by the iterative procedure of the Gatys et al. algorithm (which is also not perfect). Thus it sounds strange to me that the network is 'overspecified' (as in 'too expressive') for the task, because the it does not solve the task conclusively. So shouldn't one further qualify that statement by adding that just for the given architecture of the network, adding more parameters does not improve stylisation performance?\n\nSmall comment: \n- It would be nice to have a legend for the first plot in Fig. 5\nThe paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. \n\nThis enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer.  \n\nFinally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al. and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space.\nHere I see the most potential for improvement of the paper: The parameterisation of style in terms of scale and shift parameters of individual features seems like a promising basis to achieve interpretable style features. It would be a great addition to explore to what extend statements such as \u201cThe parameters of neuron N in layer L encodes e.g. the colour or brush-strokes of the styles\u201d can be made. I agree that this is a potentially laborious endeavour, but even just qualitative statements of this kind that are demonstrated with the respective manipulations in the stylisation would be very interesting.\n\nIn conclusion, this is a good paper presenting an elegant and valuable contribution that will have considerable impact on the design of feed-forward stylisation networks.\n", "title": "Comments and questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkQS95Qme": {"type": "rebuttal", "replyto": "BJO-BuT1g", "comment": "Dec 5 at 5:00pm PST:\nFYI, we uploaded an updated version to arxiv.org with these revisions (linked in openreview). We took the opportunity to provide some pointers to code we posted online, a YouTube video demonstrating a real-time interactive demo and added a brief section highlighting some recently posted, concurrent work on this topic:\nhttps://arxiv.org/pdf/1610.07629v2.pdf", "title": "New version of manuscript"}, "r1CZ957mg": {"type": "rebuttal", "replyto": "S1ZrrcyQg", "comment": "Thank you for the question. We apologize we were not terribly clear in the manuscript and we will amend the manuscript to make this clearer.\n\nOur comment highlights how previous methods required N feed-forward passes of N separately trained networks in order to provide N stylizations for a given photograph. Our current model requires 1 feed-forward pass in which the batch size is N.\n\nWhile there is no computational savings when styling a single image to N styles in our method, it is well known that larger mini-batches can be implemented much more efficiently on modern processors and GPUs. Thus it is much faster to do N stylizations in our method, compared to the previous approaches, which have to run the less efficient single image inference on N independent networks. \n\nIf one requests N distinct styles to be applied to a single image in a given batch, then there will be a slight performance degradation because each item in the batch would use different parameters for the conditional instance normalization. Thankfully this is not a large issue because the conditional instance normalization is just a linear transformation and the vast majority of the computation (i.e. convolutions) is a computation that is shared and amortized across the batch.\n\nWe uploaded a new version to arxiv.org with these revisions (linked in openreview). We took the opportunity to provide some pointers to code we posted online, a YouTube video demonstrating a real-time interactive demo and added a brief section highlighting some recently posted, concurrent work on this topic:\nhttps://arxiv.org/pdf/1610.07629v2.pdf", "title": "Re: Multiple styles at test-time"}, "S1ZrrcyQg": {"type": "review", "replyto": "BJO-BuT1g", "review": "In Section 2.2 the authors write that the proposed approach allows one to \u201cstylize a single image into N painting styles with a single feed forward pass of the network.\u201d Can you elaborate on this point? If I\u2019ve understood the model correctly, then when using a single network to stylize an image using two different styles, the computation can only be shared up to the first conditional instance normalization, at which point the style-specific scale and shift parameters will cause the intermediate representations to diverge; therefore I don\u2019t understand how a single forward pass can be used to stylize an image in multiple styles.CONTRIBUTIONS\nThe authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results.\n\nNOVELTY\nThe problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer is also novel. This paper is also the first that demonstrates feedforward neural style blending; though not described in published literature, optimization-based neural style blending had previously been demonstrated in https://github.com/jcjohnson/neural-style.\n\nMISSING CITATION\nThe following paper was concurrent with Ulyanov et al (2016a) and Johnson et al in demonstrating feedforward neural style transfer, though it did not use the Gram-based formulation of Gatys et al:\n\nLi and Wand, \"Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks\", ECCV 2016\n\nCLARITY\nThe paper is very well written and easy to follow.\n\nSIGNIFICANCE\nThough simple, the proposed method is a significant addition to the growing field of neural style transfer. Its benefits are especially clear for mobile applications, which are often constrained in both disk space and bandwidth. Using the proposed method, only a single trained network needs to be transmitted and stored on the mobile device; in addition the ability of the proposed method to incrementally learn new styles means that new styles can be added by transmitting only a small number of new style-specific parameters to a mobile device.\n\nEVALUATION\nLike many other papers on neural style transfer, the results are mostly qualitative. Following existing literature, the authors use style and content loss as a quantitative measure of quality, but these metrics are unfortunately not always well-correlated with the perceptual quality of the results. I find the results of this paper convincing, but I wish that this and other papers on this topic could find a way to evaluate their results more quantitatively.\n\nSUMMARY\nThe problem and method are slightly incremental, but the several improvements over prior work make this paper a significant addition to the growing literature on neural style transfer. The paper is well-written and its experiments convincingly validate the benefits of the method. Overall I believe the paper would be a valuable addition to the conference.\n\nPros\n- Simple modification to feedforward neural style transfer with several improvements over prior work\n- Strong qualitative results\n- Well-written\n- Open-source code has already been released\n\nCons\n- Slightly incremental\n- Somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic", "title": "Multiple styles at test-time", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HyA-m--Nx": {"type": "review", "replyto": "BJO-BuT1g", "review": "In Section 2.2 the authors write that the proposed approach allows one to \u201cstylize a single image into N painting styles with a single feed forward pass of the network.\u201d Can you elaborate on this point? If I\u2019ve understood the model correctly, then when using a single network to stylize an image using two different styles, the computation can only be shared up to the first conditional instance normalization, at which point the style-specific scale and shift parameters will cause the intermediate representations to diverge; therefore I don\u2019t understand how a single forward pass can be used to stylize an image in multiple styles.CONTRIBUTIONS\nThe authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results.\n\nNOVELTY\nThe problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer is also novel. This paper is also the first that demonstrates feedforward neural style blending; though not described in published literature, optimization-based neural style blending had previously been demonstrated in https://github.com/jcjohnson/neural-style.\n\nMISSING CITATION\nThe following paper was concurrent with Ulyanov et al (2016a) and Johnson et al in demonstrating feedforward neural style transfer, though it did not use the Gram-based formulation of Gatys et al:\n\nLi and Wand, \"Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks\", ECCV 2016\n\nCLARITY\nThe paper is very well written and easy to follow.\n\nSIGNIFICANCE\nThough simple, the proposed method is a significant addition to the growing field of neural style transfer. Its benefits are especially clear for mobile applications, which are often constrained in both disk space and bandwidth. Using the proposed method, only a single trained network needs to be transmitted and stored on the mobile device; in addition the ability of the proposed method to incrementally learn new styles means that new styles can be added by transmitting only a small number of new style-specific parameters to a mobile device.\n\nEVALUATION\nLike many other papers on neural style transfer, the results are mostly qualitative. Following existing literature, the authors use style and content loss as a quantitative measure of quality, but these metrics are unfortunately not always well-correlated with the perceptual quality of the results. I find the results of this paper convincing, but I wish that this and other papers on this topic could find a way to evaluate their results more quantitatively.\n\nSUMMARY\nThe problem and method are slightly incremental, but the several improvements over prior work make this paper a significant addition to the growing literature on neural style transfer. The paper is well-written and its experiments convincingly validate the benefits of the method. Overall I believe the paper would be a valuable addition to the conference.\n\nPros\n- Simple modification to feedforward neural style transfer with several improvements over prior work\n- Strong qualitative results\n- Well-written\n- Open-source code has already been released\n\nCons\n- Slightly incremental\n- Somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic", "title": "Multiple styles at test-time", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}