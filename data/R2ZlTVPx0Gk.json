{"paper": {"title": "DICE: Diversity in Deep Ensembles via Conditional Redundancy Adversarial Estimation", "authors": ["Alexandre Rame", "Matthieu Cord"], "authorids": ["~Alexandre_Rame1", "~Matthieu_Cord1"], "summary": "Driven by arguments from information theory, we introduce a new learning strategy for deep ensembles that increases diversity among members: we adversarially prevent features from being conditionally redundant, i.e., predictable from each other.", "abstract": "Deep ensembles perform better than a single network thanks to the diversity among their members. Recent approaches regularize predictions to increase diversity; however, they also drastically decrease individual members\u2019 performances. In this paper, we argue that learning strategies for deep ensembles need to tackle the trade-off between ensemble diversity and individual accuracies. Motivated by arguments from information theory and leveraging recent advances in neural estimation of conditional mutual information, we introduce a novel training criterion called DICE: it increases diversity by reducing spurious correlations among features. The main idea is that features extracted from pairs of members should only share information useful for target class prediction without being conditionally redundant. Therefore, besides the classification loss with information bottleneck, we adversarially prevent features from being conditionally predictable from each other. We manage to reduce simultaneous errors while protecting class information. We obtain state-of-the-art accuracy results on CIFAR-10/100: for example, an ensemble of 5 networks trained with DICE matches an ensemble of 7 networks trained independently. We further analyze the consequences on calibration, uncertainty estimation, out-of-distribution detection and online co-distillation.", "keywords": ["Deep Learning", "Deep Ensembles", "Information Theory", "Information Bottleneck", "Adversarial Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a new method of learning ensembles of neural networks based on the Information Bottleneck theory, which increases the diversity in an ensemble by minimizing the mutual information between latent features of the different ensemble models. It shows promising results on classification, calibration and uncertainty estimation. The paper is well-written and the comments were properly addressed."}, "review": {"lCOLVv7IHyR": {"type": "review", "replyto": "R2ZlTVPx0Gk", "review": "## Summary\nThis paper introduces a training procedure for ensembles of neural networks that improves intra-member diversity to achieve better accuracy and calibration.\n\n## Originality\nThis paper augments the VIB training objective of (Alemi et al., 2017) with two modifications: \n- adding a mutual information penalty between encodings of the same input by different ensemble members\n- conditioning the mutual information between encodings on the _input label_\n\nThe use of mutual information in training neural networks is not novel; indeed, as the authors point out, using mutual information between input and encoding; encoding and output during training is common. However, explicitly reducing mutual information between separate feature representations is, to the extend that I am aware of, a novel and intuitive contribution.\n\n## Significance\nThis paper addresses a well-known, yet still only partially understood, fundamental problem in ensemble learning: improved diversity between ensemble members is correlated with improved ensemble performance, even though requiring diversity within logits reduces each member's accuracy on what should often be easy-to-predict inputs. \n\nThe authors address this paradox by choosing instead to favor diversity within the ensemble member's feature spaces, using a mutual-information based loss, and introduce approximations to efficiently minimize this loss.\n\n## Clarity\nI found the beginning of the paper very clear and well exposed; however, section 2 requires careful reading and several passes to understand completely; given that it contains the crucial definition of this paper's main contribution, this made the overall understanding of this paper more complicated.\n\nI would recommend assigning more space to the equations, and perhaps removing Figure 3 (which, even on a screen for a non color-blind person, was difficult to parse) to allocate more space to defining crucial components of section 2.B.1.\n\nI am very open to improving my score as these concerns are addressed.\n\n## Pros/Cons\n\nThe idea behind this paper is both intuitive and elegant; however, I had a difficult time following anything more than the high-level explanation for VCEB. Although this may in part be my lack of familiarity with recent results, I found the explanation of this central part of the paper confusing.  \n\n- The appendix E.4 was helpful, but I would strongly recommend that the authors provide some clarifications in the main text: namely, defining the encoder, backward decoder, and the other terms that define the training loss.\n\n- For the empirical neural estimation: how does the discriminator $w$ scale with the size of the ensemble? I would imagine that as $M$ increases (or as the complexity of the neural networks themselves increases), $w$ scales as well.\n\n- Minor: I understand that it is difficult to fit all experimental results within the paper; however, Table 1 really needs to be larger.\n\n- Pro: the authors included significant ablation studies for each component introduced in their training loss, and show meaningful (outside of standard deviation) improvement over several important baselines.\n\nFinally, the authors may be interested in the recent work by A. Masegosa: [Learning under Model Misspecification: Applications to Variational and Ensemble methods](https://arxiv.org/pdf/1612.00410.pdf), which provides a theoretical justification for the use of diverse ensembles in improving predictive accuracy and calibration.", "title": "Increasing ensemble performance by reducing conditional mutual information", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "xtGuPOeO-1-": {"type": "rebuttal", "replyto": "HHZCekB9RS1", "comment": "We thank you for your comment and this relevant reference (as it introduced I-Diagrams) that will be included in Section 2.A.", "title": "Additional Citation"}, "yiVxOvWj0iE": {"type": "review", "replyto": "R2ZlTVPx0Gk", "review": "The paper introduces a new training criterion based on Information Bottleneck theory, which increases the diversity in an ensemble by minimizing the mutual information between latents of the different ensemble models. This leads to more diverse encodings that are useful for the task, which leads to better ensemble performance overall.\n\nOverall, I\u2019m scoring the paper with a weak accept. It provides a new principled application of Information Bottleneck objectives with good experimental results in a new area of application. I hope a baseline implementation of this new objective will be made available for others to use. However, I have a few concerns which I detail below which the authors will hopefully be able to resolve easily.\n\n### Strengths\n\nThe paper is written very well. Overall, it is clear and engaging. The I-diagram helps understand the trade-offs within the objectives and makes it easy to see the information-theoretical equivalence of IBR and DICE (1).\n\nThe main conceptual idea is to add a term to minimize the (conditional) redundancy between latents of different ensemble members, which is expressed as the mutual information of the latent $I[Z_1;Z_2]$ in the case of an ensemble with two members. This term targets correlations between the latents which otherwise would add bias to the ensemble members.\n\nThe paper clearly describes the path from the general IB objective and the conceptual idea of reducing \u201credundancy\u201d between (by minimizing to the CEB objective (Fischer, 2020) on to estimating the conditional redundancy.\n\nIt provides various experiments that show that this new method achieves better ensemble performance than similar approaches or independent ensembles that do not optimize for diversity.\n\n### Concerns\n\n1. The paper mentions that the second term on the RHS of (3) is not empirically necessary. However, they do not provide an ablation/experimental results that show the performance without dropping that term. For the sake of being principled, it would be nice to show evidence that dropping the term does not affect performance.\n\n2. Using information-theoretic principles to increase diversity is great. This reviewer wonders if the IB objective is necessary at all however: \nConditional redundancy could also be minimized independently as the idea is not itself connected to the IB objective. $I[Z_1;Z_2|Y]$ could also be meaningfully computed for deterministic encoders, for example. As such, the applicability of the contribution could be increased by applying CR as regularizer to ensembles. \n\n3. The authors write in 2.A.2: \u201cThe problem is that IBR ignores Y in the compression & redundancy terms which cause loss of necessary information\u201d as motivation for the CEB. However, this is not true. Indeed, the CEB is equivalent to the regular IB objective for $\\beta=2$. As such, there is not conceptual difference, above statement is misleading. The reviewer speculates that CEB might allow for stabler optimization compared to the regular IB objective because the terms in the regular IB objective are \u201cfighting\u201d against each other by both \u201cincluding\u201d $I[Y;Z]$ within its terms: one term to minimize it, the other to maximize it. As the two terms are usually estimated separately this can lead to instability and \u201cinfighting\u201d. Another speculation is that VCEB is more flexible than DVIB in its variational approach.\n\n---\nFollowing the author's reply, I've raised my score from a 6 to a 8.", "title": "A novel application for IB objectives and information-theoretical quantities that improves diversity in ensembles", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ly_4IVgwXA1": {"type": "rebuttal", "replyto": "yhY_usU5T8_", "comment": "We sincerely apologize\u00a0for this mistake, which\u00a0runs contrary to what we believe in and our call for diversity in research (in Appendix H). We understand and share your point of view. That was an unfortunate mistranslation. The gendered pronoun (\"his\") was edited in the previous response.", "title": "Apologies for the use of a gendered pronoun"}, "n6sB9qgBeUX": {"type": "rebuttal", "replyto": "yiVxOvWj0iE", "comment": "We would like to thank the reviewer for these very interesting questions, that we will try to address below. We will soon open source the code in Pytorch and help for the reproducibility of our findings.\n\nQ1: \u201cit would be nice to show evidence that dropping the [RHS] does not affect performance.\u201d\n\nAs a reminder, the neural estimation of the conditional redundancy leads to $\\hat{\\mathcal{I}}_{DV}^{CR}$, that is the difference between 2 components. The Left-Hand-Side (LHS) was underbraced with the term \u201cdiversity\u201d while the Right-Hand-Side (RHS) was underbraced with the term \u201cFake correlations\u201d.\nIn this paper, we only use the LHS component and get rid of the RHS. This was backed up by our empirical findings, which are now included in newly added Figure 12 in Appendix G. We obtain $77.40$ with LHS+RHS instead of $77.51$ with only the LHS, with a 4-branches ResNet-32 on CIFAR-100. Training only with the RHS and removing the LHS (the opposite of what is done in DICE) reduces diversity compared to CEB. Moreover, keeping both the LHS and the RHS leads to slightly reduced diversity and ensemble accuracy compared to DICE. \n\nLHS improves ensemble diversity and overall performances. We argue that the LHS forces features extracted from the same input to be unpredictable from each other, to simulate that they have been extracted from two different images. On the contrary, the RHS correlates members and decreases diversity. As discussed in the paragraph \u201cintuition\u201d from subsection 2.B.2, we hypothesized that the RHS would force features extracted from two different inputs from the same class to create fake correlations; to simulate that they have been extracted from the same image.\nIn conclusion, dropping the RHS performs better while reducing the training cost.\n\nQ2: \u201cUsing information-theoretic principles to increase diversity is great. Conditional redundancy could also be minimized [...] for deterministic encoders. [...] As such, the applicability of the contribution could be increased by applying CR as regularizer to ensembles.\u201d\n\nThe question is whether we could learn deterministic encoders with second order $I(Z_i; Z_j|Y)$ regularization (CR) without tackling first order $I(X; Z_i|Y)$ CEB components. This is a possible extension of our work that was mentioned briefly in our previous Appendix E.6. We ran additional experiments that are summarized below, and extensively detailed in the new Appendix F.2 and new Table 11.\n\nA first approach (feeding the discriminator $w$ with deterministic triples) increases diversity and reaches 77.09, compared to 76.78 for independent deterministic learning. A second approach, similar to instance noise [a], reached 77.33 by better protecting individual accuracies. In comparison, DICE obtains 77.51, showing that learning CR along with VCEB performs best, at almost no extra cost. In addition to the theoretical motivations, VCEB and CR work empirically in synergy.\nHowever, as compression losses may underperform for some architectures or datasets, learning only the conditional redundancy (without compression) could increase the applicability of our contributions in future works. This remark is now included in our conclusion.\n\nFinally, we would like to point out two possible explanations for the DICE success. First, the adversarial learning may be simplified as it only focuses on spurious correlations VCEB has not already deleted. Second, VCEB learns a distribution; a mean but also an input-dependant covariance $e_{i}^{\\sigma}(x)$, that fits the uncertainty of a given sample $x$: [b] has shown that large covariances were given for difficult samples. Sampling in CR from this input-dependant covariance performs better than using an arbitrary fixed variance in Table 11.\n\n| Method   |      Top-1 Accuracy on CIFAR-100 on 4-branches ResNet-32|\n|----------|:-------------:|\n| Ind. |  76.78 | \n| Determ. + CR no sampling |  77.09   | \n| Determ + CR with sampling | 77.33 | \n|  DICE (VCEB + CR with input-dependant sampling) | 77.51 | \n\n\n[a] S\u00f8nderby, Casper Kaae, et al. \"Amortised MAP Inference for Image Super-resolution.\" ICLR (2017).\n\n[b] Yu, Tianyuan, et al. \"Robust person re-identification by modelling feature uncertainty.\" ICCV. 2019.\n\nQ3:  \u201cCEB is equivalent to the regular IB objective [..] CEB might allow for stabler optimization compared to the regular IB objective\u201d\n\nThere is indeed an equivalence in [c] between the criteria for IB and CEB for appropriate $\\beta$ and our sentence was misleading. The current submission has been refined and now simply states that: \u201cThe problem is that the compression and redundancy terms in IBR also reduce necessary information related to $Y$\u201d.\nOur experiments are consistent with your intuitions. We found VCEB to lead to more stable training than DVIB: different variational approximations yield different training losses.\n\n[c] Fischer, Ian. \"The conditional entropy bottleneck.\" arXiv (2020).\n", "title": "Response to Reviewer 3"}, "38p0ZK6k32F": {"type": "rebuttal", "replyto": "rLUyEViEv18", "comment": "We would like to thank the reviewer for these positive comments. We address your concerns in detail below.\n\n\nQ1: \u201cEmpirical neural estimation: [...] sampling $\\mathcal{B}_{p}$ from the product distribution\u201d\n\nFor the training of the discriminator $w$ with $L_{ce}$ (in subsection 2.B.2), we sample from the product distribution $p(z_1, y)p(z_2|y)$. You proposed to approximate $p(z_2|y)$ by its variational approximation $b(z_2|y)$: we have tried and it worked, but not as consistently as what we have done.\nIn fact, we can estimate the true distribution $p(z_2|y)$ with a simple trick: by taking the features $z_2$ extracted from another image $x\u2019$ that has the same class $y$ as $x$. Indeed, $p(z_2|x\u2019 \\text{ belongs to } y)$ is an empirical estimate of $p(z_2|y)$ on the classification training dataset. In Figure 4, that\u2019s the reason why the two images both belong to the same class $y$. The discriminator is therefore given as input the triple from the estimated product distribution: $(z_1|x, z_2|x\u2019, y)$, where $x$ and $x\u2019$ belongs to the class $y$.\nHowever, such an example $x\u2019$ belonging to the same class as $x$ may not be found in the current batch. Indeed, the batch size (=128) is in the same order of magnitude as the number of classes (=100 for CIFAR-100). That\u2019s why we keep a memory from previous batches, as explained in the \u2018sampling\u2019 paragraph of Appendix B.4. This features' memory is of size $M$ * $d$ * $K$ * $4$, where $M$ is the number of members, $d$ the features dimension and $K$ the number of classes. It is updated at the end of each step.\nWe have used these terms (true/false) for the joint and product distributions, by analogy with the classical generative adversarial framework (true real image/fake generated image). This inappropriate use has been fixed in the new submission to avoid any misunderstanding.\n  \n\nQ2: \u201cThe value of beta for the VIB and VCEB\u201d\n\nAs detailed in paragraph \u201cScheduling\u201d from Appendix B.2, we employ the jump-start annealing method for scheduling our IB losses. It was first introduced in [a] and is now common practice for information bottleneck approaches such as recent works [b] and [c]. [c] stated that \"It makes it much easier to train models at low [$\\beta_{ceb}$] , and appears to not negatively impact final performance\".\nAs the other hyperparameters, the scheduling was adapted on a validation dataset made of 5% of the training dataset, for a 4-branches ResNet-32 ensemble trained on VCEB. For CIFAR-10, we therefore took the scheduling from [c], except that we widened the intervals to make the training loss $D_kl(e_m, b_m)$ decrease more smoothly. We trained several different models for different values of $\\log(\\beta_{ceb})$, the results were robust on the interval [1.7, 2.2] and we took the round number 2.0. No standard scheduling was available for CIFAR-100. As it is more difficult than CIFAR-10, we added additional jump-epochs with lower $\\beta_{ceb}$ with wider intervals. $\\beta_{ib}$ were deduced from the equivalence from [d]: $\\beta_{ib} = \\beta_{ceb} + 1$.\nThese schedulings have been used in all our setups, without and with redundancy losses, for ResNet-32, ResNet-110 and WRN-28-10, for $M$ from 1 to 10 members.\n\n[a] Wu, Tailin, et al. \"Learnability for the information bottleneck.\" PMLR, 2020.\n\n[b] Wu, Tailin, and Ian Fischer. \"Phase Transitions for the Information Bottleneck in Representation Learning.\" ICLR 2019.\n\n[c] Fischer, Ian, and Alexander A. Alemi. \"CEB Improves Model Robustness.\" arXiv (2020).\n\n[d] Fischer, Ian. \"The conditional entropy bottleneck.\" arXiv (2020).\n\n\nQ3: \u201cThe ensemble experiment results (Section 4C and Table 2) are really hard to understand and interpret.\u201d\n\n[e] established new standards for quantifying performances of in-domain calibration and uncertainty for image classification. [e] argues that comparisons of metrics such as negative log likelihood should be made after temperature scaling [f]. In section 4.C, we apply this procedure.\nDue to the 8-page constraint, details, full acronyms and appropriate citations of these metrics were in previous Appendix D.2. They now introduce Experiment 4.C. We also did a more detailed analysis of our results. Moreover, to simplify the readability of the associated Table, we only kept scores after temperature scaling. Those before temperature scaling were moved to Appendix A.6 in Table 9.\n\n[e] Ashukha, Arsenii, et al. \"Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning.\" ICLR. 2019.\n\n[f] Guo, Chuan, et al. \"On Calibration of Modern Neural Networks.\" ICML. 2017.\n\n\nCitation: Thank you for reminding us to cite the work from Ovadia [g], which has been a great motivation to study deep ensembles.\n\n[g] Ovadia, Yaniv, et al. \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift.\" Neurips. 2019.\n", "title": "Response to Reviewer 2"}, "t6WnxUgC06w": {"type": "rebuttal", "replyto": "lCOLVv7IHyR", "comment": "We appreciate that you read our paper closely and highlight its possible theoretical and practical impacts. We will try to address your concerns below. To put it simply, we provided additional details and tried to improve the overall layout.\n\nQ1: \u201cAllocate more space to defining crucial components of section 2.B.1\u201d\n\nSection 2 is indeed the most challenging part of our argumentation. We build upon the work from Fischer [a] in which they explained in detail the variational approximation. These computations were summarized in Appendix E.4. In subsection 2.B.1, we quickly introduced the different components in the same order that they appeared in Equation 2, the VCEB loss. Previous explanations lacked a proper practical description and intuition of each element. That\u2019s why we added the following paragraph in subsection 2.B.1 of the updated version:\n\nPractically, we parameterize all distributions with Gaussians. The encoder $e_i$ is a traditional neural network features extractor (e.g. ResNet-32) that learns distributions (means and covariances) rather than deterministic points in the features space. That's why $e_i$ transforms an image into 2 tensors; a features-mean $e_i^{\\mu}(x)$ and a diagonal features-covariance $e_i^{\\sigma}(x)$ each of size $d$ (e.g. $64$). The classifier $c_i$ is a dense layer that transforms a features-sample $z$ into logits to be aligned with the target $y$ through conditional cross entropy. $z$ is obtained via the reparameterization trick. Finally, the backward encoder $b_i$ is implemented as an embedding layer of size ($K$, $d$) that maps the $K$ classes to class-features-means $b_i^{\\mu}(z|y)$ of size $d$, as we set the class-features-covariance to $1$. The Gaussian parametrization also enables the exact computation of the $KL$, that forces (1) features-mean $e_i^{\\mu}(x)$ to converge to the class-features-mean $b_i^{\\mu}(z|y)$ and (2) the predicted features-covariance $e_i^{\\sigma}(x)$ to be close to $1$.\n\nWe hope that these additional practical details clarify the argumentation.\n\n[a] Fischer, Ian. \"The conditional entropy bottleneck.\" arXiv (2020).\n\n\nQ2: \u201cFor the empirical neural estimation: how does the discriminator scale with the size of the ensemble?\u201d\n\nOne key practical strength of our paper is the use of only one discriminator $w$ for the $\\binom{M}{2}$ neural approximations of pairwise interactions, as stated in Section 2.C. The number of weights in $w$ scales linearly with the number of members $M$ and the embedding size $d$ as $w$\u2019s input grows linearly. $w$\u2019s hidden size and the number of hidden layers remain fixed. This methodology scaled for network-ensembles at least up to 10 ResNet-32 where d=64, and at least up to 5 ResNet-110 where d=256.\nIn more details, $w$ is a multi-layer perceptron with 4 layers of size {256, 256, 100, #classes}. This architecture was described in Appendix B.4 and was inspired by Tables 3 and 4 from [b]. $w$ takes as input M tensors of size $d$ and maps them to the first layer of size 256: therefore its first matrix grows with $M$ * $d$ * $256$ .\nWe verify that our discriminator with 256 hidden size remains calibrated (as shown on Figure 11) in all our setups. As stated in [b], calibration is important to obtain consistent estimation of conditional mutual information through neural estimation. We tried to widen the hidden size of $w$ to $M*d$ (rather than a fixed 256) but it has not consistently improved overall results. That\u2019s why an additional member in the ensemble only adds $256$ * $d$ trainable weights in $w$. This important point was only briefly underlined in Section 2.C and is now better highlighted.\n\n[b] Mukherjee, Sudipto, Himanshu Asnani, and Sreeram Kannan. \"CCMI: Classifier based conditional mutual information estimation.\" PMLR, 2020.\n\nQ3: Recommendations about general layout and Table 1 in particular\n\nPrevious Table 1 is now divided into two separate tables: the main one dedicated to CIFAR-100 (new Table 1), the second one to CIFAR-10 (new Table 2). The unified Table 12 can still be found in Appendix J.\nMoreover, the general layout has been improved. We assigned more spaces to highlight the main equations in Section 2, which we hope are more quickly visible at first glimpse now. Moreover, we edited and widened Figure 3 for improved clarity, while including a zoomed version in Figure 13 in Appendix I.\n\n\nCitation: Thank you for mentioning Masegosa [c]. It is now included in our new introduction as they also tackle the trade-off between individual accuracy and ensemble diversity. Their theoretical second-order PAC-Bayes bound leads to an interesting diversity-inducing learning strategy that we included in our related work. They stated that \u201crandom initialization [...] is one of the key ingredients to make them diverse\u201d: our work demonstrates that explicitly reducing conditional redundancy is another key.\n\n[c] Andres R. Masegosa. \"Learning under Model Misspecification: Applications to Variational and Ensemble methods.\" Neurips, 2020.", "title": "Response to Reviewer 1"}, "BpjVTRnNfl": {"type": "rebuttal", "replyto": "Pr9RusQvVKG", "comment": "Thank you for your review and positive comments about the problem setting and for raising this question about higher order approximations. We detail our answer below, and also in Appendix F.1.\n\nOur training criterion was derived from information bottleneck principles (in section 2.A) for an ensemble of $M=2$ members, and was transformed into a tractable loss (in section 2.B) using two distinct methods: a variational approximation VCEB for first order $I(Z_i; X|Y)$ (in subsection 2.B.1) and a neural estimator for second order $I(Z_i; Z_j|Y)$ (in subsection 2.B.2). Then, we generalize our training criterion for M> 2 models (in section 2.C). This is where DICE indeed approximates the information between extracted features by assuming only pairwise interactions while truncating higher orders. It is common practice, for example in the feature selection literature [a,b,c].\n\nMore precisely, let\u2019s define the order of an information interaction as the number of different extracted features involved. The first order is $I(Z_i; X|Y)$ and is tackled by the CEB component. The second order is $I(Z_i; Z_j|Y)$ and is tackled by the pairwise conditional redundancy component. The third order is $I(Z_i; Z_j, Z_k|Y)$. We could up to a M-th order. \n\nHigher order interactions may capture more complex correlations. The main reason to truncate them is computational, as the number of components would grow exponentially and add significant additional cost in training. Another reason is empirical, the additional hyperparameters may be hard to optimize. Tackling first and second order interactions already brought significant and consistent gain in many setups, for many architecture variants, in terms of accuracy and uncertainty estimation. The complete analysis of these higher order interactions has huge potential and could lead to a future research project.\n\n[a] Battiti, Roberto. \"Using mutual information for selecting features in supervised neural net learning.\" IEEE Transactions on neural networks 5.4 (1994)\n\n[b] Fleuret, Fran\u00e7ois. \"Fast binary feature selection with conditional mutual information.\" Journal of Machine learning research 5.Nov (2004)\n\n[c] Brown, Gavin. \"A new perspective for information theoretic feature selection.\" Artificial intelligence and statistics. 2009.", "title": "Response to Reviewer 4"}, "jVRbYv4lmO0": {"type": "rebuttal", "replyto": "R2ZlTVPx0Gk", "comment": "We would like to sincerely thank you for your thoughtful remarks and overall positive comments. We much appreciate that you all understand our ideas and found them interesting. We have tried to answer your questions individually. Your feedback enabled us to update a new version of the paper.\n\n1. Clarity. Overall, we modified the layout by leveraging the additional 9-th page. We assigned more space to the main equations in Section 2 and further detailed the components of VCEB in Section 2.B.1 (Reviewer1 Question1, R1Q1). We explicitly state in Section 2.C that our discriminator\u2019s size grows linearly with $M$ and $d$ (R1Q2). We divided the Table 1 into two smaller Tables (R1Q3). We describe in more detail the Experiment 4.C whose Table was simplified (R2Q3). We refined some choices of words and included new relevant citations.\n\n2. Additional appendices. In Appendix F.1, we explain that DICE is a second-order approximation in terms of information interactions (R4). In Appendix F.2 and Table 11, we apply our diversity regularization to deterministic encoders (R3Q2). Appendix G and Figure 12 empirically motivate the removal of the RHS from our neural estimation (R3Q1).", "title": "General Response to All Reviewers"}, "rLUyEViEv18": {"type": "review", "replyto": "R2ZlTVPx0Gk", "review": "Summary:\nThis paper proposes a method of learning ensembles that adhere to an \"ensemble version\" of the information bottleneck principle. Whereas the information bottleneck principle says the representation should avoid spurious correlations between the representation (Z) and the training data (X) that is not useful for predicting the labels (Y), i.e. I(X;Z) or I(X;Z|Y), this paper proposes that ensembles should additionally avoid spurious correlations between the ensemble members that aren't useful for predicting Y, i.e. I(Z_i; Z_j| Y). They show empirically that the coefficient on this term increases diversity at the expense of decreasing accuracy of individual members of the ensemble.\n\nEvaluation:\nOverall, this is a strong submission and should be accepted. It is well-written and thoroughly cites and explains prior work - in several cases, I find their explanations of prior work more clear than the original papers. It makes a clear and (as far as I know) novel contribution in terms of the approach, and the empirical results seem thorough and fair. In particular, the experiments for classification accuracy are thorough and make meaningful improvements over information bottleneck baselines (VIB and VCEB). I would (of course) like to see ImageNet experiments, but given that these are ensembles of relatively large networks (ResNet32 and ResNet110) and they include CIFAR-100 results on a number of architecture variants and a handful of baselines, I think these experiments are sufficient for acceptance. I don't have enough expertise in the uncertainty and calibration results to evaluate whether those empirical results are especially strong.\n\nSuggestions for improvement:\n- Page 4, Empirical Neural Estimation: I'm not clear on one thing here - when you are sampling B_p from the product distribution, you talk about p(z_2|y) as though you have access to the true distribution here, but I'd expect that here you would only have the backward encoder distribution b(z_2|y), a variational approximation to the true p(z_2|y). (And then when you're optimizing w towards the optimal w*, you would be optimizing w towards the optimal w* for that approximate distribution, not the true distribution?) I think this is what you are alluding to when you call this the \"false product distribution\" in the next paragraph, but it wasn't clear. I believe this is inconsequential because you then drop this term entirely, but this point could use better explanation.\n- It would be useful to include an explanation of how you chose the value of beta for the VIB and VCEB baselines. (I see in a section titled \"Scheduling\" that you anneal beta according to a specific schedule, but I wasn't clear on why or whether you tuned this at all.)\n- The ensemble experiment results (Section 4C and Table 2) are really hard to understand and interpret. The acronyms in the table should be spelled out (maybe in the caption), and in Section 4C when you mention ECE and TACE for the first time, you should write out the acronyms and also cite where these metrics were introduced.\n- I'd suggest citing the ensemble + uncertainty results from Ovadia et al. 2019 (https://arxiv.org/pdf/1906.02530.pdf).", "title": "information theory for ensembles", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Pr9RusQvVKG": {"type": "review", "replyto": "R2ZlTVPx0Gk", "review": "This paper  addresses the problem of training an ensemble of learning algorithms so to boost the diversity among single learners while preserving the accuracy of each learner.\nThe paper is well written, the addressed problem is well exposed and relevant for this audience, the literature review is more than adequate.\nThe core idea and main contribution of this paper is to enforce diversity in the fature space rather than in the outputs.\nThe formalization of the approximated solution in Sec 2B has merit, however it is not clear if this is a first or higher order approximation and in the case why not considering an higher order ?\nResults on small datasets show some improvements, in some cases above 1%; yet, it is not clear how the proposed technique would perform on larger datasets such as ImageNet, which downtones a bit the value of this contribution.\n", "title": "Interesting idea with nice theoretical part, however limited experimental validation", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}