{"paper": {"title": "Speeding up Deep Learning Training by Sharing Weights and Then Unsharing", "authors": ["Shuo Yang", "Le Hou", "Xiaodan Song", "qiang liu", "Denny Zhou"], "authorids": ["~Shuo_Yang6", "~Le_Hou1", "~Xiaodan_Song1", "~qiang_liu4", "~Denny_Zhou1"], "summary": "Speeding up deep learning training by sharing weights and then unsharing", "abstract": "It has been widely observed that increasing deep learning model sizes often leads to significant performance improvements on a variety of natural language processing and computer vision tasks. In the meantime, however, computational costs and training time would dramatically increase when models get larger. In this paper, we propose a simple approach to speed up training for a particular kind of deep networks which contain repeated structures,  such as the transformer module. In our method, we first train such a deep network with the weights shared across all the repeated layers till some point. We then stop weight sharing and continue training until convergence. The untying point is automatically determined by monitoring gradient statistics. Our adaptive untying criterion is obtained from a theoretic analysis over deep linear networks.  Empirical results show that our method is able to reduce the training time of BERT  by 50%. ", "keywords": ["fast training", "BERT", "transformer", "weight sharing", "deep learning"]}, "meta": {"decision": "Reject", "comment": "\nThe paper proposed a new way for training models that stack the same basic block for multiple times -- share the weights first and then untie the weights. Ablation study shows that the proposed algorithm has marginal improvement over the baseline. The authors also provide some theoretical justifications to how the proposed idea works.\nThe proposed idea is straightforward and intuitive. Weight sharing has been used in previous works, and what\u2019s new in this paper is to unshare the weights in the middle (with a heuristic rule). The hope is that by doing so, one can achieve a better tradeoff between speedup and accuracy. However the experimental supports are somehow weak and incomplete. For example, in order to show the real speedup, one should provide the full training curve (until convergence) under different settings, instead of just showing one data point (at 500K). It is very common that one can get some speedup at 500K, but the speedup totally disappears after another 500K steps. \nFurthermore, the theoretical analysis is conducted in a simplified setting, and it is not very clear whether it can be used to explain what really happened during BERT training.\nThe reviewers conducted some lengthy discussions after the author rebuttal was available. As a final consensus, we think that there are still concerns on the paper, which makes us hesitate to give an ACCEPT recommendation.\n"}, "review": {"RZFaXOc-OX6": {"type": "rebuttal", "replyto": "Nz_cfFfxJym", "comment": "Thanks for your feedback. \n\n--- On the analysis of the deep linear model\n\nPlease look at \"Common Response and Revision\" on the top of this page. Our goal in this paper is to provide a practically useful algorithm for training BERT like models. The analysis of the deep linear models is served as a motivation for both weight sharing and using gradient correlation as the untying rule. Theoretic analysis of BERT models is not our goal here.  We didn't attempt to establish or claim such a result. \n\n--- The key contribution of our work \n\nThe key contribution here  is the \"adaptive weight untying\"  algorithm (see algorithm 2). A random chosen untying point will NOT speed up training. \n\n--- The setting of a unified threshold\n\nWe used the same threshold (=0.5) across all layers and all experiments. Experiments show that this threshold works fairly  well. Moreover, we observed that the gradients correlations all start from almost 1 and all gradually drop to 0. So, eventually all layers are untied. Layer dependent thresholds might work better but we have no idea on how to predefine them. If you have concrete ideas here, we are happy to know. \n\n--- About \\tau \n\nPlease note that our adaptive untying algorithm (algorithm 2)  does not need \\tau. The fixed point untying method (algorithm 1) with \\tau is used as a baseline of its adaptive version. Experiments show that the adaptive method can match (even outperform) the performance of the best manually chosen untying point (ran experiments with different untying points and then chose the best one to compare. See also our ablation study section). This shows our adaptive weight sharing method is effective.\n\n--- Thanks for pointing out the relevant paper. We will cite it and add discussions.\n\n--- Please see the updated version for complete experimental results. \n", "title": "Response to Reviewer #4"}, "5f2GRTFyTZC": {"type": "rebuttal", "replyto": "yU4Ykzr57s1", "comment": "Thanks for your feedback. \n\n--- Could you please explain why  the improvement of reducing training BERT time up to 50% in our paper is considered to be \"quite marginal\" in your feedback. We will really appreciate if you could point out any faster training algorithm for BERT that we can compare with. We want to understand how big speed-up will be considered as \"not marginal\" according to your standard.  \n\n--- Your argument  \"it is possible that both methods perform similarly after being trained for a longer number of steps (e.g., 1M, 1.5M).\" is unclear for us. It is a standard procedure in the literature (e.g., RoBERTa) to train BERT 1M steps to achieve its SODA results on NLP benchmarks. Now, based on our method,  we just need 0.5 M training steps to achieve similar results.  Don't you think this comparison is fair enough? \n\n---  Our theoretic analysis part is conducted for motivating and developing the *adaptive untying criterion* used in our algorithm.   Our goal in this paper is to develop a practically useful algorithm to train BERT-like models faster. We do not attempt to build  or claim convergence analysis of the BERT model, which is entirely different from our focus here.  Please check the 4th paragraph in the introduction section of the  revised version for a high-level description on the connection between the theory and algorithm.  We also rewrote the algorithm section to make the connection between the theory and algorithm clearer. Moreover, the title of section 3  was changed from \u201ctheoretical analysis\u201d to \u201ctheoretical motivation\u201d to avoid any potential confusion or misunderstanding. \n", "title": "Response to Reviewer #2:  Please provide more concrete suggestions to help us improve this work "}, "tSCtY_rkKZ4": {"type": "rebuttal", "replyto": "jz7tDvX6XYR", "comment": "-- Our goal in this paper is to develop a practically useful fast training algorithm for BERT-like models.  It is *not*  for  theoretical research on BERT or transformer, which is entirely different from and  irrelevant to our focus here. To the best of our knowledge, the presented method should be the simplest solution to speeding up BERT training. All we need to do is to add several lines of code to modify gradients (using gradient mean) to reduce the training time 50%. Please provide feedback around algorithm design & performance,  including clarifying experiment details, missing baselines,  and so on.  That will be most helpful for us to improve this work. \n\n-- The theoretic analysis on deep linear networks is used to *motivate and derive* the adaptive untying criterion used in our training algorithm.   The adaptive untying criterion plays the most fundamental role in the success of our method.  A random chosen untying point has no way to speed up training. The fixed point untying method (Algorithm 1) is used to serve as a baseline for its adaptive version (Algorithm 2). Experiments show that the adaptive method can match (even outperform) the performance of the best manually chosen untying point (ran experiments with different untying points and then chose the best one to compare). This shows our adaptive weight sharing method is effective. \n\n-- Revised the 4th paragraph in the introduction to present a high-level illustration on the connection between the theory and algorithm.  Also Revised the algorithm section to make the connection between the theory and algorithm clearer. \n\n-- Revised the title of the theory section from \"theoretic analysis\"  to \u201ctheoretic motivation\u201d  to avoid potential confusion (e.g. thinks of our paper as an attempt to build theoretic analysis for BERT/transformer). Also revised the first paragraph of the theory section to highlight the goal of our theory.  ", "title": "Common Response and Revision"}, "ezasgFbZHhE": {"type": "rebuttal", "replyto": "ZBu0w1eIxJO", "comment": "Thanks for your feedback. \n\n\n--- The \u201citers\u201d in SWE experiments stands for iters_of_weight_share + iters_of_weight_unshare. The \\tau in SWE-F stands for the iters_of_weight_share. \n\n--- Only weight sharing will NOT speed up training much unless we know when to untie the weights. How to automatically untie the shared weights is the whole point in this paper.  We don't claim weight sharing as our innovation here which has been invented since CNN/RNN.  What we claim here is *unsharing* in an adaptive way. \n\n--- Please see \"Common Response and Revision\" on our theory and its connection to the proposed algorithm. Our goal in this paper is to develop a practically useful fast training algorithm for BERT-like models. It is a simple approach which can be implemented in a few lines of code while reducing BERT training time up to 50%.  The theoretic analysis in section 3 is used to *motivate* the adaptive untying rule used in our method, which plays the most fundamental role for the success of our method.  We do *not* attempt to develop or claim any theoretical research on BERT or transformer, which is entirely different from our focus here and should be a separate research topic. On the other hand, if you think the improvement from our method is not big enough and have ideas to further improve this method, we are happy to know. ", "title": "Response to Reviewer #1"}, "nYFjbxBhKm": {"type": "rebuttal", "replyto": "IwA4ocixC8", "comment": "We will try the the idea in [1]. It looks very interesting. Next we are gonna conduct experiments on machine translation. ", "title": "Thanks for the reference"}, "LJBtLMiqaed": {"type": "rebuttal", "replyto": "Jnlf2gRtAfa", "comment": "Thanks so much for the quick comments. \n\n--For experiments on wiki+books, we exactly followed the Pre-LN paper by Xiong et al 2020 by setting the peak learning rate to 3e-4 (2x larger than the original BERT's 1e-4). We also tested 2e-4 and 4e-4. Both performed worse than 3e-4.   For the XLNet dataset with whole word masking, however, the peak learning rate of 3e-4 makes training unstable and yields worse performance than 2e-4. \n\n--In our original submission, on the wiki+books dataset, we did not normalize the input to the last layer (classification/MLM). This normalization is unnecessary for post-LN  but *needed* for pre-LN. In this revision, we fixed this issue and thus all the results are improved.\n\n--The broken sentence should be \u201cNote that the correct implementation of Pre-LN contains a final layer-norm right before the final classification/masked language modeling layer. \u201c (That is, removing \u201cwe\u201d from the original sentence. )\n", "title": "Response to Comments "}, "G5_5a3tA3Ax": {"type": "review", "replyto": "jz7tDvX6XYR", "review": "This work introduces a method to accelerate the training speed of deep learning models. The key idea is to start training with shared weights and unroll the shared weights in the middle of the training. The authors report that this strategy accelerates the convergence speed. The paper introduces heuristics on when to stop weight sharing and how many layers to share weights. It further provides an analysis via the view of deep linear models on why weight sharing helps improve the convergence speed. In the evaluation, the paper evaluates their approach against the training of BERT, and shows that their method can obtain comparable and sometimes even better accuracy on downstream tasks while with 50% faster training speed.\n \nStrengths:\n+ The paper aims to address an important problem in large model training: slow training speed.\n+ The paper proposes an easy-to-implement approach to accelerate the convergence speed of BERT-like models. \n \nWeakness:\n- The technical contribution seems rather incremental. The main difference between this work and the prior work [1], which also train Transformers via shared weights,  seems to be switching from sharing weights to unsharring weights in the middle of the training.\n- Important references are missing, making it not clear the advantage of this work as compared with existing approaches that accelerate the training of Transformer networks. \nThe comparison with existing work is inadequate. Important ablation studies are needed. \n \nComments:\n \nPrior work [1] uses weight sharing to train a smaller Transformer model to obtain similar accuracy.  However, weight sharing does not improve training speed per batch, because the training still needs to perform the same amount of forward and backward computation for each batch. Training may actually be slowed down, since the model may need to train with more batches to reach the same accuracy. This paper aims to speed up the training process by switching from shared to unshared weights in the middle of the training, and it observes faster convergence -- achieving similar downstream accuracy with less number of pretraining samples. This is an interesting empirical observation and can potentially become useful in practice. However, there are some major concerns about the paper.\n \nIt is still unclear whether this faster convergence comes from switching from sharing to unsharring weights or is an effect of the model or hyperparameter changes. First, the stop condition (e.g., the switching threshold) cannot be known as a prior. Therefore, it is controlled with an additional hyperparameter. From the text, it is unclear how this hyperparameter has been chosen or will be chosen when training a new model. It would be better to test the sensitivity of the hyperparameter on another model such as GPT-2 to verify the effectiveness of the proposed method.\n \nSecond, the paper adopts Pre-LN in its evaluation (as briefly mentioned in Section 5.1). However, from the text description, it seems it employs the original BERT as the baseline (\"We first show experiment results English Wikipedia and BookCorpus for pretraining as in the original BERT paper\"). As Pre-LN has been studied in several prior work [2,3] and has been demonstrated to also have the effect of accelerating the convergence speed, it is unclear whether the observed speedup in this paper is an effect of PreLN or weight sharing/unsharing. An ablation study with BERT-PreLN is needed if not already included.\n \nThird, the analysis on deep linear models appears to be over-simplified, where important characteristics of the DNNs such as non-linear activations and residual branches are not represented, making it difficult to connect it with the actual observations in practice.\n \nFinally, importance reference [4] is missing. [4] starts with a shallow BERT and progressively stacks Transformer blocks to accelerate training speed, which is in some sense similar to the proposed technique, which starts with shallow BERT with shared weights and switches to full-length BERT in the middle of the training. The paper might need to highlight the difference between this work and [4]. \n \nSeveral places in the paper are vague or inconsistent: \n\n1. The paper claims that it uses the same BERT implementation and training procedure as the one used by Devlin et. al.. However, the accuracy reported in Table 2 seems to be consistently lower than what was reported in the original BERT paper. For example, QQP in the original paper reaches 72.1, whereas this paper reports 71.4, QNLI was 92.7 in the BERT paper and 91.7 in this paper. If we use the original BERT reported results, the proposed technique seems to incur low accuracy on most tested datasets. Some clarification on the accuracy results is needed. \n\n2. The paper claims that \"it sounds natural for us to expect that ALBERT's performance will be improved if we stop its weight sharing at some point of training. The optimal models are supposed to not be far from weight sharing.\" However, this explanation actually creates some confusion. First, the paper does not provide an analysis of how the weight distribution between the weights trained with and without weight sharing, so it is unclear what \"not be far\" means. Second, what does it mean by \"optimal model\"?  Does it refer to models trained not through weight sharing? If so, prior work [5] identified that model weights and gradients at different layers can exhibit very different characteristics, which seems to contradict the argument that \"The optimal models are supposed to not be far from weight sharing\". \n\n3. I find it challenging to claim the proposed technique generic for models with repeatedly layers, whereas only BERT is evaluated in the experiments.\n\n4. The paper says \"This means that the weight sharing stage should not be too long\", but it is unclear how long is considered as not too long. \n \n[1] Lan et. al. \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\", https://arxiv.org/abs/1909.11942\n\n[2] Xiong et. al. \"On Layer Normalization in the Transformer Architecture\", https://arxiv.org/abs/2002.04745\n\n[3] Shoeybi et. al. \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\", https://arxiv.org/abs/1909.08053\n\n[4] Gong et. al. \"Efficient Training of BERT by Progressively Stacking\", http://proceedings.mlr.press/v97/gong19a/gong19a.pdf\n\n[5] You et. al. \"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes\", https://arxiv.org/abs/1904.00962", "title": "Official Blind Review #3", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "KptY_YoMDld": {"type": "rebuttal", "replyto": "G5_5a3tA3Ax", "comment": "We thank review #3 for the thorough and insightful review. For the concerns raised in the review:\n- On the stop weight sharing condition\n\nFor the reported results, we fix $\\rho = 0.5$ without tuning for all experiments and it offers good results. In the ablation study, table 3 shows that the performance is not sensitive to the setting of $\\rho$ in a large range.\n\n- On the theoretical analysis\n\nWe have changed the title of section 3 to theoretical motivation. Our goal here is to provide a motivation for training with weight sharing and using the gradient correlation as the untying criterion. We don\u2019t attempt to provide convergence analysis for BERT-like models. \n\n- On related work \n\nUnlike progressive stacking [4], our approach does not involve any operation that changes the network structure. It achieves acceleration by using a few lines of code for modifying the gradients.\n\nMoreover, progressive stacking [4] requires to predefine when to stack to achieve acceleration. The paper doesn\u2019t provide any guidance on this. Instead, our proposed algorithm can automatically decide when to unshare the weights.\n\n- On the experiment results\n\n  1. Concerns on Table 2 being worse than the original BERT. Results in Table 2 are obtained using the XLNet dataset which is different from the dataset used in the original BERT paper. So they are not comparable. Please see Table 1 for the comparison with the original BERT training.\n  2. Comments on Pre-LN. We updated the submission in which Pre-LN is used in all experiments on all datasets.\n", "title": "Response to Reviewer #3"}, "ZBu0w1eIxJO": {"type": "review", "replyto": "jz7tDvX6XYR", "review": "This paper proposes a simple yet effective method to train transformer-like networks for NLP tasks. Weight sharing idea is not new, it has been investigated in ALBERT as mentioned in the paper and CV areas (see refs [1],[2]).  Though novelty of this paper is not strong enough, their experimental results are adequate to show the effectiveness of proposed method.  The writing and description are quite clear, but you need to clarify the definition of 'iters' you used in  SWE exps, does it mean iters_of_weight_share + iters_of_wight_unshare?\n\nTo show the advantage of weight sharing, the authors take deep linear model as an example to show that with weight sharing, layers far away from output will not suffer from gradient vanishing or explosion, which is true for deep LINEAR models. But for deep learning based models, non-linear operations is required, which still leads to gradient vanishing or explosion even with weight sharing. For example, recurrent neural network is a typical kind of weight-sharing network and its weight is shared through time. Even with single hidden layer, RNN still suffers from gradient vanishing or explosion, and that is why LSTM is proposed in 1990s. So in my opinion, you'd better take a TRUE deep learning model as an example to analyze the effectiveness of weight sharing during training. For example, you can try to analyze deep feed forward neural networks with ReLU activation, which can be treated as a deep piece-wise linear model.\n\n[1] J. Wang and X. Hu, \u201cGated recurrent convolution neural network for ocr,\u201d in NIPS, 2017.\n[2] M. Liang and X. Hu, \u201cRecurrent convolutional neural network for object recognition,\u201d in CVPR, 2015, pp. 3367\u20133375", "title": "Simple yet effective method, deeper analysis is needed", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Nz_cfFfxJym": {"type": "review", "replyto": "jz7tDvX6XYR", "review": "This paper provides a simple way to speed up training deep networks which contain repeated structures, such as the transformer module, by sharing the weights first and then unsharing. Three methods are shown to stop weight sharing: 1) stop weight sharing at a predefined point; 2) stop weight sharing at a point using an adaptive way, in which unsharing will be triggered when the majority of the gradient correlations between layers is below a threshold; 3) gradually untie weights between layers by splitting groups based on the gradient correlations. The authors did experiments on BERT large for SQuAD and the GLUE benchmark, and showed that their approach outperform the baseline at the same iteration of 500k, and is on par w/ the baseline training at 1M iterations.\n\nBasically I think this approach could be a practical way to train deep networks w/ repeated layers. It's like between BERT large and ALBERT, the results are not that surprising. The main contribution of this work is that it provides the empirical results of this training method on several tasks.\n\nMy questions regarding to this paper are:\n\n1. In Section 3, the authors provide analysis of weighting sharing can achieve better convergence rate and for good weight sharing each layer's update needs to well correlate with its gradients for deep linear network. How about any theoretical insights for networks w/ activation functions? And How could different activation functions matter (ReLU, Swish et al)? And will these activation functions affect the parameters of when to stop weight sharing?\n\n2. For untie weights between adjacent layers, SWE-A and SWE-M, what's the motivation of setting a unified threshold for all layers? Basically I'm wondering what's the gradient correlation between adjacent layers of top layers and bottom layers, are the correlations on the same level? Will we get better accuracy if we do not limit to a unified threshold? Say, for the top 12 layers, we set the threshold to X and for the bottom 12 layers we set the threshold to Y. Need further information on this.\n\n3. In Table 2, why is SWE-A missing?\n\n4. For Table 3, I think the authors need to add the experiments when \\tau is smaller than 50k. Basically the experiments will show that \\tau should be carefully designed, an arbitrary number may not work, we still need to train some iterations w/ weight sharing for the good model performance.\n\n5. This paper may need to cite some other papers discussing some insights of weight sharing, such as  DEEPER INSIGHTS INTO WEIGHT SHARING IN NEURAL ARCHITECTURE SEARCH (https://arxiv.org/pdf/2001.01431.pdf).", "title": "Maybe a practical way to train deep networks w/ repeated layers", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "yU4Ykzr57s1": {"type": "review", "replyto": "jz7tDvX6XYR", "review": "##########################################################################\n\nSummary:\n\nThe paper proposed a new way for training models that stack the same basic block for multiple times -- share the weights first and then untie the weights. The author tried to provide theoretical insights on why it can be better. Also, ablation study shows that the proposed algorithm has marginal improvement over the baseline.\n\n##########################################################################\n\nReasons for score: \n\n\nThe improvement over the baseline method, which just trains BERT for more steps, is quite marginal. In addition, after the weights are untied, the training process becomes exactly the same as that in BERT. It's highly likely that both methods can have similar performance after being trained for a large number of steps. Also, I do not think the theoretical results on a deep linear network can help explain the  phenomena we see in BERT training. BERT uses a much more complicated building block that involves layer normalization, attention and non-linearity. Due to these reasons, I vote for rejection.\n\n##########################################################################\n\nPros: \n \n\n1. The idea of first train the model with shared weights and then untie the weights is interesting.\n \n\n##########################################################################\n\nCons: \n \n\n1. The improvement over the baseline method is not very substantial. Basically, it is possible that both methods perform similarly after being trained for a longer number of steps (e.g., 1M, 1.5M).\n\n2. I'm not convinced by the theoretical analysis. Building block in BERT is much more complicated that the building block discussed in Section 3.\n\n3. The criteria of changing from sharing to unsharing is largely heuristic.\n\n\n##########################################################################\n\nQuestions during rebuttal period: \n \n\nPlease address and clarify the cons above \n \n\n#########################################################################\n\nTypos: \n\n(1) Page 3, \"To be best of\" should be \"To the best of\"\n", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}