{"paper": {"title": "Multi-task learning with deep model based reinforcement learning", "authors": ["Asier Mujika"], "authorids": ["asierm@student.ethz.ch"], "summary": "We build a world model, based on CNN's and RNN's, to play multiple ATARI games simultaneously, achieving super-human performance.", "abstract": "In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory. The code will be released before ICLR 2017.", "keywords": ["Reinforcement Learning", "Deep learning", "Games", "Transfer Learning"]}, "meta": {"decision": "Reject", "comment": "The authors have proposed a new method for deep RL that uses model-based evaluation of states and actions and reward/life loss predictions. The evaluation, on just 3 ATari games with no comparisons to state of the art methods, is insufficient, and the method seems ad-hoc and unclear. Design choices are not clearly described or justified. The paper gives no insight as to how the different aspects of the approach relate or contribute to the overall results."}, "review": {"SyJISvaIl": {"type": "rebuttal", "replyto": "HyqxOUfVl", "comment": "I haven't updated the paper due to lack of time, but your comments will be very useful for future versions of the papers for the Workshop. Thanks for the very thoughtful review.", "title": "Review Answer"}, "ryd3RTeEl": {"type": "rebuttal", "replyto": "rkRp7zxVg", "comment": "Hi,\n\n1. Fig. 3 shows a Layer Normalization step in the Prediction network, not listed in Table 1. Which one is correct?\n\nLayer normalization is used. Take into account that the table represents the f function, but the layer normalization is part of RRNN and is applied before the f function. The table only shows the f function from Figure 3.\n\n2. Could you please clarify the definition of rj2 in 4.4? It sounds like it is equal to 1-rj1, which would be redundant, and I am not sure I understand the concept of \"earning a point\" (is the game score taken into account anywhere?)\n\nBy earn I point I mean that our score is higher than it was initially. That is, we have received more positive rewards than negatives from the environment. This are the possible scenarios, which should make clear the meaning of rj2.\n\n- If we have died: rj1 = 1, rj2 = 0\n- If our score is higher and we haven't died: rj1 = 0, rj2 = 1\n- If we haven't earned a reward but haven't died neither: rj1 = 0, rj2 = 0\n\n\n\nI hope this makes it more clear.", "title": "Answer"}, "ByXMaplNl": {"type": "rebuttal", "replyto": "BJgp1tCCfx", "comment": "Hi,\n\n- Could you please elaborate last paragraph of section 3.1. How does predicting the reward difference does not depend on the strategy being played?\n\nThe reward depends on the future set of actions that we are going to take but not on the global strategy. That is, the change in score for the next K steps, depends only on the current observation and the next K actions we perform. This means that for a given input to our model, the output is fixed (in the deterministic case) and doesn't depend on which actions we would perform after those K steps.\n\n-Where are the final results? The only results data I could find was table three which only shows the results on iteration 2 and not after convergence.\n\nThese have been added to a table of results.\n\n-Where are the comparisons with baseline, DQN and many other STOA results published since?\n\nThese have also been added to make it clear that the model doesn't reach SOTA performance. Yet, take into account that this paper presents a model based approach to reinforcement learning. To the best of my knowledge, there are no other succesfful model based approaches that work on environments as complex as the ATARI. This is the main contribution of this paper, showing that model based approaches can work in complex environments and to present an alternative to current model-free approaches.", "title": "Answer"}, "rkRp7zxVg": {"type": "review", "replyto": "rJe-Pr9le", "review": "Hi,\n\nA couple of questions on experimental details:\n\n1. Fig. 3 shows a Layer Normalization step in the Prediction network, not listed in Table 1. Which one is correct?\n\n2. Could you please clarify the definition of rj2 in 4.4? It sounds like it is equal to 1-rj1, which would be redundant, and I am not sure I understand the concept of \"earning a point\" (is the game score taken into account anywhere?)\n\nThanks!This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a \"residual recurrent neural network\", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring.\n\nThis submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below.\n\nThe first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?)\n\nIn addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines.\n\nFinally, the paper's \"previous work\" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance \"Action-Conditional Video Prediction using Deep Networks in Atari Games\" should have been an obvious \"must cite\".\n\nMinor comments:\n- Notations are unusual, with \"a\" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations\n- Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product\n- The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward\n- c_i is defined as \"The control that was performed at time i\", but instead it seems to be the control performed at time i-1\n- There is a recurrent confusion between mean and median in 3.2.2\n- x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization\n- The inequality in Observation 1 should be about |x_i|, not x_i\n- Observation 1 (with its proof) takes too much space for such a simple result\n- In 3.2.3 the first r_j should be r_i\n- The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model\n- \"Our approach is not able to learn from good strategies\" => did you mean \"*only* from good strategies\"?\n- Please say that in Fig. 4 \"fc\" means \"fully connected\"\n- It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015)\n- Please clarify r_j2 as per your answer in OpenReview comments\n- Table 3 says \"After one iteration\" but has \"PRL Iteration 2\" in it, which is confusing\n- \"Figure 5 shows that not only there is no degradation in Pong and Demon Attack\"=> to me it seems to be a bit worse, actually\n- \"A model that has learned only from random play is able to play at least 7 times better.\" => not clear where this 7 comes from\n- \"Demon Attack's plot in Figure 5c shows a potential problem we mentioned earlier\" => where was it mentioned?\n", "title": "Experimental details", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyqxOUfVl": {"type": "review", "replyto": "rJe-Pr9le", "review": "Hi,\n\nA couple of questions on experimental details:\n\n1. Fig. 3 shows a Layer Normalization step in the Prediction network, not listed in Table 1. Which one is correct?\n\n2. Could you please clarify the definition of rj2 in 4.4? It sounds like it is equal to 1-rj1, which would be redundant, and I am not sure I understand the concept of \"earning a point\" (is the game score taken into account anywhere?)\n\nThanks!This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a \"residual recurrent neural network\", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring.\n\nThis submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below.\n\nThe first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?)\n\nIn addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines.\n\nFinally, the paper's \"previous work\" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance \"Action-Conditional Video Prediction using Deep Networks in Atari Games\" should have been an obvious \"must cite\".\n\nMinor comments:\n- Notations are unusual, with \"a\" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations\n- Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product\n- The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward\n- c_i is defined as \"The control that was performed at time i\", but instead it seems to be the control performed at time i-1\n- There is a recurrent confusion between mean and median in 3.2.2\n- x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization\n- The inequality in Observation 1 should be about |x_i|, not x_i\n- Observation 1 (with its proof) takes too much space for such a simple result\n- In 3.2.3 the first r_j should be r_i\n- The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model\n- \"Our approach is not able to learn from good strategies\" => did you mean \"*only* from good strategies\"?\n- Please say that in Fig. 4 \"fc\" means \"fully connected\"\n- It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015)\n- Please clarify r_j2 as per your answer in OpenReview comments\n- Table 3 says \"After one iteration\" but has \"PRL Iteration 2\" in it, which is confusing\n- \"Figure 5 shows that not only there is no degradation in Pong and Demon Attack\"=> to me it seems to be a bit worse, actually\n- \"A model that has learned only from random play is able to play at least 7 times better.\" => not clear where this 7 comes from\n- \"Demon Attack's plot in Figure 5c shows a potential problem we mentioned earlier\" => where was it mentioned?\n", "title": "Experimental details", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H14efJ1Xx": {"type": "rebuttal", "replyto": "S186JY2Ml", "comment": "One thing that wasn't stated is that in Pong the we don't plan far ahead enough to \"see\" that we will earn a point, the agent only tries to survive. I will add this to the paper.\n\n1) We only consider the set of moves that have a survival probability higher than the \"confidence interval\" and from those the one with the highest probability of earning a point. For example, consider three sets of moves c1, c2 and c3, assume our model gives them a dying probability of 0.1, 0.01 and 0.02 respectively. If our confidence interval is 0.05, we will only consider moves c2 and c3 (we don't consider c1 because it has a probability of dying of 0.1). From those two, we pick the set of moves that have a highest probability of earning a point. If there is no set of moves to consider, we just pick the one with the lowest probability of dying (among all moves).\n\n\n2) Yes, with smaller horizons the model doesn't plan far enough into the future and can't play. In Demon Attack it takes ~20 time steps from the actions that kill you until the game engine tells you you have died. Looking at a further horizon doesn't help much in this games neither. In order to \"see\" that we would earn a point in Pong, we would need ~60 steps time horizon, which makes the whole training process go considerably slower. Also, as we sample sets of moves u.a.r., if we have larger planning horizon it becomes harder to find a good strategies.\n\n3) Yes, the probability of earning points is ignored in Pong and Breakout, surviving is a good enough strategy. For Demon Attack, it was more of a trial and error process. If the threshold is set too low initially, the agent learns to hide in a corner to avoid death, but consequently it doesn't earn any reward. Take into account that the agent initially dies very often, so it learns that it will die with high probability. If the threshold is set too high at the end (where it dies much less often), the agent just tries to earn rewards and dies too often.\n\nStill, I want to make clear that everything related to the strategy is not the focus of the paper. The focus is showing that this model based approach to reinforcement learning can work in complex environments like ATARI. The hand coded strategy is just a way to test the model and to show the approach works. In future papers I will address this problem and avoid hand coded strategies.", "title": "Answers:"}, "BJgp1tCCfx": {"type": "review", "replyto": "rJe-Pr9le", "review": "Could you please elaborate last paragraph of section 3.1. How does predicting the reward difference does not depend on the strategy being played?\n\nWhere are the final results? The only results data I could find was table three which only shows the results on iteration 2 and not after convergence.\n\nWhere are the comparisons with baseline, DQN and many other STOA results published since?The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy?\nIs r the discounted Return at time t, or the reward at time t?\nCould the author compare the method to TD learning?\nThe paper is vague and using many RL terms with different meanings without clarifying those diversions.\n\"So, the output for a given state-actions pair is always same\". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning?\nThe model description doesn't specify what is the policy, and it's only being mentioned in data generation part.\nWhy is it a model based approach?\nThe learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games.\n\nThe paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach. ", "title": "Where are the final results?", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJIwQv-Ve": {"type": "review", "replyto": "rJe-Pr9le", "review": "Could you please elaborate last paragraph of section 3.1. How does predicting the reward difference does not depend on the strategy being played?\n\nWhere are the final results? The only results data I could find was table three which only shows the results on iteration 2 and not after convergence.\n\nWhere are the comparisons with baseline, DQN and many other STOA results published since?The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy?\nIs r the discounted Return at time t, or the reward at time t?\nCould the author compare the method to TD learning?\nThe paper is vague and using many RL terms with different meanings without clarifying those diversions.\n\"So, the output for a given state-actions pair is always same\". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning?\nThe model description doesn't specify what is the policy, and it's only being mentioned in data generation part.\nWhy is it a model based approach?\nThe learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games.\n\nThe paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach. ", "title": "Where are the final results?", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S186JY2Ml": {"type": "review", "replyto": "rJe-Pr9le", "review": "A few quick questions:\n\n1) The action sequence selection procedure refers to selecting by the highest probability of earning a point or the lowest probability of dying. How exactly is this defined? Is it min/max over the sequence?\n2) Have you tried any values other than 25 for the planning horizon?\n3) The \"Confidence Interval\" part of the implementation details is a bit confusing. Are you ignoring the probability of scoring a point in Pong and Breakout and only using the probability of dying? How important is the sequence of thresholds for Demon Attack and how did you come up with it?This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish.", "title": "Pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkZFQVxVx": {"type": "review", "replyto": "rJe-Pr9le", "review": "A few quick questions:\n\n1) The action sequence selection procedure refers to selecting by the highest probability of earning a point or the lowest probability of dying. How exactly is this defined? Is it min/max over the sequence?\n2) Have you tried any values other than 25 for the planning horizon?\n3) The \"Confidence Interval\" part of the implementation details is a bit confusing. Are you ignoring the probability of scoring a point in Pong and Breakout and only using the probability of dying? How important is the sequence of thresholds for Demon Attack and how did you come up with it?This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish.", "title": "Pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}