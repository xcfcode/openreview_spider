{"paper": {"title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models", "authors": ["Pouya Samangouei", "Maya Kabkab", "Rama Chellappa"], "authorids": ["pouya@umiacs.umd.edu", "mayak@umiacs.umd.edu", "rama@umiacs.umd.edu"], "summary": "Defense-GAN uses a Generative Adversarial Network to defend against white-box and black-box attacks in classification models.", "abstract": "In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "The paper studied defenses against adversarial examples by training a GAN and, at inference time, finding the GAN-generated sample that is nearest to the (adversarial) input example. Next, it classifies the generated example rather than the input example. This defense is interesting and novel. The CelebA experiments the authors added in their revision suggest that the defense can be effective on high-resolution RGB images."}, "review": {"By-CxBKgz": {"type": "review", "replyto": "BkJ3ibb0-", "review": "This paper presents Defense-GAN: a GAN that used at test time to map the input generate an image (G(z)) close (in MSE(G(z), x)) to the input image (x), by applying several steps of gradient descent of this MSE. The GAN is a WGAN trained on the train set (only to keep the generator). The goal of the whole approach is to be robust to adversarial examples, without having to change the (downstream task) classifier, only swapping in the G(z) for the x.\n\n+ The paper is easy to follow.\n+ It seems (but I am not an expert in adversarial examples) to cite the relevant litterature (that I know of) and compare to reasonably established attacks and defenses.\n+ Simple/directly applicable approach that seems to work experimentally, but\n- A missing baseline is to take the nearest neighbour of the (perturbed) x from the training set.\n- Only MNIST-sized images, and MNIST-like (60k train set, 10 labels) datasets: MNIST and F-MNIST.\n- Between 0.043sec and 0.825 sec to reconstruct an MNIST-sized image.\n? MagNet results were very often worse than no defense in Table 4, could you comment on that?\n- In white-box attacks, it seems to me like L steps of gradient descent on MSE(G(z), x) should be directly extended to L steps of (at least) FGSM-based attacks, at least as a control.", "title": "Interesting but hard to conclude decisively from the current experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BympCwwgf": {"type": "review", "replyto": "BkJ3ibb0-", "review": "This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs.  Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution).  This projected input is then used to produce the classification probabilities.  The authors test their method on various adversarially constructed inputs (with varying degrees of noise). \n\nQuestions/Comments:\n\n- I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method.  Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data?  If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable?  \n\n- Is there anything special about the GAN approach, versus other generative approaches? \n\n- In the black-box vs. white-box scenarios, can the attacker know the GAN parameters?  Is that what is meant by the \"defense network\" (in experiments bullet 2)?\n\n- How computationally expensive is this approach take compared to MagNet or other adversarial approaches? \n\nQuality: The method appears to be technically correct.\n\nClarity: This paper clearly written; both method and experiments are presented well. \n\nOriginality: I am not familiar enough with adversarial learning to assess the novelty of this approach. \n\nSignificance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold.  I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics).", "title": "review", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJOVWxjez": {"type": "review", "replyto": "BkJ3ibb0-", "review": "The authors describe a new defense mechanism against adversarial attacks on classifiers (e.g., FGSM). They propose utilizing Generative Adversarial Networks (GAN), which are usually used for training generative models for an unknown distribution, but have a natural adversarial interpretation. In particular, a GAN consists of a generator NN G which maps a random vector z to an example x, and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution. The GAN is trained to minimize the max min loss of D on this discrimination task, thereby producing a G (in the limit) whose outputs are indistinguishable from the true distribution by the best discriminator. \n\nUtilizing a trained GAN, the authors propose the following defense at inference time. Given a sample x (which has been adversarially perturbed), first project x onto the range of G by solving the minimization problem z* = argmin_z ||G(z) - x||_2. This is done by SGD. Then apply any classifier trained on the true distribution on the resulting x* = G(z*). \n\nIn the case of existing black-box attacks, the authors argue (convincingly) that the method is both flexible and empirically effective. In particular, the defense can be applied in conjunction with any classifier (including already hardened classifiers), and does not assume any specific attack model. Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM. \n\nThe authors provide less-convincing evidence that the defense is effective against white-box attacks. In particular, the method is shown to be robust against FGSM, RAND+FGSM, and CW white-box attacks. However, it is not clear to me that the method is invulnerable to novel white-box attacks. In particular, it seems that the attacker can design an x which projects onto some desired x* (using some other method entirely), which then fools the classifier downstream.\n\nNevertheless, the method is shown to be an effective tool for hardening any classifier against existing black-box attacks \n(which is arguably of great practical value). It is novel and should generate further research with respect to understanding its vulnerabilities more completely. \n\nMinor Comments:\nThe sentence starting \u201cUnless otherwise specified\u2026\u201d at the top of page 7 is confusing given the actual contents of Tables 1 and 2, which are clarified only by looking at Table 5 in the appendix. This should be fixed.  \n", "title": "A novel idea with room for future work. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkbvmBamf": {"type": "rebuttal", "replyto": "BkJ3ibb0-", "comment": "We have posted a revision with an additional Appendix (F) for new white-box experiments on the CelebA dataset, as well as minor changes to the text.", "title": "Revision"}, "Hy120kU7f": {"type": "rebuttal", "replyto": "SkdMUQaAZ", "comment": "We thank the anonymous commenter.\nWe have added some additional results on the CelebA dataset in Appendix F.\nRegarding the suggested new attack methods, we note that:\n1- We believe that this same exact point was raised by AnonReviewer3, and we kindly refer the commenter to part A of our reply to AnonReviewer3. \n2- It is not clear to us how to \u201coutput a wrong set of Z_L\u201d and how to find an input x that will meet this criterion. \n(If by \u201coutput a wrong set of Z_L\u201d the reviewer means to inject adversarial noise directly on the set of Z_L, then the attacker has gained access and infiltrated an intermediate step of the system and might as well directly modify the classifier output. This type of attacks was never considered in this literature).\n3- We believe that the commenter mistakenly assumes the seed to be an external input accessible to and modifiable by the attacker. Even though, in Figure 1, the seed is depicted as input to the system, it is never assumed that the attacker can modify the random seed. ", "title": "Answer to anonymous commenter"}, "Bkw8Ck8QG": {"type": "rebuttal", "replyto": "ryW5rcl-f", "comment": "We thank the anonymous commenter.\nWe have modified the title of Appendix B to reflect our claim that attacks based on gradient-descent are difficult to perform. \nRegarding the modified CW optimization attack, our understanding is that the commenter is suggesting the following:\n\nMinimize (over x*, z*)  CW loss(x, x*, G(z*)) + 0.1 ||G(z*) - x*||\n\nFirst of all, this problem is significantly more difficult to solve than the original CW formulation due to the dependence on x, x*, and G(z*). \nSecond, this formulation does not guarantee that when x* is input to the system, z* will be the output of the GD block, and an example \u201cclose\u201d to an adversarial example is not necessarily adversarial itself. \nLastly, the random initialization of z in the GD block serves to add robustness and change the output every time.\n\nAll in all, we are extremely interested in further investigating new attack strategies as Defense-GAN was shown to be robust to existing attack models. ", "title": "Answer to anonymous commenter"}, "r1MMCyImM": {"type": "rebuttal", "replyto": "S1c64RJzz", "comment": "We thank the anonymous commenter. Due to the recentness of the paper referred to by the commenter, we have not had the time to analyze it in detail. However, as noted in the paper (page 3), the attacks are actually generated using gradient descent as is the case in all attacks used in our paper. \nThe mechanism considered in APE-GAN and that considered in our paper are very different. While MagNet and APE-GAN use a feedforward architecture for their \u201creconstruction\u201d step, Defense-GAN employs an optimization based projection onto the range of the generator, which holds a good representation of the true data. ", "title": "Answer to anonymous commenter"}, "SyS0aJ8Xz": {"type": "rebuttal", "replyto": "B1wgPVOzG", "comment": "We thank the anonymous commenter. The paper referred to by the commenter deals with a synthetic spheres dataset which we believe is not applicable to the use of GANs. Our focus is on real-life datasets collected from real examples. Furthermore, due to the recentness of the paper, we have not had the time to analyze it in detail.", "title": "Answer to anonymous commenter"}, "r1D5pJ87f": {"type": "rebuttal", "replyto": "BympCwwgf", "comment": "We thank the reviewer for the insightful comments and discussions.\n\nA) Defense-GAN vs. MagNet vs. other generative approaches:\nWe believe that the MagNet auto-encoder suffers lower accuracy compared to Defense-GAN due to the fact that the \u201creconstruction\u201d step in MagNet is a feed-forward network as opposed to an optimization-based projection as in Defense-GAN. Overall, the combination of MagNet and the classifier can be seen as one deeper classification network, and has a wide attack surface compared to Defense-GAN.\nAs suggested by the reviewer, if the MagNet decoder (or another generative approach) was treated as a generative model, and the same optimization-based projection approach was followed, the model with more representative power would perform better. From our experience, GANs tend to have more representative power, but this is still an active area of research and discussion. We believe that, since GANs are specifically designed to optimize for generative tasks, using a GAN in conjunction with our proposed optimization-based projection would outperform an encoder with the same projection method. However, this would be an interesting future research direction. In addition, we were able to show some theoretical guarantees regarding the use and representative power of GANs in equation (7).\n\nB) Black- and white-box attacks:\nIn our work and previous literature, it is assumed that in black-box scenarios the attacker does not know the classifier network nor the defense mechanism (and any parameters thereof). The only information the attacker can use is the classifier output. \nIn white-box scenarios, the attacker knows the entire system including the classifier network, defense mechanisms, and all parameters (which in our case, include GAN parameters). By \u201cdefense network\u201d in Experiments bullet 2, we mean the generator network. \n\nC) Computational complexity:\nDefense-GAN adds inference-time complexity to the classifier. As discussed in Appendix G (Appendix F in the original version of the paper), this complexity depends on L, the number of GD steps used to reconstruct images, and (to a lesser extent) R, the number of random restarts. At training time, Defense-GAN requires training a GAN, but no retraining of the classifier is necessary.\nIn comparison, MagNet also adds inference-time complexity. However, the time overhead is much smaller than Defense-GAN as MagNet is simply a feedforward network. At training time, the overhead is similar to Defense-GAN (training the encoder, no retraining of the classifier).\nAdversarial training adds no inference-time complexity. However, training time can be significantly larger than for other methods since re-training the classifier is required (preceded by generating the adversarial examples to augment the training dataset).\n", "title": "Answer to AnonReviewer1"}, "Bkbgpk87z": {"type": "rebuttal", "replyto": "By-CxBKgz", "comment": "We appreciate the constructive criticism and detailed analysis of our paper.\n\nA) Nearest-neighbor baseline:\nTaking the nearest neighbor of the potentially perturbed x from the training set can be seen as a simple way of removing adversarial noise, and is tantamount to a 1-nearest-neighbor (1-NN) classifier. On MNIST, a 1-NN classifier achieves an 88.6% accuracy on FGSM adversarial examples with epsilon = 0.3, found using the B substitute network. Defense-GAN-Rec and Defense-GAN-Orig average about 92.5% across the four different classifier networks when the substitute model is fixed to B. Similar trends are found for other substitute models. There is an improvement of about 4% by using Defense-GAN. It is also worth noting that in the case of MNIST, a 1-NN classifier works reasonably well (achieving around 95% on clean images). This is not the case for more complex datasets: for example, if the problem at hand is face attributes classification, nearest neighbors may not necessarily belong to the same class, and therefore NN classifiers will perform poorly.\n\nB) Only MNIST-sized images:\nBased on the reviewer\u2019s suggestion, we have added additional white-box results on the Large-scale CelebFaces Attributes (CelebA) dataset in the appendix of the paper. The results show that Defense-GAN can still be used with more complex datasets including larger and RGB images. For further details, please refer to Appendix F in the revised version.\n\nC) Time to reconstruct images:\nWe agree with the reviewer that Defense-GAN introduces additional inference time by reconstructing images using GD on the MSE loss. However, we show its effectiveness against various attacks, especially in comparison to other simpler defenses. Furthermore, we have not optimized the running time of our algorithm, as it was not the focus of this work. This is a worthwhile effort to pursue in the future by trying to better utilize computational resources. \nPer the reviewer\u2019s comment, we have timed some reconstruction steps for CelebA images (which are 15.6 times larger than MNIST/F-MNIST). For R = 2, we have:\nL = 10, 0.132 sec\nL = 25, 0.106 sec\nL = 50, 0.210 sec\nL = 100, 0.413 sec\nL = 200, 0.824 sec\nThe reconstruction time for CelebA did not scale with the size of the image.\n\nD) MagNet results are sometimes worse than no defense in Table 4:\nEven though it seems counter-intuitive that a defense mechanism can sometimes cause a decrease in performance, this stems from the fact that white-box attackers also know the exact defense mechanism used. In the case of MagNet, the defense mechanism is another feedforward network which, in conjunction with the original classifier, can be viewed as a new deeper feedforward network. Attacks on this bigger network can sometimes be more successful than attacks on the original network. Furthermore, MagNet was not designed to be robust against white-box attacks.\n\nE) Using L steps of white-box FGSM:\nPer our understanding, the reviewer is suggesting using iterative FGSM. We do agree that for a fair comparison, L steps of iterative FGSM could be used. However, we note that CW is an iterative optimization-based attack, and is more powerful than iterative FGSM. Since we have shown robustness against CW attacks in Table 4, we believe iterative FGSM results will be similar.\n", "title": "Answer to AnonReviewer2"}, "S1bEhkU7G": {"type": "rebuttal", "replyto": "rJOVWxjez", "comment": "We thank the reviewer for the constructive review and comments.\n\nA) Regarding the effectiveness against white-box attacks:\nAs the reviewer has pointed out, we have shown the robustness of our method to existing white-box attacks such as FGSM, RAND+FGSM, and CW. Indeed, a good attack strategy could be to design an x which projects onto a desired x* = G(z*). However, this requires solving for:\n\nFind x s.t. the output of the gradient-descent block is z*. \n\nPer our understanding, the reviewer\u2019s suggestion is the following:\nFind a desired x* in the range of the generator which fools the classifier.\nFind an x which projects onto x*, i.e., such that the output of the GD block is z*, where G(z*) = x*. \nStep 1 is a more challenging version of existing attacks, due to the constraint that the adversarial example should lie in the range of the generator. While step 1 could potentially be solvable, the real difficulty lies in step 2. In fact, it is not obvious how to find such an x given x*. What comes to mind is attempting to solve step 2 using an optimization framework, e.g.:\nMinimize (over x, z*) 1\nSubject to G(z*) = x*\n                 z* is the output of the GD block after L steps.\n\nWe have shown in Appendix B that solving this problem using GD gets more and more prohibitive as L increases.\nFurthermore, since we use random initializations of z, if the random seed is not accessible by the attacker, there is no guarantee that a fixed x will result in the same fixed z every time after L steps of GD on the MSE. \nDue to these factors, we believe that our method is robust to a wide range of gradient-based white-box attacks. However, we are very much interested in further research of novel attack methods.\n\nB) We have fixed the minor comments by specifically mentioning the classifier and substitute models for every Table and Figure throughout the paper.\n", "title": "Answer to AnonReviewer3"}}}