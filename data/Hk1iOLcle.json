{"paper": {"title": "MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset", "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "authorids": ["trnguye@microsoft.com", "miriamr@microsoft.com", "xiaso@microsoft.com", "jfgao@microsoft.com", "satiwary@microsoft.com", "ranganm@microsoft.com", "deng@microsoft.com"], "summary": "A large scale human annotated data set for web-based reading comprehension along with baselines.", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "keywords": []}, "meta": {"decision": "Reject", "comment": "Though the dataset is likely to have a large impact on the community, there is a general consensus among the reviewers that the authors could have done a better job characterizing the dataset (lack discussion on answer types, careful comparison with previous datasets, human performance on the dataset). So sadly, though the dataset is potentially very important, the paper does not quite cut it.\n \n Positive:\n -- an important dataset\n -- a good job with establishing baselines\n \n Negative\n -- analysis and discussions are very limited"}, "review": {"HJiuMA5rl": {"type": "rebuttal", "replyto": "Hk1iOLcle", "comment": "While this dataset would undoubtedly benefit the community, the paper itself lacks sufficient details to to establish reproducible baselines, which is a key part of a dataset submission in my opinion.\n\nFirstly, the model descriptions are lacking:\n\n1. What exactly is the \"best passage\" model?\n2. What is \"a DSSM-alike passage ranking model\"?\n3. For each model, how does the model handle each case where there are multiple selected passages, a single selected passage, and zero selected passage?\n\nSecondly, the evaluation set on which the metrics are computed are not clearly defined.\n\n1. The authors refer to \"a subset of MS MARCO\" in all evaluation tables (5, 6, and 7). What examples do these subsets contain? Without this information non of the evaluations are reproducible.\n2. How does the author evaluate the case where there are no answers or there are several answers? It is ambiguous how one computes the BLEU or ROUGE scores in these cases.\n\nLastly, there are no human baselines on this dataset, making the upper bounds unclear (unlike some of the other datasets released).", "title": "More details needed for reproducibility"}, "HJeJJA9Bl": {"type": "rebuttal", "replyto": "SJFC0BBXx", "comment": "I'm still unsure about what the best passage baseline constitutes. I see from the dataset that each example consists of multiple passages, each of which is tagged with a \"is_selected\" field.\n\n1. How is the best passage baseline Rouge obtained in the event that only one passage is selected?\n2. How is the best passage baseline Rouge obtained in the event that no passage is selected?\n3. How is the best passage baseline Rouge obtained in the event that 2 passages are selected?\n\nMoreover, what exactly is a \"DSSM-alike passage ranking model\"? It would be nice if the authors provided system descriptions for each of the models implemented in Table 5.", "title": "Clarifications on baselines"}, "SJFC0BBXx": {"type": "rebuttal", "replyto": "rJA1XaJml", "comment": "Thanks for your interest in our MS MARCO our paper and for your question!\n\n1. Could you provide more details about the Best Passage baseline?\n\nDuring the judgment process, the judges are asked not only to write the best answer but also identify passages from which the answers have been synthesized or inferred. These passages identified by the judges constitute the best passage baseline. Hence, this baseline is the upper bound performance of the passage ranking methods.  \n\n2. Could you please report inter-human agreement for the collected answers?\n\nWhereas we do not have a precise number for human error, we have done several rounds of detailed data analysis amongst the authors and the rest of our team. Due to the nature of the dataset, there will be some variance in the exact answers generated by different human judges in terms of phrasing and presentation of the answers (and we observed the same during our analysis). We have seen most of the variance along this dimension. The judges have generally been consistent in the factual content of the answers. We also plan to significantly increase the number of questions with multiple answers in the next iteration of this dataset and to measure algorithms across these multiple answers to limit the variance in accuracy scores for the different algorithms. The metric pa-BLEU, that we propose to use as a metric in the paper, also aims to address the issues arising due to intra-human variance.", "title": "Best passage baseline details and inter-human agreement for answers"}, "r1kVcM47x": {"type": "rebuttal", "replyto": "SJn5xcRzl", "comment": "Thanks for your interest in our paper and for this question! I am sorry for the delay in our reply as I was flying to NIPS at the time and did not check email frequently, unfortunately. \n\nWhereas we have not collected and measured this exact information accurately yet, we agree this would be a useful addition to the paper. Categories 1, 2, and 3 mentioned in your question should be reasonably represented in the MS MARCO dataset with 4 (requires external knowledge) being close to zero. The latter is because of the way the human judgment task was set up. Upon your request, we can definitely measure this accurately and include this information in a revised version of the paper. Please advise.", "title": "Comprehension/inference statistics for MS MARCO is in progress"}, "Skq_wf4Qe": {"type": "rebuttal", "replyto": "SJVAhPXQl", "comment": "Thanks for these insightful questions and for taking time to review our paper!\n\nCan the authors please provide more details about the Cloze-style model experiments on their MS-MARCO dataset? How did they create the missing symbols and the context texts using the queries, passages and answers present in their dataset?\n\nTo start, we\u2019d like to cite the last paragraph of section 1 in the original paper:\n\n\u201cAs we summarized in the last paragraph of Section 1, compared to other publicly available datasets, including cloze datasets, MS MARCO is unique in that (a) all questions are real user queries, (b) the context passages, which answers are derived from, are extracted from real web documents, (c) all the answers to the questions are human generated, (d) a subset of these questions has multiple answers, (e) a subset of these questions has no answers.\u201d\n\nOn a high level, the MS MARCO set differs from commonly used cloze test datasets such as CNN/Daily Mail, etc. in a few ways. We performed a cloze-style model experiment by looking into each of them:\n\n1) In MS MARCO we have free style human-generated answers instead of a set of pre-determined answer candidates. As mentioned in the paper, in order to apply cloze-style models to our dataset, we filtered the dataset to only the numeric category, where the labeled answer is a single identifiable number that has appeared in the passages. This gives us a subset of MS MARCO which was shown as test data in the table.  Accordingly, for any query the candidate answer set is composed of all identified numbers in all the given passages. Instead of generating an answer, the model\u2019s goal is to find the most probable answer among the candidates. This makes the dataset applicable for discriminative models like a cloze test model.\n\n2) Our queries are mostly real user queries in natural language, which are not in the form of sentences with missing symbols. This is less of a problem as the models under test (AS Reader and ReasoNet) take the query as a sequence input of RNN where the missing symbols are indifferent like other vocabulary words. In these models, the concatenated final RNN states were taken as query presentation, regardless of whether the query has missing-symbol format or not.\n\n3) For a given query, we have multiple context passages instead of just one. In the experiments we ask the model to take the concatenated passages as the context input.\n\nAs for the second comment: what are the take away messages from those experiments?\n\nThese experimental results are designed to showcase characteristics of the MS MARCO dataset described in Section 3. In particular, we show that real user queries are of high diversity. In order to measure the effectiveness of RC models, different evaluation metrics need to be used for different categories of queries. MS MARCO provides a much more comprehensive benchmark than other (cloze) RC datasets. Additionally, the accuracy numbers seemed to suggest that the MS MARCO numeric segment is harder to solve than CNN test data, but also sensitive to model advancement, and is directionally consistent with CNN test data.", "title": "Cloze-style model experiment details for MS MARCO dataset and key takeaway messages"}, "SJVAhPXQl": {"type": "review", "replyto": "Hk1iOLcle", "review": "Can the authors please provide more details about the Cloze-style model experiments on their MS-MARCO dataset? How did they create the missing symbols and the context texts using the queries, passages and answers present in their dataset? What are the take away messages from those experiments?Paper Summary: \nThis paper presents a new large scale machine reading comprehension dataset called MS MARCO. It is different from existing datasets in that the questions are real user queries, the context passages are real web documents, and free form answers are generated by humans instead of spans in the context. The paper also includes some analysis of the dataset and performance of QA models on the dataset.\n\nPaper Strengths: \n-- The questions in the dataset are real queries from users instead of humans writing questions given some context.\n-- Context passages are extracted from real web documents which are used by search engines to find answers to the given query.\n-- Answers are generated by humans instead of being spans in context.\n-- It is large scale dataset, with an aim of 1 million queries. Current release includes 100,000 queries.\n\nPaper Weaknesses: \n-- The authors say, \"We have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text.\", but the statement is not backed up with any study.\n-- The paper doesn't clearly present what additional information can today's QA models learn from MS MARCO which they can't from existing datasets. \n-- The paper should talk about what challenges are involved in obtaining a good performance on this dataset.\n-- What are the human performances as compared to the models presented in the paper?\n-- In section 4.1, what are the train/test splits? The results are for the subset of MS MARCO where every query has multiple answers. How big is that subset?\n-- What is DSSM mentioned in row 2, Table 5?\n-- The authors should include in the paper how experiments in section 4.2 prove that MS MARCO is a better dataset.\n-- In Table 6, the performance of Memory Networks is already close to Best Passage. Does that mean there is not enough room for improvement there?\n-- The paper seems to be written in hurry, with partial analysis, evaluation and various mistakes in the text.\n\nPreliminary Evaluation: \nThe proposed dataset MS MARCO is unique from existing datasets as it is a good representative of the QA task encountered by search engines. I think it can be a very useful dataset for the community to benefit from. Given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset's worth. I think it can benefit a lot with a more comprehensive analysis of the dataset.", "title": "Cloze-style model experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJjnf5_Eg": {"type": "review", "replyto": "Hk1iOLcle", "review": "Can the authors please provide more details about the Cloze-style model experiments on their MS-MARCO dataset? How did they create the missing symbols and the context texts using the queries, passages and answers present in their dataset? What are the take away messages from those experiments?Paper Summary: \nThis paper presents a new large scale machine reading comprehension dataset called MS MARCO. It is different from existing datasets in that the questions are real user queries, the context passages are real web documents, and free form answers are generated by humans instead of spans in the context. The paper also includes some analysis of the dataset and performance of QA models on the dataset.\n\nPaper Strengths: \n-- The questions in the dataset are real queries from users instead of humans writing questions given some context.\n-- Context passages are extracted from real web documents which are used by search engines to find answers to the given query.\n-- Answers are generated by humans instead of being spans in context.\n-- It is large scale dataset, with an aim of 1 million queries. Current release includes 100,000 queries.\n\nPaper Weaknesses: \n-- The authors say, \"We have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text.\", but the statement is not backed up with any study.\n-- The paper doesn't clearly present what additional information can today's QA models learn from MS MARCO which they can't from existing datasets. \n-- The paper should talk about what challenges are involved in obtaining a good performance on this dataset.\n-- What are the human performances as compared to the models presented in the paper?\n-- In section 4.1, what are the train/test splits? The results are for the subset of MS MARCO where every query has multiple answers. How big is that subset?\n-- What is DSSM mentioned in row 2, Table 5?\n-- The authors should include in the paper how experiments in section 4.2 prove that MS MARCO is a better dataset.\n-- In Table 6, the performance of Memory Networks is already close to Best Passage. Does that mean there is not enough room for improvement there?\n-- The paper seems to be written in hurry, with partial analysis, evaluation and various mistakes in the text.\n\nPreliminary Evaluation: \nThe proposed dataset MS MARCO is unique from existing datasets as it is a good representative of the QA task encountered by search engines. I think it can be a very useful dataset for the community to benefit from. Given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset's worth. I think it can benefit a lot with a more comprehensive analysis of the dataset.", "title": "Cloze-style model experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJA1XaJml": {"type": "review", "replyto": "Hk1iOLcle", "review": "Dear Authors,\n\n1. Could you provide more details about the Best Passage baseline?\n\n2. Could you please report inter-human agreement for the collected answers?\n\nThanks.Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.\n\nStrengths:\n\n1. The paper provides useful insights about the limitations of the existing reading comprehension datasets \u2013 questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.\n\n2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets \u2013 questions are sampled from user queries, answers are generated by humans.\n\n3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.\n\nWeaknesses/Suggestions:\n\n1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.\n\n2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.\n\n3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.\n\n4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?\n\n5. Please include the description of the best passage baseline in the paper.\n  \n6. Fix opening quotes, i.e. \u201d -> \u201c (for instance, on page 5, \u201dwhat\u201d -> \u201cwhat\u201d).\n\nReview Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation.", "title": "Clarification Question and Inter-human Agreement", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1P7LJZNe": {"type": "review", "replyto": "Hk1iOLcle", "review": "Dear Authors,\n\n1. Could you provide more details about the Best Passage baseline?\n\n2. Could you please report inter-human agreement for the collected answers?\n\nThanks.Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.\n\nStrengths:\n\n1. The paper provides useful insights about the limitations of the existing reading comprehension datasets \u2013 questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.\n\n2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets \u2013 questions are sampled from user queries, answers are generated by humans.\n\n3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.\n\nWeaknesses/Suggestions:\n\n1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.\n\n2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.\n\n3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.\n\n4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?\n\n5. Please include the description of the best passage baseline in the paper.\n  \n6. Fix opening quotes, i.e. \u201d -> \u201c (for instance, on page 5, \u201dwhat\u201d -> \u201cwhat\u201d).\n\nReview Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation.", "title": "Clarification Question and Inter-human Agreement", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}