{"paper": {"title": "Exploring LOTS in Deep Neural Networks", "authors": ["Andras Rozsa", "Manuel Gunther", "Terrance E. Boult"], "authorids": ["andras.rozsa@yahoo.com", "siebenkopf@googlemail.com", "tboult@vast.uccs.edu"], "summary": "We introduce layerwise origin-target synthesis (LOTS) that can be used for visualizing internal representations of deep neural networks, and for adversarial example generation.", "abstract": "Deep neural networks have recently demonstrated excellent performance on various tasks. Despite recent advances, our understanding of these learning models is still incomplete, at least, as their unexpected vulnerability to imperceptibly small, non-random perturbations revealed. The existence of these so-called adversarial examples presents a serious problem of the application of vulnerable machine learning models. In this paper, we introduce the layerwise origin-target synthesis (LOTS) that can serve multiple purposes. First, we can use it as a visualization technique that gives us insights into the function of any intermediate feature layer by showing the notion of a particular input in deep neural networks. Second, our approach can be applied to assess the invariance of the learned features captured at any layer with respect to the class of the particular input. Finally, we can also utilize LOTS as a general way of producing a vast amount of diverse adversarial examples that can be used for training to further improve the robustness of machine learning models and their performance as well.", "keywords": ["Deep learning", "Computer vision"]}, "meta": {"decision": "Reject", "comment": "This paper studies the effects of modifying intermediate representations arising in deep convolutional networks, with the purpose of visualizing the role of specific neurons, and also to construct adversarial examples. The paper presents experiments on MNIST as well as faces. \n \n The reviewers agreed that, while this contribution presents an interesting framework, it lacks comparisons with existing methods, and the description of the method lacks sufficient rigor. In light of the discussions and the current state of the submission, the AC recommends rejection. \n \n Since the final scores of the reviewers might suggest otherwise, please let me explain my recommendation. \n \n The main contribution of this paper seems to be essentially a fast alternative to the method proposed in 'Adversarial Manipulation of Deep Representations', by Sabour et al, ICLR'16, although the lack of rigor and clarity in the presentation of section 3 makes this assessment uncertain. The most likely 'interpretation' of Eq (3) suggests that eta(x_o, x_t) = nabla_{x_o}( || f^(l)_w(x_t) - f^(l)_w(x_o) ||^2), which is simply one step of gradient descent of the method described in Sabour et al. One reviewer actually asked for clarification on this point on Dec. 26th, but the problem seems to be still present in the current manuscript. \n \n More generally, visualization and adversarial methods based on backpropagation of some form of distance measured in feature space towards the pixel space are not new; they can be traced back to Simoncelli & Portilla '99. \n Fast approximations based on simply stopping the gradient descent after one iteration do not constitute enough novelty. \n \n Another instance of lack of clarity that has also been pointed out in this discussion but apparently not addressed in the final version is the so-called PASS measure. It is not defined anywhere in the text, and the authors should not expect the reader to know its definition beforehand. \n \n Besides these issues, the paper does not contribute to the state-of-the-art of adversarial training nor feature visualization, mostly because its experiments are limited to mnist and face datasets. Since the main contribution of the paper is empirical, more emphasis should be made to present experiments on larger, more numerous datasets."}, "review": {"rJKDcb5dl": {"type": "rebuttal", "replyto": "HyCEBhwul", "comment": "We (the PCs) have reached out to the AC and asked the AC his thoughts regarding your comments to the AC\u2019s meta-review. \n\nFirst, the AC acknowledges that the authors did address the concerns raised by the reviewers relating to the comparisons with existing methods, and apologizes for not highlighting this important improvement. However, numerical experiments are reduced to small-scale experiments (MNIST and faces), and similar papers exploring adversarial training (Sabour et al. iclr\u201916, \u2018Aversarial Machine Learning at Scale\u2019, ICLR\u201917) contain significantly more thorough empirical evaluations. \n\nAlso, the AC still finds eq (3), which defines the model, not sufficiently clear. \\eta(x_o, x_t) is defined as a gradient of some scalar function, but the authors do not specify what function. The backpropagation operator maps a gradient with respect to the output of a  layer to a gradient with respect to the input of that layer, so in eq (3) the reader has to make the effort to interpret the incoming vector f_w(x_t) - f_w(x_o) as a gradient of something. Perhaps the AC did not parse it correctly and his interpretation is indeed incorrect, but in any case, the authors should make an effort to improve the rigor. \n\nFinally, concerning the PASS score discussion, while the AC agrees that it is qualitatively discussed in the paper, it is not defined precisely. It is up to interpretation whether it is sufficiently clear from the paper how to interpret the PASS scores in the experimental section. \n\nWe, the PCs, acknowledge that this paper was quite borderline and a tough call. However, the AC and all reviewers are fairly underwhelmed by the paper (a score of 6 is not a strong endorsement). Moreover, we find the small scale of the experiments to be a compelling reason for not accepting this work at this time. We thus maintain our decision.", "title": "Re: ICLR committee final decision"}, "HyCEBhwul": {"type": "rebuttal", "replyto": "r1492fLdx", "comment": "After the unpleasant decision, we would like to respond to the justification for the rejection as we believe that it is insufficient in quality and lacks factual precedent.\n\nThe first sentence falsely claims that our approach was designed with the 'purpose of visualizing the role of specific neurons'. Instead, LOTS can be used to visualize the captured feature representations of an input at any given layer, and that is a significant difference.\n\nThe first sentence of the second paragraph states that reviewers agreed that our paper 'lacks comparisons with existing methods, and the description of the method lacks sufficient rigor'. Since there was righteous criticism about not comparing LOTS with other adversarial example generation methods, we conducted experiments and revised our paper accordingly. We compared LOTS with 3 other techniques (FGS, FGV, HC) - on LeNet/MNIST, we quantitatively and qualitatively compared these adversarial example types generated with those methods, and we did adversarial training with all types to analyze their effects on the overall performance and robustness to these types of adversarial examples. We found that using LOTS samples for adversarial training outperforms the utilization of other types - both with respect to classification performance and adversarial robustness. After our major revision addressing reviewers comments and suggestions, two of the three reviewers adjusted their ratings; one from 4 to 6, and the other from 5 to 6 - the third reviewer has not commented on our changes. Based on these feedbacks, we can confidently say that our revision significantly improved the paper. With respect to lacking 'sufficient rigor', we simply do not know what the AC refers to.\n\nWe agree and proudly acknowledge that LOTS is a fast alternative of the approach introduced by Sabour et al. ('Adversarial Manipulation of Deep Representations', ICLR 2016). Similarly, one can say that FGS (Goodfellow et al., 'Explaining and Harnessing Adversarial Examples', ICLR 2015) is a fast alternative of the adversarial generation technique proposed by Szegedy et al. ('Intriguing Properties of Neural Networks', ICLR 2014). Fast alternatives can make the difference between a practical and a prohibitively expensive method, therefore, they are very important. The AC's interpretation of Equation 3 is completely incorrect and we don't know where it is coming from. Indeed, initially we had a mathematically imprecise formula, however, we fixed that after an author of another paper posted a question with respect to our approach. After our response and revision, we did not receive any further questions/comments from anyone with respect to our Equations.\n\nThe AC also complains about 'lack of clarity that has also been pointed out in this discussion but apparently not addressed in the final version is the so-called PASS measure'. Indeed, two reviewers asked for a basic explanation 'in a sentence or two' about PASS. We revised our paper accordingly and following that, reviewers improved their ratings as described above. Based on that, it is safe to assume that we added what reviewers were missing.", "title": "Re: ICLR committee final decision"}, "ryBkZx1vl": {"type": "rebuttal", "replyto": "Sk3LwCRLl", "comment": "Thanks for pointing that out!\n\nI would not have considered using the difference of the softmax probabilities of the origin-target pair - simply because putting and backpropagating the difference not summing up to 1 seemed to be \"weird\".\n\nI just tried and the resulting adversarial example (SOFTMAX below) is indeed slightly different than the one obtained by manipulating the logits (IP2). As you can see below, they are very-very close considering the collected metrics (and looking at those images, the difference remains imperceptible to me), therefore, with respect to adversarial training I would not expect any further benefits by using these samples among with those generated on the IP2-layer (logits).  \n\nThe metrics for the adversarial examples obtained on all five layers (for the same origin and target as shown in Figure 3):\nCONV1 (1): 9\n  PASS: 0.9514 L-2: 283  L-inf: 59  \nCONV2 (2): 9\n  PASS: 0.9301 L-2: 289  L-inf: 56  \nIP1 (3): 9\n  PASS: 0.8942 L-2: 173  L-inf: 40  \nIP2 (4): 9\n  PASS: 0.8750 L-2: 158  L-inf: 31  \nSOFTMAX (5): 9\n  PASS: 0.8799 L-2: 149  L-inf: 33\n", "title": "Re: Softmax is different than logit"}, "H1j3JkASg": {"type": "rebuttal", "replyto": "SkCILwqex", "comment": "Our revised paper reflects to comments and suggestions for improvement.\n\n(1) We have added quantitative results of large-scale experiments comparing our LOTS approach to other adversarial example generation techniques such as the fast gradient sign (FGS) method, fast gradient value (FGV) method, and hot/cold (HC) approach. We have analyzed these techniques with respect to the quality of the produced images, and have also evaluated the effect of those samples when they are used for adversarial training.\n(2) We have clarified the motivation and advantage of using PASS over L-2 and L-inf norms with respect to measuring adversarial quality.\n(3) We have provided details about the internal structure of the tested network architectures, e.g., by denoting the position of layers in figures.\n(4) We have reviewed and simplified the whole paper to enhance readability and, last but not least, to allow us presenting the new results.\n\nThank you for all your comments and suggestions!", "title": "Revision addressing comments and suggestions"}, "rJRj9ixHl": {"type": "rebuttal", "replyto": "BJJ28myrg", "comment": "Alex,\n\nThose equations are mathematically imprecise.\nWe use Caffe\u2019s regular backpropagation: it computes gradient of f_{w}^{l}(x) over w_l (weights between layer l and layer below) evaluated at x_o, which is then propagated back to the image level layer-by-layer.\nThanks for pointing that out! We will revise them.", "title": "Re: Question about eq 3 and 4"}, "rJ43NilSx": {"type": "rebuttal", "replyto": "BkAeFpgEg", "comment": "Thank you for your review!\nWe plan to revise our paper and add experimental results, i.e., comparing LOTS to other approaches. Please allow us a few days!", "title": "Re: Exciting new method to generate adversarial examples and study robustness, less interesting analyses"}, "BkyFVogBe": {"type": "rebuttal", "replyto": "BJpi7ICQx", "comment": "We agree that it is not straightforward/intuitive to use the positive sign (to magnify the captured features) in order to cause misclassifications, but it is still useful to assess the classification robustness around the captured features. Indeed, for the MNIST example (Figure 1.) applying the positive sign does not change the classification label and even for faces (Figure 2.) the generated perturbations are very strong and visible, we believe that in some cases the direction provided by using the positive sign can also produce good adversarial examples - depending on the dataset and the trained classifier, of course. Since we apply gradient descent (or gradient ascent, depending on the sign) on the captured features and not on the loss, both directions (positive/negative signs) can increase the loss and result in misclassifications.\n\nWe introduced PASS score (Rozsa et al., \u201cAdversarial Diversity and Hard Positive Generation\u201d, CVPR DeepVision Workshop, 2016) simply because L-2 and L-inf norms are not applicable to measure adversarial quality in terms of human perception. While these norms consider only the perturbation/noise regardless of the spatial location of it on the original image, PASS score measures (structural) similarity of original and adversarial image pairs. In other words, PASS better quantifies the structural damage caused by the perturbation on the original image and therefore it can better measure adversarial. We revise the paper accordingly and try to better explain the advantage of PASS over L2 and L-inf norms.\n\nThe quoted sentence (\u201cIn general, LOTS cannot produce high quality adversarial examples at the lower layers\u201d) is in section 5.2 ADVERSARIAL EXAMPLES OF FACES, therefore it refers to our findings on adversarial face images.  We will clarify\n\nWe have been working on experiments to provide quantitative results (i.e., fine-tuning/training with adversarial examples generated via LOTS and compare the results with previous work). We plan to revise our paper in a few days.\nThank you for your suggestions!", "title": "Re: Interesting but somewhat incomplete analysis"}, "SktmNieSe": {"type": "rebuttal", "replyto": "HyZh5Vgrg", "comment": "Thank you for your valuable suggestions for improvement!\n\nWe believe that operating directly on softmax to produce adversarial examples, i.e., taking the difference of the captured probabilities, would not be different than using the logits as those probabilities are calculated directly from the logits. (Also, considering that those probabilities add up to one, taking the difference would violate that.)\n\nWe will upload a revised paper in a few days. We try to address all suggestions/comments coming from reviewers including using them for improved learning. ", "title": "Re: Great start; recommended as workshop paper."}, "BJJ28myrg": {"type": "rebuttal", "replyto": "SkCILwqex", "comment": "Hi,\n\nI have a question about eq. 3 and 4.\n\nAs far as I understood from the beginning of section 3, f_{w}^{l} (x) is essentially activations at layer l, which means that value of f_{w}^{l} (x) is in R^{d_l} space where d_l is dimensionality of output of layer l.\nIn eq. (3) and (4) you \\eta compute gradient of f_{w}^{l} (x) over input x, which I would expect to be Jacobian matrix with size d_l*n (where n - dimensionality of input x). So looking at eq (3) and (4) I would expect that \\eta and x has different dimensionality. At the same time in section 4 you add s*\\eta to x.\nCould you explain this discrepancy in dimensionality and how \\eta should be calculated?\nMaybe you meant that numerator of eq (3) and (4) contain norm of f_{w}^{l} instead of it's value?\n\nThanks,\nAlex", "title": "Question about eq 3 and 4"}, "SJnI2YjQe": {"type": "rebuttal", "replyto": "HkW_oXo7g", "comment": "It was a bug in the code visualizing the perturbations for given original and adversarial image pairs.\nThank you very much for pointing out the problem! It has been fixed.", "title": "Re: Plotting or experimental bug?"}, "HkW_oXo7g": {"type": "review", "replyto": "SkCILwqex", "review": "In several panes of Figure 1 (right of b, c, and perhaps others), pixel values seem to wrap around from dark to light. Is this a bug? If so, is the bug only in the plot (in which case the plot should be fixed), or was it a bug in the actual perturbed image values as well (in which case the experiment may need to be re-run)?This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input\u2019s representation and difference vectors between representations of two inputs, where the inputs are of different classes.\n\nIn the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it\u2019s an interesting computation to perform, the value of the visualizations is not very clear.\n\nIn the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).\n\nThe paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper.\n\nA few suggestions for improvement:\n - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn\u2019t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.\n - How many layers are in the networks used in the paper, and what is their internal structure? This isn\u2019t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.\n - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)\n - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.\n - 4.2 states \u201cIn summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.\u201d I don\u2019t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.\n\n\n\n1/19/17 UPDATE AFTER REBUTTAL:\nGiven that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold.", "title": "Plotting or experimental bug?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyZh5Vgrg": {"type": "review", "replyto": "SkCILwqex", "review": "In several panes of Figure 1 (right of b, c, and perhaps others), pixel values seem to wrap around from dark to light. Is this a bug? If so, is the bug only in the plot (in which case the plot should be fixed), or was it a bug in the actual perturbed image values as well (in which case the experiment may need to be re-run)?This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input\u2019s representation and difference vectors between representations of two inputs, where the inputs are of different classes.\n\nIn the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it\u2019s an interesting computation to perform, the value of the visualizations is not very clear.\n\nIn the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).\n\nThe paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper.\n\nA few suggestions for improvement:\n - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn\u2019t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.\n - How many layers are in the networks used in the paper, and what is their internal structure? This isn\u2019t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.\n - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)\n - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.\n - 4.2 states \u201cIn summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.\u201d I don\u2019t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.\n\n\n\n1/19/17 UPDATE AFTER REBUTTAL:\nGiven that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold.", "title": "Plotting or experimental bug?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1CEwyxmg": {"type": "rebuttal", "replyto": "BkdtMHymg", "comment": "Looking at the available Deep Dream code (https://github.com/google/deepdream), we can see that a given source image is modified by a perturbation to get \u201ccloser\u201d to the captured internal representation of the destination image, jittering is used to improve the quality, and different scales are used to form those amazing psychedelic images.  For visualization purposes, LOTS can either amplify or reduce the internal representation of the given image - no jittering or various scales are applied. Deep Dream uses Eq. 4. on the destination image to determine the \u201cdirection\u201d (with the plus sign), then normalizes and applies the scaled perturbation to the source image. \nAs Deep Dream was initially invented to help scientists and engineers to see what a deep neural network is seeing at various layers, we assume that it was similar to LOTS visualizing captured internal representations via Eq. 4. with the plus sign. However, it is not detailed in the blog (https://research.googleblog.com/2015/07/deepdream-code-example-for-visualizing.html), therefore, we cannot directly compare. Due to the fact that Deep Dream project is published \"only on a blog\" and is not a reviewed paper (and cannot be found on arXiv either), we decided not to cite it  - although it is definitely closely related to our LOTS approach.\n\nFor visualization, step-sizes can be chosen arbitrarily - depending on how much details we would like to see. To find perturbations with small(est) magnitudes that lead to different classifications, we first normalized the perturbation calculated via Eg. 4. in order to have high granularity, then applied line-search with increasing multipliers (1,2,4,8, etc.) to quickly find the magnitude causing an altered classification. Finally, we used a binary search in the last section to find the smallest multiplier (e.g., assuming that multiplying the perturbation by 1024 leads to a different label than the original image has, binary search will find the smallest multiplier in (512,1024] range). Overall, it is a line-search optimized for speed.", "title": "RE: small questions"}, "SyBvU1e7e": {"type": "rebuttal", "replyto": "B1WRy2kmx", "comment": "We have not compared LOTS to other methods with respect to perturbation sizes for several reasons.\n(1) Perturbation sizes do not translate well to adversarial quality (Sabour et al., \u201cAdversarial Manipulation of Deep Representations\u201d, ICLR, 2016; Rozsa et al., \u201cAdversarial Diversity and Hard Positive Generation\u201d, CVPR DeepVision Workshop, 2016).\n(2) What would be the standard method to compare to?\nTo date, based on our knowledge there is only one approach which works similarly to LOTS with original and target image pairs (Sabour et al., \u201cAdversarial Manipulation of Deep Representations\u201d, ICLR, 2016). However, their technique goes further than simply causing misclassifications - they use the computationally expensive L-BFGS optimization technique to try \u201cmatching\u201d internal representations of image pairs at the given layer.\n(3) LOTS is not optimized for the quality of adversarial examples. We mentioned in the paper that the quality of the produced adversarial examples could be further improved by using a more expensive \u201dstep-and-adjust\u201d method instead of the line-search. We consider adversarial examples as useful samples to improve the robustness of deep neural networks. Since it was demonstrated that larger magnitudes than the sufficient minimal distortions that cause misclassifications have more benefits (Rozsa et al., \u201cAdversarial Diversity and Hard Positive Generation\u201d, CVPR DeepVision Workshop, 2016), we are primarily interested in finding computationally cheap ways to produce a large number of diverse adversarial examples.", "title": "Re: Quantitative comparison of perturbation sizes with other methods"}, "B1WRy2kmx": {"type": "review", "replyto": "SkCILwqex", "review": "Have you compared the minimal necessary perturbation on various layers with some target image of a class using LOTS to the minimal necessary perturbation to change the classification using the previous standard method?The paper presents a new exciting layerwise origin-target synthesis method both for generating a large number of diverse adversarials as well as for understanding the robustness of various layers. The methodology is then used to visualize the amount of perturbation necessary for producing a change for higher level features.\n\nThe approach to match the features of another unrelated image is interesting and it goes beyond producing adversarials for classification. It can also generate adversarials for face-recognition and other models where the result is matched with some instance from a database.\n\nPro: The presented approach is definitely sound, interesting and original. \nCon: The analyses presented in this paper are relatively shallow and don't touch the most obvious questions. There is not much experimental quantitative evidence for the efficacy of this method compared with other approaches to produce adversarials. The visualization is not very exciting and it is hard to any draw any meaningful conclusions from them.\n\nIt would definitely improve the paper if it would present some interesting conclusions based on the new ideas.\n\n\n\n\n\n\n", "title": "Quantitative comparison of perturbation sizes with other methods", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkAeFpgEg": {"type": "review", "replyto": "SkCILwqex", "review": "Have you compared the minimal necessary perturbation on various layers with some target image of a class using LOTS to the minimal necessary perturbation to change the classification using the previous standard method?The paper presents a new exciting layerwise origin-target synthesis method both for generating a large number of diverse adversarials as well as for understanding the robustness of various layers. The methodology is then used to visualize the amount of perturbation necessary for producing a change for higher level features.\n\nThe approach to match the features of another unrelated image is interesting and it goes beyond producing adversarials for classification. It can also generate adversarials for face-recognition and other models where the result is matched with some instance from a database.\n\nPro: The presented approach is definitely sound, interesting and original. \nCon: The analyses presented in this paper are relatively shallow and don't touch the most obvious questions. There is not much experimental quantitative evidence for the efficacy of this method compared with other approaches to produce adversarials. The visualization is not very exciting and it is hard to any draw any meaningful conclusions from them.\n\nIt would definitely improve the paper if it would present some interesting conclusions based on the new ideas.\n\n\n\n\n\n\n", "title": "Quantitative comparison of perturbation sizes with other methods", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkdtMHymg": {"type": "review", "replyto": "SkCILwqex", "review": "* eq 4: how different is this from deepdream?\n* it's not entirely obvious how the optimization procedure for selecting the step-sizes s is actually done, would be nice to clarify (it's implicit in that the authors choose deformations that are smallest as possible, but there are many ways to do that).\nThis paper presents a relatively novel way to visualize the features / hidden units of a neural network and generate adversarial examples. The idea is to do gradient descent in the pixel space, from a given hidden unit in any layer. This can either be done by choosing a pair of images and using the difference in activations of the unit as the thing to do gradient descent over or just the activation itself of the unit for a given image. In general this method seems intriguing, here are some comments:\n\nIt\u2019s not clear that some of the statements at the beginning of Sec 4.1 are actually true, re: positive/negative signs and how that changes (or does not change) the class. Mathematically, I don\u2019t see why that would be the case? Moreover the contradictory evidence from MNIST vs. faces supports my intuition.\n\nThe authors use the PASS score through the paper, but only given an intuition + citation for it. I think it\u2019s worth explaining what it actually does, in a sentence or two.\n\nThe PASS score seems to have some, but not complete, correlation with L_2, L_\\{infty} or visual estimation of how \u201cgood\u201d the adversarial examples are. I am not sure what the take-home message from all these numbers is.\n\n\u201cIn general, LOTS cannot produce high quality adversarial examples at the lower layers\u201d (sec 5.2) seems false for MNIST, no?\n\nI would have liked this work to include more quantitative results (e.g., extract adversarial examples at different layers, add them to the training set, train networks, compare on test set), in addition to the visualizations present. That to me is the main drawback of the paper, in addition to basically no comparisons with other methods (it\u2019s hard to judge the merits of this work in vacuum).\n\n-----\n\nEDIT after rebuttal: thanks to the authors for addressing the experimental validation concerns. I think this makes the paper more interesting, so revising my score accordingly. ", "title": "small questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJpi7ICQx": {"type": "review", "replyto": "SkCILwqex", "review": "* eq 4: how different is this from deepdream?\n* it's not entirely obvious how the optimization procedure for selecting the step-sizes s is actually done, would be nice to clarify (it's implicit in that the authors choose deformations that are smallest as possible, but there are many ways to do that).\nThis paper presents a relatively novel way to visualize the features / hidden units of a neural network and generate adversarial examples. The idea is to do gradient descent in the pixel space, from a given hidden unit in any layer. This can either be done by choosing a pair of images and using the difference in activations of the unit as the thing to do gradient descent over or just the activation itself of the unit for a given image. In general this method seems intriguing, here are some comments:\n\nIt\u2019s not clear that some of the statements at the beginning of Sec 4.1 are actually true, re: positive/negative signs and how that changes (or does not change) the class. Mathematically, I don\u2019t see why that would be the case? Moreover the contradictory evidence from MNIST vs. faces supports my intuition.\n\nThe authors use the PASS score through the paper, but only given an intuition + citation for it. I think it\u2019s worth explaining what it actually does, in a sentence or two.\n\nThe PASS score seems to have some, but not complete, correlation with L_2, L_\\{infty} or visual estimation of how \u201cgood\u201d the adversarial examples are. I am not sure what the take-home message from all these numbers is.\n\n\u201cIn general, LOTS cannot produce high quality adversarial examples at the lower layers\u201d (sec 5.2) seems false for MNIST, no?\n\nI would have liked this work to include more quantitative results (e.g., extract adversarial examples at different layers, add them to the training set, train networks, compare on test set), in addition to the visualizations present. That to me is the main drawback of the paper, in addition to basically no comparisons with other methods (it\u2019s hard to judge the merits of this work in vacuum).\n\n-----\n\nEDIT after rebuttal: thanks to the authors for addressing the experimental validation concerns. I think this makes the paper more interesting, so revising my score accordingly. ", "title": "small questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}