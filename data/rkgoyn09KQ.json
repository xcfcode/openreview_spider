{"paper": {"title": "textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR", "authors": ["Pankaj Gupta", "Yatin Chaudhary", "Florian Buettner", "Hinrich Schuetze"], "authorids": ["pankaj_gupta96@yahoo.com", "yatinchaudhary91@gmail.com", "fbuettner.phys@gmail.com", "hinrich@hotmail.com"], "summary": "Unified neural model of topic and language modeling to introduce language structure  in topic models for contextualized topic vectors ", "abstract": "We address two challenges of probabilistic topic modelling in order to better estimate\nthe probability of a word in a given context, i.e., P(wordjcontext) : (1) No\nLanguage Structure in Context: Probabilistic topic models ignore word order by\nsummarizing a given context as a \u201cbag-of-word\u201d and consequently the semantics\nof words in the context is lost. In this work, we incorporate language structure\nby combining a neural autoregressive topic model (TM) with a LSTM based language\nmodel (LSTM-LM) in a single probabilistic framework. The LSTM-LM\nlearns a vector-space representation of each word by accounting for word order\nin local collocation patterns, while the TM simultaneously learns a latent representation\nfrom the entire document. In addition, the LSTM-LM models complex\ncharacteristics of language (e.g., syntax and semantics), while the TM discovers\nthe underlying thematic structure in a collection of documents. We unite two complementary\nparadigms of learning the meaning of word occurrences by combining\na topic model and a language model in a unified probabilistic framework, named\nas ctx-DocNADE. (2) Limited Context and/or Smaller training corpus of documents:\nIn settings with a small number of word occurrences (i.e., lack of context)\nin short text or data sparsity in a corpus of few documents, the application of TMs\nis challenging. We address this challenge by incorporating external knowledge\ninto neural autoregressive topic models via a language modelling approach: we\nuse word embeddings as input of a LSTM-LM with the aim to improve the wordtopic\nmapping on a smaller and/or short-text corpus. The proposed DocNADE\nextension is named as ctx-DocNADEe.\n\nWe present novel neural autoregressive topic model variants coupled with neural\nlanguage models and embeddings priors that consistently outperform state-of-theart\ngenerative topic models in terms of generalization (perplexity), interpretability\n(topic coherence) and applicability (retrieval and classification) over 6 long-text\nand 8 short-text datasets from diverse domains.", "keywords": ["neural topic model", "natural language processing", "text representation", "language modeling", "information retrieval", "deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents an extension of an existing topic model, DocNADE. Compared to DocNADE and other existing bag-of-word topic models, the primary contribution of this work is to integrate neural language models into the topic model in order to address two limitations of the bag-of-word topic models: expressiveness and interpretability. In addtion, the paper presents an approach to integrate external knowledge into the neural topic models to address the empirical challenges of the application scenarios where there might be only a small training corpus or limited context available. \n\nPros: \nThe paper presents strong and extensive empirical results. The authors went above and beyond to strengthen their paper during the rebuttal and address all the reviewers' questions and suggestions (e.g., the submitted version had 7 baselines, and the revised version has 6 additional baselines per reviewers' requests).\n\nCons:\nThe paper builds on an earlier paper that introduced the DocNADE model. Thus, the modeling contribution is relatively marginal. On the other hand, the extended model, albeit based on a relatively simple idea, is still new and demonstrates strong empirical results.\n\nVerdict:\nProbably accept. While not groundbreaking, the proposed model is new and the empirical results are strong. "}, "review": {"HylflGiASV": {"type": "rebuttal", "replyto": "rkgoyn09KQ", "comment": "Camera Ready Uploaded !!", "title": "Camera Ready Uploaded"}, "S1geWBgPC7": {"type": "rebuttal", "replyto": "H1xTIhGKn7", "comment": "We are very happy for your comment about our revised version: \"Wow guys, what a great revision\". \n\nWe appreciate and thank you for raising the overall rating. \n\n", "title": "Thanks for revising rating to 'clear accept'"}, "H1xTIhGKn7": {"type": "review", "replyto": "rkgoyn09KQ", "review": "DocNADE has great performance so this is a welcome bit of\nresearch extending it.\n\nThere has been a huge amount of activity in combining topic models with\n(1) embeddings and (2) neural networks such as LSTMs and RNNs.\nI will say I have great sympathy for the poor author trying to do\nfair comparisons against start-of-the-art because the standards are\nmoving quickly.\n\nIn this case, some neural network papers I have seen are TopicRNNs by\nDieng, Wang Gao and Paisley, and LLA by Zaheer, Ahmed and Smola.  The\nlatter is still a bag-of-words model and but places the LSTM over the\nsequence of topic proportions.  The Gauss-LDA and glove-DMM work is\nfairly dated (in our fast-paced ML world) and their performance is\nknown to be poor, as some papers in 2017 show.  Now I know historically LDA has been fairly poor\nwith IR tasks, but I would expect the recent supervised LDA methods,\nsome also have word embeddings, to do better as well.\nSo the discussion of related work and comparative experiments\nare poor.\n\nIf you want to illustrated good improvememts gained using embeddings,\nit helps to try different proportions, say 20/40/60/80% of a data\nset and plot.  Usually, you should see embeddings aid performance\ndramatically for smaller fractions of data sets.  Hence, your results\nseem strange.\n\nNote the data sets are all fairly small, which makes me wonder about\nthe computation time.  Could you give some computational performance\nstats for a data set?\n\nIn section 2.2 top of page 5, why is it \"pseudo\" log likelihood?\nIsn't that formula exact?\n\nThe paper has a relatively small part devoted to the model, and\nvirtually nothing on the algorithm, although this is probably covered\nin earlier DocNADE papers.   I'm assuming the model is\ntrained by SGD on the log likelihood with all the parameters\nshoved in there in one go.  Is that right?  Would be nice to mention\nwhatever it is.\n\nThe use of four different kinds of evaluations (classification, IR,\nperplexity, etc.) is good.  Note that the improvement over the earlier\nDocNADE is quite small but clearly significant, and improvement of adding\nembeddings seems even smaller, though seems better for short texts.\nI wonder if the method for including embeddings is much good!\nNot fully convinced.\n\nAFTER RESPONSE:   Wow guys, what a great revision.  Thanks so much.", "title": "related work not great, but experiements extensive otherwise", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkxOZ4TU0m": {"type": "rebuttal", "replyto": "H1l3AiAWCQ", "comment": "Please find the response below in continuation to the above one:\n\nAbout \"computational performance\":\n-----------------------------------------------------\nIn terms of training time for 1 epoch, the ctx-DocNADE takes 4.40 and 109.12 seconds for 20NSshort (short-text documents) and 20NSsmall (long-text documents, but a corpus of few documents) datasets, respectively when run on a CPU machine and the number of threads is set to 1.  The batch size set to 100. \n\nAbout \"recent supervised LDA methods\":\n---------------------------------------------------------\nIn our work, we focus on unsupervised techniques in modeling text documents. \n\nAbout \"comparative experiments\":\n-------------------------------------------------\nWe have now included in total 13 baselines (including 6 additional) in the revised version. Based on your feedback, we have also covered recent baselines, such as Topic-RNN, TCNLM, TDLM, etc. and compared with our proposed models for topic coherence and information retrieval (IR) tasks, where we have shown that our methods are competitive for topic coherence and outperform the related studies for IR by a large margin for 8 short-text datasets.  \n\n**Please see the revised version for the improved related works and additional experiments.**    \n\n", "title": "(continued) Response to additional queries (*please see revised version for improved related works*)"}, "S1en6cDSRQ": {"type": "rebuttal", "replyto": "Bkl-lUgBRQ", "comment": "We appreciate that you acknowledged our additional comparisons in the revised version. \n\nWe are glad to see the improved overall score and an 'accept'. Thank you for revising your scores.", "title": "Thanks for raising overall score and positive comment: \"very comprehensive comparison\""}, "Hkg2dz7Kh7": {"type": "review", "replyto": "rkgoyn09KQ", "review": "\nCons: \nThe proposed method is not novel. For example, Lauly et al., 2017 have proposed a similar way of combining LM and DocNADE. This paper does not provide some motivations or theories behind such artificial combination (i.e., just linearly combine their hidden state) to explain why it works better than other alternatives (e.g., what about adding some linear layers before combining h_i^{DN} and h_i^{LM}).\n\nPros: \nHowever, the results seem to be solid and significantly better than the previous state-of-the-art methods. I think some recent neural topic models such as [1,2,3] are still missing even though there are already many tables in the paper (I am not an expert on neural topic modeling or embedding for IR tasks, so there might be others missing state of the arts which I am not aware of). In addition, why does Table 5 only compares perplexity between 3 methods and Table 6 only compares coherence between 4 or 5 methods, while there are 9 or 12 methods are compared in IR task (Table 3 and 4). What's the difficulty of comparing the coherence and perplexity of all different topic models (including [1,2,3])?\nI will vote for acceptance if the mentioned baselines are also compared or there are good reasons why they cannot be compared.\n\n\nWriting and presentation:\nThe quality of writing should be improved. Here are several examples.\n1. In the abstract, the following sentence needs to be rewritten and the rule of capitalization should be consistent. \"(2) Limited Context and/or Smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging.\"\n2. I do not understand what's the purpose of the right figure in Figure 1. I think the paper does not do any matching like that.\n3. In the 3rd paragraph of the introduction, \"topmost\" -> top most \n4. The paper should have a related work section. In addition to the related work discussion scattered in the introduction, authors should discuss the difference between this work and Lauly et al., 2017. Authors should also include some related work such as [1,2,3].\n5. Just below (1), \"where,\" -> , where\n6. In the last sentence of the paragraph after (1), you mentioned \"v_{<i} are orderless\", so what's the ordering used in experiments? Random ordering?\n7. I guess \"a\" in algorithm 1 means sum_{k<i}(W_{:,v_k}), but I cannot find the explicit explanation about the purpose of \"a\".\n8. For ctx-DocNADEe, is W+E the embedding of words at input layer in LM?\n9. In the 3rd paragraph of section 2.2, you said: \"each row vector W_{j,:} is a distribution over vocabulary of size K\". Could W has negative values during optimization?  If yes, why a distribution representing a topic could have negative value. If no, you should explicitly mention this non-negativity constraint.\n10. Why are some values in Table 12 and 13 missing?\n\n[1] Cao, Z., Li, S., Liu, Y., Li, W., & Ji, H. (2015, January). A Novel Neural Topic Model and Its Supervised Extension. In AAAI (pp. 2210-2216).\n[2] Srivastava, A., & Sutton, C. (2017). Autoencoding variational inference for topic models. ICLR\n[3] Card, D., Tan, C., & Smith, N. A. (2017). A Neural Framework for Generalized Topic Models. arXiv preprint arXiv:1705.09296.", "title": "Method is not novel but results seem to be solid", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJl6p4JHR7": {"type": "rebuttal", "replyto": "rkgoyn09KQ", "comment": "Please see the revised version. \n\nThe updates are:\n1. Figure 3f: Added IR curve for ProdLDA\n2. section 3: additional evaluation including ProdLDA and SCHOLAR\n3. appendices", "title": "Revised version updated with additional comparisons with ProdLDA and SCHOLAR"}, "H1giEMySA7": {"type": "rebuttal", "replyto": "SyePgiuz0X", "comment": "Thanks for your comments again. The additional experiments as suggested would help in strengthening our paper.\n\nWe have now revised our paper with the suggested topic models (ProdLDA and SCHOLAR) and compared them for three evaluation types: perplexity, topic coherence and information retrieval task.  \n\nIn our previous response, we have discussed the difference in motivation of our proposed models and the suggested topic models, and now included them in the revised version based on your feedback. \n\nImportantly in the revised version, we have included more related baselines, such as Topic-RNN, TDLM and TCNLM, as suggested by other reviewers. \n\nPlease see the revised version. The updates are:\n1. Figure 3f: Added IR curve for ProdLDA\n2. section 3: additional evaluation including ProdLDA and SCHOLAR\n3. appendices\n\nAbout \"the values of perplexity in Table 5 very different from the values in Table 13\":\n-----------------------------------------------------------\nTable 5 for PPL scores on test set\nTable 13 for PPL scores on validation set\n\n\n\n   ", "title": "Revised version updated with suggested methods: ProdLDA and SCHOLAR "}, "BJgGJZ_e2m": {"type": "review", "replyto": "rkgoyn09KQ", "review": "The paper extends an existing topic model - DocNADE - by replacing the feedforward part of the network which combines the textual context with an LSTM sequence model. Hence this paper fits in a long tradition of work which aims to extend the bad-of-words model from the original LDA paper with some sequence information.\n\nThe authors do a commendable job in thoroughly evaluating the proposed extension, using a number of evaluations based on perplexity, topic coherence, and text retrieval and categorization.\n\nMy main problem with the paper as it stands is that it a) arguably oversells the contribution, and b) is unclear when explaining certain crucial aspects of the model.\n\nIt would also help to have a clearer statement of whether the contribution here is on the document modeling side, or the language modeling side. Motivation is provided from both angles, but the evaluation focuses largely on the topic modeling (which is fine, just need to say it).\n\nMore specific comments:\n--\nThe abstract should mention that the DocNADE model already exists, and that the contribution of the current work is to extend that existing model in a particular way. For those readers unfamiliar with DocNADE, this will help situate the work with regard to the existing literature.\n\nUsing existing word embeddings as a 'prior' for the LSTM word embeddings is a completely standard alternative now to learning those embeddings from scratch. I'm not sure that can count as a second, major contribution of the paper. (I'm also not sure that either extension to DocNADE warrants a new name, but I'll leave that to the authors' judgement.)\n\nI'm confused by one aspect of the DocNADE model: \"the topic assigned ...equally depends all the other words appearing in the same document\". But the model is generative, no? And eqn 1 suggests that each word is generated conditioned on the *previous* words in the document, or did I miss something? \n\nRelated to this point, DocNADE transforms its BoWs into a sequence. But what's the order? Is it just the order of the words in the document? In which case it's very similar to the LSTM extension, except the LSTM keeps the order in the history, whereas the bag-of-words model doesn't.\n\nRelation to generative models: LDA is a generative model with a generative story. It's not completely obvious to me what the generative story is in the new model. Talking about \"distributed compositional priors\" doesn't help, since I'm assuming these aren't priors in a Bayesian modeling sense? (It's also not clear in what sense these \"priors\" are compositional, but that's a separate question.)\n\nEquation 2: what's the motivation for mixing the LSTM history with the bag-of-words (esp. if the history is from the same bag of words in each case). Why not just use the LSTM?\n\nIt would be useful to state in the main body of the text what the value of lambda ends up being. In 3.1 there's a suggestion this might be 0.01, but that effectively ignores the LSTM?\n\nSimilar question: how can the DocNADE model provide a *global* context if the model is generative?\n\nPerplexity is a reasonable thing to measure, but presumably the auto-regressive nature of the LSTM means that it's more-or-less guaranteed to do better than a bag-of-words model? I wonder if it's worth acknowledging this fact?\n\nI don't understand why lambda has to be zero \"to compute the exact log-likelihood\".\n\nThe first line of the conclusion doesn't say much: it's pretty obvious that the ordering of the words is going to help better estimate the probability of a word in a given context; 50 years of language modeling research has already taught us that.\n\nMinor presentational comments:\n--\nSome of the hyphenation looks odd, eg in the title. Are you using the standard LaTeX hyphenation settings?\n\nStrictly speaking, I'm not sure that 'bear' in the example is a proper noun.\n\n\"orderless sets of words\": bags, not sets, since the counts matter, no?\n\nThe tables are too small, with a lot of numbers in them. One option is to move some of the details to the Appendix. Either way there needs to be more summary in the main body explaining what the numbers tell us.\n", "title": "Extends existing DocNADE model by replacing the feedforward network with an LSTM", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJg9nFA-RX": {"type": "rebuttal", "replyto": "rkgoyn09KQ", "comment": "\nWe thank all the official and non-official reviewers for your insightful comments. \n\nWe are glad to hear positive comments, especially from the official reviewers, such as \"solid and significantly better results\", \"extensive experiments\", \"four different kinds of evaluation is good\", \"commendable job in thoroughly evaluating\", etc. \n\nBased on your feedbacks, we have revised our paper with additional experimental results including in total **6 additional baselines** and some minor comments about presentation.  We have also addressed specific questions from each of the official reviewers. \n\nRemark about suggested baselines: \n---------------------------------\nWe have run additional experiments to cover the suggested baselines, such as NTM, ProdLDA, SCHOLAR, TDLM, Topic-RNN and TCNLM. We have shown that our proposed models outperform or are competitive to the baselines. Importantly, the related studies (e.g., TDLM, Topic-RNN and TCNLM) in marrying topic and language models is an active area of research and primarily focus on improving language modeling tasks using topic models: in contrast, the focus of our work is on improving topic models for textual representations (short-text or long-text documents) by incorporating language concepts (e.g., word ordering, syntax, semantics, etc.) via neural language models. In addition, we have also addressed the challenges of topic learning that arise from limited context in collections of \"short-text\" and/or  \"few\" documents. Therefore, as part of our second contribution, we incorporate external knowledge, i.e., word embeddings into neural autoregressive topic model to better model such document collections. In contrast to this sparse-data setting, the related approaches were designed for collections of long-text as well as a sufficient number of documents.\n\nIn spite of the differences with the related studies (as suggested), we have now run additional experiments using these baselines: IR task using TDLM on 8 short-text datasets and topic coherence (NPMI) using an additional dataset to compare our models (i.e., ctx-DocNADE and ctx-DocNADEe) with NTM, TDLM, Topic-RNN and TCNLM. In the revised version, we have further shown that our proposed models also outperform TDLM by a noticeable margin in terms of text retrieval and categorization for all the 8 short-text datasets.    \n\n\nTo summarize, the updates in revised version: \n1. [New] Table 7 of additional experiments, as suggested.\n2. [New] Figure 4\n3. [Update] Table 3\n4. [Update] Figure 3 (a,b,c,d and e)\n5. [Update] section 3.2\n6. [Update] section 3.3 (for additional experiments)\n7. [Update] section 3.4 (additional paragraph for new experiments)\n\nRemark: Diff in the two versions looks significant (*although it is not*) due to re-positioning of figures and tables. The table 7 is added after Table 6 and figure 3 (which is before Table 6 in the submission version) is now moved after Table 7  in the revised version. \n\n*** Please see the revised version ****", "title": "Revised version with suggested related works and experimental results"}, "Ske8KU1f07": {"type": "rebuttal", "replyto": "Hkg2dz7Kh7", "comment": "Thank you for your insightful comments. We appreciate your positive comments: \"solid and significantly better results\".\n\nWe have been running additional experiments for the suggested baselines and revised our submission based on your feedback.\n\nFollowing are the clarifications and answers to your questions:\n\n\"I will vote for acceptance if the mentioned baselines are also compared or there are good reasons why they cannot be compared\":\n\nAs discussed in detail below, we have quantitatively *compared* our proposed models with *all* the suggested baselines [1,2,3] and also *reasoned* about different evaluations with glove-DMM, glove-LDA, etc., as suggested. Please see Table 7, Table 3, Figure 3 and section 3.3 (last paragraph) for additional experimental results/baselines in the revised version.\n\nIn summary, we have now employed in total *13 baseline methods* (including 6 additional, as suggested by different reviewers): DocNADE, glove, glove-DMM, glove-LDA, gaussian-LDA, doc2vec, LDA, NTM[1], ProdLDA[2], SCHOLAR[3], TDLM, Topic-RNN and TCNLM to quantify our  contributions in different dimensions within a single model. In this work, we have focused on improving topic models (TM) and shown significantly better results in terms of perplexity, topic coherence, text retrieval and classification using 15 datasets.\nGiven the limits on submission size and a very active area of research, we attempt to cover the most recent, strong and comparable/fair baselines in the revised version.\n\n\"recent neural topic models\" as suggested [1,2,3]:\n\nOur motivation in this work is to improve topic models with language concepts, learned via LSTM-LM.\nSince DocNADE (Lauly et a., 2017) outperforms LDA, RSM, o-RSM, autoencoders and word2vec baselines in terms of perplexity and information retrieval, we have chosen DocNADE  as baseline model that we extend (also acknowledged by \"AnonReviewer3\": \"DocNADE has great performance so this is a welcome bit of research extending it\"). Additionally, the autoregressive property of DocNADE allows for an easy integration of LSTM-LM. In general, our approach of introducing language concepts can be further used in other flavors of topic models.\nAs suggested, we *additionally* show comparison of our proposed methods (i.e., ctx-DocNADE and ctx-DocNADEe) with NTM [1], ProdLDA [2] and SCHOLAR [3].\n\nComparison with NTM [1]: In terms of classification accuracy on 20NS dataset, we show that both ctx-DocNADE (0.744  vs 0.72) and ctx-DocNADEe (0.751 vs 0.72) outperform NTM (results taken from [1], Figure 2). In terms of topic coherence (NPMI) on the BNC dataset (new experiments), DocNADE, ctx-DocNADE and ctx-DocNADEe outperform NTM over 50, 100 and 150 topics (see Table 7 in revised version).\n\nComparison with ProdLDA [2]: A *direct* comparison with our work is *not fair* due to differences in its motivation (focus on improving inference in LDA) and experimental setup, for instance the vocabulary size. In order to fairly compare the DocNADE baseline  and ProdLDA in the same experimental setup as ProdLDA, we rerun DocNADE on the 20NS dataset for 200 topics and oberved that DocNADE outperformed ProdLDA, i.e., a PPL score of 830 vs 1168, which is *better* than ProdLDA [2] (result taken from Table 3 of [2]).\n\nComparison with SCHOLAR [3]: In terms of classification accuracy, the proposed models ctx-DocNADE (0.744  vs 0.71) and ctx-DocNADEe (0.751 vs 0.71) outperform SCHOLAR ([3], Table 2) on the 20NS dataset. Importantly, SCHOLAR focuses on incorporating meta-data (author, source, date, etc.) into topic models, different to our motivation of introducing language concepts using the document text only. Note that in the submission, we had reported a corresponding F1 score of .745 by ctx-DocNADEe in Table 4.\n\nThe above arguments clearly indicate that DocNADE is a *stronger* baseline than the three suggested baselines (NTM[1], ProdLDA[2] and SCHOLAR[3]). Additionally, we have quantitatively demonstrated that our proposed models ctx-DocNADE and ctx-DocNADEe outperform DocNADE.\n", "title": "\"vote for acceptance if the mentioned baselines are also compared or reasoned\""}, "SygdZ8JGR7": {"type": "rebuttal", "replyto": "Hkg2dz7Kh7", "comment": "\"Novelty\":\nAs mentioned by \"AnonReviewer2\" about our contributions, i.e., \"This paper fits in a long tradition of work which aims to extend the bag-of-words model from the original LDA paper with some sequence information\", we would like to restate the novelty/contribution of our work below:\n(1) We focus on improving topic models using language models within a unified neural network framework. Therefore, all evaluations are anchored on topic modeling.\n(2) To address challenges of topic modeling in sparse data settings we introduce word embeddings via LMs, thereby improving topic modeling for short-text or a corpus of few documents.\n\n\"Motivation\":\nAs we have discussed in our submission: \u201crecently, Peters et al. (2018) have shown that a deep contextualized LSTM-based language model (LSTM-LM) is able to capture different language concepts in a layer-wise fashion\u201d. On other hand, the baseline DocNADE models word co-occurrences in orderless BoW fashion by a linear aggregation of word representations (equation 1), i.e., without incorporating the contextual information for *each word*.\n \nThis has motivated us to introduce language concepts for each word (learned via LSTM-LM) into a autoregressive neural topic model (DocNADE). We jointly model the two neural networks in order to exploit the complementary learning and boost each of the autoregressive hidden representations (for every word, equation 2). This improves topic modeling *word-by-word* with contextualized word representations, word ordering information and local dynamics of the sequences.\n\n\"DocNADE-LM\":\nUnlike our contributions, DocNADE-LM (Lauly et a., 2017) restricts the history for each target word to the n \u2212 1 preceding words, following an n-gram assumption, where the hidden states due to LM part are generated from a linear combination of n-1 word embeddings (not pre-trained). Additionally, DocNADE-LM is not extensively studied.\nUnlike DocNADE-LM, our contribution lies in marrying the two networks: DocNADE and LSTM-LM to incorporate language concepts for each word in the sequence in order to improve autoregressive topic models such as DocNADE. \n\n\"Comparing coherence and perplexity of all different topic models\":\nWe have already compared our multi-fold contributions to several baselines (e.g., DocNADE, glove, glove-DMM, glove-LDA, gaussian-LDA, doc2vec, TDLM, Topic-RNN, TCNLM, etc.) that either (1) do not model contextualized word information (e.g., DocNADE, glove-DMM, glove-LDA and gaussian-LDA), or (2) do not incorporate word embeddings (e.g., DocNADE), or (3) ignore both in TMs (e.g., DocNADE) (4) incorporate word embeddings (e.g., glove-DMM, glove-LDA and gaussian-LDA) (5) jointly trained topic and language models (e.g., TDLM, Topic-RNN and TCNLM).\nGiven the page limits, we have attempted to include related works for each of the different contributions within a single framework that we have proposed. One key benefit of our contributions is that it offers a general architecture addressing different challenges and extending state-of-the-art on different tasks, instead of having different models to deal with specific challenges/tasks.\n\nWe have not included perplexity in Table 5 for glove-DMM, glove-LDA and Gaussian-LDA, since these baselines *ONLY* focus on improving topic models in terms of generating more coherent topics by incorporating word embeddings. Therefore, we compare them with our proposed models for topic coherence in Table 6. Additionally, we have already compared them for IR and categorization tasks. Since, Chang et al. (2009) and Newman et al. (2010) have shown that the perplexity is not a good metric for qualitative evaluation of topics, the practice of *introducing word embeddings into topic models* is often evaluated via topic coherence; in addition to this evaluation strategy, we also perform extrinsic evaluations via text retrieval and classification tasks.\n\n*Remark*: Since the baseline methods doc2vec and glove are not topic models but methods for generating paragraph/word embeddings, they are not part of Table 6. Instead, we have compared them with our proposed methods (Table 3 and 4) via text retrieval and categorization tasks to quantify the quality of representations learned.", "title": "Clarification to additional queries"}, "HJxunB1MCm": {"type": "rebuttal", "replyto": "Hkg2dz7Kh7", "comment": "\"the purpose of the right figure in Figure 1\": \nWe illustrate the motivation for incorporating word embeddings into topic models.\n\n\"a section about related works\": \nWe have now included the suggested related works in the revised version where we have motivated our task and contributions, for instance, \"contribution1\" and \"contribution2\" in the introduction section. DocNADE-LM (Lauly et al., 2017) is briefly mentioned in paragraph before the \"contribution1\". We have included [1]; however, as discussed above [2] and [3] have different motivation to our work, although a comparison is provided in the experimental section. Additionally, we have included more related works, such as TDLM, Topic-RNN and TCNLM.\n\n\"linearly combine their hidden\":\nELMo (Peters et al., 2018), a recent study, demonstrated that the hidden/internal state of each word in LSTM-LM encodes language concepts such as word ordering, syntactic and semantic information. We employ these internal states to improve latent topic vectors, h^DN (equation 2). In our proposed modeling, we motivate the linear combination of h_i^{DN} and h_i^{LM} to maintain architectural simplicity, following Lauly et al. (2017). The motivation behind the combination is: For each word v_i of a document v, the hidden state h_{i}^{DN} at the ith autoregressive step encodes topic semantics via DocNADE, while h_{i}^{LM} encodes language concepts via LSTM-LM.\nHowever, further investigations about applying linear layers would be an interesting future activity.\n\n\"ordering\":\nYes. Random ordering, following DocNADE.\n\n\"purpose of \"a\"\":\nIt is a linear activation, mentioned on page 5 in paragraph before \"ctx-DeepDNEe\".\n\n\"W+E the embedding of words at input layer in LM?\":\nYes, where W is trainable.\n\n\"distribution representing a topic\":\nThanks for your insightful observation. Yes, we have rephrased it though it is the property of DocNADE. As mentioned in section 6.3 (of Larochelle and Lauly, 2012), the matrix W is being used to compute topics as well as word representations.\n\n\"values in Table 12 and 13 of appendices\": \nGiven the extensive evaluation on 15 datasets in our work, we had to a run a large number of experiments (>400) and therefore tried to minimize the grid-search. We have included the missing numbers in the appendices of the revised version.\n", "title": "Clarification to minor comments/questions"}, "Hygw_x1MRQ": {"type": "rebuttal", "replyto": "BJgGJZ_e2m", "comment": "Thanks for your review comments and positive feedback about evaluation: \"commendable job in thoroughly evaluating\".\n\nPlease, see Table 7, Table 3, Figure 3 and Figure 4 in the revised version for additional experimental results.  \n\n\"explaining certain crucial aspects\":\nWe have tried to better motivate our tasks/contributions in the revised version, along with recent studies (including the suggested baselines) in the introduction and baseline sections. We would appreciate if could specifically point out the crucial aspects, in case we missed any. \n\n\"clearer statement... which is fine, just need to say it\":\nWe state several times throughout the paper (e.g., in the abstract, contribution1, contribution2, etc.) that we focus on improving topic models using language concepts learned from language models. Based on your feedback, we now also emphasize this in the evaluation section (1st paragraph) and also updated the title.\n\n\"embeddings in LSTM\":\nWe clarify that our contribiution2 is about incorporating word embeddings into topic models to handle data sparsity challenges in topic models. To this end, we incorporate word embeddings into topic models via an LSTM and compare our contribution, i.e., ctx-DocNADEe with related works, such as glove-LDA, glove-DMM and gaussian-LDA that also extend topic models with word embedding features. Our modeling approach offers an easy integration of word embeddings into topic models, that substantially improves topic coherence.\n\n\"DocNADE: generative and global semantics\":\nDocNADE is a generative model with orderless BoW input.  As detailed in equation 12 (Larochelle and Lauly, 2012), a random permutation of its words is used to produce the observed document - this random order is re-shuffled during learning so that all information related to the original word order is lost. \nThe order of the input in DocNADE is arbitary, while the original order of words in the document is presented via LSTM. Our contribution of combining the two networks is further motivated by recent studies, such as ELMo (Peters et al., 2018) that have shown that internal states of LSTM-LM capture language concepts (such as word order, syntactic and semantic information). Therefore, we have extended DocNADE by introducing these language concepts and shown improvements on different tasks using 15 datasets.\n\n\"Prior and compositional\":\nThe term 'prior' is used for the external information i.e., word embeddings encoding word relatedness in distributional semantic space. The term 'compositional' is used for the compositional property of RNN-LSTM, where LSTM-LM generates latent vectors (i.e., internal states of LSTM) for each input context via sequential compositionality. Each of the internal states for the corresponding word sequence captures language concepts that are presented to DocNADE, correspondingly at each of the autoregressive steps.\n\n\"history\":\nThe two history vectors: v<i and c_i are two different representations, in the sense that v<i is sampled from orderless BoW. This arbitary sequence is different to c_i in the sense that c_i is the original sequence of preceding words for the ith word in the document. The motivation is to combine the benefits of the two networks (i.e., DocNADE and LSTM-LM) to improve topic models, i.e., introduce local dynamics into global semantics of topic models. We control the mixture via a mixture weight $\\lambda$.\n\n\"lambda values\":\nIt is a hyperparamter, determined using the validation set; optimal values vary between datasets. Please see appendices (Table 13) for the ablation study.\n\n\"perplexity measure\":\nAs discussed, we set lambda to 0 during evaluation, i.e., no LSTM component is used during testing. We only exploit LSTM-LM during training to learn language concepts, encoded in the W matrix. Beyond perplexity, we also perform topic coherence, text retrieval and classification tasks.\n\n\"pseudo log likelihood\":\np(v) is exact for DocNADE (see details in Larochelle and Lauly (2012)), however not in its extensions ctx-DocNADE and ctx-DocNADEe. While in the proposed models (as discussed in section 2.2), each autoregressive conditional p(v_i | v<i) is a function of h_i and h_i (Table 1) is a function of v<i and context c_i, the likelihood is not exact due to difference in contexts: v<i and c_i. In further detail, v<i is based on the orderless BoW (and therefore an arbitary order) where each v_i is the index in the vocabulary. In contrast, c_i is the context of ith word in the document (and therefore the original word sequence of the context preceding the ith word). Importantly, to compute PPL scores during testing, we set lambda to 0, allowing a fair comparison between DocNADE and our proposed models (that is, the test liklihood is exact again).\n\n\"first line of conclusion\":\nWe have updated the conclusion section.\n\n\"using the standard LaTeX hyphenation settings?\":\nYes.\n\n\"bear as proper noun\":\nWe use stanford parser (http://nlp.stanford.edu:8080/parser/index.jsp) to perform tagging.", "title": "Clarification about the queries/comments"}, "H1l3AiAWCQ": {"type": "rebuttal", "replyto": "H1xTIhGKn7", "comment": "Thank you for your positive comments, especially about different kinds of evaluations and improvements. \n\nWe agree that the fast-paced ML research in this area makes exhaustive comparisons challenging. However, we do attempt to cover the most recent, strong and comparable/fair baselines (within the limits on submission size) and have now included several additional baselines in the revised version. More specifically we have followed your suggestions and now include TDLM, Topic-RNN and TCNLM (\"additional baselines\", section 3.2) in the revised version (see also Table 3, Table 7, Figure 3 and Figure 4 in the revised version).\nHowever, most recent related studies focus on improving language models (LMs) using TMs: in contrast, the focus of our work is on improving TM for textual representations (short-text or long-text documents) by incorporating language concepts (e.g., word ordering, syntax, semantics, etc.) via neural LMs. In addition, we also address the challenges of topic learning that arise from limited context in collections of \"short-text\" and/or  \"few\" documents. Therefore, as part of our second contribution, we incorporate external knowledge, i.e., word embeddings into neural autoregressive TM to better model such document collections. It is worth noting that in contrast to this sparse data setting, the related approaches you mention were designed for collections of long-text as well as a sufficient number of documents.\n\n\"Additional Baselines\":\nWe now compare our models to other approaches (TDLM, Topic-RNN and TCNLM) combining TMs with LM models. To this end, we follow the most recent approach presented in \"Topic Compositional Neural Language Model\" (Wang et al., 2018) and quantitatively compare TM performance in terms of topic coherence (NPMI) on the BNC dataset. \n\n*Additional experimental results on topic coherence* (comparing LDA, NTM, TDLM, Topic-RNN, TCNLM, DocNADE, ctx-DocNADE and ctx-DocNADEe on the BNC dataset) are included in the revised version (Table 7, Left and Right), showing competitive performance of our approach, although the related studies focus on improving topic models. \n  \n**Additional experimental results on IR and classification tasks**: We run additional experiments and execute TDLM on all the short-text datasets to show its performance for the IR and classification tasks. Please see Table 3 in the revised version. In Figure 3, we have additionally included IR-precision by TDLM model at different fractions and show that our contributions (i.e., ctx-DocNADE and ctx-DocNADEe) outperform TDLM.   \n\nRemark: Code for the other recent studies is not available. \n\n\"embedding gain with different data fraction\":\nAs suggested, we have now included additional analysis to demonstrate embedding gains with different data fractions. Please, see section 3.4 and Figure 4.\n\n\"pseudo log likelihood\":\np(v) is exact for DocNADE (see details in Larochelle and Lauly (2012)), however not in its extensions ctx-DocNADE and ctx-DocNADEe. While in the proposed models (as discussed in section 2.2), each autoregressive conditional p(v_i | v<i) is a function of h_i and h_i (Table 1) is a function of v<i and context c_i, the likelihood is not exact due to difference in contexts: v<i and c_i. In further detail, v<i is based on the orderless BoW (and therefore an arbitary order) where each v_i is the index in the vocabulary. In contrast, c_i is the context of ith word in the document (and therefore the original word sequence of the context preceeding the ith word). Importantly, to compute PPL scores during testing, we set lambda to 0, allowing a fair comparison between DocNADE and our proposed models (that is, the test liklihood is exact again).\n\n\"trained by SGD on the log likelihood\":\nYes. We have now included such details in the revised version.\n\n\"model description\":\nWe have devoted section 2.1, section 2.2, algorithm 1 and Table 1 to explain our proposed models as well as DocNADE in more detail.  \n\n\"performance on including embeddings\":\nWe have extended DocNADE models specifically to better model short-text or a corpus of few documents (see also \"motivation2\"); we have introduced word embeddings to better deal with this data sparsity and hence expect improvements resulting from embeddings mainly for such sparse data settings.\n\nFor such short-text datasets, ctx-DocNADEe results in substantially improved IR, F1, PPL and topic coherence: (1) improved IR (DocNADE vs ctx-DocNADEe): 0.600 vs 0.630 in Table 3, (2) improved F1 (DocNADE vs ctx-DocNADEe): 0.683 vs 0.705 in Table 3, (3) improved PPL in Table 5 and (4) improved topic coherence (DocNADE vs ctx-DocNADEe): 0.755 vs 0.790. As suggested, we have now included additional analysis to demonstrate embedding gains with different data fractions. Please, see section 3.4 and Figure 4. \n\nGiven these results on several datasets for the four tasks, we argue that the introduction of embeddings helps in improving topic models, especially in sparse data settings. ", "title": "Improved related works, covered in total 13 related baselines (6 additional now in the revised version)"}, "Bygx_-FN37": {"type": "rebuttal", "replyto": "BygKj5yM2X", "comment": "Thanks for your interest in our work, positive comments and questions.\n\nPlease find our response below:\n\n1.  We briefly discussed the model complexity in section 2.2. \n\n2. Yes, we will release the source code and also the pre-trained models.", "title": "interesting work"}, "HygAK7Te2m": {"type": "rebuttal", "replyto": "HJe3T9ORsm", "comment": "Thanks for your interest and comments about our work. \n\nPlease find our answers below:\n\n(a) As mentioned in Larochelle & Lauly (2012), due to the bag-of-word nature of DocNADE, a document v  takes the form of a set of word counts vectors in which the original word order is lost; this, in turn, is required by DocNADE to compute the sequence of conditionals p(v_i | v<i). Consequently, a random permutation of its words is generated in each iteration an, as a result, a distribution over all possible permutations that could have generated the original document v is obtained  (see also section 4.1 in Larochelle & Lauly, 2012). \nWhile in principle it is an interesting approach to use only those token IDs corresponding to the context of a given target word v_i, it is not possible to obtain the token IDs in the document, given the word IDs from its bag-of-word representation. For instance, given a word ID that occurs n times, it is not feasible to trace back the token ID(s) in the document. \n\nWe would also like to add that the neural language model not only introduces word ordering into topic models, but also language concepts such as the syntax and semantics encoded in its internal states (\"Deep contextualized word representations\", Peters et al. (2018)).   \n\n(b) Yes. Following the baseline topic model (i.e., DocNADE), our modeling approaches explicitly do represent a document as a probability distribution over topics. This can be seen in equations (1) and (2) in the paper, where the autoregressive conditional p(v_i = w | v_<i) computes H (i.e., number of topics) distributions over the vocabulary for the input document v, using the H dimensional hidden topic vector.", "title": "Some clarifications"}, "Hkgb3c2e2X": {"type": "rebuttal", "replyto": "H1Pw8ng27", "comment": "In continuation to our response above, we further qualitatively show the top 5 words of six learnt topics (topic name summarised by Wang et al., 2018):\n\n---------------------------------------------------------------------------------------------------------------------------\n\tTopic\t\t\tModel        \t    Topic-words (ranked by probabilities)\n---------------------------------------------------------------------------------------------------------------------------\n\t\t\t\tTCNLM#\t\t\t  pollution, emissions, nuclear, waste, environmental\nenvironment\t\tctx-DocNADE*       ozone, pollution, emissions, warming, waste\n                                ctx-DocNADEe*     pollution, emissions, dioxide, warming, environmental\n---------------------------------------------------------------------------------------------------------------------------\n                               TCNLM#                   elections, economic, minister, political, democratic\npolitics                  ctx-DocNADE*        elections, democracy, votes, democratic, communist\n                               ctx-DocNADEe*      democrat, candidate, voters, democrats, poll\n---------------------------------------------------------------------------------------------------------------------------    \n                               TCNLM#                   album, band, guitar, music, film\nart                         ctx-DocNADE*         guitar, album, band, bass, tone\n                              ctx-DocNADEe*       guitar, album, pop, guitars, song\n---------------------------------------------------------------------------------------------------------------------------\n                              TCNLM#                    bedrooms, hotel, garden, situated, rooms\nfacilities               ctx-DocNADE*          bedrooms, queen, hotel, situated, furnished\n                              ctx-DocNADEe*       hotel, bedrooms, golf, resorts, relax\n---------------------------------------------------------------------------------------------------------------------------\n                             TCNLM#                    corp, turnover, unix, net, profits\nbusiness             ctx-DocNADE*          shares, dividend, shareholders, stock, profits\n                             ctx-DocNADEe*        profits, growing, net, earnings, turnover\n---------------------------------------------------------------------------------------------------------------------------\n                             TCNLM#                     eye, looked, hair, lips, stared\nexpression         ctx-DocNADE*           nodded, shook, looked, smiled, stared\n                            ctx-DocNADEe*         charming, smiled, nodded, dressed, eyes\n---------------------------------------------------------------------------------------------------------------------------\n\nWe will include the topics (above) in the revised version.", "title": "citing and comparing to recent work"}, "H1Pw8ng27": {"type": "rebuttal", "replyto": "Bkgy0EL8jQ", "comment": "We appreciate your comments about missing citations of related neural topic models (TMs) and we will make sure to include them in a revised version. While in our initial submission we considered a comparison of our model to these studies as out of scope, we have since been running additional experiments to quantitatively compare TM performance of our proposed models to the studies you mention. We will include these new comparisons in the revised version.\n\nIn the following, we briefly summarize our contributions again and contrast them to the related work you mention. We then present the results of our new experiments and show that DocNADE-based models have a competitive or superior performance in terms of topic coherence, compared to Topic-RNN, TCNLM and TDLM.\n\nThe related studies you mention focus on improving language models (LMs) using TMs: in contrast, the focus of our work is on improving TM for textual representations (short-text or long-text documents) by incorporating language concepts (e.g., word ordering, syntax, semantics, etc.) via neural LMs. In addition, we also address the challenges of topic learning that arise from limited context in collections of \"short-text\" and/or  \"few\" documents. Therefore, as part of our second contribution, we incorporate external knowledge, i.e., word embeddings into neural autoregressive TM to better model such document collections. In contrast to this sparse-data setting, the related approaches you mention were designed for collections of long-text as well as a sufficient number of documents.\n\nWe have already compared our contributions to several baselines (e.g., DocNADE, glove, glove-DMM, glove-LDA, gaussian-LDA, doc2vec, etc.) that either\n(1) do not model contextualized word information, or\n(2) do not incorporate word embeddings, or\n(3) ignore both in TMs.\nWe outperform them in terms of perplexity, topic coherence, text retrieval and classification, evaluated on 14 datasets.\n\nWe now further compare our models to other approaches combining TMs with autoregressive neural models. To this end, we follow the approach presented in \"Topic Compositional Neural Language Model\" (Wang et al., 2018) and quantitatively compare TM performance in terms of topic coherence (NPMI) on the BNC dataset. \n\nModel\t\t\t\tTopic Coherence (BNC dataset)\n\t\t\t\t         #Topics   =50    =100\t =150\n------------------------------------------------------------------------\nLDA# \t\t\t\t\t\t.106    .119     .119\nNTM# \t\t\t\t\t\t.081    .070     .072\nTDLM(s)# \t\t\t\t\t.102    .106\t.100\nTDLM(l)#    \t\t\t\t        .095\t    .101\t.104\nTopic-RNN(s)#                              .102    .108     .102\nTopic-RNN(l)#                               .100    .105     .097\nTCNLM(s)#                                    .114    .111     .107\nTCNLM(l)#                                     .101    .104     .102 \n                \n                          (sliding window=20)\nDocNADE                                       .097    .095     .097\nctx-DocNADE*(lambda=0.2)      .102    .103     .102\nctx-DocNADE*(lambda=0.8)      .106    .105     .104\nctx-DocNADEe*(lambda=0.2)    .098    .101       -\nctx-DocNADEe*(lambda=0.8)    .105    .104       -\n       \n                         (sliding window=110)\nDocNADE                                       .133    .131     .132\nctx-DocNADE*(lambda=0.2)      .134     .141     .138\nctx-DocNADE*(lambda=0.8)      .139    .142     .140\nctx-DocNADEe*(lambda=0.2)    .133    .139       -    \nctx-DocNADEe*(lambda=0.8)    .135    .141       -\n                \n        \nHere, the asterisk (*) indicates our proposed models and (#) taken from Wang et al.; lambda is the mixture weight of the LM component in the topic modeling process and sliding window is one of the hyper-parameters for computing topic coherence (Wang et al. (2018) and \"Exploring the Space of Topic Coherence Measures\" (R\u00f6der et al., 2015). A sliding window of 20 is used in Wang et al.; in addition we also present results for a window of size 110.\n\nOur results (table above) suggest that our contribution (i.e., ctx-DocNADE) of introducing language concepts into bag-of-word topic model (i.e., DocNADE) improves topic coherence. Better performance for high values of lambda illustrate the relevance of the LM component for topic coherence (DocNADE corresponds to lambda=0). Similarly, the inclusion of word embeddings (i.e., ctx-DocNADEe) results in more coherent topics than the baseline DocNADE. \n\nImportantly, while ctx-DocNADEe was motivated by sparse data settings, the BNC dataset is neither a collection of short-text nor a corpus of few documents. Consequently, ctx-DocNADEe does not show improvements in topic coherence over ctx-DocNADE.\n\nBeyond topic coherence, we also evaluate TMs for text retrieval as well as classification tasks on 14 datasets. However, since the BNC dataset is unlabeled, we are here restricted to comparing model performance in terms of topic coherence only.\n\nWe will include the additional results (above) in the revised version.", "title": "citing and comparing to recent work "}, "ByebqQXoiX": {"type": "rebuttal", "replyto": "BJegm-88sX", "comment": "Thanks for your comments/suggestions. \n\nPlease find our response below:\n\n1.\tExperimental results on IR and F1:\n\nIn this work, we have introduced contextual information and word embeddings into neural autoregressive topic models (DocNADE) with ease and shown that our contributions outperform all the baseline topic models, in terms of generalization (perplexity), topic coherence, text retrieval and classification. \n\nTable 3 illustrates the scores for information retrieval (IR) and classification (F1) tasks on *short-text* datasets, quantifying the quality of representations learned by the baseline topic models (row #4-#9). The scores in rows #1-#3 are additional baselines that we used in this work to quantify text representations generated by glove and doc2vec methods. \n\nAbout IR, we have clearly shown a strong promise (Table 3 and 4), compared to the 'glove' baseline:\n- On an average over 8 *short-text* datasets, we (i.e., ctx-DocNADEe) show a gain of 13.7% (.630 vs .554) in precision at retrieval fraction 0.02.  \n- On an average over 6 *long-text* datasets, we (i.e., ctx-DocNADEe) show a gain of 20.0% (.601 vs .501) in precision at retrieval fraction 0.02.  \n\nAdditionally, to quantify the quality of word representations learned, we also compare the text classification scores of our proposed method (i.e., ctx-DocNADEe) and glove embeddings. While only for the short-text, we observe that the average F1 scores over 8 short-text datasets due to glove baseline is competitive, ctx-DocNADEe outperforms glove (F1: 0.618 vs 0.575) on average over 6 long-text datasets. The behavior is obvious because \"the application of topic models is challenging to short-text datasets due to a small number of word occurrences (i.e., lack of context)\" and therefore, the difficulties in learning representations. However, one can exploit the glove vectors to generate more coherent topics. It is further demonstrated in the related works of topic modeling, such as DocNADE, Gauss-LDA, glove-DMM and glove-LDA (Table 3, row #4-#9). We treat them as the baselines and outperform by a noticeable margin in all evaluations. \n\nWith a focus on improving topic models for unsupervised tasks, such as generalization (perplexity), text retrieval and topic extraction (i.e., coherence), our proposed models (i.e., ctx-DocNADE and ctx-DocNADEe) outperform all the baselines for both the short-text and long-text datasets, where the topic models extract topics, encoding the thematic structures in documents that explains them. On other hand, the glove vectors encode semantic similarity in word representations.     \n\n2.\tIn this work, we focus on unsupervised systems in context of topic modeling.  Following the experimental setup of RSM (Salakhutdinov and Hinton, 2009) and DocNADE (Larochelle and Lauly, 2012; Lauly et al., 2017), we perform the task of IR and report precision at different retrieval fractions. The IR task is similar to the one suggested.", "title": "Small questions about your experimental setup and results "}, "BJlyLFmsi7": {"type": "rebuttal", "replyto": "H1ec-sqtiX", "comment": "Thanks for your positive comments. \n\nWe will incorporate your suggestions in our revised version.\n\nYes, we will release the code and pre-processed datasets. ", "title": "Few Comments/Questions"}}}