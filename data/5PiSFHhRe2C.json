{"paper": {"title": "Meta Auxiliary Labels with Constituent-based Transformer for Aspect-based Sentiment Analysis", "authors": ["Ling Min Serena Khoo", "Hai Leong Chieu"], "authorids": ["~Ling_Min_Serena_Khoo1", "~Hai_Leong_Chieu1"], "summary": "A Constituent based Transformer with Meta-learnt auxiliary labels for Aspect based Sentiment Analysis", "abstract": "Aspect based sentiment analysis (ABSA) is a challenging natural language processing task that could benefit from syntactic information. Previous work exploit dependency parses to improve performance on the task, but this requires the existence of good dependency parsers.  In this paper, we build a constituent-based transformer for ABSA that can induce constituents without constituent parsers. We also apply meta auxiliary learning to generate labels on edges between tokens, supervised by the objective of the ABSA task.  Without input from dependency parsers, our models outperform previous work on three Twitter data sets and match previous work closely on two review data sets.", "keywords": ["Natural Language Processing", "Sentiment Analysis"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a constituent-based transformer for aspect-based sentiment analysis. The approach allows conducting aspect-based sentiment analysis to leverage the syntactic information without pre-specified dependency parse trees. \n\nOverall, the idea is interesting. However, all the reviewers shared the following concerns: \n\n- Paper descriptions of methodology and experiments are not clear and require significant rewriting and reorganization. \n- The proposed approach is not well-justified by the empirical study presented in the paper. Especially, a more detailed ablation study is required to justify the design. \n\nWe would suggest the authors addressing the feedback from the reviewers to improve the paper. "}, "review": {"wmp9BxX3dEV": {"type": "review", "replyto": "5PiSFHhRe2C", "review": "This paper presents a Transformer-based model for aspect-based sentiment analysis, intended to support the unsupervised induction of constituents within the Transformer forward pass. Their evaluations demonstrate that their model can match (and in some cases improve upon) models which depend upon explicit dependency parse information in the input, and reliably exceed parse-free models.  \n\nI strongly vote for rejection, largely on grounds of quality, elaborated below.  \n\nPros: The paper begins with an interesting idea and implementation, and the results support the claim that their architecture may replicate some of the contribution of dependency parse information.\n\nCons: The presentation is unclear on the concept of \"constituent\" and the motivation of the model. The later iterations of the model become rather complicated and don't seem well-motivated. The qualitative evaluations don't strongly support the claims of the paper.  \n\nQuality\n\n1. The design of the model and the use of the word \"constituent\" seems conceptually problematic to me. What do you take the word \"constituent\" to mean for your motivation and model design? It seems to me that it might be sufficient to call the ConsTrans model a \"spatial attention smoothing\" model. Why isn't this a sufficient description? What does the concept \"constituent\" add?This question is relevant because the later model iterations are developed on further syntactic ideas (typed/labeled relations, syntactic \"distance\"). But if the model doesn't have a necessary syntactic framing, it's not clear these are the correct model improvements to consider.\n2. The qualitative evaluations are far too light, testing only a small amount of the model performance.\n  a. Grammar induction: In particular, I would appreciate a far more in-depth evaluation of the inferred constituent structures. How do they compare to gold and silver dependency parses of within-domain sentences? The current evaluation checks for model inferences on just one short span of text (the aspect term). This is probably one of the easiest terms for the model to recognize as a constituent, too, since the aspect terms are known to be constituents and have verbatim copies in the input sentence. The current evaluation is also only performed on Twitter17 --- why?\n  b. Interpreting learnt relation labels: I found this evaluation extremely confusing, involving an ad-hoc dependency parsing algorithm built upon an a posteriori fact discovered in model analysis (that relation embedding L2 norm indicates inverse syntactic distance). The resulting parse in Figure 5 is almost entirely incorrect and commits many basic mistakes (for example, not linking the determiner \"the\" with its immediately adjacent noun). The claim about linking adjectives and nouns is not particularly interesting to me, since this is far less ambitious than the motivation of the model --- if it were, the model could have been quite a bit simpler, I think.\n3. Significance results are given (thanks!) but with a strangely high significance threshold (0.15 at one point and 0.2 at another). This is not a reasonable significance threshold in my view.  \n\nClarity\n\nSome minor comments:\n\n1. The constituent derivation algorithm is not clear. Eqn 3 suggests that constituent probabilities are a function of token pairs, but Algorithm 1 line 11 suggests that they can be indexed by a single token position. Is something missing from the algorithm presentation?\n2. Eqn 3 first condition should be i <= j, I think.\n3. Figure 4 is not a complete sentence. There's no verb. There's also no clear sentiment (aspect-based or otherwise) that we could talk about for this sentence. A different sentence could serve as both a motivating example for constituency and as a more revealing example of the model's syntactic knowledge.\n\n## Post-rebuttal response\n\nI have read the other reviews and the authors' responses, and do not wish to change my review. The proposed model seems quite complex with somewhat unclear conceptual motivations, and does not clearly demonstrate impressive performance gains despite the complexity. I would suggest that the authors attempt to change one of these things in a later paper, either by revisiting the model design, or task choice and evaluation (to better motivate the model).", "title": "Overly complex model, conceptually unclear, and not well evaluated", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "JVNtqd0JFI": {"type": "rebuttal", "replyto": "uDLoSKFBej_", "comment": "Thank you for taking time to review the paper and for the constructive feedback provided. We address your questions and comments below:\\\n\\\nQuestion 1: \\\na.\tThe generated $l_{ij}$ is a continuous value, how can it be made into a relation label? \\\nb.\tWhy was the L2 norm on $r_{ij}$ in (10) taken? \\\nc.\tWhat is $L(\\hat{y} , y)$ in (11) and how is it computed? \\\nd.\tHow could it [y (sentiment label of the aspects)] be used to update the label generator  $l_{ij}$?\\\ne.\tWhat is the intuition of training $\\theta_{main}$ and $\\theta_{aux}$ using separate datasets? Could the difference in terms of the performance be shown?\\\n\\\nResponse to Q1a & Q1b:\\\nYour observation that  $l_{ij}$ is a continuous value is correct, and we apologize for not clarifying this point in the paper: while relation label is conventionally a categorical value (in dependency parses), we have chosen to train our model to learn scalar relational labels.\\\nWe took inspiration from [1] where the authors observed that the L2 norm of the linear projection of token embeddings obtained from BERT could recover the parse tree distances between the tokens. Hence, we train the relation scalar label by supervising it with the L2 norm of the relational embedding with the MSE loss.\n\nResponse to Q1c:\\\n$L(\\hat{y} , y)$ in (11) is the cross entropy loss between the ABSA ground truth and the model prediction for ABSA. \\\n\\\nResponse to Q1d:\\\nA sketch of our learning framework is as follows: \\\n(1) We first train an ABSA predictor with the cross entropy loss, and \\\n(2) we then train the label generator using a separate training set. The label generator is optimized such that if the ABSA predictor is trained with the labels produced by the label generator, the loss for the ABSA task is minimized. \\\nThe model parameters of the label generator $\\theta_{aux}$ is frozen in optimizing (11). The label generator is only trained in optimizing $\\theta_{aux}$ in Eqn (13).\n\nResponse to Q1e:\\\nThe label generator is trained on a separate set to encourage labels generated to be optimal for a dataset unseen by the ABSA predictor during training. This would encourage the label generator to generate labels useful for generalization since the train setting is similar to the test setting. \\\nWe sampled this unseen set by selecting records that fulfil the following conditions:\n1.\tContains more than 1 aspect term in the sentence and \n2.\tThe aspect terms have a different sentiment labels\nWe argue that these are more \u201cdifficult\u201d examples, and having a relation label that represents the relationship between tokens would be exceptionally useful. \n\nQuestion 2: \\\nGiven the complexity of the model, the improvement compared to the baseline models are relatively trivial. And it is somewhat insufficient to limit the application only to aspect-based sentiment analysis. The contribution is thus limited.\n\nResponse to question 2:\\\nOur baseline models are equally complex models such as RGAT-Wang (Wang et al., 2020) and LCFS-ASC-CDW (Phan & Ogunbona, 2020), which built transformer layers on top of BERT as we have done. Moreover, these baselines require additional inputs such as dependency parses or multi-modal inputs (images as well as text).\n\nQuestion 3:\\\nHow was the meta-train set sampled? Is the result over one meta-train set or averaged over different meta-train set?\n\nResponse to question 3:\\\nWe have attempted to answer this question in our response to Q1d and Q1e. \n\nResponse to comments:\\\nWe will revise the paper to address the comment of missing caption and error in equation 3.\n\n[1] A Structural Probe for Finding Syntax in Word Representations. John Hewitt, Christopher D. Manning. NAACL 2019\n", "title": "Response to AnonReviewer1"}, "bb8KJ3rwHb": {"type": "rebuttal", "replyto": "uJBRu69Qpa", "comment": "Thank you for taking time to review the paper and for the constructive feedback provided. We address your questions and comments below:\\\n\\\nQuestion 1:\\\na.\tOver dependence on the pre-trained language model: Although it was claimed that explicit syntactic information is not used, syntactic information contained in the pre-trained language models such as BERT was used to promote the training of ConsTrans, RelConsTrans, and RelConsTransLG. \\\nb.\tWhat is the performance of using a weaker pre-trained language model like Distibert? How about the performance in stronger pre-trained models like RoBERTa?\\\n\\\nResponse to Q1a:\\\nWe agree that we are exploiting syntactic information in BERT to replace dependency parsers. We have shown that this is useful for Twitter, where we outperformed SOTA results that use dependency parsers on ABSA. This is also useful for resource-poor languages: it is easier to build BERT for a resource poor language than to build a dependency parser, since SOTA dependency parsers require annotated dependency data sets, while BERT only requires raw unlabelled data.\n\nResponse to Q1b:\\\nWe have explored using a stronger model (XLNET) and there was a performance gain of approximately 2% F-score across all the datasets. However, for fair comparison against previous SOTA on the ABSA task (e.g., [1] and 2]), we have only reported results for BERT. \\\nWe would provide results for weaker pre-trained language models like DistilBERT in the revised version. \n\nQuestion 2:\\\na.\tModel size and inference performance: the extra transformer layers added makes the result not directly comparable with that of BERT. Therefore, effect of the BERT baseline under the same parameter scale needs to be reported.\\\nb.\tTo what extent will the introduction of the constituent probability scorer and relation label generator affect the inference performance of the model? \\\nc.\tFor this simple task of sentiment analysis, is such a complex model design and training method design needed after the already huge pre-trained language model such as BERT? \n\nResponse to Q2a:\\\nOur baseline model is not a simple BERT classifier. Our baseline models are equally complex models such as RGAT-Wang [1] and LCFS-ASC-CDW [2], which built transformer layers on top of BERT as we have done. Moreover, these baselines require additional inputs such as dependency parses, or multi-modal inputs (images as well as text).\n\nResponse to Q2b:\\\nWe have shown that we consistently outperform a vanilla transformer with the same number of layers on 2 datasets (Restaurant and Laptop) with the addition of the constituent probability scorer (ConsTrans). The label generator is also shown to consistently outperform the model trained without it. \n\nResponse to Q2c:\\\nOur task is Aspect based Sentiment Analysis (ABSA) and not general Sentiment Analysis (SA) The ABSA task is a rich and difficult NLP task: for example, to classify \u201cI will migrate to Europe if Trump gets re-elected\u201d as negative towards \u201cTrump\u201d requires contextual information, and the sentiment changes from negative to positive if we change \u201cEurope\u201d to \u201cthe US\u201d. There is a lot of literature on ABSA and we have shown that we have outperformed all previous work including two recent papers published in ACL 2020 [1,2].\n\nQuestion 3:\\\nThe only conclusion that can be drawn is that the model is further enhanced with the features of dependency syntax recently, which has little relationship with meta-learning.\n\nResponse to question 3:\\\nWe are applying the meta-auxiliary learning approach from [4] to learn auxiliary relation labels. This is different from the vanilla meta-learning from multiple supervised tasks. In [4], they used the term meta-learning as they argued: \u201cleveraging the performance of the multi-task network to train the label-generation network can be considered as a form of meta learning\u201d.\n\nResponse to comments:\\\nWe have attempted to address the comments in our responses for question 1 and 3. \n\n[1] Relational graph attention network for aspect-based sentiment analysis. Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, and Rui Wang. ACL 2020.\\\n[2] Modelling context and syntactical features for aspect based sentiment analysis. Minh Hieu Phan and Philip O. Ogunbona. ACL 2020.\\\n[3] Adapting BERT for target-oriented multimodal sentiment classification. Jianfei Yu and Jing Jiang. IJCAI 2019.\\\n[4] Self-Supervised Generalisation with Meta Auxiliary Learning. Shikun Liu, Andrew J. Davison, Edward Johns. Neurips 2020.\n", "title": "Response to AnonReviewer3"}, "hnhVtVVro2L": {"type": "rebuttal", "replyto": "wmp9BxX3dEV", "comment": "Thank you for taking time to review the paper and for the constructive feedback provided. We address your questions and comments below: \\\n\\\nQuestion 1: \n\\\na.\tWhat was the meaning of the word \"constituent\" taken to be for the your motivation and model design?\\\nb.\tIt seems that it might be sufficient to call the ConsTrans model a \"spatial attention smoothing\" model. \\\n\\\nResponse to Q1a:\n\\\nThe motivation of the model is to group tokens into meaningful phrases supervised by the aspect-based sentiment analysis (ABSA) task, hence the term \u201cconstituent\u201d. Consider the example \u201cPork and chives dumplings in this restaurant is juicy and fresh\u201d \u2013 the model needs to understand that the \u201cjuicy and fresh\u201d is applied to the whole phrase \u201cpork and chives dumplings\u201d and not just \u201cdumplings\u201d alone. It is therefore necessary to group the tokens \u201cjuicy\u201d, \u201cand\u201d, \u201cfresh\u201d as a constituent (VP) and group the tokens \u201cpork\u201d, \u201cchives\u201d, \u201cdumplings\u201d as a constituent (NP). Wu et al. [1] stated that \u201ca concept of phrase dependency parsing\u201d could benefit \u201c[opinion] mining task\u201d since \u201ca lot of product features are phrases\u201d. The task of opinion mining is similar to ours where the goal is to identify opinions for product features or aspect terms.\\\n\\\nResponse to Q1b:\n\\\nWe designed our models to assign higher attention weights to tokens from the same constituent, by optimizing for the ABSA objective. Since there could be large variations in the length of constituents, this allows the model to adjust weights based on the probability of words belonging to the same constituent, rather than solely based on proximity. Definitely, the concept of spatial attention smoothing is similar to our method since tokens from the same constituent are close in proximity by nature. \\\n\\\nQuestion 2:\n\\\na.\tHow do they compare to gold and silver dependency parses of within-domain sentences?  The current evaluation checks for model inferences on just one short span of text (the aspect term). This is probably one of the easiest terms for the model to recognize as a constituent. \\\nb.\tThe claim about linking adjectives and nouns is not particularly interesting since this is far less ambitious than the motivation of the model.\\\n\\\nResponse to Q2:\n\\\nWe acknowledge that stating that our model is able to perform \u201cgrammar induction\u201d generally is a far too ambitious a statement since our model was only trained for ABSA. The motivation for our proposed models is to induce the necessary syntactic bias that would be useful for ABSA. Therefore, we looked at recalling aspect terms as a way to gauge if the model was able to induce sufficient syntactic bias for ABSA.\\\nIn our revision, we will use \u201cinterpretability of induced constituents\u201d rather than \u201cgrammar induction\u201d to explain this. We have performed analysis on other datasets but did not include them in the paper due to space constraint and would also include this analysis in the revised version of our paper. \\\n\\\nQuestion 3:\n\\\nSignificance results are given but with a strangely high significance threshold. This is not a reasonable significance threshold. \\\n\\\nResponse to question 3:\n\\\nWe acknowledge that our significance results are weak for models such as RGAT-Wang (p=0.15). However, these models use dependency parsers while ours do not. Furthermore, the threshold of BERT+BL was actually lower (p=0.1). \nThe higher threshold (p=0.2) used for a significance test was for grammar induction. We will remove the claims of grammar induction in a revised version and instead, use it as an interpretability analysis of our model.\\\n\\\nResponse to clarity comments:\n\\\nWe will revise the paper to address the comments on clarity. The example in Figure 4 is a complete tweet, which are sometimes not in complete sentences.\\\n\\\n[1] Phrase Dependency Parsing for Opinion Mining. Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu. EMNLP 2009.", "title": "Response to AnonReviewer2"}, "uJBRu69Qpa": {"type": "review", "replyto": "5PiSFHhRe2C", "review": "According to two phenomena in Transformer's pre-trained language model: \n1. Tokens with similar self-attention distribution tend to be distributed in the same constituent; \n2. The L2 distance between token representations reveals the degree of syntactic dependency; \n\nan aspect-based sentimental analysis system is designed which does not rely on explicit syntactic information. It is not a surprising point to combine the so-called syntactic information learning with sentential analysis. Although the model has achieved comparable results with systems using explicit syntax, the meta-learning is somewhat overclaimed.\n\nThere are some defects in this work:\n1. Over dependence on the pre-trained language model: Although the author claims that explicit syntactic information is not used, the system relies on the syntactic information contained in the pre-trained language models such as BERT to promote the training of ConsTrans, RelConsTrans, and RelConsTransLG. I doubt whether the training of the model can converge without the pre-trained language model only relying on the randomly initialized transformer and still achieve comparable accuracy with systems with explicit syntax? What is the performance of using a weaker pre-trained language model like Distibert? How about the performance in stronger pre-trained models like RoBERTa?\n2. Model size and inference performance: the extra transformer layers added makes the result not directly comparable with that of BERT. Therefore, the author needs to report the effect of the BERT baseline under the same parameter scale. In addition, to what extent will the introduction of the constituent probability scorer and relation label generator affect the inference performance of the model? For this simple task of sentiment analysis, do we need such a complex model design and training method design after the already huge pre-trained language model such as BERT?\n3. Inappropriate description: \n(1) In RelConsTrans, the proposed model encodes the syntactic relation as a feature for constituent scoring based on the role of syntactic relation for sentiment analysis. But according to Eq. 6 and 7, the so-called relation (that could not be determined) act as a bias in scoring function, I think it just provides a more feature source (dependency syntax) for scoring. \n(2) In RelConsTransLG, the r_{i,j} and l_{i,j} is similar. I think the only conclusion that can be drawn is that the model is further enhanced with the features of dependency syntax recently, which has little relationship with meta-learning. \n\nGenerally speaking, this work can be regarded as integrating the implicit constituent and dependency syntactic features from the pre-trained language models into the training in the sentient analysis, so that explicit syntax is no longer needed. Appendix A.6 and A.7 also confirm this point. Although it has a positive effect on the need to remove explicit syntax, its reliance on pre-trained language models limits the extensibility of this approach and is likely to be ineffective on traditional unpretrained LSTM, CNN, and Transformer models. In addition, since the writing is pretended to make the model close to a concept of meta-learning, it ignores the important points that should be clarified.", "title": "This paper studies the model design of removing explicit syntax from sentiment analysis task, but it also has some limitations.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "uDLoSKFBej_": {"type": "review", "replyto": "5PiSFHhRe2C", "review": "This paper presents a constituent-based transformer model with auxiliary relation embeddings and labels to enhance the performance of aspect-based sentiment analysis. The constituent-based transformer modifies the original transformer by re-weighting the attention weights with constituent-based similarities. The auxiliary relation predictions helps to differentiate the relations that are useful for sentiment prediction. \n\nIn general, this paper is readable and has a clear motivation. The strengths include:\n1. The idea of modifying the basic transformer to a constituent-based transformer by incorporating constituent similarities without any supervision is interesting. \n2. The authors propose three model variations progressively, namely ConsTrans, RelConsTrans and RelConsTransLG to clearly demonstrate the motivation and effect of each variation.\n\nHowever, the paper still lack the following aspects:\n1. Some parts of the description is unclear with details missing. From (9), the generated $l_{ij}$ is a continuous value, how do you make it a relation label? Why do you take the L2 norm on $r_{ij}$ in (10)? What is the intuition of such computation to obtain the MSE? What is $L(\\hat{y},y)$ in (11) and how to compute it? If $y$ is the sentiment label of the aspects, how it could be used to update the label generator $l_{ij}$? What is the intuition of training $\\theta_{main}$ and $\\theta_{aux}$ using separate datasets? Could you show what is the difference in terms of the performance?\n2. Given the complexity of the model, the improvement compared to the baseline models are relatively trivial. And it is somewhat insufficient to limit the application only to aspect-based sentiment analysis. The contribution is thus limited.\n3. How do you sample the meta-train set? Is the result over one meta-train set or averaged over different meta-train set?\n4. Minor points: caption is missing for figure 2. The first row in (3) should be $i\\leq j$.\n\n", "title": "Interesting modification on transformer, but limited applications and improvements", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}