{"paper": {"title": "Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding", "authors": ["Li Ziang", "Wu Kailun", "Yiwen Guo", "Changshui Zhang"], "authorids": ["~Li_Ziang1", "~Wu_Kailun1", "~Yiwen_Guo1", "~Changshui_Zhang2"], "summary": "We advocate an error-based thresholding (EBT) mechanism for LISTA, with superior performance and no extra learnable parameters.", "abstract": "The learned iterative shrinkage thresholding algorithm (LISTA) introduces deep unfolding models with learnable thresholds in the shrinkage function for sparse coding. Drawing on some theoretical insights, we advocate an error-based thresholding (EBT) mechanism for LISTA, which leverages a function of the layer-wise reconstruction error to suggest an appropriate threshold value for each observation on each layer. We show that the EBT mechanism well-disentangles the learnable parameters in the shrinkage functions from the reconstruction errors, making them more adaptive to the various observations. With rigorous theoretical analyses, we show that the proposed EBT can lead to faster convergence on the basis of LISTA and its variants, in addition to its higher adaptivity. Extensive experimental results confirm our theoretical analyses and verify the effectiveness of our methods.", "keywords": ["Sparse coding", "Learned ISTA", "Convergence Analysis"]}, "meta": {"decision": "Reject", "comment": "The paper received mixed reviews, with one review voting for acceptance, one strongly opposed, and two borderline ones. The discussion essentially involved R1 and R2, who gave the most informative reviews. After discussion, they did not update their score, even though they appreciated the work and effort done by the authors during the rebuttal. \n\nIn short, the paper has some merit, but several concerns were raised, which the area chair agrees with, leading to a rejection recommendation. The innovation was found to be limited and the discussion between practice and theory (meaning assumptions made in this work) are not discussed in a convincing manner, and these concerns remained after the rebuttal. The experiments were also subject to improvements.\n\nIt is however likely that with a major revision, this work may become publishable to a another venue."}, "review": {"LeFq8XDkxgW": {"type": "review", "replyto": "UfJn-cstSF", "review": "### Strengh\n\n- The idea makes sense to adapt the thresholding mechanism to an input distribution with various reconstruction error. It might bring a much better empirical performance compared to thresholds fixed globally and it seems to be adapted in a denoising setting.\n\n\n### Weakness\n\n- The motivation for this work is not sufficiently furnished. The authors claim that it is useful when there is a discrepancy between train/test distribution but do not provide reference to realistic situation where such a problem arise.\n- Also, this method cannot really be used for real sparse coding problems as the network needs to be trained with the ground truth which is often not known in practice.\n- The theoretical contribution is marginal as it is almost straightforwardly adapted from Chen et al. (2018) and Liu et al. (2019).\n- The theoretical results are not precise enough (`s small enough`) while this assumption might very well render all the results only applicable to toyish case. Moreover, these assumptions are stronger than the one in `Chen et al. (2018)` and `Liu et al. (2019)` for $x_s$ realizing the sup in the expression of $b^t$ (Eq.7).\n- Almost all the experiments use the same setting as previous work, failing to highlight the advantage of the proposed method.\n- The performance advantage over LISTA seems to be minor from Figure.2.\n\n\n## Extra remarks\n\n\n- The proposed goal is to adapt LISTA in a setting where the input training distribution is *different* from the testing one. However, in this setting, using an algorithm like LISTA does not make sense as the learned weights have no reason to be adapted to the new distribution if the distribution do not overlap at all. There might be some degree to which it is possible to adapt but this should be made more explicit and better discussed. In particular, what type of distribution shift are considered and would make sense -- sparsity is mentionned but it is unclear.\n\n- p.3: `normal training of LISTA leads to it.` I don't think there is any results showing that SGD over LISTA achieves such threshold in theory and I haven't seen any proper empirical validation. If it exists, a proper citation is needed. Else, the statement should be updated.\n\n- p.3: `According to some prior works, we also know that U(t)\u2208 W(A)`: in the three cited papers, there seems to be no results showing that the learned `U(t)` verifies this. The statement is once again too strong.\n\n- Eq.(8): Would it be interesting to evaluate the usage of $\\rho^{(t)} = \\mu(A)$?\n\n- p.4: `the main results are obtained under a mild assumption of the ground-truth sparse code` -> The assumption is not mild. For instance, it seems to never be verified in any of the experiments. The statement $\\mu(A)s \\ll 1$ seems not backed by any experimental evidence and I don't think this is true.\n\n- p.4: `the above assumption gives a more detailed description of the distribution for $x_s$` while it is true that it sets a distribution on the space $\\mathcal X$, it is not more precise as in the assumptions by `Chen et al. (2018)`, I believe no distribution is mentionned. so overall it constrains the type of distribution while it is not required in `Chen et al. (2018)`\n\n\n## Minor comments, nitpicks and typos\n\n- citations in () could use `citealt` to remove the extra parenthesis.\n- p.1: Lasso can also be solved using CD algorithms, which are typically state of the art.\n- p.1: In Gregor&LeCun (2010), the thresholding is not modified compared to ISTA.\n- p.2: `with W(t)=I\u2212U(t)A holds for any layer` -> `in the case where W(t)=I\u2212U(t)A holds for any layer`.\n- Eq.(7):\n- `Liu et al. (2018)`: The proper citation is `Liu, J., Chen, X., Wang, Z. & Yin, W. ALISTA: Analytic Weights are as good as Learned weigths in LISTA. in International Conference on Learning Representation (ICLR) 1113\u20131117 (2019).`\n- p.4: `a truncated distribution` -> I assume the authors mean `truncated gaussian distribution`?\n", "title": "Interesting idea but theoretical results and evaluation  are not satisfactory", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "U8tTGrX0GUA": {"type": "rebuttal", "replyto": "VORfIfTRfVD", "comment": "To Q1: Indeed, LISTA performs worse when the sparsity level of the test data is different from that of the training data, and our experimental results validate this. However, introducing our EBT mechanism largely mitigates the performance degradation (see Figure 5 in the updated paper). We have also added experimental results in Figure 5(a) to show how the models perform under larger distribution shifts, and it can be seen that the superiority of our method is more significant under such circumstances.\n\nTo Q3: We have actually added an explanation at the very beginning of Section 4 for the assumption that $s$ being sufficiently small (highlighted in blue in the current version of the paper). As explained in our previous response to your comments, we aimed to say $\\mu(A)s\\ll 1$ when mentioning $s$ being sufficiently small. We have added more discussions in Section 5.1 about such an assumption and how it would affect the empirical sparse coding results when our EBT is used. Comparing experimental results on $p_b=0.8$, $p_b=0.9$, and $p_b=0.95$ in Figure 7 and Figure 9, we can see that the performance gain of our EBT mechanism on the basis of LISTA and its variants is larger with $p_b=0.95$, for which the assumption of a sufficiently small $s$ is more likely to hold. \n\nTo Q4: We appreciate the suggestion and we have now added experimental results in a setting where a larger distribution gap exists (see Figure 5(b) in the updated paper). Specifically, for Figure 5(a), the ratio of non-zero elements in the test sparse codes (with $p_b=0.99$) is generally 20x smaller than in the training sparse codes (with $p_b=0.8$) and our method still performs favourably well.\n\nTo Q6 and Q7: Indeed, the assumption of our theory mostly follows that of prior theoretical work, and we would like to study them empirically and report results in a future version of the paper, due to the time limit of the rebuttal period.\n", "title": "Further clarifications"}, "XlJIqubq3Ba": {"type": "rebuttal", "replyto": "lsMb7BH-r8g", "comment": "We would like to thank the reviewer again for recognizing our technical contributions and providing further suggestions. We have done an extra experiment to test the model performance in dealing with larger sparsity shifts (e.g., from 0.8 to 0.99). The results show that the superiority of our method is more significant under larger sparsity shifts (see Figure 5(a) in the updated paper). Following the suggestions from AnonReviewer1, we have revised the paper to add more explanations and remove imprecise statements (e.g., we have pointed out that for a sufficiently small $s$, we meant $\\mu(A)s \\ll 1$ at the beginning of Section 4). We will definitely keep revising the paper to improve the organization and highlight the theoretical contributions.", "title": "Thanks for the further comments"}, "ulm9BAFVOL7": {"type": "rebuttal", "replyto": "LeFq8XDkxgW", "comment": "We thank the reviewer for the constructive comments. Our responses are given as below:\n\nQ1 & Q2: Motivation of the work when there is a discrepancy between train/test distribution, and the usage in real sparse coding problems.\n\nA: Similar to LISTA and its variants, our work considers a setting for model training where a set of observations paired with the corresponding ground-truth sparse codes are used. We believe that the discrepancy between train/test distribution arises in many practical applications. For instance, in the task of photometric stereo analysis, if the training and test observations are obtained from two different devices, it is highly likely that the sparsity of the training and test noise vectors are different. In addition, in many practical applications, as has been mentioned by the reviewer, we may not be given the ground-truth sparse representations of the observations, yet, with the dictionary matrix $A$, we can still simulate a set of sparse codes together with the observations $\\{(y,x_s)\\}$ for training, which leads to a discrepancy between the train and test distribution. Our method can be used to handle such scenarios.\n\nQ3: Theory results are not precise and the assumptions are not mild.\n\nA3: We appreciate the pointer to imprecise statements. We aimed to say $\\mu(A)s \\ll 1$ when mentioning $s$ being sufficiently small. Note that in the prior work, $s$ is also assumed to be sufficiently small to satisfy $\\mu(A)(2s-1)<1$ (see Theorem 2 in [2] and Theorem 1 in [3]). Our assumption is given in a similar manner. Experimental results in Figure 6(a) ($p_b=0.95$), Figure 9(a) ($p_b=0.9$), and Figure 9(b) ($p_b=0.8$) also show that our EBT mechanism leads to better performance when $p_b$ is larger, i.e., when $s$ is smaller, which well validates our theory. In addition, even without such an additional assumption, we still have some theoretical results that our EBT-LISTA and EBT-LISTA-SS converge faster than LISTA and LISTA-SS with probability (see our proof). \nOverall, we argue that the theoretical contribution of our work is significant. We not only demonstrate how much performance gain our EBT can achieve in theory but also shed light on the two convergence phases of the variants of LISTA.\n\nQ4: The experiments use the same setting as previous work, failing to highlight the advantage of the proposed method.\n\nA4: We followed some common practice to design experiments for fair comparisons with prior work, and, in order to show the advantages of our method more clearly in some concerned settings, we have shown more results in the two paragraphs of \u201cAdaptivity to unknown sparsity\u201d (page 8) and \u201cRandom sparsity\u201d (page 11) in our paper. Experimental results demonstrate that our EBT mechanism leads to superior performance than that of all competitors. We encourage the reviewer to take a closer look at the two paragraphs for more details.\n\nQ5: Comment on Figure 2 for our empirical advantage.\n\nA5: Figure 2 is shown to validate our theoretical results that many variants of LISTA have two convergence phases and our EBT accelerates, in particular, the first phase. We have revised the caption of Figure 2 to make our statement more concrete. \n\nQ6: Comment on \u201cNormal training of LISTA leads to it\u201d.\n\nA6: We thank the reviewer for pointing out the confusing statement. We aimed to say that linear convergence can be obtained if $b^{(t)} = \\mu(A)\\mathrm{sup}_{i=0,1,\\ldots,n}\\Vert x_i^{(t)}-x_{s_i}\\Vert_p$ and some other conditions (including $U^{(t)}\\in \\mathcal{W}(A)$ as mentioned) hold. This is a theoretical result of prior work (see Eq. (30) in [2]), and we have added proper reference there in the updated version of our paper.\n\nQ7: Comment on $U^{(t)}\\in \\mathcal{W}(A)$.\n\nA7: $U^{(t)}\\in \\mathcal{W}(A)$ is also one of the assumptions for guaranteeing linear convergence of LISTA [2][3][4] (see Theorem 2 in [2], Theorem 1 in [3], and Proposition 1 in [4] for more details). We have revised the statement in the updated version of our paper.", "title": "Response to AnonReviewer1"}, "89nTa4NuIm": {"type": "rebuttal", "replyto": "dESHMouLwP2", "comment": "Thanks for the positive feedback and comments. Our responses are given as below:\n\nQ1: Whether the new parameterization used in EBT changes the training dynamics or not.\n\nA1: Following the suggestion, we have redrawn Figure 3 in the paper for better illustration of the training dynamics. In the figure, we can see that the learned thresholds in our EBT-based methods and the original LISTA and LISTA-SS are similar, which indicates that the introduced EBT mechanism does not modify the training dynamics of the original methods, and it works by disentangling the reconstruction error and learnable parameters.\n\nQ2: How many samples are used for training?\n\nA2: Following prior work [1] [2] [3], we synthesized in-stream data for training all models in comparison, thus the number of training samples grows as the training proceeds. Since all models were trained for the same number of iterations, they all share the same training complexity.\n\nQ3: Mistakes in eqn (15).\n\nA3: Thanks for pointing out. In our paper, $L^{\\dagger}$ is the orthogonal complement of $L$ rather than the pseudo-inverse, and thus Eq.(14) can be rewritten as $$ L^{\\dagger}o = \\rho L^{\\dagger}Ln + L^{\\dagger}e = L^{\\dagger}e.$$ We have revised Eq.(15) in the latest version of the paper for better clarity.\n\n[1]Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of unfolded ISTA and its practical weights and thresholds. In Advances in Neural Information Processing Systems (NeurIPS), pp. 9061\u20139071, 2018.\n\n[2] Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. ALISTA: Analytic weights are as good as learned weights in LISTA. In Proceedings of the International Conference on Learning Representations (ICLR), pp. 1113\u20131117, 2019. \n\n[3] Kailun Wu, Yiwen Guo, Ziang Li, and Changshui Zhang. Sparse coding with gated learned ISTA. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.\n", "title": "Response to AnonReviewer2"}, "JFUykbgLHYK": {"type": "rebuttal", "replyto": "LeFq8XDkxgW", "comment": "Q8: Evaluate the usage of $\\rho^{(t)}=\\mu(A)$.\n\nA8: We appreciate the suggestion of testing $\\rho^{(t)}=\\mu(A)$. We have run the experiment based on LISTA and LISTA-SS (the new methods are codenamed EBT-LISTA-fixed and EBT-LISTA-SS-fixed). The below table compares the result (shown in dB) of EBT-LISTA, EBT-LISTA-SS, EBT-LISTA-fixed, and EBT-LISTA-SS-fixed. It can be seen that using $\\rho^{(t)}=\\mu(A)$ does not lead to superior performance that that of our original EBT mechanism.\n\n|  Method   | Step 10  | Step 16 |\n|  :----  | :----:  | :----:|\n| EBT-LISTA-fixed  | -20.78 | -22.47|\n| EBT-LISTA  | -22.14 | -37.48|\n|EBT-LISTA-SS-fixed| -40.83 | -64.39|\n|EBT-LISTA-SS| -45.24 | -68.73 |\n\n\nQ9: Comment on \u201cmore detailed description\u201d in our discussions about the assumption.\n\nA9: Indeed, we further introduce an assumption about the distribution of the sparse codes, and we believe that it is necessary for studying LISTA in scenarios where there exists a discrepancy between training and test data. We have rephrased the paper to state that \u201ca more detailed yet also stricter description\u201d is provided.\n\nQ10: Minor comments, nitpicks and typos.\n\nA10: Thanks for pointing out the typos and making suggestions on our writing. We have revised the paper accordingly. Besides, we would like to clarify that in LISTA [1], the thresholds are actually learnable parameters, which is different from ISTA. The related descriptions can be found in [1] (its Section 3.3, page 6).\n\n[1] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 399\u2013406. Omnipress, 2010\n\n[2] Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of unfolded ISTA and its practical weights and thresholds. In Advances in Neural Information Processing Systems (NeurIPS), pp. 9061\u20139071, 2018.\n\n[3] Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. ALISTA: Analytic weights are as good as learned weights in LISTA. In Proceedings of the International Conference on Learning Representations (ICLR), pp. 1113\u20131117, 2019. \n\n[4] Kailun Wu, Yiwen Guo, Ziang Li, and Changshui Zhang. Sparse coding with gated learned ISTA. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.\n", "title": "Response to AnonReviewer1 (Continued)"}, "RN8NdxtN1bu": {"type": "rebuttal", "replyto": "dOelrnEa4Kc", "comment": "Thanks for the positive feedback and constructive suggestions. We have revised the paper accordingly. The architecture of LISTA and our proposed EBT-LISTA has been shown in Section 3, and Table 1 has been modified. The results of EBT-ISTA and EBT-FISTA have been shown in Appendix A.1 (Figure 11). Specifically, introducing our EBT mechanism in the classical ISTA and FISTA leads to faster initial convergences but worse final performance. This is because ISTA and FISTA converge in a sub-linear manner, but our EBT is mostly proposed for methods that converge linearly, which might cause a mismatch to our assumptions.", "title": "Response to AnonReviewer3"}, "N29QmkKSEO": {"type": "rebuttal", "replyto": "tT8GftLKBU1", "comment": "We would like to thank the reviewer for the positive feedback.", "title": "Response to AnonReviewer4"}, "tT8GftLKBU1": {"type": "review", "replyto": "UfJn-cstSF", "review": "In the paper, authors propose a new error-based thresholding mechanism for LISTA which introduces a function of the evolving estimation error to provide each threshold in the shrinkage functions. They provided the theoretical analysis for EBT-LISTA and EBT-LISTA with support selection and proved that the  estimation error of the proposed algorithm is theoretically lower than compared methods. The authors also evaluated the proposed method on multiple synthetic or real tasks. Experimental results show that the proposed method achieves a better estimation error and  higher adaptivity to different observations with a variety of sparsity.\n\n", "title": " A new error-based thresholding mechanism for LISTA with theoretical guarantee", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "dESHMouLwP2": {"type": "review", "replyto": "UfJn-cstSF", "review": "This paper disentangles the threshold parameters in LISTA-type models from the reconstruction errors, proposing the Error-Based Threholding (EBT) mechanism which mainly follows a theoretical results in (Chen et al., 2019; Liu et al., 2018), where the threshold at one layer is proportional to the recovery error of current iterate. The benefits brought by the proposed EBT method are faster convergence and better adaptivity to a wider range of samples. To bypass the requirement of ground truth sparse signals, EBT uses the reconstruction error following a learned linear transform, which in theory has good coherence property with the dictionary and therefore can approximate the recovery error well. The authors theoretically show that the proposed EBT mechanism enjoys faster convergence in both cases with and without the support selection technique. Emprirical experiments on standard synthetic setting and cross-sparsity setting are shown to support the efficacy of EBT. The authors also do real-world photometric stereo analysis to show the superiority of EBT.\n\nPros:\n- This paper is a successful extention of basic LISTA-type models by disentangling the learnable threshold parameters.\n- Theoretical analysis of the benefits of EBT is provided and looks correct to me.\n- The empirical experiments are solid enough to show the superiority of EBT.\n\nCons:\n- My main concern is about that this paper is a little bit incremental, as it seems to be a very direct extension based on previous theoretical results.\n\nOther comments:\n- In Figure 1, it can be observed that the thresholds learned LISTA-SS have a bumpy curve, which is also observed in previous works. In contrast, the $\\rho^{(t)}$ parameters look much stabilized. It might be better to also show the curves of reconstruction error term, i.e. the $\\ell_p$ term in eqn (12) to see how they look like. This would help to understand if the new parameterization used in EBT changes the training dynamics or not.\n- In the basic settings part, the numbers of validation and testing samples are provided. How many samples are used for training?\n- In eqn (15), is the term about $\\rho n$ omitted on purpose, or mistakenly?\n\nIn summary, I think this is a good extension paper but I have a little concern about its incremental contribution. Overall I think it is slightly above the threshold. I am open to more opinions from other reviewers.\n", "title": "A very good extention to LISTA-type models but looks a little bit incremental", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "dOelrnEa4Kc": {"type": "review", "replyto": "UfJn-cstSF", "review": "Summary:\nThe authors propose an automatic threshold selection for LISTA-style neural nets. The threshold introduces negligible number of parameters (either 0, or one per layer). This choice is shown theoretically and empirically to have faster convergence than methods without it and popular alternatives.\n\nClarity:\nThe clarity of your idea would be improved using a graphic: for example showing the LISTA architecture, and what sort of computations are performed in a \"feed in\" direction toward the shrinkage operator. Just a thought, not a criticism. The information in Table 1 might be more succinctly reported using exponential notation, or speedup as a percentage (I count 39 extraneous 0's)\n\nQuality:\nThe quality would be improved if you applied this adaptive thresholding idea to classic (F)ISTA, i.e. updating (F)ISTA's thresholds via the parameterless form of EBT. How would this impact standard (F)ISTA convergence, i.e., compared to using a fixed threshold run on a whole test set? (is ebt-threshold selection better than say, doing a parameter search for fixed threshold \"b\", over a training set on (F)ISTA?)\n\nThe experiment variety is of good quality: experiments across condition numbers and sparsities, validation of theorems, etc.\n\nOriginality: \nI am not familiar with other works like this, and I like the idea. I wish we were given insight to how it affects nonlearned (F)ISTA algorithms, which are still widely used in practice.\n\nSignificance:\nThe result is significant in the specific application of LISTA.", "title": "automatic threshold selection for LISTA-like nets", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}