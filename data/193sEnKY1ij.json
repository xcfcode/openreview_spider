{"paper": {"title": "No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks", "authors": ["Shyamgopal Karthik", "Ameya Prabhu", "Puneet K. Dokania", "Vineet Gandhi"], "authorids": ["~Shyamgopal_Karthik1", "~Ameya_Prabhu1", "~Puneet_K._Dokania1", "~Vineet_Gandhi2"], "summary": "Conditional risk framework exploiting the label hierarchy outperforms state of the art and makes a strong baseline for future explorations.", "abstract": "There has been increasing interest in building deep hierarchy-aware classifiers that aim to quantify and reduce the severity of mistakes, and not just reduce the number of errors. The idea is to exploit the label hierarchy (e.g., the WordNet ontology) and consider graph distances as a proxy for mistake severity. Surprisingly, on examining mistake-severity distributions of the top-1 prediction, we find that current state-of-the-art hierarchy-aware deep classifiers do not always show practical improvement over the standard cross-entropy baseline in making better mistakes. The reason for the reduction in average mistake-severity can be attributed to the increase in low-severity mistakes, which may also explain the noticeable drop in their accuracy. To this end, we use the classical Conditional Risk Minimization (CRM) framework for hierarchy-aware classification. Given a cost matrix and a reliable estimate of likelihoods (obtained from a trained network), CRM simply amends mistakes at inference time; it needs no extra hyperparameters and requires adding just a few lines of code to the standard cross-entropy baseline. It significantly outperforms the state-of-the-art and consistently obtains large reductions in the average hierarchical distance of top-$k$ predictions across datasets, with very little loss in accuracy. CRM, because of its simplicity, can be used with any off-the-shelf trained model that provides reliable likelihood estimates.", "keywords": ["Hierarchy-Aware Classification", "Conditional Risk Minimization", "Post-Hoc Correction"]}, "meta": {"decision": "Accept (Poster)", "comment": "The approach explore the use of Conditional Risk Minimization (CRM) as a post-hoc operation to amend a classifier decision by averaging a prior class hierarchy. The authors show that it is beneficial for ranking predictions without sacrifying top-1 accuracy.\n\nThe rebuttal period clarified some reviewers' concern on paper presentation and experiments, and all reviewers recommend acceptance after the discussion period.\n\nAlthough the approach is simple and directly revisits the use of CRM for deep models, the AC considers that the contribution is meaningful, and that the proposed method provides predictions with good ranking and calibration properties. The paper also sheds light into interesting issues in state-of-the-art methods integrating class hierarchies during training.\nThe AC therefore recommends paper acceptance.\n"}, "review": {"xgVekD79bPM": {"type": "rebuttal", "replyto": "I6ccwnJpZqo", "comment": "We thank AR3 for their active engagement and constructive feedback. We are very happy the confusion was resolved, and they found our method useful for tackling this problem. We agree with the concerns AR3 raises, and hope to improve the writing -- especially in Section 4.1, which could be misunderstood due to confusion about Fig 2.\n\nWe have updated the draft with the following changes:\n\n> (4.1) badly communicated, dodgy statements\n\n- We rewrote Section 4.1 entirely to eliminate any such statements, reframing it similar to our rebuttal comment about Fig 2 but with more clear, concise statements which we hope entirely resolves this issue.  \n\n> The paper/results section should also be very clearly structured to indicate that there are 2 different but related points being made\n\n- We restructured the results subsection by replacing it with three subsections, each  corresponding to a different aspect in hierarchy-aware classification\n  - 4.1 being making better top-1 mistakes\n  - 4.2 being about ranking classes aka top-k mistakes across k\n  - 4.3 talking about calibration aspects of the problem\nThis structure aligns better with the three points stated in the contributions (introduction).\n\nWe hope this fully resolves their concerns about structure and clarity.", "title": "Thanks for active feedback! We made changes according to suggestions, and hope to resolve the clarity aspect"}, "O15ECKOa1ui": {"type": "rebuttal", "replyto": "NNw4YBVKmzz", "comment": "We trained 2 vectors  using this equation exp(alpha_i x + beta_i) / \\sum_{i=1}^N exp(alpha_i x + beta_i) which modify the logits in a post-hoc manner. This is done by minimizing the Negative Log Likelihood (NLL) loss on the validation set. We used the Adam optimizer with a learning rate of 0.1 and trained it for 10 epochs. The model is then evaluated on the test set.\n\n**Tiered-ImageNet**\n\n| Model |  ECE(Temp.)  | ECE(Vector) | Accuracy(Temp.) | Accuracy(Vector) |  Hier Dist@5(Temp.) | Hier Dist@5(Vector) | Hier Dist@20(Temp.) | Hier Dist@20(Vector) |\n| :------------------- |:----------|:------------|:------------|:-------------|:------|:------------|:------------|:-------------|\n| Cross-Entropy        | 1.61      | 1.35        | 68.76        | 69.03         | 5.64  | 5.64        | 7.08        | 7.08         |\n| Soft-Labels(Beta=4)  | 11.12     | 6.74        | 58.78        | 48.21         | 5.19  | 5.36        | 6.26        | 6.40         |\n| Soft-Labels(Beta=5)  | 10.92     | 7.83        | 62.78        | 56.26         | 5.20  | 5.28        | 6.28        | 6.36         |\n| Soft-Labels(Beta=10) | 6.36      | 7.24        | 67.63        | 67.28         | 5.42  | 5.44        | 6.60        | 6.65         |\n| Soft-Labels(Beta=15) | 4.79      | 2.74        | 68.88        | 69.13         | 5.55  | 5.55        | 6.84        | 6.87         |\n| HXE(Alpha=0.2)       | 1.53      | 1.56        | 68.23        | 68.51         | 5.54  | 5.54        | 6.91        | 6.92         |\n| HXE(Alpha=0.4)       | 2.44      | 2.37        | 65.88        | 65.55         | 5.48  | 5.47        | 6.79        | 6.80         |\n| HXE(Alpha=0.5)       | 2.61      | 4.01        | 64.22        | 64.00         | 5.46  | 5.46        | 6.75        | 6.76         |\n| HXE(Alpha=0.6)       | 3.28      | 6.19        | 57.70        | 57.63         | 5.47  | 5.47        | 6.67        | 6.68         |\n\n**iNaturalist19**\n\n| Model |  ECE(Temp.)  | ECE(Vector) | Accuracy(Temp.) | Accuracy(Vector) |  Hier Dist@5(Temp.) | Hier Dist@5(Vector) | Hier Dist@20(Temp.) | Hier Dist@20(Vector) |\n| :------------------- |:----------|:------------|:------------|:-------------|:------|:------------|:------------|:-------------|\n| Cross-Entropy        | 1.42      | 2.01        | 59.74        | 59.22         | 1.83  | 1.84        | 2.85        | 2.86         |\n| Soft-Labels(Beta=4)  | 11.46     | 5.73        | 20.18        | 9.18          | 1.55  | 1.63        | 2.00        | 2.11         |\n| Soft-Labels(Beta=5)  | 17.20     | 5.75        | 26.10        | 13.92         | 1.50  | 1.54        | 1.97        | 2.04         |\n| Soft-Labels(Beta=10) | 19.87     | 26.55       | 46.61        | 42.24         | 1.41  | 1.43        | 1.94        | 1.96         |\n| Soft-Labels(Beta=15) | 16.63     | 24.31       | 53.28        | 50.53         | 1.42  | 1.43        | 1.98        | 1.99         |\n| HXE(Alpha=0.2)       | 1.50      | 0.58        | 58.50        | 58.25         | 1.66  | 1.65        | 2.53        | 2.54         |\n| HXE(Alpha=0.4)       | 1.13      | 2.89        | 54.29        | 53.60         | 1.57  | 1.57        | 2.33        | 2.33         |\n| HXE(Alpha=0.5)       | 2.46      | 5.21        | 50.40        | 49.50         | 1.56  | 1.56        | 2.27        | 2.26         |\n| HXE(Alpha=0.6)       | 5.24      | 5.99        | 44.34        | 42.97         | 1.58  | 1.58        | 2.24        | 2.24         |\n\n\nWe observe from the table that:\n - As predicted, Softlabels beta=4/5 get better calibrated (far lower ECE) but the hierarchical corrections get undone -- regressing significantly on all performance metrics-- accuracy and average hierarchical dist@5/20 significantly.\n - In other cases there isn't a significant impact on the hierarchical corrections since the change in calibration is slight. However, similar to observations in Guo etal [1] -- vector scaling does not necessarily achieve better calibration compared to temperature scaling in all cases, it often overfits to the validation set.", "title": "Vector scaling results"}, "e9K6CxkXgn": {"type": "review", "replyto": "193sEnKY1ij", "review": "Summary: \nThe authors propose a model to improve the output distribution of neural nets in image classification problems. Their model is a post hoc procedure and is based on the tree structure of WordNet. The model revises the classifier output based on the distance of the labels in the tree. Intuitively, their solution is to pick the candidate label that is located in the region of the tree with a higher accumulated probability mass value. They also experimentally show that the previous evaluation metrics are inconclusive. \n\nPros:\n- The authors provide a different perspective on the evaluation procedure of the previous studies and experimentally show that it was incomplete. This is an important finding.\n- Their experiments are thorough.\n\nCons:\n- The article lacks enough novelty: The problem has been investigated before. The solution is not novel. The WordNet tree structure has been extensively used in the information retrieval community before.\n- The article is not written well: There are informal vocabulary in the paper (e.g., \u201csomething similar\u201d or \u201cgrossly miscalibrated\u201d). There are also typos (e.g., see the paragraph before Theorem 1). In Section 3 it is not formally stated that the tree structure is derived from WordNet (the authors mention this in Abstract section). In Section 4 the baselines are not cited!", "title": "The authors show that previous related studies have used an incomplete metric for evaluation, which is an important finding. But their solution lacks enough novelty.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "O4LEe0Vuiet": {"type": "review", "replyto": "193sEnKY1ij", "review": "This paper addresses the problem of hierarchy-aware classification, which utilizes a hierarchy to specify certain mistakes as being worse than others. They make two main contributions. \n\nFirst, they claim that the metric used in prior work, average mistake severity, is flawed because it rewards methods that make many \"easy\" mistakes, as opposed to fewer, \"harder\" mistakes. Based on this analysis, they claim that no prior methods actually improve over the simplest baseline of cross-entropy (i.e. doing nothing).\n\nSecond, they introduce their own method, an adaptation of the classical CRM framework. They argue that CRM:\n1) Is the only method to improve over cross-entropy under average mistake severity (the metric from prior work they argue is flawed)\n2) Beats all other metrics under their new, improved metric\n3) Improves on the calibration of the predictions\n\nStrengths:\nThe method is very simple and easy to understand, builds upon existing work, and I could probably implement it from scratch in 30 minutes on top of my existing models. While ML reviewers very frequently about a lack of complexity, I think this is a great strength.\n\nThe methods section is very clearly written, and it was quite easy to quickly understand what the method is doing.\n\nWeaknesses:\nI am very concerned about the experiment sections. \n\n1)\nTo my understanding, Figure 2/Section 4.1 are factually incorrect. In particular, it appears that the soft-labels technique does essentially the same, or better than, CRM, across all fronts. In detail,\na) \nIn Figure 2(a), the leftmost softlabel point is equal to or better than CRM (and cross-entropy)\n\nb) \nFigure 2(b) really concerns me, as it appears that the hyperparameters have been chosen to make a fairly narrow point - that it is possible to have low average hierarchical distance, and high top-1 error. I agree that that indicates a problem with the metric. \n\nHowever, the authors make the fair broader claim that CRM is the only method which beats cross-entropy, which I do not think is justified. Looking at Figure 4 in A.1, it is readily apparent that choosing different hyperparameters for existing methods would yield similar error distributions to CRM. Having these plots in the appendix, combined with claims that the authors chose the best hyperparameters, feels a bit misleading.\n\nc)\nAs in 1), soft labels is essentially on top of CRM and Cross entropy (for iNaturalist19, it looks like a higher beta value would be directly on top, it's unclear why the authors did not extend the curve further)\n\n2)\nThese results, at first blush, seem fairly impressive. For the leftmost plots, I am concerned that the authors are using subpar hyperparameters, similarly to 1)(b) above. Strangely, in this instance the results for other hyperparameters are not included in the appendix. The remaining experiments are fairly convincing, though.\n\nReccomendation\nI do not think this paper can be accepted in its current form. While I suspect that CRM is a good method that I would like to use, some of the core arguments (Figure 2/Section 4.1) in the paper appear to be fatally flawed.\n\nSmaller notes:\n- The paper could use an additional proofread, as there are often odd phrasings. I found the experiment section particularly hard to follow\n- The acronym HXE is never defined, or linked to a citation", "title": "Promising, simple method but flawed experiments/arguments", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Kgz-ozfFbW1": {"type": "rebuttal", "replyto": "e9K6CxkXgn", "comment": "We thank the reviewer for their thoughtful feedback. \n\n> The article lacks enough novelty: The problem has been investigated before. The solution is not novel.\n\nWe would like to highlight the following here:\n - As opposed to all recent literature in hierarchy-aware classification, our approach *does not require retraining*\n - Previous works often introduce algorithmic novelty by designing ad-hoc loss functions and retraining models on ImageNet with sensitive additional hyperparameters, which we demonstrate do not improve over the cross-entropy baseline in making better mistakes. \n- Our simple method beats all existing sophisticated methods with no retraining in ranking classes on hierarchical distance@k by large margins while preserving far better calibration necessary for real-world use.\n- The novelty we introduce is not in the algorithm itself, but primarily in uncovering important shortcomings in literature, demonstrating a simple, classical approach which works effectively when combined with deep models and use its simplicity to provide insights into the problem (Theorem 1).\n - We believe our method can have a major real-world impact due to its simplicity and deployability with no additional training.\n\nWe request the reviewer to reconsider and not dismiss the paper solely on this ground.\n\n> In Section 3 it is not formally stated that the tree structure is derived from WordNet\n\nThe taxonomy need not be from WordNet-- approaches should work with any given hierarchy. We also demonstrate this in experiments with the large-scale dataset iNaturalist-19 which uses the biological taxonomy. We have added this in Section 3 for better clarity.\n\n> informal vocabulary, typos and missing citations.\n\nThanks for pointing this out! We have corrected informal vocabulary, added detail to make crisper statements, fixed all typos and missing citations issues in the updated draft. Hope it resolves this issue.\n", "title": "Request to not dismiss the paper solely because of lack of novelty"}, "YKwGV4bjgH": {"type": "rebuttal", "replyto": "O4LEe0Vuiet", "comment": "We thank the reviewer for their feedback. We also thank the reviewer for finding simplicity as one of the major strengths of our approach (CRM) as it does not require retraining, hyperparameter cross-validation, and can be used with almost no computational overhead. \n\nThe major questions/concerns raised by the reviewer primarily are related to Figure 2, evaluation metric, and hyperparameters. Below we discuss them all in detail and hope to resolve all the concerns. We hope the reviewer makes an additional pass over the experiment section after resolving the confusion for better clarity. \n\nFigure 2\n\nPerformance: Note that the purpose of Figure 2 is to show how current metrics to evaluate mistake severity do not reflect the true nature of the models. We do this by highlighting that the current evaluation of mistake severity, which is evaluated _only_ over the incorrectly classified samples (hence _different_ test sets for different models as the mistakes will be different), will favour models making additional _easy mistakes_ (as it involves division by the number of mistakes) which is undesirable. We propose an easy fix to correct this bias which involves evaluation of mistake severity on a _fixed_ test set (not only on the misclassified ones), and, as expected, this simple fix changes the evaluation significantly and provides a reliable evaluation. \n\nNow, looking at Fig 2(a) (the old way of evaluating only on mistakes), it seems like softlabels with $\\beta = 4$ and HXE with $\\alpha=0.6$ are the best models with the lowest hierarchical distance@1 over mistakes (average mistake severity) as seen on the bottom right. However, as we correct the above said bias, those very models show the highest hierarchical distance@1 (Fig 2(c) top right). And to answer why this is the case, the mistake distribution in Fig 2(b) clearly shows that this model is just making more easy mistakes while *not* improving at all the hierarchical mistakes over the entire test set.\n\nHyperparameters: We *did not* cherry-pick hyperparameters $\\alpha$ and $\\beta$ in Fig 2(b). They are the best-performing ones (refer [2]). In fact, we provided a proper analysis to show the impact of hyperparameter on the mistake distribution (Figure 4 in A.1). In the case of softlabels, as $\\beta$ decreases, the model is supposed to become better aligned towards the hierarchy with a tradeoff with top1 accuracy, but in reality, performance simply gets worse. Note, with increasing values of $\\beta$, the softlabel method becomes similar to cross-entropy. Practically, beyond $\\beta=50$ its performance converges, remaining almost the same. To make it more evident, we added four additional points corresponding to $\\beta$ = [50,75,100,200] in Figure 2 for iNaturalist19. As expected, they all converge to a point (refer Table2 in A.4 for exact values).\n\nTherefore, at high values of $\\beta$, softlabel is very similar to cross-entropy; otherwise, it merely adds easy mistakes. Please note, this observation is for hierarchical distance@1. As we have shown in Figure 3, softlabel performs better than cross-entropy for ranking classes measured by distance@k, but still worse than CRM.\n\n> For the leftmost plots (in Fig 3), I am concerned that the authors are using subpar hyperparameters.\n\nRegarding Figure 3, for a fair comparison, we picked the hyperparameters for other models where their best results occurred. We have added results with all other hyperparameters in the appendix (Fig 5&6) to justify this claim: We can observe that the best results mostly occur at $\\beta=4$ and $\\alpha=0.6$. We hope this addresses the reviewer\u2019s concern and shows that our method outperforms existing methods at their optimal parameters (y-axis is in log-scale w.r.t classes) both in terms of ranking classes in Fig 3 (left) and tradeoffs between hierarchical distance@k and accuracy in Fig 3 (centre and right). \n\n> The authors make the fair broader claim that CRM is the only method which beats cross-entropy, which I do not think is justified:\n \nWe have rephrased the suggested line in Section 4.1 to better align with our intended claim (as stated in abstract & introduction)-- the existing methods do not practically improve over the cross-entropy baseline when considering the top-1 hierarchical distance. \n\n> Odd phrasings, missing citations. HXE acronym\n\nThanks for pointing this out! We have fixed these in the latest draft for ensuring better clarity for readers.\n\n[2] Bertinetto et al., Making Better Mistakes: Leveraging Class Hierarchies with Deep Networks, CVPR20\n", "title": "Resolving confusion about Figure 2 and hyperparameters. Glad they found simplicity as one of the major strengths of our approach."}, "NNw4YBVKmzz": {"type": "rebuttal", "replyto": "4DJy7q1xmwO", "comment": "We thank the reviewer for their positive feedback. \n\n> Wouldn't it make more sense to train exp(alpha_i x + beta_i) / \\sum_{i=1}^N exp(alpha_i x + beta_i) instead of T?\n\nWe agree that there are many more sophisticated approaches to improve calibration; however since our focus was to show the effectiveness of CRM. We used temperature scaling as it has only 1 degree of freedom and does not affect the ranking of the classes or accuracy. The suggested approach, which is similar to vector scaling [1], and similarly more sophisticated ways to enforce calibration have an important issue: they are expected to significantly alter the weighting between different classes which might have an unintended effect on the hierarchical corrections made and impact their decisions-- hence we avoid them. \n\nWe are nevertheless performing experiments as requested based on this suggestion and most likely post our findings within the next two days. \n\n[1] Guo et al. On Calibration of Modern Neural Networks, ICML17\n", "title": "Thanks for the encouraging feedback. Glad they found our paper to be insightful and our experiments thorough"}, "7NZ3de_e5X4": {"type": "rebuttal", "replyto": "Z2JmBvqm_d", "comment": "We thank the reviewer for their positive feedback and are glad that they found our demonstration of CRM combined with modern deep learning tools impactful in its simplicity and intuition.\n\n> I wonder whether your approach is still effective for (multi-label) models using binary cross-entropy\n\nWe believe it would not be trivial to directly apply CRM, or any recent label-hierarchy approach directly with binary cross-entropy since there is no straightforward way to combine L sigmoids into one so that different probabilities can be compared. We did not think about this particular problem as the community at large is mostly focused on single-label classification; however, it definitely is an interesting question, and we would like to explore it properly in the future. Thank you for this proposal.\n\n> It would also be better if the authors can show some experimental results on hierarchical text classification.\n\nCould the reviewer point to a suitable hierarchical multi-class text classification datasets with an elaborate hierarchy (>3 depth)? We will experiment using cross-entropy and CRM corrections on them and do our best to post our findings if available before the closure of the discussion phase otherwise will add them in the draft.\n", "title": "Thanks for the positive feedback. Glad they found CRM combined with modern deep learning tools impactful in its simplicity and intuition"}, "Z2JmBvqm_d": {"type": "review", "replyto": "193sEnKY1ij", "review": "This paper proposes to use conditional risk minimization (CRM) for hierarchy aware classification. The proposed method simply amends mistakes using a cost matrix with the lowest common ancestor information. The method outperforms SOTA deep hierarchy-aware classifiers by large margins at ranking classes with little loss in classification accuracy.\n\nAs the authors mentioned, CRM was already proposed several decades ago, so the novelty of this paper is limited. However, the paper does demonstrate the power of this old technique when equipped with modern deep learning tools. Also, the simplicity and intuition are much appreciated.\n\nI myself am not an expert in image classification. However, for text classification, recent studies (e.g., [1]) shows that using binary cross-entropy (i.e., viewing a multi-label classification problem as L binary classification task, where L is the number of classes) can achieve higher performance than multi-label cross-entropy. In this case, it is possible that more than one label can have p(y|x) > 0.5. I wonder whether your approach is still effective for models using binary cross-entropy. It would also be better if the authors can show some experimental results on hierarchical text classification.\n\n[1] Liu et al. Deep Learning for Extreme Multi-label Text Classification. SIGIR'17.", "title": "A straightforward technique with satisfying performance", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "4DJy7q1xmwO": {"type": "review", "replyto": "193sEnKY1ij", "review": "The paper addresses hierarchical classification, where the classes live in a hierarchy, and the cost of a mistake is the tree distance between the nodes.\n\nThe paper tests the latest cool algorithms for hierarchical-aware loss functions, versus a very old idea: CRM. In CRM, you make your best estimate of the posterior probability of a class y given input x P(y|x), and then you make a final decision based on minimizing the expected loss.\n\nThere seems to be a belief that modifying the loss function to be hierarchy-aware is clearly better than doing boring old CRM. But there is not much evidence in favor of that hypothesis. This paper offers negative evidence for that hypothesis, with two experiments:\n\n1.  By comparing hierarchical loss to top-1 loss with modified loss functions, there is a tradeoff, and there does not seem to be an advantage in using the modified loss function.\n2.  For the top-k case, using CRM clearly dominates the proposals for modifying the loss function.\n\nThese support the use of CRM.\n\nI find this paper to be really nice -- I'd far rather have a paper with good experiments with known algorithms, where I can learn something useful; than a paper with a new algorithm with somewhat useless experiments. So I would argue for acceptance.\n\nOne thing for the authors to think about:\n\nWhen they test the calibration of the modified loss functions, they find them to be poorly calibrated. This is not surprising, since the modified loss functions are not proper scoring rules. They attempt to calibrate by using a softmax  with variable T. Wouldn't it make more sense to train exp(alpha_i x + beta_i) / \\sum_{i=1}^N exp(alpha_i x + beta_i) ? that is, a gain and offset for all classes after the first one?\n", "title": "A nice paper that thoroughly tests CRM for hierarchical classification using latest NNs", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}