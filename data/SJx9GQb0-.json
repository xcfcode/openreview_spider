{"paper": {"title": "Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect", "authors": ["Xiang Wei", "Boqing Gong", "Zixia Liu", "Wei Lu", "Liqiang Wang"], "authorids": ["yqweixiang@knights.ucf.edu", "boqinggo@outlook.com", "zixia@knights.ucf.edu", "luwei@bjtu.edu.cn", "lwang@cs.ucf.edu"], "summary": "", "abstract": "Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n", "keywords": ["GAN", "WGAN"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes various improvements to Wasserstein distance based GAN training. Reviewers agree that the method produces good quality samples and are impressed by the state of the art results in several semi-supervised learning benchmarks. The paper is well written and the authors have further improved the empirical analysis in the paper based on reviewer comments."}, "review": {"SkNSwnmrG": {"type": "rebuttal", "replyto": "BJNCqPqxM", "comment": "AnonReviewer2: \"But still, without the analysis of the temporal ensembling trick [Samuli & Timo, 2017]\" \n\nWe have actually reported the ablation study about this temporal ensemebling technique in the rebuttal. Please read our answer to Q4 in the rebuttal. \n\n=Q4=\n\"... which part of the model works\"\n\nPlease see either Appendix C of the revised paper or the following for our answer to this question.\n\nWe have done some ablation studies about our semi-supervised learning approach on CIFAR-10. \nMethod, Error\nw/o CT, 15.0\nw/o GAN (note 1), 12.0\nw batch norm (note 2), --\nw/o D_, 10.7\nOURS, 10.0\n\nNote 1: This almost reduces to TE (Laine & Aila, 2016). All the settings here are the same as TE except that we use the extra regularization ($D\\_(.,.)$ in CT) over the second-to-last layer.\nNote 2: We use the weight normalization as in (Salimans et al., 2016), which becomes a core constituent of our approach. The batch normalization would actually invalidate the feature matching in (Salimans et al., 2016).\n\nWe can see that both GAN and the temporal ensembling effectively contribute to our final results. The results without our consistency regularization (w/o CT) drop more than those without GAN. We are running the experiments without any data augmentation and will include the corresponding results in the paper.", "title": "About the analysis of the temporal ensembling trick [Samuli & Timo, 2017]"}, "r17cU27HG": {"type": "rebuttal", "replyto": "BJNCqPqxM", "comment": "Following (Laine & Aila, 2016, Miyato et al., 2017, Tarvainen & Valpola, 2017), we do not apply any augmentation to MNIST and yet augment the CIFAR10 images in the following way. We flip the images horizontally and randomly translate the images within [-2,2] pixels horizontally. \n\nSamuli Laine and Timo Aila.  Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.\nAntti Tarvainen and Harri Valpola.  Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017.", "title": "About data augmentation"}, "rJJbI3QHM": {"type": "rebuttal", "replyto": "B11HQF84M", "comment": "FYI, we have finally finished the experiments on LSUN-Bedroom. The results are comparable to those reported in (Gulrajani et al. 2017) except that our generated images are more diverse in terms of the color theme. \n\n1. https://goo.gl/MvK2x8\n2. https://goo.gl/Cidqgu\n3. https://goo.gl/f6WMeJ\n4. https://goo.gl/N3Jc6M\n5. https://goo.gl/XCpmaK", "title": "Experiments on LSUN finished"}, "BJNCqPqxM": {"type": "review", "replyto": "SJx9GQb0-", "review": "Updates: thanks for the authors' hard rebuttal work, which addressed some of my problems/concerns. But still, without the analysis of the temporal ensembling trick [Samuli & Timo, 2017] and data augmentation, it is difficult to figure out the real effectiveness of the proposed GAN. I would insist my previous argument and score. \n\nOriginal review:\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nThis paper presented an improved approach for training WGANs, by applying some Lipschitz constraint close to the real manifold in the pixel level.  The framework can also be integrated to boost the SSL performances. In experiments, the generated data showed very good qualities, measured by inception score. Meanwhile, the SSL-GANs results were impressive on MNIST and CIFAR-10, demonstrating its effectiveness. \n\nHowever, the paper has the following weakness: \n\nMissing citations: the most related work of this one is the DRAGAN work. However, it did not cite it. I think the author should cite it, make a clear justification for the comparison and emphasize the main contribution of the method. Also, it suggested that the paper should discuss its relation to other important work, [Arjovsky & Bottou 2017], [Wu et al. 2016].\n\nExperiments: as for the experimental part, it is not solid. Firstly, although the SSL results are very good, it is guaranteed the proposed GAN is good [Dai & Almahairi, et al. 2017]. Secondly, the paper missed several details, such as settings, model configuration, hyper-parameters, making it is difficult to justify which part of the model works. Since the paper using the temporal ensembling trick [Samuli & Timo, 2017],  most of the gain might be from there. Data augmentation might also help to improve. Finally, except CIFAR-10, it is better to evaluate it on more datasets. \n\nGiven the above reason, I think this paper is not ready to be published in ICLR. The author can submit it to the workshop and prepare for next conference. ", "title": "Official review for paper 1144", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkbRNKwef": {"type": "review", "replyto": "SJx9GQb0-", "review": "Summary:\n\nThe paper proposes a new regularizer for wgans, to be combined with the traditional gradient penalty. The theoretical motivation is bleak, and the analysis contains some important mistakes. The results are very good, as noticed by the comments, the fact that the method is also less susceptible to overfitting is also an important result, though this might be purely due to dropout. One of the main problems is that the largest dataset used is CIFAR, which is small. Experiments on something like bedrooms or imagenet would make the paper much stronger. \n\nIf the authors fix the theoretical analysis and add evidence in a larger dataset I will raise the score.\n\nDetailed comments:\n\n- The motivation of 1.2 and the sentence \"Arguably, it is fairly safe to limit our scope to the manifold that supports the real data distribution P_r and its surrounding regions\" are incredibly wrong. First of all, it should be noted that the duality uses 1-Lip in the entire space between Pr and Pg, not in Pr alone. If the manifolds are not extremely close (such as in the beginning of training), then the discriminator can be almost exactly 1 in the real data, and 0 on the fake. Thus the discriminator would be almost exactly constant (0-Lip) near the real manifold, but will fail to be 1-lip in the decision boundary, this is where interpolations fix this issue. See Figure 2 of the wgan paper for example, in this simple example an almost perfect discriminator would have almost 0 penalty.\n\n- In the 'Potential caveats' section, the implication that 1-Lip may not be enforced in non-examined samples is checkable by an easy experiment, which is to look for samples that have gradients of the critic wrt the input with norm > 1. I performed the exp in figure 8 and saw that by taking a slightly higher lambda, one reaches gradients that are as close to 1 as with ct-gan. Since ct-gan uses an extra regularizer, I think the authors need some stronger evidence to support the claim that ct-gan better battles this 'potential caveat'.\n\n- It's important to realize that the CT regularizer with M' = 1 (1-Lip constraint) will only be positive for an almost 1-Lip function if x and x' are sampled when x - x' has a very similar direction than the gradient at x. This is very hard in high dimensional spaces, and when I implemented a CT regularizer indeed the ration of eq (4) was quite less than the norm of the gradient. It would be useful to plot the value of the CT regularizer (the eq 4 version) as the training iterations progresses. Thus the CT regularizer works as an overall Lipschitz penalty, as opposed to penalizing having more than 1 for the Lipschitz constant. This difference is non-trivial and should be discussed.\n\n- Line 11 of the algorithm is missing L^(i) inside the sum.\n\n- One shouldn't use MNIST for anything else than deliberately testing an overfitting problem. Figure 4 is thus relevant, but the semi-supervised results of MNIST or the sample quality experiments give hardly any evidence to support the method.\n\n- The overfitting result is very important, but one should disambiguate this from being due to dropout. Comparing with wgangp + dropout is thus important in this experiment.\n\n- The authors should provide experiments in at least one larger dataset like bedrooms or imagenet (not faces, which is known to be very easy). This would strengthen the paper quite a bit.", "title": "Review for \"Improving the Improved Training of Wasserstein GANs\"", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryT2f8KgM": {"type": "review", "replyto": "SJx9GQb0-", "review": "This paper continues a trend of incremental improvements to Wasserstein GANs (WGAN), where the latter were proposed in order to alleviate the difficulties encountered in training GANs. Originally, Arjovsky et al.  [1] argued that the Wasserstein distance was superior to many others typically used for GANs. An important feature of WGANs is the requirement for the discriminator to be 1-Lipschitz, which [1] achieved simply by clipping the network weights. Recently, Gulrajani et al. [2] proposed a gradient penalty \"encouraging\" the discriminator to be 1-Lipschitz. However, their approach estimated continuity on points between the generated and the real samples, and thus could fail to guarantee Lipschitz-ness at the early training stages. The paper under review overcomes this drawback by estimating the continuity on perturbations of the real samples. Together with various technical improvements, this leads to state-of-the-art practical performance both in terms of generated images and in semi-supervised learning.  \n\nIn terms of novelty, the paper provides one core conceptual idea followed by several tweaks aimed at improving the practical performance of GANs. The key conceptual idea is to perturb each data point twice and use a Lipschitz constant to bound the difference in the discriminator\u2019s response on the perturbed points.  The proposed method is used in eq. (6) together with the gradient penalty from [2]. The authors found that directly perturbing the data with Gaussian noise led to inferior results and therefore propose to perturb the hidden layers using dropout. For supervised learning they demonstrate less overfitting for both MNIST and CIFAR 10.  They also extend their framework to the semi-supervised setting of Salismans et al 2016 and report improved image generation. \n\nThe authors do an excellent comparative job in presenting their experiments. They compare numerous techniques (e.g., Gaussian noise, dropout) and demonstrates the applicability of the approach for a wide range of tasks. They use several criteria to evaluate their performance (images, inception score, semi-supervised learning, overfitting, weight histogram) and compare against a wide range of competing papers. \n\nWhere the paper could perhaps be slightly improved is writing clarity. In particular, the discussion of M and M' is vital to the point of the paper, but could be written in a more transparent manner. The same goes for the semi-supervised experiment details and the CIFAR-10 augmentation process. Finally, the title seems uninformative. Almost all progress is incremental, and the authors modestly give credit to both [1] and [2], but the title is neither memorable nor useful in expressing the novel idea. \n[1] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan.\n\n[2] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. \n\n", "title": "The paper continues a line of improvement to Wasserstein GANs, and suggests an approach based a double perturbation of each data point, penalizing deviations from Lipshitz-ness. Empirical results demonstrate the effectiveness of the proposal. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SywG98VGM": {"type": "rebuttal", "replyto": "HkbRNKwef", "comment": "We thank the reviewer for the insightful comments and suggestions! The paper has been revised accordingly. Next, we answer the questions in detail.\n\n== Q1: The motivation ==\nWe acknowledge that the duality uses 1-Lipschitz continuity in the entire space between Pr and Pg, and it is impossible to visit everywhere of the space in the experiments. We instead focus on the region around the real data manifold to complement the region checked by GP-WGAN --- the gradient penalty term is kept in our overall approach. We have clarified this point by the following in the revised paper.\n\nArguably, it is fairly safe to limit our scope to the manifold that supports the real data distribution $\\mathbb{P}_r$ and its surrounding regions mainly for two reasons. First, we  keep the gradient penalty term  and improve it by the proposed consistency term in our overall approach. While the former enforces the continuity over the points sampled between the real and generated points, the latter complement the former by focusing on the region around the real data manifold instead. Second, the distribution of the generative model $\\mathbb{P}_G$ is virtually desired to be as close as possible to $\\mathbb{P}_r$.\n\n\n== Q2: That 1-Lip may not be enforced in non-examined samples is checkable ==\nThe non-examined samples can refer to all the possible samples in the continuous space which cannot be traversed in a discrete manner. Figure 8 plots the norm of the gradients (of the critic with respect to the input) over the real data points only. In other words, Figure 8 is only part of the consequence, and certainly not the cause, of the discriminators trained by GP-WGAN and our CT-GAN, respectively. It is not surprising that the norms by CT-GAN are closer to 1 than by GP-WGAN because we explicitly enforce the continuity around the real data. \n\nWe have run more experiments with larger \\lambda values in GP-GAN, and found the gradient norms can indeed reach those of CT-GAN when the \\lambda is four times larger than the original one used in the authors\u2019 code. However, the inception score on CIFAR-10 drops a little, and the overfitting remains. \n\nStronger evidence? In addition to the gradient norm, we have also examined the 1-Lipschitz continuity of the critic using the basic definition. For any two inputs x and x', the difference of the critic's outputs should be no more than M*|x-x'|. This notion is captured by our CT term defined in eq. (4). We plot the CT versus the training iterations as Figure 9 in the revised paper. In particular, for every 100 iterations, we randomly pick up 64 real examples and split them into two subsets of the same size. We compute d(D(x1)-D(x2))/d(x1-x2) for all the (x1,x2) pairs, where x1 is from the first subset and x2 is from the second. The maximum of d(D(x1)-D(x2))/d(x1-x2) is plotted in Figure 9. We can see that the CT-GAN curve converges under a certain value much faster than GP-WGAN.\n\n==  Q3: Plot the value of the CT regularizer ==\nPlease see Figures 9 and 10 in the revised paper for the plots. Note that M\u2019 has absorbed the term d(x\u2019,x\u2019\u2019) in the final consistency term (eq. (5)), so we have to tune its value as opposed to fixing it to 1. Also, because of this fact, we agree with the comment that \u201cThus the CT regularizer works as an overall Lipschitz penalty, as opposed to penalizing having more than 1 for the Lipschitz constant.\u201d We will clarify this part in the final paper, if it is accepted. \n\n== Q4: Line 11 ==\nIt is correct and is another way of denoting the gradient.\n\n== Q5: MNIST ==\nWe understand your concern with the use of MNIST and appreciate that you agree the overfitting experiments (Figure 4) are relevant. The other results (e.g., the generated samples and the test error in semi-supervised learning) can give the readers a concrete understanding about our model, but we agree one should not use MNIST to compare different algorithms.\n\n\n== Q6: GP-WGAN + Dropout ==\nPlease see Appendix E for the experimental results of GP-WGAN+Dropout on CIFAR-10 using 1000 training images. The corresponding inception score is better than GP-WGAN and yet still significantly lower than ours (2.98+-0.11 vs. 4.29+-0.12 vs. 5.13+-0.12). Figure 12, which is about the convergence curves of the discriminator cost over both training and testing sets, shows that dropout is indeed able to reduce the overfitting, but it is not as effective as ours.\n\n\n== Q7: Experiments in larger datasets ==\nIn Appendix F of the revised paper, we present results on the ImageNet and LSUN bedroom datasets following the experiment setup of GP-WGAN. After 200,000 generator iterations on ImageNet, the inception score of CT-GAN is 10.27+-0.15, whereas GP-WGAN's is 9.85+-0.17. Since there is only one class in LSUN bedroom, the inception score is not a proper evaluation metric for the experiments on this dataset. Visually, there is no clear difference between the generated samples of GP-WGAN and CT-GAN up to the 124,000th generator iteration.\n", "title": "Response to ICLR 2018 Conference Paper1144 AnonReviewer3"}, "r1QGQhVfG": {"type": "rebuttal", "replyto": "SksBJ-fzz", "comment": "Thank you for checking out our code! If you are interested, please also test M\u2019=0.2 for CIFAR-10 and you should be able to see a slightly higher inception score than M\u2019=0. We fix M\u2019=0 for all the experiments in the paper for consistency, but as we wrote in the paper, the best results are obtained between M\u2019=0 and M\u2019=0.2. \n\nWe noted that the assumption of d(x_1,x_2) being a constant can be relaxed. Our derivations still hold as long as d(x\u2019,x\u2019\u2019) is bounded by a constant, and we can absorb the constant to M\u2019. \n\nWe have actually reported two sets of experiments for CIFAR-10 in the paper. The first set is done using 1000 images and the second uses the whole CIFAR-10 dataset to train a ResNet. These setups are the same as in [1]. Additionally, we are running experiments on ImageNet and LSUN; we will update the response once the experiments are done. \n\nAbout the overfitting, please see Appendix E of the revised paper for the experimental results of GP-WGAN+Dropout on CIFAR-10 using 1000 training images. The corresponding inception score is better than GP-WGAN and yet still significantly lower than ours (2.98+-0.11 vs. 4.29+-0.12 vs. 5.13+-0.12). Figure 12, which is about the convergence curves of the discriminator cost over both training and testing sets, shows that dropout is indeed able to reduce the overfitting, but it is not as effective as ours.\n\nIn Appendix F of the revised paper, we further present  experimental results on the large-scale ImageNet and LSUN bedroom datasets. The experiment setup (e.g., network architecture, learning rates, etc.) is exactly the same as in the GP-WGAN work. After 200,000 generator iterations on ImageNet, the inception score of the proposed CT-GAN is 10.27+-0.15, whereas GP-WGAN's is 9.85+-0.17. Since there is only one class in LSUN bedroom, the inception score is not a proper evaluation metric for the experiments on this dataset. Visually, there is no clear difference between the generated samples of GP-WGAN and CT-GAN.", "title": "Thank you for checking out our code!"}, "ryr5SEEfG": {"type": "rebuttal", "replyto": "ryT2f8KgM", "comment": "We thank the reviewer for the very positive and affirmative comments about our work. \n\nWe also appreciate the suggestions for improving the writing clarify of the paper. The following has been incorporated in the revised paper.\n\n== M vs. M' == \nWe use the notation $M$ in eq. (3) and a different $M'$ in eq. (4) to reflect the fact that the continuity will be checked only sparsely at some data points in practice. ... ... Note that, however, it becomes impossible to compute the distance $d(\\bm{x}',\\bm{x}'')$ between the two virtual data points. In this work, we assume it is bounded by a constant and absorb the constant to $M'$. Accordingly, we tune $M'$ in our experiments to take account of this unknown constant; the best results are obtained between $M'=0$ and $M'=0.2$. \n\n== Semi-supervised experiment details and the CIFAR-10 augmentation process ==\n\nMNIST: There are 60,000 images in total. We randomly choose 10 data points for each digit as the labeled set. No data augmentation is used.\n\nCIFAR-10: There are 50,000 image in total. We randomly choose 400 images for each class as the labeled set. We augment the data by horizontally flipping the images and randomly translating the images within [-2,2] pixels. No ZCA whitening is used.\n\nModel Configuration\n\nTable 1: MNIST\n--------------\nClassifier C                        | Generator G\nInput: Labels y, 28*28 Images x     | Input: Noise 100 z\nGaussian noise 0.3, MLP 1000, ReLU  | MLP 500, Softplus, Batch norm \nGaussian noise 0.5, MLP 500, ReLU   | MLP 500, Softplus, Batch norm \nGaussian noise 0.5, MLP 250, ReLU   | MLP 784, Sigmoid, Weight norm \nGaussian noise 0.5, MLP 250, ReLU   |                               \nGaussian noise 0.5, MLP 250, ReLU   |                               \nGaussian noise 0.5, MLP 10, Softmax |                              \n\nTable 2: CIFAR-10\n-----------------\nInput: Labels y, 32*32*3 Colored Image x,               |   Input: Noise 50 z\n------------------------------------------------------------------------------\n0.2 Dropout                                             |   MLP 8192, ReLU, BN      \n3*3 conv. 128, Pad =1, Stride =1, lReLU, Weight norm    |   Reshape 512*4*4                         \n3*3 conv. 128, Pad =1, Stride =1, lReLU, Weight norm    |   5*5 deconv. 256*8*8,    \n3*3 conv. 128, Pad =1, Stride =2, lReLU, Weight norm    |   ReLU, Batch norm                         \n------------------------------------------------------------------------------\n0.5 Dropout                                             |\n3*3 conv. 256, Pad =1, Stride =1, lReLU, Weight norm    |\n3*3 conv. 256, Pad =1, Stride =1, lReLU, Weight norm    |   5*5 deconv. 128*16*16,  \n3*3 conv. 256, Pad =1, Stride =2, lReLU, Weight norm    |   ReLU, Batch norm                             \n------------------------------------------------------------------------------\n0.5 Dropout                                             |    \n3*3 conv. 512, Pad =0, Stride =1, lReLU, Weight norm    |\n3*3 conv. 256, Pad =0, Stride =1, lReLU, Weight norm    |   5*5 deconv. 3*32*32,   \n3*3 conv. 128, Pad =0, Stride =1, lReLU, Weight norm    |   Tanh, Weight norm  \n-----------------------------------------------------------------------------\nGlobal pool                                             |\nMLP 10, Weight norm, Softmax                            |\n\n== Hyper-parameters == \nWe set \\lambda = 1.0 in Eq.(7) in all our experiments. For CIFAR-10, the number of training epochs is set to 1,000 with a constant learning rate of 0.0003. For MNIST, the number of training epochs is set to 300 with a constant learning rate of 0.003.  The other hyper-parameters are exactly the same as in the improved GAN (Salimans et al., 2016).\n\n== New Title == \nImproving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect\n", "title": "Response to ICLR 2018 Conference Paper1144 AnonReviewer1"}, "Bkee-E4zG": {"type": "rebuttal", "replyto": "BJNCqPqxM", "comment": "We are pleased to see that the reviewer thinks our \"generated data showed very good qualities\" and \"the SSL-GANs results were impressive\".\n\n=Q1=\n\"Missing citations: the most related work of this one is the DRAGAN\"\n\nWe would consider WGAN and WGAN-GP as the most related works to ours and DRAGAN ranks after them. As a matter of fact, DRAGAN is an unpublished work and has not been peer-reviewed. As another matter of fact, the gradient penalty in DRAGAN is the same as in WGAN-GP except that it is imposed around the real data while WGAN-GP applies it to the points sampled between the real and the generated ones. \n\nNext, we highlight some key differences between DRAGAN and ours. \n\nWe propose to improve Wasserstein GAN, while DRAGAN works with GAN. \n\nDRAGAN aims to reduce the non-optimal saddle points in the minmax two-player training of GANs. In contrast, we propose an approach to enforcing the 1-Lipschitz continuity over the critic of WGANs. \n\nOne of our key observations is that it blurs the generated samples if we add noise directly to the data points, as done in DRAGAN. Instead, we perturb the hidden layers of the discriminator. \n\nDRAGAN perturbs a data point once while we do it twice in each iteration. After the perturbations, DRAGAN penalizes the gradients while we enforce the consistency of the outputs.\n\nOne of the most distinct features of our approach is that it seamlessly integrates the semi-supervised learning method by Laine & Aila (2016) with GANs. \n\n=Q2=\n\"the paper should discuss ... [Arjovsky & Bottou 2017], [Wu et al. 2016]\"\n\nWe had included both in our paper. Arjovsky & Bottou 2017 analyzes some distribution divergences and their effects in training GANs. Wu et al. 2016 propose to quantitatively evaluate the decoder-based generative models by annealed importance sampling. In our paper, we focus on a different subject, i.e., to design an algorithmic solution to the difficulty of training GANs.\n\n=Q3=\n\"the paper missed several details\"\n\nPlease see either Appendices A and B of the revised paper or the following for our answer to this question.\n\nGiven the context of the question, we believe it is about SSL. We follow the experiment setups in the prior works so that our results are directly comparable to theirs. Please see below for more details. If you are interested, you may also check out our code: https://github.com/biuyq/CT-GAN/blob/master/CT-GANs/Theano_classifier/CT_CIFAR-10_TE.py.\n\nMNIST: There are 60,000 images in total. We randomly choose 10 data points for each digit as the labeled set. No data augmentation is used.\n\nCIFAR-10: There are 50,000 image in total. We randomly choose 400 images for each class as the labeled set. We augment the data by horizontally flipping the images and randomly translating the images within [-2,2] pixels. No ZCA whitening is used.\n\nModel Configurations: We had included them in the appendix.\n\nHyper-parameters: We set lambda = 1.0 in Eq.(7) in all our experiments. For CIFAR-10, the number of training epochs is set to 1,000 with a constant learning rate of 0.0003. For MNIST, the number of training epochs is set to 300 with a constant learning rate of 0.003.  The other hyper-parameters are exactly the same as in the improved GAN (Salimans et al., 2016).\n\n=Q4=\n\"... which part of the model works\"\n\nPlease see either Appendix C of the revised paper or the following for our answer to this question.\n\nWe have done some ablation studies about our semi-supervised learning approach on CIFAR-10. \nMethod, Error\nw/o CT, 15.0\nw/o GAN (note 1), 12.0\nw batch norm (note 2), --\nw/o D_, 10.7\nOURS, 10.0\n\nNote 1: This almost reduces to TE (Laine & Aila, 2016). All the settings here are the same as TE except that we use the extra regularization ($D\\_(.,.)$ in CT) over the second-to-last layer.\nNote 2: We use the weight normalization as in (Salimans et al., 2016), which becomes a core constituent of our approach. The batch normalization would actually invalidate the feature matching in (Salimans et al., 2016).\n\nWe can see that both GAN and the temporal ensembling effectively contribute to our final results. The results without our consistency regularization (w/o CT) drop more than those without GAN. We are running the experiments without any data augmentation and will include the corresponding results in the paper.\n\n=Q5=\n\"...it is better to evaluate it on more datasets\"\n\nWe have run some new experiments on the SVHN dataset. Ours is the best among all the GAN based semi-supervised learning methods, and is on par with the state of the arts.\nMethod, Error\nPI Laine & Aila 2016, 4.8\nTE Laine & Aila 2016, 4.4\nTarvainen & Valpola 2017, 4.0\nMiyato et al. 2017, 3.86\nSalimans et al. 2016, 8.1\nDumoulin et al. 2016, 7.4\nKumar et al. 2017, 5.9\nOurs, 4.2", "title": "Response to ICLR 2018 Conference Paper1144 AnonReviewer2"}, "SkTmQbkbz": {"type": "rebuttal", "replyto": "BJhxQVieM", "comment": "That\u2019s a great question! We were actually wondering about a similar one. We can certainly interpret the formulation as that it encourages the network to be resilient to the dropout noise --- one of the notions that motivates the temporal ensembling semi-supervised learning method. In addition to that, however, it also enforces the Lipschitz continuity over the discriminator because of equation (3). Thanks to the dual roles of this formulation, we are able to use it to both improve the training of WGANs and connect GAN with the temporal ensembling method. \n\nWe have re-run the experiments using margin $M\u2019=0.2$. To show that the margin, albeit small, plays an active role, we have got some statistics of the $d(D,D)+0.1 d(D_,D_)$ term over the last 10 epochs. We can see that the median values of that term are smaller and the max values are larger than the margin. \n\nMin          0.0162    0.0153    0.0149    0.0171    0.0170    0.0159    0.0140    0.0146    0.0159    0.0144\nMedian    0.1130    0.1133     0.1124    0.1138    0.1114      0.1123    0.1124     0.1122    0.1125     0.1111\nMax          7.1718    6.1229    7.1985    7.3505    4.9636    5.2252   5.2559   5.3058    5.9905    4.8519\n\n\n\n\n\n\n\n\n", "title": "The dual roles of that formulation"}, "BynjqwxJf": {"type": "rebuttal", "replyto": "r1mlEqyyz", "comment": "We have done some ablated studies but they are not as thorough as you suggested. We will complete them and then get back to you soon. Thanks! \n\nObservations thus far: Both the consistent regularization and GAN are necessary to arrive at the report results, and the results without the consistency drop more than those without GAN.\n\n", "title": "Ablated studies"}, "SkpgYPxyz": {"type": "rebuttal", "replyto": "rJm5I9kJM", "comment": "Hello Hongyi, \n\nWe will add a column or a new table about the results with and without the data augmentations. Thank you for the pointer! \n\nBest,\n\n\n", "title": "Will add a column about the data agumentations"}, "Hk9rHlS0-": {"type": "rebuttal", "replyto": "ry2iCPQRb", "comment": "Thank you for directing us to DRAGAN. Sorry for missing it in our paper. We will include it in the updated version.\n\nGoing back to your question, the short answer is no because we do not actually aim to smooth the discriminator though the approach may have that effect. The long answer below clarifies it further and additionally highlights some differences between ours and DRAGAN.\n\nMotivations: DRAGAN aims to reduce the non-optimal saddle points in the minmax two-player training of GANs by drawing results from the minimax theorem for zero-sum game. In sharp contrast, we propose an alternative way of enforcing the 1-Lipschitz continuity over the \u201ccritic\u201d of WGANs thanks to the recent results by Arjovsky & Bottou (2017). \n\nHow to add the perturbations: One of the key observations in our experiments is that it reduces the quality of the generated samples if we add noise directly to the data points, as what is done in DRAGAN. Similar observations are reported by Arjovsky & Bottou (2017) and Wu et al. (2016). After many painstaking trials, we find good results by perturbing the hidden layers of the discriminator instead (as opposed to perturbing the original data). Besides, DRAGAN perturbs a data point once while we do it twice in an iteration. \n\nHow to use the perturbation: Similar to the gradient penalty proposed in (Gulrajani et al., 2017), DRGAN introduces a same regularization whereas for different reasons. In contrast, ours is a consistent regularization derived from the basic definition of Lipschitz continuous functions. \n\nSemi-supervised learning: One of the most notable features of our approach is that it seamlessly integrates the semi-supervised learning method by Laine & Aila (2016) with GANs. \n\nFinally, here is the DRAGAN paper we found on ArXiv: https://arxiv.org/abs/1705.07215 just to confirm it with you. Going back to the DRGAN work, it would be interesting to investigate whether it generates blurry images too, for example by comparing the results of different amount of noise including no noise. It may do not because it constraints the gradient as oppose to the discriminator\u2019s output. \n \n", "title": "Thank you for directing us to DRAGAN"}, "BJhcHerRb": {"type": "rebuttal", "replyto": "HkMNKONR-", "comment": "Sorry about that and Thank you for noting it! We will correct it in the updated version. ", "title": "Thank you and we will correct it in the updated version"}, "BkWTPbHCW": {"type": "rebuttal", "replyto": "HJ-LeemC-", "comment": "Thank you for the interest in our work. Please see below for the answers to your questions. \n\n(1) Dropout ratio: \n\n## The network used to learn from 1000 labeled CIFAR10 images only\nDiscriminator                                                                             Generator\nInput: 3*32*32 Image x                                                            Input: Noise z 128\n5*5 conv. 128, Pad = same, Stride = 2, lReLU                        MLP 8192, ReLU, Batch norm\n0.5 Dropout                                                                               Reshape     512*4*4\n5*5 conv. 256, Pad = same, Stride = 2, lReLU                       5*5 deconv. 256*8*8,\n0.5 Dropout                                                                              ReLU, Bach norm\n5*5 conv. 512, Pad = same Stride = 2, lReLU                       5*5 deconv. 128*16*16\n0.5 Dropout                                                                              ReLU, Batch norm\nReshape 512*4*4      (D_)                                                        5*5 deconv. 3*32*32\nMLP 1                          (D)                                                         Tanh\n\n## ResNet:\nDiscriminator                                                           |                Generator\nInput: 3*32*32 Image x                                          |                Input: Noise z 128\n[3*3]*2 Residual Block, Resample = DOWN        |                MLP 2048\n128*16*16                                                               |                Reshape 128*4*4\n[3*3]*2 Residual Block, Resample = DOWN        |                [3*3]*2 Residual Block, Resample = UP\n128*8*8 0.2 Dropout                                             |                 128*8*8\n[3*3]*2 Residual Block, Resample = None           |                 [3*3]*2 Residual Block, Resample = UP\n128*8*8 0.5 Dropout                                             |                 128*16*16\n[3*3]*2 Residual Block, Resample = None           |                 [3*3]*2 Residual Block, Resample = UP\n128*8*8 0.5 Dropout                                             |                 128*32*32\nReLU, Global mean pool    (D_)                              |                 3*3 conv.  3*32*32\nMLP 1                                  (D)                                |                 Tanh\n\n## The network for MNIST\nDiscriminator                                                                               Generator\nInput: 1*28*28 Image x                                                              Input: Noise z 128\n5*5 conv. 64, Pad = same, Stride = 2, lReLU                            MLP 4096, ReLU\n0.5 Dropout                                                                                 Reshape 256*4*4\n5*5 conv. 128, Pad = same, Stride = 2, lReLU                         5*5 deconv. 128*8*8\n0.5 Dropout                                                                                 ReLU, Cut 128*7*7\n5*5 conv. 256, Pad = same, Stride = 2, lReLU                         5*5 deconv. 64*14*14\n0.5 Dropout                                                                                 ReLU\nReshape 256*4*4       (D_)                                                          5*5 deconv. 1*28*28\nMLP 1                         (D)                                                             Sigmoid\n\n(2) No. There are only two perturbations, as denoted by x' and x\u2019\u2019, for a data point x in each iteration. They are independently generated by the dropout as shown in my answer to your question (1). In other words, the two terms equation (5) are actually calculated over the same pair of x' and x'' for each draw x ~ P_r.\n\nWe will try to release the code in one or two weeks.", "title": "More experiment details"}}}