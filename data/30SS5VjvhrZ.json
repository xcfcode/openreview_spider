{"paper": {"title": "Bayesian Neural Networks with Variance Propagation for Uncertainty Evaluation", "authors": ["Yuki Mae", "Wataru Kumagai", "Takafumi Kanamori"], "authorids": ["~Yuki_Mae1", "~Wataru_Kumagai2", "~Takafumi_Kanamori1"], "summary": "We developed a sampling-free method for uncertainty evaluation by converting DNNs/RNNs trained using dropout to the Bayesian neural networks with variance propagation. ", "abstract": "Uncertainty evaluation is a core technique when deep neural networks (DNNs) are used in real-world problems. In practical applications, we often encounter unexpected samples that have not seen in the training process. Not only achieving the high-prediction accuracy but also detecting uncertain data is significant for safety-critical systems. In statistics and machine learning, Bayesian inference has been exploited for uncertainty evaluation. The Bayesian neural networks (BNNs) have recently attracted considerable attention in this context, as the DNN trained using dropout is interpreted as a Bayesian method. Based on this interpretation, several methods to calculate the Bayes predictive distribution for DNNs have been developed. Though the Monte-Carlo method called MC dropout is a popular method for uncertainty evaluation, it requires a number of repeated feed-forward calculations of DNNs with randomly sampled weight parameters. To overcome the computational issue, we propose a sampling-free method to evaluate uncertainty. Our method converts a neural network trained using the dropout to the corresponding Bayesian neural network with variance propagation. Our method is available not only to feed-forward NNs but also to recurrent NNs including LSTM. We report the computational efficiency and statistical reliability of our method in numerical experiments of the language modeling using RNNs, and the out-of-distribution detection with DNNs. ", "keywords": ["uncertainty evaluation", "sampling-free method", "variance propagation", "LSTM", "out-of-distribution"]}, "meta": {"decision": "Reject", "comment": "This paper proposes an approach to estimating uncertainty in deep neural network models that avoids the need to make multiple forward passes through a network or through multiple individual models in a posterior ensemble. In terms of strengths, this is an important and timely topic that is of significant interest. The paper is clearly written for the most part. In terms of weaknesses, the significance of the work is low. As the reviewers note, there are multiple questions around the experimental evaluation that remain unresolved following the author feedback and discussion. In particular, the authors do not compare to baseline MCMC methods like HMC/SGHMC that can yield gold standard estimates of posterior predictive uncertainty. While not feasible for large-scale models, MCMC methods provides crucial sanity checks for uncertainty estimation on small-scale (e.g., MNIST-scale) models. Posterior distillation methods like Bayesian Dark Knowledge are also not considered in the evaluation and should be compared to where the distillation computation is feasible. There are also foundational technical correctness issues with respect to uncertainty quantification due to the fact that the paper is approximating the measure of uncertainty produced by MC Dropout, which itself only approximates the true Bayesian posterior predictive distribution under additional assumptions. This makes empirical comparissons to MCMC methods all the more important. Following the discussion, the reviewers agree that the paper is not yet ready for publication."}, "review": {"X5O47q1dD2J": {"type": "rebuttal", "replyto": "WWzaO2n2Nv0", "comment": "Thank you very much for your constructive feedback and comments.\n\n- The chief issue with this work is the 'generality' of the work. (omitted) I find this assertion unfair to other works which attempt to estimate various types of uncertainties (e.g. epistemic) in a principled manner.\n\nWe provided the formula of uncertainty propagation for major layers used in DNNs. The SGD with mini-batch is quite common, and the dropout is a standard technique for DNN learning. Hence, our method has a wide range of applications.\nAs the reviewer pointed out, there are many attempts to estimate various types of uncertainties (e.g., epistemic) in a principled manner.\nIn our paper, we focused on the epistemic uncertainty of DNNs.\nFor instance, probabilistic neural networks (PNN) are popular methods for assessing epistemic uncertainty. However,\nWe need to prepare the specific network architecture and learning algorithm for PNN learning, while our approach exploits the standard network architectures and learning algorithms. In numerical experimenters, we compared our method with existing methods, which exploit the standard network architectures and learning algorithms such as Postels et al. (2019).\n\n- The proposed approach depends on the hyper-parameter rho.\n\nThe implementation to incorporate the parameter rho is very simple. That is shown in Appendix A of our paper.  As for the estimation of \\rho, there are several approaches to determine the relevant rho with a light computation. A simple method is to use the validation data to estimate \\rho. In Section 3.5 of the revised paper, we explained an estimation method of \\rho and added some numerical results in Section 5. \n\n\n- In Table 1, how do the authors address that their approach (which is meant to be an approximation to MC) appears to outperform MC? \n\nThe VPBNN is not necessarily for the approximation of MC dropout. Both MC dropout and VPBNN approximate the true posterior distribution, though MC dropout with a sufficient number of feed-forward calculations tends to provide a satisfactory result. We supplemented this fact in the revision.\n\n- Although the proposed approach outperforms MC sampling in detecting out of distribution sampling, I think this benchmark is rather unfair (omitted) I'd like to see comparisons to other works which quantify uncertainty \n- I would strongly like to view comparisons to other works in quantifying network uncertainty.\n\nIn our paper, our interest is to develop a method of assessing the uncertainty for the DNNs having the standard network architecture trained by the common methods such as SGD with dropout. Hence, we compared our method with the MC dropout proposed by Gal & Ghahramani (2016a) and the Tayler approximation by Postels et al. (2019).\n", "title": "To reviewer 2"}, "5Nmi2XGD8Cn": {"type": "rebuttal", "replyto": "LrY9PW6PiEF", "comment": "Thank you very much for your constructive feedback and comments.\n\n- The technical details of such uncertainty propagation have been worked out by several authors in the past (see 1, 2, 3, and the papers cited by the authors Wu et al., 2019 and Shekhovtsov & Flach 2019).\n- While the application to amortizing MC-dropout is interesting, it appears to be a direct application of previous work.\n- The notion of upper bounding the variance during the propagation is indeed distinct from previous work, but is neither principled nor empirically vetted to be consistently useful.\n- Amortization of the posterior predictive distribution is not a new idea.\n\nWe introduced the correlation parameter \\rho to avoid the overconfident prediction. There are several approaches to determine the relevant rho with a light computation. A simple method is to use the validation data to estimate \\rho. In Section 3.5 of the revised paper, we explained an estimation method of \\rho and added some numerical results in Section 5.\n\n\n- Neither the language modeling task nor the OOD detection task make use of the upper bound.\n\nFor the OOD detection task, numerical results with an adaptive choice of \\rho were added in the revision. \n\n- the experimental section would benefit from including calibration metrics (ECE / Brier score; see 6).\n\nIn language modeling tasks, we reported the perplexity that is the standard performance measure in this task. Though we can compute the calibration metrics such as ECE, the perplexity also provides a similar calibration criterion. The improvement of the perplexity indicates that our method yields the relevant calibration.\n", "title": "To reviewer 1"}, "IwFF2S3JM5E": {"type": "rebuttal", "replyto": "oFoQZtDr0AS", "comment": "Thank you very much for your constructive feedback and comments.\n\n- The first approach (a) assumes independence in the input. The second approach (b) will be necessarily true if \\rho >= \\max_{j,j\u2019:j!=j\u2019} \\rho_{j,j\u2019}.\n\nWe think that the second approach is important to avoid the overconfidence for the prediction with the Bayesian method. So, we propose to use the second approach. Hence, we dropped the formula (a) and presented only (b) in the revision. \nIn our method, the overestimation of the variance can occur. But, the relevant choice of rho provides a meaningful assessment of the uncertainty. In the numerical experiments of the revision, we added the method to select the relevant rho with a light computation cost.\n\nAlso, we will supplement the detailed derivation of the equation in (b). \n\n- To my knowledge, Wang & Manning (2013)\u2019s Gaussian approximation holds due to central limit theorem as the number of samples approaches infinity but I do not think this will be applicable in this sample-free setting.\n\nWe think that the Gaussian approximation also holds in the sample-free setting. In Wang & Manning (2013) and other related papers such as Postels(2019), the Gaussian approximation is considered under also the condition that the dimension of input to the middle layer is sufficiently large. \n\n\n- I would suggest that the authors formally assess the fit using metrics like confidence interval widths and coverage. \n\nIn the revision, we add the width of confidence intervals to the numerical results. \n\n\n- Estimation of rho\n\nThere are several approaches to determine the relevant rho with a light computation. A simple method is to use the validation data to estimate \\rho. In Section 3.5 of the revised paper, we explained an estimation method of \\rho and added some numerical results in Section 5. \n\n\n", "title": "To reviewer 3"}, "QkEIl-4Rkw7": {"type": "rebuttal", "replyto": "65ofW10Yc5T", "comment": "Thank you very much for your constructive feedback and comments.\n\n\n- p. 1: \"The uncertainty is defined based on the posterior distribution.\" \n- p. 2: \"The MC dropout requires a number of repeated feed-forward calculations with randomly sampled weight parameters in order to obtain the predictive distribution.\" \n- p. 2: Lakshminarayanan et al. (2017)\n- Minor comments\n\nIn the revision, we corrected the expressions. \n\n- p. 4: For variance propagation in a dropout layer with stochastic input, it's not exactly clear from the text how variance from the inputs and dropout is being combined into an output Gaussian.\n\nSince the stochastic input and the weight parameter in the dropout layer are independent, one can exactly calculate the variance of the product using each expectation and variance. We supplemented the details in the revision. \n\n- p. 7: \"Estimation of \u03c1\"\n\nThere are several approaches to determine the relevant rho with a light computation. A simple method is to use the validation data to estimate \\rho. In Section 3.5 of the revised paper, we added an estimation method of \\rho and added some numerical results in Section 5. \n\n- p. 7, 8: For the language modeling experiment, why do you think VPBNN was able to achieve lower perplexity values than MC dropout?\n\nThe VPBNN is not necessarily the approximation of MC dropout. Both MC dropout and VPBNN approximate the true posterior distribution, though MC dropout with a sufficient number of feed-forward calculations tends to provide a satisfactory result.\n\n- p. 8: For the OOD detection experiment, I'm surprised that rho=0 was the only VPBNN model used\n\nIn the revision, we added the numerical result of VPBNN with an adaptive selection of rho. We find that a small rho is good to achieve high AUC values. \n\n- Can you include a discussion and measurements for FLOPS and memory usage for VPBNN?\n\nIn the prediction phase, the computation time and memory cost of VPBNN is almost doubled compared to the standard feed-forward calculation since the mean value, and the variance should be computed. On the other hand, MC-dropout's computation time is linearly increased with the number of sampling. In Section 3.5, we show the computation cost of VPBNN with adaptive \\rho is less than that of MC dropout. Also, in the experiments on OOD, we added a comment on the computation time.\n\n", "title": "To reviewer 4"}, "fYMIconZMDJ": {"type": "rebuttal", "replyto": "30SS5VjvhrZ", "comment": "We thank all the reviewers for the constructive feedback and helpful comments. We have revised our paper by taking into account their suggestions. (During the rebuttal phase the page limit is 9 pages). The major changes are summarized below.\n\n- We added Section 3.5 in which the estimation method of the correlation parameter, rho, is proposed. \n- In numerical experiments, some numerical results using our method, VPBNN, with the adaptive correlation parameter were added. \n\n", "title": "To all reviewers"}, "WWzaO2n2Nv0": {"type": "review", "replyto": "30SS5VjvhrZ", "review": "This paper proposes a variational-approximation of Neural Network uncertainty dependent on the 'bayesian interpretation' of dropout. Using this approach uncertainty measurements are provided using 'variance propagation.' Validation is provided on synthetic data, and MNIST/FMNIST.\n\nAlthough a genuine effort is made towards deriving uncertainty through 'variance propagation,' I think the proposed approach is rather ad-hoc. The proposed approach relies on a 'bayesian interpretation,' of dropout, and is of limited applicability requiring hard coded rules for propagating through each layer. Currently, rules are derived for a small set of neural network layers.\n\nThe chief issue with this work is the 'generality' of the work. The promised approach here is applicable to specifically neural networks trained with SGD using dropout. Thus although the proposed approach claims to evaluate uncertainty, it is rather estimating the uncertainty introduced by dropout using a variational approximation, and assuming this is the same as uncertainty. I find this assertion unfair to other works which attempt to estimate various types of uncertainties (e.g. epistemic) in a principled manner.\n\nThe proposed approach depends on the hyperparameter \\rho, which requires additional tuning to allow variance propagation through affine (e.g., fully connected) layers. This additional hyperparameter gives some doubt whether the proposed approach is something which can be easily integrated into existing models. \n\nIn Table 1, how do the authors address that their approach (which is meant to be an approximation to MC) appears to outperform MC? Shouldn't an approximation perform worse?\n\nAlthough the proposed approach outperforms MC sampling in detecting out of distribution sampling, I think this benchmark is rather unfair. I'd like to see comparisons to other works which quantify uncertainty. The issue is that this only compares against model uncertainty assuming the 'bayesian interpretation' of MC is valid. I would strongly like to view comparisons to other works in quantifying network uncertainty.\n\nOn the whole, the lack of generality of the proposed approach, as well as the limited scope of 'variational approximation of model uncertainty assuming bayesian interpretation of dropout,' causes me to be highly skeptical of this work.\n\n", "title": "Limited scope and novelty; broader implications unclear.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "LrY9PW6PiEF": {"type": "review", "replyto": "30SS5VjvhrZ", "review": "The paper proposes a sampling free approach for estimating predictive uncertainties in Bayesian neural networks trained via Monte-Carlo dropout. In particular,  given a dropout trained neural network, the paper develops a deterministic approximation to the test time predictive distribution that is otherwise approximated through Monte-Carlo simulations.\n\nThe paper is clearly written and proposes a solution to an important problem. Extracting fast and reliable predictive uncertainties from BNNs, trained via MC-dropout or other approximate inference techniques, is crucial for deploying BNNs in resource constrained and/or real time applications. Deterministic approximations like the one presented here are promising. \n\nThat being said, I have several concerns with the paper that revolve around differentiation from previous work,  missing empirical comparisons, and some curious modeling choices. \n\n* The primary contribution of the paper appears to be the amortization of the MC-dropout posterior predictive density. This is achieved by propagating  uncertainty (first and second moments of the inputs) through the network.  The technical details of such uncertainty propagation have been worked out by several authors in the past (see 1, 2, 3, and the papers cited by the authors Wu et al., 2019 and Shekhovtsov & Flach 2019). While the application to amortizing MC-dropout is interesting, it appears to be a direct application of previous work.\n* The notion of upper bounding the variance during the propagation is indeed distinct from previous work, but is neither principled nor empirically vetted to be consistently useful.  The experiments provide limited evidence in support of using the upper-bound. The derived uncertainty seems to be crucially dependent on the weighting factor $\\rho$ and it is unclear how one would select this weighting factor. In general, injecting additional noise does not guarantee better calibrated uncertainties. Neither the language modeling task nor the OOD detection task make use of the upper bound.\n* Amortization of the posterior predictive distribution is not a new idea. A popular approach is to use distillation [4, 5] based techniques to approximate the posterior predictive distribution with a second neural network. This work needs to be both discussed (advantages / disadvantages of the proposed approach over distillation)  and empirically compared against. \n* Since the goal of the paper is to sufficiently well approximate the posterior predictive density, the experimental section would benefit from including calibration metrics (ECE / Brier score; see 6). \n* (Minor) The paper at several places claims that the proposed approach \u201cUnlike various kinds of probabilistic NNs, we do not need any specialized training procedure to evaluate the uncertainty\u201d. While this is technically true, this is also true for other distillation based amortization techniques. Which arguably are simpler because one can just use an off the shelf network and not have to worry about propagating moments correctly. \n* (Minor) Since the categorical Gaussian integral is intractable, the authors replace the softmax layer with a series of independent sigmoids and use an approximation for the sigmoid Gaussian convolution. This is strange. If we are going down this path, why not use independent probits instead of sigmoids? The probit-Gaussian integral is exactly computable and requires no further approximations (see equation 3.82 in http://www.gaussianprocess.org/gpml/chapters/RW3.pdf). \n\n[1] https://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights.pdf\n\n[2] https://arxiv.org/abs/1502.05336\n\n[3] https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12391\n\n[4] https://arxiv.org/abs/1506.04416\n\n[5] https://arxiv.org/abs/2005.08110\n\n[6] Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):243\u2013268, 2007. ", "title": "The paper doesn't differentiate itself well from existing work and misses key comparisons. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "oFoQZtDr0AS": {"type": "review", "replyto": "30SS5VjvhrZ", "review": "This paper proposes a sampling free technique based on variance propagation to model predictive distributions of deep learning models. Estimating uncertainty of deep learning models is an important line of research for understanding the reliability of predictions and ensuring robustness to out-of-distribution data. Results are shown using synthetic data, perplexity analysis for a language modeling task and out-of-distribution detection performance using a convolutional network.\n\nOverall I vote for rejecting the paper. The paper proposes an upper bound to the variance estimate of predictive distributions. However, the paper does not explain how the upper bound can be ensured. Furthermore, the experiments based on real data are conducted not using the upper bound approach but assuming strong independence assumptions (\\rho = 0). In my opinion, the independence assumption needs extensive experiments to validate its performance under different scenarios. Furthermore, for further adoption of the upper bound approach, I think the authors need to provide extensive experiments to showcase the tradeoffs. For instance, as a reader of the paper I would like to get a sense of how much I will be overestimating the variance under typical scenarios. For this, I would recommend a formal analysis of the uncertainty estimates by inspecting the confidence intervals through coverage properties. \n\nMy specific comments related to this approach:\n-\tThe authors propose two approaches in Section 3.1 for estimating the variance in predictions and the remaining subsections in Section 3 build on this. \no\tThe first approach (a) assumes independence in the input. In my opinion, this is a very strong assumption. For instance, multi-collinearity in features in DNN\u2019s is a very common usage pattern. In domains like images, by construction of the problem, you expect a spatial correlation structure. Furthermore, the multi-layer perceptron architecture by construction is susceptible to correlation among variables. I would highly recommend adding a discussion on the validity independence assumption in general.\no\tThe second approach (b) will be necessarily true if `\\rho >= \\max_{j,j\u2019:j!=j\u2019} \\rho_{j,j\u2019}`. Only then this will guarantee that you will obtain an upper bound to the variance but it is very likely that you will overestimate the variance since your estimate is going to be bound by the highest correlation in your system. Again, in my opinion, this is a significant limitation of this approach and I recommend that authors highlight these points. Another minor thing to note in the manuscript is that when \\rho=0, option (b) reduces to option (a).\no\tI would suggest adding another summation term in Var(y_i) related equations to denote the double summation (indexed by j\u2019) happening over the covariance terms (in Section 3.1).\no\tRelated to the above point, I could not follow the mathematical derivation from line (1) to line (2) in Equation (1). Could the authors provide an explicit derivation to ensure that the derivation is correct?\no\tFurthermore, author\u2019s claim that \u201cdistribution of y_i is well approximated by the univariate Gaussian distribution if the correlation among x is small\u201d (Section 3.1) needs justification. To my knowledge, Wang & Manning (2013)\u2019s Gaussian approximation holds due to central limit theorem as the number of samples approaches infinity but I do not think this will be applicable in this sample-free setting.\n\n-\tRegarding the results:\no\tThe results shown in Figure 1 demonstrates that choosing an appropriate `\\rho can be challenging. We see underestimation of uncertainty with \\rho <=0.15 and overestimation with \\rho = 1. However, since these are based on synthetic data, I would suggest that the authors formally assess the fit using metrics like confidence interval widths and coverage.\no\tThe authors note that \u201cEstimation of \u03c1 is possible by observing the outputs of middle layers several times under the approximate predictive distribution.\u201d. I am not convinced that this is an easy problem, I would recommend that the authors provide an example and elaborate this in depth.\n\nMinor comments:\n-\tPlease indicate the best results in the tables by highlighting them. \n-\tThe uncertainty in deep learning literature typically employs distinction between aleatory and epistemic uncertainties. I think the manuscript can benefit how this proposed approach maps to the different sources of uncertainties. ", "title": "Recommendation to Reject", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "65ofW10Yc5T": {"type": "review", "replyto": "30SS5VjvhrZ", "review": "Thank you for the interesting paper!\n\nSummary\n\nThe authors focus on the important problem of efficient uncertainty quantification. More specifically, they propose a methodology that approximates the variance across samples from an MC dropout model with a single forward pass. To do this, they define analogs to existing layers such that they analytically (and often approximately) propagate variance from the input to the output of the layer. By repeating this for all layers, they construct a \"variance-propagation BNN\" (VPBNN) model that otherwise has the same architecture as the existing MC dropout model but is able to effectively output sample variance with a single forward pass. They demonstrate the effectiveness of their approach on (1) a simple synthetic problem, (2) a language modeling task, and (3) OOD detection.\n\nStrengths\n\n- Efficiently (from a FLOPS standpoint) propagating model uncertainty in a BNN is an important research area, particularly for compute-constrained use cases.\n- The authors focus on a relevant set of experiments to demonstrate both the ability of their method to approximate MC dropout, and the ability to perform at, or better than, existing methods on downstream tasks.\n\n\nWeaknesses\n\nAs noted below, I have concerns around the experimental results. More specifically, I feel that there is a relative lack of discussion around the (somewhat surprising) outperformance of baselines that VPBNN is aiming to approximate, and I feel that the experiments are missing what I see as key VPBNN results that otherwise leave the reader with questions. Additionally, I think the current paper would benefit from including measurements and discussion around the specifics of computational and memory costs of their method.\n\nRecommendation\n\nIn general, I think this could be a great paper. However, given the above concerns, I'm currently inclined to suggest rejection of the paper in its current state. I would highly recommend that authors push further on the noted areas!\n\nAdditional comments\n\n- p. 1: \"The uncertainty is defined based on the posterior distribution.\" For more clarity it could be helpful to update this to say that the epistemic model uncertainty is represented in the prior distribution, and upon observing data, those beliefs can be updated in the form of a posterior distribution, which yields model uncertainty conditioned on observed data.\n- p. 2: \"The MC dropout requires a number of repeated feed-forward calculations with randomly sampled weight parameters in order to obtain the predictive distribution.\" This should be updated to indicate that in MC dropout, dropout is used (in an otherwise deterministic model) at test time with \"a number of repeated feed-forward calculations\" to effectively sample from the approximate posterior, but not directly via different weight samples (as in a variational BNN). With variational dropout, this ends up having a nice interpretation as a variational Bayes method, though no weight distributions are typically directly used with direct MC dropout.\n- p. 2: Lakshminarayanan et al. (2017) presented random seed ensembles, not bootstrap ensembles (see p. 4 of their work for more info). They used the full dataset, and trained M ensemble members with different random seeds, rather than resampled data.\n- p. 4: For variance propagation in a dropout layer with stochastic input, it's not exactly clear from the text how variance from the inputs and dropout is being combined into an output Gaussian. I believe using a Gaussian is an approximation, and while that would be fine, I think it would be informative to indicate that. The same issue comes up with local reparameterization for BNNs with parameter distributions, where they can be reparameterized exactly as output distributions (for, say, mean-field Gaussian weight dists) so long as the inputs are deterministic. Otherwise, the product of, say, two Gaussian RVs is non-Gaussian.\n- p. 7: Figure 1 is too small.\n- p. 7: \"Estimation of \u03c1 is possible by observing the outputs of middle layers several times under the approximate predictive distribution. The additional computation cost is still kept quite small compared to MC dropout.\" How exactly is $\\rho$ estimated? Is it a one-time cost irregardless of data that can then be used for all predictions from the trained model? Without details, this seems like a key component that can yield arbitrary amounts of uncertainty.\n- p. 7, 8: For the language modeling experiment, why do you think VPBNN was able to achieve lower perplexity values than MC dropout? The text generally focuses on VPBNN as an approximation to MC dropout, and yet it outperforms it. The text would greatly benefit from more discussion around this point. \n- p. 8: For the OOD detection experiment, I'm surprised that $\\rho = 0$ was the only VPBNN model used, since Section 5.1 and Figure 1 indicated that it led to overconfident models. Can you include results with other settings of $\\rho$? Moreover, from Figure 1 we see that (for that model) VPBNN with $\\rho = 0$ qualitatively yielded the same amount of predictive variance as the Taylor approximation. However, in Table 2, we see VPBNN with $\\rho = 0$ outperform MC dropout (with 100 or 2000 samples) and the Taylor approximation. Why do you think this is the case, particularly if the standard deviation was used as the uncertainty signal for the OOD decision. I see that \"This is because the approximation accuracy of the Taylor approximation is not necessarily high as shown in Section B\", but I did not find Section B or Figure 3 to be clear. I think the text would benefit from more discussion here, and from the additional experiments for $\\rho$.\n- Can you include a discussion and measurements for FLOPS and memory usage for VPBNN? Specifically, given the discussion around efficiency and the implementation that doubles the dimensionality of the intermediates throughout the model, I believe it would be informative to have theoretical and possibly runtime measurements.\n\nMinor\n\n- p. 1: s/using the dropout/using dropout/\n- p. 1: s/of the language modeling/of language modeling/\n- p. 2: s/is the representative of/is representative of/\n- p. 2: s/In the deep learning/In deep learning/\n- p. 2: s/This relations/This relation/\n- p. 5: Need to define $s$ as the sigmoid function in the LSTM cell equations.", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}