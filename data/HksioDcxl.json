{"paper": {"title": "Joint Training of Ratings and Reviews with Recurrent Recommender Networks", "authors": ["Chao-Yuan Wu", "Amr Ahmed", "Alex Beutel", "Alexander J. Smola"], "authorids": ["cywu@cs.utexas.edu", "amra@google.com", "alexbeutel@google.com", "alex@smola.org"], "summary": "", "abstract": "Accurate modeling of ratings and text reviews is at the core of successful recommender systems. While neural networks have been remarkably successful in modeling images and natural language, they have been largely unexplored in recommender system research. In this paper, we provide a  neural network model that combines ratings, reviews, and temporal patterns to learn highly accurate recommendations. We co-train for prediction on both numerical ratings and natural language reviews, as well as using a recurrent architecture to capture the dynamic components of users' and items' states. We demonstrate that incorporating text reviews and temporal dynamic gives state-of-the-art results over the IMDb dataset.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper has some nice ideas, but requires a bit to push it over the acceptance threshold. I agree with the reviewers who ask for comparisons with other rating-review methods, and that other evaluation metrics more appropriate to the recommendation tasks should be reported. More analysis of the model, and the factors that contribute to its performance, would greatly improve the paper."}, "review": {"BkPGrmkDx": {"type": "rebuttal", "replyto": "BJ9p-XmEx", "comment": "Thank you for for your time and comments.  We are working on running more experiments, particularly with respect to additional baselines so as to compare the impact of BOW-style modeling versus RNN based models, and also compare different strategies of linking review model to factors.  Additionally, we believe that using an RNN model is a step in the direction toward generating reviews and explaining predictions. \n\nSecond, we decided to begin with the rating prediction task as it offers a standard intrinsic evaluation that directly measures the model accuracy and is widely used across the academic literature.  However, we agree that extending this model to other objective functions is an interesting next step for the work.\n", "title": "re: no title"}, "ByG_4Qywx": {"type": "rebuttal", "replyto": "r1J3OZY4l", "comment": "Thank you for the feedback.  Broadly speaking, we are working to expand our experiments overall.  As we mention in another comment below, we are working to compare against other rating & review systems, which will give us a better sense of how our model uses review data to improve prediction accuracy. \n\nWith respect to applicability to ICLR, we believe that our model design is novel and provides insight into how to successfully jointly learn from temporal data, ratings, and reviews.  As co-training is a continually growing area of interest in the ICLR community, exploring those ideas and how they work in the recommendation setting, which differs from the CV or NLU settings, is important in furthering our understanding of deep networks. \n", "title": "re: Review"}, "HyfOBJMNg": {"type": "rebuttal", "replyto": "BJa3UMg4e", "comment": "Thank you for the valuable comments. \n\nIn real-world recommendation settings, a system needs to predict future ratings instead of interpolating past ratings with the benefit of hindsight. The former is much harder than the latter. For instance, knowing that a user will have developed a liking for Pedro Almod\u00f3var in the future makes it much easier to estimate his opinion about La Mala Educaci\u00f3n. The violation of causality in statistical analysis also makes it impossible to translate the reported accuracy to online performance in practice.\n\nWe are working on carrying out some comparison to other review-rating models, and the results will be presented in the final version. ", "title": "re: not sure why my review needs a title"}, "BkF4PJfVx": {"type": "rebuttal", "replyto": "r1IK1cUQg", "comment": "Thank you for the valuable comments. For serving, it is not slower than traditional methods, as rating prediction only requires inner products of the last hidden states, which could be stored as traditional methods. \n\nEven though each training of RRN is more computationally expensive than matrix factorization, since we are estimating the functions that find the states instead of estimating the states directly, we can get updated states by forwarding the new data without frequent re-training. Thus, overall RRN might not be more expensive. In addition, RRN achieves better accuracy. ", "title": "re: time complexity"}, "BkoRLyzNl": {"type": "rebuttal", "replyto": "B161DokQx", "comment": "While our model is well situated to generate personalized reviews, generating coherent reviews is in itself an interesting problem with its own challenges.  In future work we hope to directly explore novel NLP techniques to enable generating coherent, long, personalized reviews.  In particular, we think that joining our model with recent success in NLP, such as attention mechanisms, will be an exciting direction.\n\nAdditionally, we are currently working on carrying out some comparison to other review-rating models, and the results will be presented in the final version. ", "title": "re: pre-review questions"}, "SyX7L1fVg": {"type": "rebuttal", "replyto": "BywGemd7g", "comment": "Thank you for the great comments and questions. All these papers are highly valuable and interesting, but we would like to explain some major distinctions. [3, 5] consider session-based modeling in cases where user responses are scarce and short lived. In our work, we consider a more challenging setting to model both short-term and long-term dynamics, and we jointly model the evolution of both user states and item states, as well as personalized reviews. \n\nVery recent work [4], which is developed independently of this paper almost at the same time, also models the coevolution of states. One important distinction between [4] and this paper is that [4] assumes more specific function forms (Hawkes processes with exponential triggering kernel), while in this paper we use more flexible non-parametric approaches. We will add comparisons to [4] in the future. \n\nWe agree that online metrics such as CTR, user membership retention, top-k performance or profits are more desirable, but these data are not publicly available. In addition, these metrics are not universal in the sense that their importance are different for different applications. What RMSE offers is a standard intrinsic evaluation that directly measures the accuracy of predictions. Also, it is widely used in the academic literature, and as a result, it allows more comprehensive comparisons.  In particular, we have seen in the past that even improvements by 1% in RMSE can be significant (in 2008 SVD++ reported a 1.09% improvement over SVD, and in 2010 Time-SVD++ was published with a 1.26% improvement over SVD++); as such, we believe that our 2% improvement in RMSE is a valuable contribution.", "title": "re: Problems with the evaluation & missing related work"}, "BywGemd7g": {"type": "rebuttal", "replyto": "HksioDcxl", "comment": "The evaluation in this paper leaves much to be desired. Its biggest flaw is that it only addresses rating prediction and completely forgets about the recommendations themselves. It is well known in the recommender systems research community that rating prediction accuracy doesn't really matter, because it isn't well correlated with recommendation accuracy: algorithms with low RMSE may fall behind in the top-k recommendation task. See [1] for an extensive study on the topic (other papers with both RMSE and accuracy measurements, such as [2], show similar results). Therefore it is crucial that modern recommendation algorithms are evaluated wrt. accuracy (e.g. Precision@K, Recall@K, AUC@K, etc.) or ranking metrics (NDCG@K, MRR@K, MAP@K, etc.).\n\nOther than that, Table2 shows that there is only 2% improvement over PMF, but with a much more complex model. Even if this were in recommendation accuracy (as opposed to RMSE), it raises the question whether using a much more complex model for 2% improvement worths it, considering e.g. training and serving times. In any other field, the answer would be yes. However, with recommender systems it is well known that offline evaluation is a necessary approximation step in the evaluation and improvements there doesn't translate into improvements in online metrics (e.g. CTR, GMV, profit, etc.) directly. There is correlation, but small offline improvements generally don't boost online performance.\n\nThe paper also misses some notable work on using RNNs for recommendations, e.g. [3], [4], [5].\n\n\nReferences:\n[1] P. Cremonesi, Y. Koren, R. Turrin: Performance of recommender algorithms on top-n recommendation tasks. RecSys 2010.\n[2] S. Dooms, A. Bellog\u00edn, T. De Pessemier, L. Martens: A Framework for Dataset Benchmarking and Its Application to a New Movie Rating Dataset. ACM TIST.\n[3] B. Hidasi, A. Karatzoglou, L. Baltrunas, D. Tikk: Recurrent Neural Networks for Session-based Recommendations. ICLR 2016.\n[4] H. Dai, Y. Wang, R. Trivedi, L. Song: Coevolutionary Latent Feature Processes for Continuous-Time User-Item Interactions. DLRS 2016.\n[5] YK. Tan, X. Xu, Y. Liu: Improved Recurrent Neural Networks for Session-based Recommendations. DLRS 2016.", "title": "Problems with the evaluation & missing related work"}, "r1IK1cUQg": {"type": "review", "replyto": "HksioDcxl", "review": "It would be appropriate to have a discussion on time complexity and how it might affect the serving performance. As from the description, this seems to be more time consuming that traditional factorization based modelsThis paper proposed a joint model for rate prediction and text generation.  The author compared the methods on a more realistic time based split setting, which requires \u201cpredict into the future.\u201d\nOne major flaw of the paper is that it does not address the impact of BOW vs the RNN based text model, specifically RRN(rating+text) already uses RNN for text modeling, so it is unclear whether the improvement comes from RNN(as opposed to BOW) or application of text information. A more clear study on impact of each component could make it more clear and benefit the readers.\nAnother potential improvement direction of the paper is to support ranking objectives, as opposed to rate prediction, which is more realistic for recommendation settings.\nThe overall technique is intuitive and novel, but can be improved to give more insights to the reader,.\n\n\n\n", "title": "time complexity", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJ9p-XmEx": {"type": "review", "replyto": "HksioDcxl", "review": "It would be appropriate to have a discussion on time complexity and how it might affect the serving performance. As from the description, this seems to be more time consuming that traditional factorization based modelsThis paper proposed a joint model for rate prediction and text generation.  The author compared the methods on a more realistic time based split setting, which requires \u201cpredict into the future.\u201d\nOne major flaw of the paper is that it does not address the impact of BOW vs the RNN based text model, specifically RRN(rating+text) already uses RNN for text modeling, so it is unclear whether the improvement comes from RNN(as opposed to BOW) or application of text information. A more clear study on impact of each component could make it more clear and benefit the readers.\nAnother potential improvement direction of the paper is to support ranking objectives, as opposed to rate prediction, which is more realistic for recommendation settings.\nThe overall technique is intuitive and novel, but can be improved to give more insights to the reader,.\n\n\n\n", "title": "time complexity", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B161DokQx": {"type": "review", "replyto": "HksioDcxl", "review": "Given that you have a model that seems to be capable of generating reviews, what do the generated reviews look like? Are they qualitatively reasonable?\n\nOther than generating reviews, several works use reviews to improve rating prediction in some way (including work by the authors), e.g. HFT, JMARS, CoBaFi, RMR, etc. How do these compare in terms of RMSE performance?\nThe paper seeks to jointly model ratings and reviews in addition to temporal patterns. Existing papers have captured these aspects previously, but what's novel here is the particular combination of parts and choice of techniques. In particular, the use of LSTMs as opposed to \"bag-of-words\" models that have previously been used when combining the same components.\n\nA criticism is made of existing models that use bag-of-words features as being too \"coarse\" to capture the real dynamics of reviews. This seems a valid criticsm, though it's not clear exactly what features those existing models miss. Something missing from this paper is any interpretation of what the model \"learns\" that may explain its better performance.\n\nI also don't know about the significance of predicting \"future\" ratings as opposed to random splitting of the data. Lots of work on temporal recommender systems uses a variety of hold-out strategies besides random sampling, e.g. holding out the final ratings. Is there a methodological contribution associated with this change?\n\nThe experiments evaluate the extent to which the model achieves good performance due to its ability to capture evolving temporal patterns at the level of items, and the ability of the model to capture additional dynamics from text. Text makes a small but significant contribution; I was surprised by how large an improvement is achieved given how little text was included.\n\nOverall this is a reasonably strong experimental comparison, though could be improved in two dimensions:\n(a) There's no comparison to models that use ratings + text (e.g. CoBaFi, JMARS, etc.). These methods have reported substantial improvements over rating-only models in the past, perhaps even larger improvements than what is reported here. Such an experiment is important given the claim that existing models (in particular bag-of-words models) are too coarse to capture the dynamics of text.\n(b) The restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation. In particular, they're extremely dense datasets that support very parameter-rich models. I'd question whether the results would hold on sparser data (though in fact I'd suspect they would hold, since on a sparser dataset the contribution due to using reviews ought to be larger).\n\nOtherwise the experiments are fine. Perplexity results are nice but essentially what we'd expect. It is a shame that unlike other papers that use text to inform recommender systems there's no high-level analysis here of what the model has uncovered.\n", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJa3UMg4e": {"type": "review", "replyto": "HksioDcxl", "review": "Given that you have a model that seems to be capable of generating reviews, what do the generated reviews look like? Are they qualitatively reasonable?\n\nOther than generating reviews, several works use reviews to improve rating prediction in some way (including work by the authors), e.g. HFT, JMARS, CoBaFi, RMR, etc. How do these compare in terms of RMSE performance?\nThe paper seeks to jointly model ratings and reviews in addition to temporal patterns. Existing papers have captured these aspects previously, but what's novel here is the particular combination of parts and choice of techniques. In particular, the use of LSTMs as opposed to \"bag-of-words\" models that have previously been used when combining the same components.\n\nA criticism is made of existing models that use bag-of-words features as being too \"coarse\" to capture the real dynamics of reviews. This seems a valid criticsm, though it's not clear exactly what features those existing models miss. Something missing from this paper is any interpretation of what the model \"learns\" that may explain its better performance.\n\nI also don't know about the significance of predicting \"future\" ratings as opposed to random splitting of the data. Lots of work on temporal recommender systems uses a variety of hold-out strategies besides random sampling, e.g. holding out the final ratings. Is there a methodological contribution associated with this change?\n\nThe experiments evaluate the extent to which the model achieves good performance due to its ability to capture evolving temporal patterns at the level of items, and the ability of the model to capture additional dynamics from text. Text makes a small but significant contribution; I was surprised by how large an improvement is achieved given how little text was included.\n\nOverall this is a reasonably strong experimental comparison, though could be improved in two dimensions:\n(a) There's no comparison to models that use ratings + text (e.g. CoBaFi, JMARS, etc.). These methods have reported substantial improvements over rating-only models in the past, perhaps even larger improvements than what is reported here. Such an experiment is important given the claim that existing models (in particular bag-of-words models) are too coarse to capture the dynamics of text.\n(b) The restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation. In particular, they're extremely dense datasets that support very parameter-rich models. I'd question whether the results would hold on sparser data (though in fact I'd suspect they would hold, since on a sparser dataset the contribution due to using reviews ought to be larger).\n\nOtherwise the experiments are fine. Perplexity results are nice but essentially what we'd expect. It is a shame that unlike other papers that use text to inform recommender systems there's no high-level analysis here of what the model has uncovered.\n", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJgqzryXl": {"type": "rebuttal", "replyto": "rJNaKDqGe", "comment": "Thank you for the comments.\n\nAll of the papers listed are valuable and interesting. We will make sure to add them to our related work, but we want to clarify the significant differences between our contributions and their models.\n\nIn particular, we differ from all of the listed works by modeling temporal dynamics and by modeling personalized reviews. In fact, to the best of our knowledge, this is the first paper that jointly models temporal dynamics of both users and items, ratings, and personalized reviews. [1,2,4,5,6] model item-specific content features, sometimes resulting in an RNN model but ignoring personalized reviews ([3] only uses ratings information). Rather, modeling personalized reviews presents unique challenges in integrating user and item knowledge, which we explore in this paper.\n\nOur primary goal in modeling all of these features is to improve the accuracy of ratings predictions. Because of this, when comparing with traditional approaches that do not consider temporal effects, we compare with baselines that have the strongest rating prediction performance on the Netflix and Movielens datasets. [1,2,4,5,6] did not present any such results. [3] shows overall comparable results to I-AutoRec (on MovieLens 1M: U-CF-NADE-S: 0.845, I-CF-NADE-S: 0.829, and I-AutoRec: 0.831; on Netflix [3] outperforms AutoRec.)  We therefore think that I-AutoRec already provides a strong neural network baseline.\n\nWe appreciate your valuable comments and will add these discussions into our next revision to clarify the difference between this paper and previous works.", "title": "re: Seem to miss references on DL-based RS"}, "HJJAWNpGx": {"type": "rebuttal", "replyto": "SkzM_kpfg", "comment": "Thank you for the questions. \n\n1) y_t is a function of x_t, 1_{newbie}, \\tau_t and \\tau_{t-1}.\n \nFor user RNN, the j-th element of x_t is the rating the user gave for movie j at time t, and is 0 if the user didn't rate movie j at time t.\nFor movie RNN, the i-th element of x_t is the rating the movie received from user i at time t, and is 0 if the movie was not rated by user i at time t.\n\nFor user RNN, 1_newbie is an indicator for new users (users who did not rate any movies before time t.\nFor movie RNN, 1_newbie is an indicator for new movies (movies that have no ratings before time t.\n\n\\tau is wall-clock time, that has the same definition for both users and movies. \n\n2) x_t, 1_{newbie}, \\tau_t and \\tau_{t-1} are concatenated as an 1-dimensional vector. y_t can thus be calculated by matrix-vector multiplication. Wall-clock time \\tau_t is a scalar, which is rescaled globally to have zero mean and unit variance. ", "title": "re: user and movie RNN inputs"}, "SkzM_kpfg": {"type": "review", "replyto": "HksioDcxl", "review": "1) How are the input y_t of user RNN differ from the input of the movie RNN?\n2) Can you provide how y_t is computed in detail? how are x_t, 1_{newbie}, \\tau_t and \\tau_{t-1} combined? How is \\tau_{t} represented?This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews. Interestingly, the model infers time-dependant\u00a0user/item vectors by applying an RNN to previous histories. Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews. The paper is well written and the architectural choices make sense. The main shortcomings of the paper are in the experiments:\n\n1) The full model (rating+text) is only applied to one and relatively small dataset. Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing.\n\n2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful. A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature.\n\nFinally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from. This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.", "title": "user and movie RNN inputs", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1J3OZY4l": {"type": "review", "replyto": "HksioDcxl", "review": "1) How are the input y_t of user RNN differ from the input of the movie RNN?\n2) Can you provide how y_t is computed in detail? how are x_t, 1_{newbie}, \\tau_t and \\tau_{t-1} combined? How is \\tau_{t} represented?This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews. Interestingly, the model infers time-dependant\u00a0user/item vectors by applying an RNN to previous histories. Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews. The paper is well written and the architectural choices make sense. The main shortcomings of the paper are in the experiments:\n\n1) The full model (rating+text) is only applied to one and relatively small dataset. Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing.\n\n2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful. A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature.\n\nFinally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from. This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.", "title": "user and movie RNN inputs", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJNaKDqGe": {"type": "rebuttal", "replyto": "HksioDcxl", "comment": "Thanks for the interesting work. The design looks intuitive and the performance looks good. It would be more convincing if more DL-based RS models are used as baselines, especially hybrid ones that take both ratings and content information (e.g., [4,5,6]). Besides the baselines, it seems to miss recent related work on deep learning methods [1-6] (especially recurrent ones) for recommender systems.\n\nThis paper looks particularly similar to the recent NIPS paper on Recurrent Autoencoder, considering they both use ratings and text data. Could you please explain more on the differences between them two?\n\n[1] Collaborative Recurrent Autoencoder: Recommend While Learning to Fill in the Blanks, NIPS 2016\n[2] Deep Neural Networks for YouTube Recommendations, RecSys 2016\n[3] A Neural Autoregressive Approach to Collaborative Filtering, ICML 2016\n[4] Collaborative Deep Learning for Recommender Systems, KDD 2015\n[5] Improving Content-based and Hybrid Music Recommendation Using Deep Learning, ACM MM 2014\n[6] Deep Content-based Music Recommendation, NIPS 2013", "title": "Seem to miss references on DL-based RS"}}}