{"paper": {"title": "Learning Private Representations with Focal Entropy", "authors": ["Tassilo Klein", "Moin Nabi"], "authorids": ["~Tassilo_Klein1", "~Moin_Nabi1"], "summary": "We propose a variant of entropy embedded in an adversarial representation learning setting to leverage privacy sanitization in a semantic-aware fashion.", "abstract": "How can we learn a representation with good predictive power while preserving user privacy?\nWe present an adversarial representation learning method to sanitize sensitive content from the representation in an adversarial fashion.\nSpecifically, we propose focal entropy - a variant of entropy embedded in an adversarial representation learning setting to leverage privacy sanitization. Focal entropy enforces maximum uncertainty in terms of confusion on the subset of privacy-related similar classes, separated from the dissimilar ones. As such, our proposed sanitization method yields deep sanitization of private features yet is conceptually simple and empirically powerful. We showcase feasibility in terms of classification of facial attributes and identity on the CelebA dataset as well as CIFAR-100. The results suggest that private components can be removed reliably. ", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper focuses on a notion of privacy in learning representations. \n\nOne of the primary concerns of the reviewers was clarity of the writing and results. Numerous concerns are mentioned in the reviews, and also more engagement with the fairness literature was desired. One reviewer felt that some of the claims in the paper were unsubstantiated, for example: understanding the sanitization process in a human-understandable visual way\", \"integration of a notion of interpretability\". It was felt that the changes required were more than could be expected for a camera ready version. The authors are recommended to revise the paper with a particular eye for clarity to a new reader.\n\nThe notion and measurement of privacy was also considered to be somewhat shaky. It is understood that the nature of privacy considered in this paper is different from differential privacy. That said, the latter is a rigorous definition, and the one in this paper seems to be rather empirical in nature. There are no formal guarantees in terms of privacy preservation, and it is not clear whether the representations could leak information when evaluated with a different network. As privacy is a mission-critical property, some justification of why the heuristic measurement of privacy is acceptable.\n\nAs a side note, the authors should consider using the \\citep command for parenthetical citations in the text."}, "review": {"bL5Z_W7XfEg": {"type": "rebuttal", "replyto": "RpZr1ZmJ0eb", "comment": "We thank the reviewer for the valuable and constructive feedback. \n\n**Problem setup: I see that this isn't the same as fair RL but I struggle to figure out exactly what it is. Perhaps some examples in the text would help - for instance by the time that I see that ID is the sensitive attribute in CelebA I have a better understanding; I wonder what other reasonable examples there are of target attributes? This would help greatly to explain the setup. And in this case, is the ID the sensitive attribute? or are the sensitive attributes the ones which allow you to infer ID? This is also a source of confusion for me**\nYes, sensitive attributes (S) correspond to the ID labels in the case of privacy-preserving RL, and the target attributes (T) is everything related to the desired downstream task. Examples of target attributes in the context of CelebA are 'sun-glasses,' 'gender,' 'beard,' etc. - see Table 2 in the supplementary material. The goal of ARL here is to learn a model that generates \u2018sanitized\u2019 data representations. This implies that the data representation facilitates training a post-classifier on it for a target attribute classification task. Simultaneously, it does not enable learning a classifier for sensitive attributes (i.e., ID). However, \u201cAttribute-level Privacy Analysis\u201d reveals that the accuracy of individual attributes that allow inferring ID is lower than those that correlate less with the ID.\n\n**Overlapping information: There is in fact fair RL work on this topic - for instance, learning representations under an equalized odds constraint (conditional independence). Your example (beard and gender) is quite unrelated to the experiments - maybe more pointed examples would help here as well** \nThanks for mentioning the related works in fairness literature. We agree fair RL is a related notion to our private RL problem, so we will cite relevant works and discuss them in the final version of the paper. \nExamples that expose correlations between sensitive attribute and target attribute are: \u2018Mahatma Gandhi\u2019 and \u2018bald,\u2019 \u2018eyeglasses,\u2019 \u2018Stevie Wonder\u2019 and \u2018sunglasses,\u2019 \u2018Queen Elizabeth II\u2019 and \u2018wearing hat,\u2019 \u2018Naomi Campbell\u2019 and \u2018high cheekbones,\u2019 \u2018attractive,\u2019 to name a few examples. In these examples, one or more target attributes are the characteristic attributes of the person. We will adapt the paper to accommodate the points.\n\n\n**Fig 3b - if only the high accuracy models matter, then why show the rest of them? If that's your criteria of evaluation I recommend being clear about that - Thank you for including the focal entropy equation, that is helpful.** \nThe \u2018privacy\u2019 and \u2018utility\u2019 both matter simultaneously in private RL, which is why in Fig. 3, we provide the performance across the entire trade-off spectrum. However, in our experiments, we emphasized more on the accuracy gain achieved in terms of utility, as it is actually where the challenge lies in the CIFAR benchmark. We will clarify this point about the evaluation in the paper. In fact, the utility is an often-neglected component in the privacy-preserving ML literature. This is one of the main reasons why lots of works on privacy can not be utilized in practice despite its theoretical merits. What makes our method superior compared to the competitors is that we managed to achieve the accuracy gain at the same or higher level of privacy. \n\n\n**Clarifying \"proper sanitization ...\": I'm still pretty unclear on this sentence. I understand the concept of focal entropy but I don't really follow the connection to representation learning and sensitive information.**\nFocal entropy incorporates the inter-class similarity in the entropy maximization formulation. It enforces the representation of the samples from \u2018similar\u2019 classes to be more indistinguishable from each other. This phenomenon sanitizes the representation from class information, such that (ideally) no later post-classifier can maliciously exploit it. Let us make this more concrete with a CelebA example. Given an image of \u2018Yoshua Bengio,\u2019 focal entropy would maximize the entropy w.r.t. \u2018Samy Bengio\u2019 rather than with \u2018Yann LeCun.\u2019\nConsequently, a hub would emerge, usurping \u2018Yoshua Bengio\u2019 and \u2018Samy Bengio\u2019\u2019. As this hub is only sanitized w.r.t. sensitive information that allows distinguishing between them, information of target attributes remains largely untouched. This explains the comparably high utility of the proposed approach compared to alternative approaches. See the end of Section 3.2 for a comprehensive discussion and supplementary material (5.2 Adversarial Identity Mapping) for empirical evidence of it.\t\t\t\t\n\n**this explanation on the failings of adversarial RL is good and should be expanded upon in the paper where the claim is made - as far as I know it's not something to expect readers to know** \nWe are happy that our explanation on failings of Adversarial RL made this point clear to you, so we will expand the paper accordingly in the final version of the manuscript.", "title": "Rebuttal"}, "S8Y8TsrgHc9": {"type": "rebuttal", "replyto": "j5yB8TANtJq", "comment": "**\u201cProblem setup is not explained well\u201d\n\"Difference to the fair RL**\nThe proposed approach does not specifically consider the notion of fairness. However, as the concepts are related our method theoretically could also be extended in this regard, leading to exciting future research. A key difference to fairness approaches is that our approach deals with attributes exhibiting *different levels* of privateness. Specifically, the model does not have any prior info about each attribute's \u201cprivateness\u201d level. It should learn a representation, which contains information about non-sensitive attributes while refraining from encoding sensitive attributes. As far as we know, no previous work in fairness literature looked at this. In contrast to that, the notion of sensitive and non-sensitive attributes is typically given a-priori in the fairness literature.  To shed more light on the correlation between privacy and individual attributes, we added the \u201cAttribute-level Privacy Analysis\u201d results to the appendix. It manifests that the more related an attribute is to the identity, i.e., the higher the privateness, the lower the achievable accuracy (e.g., male and mustache). \n\n**\u201cMotivation of \u201chighly overlapping information\u201d**\nWith overlap in information, we refer to the correlation between sensitive and non-sensitive, i.e., $S \\not\\perp T$. This is crucial, as this determines the degree of possible disentanglement in the adversarial setup. Whereas CelebA does not permit such clear segregation between public and private due to the inherent correlation, CIFAR-100 does permit such a separation due to the \u2018artificial\u2019 hierarchy between the sub and superclass.\n\n**\u201cFig 3b - trade-off curve\u201d**\nPlease, see answer *Results* to *AnonReviewer1*.\n\n**\u201cExact equation for focal entropy \u201d\n\u201creweighting vector is not introduced\u201d**\nTo make the concept of focal entropy and its optimization clearer, we added Equation (9) and (10) in the revised manuscript in Section 3. It explicitly presents how to obtain off-centered entropy.\n\n**Clarifying: \u201cproper sanitization must be conducted w.r.t to focus classes\u201dand \u201centropy peak shifted such that uniformity wrt similar classes gets dominant\u201d**\nThe proposed approach builds upon the idea of calibrating the distribution matching for the sensitive attribute, tailoring it to a privacy setup. This is achieved by a simple yet effective modification of Shannon entropy. Shannon-entropy has its maximum in a class-agnostic fashion at uniform distribution. Disregarding the inherent structure makes optimization susceptible to shallow sanitization (shortcuts). In contrast, focal entropy enforces group-wise equiprobability by dividing the probability mass. Hence, maximization with focal-entropy is analogous to an asymmetric entropy. Asymmetry leads to inverse skew-sensitivity due to matching an imbalanced distribution.  The high-level notion of focal entropy is further illustrated in Fig. 2. Please also see the discussion about *equilibrium* with *AnonReviewer1* for more details. We added equations (9) and (10) in Section 3 to make the focal entropy concept clearer. \n\n**Clarifying: \u201cExplain the suboptimality\u201dand \u201cthe solutions mentioned earlier can only meet its practical promises when the private attr. do not strongly correlate with the target attr.\u201d**\nStandard ARL entails maximization of two likelihoods. First, an adversarial network that seeks to classify and extract sensitive information from a given representation. Second, an encoder network seeks to infer a compact data representation while preventing sensitive information leakage. This results in a zero-sum minimax game formulation, which is practically sub-optimal from the perspective of preventing information leakage. This is due to the requirement of two conditions to be fulfilled to reach optimality: First, the existence of an equilibrium of minimax game players, and second, the practical optimization ability procedures to converge to such an equilibrium. Inability to reach equilibrium in terms of likelihoods consequently entails leaking the most amount of information. In contrast, optimizing for the uniformity in distribution over the sensitive labels (maximization of entropies) provides no information to the adversary and hence is less likely to leak sensitive information. However, the optimality condition of uniform log-likelihood of the predictor is predicated on the existence of an optimal discriminator (oracle classifier) and equilibrium between the players, which in practice is unattainable. Please see *(Roy & Bodeti, 2019)* for a comprehensive discussion.\n\n**\u201cWould like to see some sort of statistic on hubs, along with these pictures\u201d**\nIn the revised manuscript, we added the average node degree for each graph. Starting with conventional entropy and then continuing to focal entropy - the average node degree decreases monotonically with the growing size of k-NN. See the very end of section 4.2.\n", "title": "Rebuttal:"}, "qjODClYjqso": {"type": "rebuttal", "replyto": "k26Ujfge5lR", "comment": "**\u201c(a) Motivation for two pairs of the classifiers\u201d\n\u201c(b) Reasoning behind adv. loss on target\u201d**\nThe non-adversarial predictors enforce the utility and the presence of the associated information in the representation (target information in the $z_{tar}$ and sensitive in $z_{res}$, the adversaries inhibit undesirable information leakage across the representation partitions. Thus are responsible for disentanglement and sanitization. Moreover, as the adversaries are learned during training, they act as surrogates for oracle post-classifiers.\n\n**(c) \u201cWhy is $\\tilde{S}$ a function of $\\tilde{\\theta}_{tar}$?\u201d \n(d) \u201cAdv. predictor is not defined\u201d\n(e) \"Extend Eq. 5\u201d**\nWe significantly modified the method section and incorporated the suggested hints. Specifically, we adapted the notation towards simpler terms and fixed typos that confused. In terms of the sanitization terms, we added Equation (9) and (10) to make the focal entropy concept and its optimization clearer.\n\n**(f)  VAE**\nIntegration of VAE into the proposed approach was done to *i)* improve representation learning accompanied by disentanglement, and *ii)* provide interpretability.\n*i)* It has been shown that integration VAEs into representation learning in combination with supervision boosts the performance of downstream tasks (Le et al. (2018); Gyawali et al. (2019)). We similarly observed higher predictor acc. Additionally, VAEs are known to promote disentanglement, which can be attributed to the encoder KL-term via independent priors (Burgess et al., (2018)). In our scenario, this serves the objective of separating sensitive and non-sensitive attributes into separate subspaces.\n*ii)*  Interpretability is crucial yet often overlooked in privacy literature. Integrating the VAE into the minimax game facilitates understanding the sanitization in a more human interpretable fashion.\n\n**\"increases the uncertainty in a more organic fashion\"**\nThis refers to the removal of sensitive information in a semantic-aware and systematic fashion, leveraging the inherent class structure. Given the example of CIFAR, we want a 'whale' to be more confused with another type of fish, rather than, e.g., with the class \u2018vehicles\u2019. Doing so helps eliminate \u2018fine-grained\u2019 features, giving rise to the guided sanitization.\n\n**Results**\nGiven a correlation between the target and sensitive attributes, the minimax nature of the ARL entails a compromise between privacy and utility.  The proposed approach accommodates this trade-off in a principled fashion, as changing the *'' focus''* and the size of it (i.e., nearest neighbors) directly affects the shape of the trade-off response curve (see Figure 2). In this respect, *MaxEnt-ARL* (Roy & Bodeti, 2019)} which imposes uniformity in a class agnostic fashion is a special case of our proposed focal entropy. In the trade-off curve, focal-entropy performs significantly better than *MaxEnt-ARL* in the relevant operating range (i.e., high target acc.).  \nWe also outperform *Kernel-SARL* sharply in the \u2018linear\u2019 case. In the \u2018kernelized\u2019 case, we gained higher performance in the targeted utility domain as well (achieving 2% accuracy improvement at an identical privacy rate is extremely challenging ). The \u2018kernelized\u2019 case, however, cannot be directly optimized in an end-to-end fashion, so it is not directly applicable to DNN-based solutions(as admitted by the authors). Furthermore, the performance reported by both methods is obtained as non-dominated solutions and may not be directly comparable. \n\n**\u201cleakage of information when not achieving equilibrium\u201d**\nEmploying *ML-ARL* is subject to information leakage due to optimization of log-likelihood for sanitization. In contrast, the theoretical optimality of MaxEnt-ARL for sanitization is predicated on the existence of an optimal discriminator (oracle classifier) and minimax equilibrium. Despite the theoretical optimality, the objective of the latter is unattainable in practice. Our proposed Focal-entropy method goes towards maximization of entropy by integrating MaxEnt-ARL as a special case. This is achieved trading in optimality with quasi-optimality by means of \u2018approximating\u2019 the optimization objective. Such approximation is simply achieved by enforcing two separate uniform distributions for the \u2018similar\u2019 and \u2018dissimilar\u2019 classes (see revised manuscript for a discussion).\n\n**Evaluation on reconstructed samples**\nThe purpose of reconstruction is to improve representation learning and interpretability. As the reconstruction quality was a minor point for us, we just added some example reconstructions to the supplementary material. However, we consider reporting the Inception Score of the reconstruction of our method (with the final version of this manuscript) for the sake of future comparison.\n\n**\u201cSome terms seem to be overloaded\u201d**\nWe adapted and, in particular, streamlined the notation to avoid misunderstanding and clutter.", "title": "Rebuttal:"}, "N-rVES4i6o": {"type": "rebuttal", "replyto": "2fLzVfVuS7h", "comment": "We thank the reviewer for constructive feedback. \n\n**1) \u201cHow to divide the representation $z$ encoded by the encoder into two sub-representations, namely, target and residual representations.\u201d** \nThe representation is divided equally, with dimensionality was chosen to be comparable to other approaches. Dividing the representation imposes no hard restriction -- as long as the latent dimensionality provides ample capacity for the downstream tasks. Furthermore, to ensure sufficient capacity and avoid degenerate solutions, we also evaluated the classification accuracy not only for the target variable on $z_{tar}$ but also of the sensitive variable on $z_{res}$.\n\n**2) \u201cThere is a high-level description of focal entropy instead of equations.\u201d**\nTo make the concept of focal entropy more clear, we added equations (9) and(10) to the manuscript, which details the recalibration of probabilities to achieve the off-centered maximum of the entropy.\n\n**3) \"Some notations are very confusing. For example, in the optimization goal (Equation (1)), the adversarial loss on target is related to the parameter $\\theta_{res}$. However, in the following description, this loss is related to $\\theta_{tar}$ (Equation (4))\"**\nWe thank the reviewer for the hint. To make the manuscript clearer and avoid potential misunderstanding, we adapted the notation, added color coding, and the corresponding graphical model ( see Fig. 1).\n\n**4) \u201cThis work is done in an adversarial training manner, can leakage reduction be achieved in a differentially private training manner, i.e., training the encoder using dp-sgd?\u201d**\nOur method is fundamentally different from DP-SGD, as we aim to learn a private *representation* instead of preserving privacy at the *parameter* level. While we do not consider a DP framework here, our method could employ differential privacy during the post-classifier training. We emphasize that the methods dealing with DP are rather orthogonal to the direction of the proposed work. So focal-entropy can be integrated seamlessly into DP frameworks, making representation learning more robust.\u00a0", "title": "Rebuttal:"}, "2-98eITShT": {"type": "rebuttal", "replyto": "J94qMQPxRSW", "comment": "We thank the reviewer for the valuable and constructive feedback. We incorporated the feedback of the review in the revised manuscript.\n\n**1) \u201cMy main concern is the experimental results are only on two datasets [...]\"** \nWe perform an extensive set of experiments on two standard benchmarks. Please also see supplementary material. As the first testbed, we adopt the \u201csimulated\u201d privacy problem proposed by Roy & Boddeti (2019) designed on the CIFAR-100 dataset. We picked this benchmark as it captures a \u201cperfect\u201d separation between target and sensitive information, i.e., $S \\perp T$. We compare with state of the art adversarial and entropy-based representation learning methods for privacy on this benchmark. Second, the CelebA dataset where no clear segregation between target and residual spaces is possible, i.e., $S \\not\\perp T$. The \u201cin-the-wild\u201d nature of face images offers a richer testbed for our method as both identities, and contingent factors are significant sources of variation. We compare with many recent baselines on this challenging benchmark, demonstrate performance curves, and additional analysis to explain the model's behavior (as pointed out by other reviewers too). We agree that having more \u201clarge-scale\u201d datasets for the evaluation would be better to substantiate the claim of the empirical effectiveness of the proposed method. However, such benchmarks are still pretty limited, which can largely be attributed to the nature of the topic -- privacy. We expect more of such datasets to emerge as the topic lately has enjoyed increased attention in the community. Nevertheless, we are convinced that the results with the proposed approach would show the same pattern on other datasets.\nTo the best of our knowledge, our paper is the first work that proposes to taking the class similarity into account for the entropy of an adversary.\n\n**2) \u201cI'm guessing it can also protect the dataset-level (proprietary) attribute inference. I'm interested in seeing the results [...]\u201d**\nWe thank the reviewer for the suggestion and agree on this point. In this regard, we added an analysis of the correlation between privacy and individual attributes on CelebA.  In line with the reviewer's expectation, our attribute-level analysis shows that our method achieves relatively lower target accuracy for dataset-level proprietary attributes compared to the more generic attributes. See the \u201cAttribute-level Privacy Analysis\u201d section in the supplementary material.\n\n**3) \u201cIt's not clear if focal entropy should be used in both $\\tilde{T}$ and $\\tilde{S}$. The paper mentioned \"training of  $\\tilde{T}$  leverages a modification of entropy\u201d**\nWhereas we want the target representation to be sanitized w.r.t. sensitive attributes, we \u201conly\u201d enforce disentanglement w.r.t. target attributes on the residual representation. As the target attributes on the residual representation are of no privacy relevance, we leverage only conventional entropy Equation (5) to enforce disentanglement. Whereas for sanitization of the target representation, we leverage focal-entropy Equation (9),(10).\n\n**4) \u201cI don't think \u03c0 is clearly mentioned before Equation (9). The notation should be more precise.\u201d**\nTo make the focal entropy concept and its optimization clearer, we added Equations (9) and (10) in the revised manuscript. As requested, we further elaborate on the transformation of probabilities and the final objective to maximize focal entropy.\n", "title": "Rebuttal:"}, "2fLzVfVuS7h": {"type": "review", "replyto": "wG5XIGi6nrt", "review": "\n### Summary\nThis paper presented a method for learning private representations. This method is based on adversarial representation learning and the main technique contribution comes from focal entropy. The experimental results show that focal entropy can improve the accuracy of the target predictor without increasing adversarial accuracy.\n\n### Pros\n1. Focal entropy is effective in reducing the information leakage of the learned representation while improves the target accuracy over the state-of-the-art.\n2. This paper covers a wide range of experiments.\n\n### Cons\n1. Some technique details are not presented clearly. I have listed some of them here:\n   a. How to divide the representation $z$ encoded by the encoder into two sub-representations, namely, target and residual representations.\n   b. There is a high-level description of focal entropy instead of equations.\n2. Some notations are very confusing. For example, in the optimization goal (Equation (2)), the adversarial loss $\\phi_{tilde{S}}$ is related to the parameter $\\theta_{res}$. However, in the following description, this loss is related to $\\theta_{tar}$(Equation (6))\n\n### Comments\n1. The optimization objective involves many individual terms. The experiments should further provide results to show how the trade-off parameters $\\beta$ controls privacy leakage and target accuracy.\n2. This work is done in an adversarial training manner, can leakage reduction be achieved in a differentially private training manner, i.e., training the encoder using dp-sgd?", "title": "Review ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "j5yB8TANtJq": {"type": "review", "replyto": "wG5XIGi6nrt", "review": "Summary: This paper gives a method in the class of learning representations which have some information censored. In particular, the authors propose a setup where there are many \u201cprivate\u201d classes, and some classes are more similar than others \u2013 this maps (I think) onto the privacy setting, where each class is like one individual. They give a modification of an entropy loss, focal entropy, which is conducive to this type of learning, and show in experiments that this method can be successful.\n\nI recommend reject due to a lack of clarity in the formulation, motivation, and writing of this paper. I think the idea has promise, the experiments are fine, and if correctly communicated can be a successful paper, but as it stands I found the paper pretty confusing and I\u2019m not really sure what the method is for.\n\nStrong points: \n-\tThe problem as I understand it is interesting and the work is well-situated in the privacy literature. \n-\tFocal entropy seems like a good idea and as far as I know is novel as a loss. \n-\tExperiments on CIFAR and CelebA demonstrate some good behaviours from this method, mostly beating baselines\n-\tExperiments are mostly good, decently thorough\n\nWeak points and Clarifications:\n-\tThe main weak point of this paper is the exposition and clarity \u2013 I find that the problem setup is not explained particularly well. Especially as someone who is more familiar with the fair representation learning literature, I get confused when the authors refer to a sensitive attribute, or private part of the data, in this setting \u2013 it seems to be a different notion than I am used to and it is never clearly defined. If I squint I can see how it maps onto privacy nicely but I would prefer if the authors make that clear.\n-\tThe introduction could use a rewrite \u2013 it doesn\u2019t set up the main points of the paper particularly clearly and leaves the reader a little confused. Consider stating more clearly off the top what the problem statement is, and move much of the content of p1 to related work\n-\tThe motivation of \u201chighly overlapping information\u201d is pretty imprecise. Having read the paper I can kind of see what you\u2019re getting at but it\u2019s not clear \u2013 I think you mean instead information hierarchy? Sub and super classes\n-\tAs far as I can tell the exact equation for focal entropy is never actually given? Let me know if I\u2019m wrong\n-\tFigure 3b \u2013 you mentioned the better tradeoff in the high accuracy domain, but not what appears to be the worse tradeoff in the low accuracy domain. Is there anything I\u2019m missing about understanding this figure?\n\nOther feedback:\n-\tP2 \u201cthe solutions mentioned earlier can only meet its practical promises \u2026\u201d \u2013 need a citation for this or explanation\n-\tP2: explain these suboptimalities more, I\u2019m not sure exactly what you\u2019re referring to\n-\tP2: you bring up a \u201cvast number of dissimilar classes\u201d without explaining why that\u2019s relevant \u2013 in much work we deal with binary attributes/classes\n-\tEq 4: looks a lot like standard fair representation learning, so need to explain more clearly in motivation why it\u2019s different\n-\t\u201cAlthough maximization of entropy is sufficient for nonprivate attributes to minimize information leakage across representation partitions, we postulate that proper sanitization must be conducted w.r.t to focus classes in a similarity-aware fashion\u201d \u2013 this sentence is very important to the paper, and I don\u2019t quite understand any of it. If you work hard on this sentence (why do you postulate that? What is a focus class? Why would we ever consider similarity-awareness and what is that?) you\u2019ll go a long way to clarifying your intro\n-\tTop of 3.2 \u2013 need to clarify what s is\n-\tEq 7 \u2013 this makes it look like Y_similar is a set of tuples but I think you mean set of elements of Y?\n-\t\u201cwould like the entropy peak shifted such that uniformity wrt similar classes gets dominant\u201d \u2013 this is another idea which is not being clearly communicated right now and seems central to this work\n-\tThe \u201creweighting vector\u201d is not introduced \u2013 not clear what we will do with it. You never actually show how this is included in the entropy calculation\n-\tWould like to see some sort of statistic on hubs, along with these pictures\n", "title": "I found the paper confusing, obscuring what might be a nice idea & experiments", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "k26Ujfge5lR": {"type": "review", "replyto": "wG5XIGi6nrt", "review": "## Summary\n- The paper tackles the problem of adversarial representation learning i.e., to learn a low-dimensional representation of the image that encodes target-relevant non-sensitive information, but not (often correlated) sensitive information.\n- The approach follows a VAE-based adversarial framework, with the core novelty being the use of maximizing focal entropy  (i.e., entropy among a set of similar attributes) over sensitive attributes in the representation.\n- Evaluation performed on CIFAR100 and CelebA indicates that the approach outperforms similar baselines under certain conditions.\n\n---\n\n## Strengths\n\n**1. Focal entropy**\n- I appreciate the insight exploited by the authors -- to encourage representations with high entropy over a small set of correlated sensitive attributes\n\n**2. Evaluation**\n- I found the evaluation quite thorough. The paper compares with many recent baselines, demonstrates performances curves and additional analysis to explain the model's behaviour.\n\n---\n\n## Concerns\n\n### Major Concerns\n\n**1. Objective**\n- There are a few things unclear to me in the objective (Eq. 1-6) and would appreciate if the authors clarified them.\n- (a) I don't get the motivation for the two pairs of the classifiers $(T, S)$ and adversarial $(\\tilde{T}, \\tilde{S})$ that tries to simultaneously minimize/maximize both the target $p(a|z)$ and sensitive attributes $p(y|z)$.\n- (b) Especially, I don't understand the reasoning behind having an adversarial loss on the target attribute -- shouldn't this be maximized in all cases?\n- (c) I am also confused with the sanitization term (Eq. 6). In particular of why the sensitive attribute classifier $\\tilde{S}$ is a function of the target attribute classifier $\\tilde{\\theta}_{tar}$?\n- (d) The term $\\phi_{\\tilde{T}}$ appears to be undefined to complement Eq. 6.\n- (e) It would also be nice if the authors extended Eq. 6 (which maximizes standard entropy) with the final objective to maximize focal entropy.\n- (f) Is the VAE term (Eq. 5) necessary (esp. reconstruction)? After all, this appears as a secondary objective opposed to the primary objective of learning a representation $z$ which minimizes information leakage.\n\n**2. Writing - Sec. 3**\n- My issues in understanding can be partly attributed to the writing in Sec. 3, which I found difficult to follow for a few reasons.\n- (a) Some terms seem to be overloaded e.g., two notations for encoder $q(x; \\theta_E)$ and $E(x; \\theta_E)$. I was also thrown off by many ways the components of the model/objective are conveyed: Losses $\\phi_T$, parameters $\\theta_T$, player $T$, adv. player $\\hat{T}$, etc. I recommend finetuning the second paragraph on page 4.\n- (b) While the architecture in Fig. 1 is designed to accompany the text, I find it somewhat incomplete and unclear. For one, there are six players/blocks in the proposed architecture. However, only the encoder and decoder are shown in the figure. Furthermore, while there are five loss terms, only four of them are shown in the figure. I am also not sure if the blue/red backgrounds in the residual and target streams code something in particular. Perhaps one solution is to split the figure into two: one for the architecture and another for information flow?\n\n**3. Results**\n- While the focal entropy makes sense to me (i.e., by increasing entropy over a small set of similar classes), I wonder if the results confidently back up that is it indeed better than related baselines.\n- (a) The improvements seem somewhat marginal e.g., an improvement of 2% accuracy (Table 1) over prior work on both CIFAR100 and CelebA.\n- (b) But what I find more revealing of the performance is the trade-off curve in Fig. 3b (thanks for the presenting this!). It appears that the proposed focal entropy approach (blue curve) outperforms Kernel-SARL (red curve) only for in a small operating range of high target accuracy. Overall it appears that MaxEnt-ARL and Kernel-SRL offers better trade-offs.\n\n\n### Minor Concerns\n\n**4. Decoder/Reconstruction**\n- Since the task is to partly perform reconstruction as well, I find missing evaluation on how good the reconstructed samples are.\n\n**5. Same equilibrium issues as before?**\n- The introduction (second paragraph, p2) remarks that the adversarial min-max formulation of the problem has an issue that there is significant leakage if the optimization does not reach equilibrium.\n- Wouldn't this be an issue in the proposed work as well?\n\n### Nitpicks\n\n**6. Some nitpicks**\n- Please label the axes in Fig. 2.\n- Not sure what this sentence means \"increases the uncertainty in a more organic fashion\" - please rephrase\n- Typo in citation \"Radovanovi263 et al. (2010)\"", "title": "Blind Review", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "J94qMQPxRSW": {"type": "review", "replyto": "wG5XIGi6nrt", "review": "########################################################################## Summary: The paper studies how to learn private representations that only captures the non-sensitive attributes of the dataset. They propose an adversarial representation learning method that employs VAEs. Specifically, the architecture in the VAE contains 6 players with 2 as adversarial classifiers. They introduce focal entropy as the objective function instead of entropy for adversarial classifiers to achieve deep sanitization. They empirically evaluate the method by reporting the target task accuracy and attribute inference accuracy on two datasets.\n\n########################################################################## Reasons for score: I like the paper largely. It provides a nice practical method for sanitization, although it is hard to give theoretical privacy guarantees of VAEs. The method has shown to be a good defense for individual attribute inference attacks. My main concern is the experimental results are only on two datasets with one task/sensitive attribute setting. As an empirical/methodology paper, I would expect more empirical results. \n\nAlso, I have a question regarding the sanitization section. It's not clear if focal entropy should be used in both $\\tilde{T}$ and $\\tilde{S}$. The paper mentioned \"training of $\\tilde{T}$ leverages a modification of entropy ...\" However, I think it should be $\\tilde{S}$ or both. It seems that Equation (6) should be negative KL divergence since we want to force the distribution to be close to the uniform distribution. \n\n########################################################################## Minor Comments:\n\n1. In Equation (6), the bracket should be after $\\theta$.\n\n2. I don't think $\\pi$ is clearly mentioned before Equation (9). The notation should be more precise.\n\n 3. I'm guessing it can also protect the dataset-level (proprietary) attribute inference. I'm interested in seeing the results, but it is not required for this submission.", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}