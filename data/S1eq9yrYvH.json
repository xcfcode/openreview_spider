{"paper": {"title": "Subjective Reinforcement Learning for Open Complex Environments", "authors": ["Zhile Yang*", "Haichuan Gao*", "Xin Su", "Shangqi Guo", "Feng Chen"], "authorids": ["yzl18@mails.tsinghua.edu.cn", "ghc18@mails.tsinghua.edu.cn", "suxin16@mails.tsinghua.edu.cn", "gsq15@mails.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn"], "summary": "", "abstract": "Solving tasks in open environments has been one of the long-time pursuits of reinforcement learning researches. We propose that data confusion is the core underlying problem. Although there exist methods that implicitly alleviate it from different perspectives, we argue that their solutions are based on task-specific prior knowledge that is constrained to certain kinds of tasks and lacks theoretical guarantees. In this paper, Subjective Reinforcement Learning Framework is proposed to state the problem from a broader and systematic view, and subjective policy is proposed to represent existing related algorithms in general. Theoretical analysis is given about the conditions for the superiority of a subjective policy, and the relationship between model complexity and the overall performance. Results are further applied as guidance for algorithm designing without task-specific prior knowledge about tasks.\n", "keywords": ["reinforcement learning theory", "subjective learning"]}, "meta": {"decision": "Reject", "comment": "The authors propose a learning framework to reframe non-stationary MDPs as smaller stationary MDPs, thus hopefully addressing problems with contradictory or continually changing environments. A policy is learned for each sub-MDP, and the authors present theoretical guarantees that the reframing does not inhibit agent performance.\n\nThe reviewers discussed the paper and the authors' rebuttal. They were mainly concerned that the submission offered no practical implementation or demonstration of feasibility, and secondarily concerned that the paper was unclearly written and motivated. The authors' rebuttal did not resolve these issues.\n\nMy recommendation is to reject the submission and encourage the authors to develop an empirical validation of their method before resubmitting."}, "review": {"rJgpj9tDjB": {"type": "rebuttal", "replyto": "SklceeAvKS", "comment": "We appreciate the time you took reviewing our submission and hope our response help address some of your concerns.\n\nReplies to the main part of review:\n(1) Q: \u201cthe theoretical analysis is difficult to follow and there is sometimes a lack of clarity throughout the paper\u201d\nA: We noticed some mistakes in symbols and definitions of variables and corrected them in the updated version. Hope the new version can make it easier to understand.\n\n(2) Q: \u201cit isn\u2019t very clear how easy this framework would be to implement aside from the theoretical guarantees and there aren\u2019t any experiments or proofs of concept that would demonstrate the feasibility or practicality of the proposed framework in a real scenario\u201d\nA: As we mentioned in section 5, we realize that there may be some serious difficulties in application of our framework to real scenes. In fact, the main intent of this paper is to take a step forward in theory towards algorithm design considering general cases of reinforcement learning tasks. We acknowledge that concrete examples may well help exhibit the problem we are considering and prove the practicability of our work.\n\n(3) Q: \u201cthe paper would benefit from more discussion of how their work differs from related techniques (like hierarchical RL, various forms of meta-learning, etc.).\u201d\nA: In section 2 we briefly introduce some closely related fields. We summarize that techniques in these fields are introducing task-specific designs to reach better performance in certain kinds of tasks. Our work hope to unify such techniques and provide efficient algorithm designs without human prior. From this perspective, we agree that more discussions can help strengthen our conclusion and clarify our idea, but we think there is no other comparable differences between our work and other concrete techniques. Please see (3) in the part below for one example.\n\nReplies to the concrete points:\n(1) Please see replies to question 2 in above part.\n\n(2) In our framework, \u201c\\kappa\u201d denotes the information provided by the environment other than current state \u201cs_t\u201d, and so our method only chooses how to utilize it according to its actual instantiation in tasks. For a concrete example, consider our subjective \u201c\\bold{h}\u201d is designed to be a one-hot vector, then we get m=N_S; the actual data (including \\kappa) and the chosen form of loss function \u201c\\script{L}\u201d give concrete bounds defined in eq. (24); given expected \\zeta and \\eta, inequation (23) gives us the relationships m, u_b, u_d (VC dimensions of function approximators) should obey. In this way, we can determine N_S with given data and without domain knowledge.\n\n(3) In hierarchical RL the hyperparameters of function approximators (i.e. the layer num of neural networks) and the maximum number of lower-level policies are designed by designer; while in our work we propose to adjust them according to actual data automatically and with theoretical guarantee, based on our analysis of the relationship between overall performance and some key variables.\n\n(4) \na) Thanks for your advice. We will pay more attention to the simplicity of notations in our future work.\nb) We wanted to express \u201cthe number of each kind of data samples tends to infinity\u201d as the condition for theorem 1, to remove the possibility of insufficient exploration. To make it clearer, we update it to \u201call possible data samples appear infinite times\u201d.\nc) The main idea in the proof of theorem 1 is to show that any policy in original forms can be expressed by a subjective policy. We use \u201cfake\u201d to emphasize that the defined \u201c\\pi_{z,fake}\u201d shares the same form as subjective policy but is actually equal to the policy in the original form.\nd) Yes, we are assuming \\kappa is pre-determined with the tasks. In this paper we only focus on how to use given information with theoretical guarantee. When there are more than one sources (types) of \\kappa available, we may consider applying eq. (23) to these sources respectively and select one according to some preferences, i.e. affordable VC dimension of function approximators.\n", "title": "Author response to Review #1"}, "ryedVcYPsr": {"type": "rebuttal", "replyto": "SJlHqPKaKS", "comment": "Thank you for taking the time to review our submission and for your feedback. We hope the following may address some of your concerns.\n\nReplies to the main part of review:\nWe regret not having made it clearer about our design of the theorems:\n1) Theorem 1 aims to provide a basic result that the subjective policy will not get worse results than the original form of policy; this is quite easy to get and not practically useful.\n2) In theorem 2 we continue to analyze the relation between the bound of performance of a converged policy (lim|G(\\pi)-G*|) and the worst-case bound of error on function approximation (\\epsilon); this enables us to analyze the final performance through analyzing the function approximator, where the subjective policy makes a difference.\n3) Then in theorem 3 we get the relationship between the risk bound of the function approximator and some variables that are determined by data and the hyperparameters of selected function model; this enables us to control the overall performance through adjusting the related variables according to the data got in specific tasks.\n\nIn this paper, we wish to take a step forward in theory towards algorithm design with considering general cases of reinforcement learning tasks. As we analyzed in section 5, currently the variables (e.g. m, u_b, u_d) can theoretically be adjusted but there do exist difficulties when considering some function models, and we acknowledge that concrete examples may well help prove the practicability of our work.\n\nReplies to \u201cgeneral comments\u201d:\n1) Q: \u201cThe definition of rewards is confusing; script R was never defined.\u201d\nA: We noticed this mistake and have corrected it in the updated version.\n\n2) Q: \"minimize objective 1\" should be actually be \"maximize objective 1\"?\nA: This is another mistake in writing. Thanks for your correction. Again, we corrected it in the updated version.\n\n3) Q: Rewards are usually defined over state-action pairs, not just states. Why choose this unconventional formulation for rewards?\nA: We recognize that in many publications the reward function is defined as R_0(s, a). In this paper, we are not introducing anything special by defining reward function as R(s). In fact, we think these forms are equal, because R_0(s, a)=E_{s\u2019}[R(s\u2019)P(s\u2019| s, a)] (here s\u2019 stands for the next state).\n", "title": "Author response to Review #2"}, "Hye0HYKwsB": {"type": "rebuttal", "replyto": "rJg8QSNJ9S", "comment": "Thank you for your detailed and insightful review. We hope the following address some of your concerns.\n\nQ: \u201cI believe this hinges on \"h\" being sensible, which might not be feasible without task-specific prior knowledge.\u201d\nA: In this paper, \u201ch\u201d is used as one component to model the decomposition of the policy according to \u201c\\kappa\u201d i.e. information provided by the environment other than current state \u201cs_t\u201d. We acknowledge that 1) in cases where there are not enough \u201c\\kappa\u201d to decompose the policy and thus to avoid data confusion, our proposed framework cannot bring improvements, and 2) when there is task-specific knowledge available, \u201ch\u201d can be designed in a task-specific way and the overall performance may be better. However, we think these does not mean \u201ch\u201d relies on task-specific prior knowledge. Despite the difficulties in application, the proposed guidance for algorithm bases on eq. (23), which consists of only properties of the data collected, but not prior knowledge. Maybe we should have made it clearer that, the intent of this paper is to take a step forward in theory towards efficient algorithm design considering general cases of reinforcement learning tasks.\n\nQ: \u201cthere is not a single experiment demonstrating how any of this would behave in practice\u201d; \u201cI am not convinced that the proposed bounds can easily be concretized for an instantiation of the proposed framework, especially when considering deep networks\u201d\nA: We acknowledge that the lack of example instantiations makes our work not satisfactorily convincing, but we tend to take a step forward in theory and focus on more general cases. Concrete examples and algorithm instantiations are planned as future work.\nAs for concerns about neural networks, our result eq. (23) requires \u201cu_b\u201d and \u201cu_d\u201d i.e. the VC dimension of functions, which have been analyzed by some works such as [1]. If we construct useful relationships between hyperparameters of neural networks and its VC dimension, our theoretical result can be applied.\n\nQ: \u201cwhat is \"/R\" (curly R)?\u201d\nA: We are sorry for the mistake in some symbols. In previous version we wanted to use \u201cnormal R\u201d to denote the reward function defined on the state space, and the \u201ccurly R\u201d to denote the set of possible reward values. In the updated version we correct this mistake, i.e. only \u201cnormal R\u201d is used and is in the form of a probability distribution. Further, we expand the notation for risk functions to \u201cRisk(...)\u201d to avoid confusion.\n\nQ: \u201cIn Theorem 1, I believe it should be \"the gap \\delta >= 0\" and not \"the gap g >= 0\", no?\u201d\nA: Thanks for your correction. We fixed this mistake in the updated version.\n\nQ: some writing problems\nA: In our updated version, we have corrected some mistakes in grammar and citation, including the ones mentioned in this problem.\n\n[1] Sontag, E. D. (1998). VC dimension of neural networks. NATO ASI Series F Computer and Systems Sciences, 168, 69-96.\n", "title": "Author response to Review #3"}, "SklceeAvKS": {"type": "review", "replyto": "S1eq9yrYvH", "review": "This paper introduces a new framework for reinforcement learning (named subjective reinforcement learning) which aims to resolve some of the inherent problems with RL in open environments.  The authors posit that one problem with RL in open settings is \u201cdata confusion\u201d, which they describe as being situations where there are external factors (e.g. timeframe) that affect the action space differently.  They propose a \u201csubjective reinforcement learning framework\u201d which, as I understand it, can be described as an ensemble of traditional MDP\u2019s subject to external factors k.  The paper evaluates how this subjective policy compares to traditional MDP\u2019s in terms of theoretical bounds on performance. \n\nAlthough this learning framework seems like a potentially interesting future research direction, I tend to lean towards rejection for the reasons that: (1) the theoretical analysis is difficult to follow and there is sometimes a lack of clarity throughout the paper, (2) it isn\u2019t very clear how easy this framework would be to implement aside from the theoretical guarantees and there aren\u2019t any experiments or proofs of concept that would demonstrate the feasibility or practicality of the proposed framework in a real scenario, (3) the paper would benefit from more discussion of how their work differs from related techniques (like hierarchical RL, various forms of meta-learning, etc.).\n\nHere are some more concrete points:\n(1) The feasibility of this framework in a real-world scenario seems a bit hard to imagine and a strong use-case or proof-of-concept would be very helpful.  I liked that this paper provided some theoretical analysis for guiding the design of these systems.  However, it seems like these claims would also benefit from detailed empirical analysis and experiments.  Without empirical results, I feel a bit skeptical about how straightforward it would be to implement such a system or whether it would really be significantly useful in practice.  Similarly, though there are theory-based suggestions for how to optimally design such a system, it might be difficult to implement this system with optimal hyperparameters in a real-world use-case and the challenges in doing so are not really addressed.\n(2) In spite of claims that this method is able to be trained without domain knowledge, it seems like domain knowledge would still be necessary for things like determining what external information (K) is available and necessary, determining the appropriate N_S, etc.  It may be helpful for the authors to explain a bit more about how these things can be determined in a truly agnostic way.\n(3) It seems like there should be more discussion of the difference from hierarchical reinforcement learning.  In practice, hierarchical RL also can be used in similar ways to what\u2019s described here.  As the authors point out, hierarchical RL is not necessarily splitting into submodels that handle data confusion problems, it seems like that is a constraint that could be added into a hierarchical framework\u2019s design.\n(4)  I appreciate that the authors provide detailed theoretical analysis, but it can sometimes be confusing and difficult to follow.  I had some trouble evaluating the correctness of several of the proofs.  It may benefit from re-writing with more concise definitions of all of the variables and more clearly stated assumptions about observable information.  Here are some points of confusion for me:\n    - It seems like certain letters (eg. A or K or N_s vs N_d) are being overloaded as variable names with different fonts.  I realize that this is somewhat unavoidable, but I would recommend that the authors try to disentangle the naming a bit for improved clarity.\n    - In theorem 1, you stated \"the number of all possible data samples tends to infinity when the total number of samples N_d approaches infinity\".  This proposition seemed confusingly worded to me.  Maybe I am misunderstanding the wording, but it seems like possibly a tautology?\n    - I\u2019m not sure I understand what is meant by \u201cfake subjective policies\u201d in Theorem 1.  Could you explain what is meant by that and the intuition here a bit more?\n    - It\u2019s still unclear to me how K (that is, the external information) is being collected at any given timeframe.  Are you assuming that the necessary types of external information corresponding to K (your examples are \u2018state history, out-of-MDP task encodings, samples from related tasks, etc.\u2019) have been pre-determined?  If so, how could the most-appropriate type of external information be chosen in a practical way?\n\nI also noticed a few (very minor) grammar errors that authors may want to fix, though they did not affect my review:\n- page 1: \"tasks in open environments poses difficulties\" --> \"tasks in open environments pose difficulties\"\n- page 1: \"Problem is that both...\" --> \"The problem is that both...\"\n- page 2: \"we propose a novel framework named as Subjective reinforcement\" --> \"we propose a novel framework named Subjective reinforcement\"\n- page 4: \"should contain no data confusion\" --> \"should not contain data confusions\"", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 1}, "SJlHqPKaKS": {"type": "review", "replyto": "S1eq9yrYvH", "review": "The paper introduces the Subjective Reinforcement Learning framework to formalize the problem of using extra information to split large, nonstationary environments into separate, simple, stationary MDPs. First, the paper introduces and motivates the problem on a high level: the same state-action pairs may transit to different successive states and rewards because of nonstationary dynamics/reward function, or variance in the environment or tasks. This phenomenon is termed \"data confusion\". The paper then summarizes some related approaches to dealing with this phenomenon. Next, the paper introduces the subjective RL framework in detail in section 3:\n- Extra information (kappa) is needed to resolve the data confusion.\n- The \"subjectivity\" (h) is a function that maps the extra information to a vector of weights over \"subjective\" MDPs.\n- A policy is maintained for each subjective MDP, and the overall policy is the vector product of h and the vector of subjective policies.\nIn section 4, the paper presents 3 theorems arguing that using the subjective RL framework doesn't harm performance. The paper then gives brief guidelines for designing algorithms using the subjective RL framework before concluding.\n\nAt the present time I recommend rejecting the paper. It does not actually present a concrete solution method, instead simply giving brief guidelines for the reader to design algorithms by. The subjective RL framework unifies and subsumes several existing approaches, but I don't feel that in itself is a significant enough contribution to warrant publication. The theorems presented essentially argue that using the subjective RL framework does not harm performance, but there are no mentions of the computational costs involved with maintaining policies for each subjective MDP. In addition, it's not clear where the subjective MDPs come from.\nThe paper also had issues with clarity, including many grammatical errors.\n\nI think this paper tackles an important problem from an interesting point of view, but stops short of giving a concrete algorithm that can be implemented and tested. It seems like a good candidate for a workshop, which could be a good opportunity for discussion and feedback.\n\nGeneral comments:\n- The definition of rewards is confusing; script R was never defined.\n- \"minimize objective 1\" should be actually be \"maximize objective 1\"?\n- Rewards are usually defined over state-action pairs, not just states. Why choose this unconventional formulation for rewards?", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 1}, "rJg8QSNJ9S": {"type": "review", "replyto": "S1eq9yrYvH", "review": "The paper suggests that one common problem encountered by reinforcement learning algorithms in open environments is \"data confusion\", which essentially means showing the same input data with different --possibly contradictory-- labels/targets.\n\nThe proposed solution to this conceptual problem is to split the original MDP \"M\" up into multiple simpler MDPs \"Mk\", where M does contain possibly contradictory (\"confusing\") data, while each individual Mk does not contain any such problem and, even better, is stationary.\n\nThe \"subjectivity\" function \"h\" then has as role to split any data tuple across Mk, possibly using extra information kappa.\n\nFurthermore, several theorems show that under several conditions, the return of the subjective policy (learned via Mk) is not worse than that of the \"traditional\" policy.\n\n\nI lean towards rejecting this paper. The whole gist of the framework can be crudely summarized as \"if data contradicts, split up into non-contradictory sets using extra info.\" The motivation keeps repeating that no task-specific prior knowledge being necessary, but I believe this hinges on \"h\" being sensible, which might not be feasible without task-specific prior knowledge.\n\nFurthermore, and this is my main concern, there is not a single experiment demonstrating how any of this would behave in practice. It would be good to have one (possibly constructed) experiment showing that data confusion indeed is a problem in practice (intuitively, it is), and then a specific instantiation of the framework that solves this example. Furthermore, I am not convinced that the proposed bounds can easily be concretized for an instantiation of the proposed framework, especially when considering deep networks; again, this concern could be alleviated by an example instantiation. Proposing something that is in principle more general and \"in principle cannot be worse\" but then not demonstrating that it actually is the case is, in my opinion, not enough.\n\n\n\nFinally, and this is not a deciding factor in my rating, the paper has quite some writing problems. On the first page alone, I found a lot of spelling and grammatical mistakes (see list at end) and the notation is sometimes confusing to me. For example, \"R\" is defined as a mapping of S x /R -> [0,1], but what is \"/R\" (curly R)? And then in (1) R is used with a single argument while in (2) not anymore. I can guess what is meant, but it feels inconsistent. In Theorem 1, I believe it should be \"the gap \\delta >= 0\" and not \"the gap g >= 0\", no?\n\nAbstract and 1st paragraph mistakes (unfortunately, no line numbers in this template!): \"researches\" -> \"research\", \"algorithm designing\" -> \"algorithm design\", \"task-specific prior knowledge about tasks.\" -> \"task-specific prior knowledge.\", \"not known in prior\" -> \"not known a priori\", \"Classical RL model environment...\" -> \"Classical RL models environment...\".\nAlso, quite some citations are missing the year, e.g. Schaul et al., Papavassiliou&Russell, ...", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 3}}}