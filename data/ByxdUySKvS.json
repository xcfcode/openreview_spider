{"paper": {"title": "Adversarial AutoAugment", "authors": ["Xinyu Zhang", "Qiang Wang", "Jian Zhang", "Zhao Zhong"], "authorids": ["zhangxinyu10@huawei.com", "wangqiang168@huawei.com", "zhangjian157@huawei.com", "zorro.zhongzhao@huawei.com"], "summary": "We introduce the idea of adversarial learning into automatic data augmentation to improve the generalization  of a targe network.", "abstract": "Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment (Cubuk et al., 2019) can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data.", "keywords": ["Automatic Data Augmentation", "Adversarial Learning", "Reinforcement Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a method to learn data augmentation policies using an adversarial loss. In contrast to AutoAugment where an augmentation policy generator is trained by RL (computationally expensive), the authors propose to train a policy generator and the target classifier simultaneously. This is done in an adversarial fashion by computing augmentation policies which increase the loss of the classifier. The authors show that this approach leads to roughly an order of magnitude improvement in computational cost over AutoAugment, while improving the test performance.\nThe reviewers agree that the presentation is clear and that the proposed method is sound, and that there is a significant practical benefit of using such a technique. As most of the concerns were addressed in the discussion phase, I will recommend acceptance of this paper. We ask the authors to update the manuscript to address the remaining (minor) concerns.\n"}, "review": {"HJx91T8hjr": {"type": "rebuttal", "replyto": "SJgRylr2sr", "comment": "Thank you very much. The helpful discussion improves the paper substantially. We will consider your suggestion.", "title": "Response to Review #2"}, "rkg8m0BKtr": {"type": "review", "replyto": "ByxdUySKvS", "review": "This paper proposes a technique called Adversarial AutoAugment which dynamically learns good data augmentation policies during training. An adversarial approach is used: a target network tries to achieve good classification performance on a training set, while a policy network attempts to foil the target network by developing data augmentation policies that will produce images that are difficult to classify. To train the policy network, each mini-batch is augmented multiple times with different policies sampled from the policy network. Each augmented mini-batch is passed through the target network to produce a corresponding training loss, which is used as the training signal for the policy network. Experimental results are shown for CIFAR-10, CIFAR-100, and ImageNet datasets, where Adversarial AutoAugment outperforms competing methods (AutoAugment and Population Based Augmentation) on a variety of model architectures.\n\nIn its current state, I would tend towards rejecting this paper. The overall structure, the figures, and the experimental results are very nice, but there are two major issues that are holding it back. First, I am skeptical that the policy network is actually learning useful policies. Secondly, there are many grammatical errors in the paper which hamper readability. Upon reading the first paragraph of the paper, my initial impression was already quite negative, simply due to the number of grammatical errors. Fixing these would strengthen the paper considerably, and I would increase my score accordingly if properly addressed.\n\nPrimary Concerns:\n1) One of my main concerns with this paper is this line here: \"To guarantee the convergence during adversarial learning, the magnitude of all the operations are set in a moderate range\". Since the policy network has no incentive to select transformations that the target network can still learn from, I assume that the ranges are required so that the policy network cannot choose to apply extreme transformations which always fool the target network, such as setting brightness to 0 to make the entire image black. How are acceptable ranges determined? If cross validation is required, then this becomes very much like the original hand-tuning of data augmentations that we wanted to avoid in the first place. \n\nAdditionally, I think it would be useful to to see a plot of the magnitude of each transformation versus training epoch, similar to Figure 4a in [1]. If the policy network simply learns to use the most extreme augmentations available in order to fool the target network, then this may indicate that the gain in performance is from tuning the magnitude ranges, and not from the policy network selecting good policies.\n\n2) The paper could benefit greatly from some revision of the grammar. I would recommend either having a friend or colleague read it over, or even using an automated grammar checking program, such as Grammarly. For example, in the first paragraph alone there are several sentences that could be improved:\n\"Massive amount of data promotes the great success\" -> \"Massive amounts of data have promoted the great success\"\n\"when more supervised data available\" -> \"when more supervised data is available\"\n\"or better data augmentation method adapted\" -> \"or a better data augmentation method is adopted\"\n\"which can automated learn\" -> \"which can automatically learn\"\n\"there still requires tens of thousands of GPU-hours consumption.\" -> \"tens of thousands of GPU-hours of computation are still required\"\n\n\nThings to improve the paper that did not impact the score:\n3) In the first paragraph it is claimed that data augmentation policies have weak transferability across different tasks and datasets. I do not fully agree with this claim, since papers such as AutoAugment [2] have shown that learned policies are highly transferable to new datasets, and augmentation strategies such as CutMix have been shown to be highly effective for a variety of tasks. \n\n4) No citation for the original GAN paper, despite multiple mentions of adversarial learning, and GANs themselves.\n\n5) There is no computation time comparison with PBA, which is about 1000x faster than Autoaugment, and therefore roughly 100x faster than Adversarial AutoAugment.\n\n6) CIFAR-10 results are not necessarily state-of-the-art. The 1.36% error rate claimed by the paper is surpassed by work in [3], which achieved 1.33% using a combination of AutoAugment and mixup. \n\n7) Citation for the CIFAR-10 dataset incorrectly refers to the Adam optimizer paper [4].\n\n\nReferences: \n[1] Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen. Population based augmentation: Efficient learning of augmentation policy schedules. ICML, 2019.\n\n[2] Cubuk, Ekin D., Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. \"Autoaugment: Learning augmentation policies from data.\" CVPR (2019).\n\n[3] Wistuba, Martin, Ambrish Rawat, and Tejaswini Pedapati. \"A Survey on Neural Architecture Search.\" arXiv preprint arXiv:1905.01392 (2019).\n\n[4] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015\n\nEDIT: The authors have addressed the majority of my concerns, and as such I have increased my score from a 3 to a 6.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "HygeSPE3jr": {"type": "rebuttal", "replyto": "r1el_7gojr", "comment": "Thanks for your positive comments on our work.\n\n>>> Response to Q1:\nFrom our point of view, extreme learning rate or entropy penalty values may cause rapid convergence, which is unexpected. However, we do not think that equals to the collapsing results you worried about. We didn\u2019t make specific experiments on those extreme hyperparameters, but we would like to make a deduction based on our understanding of the work: \nFirst, all magnitudes are set in a moderate range as mentioned before, which protect the network from collapsing during training even if they are picked at boundary.\nThen, with the training processes, loss of target network decreases gradually, which weaken the reward signal while converging policy network. Hence, extreme values are avoided during training.\n\nIn Figure 6, the probability distribution of the parameters is visualized over 600 epochs (Appendix A1). We think this long-epoch training has made the policy network steadily converged, which can be observed from Figure 6(b). This indicates that the policy network may not collapse to extreme values given more epochs. \n\n>>> Response to Q2:  \nWe think the sensitivity analysis of the hyperparameters is benefit to consolidate our work, which would be considered in our future work due to the space and time limits.\nThe setting of the hyperparameters is almost the same as AutoAugment. The extensive results have shown that it also works well on different datasets in our method, including CIFAR-10/100 and ImageNet. Hence, we have reason to believe that it probably could be used for other similar datasets. However, without exhaustive experiments, it is very difficult for us to determine whether it is suitable for any new dataset. If it is unfortunate that the hyperparameters are needed to be re-tuned for new datasets, we think AutoAugment perhaps will face the same problem during the policy search as well. According to the efficiency comparison between AutoAugment and our method in Table 5, we believe that the tuning of the hyperparameters for new datasets would not negate the gain in efficiency over AutoAugment. \n\nHope our response answers your questions.\n", "title": "Response to Review #2 "}, "S1lbi609iS": {"type": "rebuttal", "replyto": "ryeaPUKKsH", "comment": "Thank you for your valuable comments and suggestions. Hope your concerns can be well addressed in the new revision.\n\n>>> Response to \u201cWhether the policy network is actually learning useful policies?\u201d:\nWe have visualized the probability distribution of the parameters in the learned augmentation policies on CIFAR-10 over training epochs in the new revision, and also added the analysis as follows:\n\n\u201cAs shown in Figure 6(a) and 6(b), we further visualize the probability distribution of the parameters of the augmentation policies learned with PyramidNet+ShakeDrop on CIFAR-10 over time. \u2026. This indicates that our method does not simply learn the transformations with the extremes of the allowed magnitudes to spoil the target network.\u201d \n\nThe revision above indicates that the policy network does learn useful policies along with the training process rather than simply learn the policies with the extremes of the allowed magnitudes to spoil the target network.\n", "title": "Response to Review #2"}, "B1l0K2oLjS": {"type": "rebuttal", "replyto": "rkg8m0BKtr", "comment": "We want to express our deep gratitude to for the constructive suggestions and positive comments on the novelty, motivation, and performance. We will explain your concerns point by point and hope you find our new revision satisfactory.\n\n>>> Response to \u201cWhether the policy network is actually learning useful policies?\u201d:\n1)\tIn this paper, the generalization of the target network is improved by finding the best data augmentation policy to perform label-preserved image transformation. Although adversarial augmentation policies are generated by the policy network, the extreme and unmeaning image transformation that will almost destroy all the image information, such as making the entire image black or white, should not be included in the search space. This can be regarded as the guideline to determine the acceptable range of the magnitude of all the operations. How to handle extreme image transformations will be considered in our future works.\n2)\tWe think that the ablation study has shown the effectiveness of adversarial augmentation policies. Through training the target network with adversarial augmentation policies, our method achieves the best test error, which indicates that the policy network can generate adversarial policies which are more adaptive to the training process, rather than apply extreme transformations which always fool the target network.\n\n>>> Response to \u201cgrammatical errors and typos\u201d:\nSincerely apologize for these mistakes. We have corrected these grammatical errors and typos, and revised the paper accordingly. Following your kind advice, we have also invited some native speakers to proofread the paper carefully. Wish you find our new revision satisfactory. Thanks again for pointing out our mistakes.\n\n>>> Response to \u201cclaim about weak transferability of human-designed augmentation policies\u201d: \nWe are very sorry for the misunderstanding. We just want to claim that human-designed augmentation policies sometimes show the weak transferability across different datasets, which is also presented in [1]. We have modified it in the new version.\n\n>>> Response to \u201cNo citation for the original GAN paper\u201d: \nThank you for the reference, which has been included in the new version.\n\n>>> Response to \u201cNo computation time comparison with PBA\u201d:\nBecause the experiment analysis on ImageNet dataset is not provided in PBA, we don\u2019t compare computation time with it in Table 5. Although PBA is 1000x faster than AutoAugment in term of the searching cost, the cost of target network training is still non-trivial. Besides, we think that the comparison of computation time on large-scale tasks is more meaningful. Hence, we compare the total computing cost and time overhead in training ResNet-50 on ImageNet dataset between AutoAugment and our method.\n\n>>> Response to \u201cCIFAR-10 results are not necessarily state-of-the-art\u201d: \nWe are very sorry for the omission. Due to the trivial difference, we think that the Top1 error of 1.36% on CIFAR-10 is comparable to the SOTA results in [2]. Meanwhile, we demonstrate that our overall performance is largely robust (insensitive) to the random factor due to the small variances on five runs in the revised version.\n\n>>> Response to \u201ccitation error\u201d:\nApologize for the typo. We have fixed it in the revised version. \n \n[1] Cubuk, Ekin D., Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. \"Autoaugment: Learning augmentation policies from data.\" CVPR (2019).\n\n[2] Wistuba, Martin, Ambrish Rawat, and Tejaswini Pedapati. \"A Survey on Neural Architecture Search.\" arXiv preprint arXiv:1905.01392 (2019).", "title": "Response to Review #2"}, "BkxK-ssIjr": {"type": "rebuttal", "replyto": "SJxZB33S5S", "comment": "We want to express our deep gratitude to for the constructive suggestions and positive comments on the novelty, motivation, and performance. \n\n>>> Response to \u201cevaluating transferability\u201d: \nThanks for your constructive advice. We have added the transferability analysis of the learned policies in the new version. Although the augmentation policies are dynamically changed in an adversarial manner, the learned policies can still transfer well to different datasets and architectures. Through directly applying the learned dynamic policies to various networks, the competitive performance is also achieved.\n\n>>> Response to \u201ca measure of uncertainty\u201d: \nFollowing your kind suggestion, we have stated the confidence interval with all the reported values in the new version. We also list the table below:\nTable 1: Top-1 test error(%) on CIFAR-10\n+-------------------------+----------+--------+-------------+------+---------------+\n|          Model          | Baseline | Cutout | AutoAugment |  PBA |   Our Method  |\n+-------------------------+----------+--------+-------------+------+---------------+\n|    Wide-ResNet-28-10    |   3.87   |  3.08  |     2.68    | 2.58 | 1.90$\\pm$0.15 |\n+-------------------------+----------+--------+-------------+------+---------------+\n|  Shake-Shake (26 2x32d) |   3.55   |  3.02  |     2.47    | 2.54 | 2.36$\\pm$0.10 |\n+-------------------------+----------+--------+-------------+------+---------------+\n|  Shake-Shake (26 2x96d) |   2.86   |  2.56  |     1.99    | 2.03 | 1.85$\\pm$0.12 |\n+-------------------------+----------+--------+-------------+------+---------------+\n| Shake-Shake (26 2x112d) |   2.82   |  2.57  |     1.89    | 2.03 | 1.78$\\pm$0.05 |\n+-------------------------+----------+--------+-------------+------+---------------+\n|   PyramidNet+ShakeDrop  |   2.67   |  2.31  |     1.48    | 1.46 | 1.36$\\pm$0.06 |\n+-------------------------+----------+--------+-------------+------+---------------+\n\nTable 2: Top-1 test error(%) on CIFAR-100\n+------------------------+----------+--------+-------------+-------+----------------+\n|          Model         | Baseline | Cutout | AutoAugment |  PBA  |   Our Method   |\n+------------------------+----------+--------+-------------+-------+----------------+\n|    Wide-ResNet-28-10   |   18.80  |  18.41 |    17.09    | 16.73 | 15.49$\\pm$0.18 |\n+------------------------+----------+--------+-------------+-------+----------------+\n| Shake-Shake (26 2x96d) |   17.05  |  16.00 |    14.28    | 15.31 | 14.10$\\pm$0.15 |\n+------------------------+----------+--------+-------------+-------+----------------+\n|  PyramidNet+ShakeDrop  |   13.99  |  12.19 |    10.67    | 10.94 |  10.42$\\pm$0.20 |\n+------------------------+----------+--------+-------------+-------+----------------+\n\nTable 3: Top-1 / Top-5 test error(%) on ImageNet\n+-------------+--------------+--------------+-----+--------------------------------+\n|    Model    |   Baseline   |  AutoAugment | PBA |           Our Method           |\n+-------------+--------------+--------------+-----+--------------------------------+\n|  ResNet-50  | 23.69 / 6.92 | 22.37 / 6.18 |  -  | 20.60$\\pm$0.15 / 5.53$\\pm$0.05 |\n+-------------+--------------+--------------+-----+--------------------------------+\n| ResNet-50-D | 22.84 / 6.48 |       -      |  -  | 20.00$\\pm$0.12 / 5.25$\\pm$0.03 |\n+-------------+--------------+--------------+-----+--------------------------------+\n|  ResNet-200 | 21.52 / 5.85 | 20.00 / 4.90 |  -  | 18.68$\\pm$0.18 / 4.70$\\pm$0.05 |\n+-------------+--------------+--------------+-----+--------------------------------+\nThe small variances on 5 models demonstrate that our overall performance is largely robust (insensitive) to the random factor.\n\n>>> Response to \u201cgrammatical errors and typos\u201d: \nApologize for these errors and typos. We have corrected these grammatical errors and typos, and revised the paper accordingly.\n\n>>> Response to \u201cthe breadth of the claim\u201d: \nThanks for pointing out it. We have modified the claim in the new version.\n\n>>> Response to \u201cFigure 4\u201d: \nWe have added the axis units on the figure. ", "title": "Response to  Review #4"}, "rJlknssIjH": {"type": "rebuttal", "replyto": "SkeDSmkQ5r", "comment": "We want to express our deep gratitude to for the constructive suggestions and positive comments on the novelty, motivation, and performance. \n\n>>> Response to \u201cincremental contributions\u201d: \nWe are very sorry for the misunderstanding. There are several reasons that strongly support for our substantial contributions.\na)\tTraditional GANs are used to enlarge datasets through directly synthesizing new images.  Although our method also plays in a GANs setting to jointly optimize target network and policy search, our goal is to find the best augmentation policy to perform label-preserved image transformations, rather than synthesize new images.\nb)\tThe exciting performance achieved by AutoAugment shows the potential benefits of automated data augmentation for training DNNs. However, the process of augmentation policy search in AutoAugment is very computationally expensive, and these augmentation policies learned on proxy tasks are not guaranteed to be optimal on target tasks. In this paper, we propose an adversarial framework to address these drawbacks. Through reusing the computing resource of target network training, the computing cost of policy search can be extremely degraded, as shown in Table 5. Directly learning policies on target tasks avoids the performance degradation caused by the transfer from proxy tasks to target tasks.\n\n>>> Response to \u201cBenefits of the GAN approach against pre-training policies\u201d: \nWe are very sorry for confusing you. We think that the reported results have shown the benefits of the GAN approach against pre-training policies. Taking ResNet-50 trained on ImageNet dataset as an example, AutoAugment can achieve a drop of 1.32% in Top-1 error compared to baseline with the 15000 GPU*hours of consumption. However, our method achieves a drop of 3.09% in Top-1 error compared to baseline only with the (1280-160=1120) GPU*hours of additional consumption, which is even 1.77% better than AutoAugment. The transferability further shows that our method is more computing-efficient.\n\n>>> Response to \u201cMore complete results\u201d: \na)\tThe confidence interval with all the reported values and the Top-5 results on ImageNet have been provided in the new revision. We also list the table below:\nTable 3: Top-1 / Top-5 test error(%) on ImageNet\n+-------------+--------------+--------------+-----+--------------------------------+\n|    Model    |   Baseline   |  AutoAugment | PBA |           Our Method           |\n+-------------+--------------+--------------+-----+--------------------------------+\n|  ResNet-50  | 23.69 / 6.92 | 22.37 / 6.18 |  -  | 20.60$\\pm$0.15 / 5.53$\\pm$0.05 |\n+-------------+--------------+--------------+-----+--------------------------------+\n| ResNet-50-D | 22.84 / 6.48 |       -      |  -  | 20.00$\\pm$0.12 / 5.25$\\pm$0.03 |\n+-------------+--------------+--------------+-----+--------------------------------+\n|  ResNet-200 | 21.52 / 5.85 | 20.00 / 4.90 |  -  | 18.68$\\pm$0.18 / 4.70$\\pm$0.05 |\n+-------------+--------------+--------------+-----+--------------------------------+\n\nb)\tWe have tried our best to evaluate our method on the same networks as Cubuk19 except AmoebaNet.\nThis is because our baseline training of AmoebaNet cannot achieve the reported accuracy in the original paper. \nThe problem is also reported in [1].\n\n>>> Response to \u201ccouple the learned policies\u201d: \nGenerally, we don\u2019t couple the learned policies again after target network training. This is because our method can directly perform the augmentation policy search along with target network training with a relatively low computing cost.However, to make our method more computing-efficient, we have shown the transferability of the learned policies between datasets and architectures in the new version.\n\n[1] Chen Lin, Minghao Guo, Chuming Li, Wei Wu, Dahua Lin, Wanli Ouyang, and Junjie Yan. Online\nhyper-parameter learning for auto-augmentation strategy. CoRR, abs/1905.07373, 2019.", "title": "Response to Review #1"}, "SkeDSmkQ5r": {"type": "review", "replyto": "ByxdUySKvS", "review": "The authors propose a method for adversarial data augmentation which jointly trains the target network and the augmentation policy network. The authors claim that such learning set-up prevents overfitting and reduces computational cost with respect to competitors. Finally they provide extensive results and show that outperform the state-of-the-art in multiple datasets. \n\n* The rationale behind the idea is properly introduced and justified \n* The formulation is clear and sound\n* The results show improvements over competitors in terms of accuracy and computational cost\n\n* The contributions seem very incremental. Applying GANs for data augmentation is not new and regarding the policy search the authors strongly base their approach in Cubuk19. The main differentiator wrt the work of Cubuk is to jointly train both target and policy learner in a GAN setting, rather than learning the policies a priory with a fixed target network.\n\n* I miss further discussion regarding the benefits of the GAN approach against pre-training policies. The overfitting argument seems pretty weak given the relatively marginal gains of accuracy. \n\n* It would make the experiments more complete if the Top-1 and Top-5 results were provided, as well as running experiments on the same networks as the Cubuk19 paper.\n\n* How coupled are the policies learned to the specific dataset after training?. The work by Cubuk19 shows transferability properties that could somehow diminish the gain achieved by this work regarding computational cost.\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "SJxZB33S5S": {"type": "review", "replyto": "ByxdUySKvS", "review": "This paper describes a method to learn data augmentation policies using an adversarial loss. It builds on the AutoAugment method. In AutoAugment, an augmentation policy generator is trained by reinforcement learning. At each iteration, a classifier network is trained from scratch based on the current augmentation policy, and its validation accuracy is used as the reward signal. This is extremely costly because it requires to train a complete network for every training step of the policy generator. Instead, the current paper proposes to train the policy generator and the classifier simultaneously. The policy generator is trained adversarially to find augmentation policies that increase the loss of the classifier. This leads to a significant speedup compared to classic AutoAugment.\n\nThe presentation of the algorithm and the results is very clear. The proposed method yields improved performance compared to AutoAugment at ~1/10 of the computational cost, which is impressive. Although the paper only evaluates on two datasets (CIFAR and ImageNet), the idea is likely applicable very generally. I recommend this paper for publication, but have some comments that should be addressed:\n\nMajor comments:\n- It would be good to evaluate how well the learned policies transfer between datasets and architectures. Adversarial AutoAugment still comes with a significant computational cost compared to hand-crafted augmentation, so transfer of policies would be useful. AutoAugment is transferable by design, so any competing algorithm should evaluate transferability.\n- The authors state that all results are mean of 5 initializations, which is great. Please use these replicates to compute a measure of uncertainty (SEM or confidence interval) and state this with all values in the tables.\n\nMinor comments:\n- Overall, there are many grammatical errors and typos that sometimes require interpretation and reduce clarity. Please proof-read carefully.\n- Abstract sentence \u201c... can simultaneously optimizes\u2026\u201d has grammatical issues.\n- Second sentence of introduction has grammatical issues.\n- Third sentence of intro: \u201cet al.\u201d is used for persons, use \u201cetc.\u201d for things.\n- Contribution section: \u201c...our proposed method outperforms all previous augmentation method.\u201d Please be careful with the breadth of your claims. You do not compare against *all* previous augmentation methods.\n- Figure 4: Please add units and/or refer to Table 5 in the legend.\n", "title": "Official Blind Review #4", "rating": "6: Weak Accept", "confidence": 2}}}