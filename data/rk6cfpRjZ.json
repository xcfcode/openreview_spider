{"paper": {"title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory", "authors": ["Wei Wen", "Yuxiong He", "Samyam Rajbhandari", "Minjia Zhang", "Wenhan Wang", "Fang Liu", "Bin Hu", "Yiran Chen", "Hai Li"], "authorids": ["wei.wen@duke.edu", "yuxhe@microsoft.com", "samyamr@microsoft.com", "minjiaz@microsoft.com", "wenhanw@microsoft.com", "fangliu@microsoft.com", "binhu@microsoft.com", "yiran.chen@duke.edu", "hai.li@duke.edu"], "summary": "", "abstract": "Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is available.", "keywords": ["Sparsity", "Model Compression", "Acceleration", "LSTMs", "Recurrent Neural Networks", "Structural Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The reviewers really liked this paper. This paper presents a tweak to the LSTM cell that introduces sparsity, thus reducing the number of parameters in the model.\n\nThe authors show that their sparse models match the performance of the non-sparse baselines. While the results are not state-of-the-art but vanilla implementations of standard models, this is still of interest to the community."}, "review": {"Hy8TTvTBG": {"type": "rebuttal", "replyto": "r1b07JTBG", "comment": "Thanks for accepting our paper!!! We've taken comments from the reviewers and advanced ptb models to the state-of-the-art during the rebuttal. We will manage to advance the results on SQuAD and more. Thanks for reviewing.", "title": "Updating to the state-of-the-art"}, "SJ15MyGeG": {"type": "review", "replyto": "rk6cfpRjZ", "review": "Quality: \nThe motivation and experimentation is sound.\n\nOriginality:\nThis work is a natural follow up on previous work that used group lasso for CNNs, namely learning sparse RNNs with group-lasso. Not very original, but nevertheless important.\n\nClarity:\nThe fact that the method is using a group-lasso regularization is hidden in the intro section and only fully mentioned in section 3.2 I would mention that clearly in the abstract.\n\nSignificance:\nLeaning small models is important and previous sparse RNN work (Narang, 2017) did not do it in a structured way, which may lead to slower inference step time. So this is an investigation of interest for the community.\n\nMinor comments:\n- One main claim in the paper is that group lasso is better than removing individual weights, yet not experimental evidence is provided for that.\n- The authors found that their method beats \"direct design\". This is somewhat unintuitive, yet no explanation is provided. ", "title": "Nice work on RNN compression ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B15mUMdef": {"type": "review", "replyto": "rk6cfpRjZ", "review": "The paper spends lots of (repeated)  texts on motivating and explaining ISS. But the algorithm is simple, using group lasso to find components that are can retained to preserve the performance.  Thus the novelty is limited.\n\nThe experiments results are good.\n\nSec 3.1 should be made more concise. ", "title": "the text is verbose but method is simple", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r14eWGtez": {"type": "review", "replyto": "rk6cfpRjZ", "review": "The authors propose a technique to compress LSTMs in RNNs by using a group Lasso regularizer which results in structured sparsity, by eliminating individual hidden layer inputs at a particular layer. The authors present experiments on unidirectional and bidirectional LSTM models which demonstrate the effectiveness of this method. The proposed techniques are evaluated on two models: a fairly large LSTM with ~66.0M parameters, as well as a more compact LSTM with ~2.7M parameters, which can be sped up significantly through compression.\nOverall this is a clearly written paper that is easy to follow, with experiments that are well motivated. To the best of my knowledge most previous papers in the area of RNN compression focus on pruning or compression of the node outputs/connections, but do not focus as much on reducing the computation/parameters within an RNN cell. I only have a few minor comments/suggestions which are listed below:\n\n1. It is interesting that the model structure where the number of parameters is reduced to the number of ISSs chosen from the proposed procedure does not attain the same performance as when training with a larger number of nodes, with the group lasso regularizer. It would be interesting to conduct experiments for a range of \\lambda values: i.e., to allow for different degrees of compression, and then examine whether the model trained from scratch with the \u201coptimal\u201d structure achieves performance closer to the ISS-based strategy, for example, for smaller amounts of compression, this might be the case?\n\n2. In the experiment, the authors use a weaker dropout when training with ISS. Could the authors also report performance for the baseline model if trained with the same dropout (but without the group LASSO regularizer)?\n\n3. The colors in the figures: especially the blue vs. green contrast is really hard to see. It might be nicer to use lighter colors, which are more distinct.\n\n4. The authors mention that the thresholding operation to zero-out weights based on the hyperparameter \\tau is applied \u201cafter each iteration\u201d. What is an iteration in this context? An epoch, a few mini-batch updates, per mini-batch? Could the authors please clarify.\n\n5. Clarification about the hyperparameter \\tau used for sparsification: Is \\tau determined purely based on the converged weight values in the model when trained without the group LASSO constraint? It would be interesting to plot a histogram of weight values in the baseline model, and perhaps also after the group LASSO regularized training.\n\n6. Is the same value of \\lambda used for all groups in the model? It would be interesting to consider the effect of using stronger sparsification in the earlier layers, for example.\n\n7. Section 4.2: Please explain what the exact match (EM) and F1 metrics used to measure performance of the BIDAF model are, in the text. \n\nMinor Typographical/Grammatical errors:\n- Sec 1: \u201c... in LSTMs meanwhile maintains the dimension consistency.\u201d \u2192 \u201c... in LSTMs while maintaining the dimension consistency.\u201d\n- Sec 1: \u201c... is public available\u201d \u2192 \u201cis publically available\u201d\n- Sec 2: Please rephrase: \u201cAfter learning those structures, compact LSTM units remain original structural schematic but have the sizes reduced.\u201d\n- Sec 4.1: \u201cThe exactly same training scheme of the baseline ...\u201d \u2192 \u201cThe same training scheme as the baseline ...\u201d", "title": "Review", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJsqUdpzG": {"type": "rebuttal", "replyto": "By6GmBPGz", "comment": "To follow up the concerns on the ptb dataset.\n\nWe have added state-of-the-art model for ptb to show the generalizability of our approach. We select Recurrent Highway Networks. Please refer to Table 1 in the paper (https://arxiv.org/pdf/1607.03474.pdf) for the state-of-the-art models on ptb. \"Variational RHN + WT\" is the one we used as the baseline. \nOur results are covered in Table 2 in our paper. In a nutshell, our approach can reduce the RHN width (the number of units per layer) from 830 to 517 without losing perplexity.", "title": "A better ptb baseline -- the RHN model -- is added"}, "Hyt62VvGG": {"type": "rebuttal", "replyto": "r14eWGtez", "comment": "Thanks for reviewing.\n\n1. ISS approach can learn an \u201coptimal\u201d structure whose accuracy is better than the same \u201coptimal\u201d model but trained without using group Lasso.\nFor example, in Table 1, the first model learned by ISS approach has \u201coptimal\u201d structure with hidden sizes of (373, 315), and its perplexity is better than the same model (in the last row) but trained without using group Lasso regularization.\n\n2. With the same dropout (keep ratio 0.6), the baseline model overfits, and, with early stop, the best validation perplexity is 97.73 which is worse than the original 82.57.\n\n3. Changed green to yellow.\n\n4. Per mini-batch\n\n5. Yes, \\tau is determined purely based on the trained model without group LASSO regularization. No training is needed to select it.\nThanks for sharing this thought. Histogram is added in Appendix C. Instead of plotting the histogram of all weights, we plot the histogram of vector lengths of each \u201cISS weight groups\u201d. We suppose it is more interesting because group Lasso essentially squeezes the length of each vector. The plot shows that the histogram is shifted to zeros by group Lasso regularization.\n\n6. Yes, to reduce the number of hyper-parameters, an identical \\lambda is used for all groups.\nWe tried to linearly scale the strength of regularization on each group by the vector length of the \u201cISS group weight\u201d as used by Alvarez et al. 2016, however, it didn\u2019t help to improve sparsity in our experiments.\n\n7. We now add the reference of the definition of EM and F1 (Rajpurkar et al. 2016) into the paper:\n\u201cExact match. This metric measures the percentage of predictions that match any one of the ground truth answers exactly.\u201d\n\u201c(Macro-averaged) F1 score. This metric measures the average overlap between the prediction and ground truth answer. We treat the prediction and ground truth as bags of tokens, and compute their F1. We take the maximum F1 over all of the ground truth answers for a given question, and then average over all of the questions.\u201d\n\n8. Corrected. Thanks for so many useful details.\n\nPaper are revised based on the comments.", "title": "Response to reviews"}, "SyjB0Vvfz": {"type": "rebuttal", "replyto": "B15mUMdef", "comment": "Thanks for reviewing.\n\nWe have made sec 3.1 as concise as possible. We have moved some to the Appendix A. \n\nThe key novelty/contribution of the paper is to identify the structure inside RNNs (including LSTMs and RHNs) that shall be considered as a group (\"ISS\") to most effectively explore sparsity.  Once the group is identified, using group lasso becomes intuitive.  That is why we describe ISS, the structure of the group, in details and illustrate the intuitions and analysis behind it.  We clarified this in the revision.", "title": "Response to reviews"}, "SkkbFvaGz": {"type": "rebuttal", "replyto": "rkmp_naTZ", "comment": "Hi Aaron,\n\nWe have added state-of-the-art model for ptb to show the generalizability of our approach. In limited time, we select Recurrent Highway Networks for fast evaluation since it is open source here https://github.com/julian121266/RecurrentHighwayNetworks. You may refer to Table 1 in the paper (https://arxiv.org/pdf/1607.03474.pdf) for the state-of-the-art models on ptb. \"Variational RHN + WT\" is the one we used as the baseline. \nOur results are covered in Table 2 in our paper. In a nutshell, our approach can reduce the RHN width from 830 to 517 without losing perplexity.\n\nThanks.", "title": "State-of-the-art experiments on ptb added"}, "By6GmBPGz": {"type": "rebuttal", "replyto": "SJ15MyGeG", "comment": "Thanks for reviewing.\n\nTo clarity: \nWe have mentioned group Lasso in the abstract. However, please note that, any structured sparsity optimization can be integrated into ISS, like group connection pruning based on the norm of the group (as used by Hao Li et. al. 2017 ).\n\nTo minor comments:\n- The speedup vs sparsity is added in Fig. 1, to quantitatively justify the gain of structured sparsity over non-structured sparsity.\n- In our context, \"direct design\" refers to using the same network architecture but with smaller hidden sizes. The comparison is in Table 1.\n- We are working on a better ptb baseline -- the RHN model (https://arxiv.org/abs/1607.03474), to solve the concerns on ptb dataset. The training takes time, but we will post our results as soon as the experiments are done. However, our results on SQuAD may reflect that the approach works in general.\n\nThanks!", "title": "Response to reviews"}}}