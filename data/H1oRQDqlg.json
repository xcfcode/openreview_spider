{"paper": {"title": "Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning", "authors": ["Dilin Wang", "Qiang Liu"], "authorids": ["dilin.wang.gr@dartmouth.edu", "qiang.liu@dartmouth.edu"], "summary": "", "abstract": "We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. As an application of our method, we propose an amortized MLE algorithm for training deep energy model, where a neural sampler is adaptively trained to approximate the likelihood function. Our method mimics an adversarial game between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results.", "keywords": ["Unsupervised Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "This paper presents an idea with a sensible core (augmenting amortized inference with per-instance optimization) but with an overcomplicated and ad-hoc execution. The reviewers provided clear guidance for how this paper could be improved, and thus I invite the authors to submit this paper to the workshop track."}, "review": {"rJpI9SVwg": {"type": "rebuttal", "replyto": "H1oRQDqlg", "comment": "We highly appreciate the time and feedback from all the reviewers, all of which we will take into serious consideration in our revision. We will particularly strengthen and clarify the empirical experiments. Below we address some of the major points: \n\n[Testing Accuracy Score]\nWe agree with the reviewers' point on the \"testing accuracy\" score, but think that it still provides some valuable insight about the dataset. Its blindness to the background can be a good thing in that it captures more information about the \"effective amount\" of objects the dataset contains.  The problem is that it is very difficult to obtain a *perfect* score, and reporting more than one metrics (in an objective fashion) can help to gain more comprehensive understandings. \n\n[Repulsive Term in High Dimension]\nOur repulsive force works due to two tricks: 1) scaling the bandwidth with the data diversity using the median trick, which alleviates the exponential decay of RBF kernel. 2) define kernel on the feature space instead of the raw pixels of the images, which allows us to respect the manifold structure of the images. The framework of SVGD allows us to use any positive definite kernels and change it adaptively during iterations, because the kernel only defines the \"tangent space\" for improvement. \n\nSteinGAN without kernel corresponds to Viterbi training of the energy model and we find it work well with careful tuning of parameters, but tend to converge to a small number of bad-looking images after running a large number of iterations; adding the kernel under the same setting helps prevent this problem. Our current results on CIFAR10 shows that SteinGAN without kernel gives an inception score of 6.34, while that SteinGAN with kernel gives 6.76. \n\n[Amortized is slower than non-amortized]\nAlthough the amortized algorithm has the overhead of updating $\\xi$, it stores the information in a generative network, and allows us to simulate as many images as we need. By using the one-step gradient update we proposed, the update of $\\xi$ is the same as standard backpropagation except replacing the Dlogp with the SVGD gradient. ", "title": "Thank you for your review and comments"}, "B1qKkApXx": {"type": "rebuttal", "replyto": "SJwukvSQe", "comment": "Thanks for the comments. I hope the following note could address your concern well. Thanks. \n\nhttp://goo.gl/gCSAl3", "title": "Reply to the pre-review from AnonReviewer1"}, "SJwukvSQe": {"type": "review", "replyto": "H1oRQDqlg", "review": "Can you elaborate on the derivation of equation 10? I'm not sure I follow how it is obtained by \"performing only one step of gradient descent of (8) (or (9))\".The authors propose amortized SVGD, an amortized form of prior work on SVGD, which is a particle variational method that maximally decreases the KL divergence at each update. \"amortized SVGD\" is done by training a neural network to learn this dynamic. They then apply this idea to train energy-based models, which admit a tractable unnormalized density.\n\nIn SVGD, the main difference from just MAP is the addition of a \"repulsive force\" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode. How this is able to still act as a strong enough entropy-like term in high dimensions is curious. From my understanding of their previous work, this was not a problem as the only experiments were on toy and UCI data sets.\n\nIn the experimental results here, they apply the kernel on the hidden representation of an autoencoder, which seems key, similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise. However, unlike Li et al. (2015) the autoencoder is part of the model itself and not fixed. This breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method.\n\nUnlike previous literature which uses inference networks, their amortized SVGD approach seems in fact slower than the non-amortized approach. This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches, this would be like having to perform local inferences before then updating inference network parameters, or at least partially performing the local inference). This seems quite costly during training.\n\nI recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs non-amortized approaches. The current results are promising but unclear why given the many knobs that the authors are playing with.\n\nReferences\n\nLi, Y., Swersky, K., & Zemel, R. (2015). Generative Moment Matching Networks. Presented at the International Conference on Machine Learning.", "title": "equation 10", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byj2SWzVx": {"type": "review", "replyto": "H1oRQDqlg", "review": "Can you elaborate on the derivation of equation 10? I'm not sure I follow how it is obtained by \"performing only one step of gradient descent of (8) (or (9))\".The authors propose amortized SVGD, an amortized form of prior work on SVGD, which is a particle variational method that maximally decreases the KL divergence at each update. \"amortized SVGD\" is done by training a neural network to learn this dynamic. They then apply this idea to train energy-based models, which admit a tractable unnormalized density.\n\nIn SVGD, the main difference from just MAP is the addition of a \"repulsive force\" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode. How this is able to still act as a strong enough entropy-like term in high dimensions is curious. From my understanding of their previous work, this was not a problem as the only experiments were on toy and UCI data sets.\n\nIn the experimental results here, they apply the kernel on the hidden representation of an autoencoder, which seems key, similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise. However, unlike Li et al. (2015) the autoencoder is part of the model itself and not fixed. This breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method.\n\nUnlike previous literature which uses inference networks, their amortized SVGD approach seems in fact slower than the non-amortized approach. This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches, this would be like having to perform local inferences before then updating inference network parameters, or at least partially performing the local inference). This seems quite costly during training.\n\nI recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs non-amortized approaches. The current results are promising but unclear why given the many knobs that the authors are playing with.\n\nReferences\n\nLi, Y., Swersky, K., & Zemel, R. (2015). Generative Moment Matching Networks. Presented at the International Conference on Machine Learning.", "title": "equation 10", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkrnKeCGg": {"type": "rebuttal", "replyto": "SyINHx0zg", "comment": "The experiment settings are as follows:\n\n1. Train ResNet classifiers on a) \"Real Training Set\" b) 100 copies of 500 examples taken at random from the \"Real Training Set\"  c) 50,000 DCGAN samples d) 50,000 SteinGAN samples\n2. Measure the classifiers' accuracy on the \"Real Testing Set\"\n\nThanks.", "title": "Thank you for reviewing our paper"}, "SyINHx0zg": {"type": "review", "replyto": "H1oRQDqlg", "review": "Can you clarify the method used to obtain the results presented in Figure 2's \"Testing Accuracy\" table? My understanding is as follows:\n\n1. Train a ResNet classifier on 50,000 SteinGAN samples.\n\n2. Measure the classifier's accuracy on a) the dataset used to train the SteinGAN model (\"Real Training Set\"), b) 100 copies of 500 examples taken at random from the \"Real Training Set\", c) DCGAN samples (how many?), and d) held-out SteinGAN samples (how many?).\n\nIs this correct?This paper proposes an amortized version of the Stein variational gradient descent (SVGD) method in which \"a neural network is trained to mimic the SVGD dynamics\". It applies the method to generative adversarial training to yield a training procedure where the discriminator is interpreted as an energy-based probabilistic model.\n\nOne criticism I have of the presentation is that a lot of time and energy is spent setting the table for a method which is claimed to be widely applicable, and the scope of the empirical evaluation is narrowed down to a single specific setting. In my view, either the paper falls short of its goal of showing how widely applicable the proposed method is, or it spends too much time setting the table for SteinGAN and not enough time evaluating it.\n\nThe consequence of this is that the empirical results are insufficient in justifying the approach proposed by the paper. As another reviewer pointed out, DCGAN is becoming outdated as a benchmark for comparison.\n\nQualitatively, SteinGAN samples don't look significantly better than DCGAN samples, except for the CelebA dataset. In that particular case, the DCGAN samples don't appear to be the ones presented in the original paper; where do they come from?\n\nQuantitatively, DCGAN beats SteinGAN by a small margin for the ImageNet Inception Score and SteinGAN beats DCGAN by an even smaller margin for the CIFAR10 Inception Score. Also, in my opinion, the \"testing accuracy\" score is not a convincing evaluation metric: while it is true that it measures the amount of information captured in the simulated image sets, it is only sensitive to information useful for the discrimination task, not for the more general modeling task. For instance, this score is likely completely blind to information present in the background of the image.\n\nBecause of the reasons outlined above, I don't think the paper is ready for publication at ICLR.", "title": "Testing accuracy question", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "By-ehDEEg": {"type": "review", "replyto": "H1oRQDqlg", "review": "Can you clarify the method used to obtain the results presented in Figure 2's \"Testing Accuracy\" table? My understanding is as follows:\n\n1. Train a ResNet classifier on 50,000 SteinGAN samples.\n\n2. Measure the classifier's accuracy on a) the dataset used to train the SteinGAN model (\"Real Training Set\"), b) 100 copies of 500 examples taken at random from the \"Real Training Set\", c) DCGAN samples (how many?), and d) held-out SteinGAN samples (how many?).\n\nIs this correct?This paper proposes an amortized version of the Stein variational gradient descent (SVGD) method in which \"a neural network is trained to mimic the SVGD dynamics\". It applies the method to generative adversarial training to yield a training procedure where the discriminator is interpreted as an energy-based probabilistic model.\n\nOne criticism I have of the presentation is that a lot of time and energy is spent setting the table for a method which is claimed to be widely applicable, and the scope of the empirical evaluation is narrowed down to a single specific setting. In my view, either the paper falls short of its goal of showing how widely applicable the proposed method is, or it spends too much time setting the table for SteinGAN and not enough time evaluating it.\n\nThe consequence of this is that the empirical results are insufficient in justifying the approach proposed by the paper. As another reviewer pointed out, DCGAN is becoming outdated as a benchmark for comparison.\n\nQualitatively, SteinGAN samples don't look significantly better than DCGAN samples, except for the CelebA dataset. In that particular case, the DCGAN samples don't appear to be the ones presented in the original paper; where do they come from?\n\nQuantitatively, DCGAN beats SteinGAN by a small margin for the ImageNet Inception Score and SteinGAN beats DCGAN by an even smaller margin for the CIFAR10 Inception Score. Also, in my opinion, the \"testing accuracy\" score is not a convincing evaluation metric: while it is true that it measures the amount of information captured in the simulated image sets, it is only sensitive to information useful for the discrimination task, not for the more general modeling task. For instance, this score is likely completely blind to information present in the background of the image.\n\nBecause of the reasons outlined above, I don't think the paper is ready for publication at ICLR.", "title": "Testing accuracy question", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}