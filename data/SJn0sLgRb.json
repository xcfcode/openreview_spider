{"paper": {"title": "Data Augmentation by Pairing Samples for Images Classification", "authors": ["Hiroshi Inoue"], "authorids": ["inouehrs@jp.ibm.com"], "summary": "", "abstract": "Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate N^2 new samples from N training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.\n", "keywords": ["Data augmentation", "Image classification"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a data augmentation technique for image classification which consists in averaging two input images and using the label of one of them. The method is shown to outperform the baseline on the image classification task, the but evaluation doesn\u2019t extend beyond that (to other tasks or alternative augmentation mechanisms); theoretical justification is also lacking."}, "review": {"ryOCyetxf": {"type": "review", "replyto": "SJn0sLgRb", "review": "The paper proposes a new data augmentation technique based on picking random image pairs and producing \na new average image which is associated with the label of one of the two original samples. The experiments show\nthat this strategy allows to reduce the risk of overfitting especially in the case of a limited amount of training \nsamples or in experimental settings with a small number of categories.\n\n+ The paper is easy to read: the method and the experiments are explained clearly.\n\n- the method is presented as a heuristic technique. \n1) The training process has some specific steps with the Sample Pairing intermittently disabled. \nThe number of epochs with enabled or disabled Sample Pairing changes depending on the dataset.\nHow much si the method robust/sensitive to variations on these choices?\n2) There is no specific analysis on the results besides showing the validation and training errors: would it\nbe possible to see the results per class? Would the confusion matrices reveal something more about the\neffect of the method?  Does Sample Pairing help to differentiate similar categories even if they are mixed\nat trainign time?\n3)  Would it be possible to better control the importance of each sample label rather\nthan always choosing one of the two as ground truth? \n\nThe paper misses an in-depth analysis of the proposed practical strategy.\n\n", "title": "interesting, but limited contribution", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1CYm85gM": {"type": "review", "replyto": "SJn0sLgRb", "review": "The paper investigates a method of data augmentation for image classification, where two images from the training set are averaged together\u00a0as input, but the label from only one image is used as a target.  Since this scheme is asymmetric and uses quite unrealistic input images, a training scheme is used where the technique is only enabled in the middle of training (not very beginning or end), and in an alternating on-off fashion.  This improves classification performance nicely on a variety of datasets.\n\nThis is a simple technique, and the paper is concise and to the point.  However, I would have liked to see a few additional comparisons.\n\nFirst, this augmentation technique seems to have two components:  One is the mixing of inputs, but another is the effective dropping of labels from one of the two images in the pair.  Which of these are more important, and can they be separated?  What if some of the images' labels are changed at random, for half the images in a minibatch, for example?  This would have the effect of random label changes, but without the input mixing.  Likewise, what if both labels in the pair are used as targets (with 0.5 assigned to each in the softmax target)?  This would mix the images, but keep targets intact.\n\nSecond, the bottom of p.3 says that multiple training procedures were evaluated, but I'd be interested to see the results of some of these.  In particular, is it important to alternate enabling and disabling SamplePairing, or does it also work to mix samples with and without it in each minibatch (e.g. 3/4 of the minibatch with pairing augmentation, and 1/4 without it)?\n\nI liked the experiment mixing images from within a restricted training set composed of a subset of the CIFAR images, compared to mixing these images with CIFAR training set images outside the restricted sample (p.5 and Fig 5).  This suggests to me, however, that it's possible the label manipulations may play an important role.  Or, is an explanation why this performs not as well that the network will train these mixing images to random targets (that of the training image in the pair), and never see this example again, whereas by using the training set alone, the mixing image is likely to be repeated with its correct label?  Some more discussion on this would be nice.\n\nOverall, I think this is an interesting technique that appears to achieve nice results.  It could be investigated deeper at some key points.\n", "title": "simple technique with nice results, could be analyzed a little deeper", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryjXhymZM": {"type": "review", "replyto": "SJn0sLgRb", "review": "The paper reports that averaging pairs of training images improves image classification generalization in many datasets. \nThis is quite interesting. The paper is also straightforward to read and clear, which is positive. Overall i think the finding is of sufficient interest for acceptance.\n\nThe paper would benefit from adding some speculation on reasons why this phenomenon occurs.\nThere are a couple of choices that would benefit from more explanation / analysis:  a) averaging, then forcing the classifier to pick one of the two classes present; why not pick both? b) the choice of hard-switching between sample pairing and regular training - it would be interesting if sample-pairing as an augmentation meshed better with other augmentations implementation-wise, so that it could be easier to integrate in other frameworks.", "title": "Interesting finding", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1DpKzaQM": {"type": "rebuttal", "replyto": "S1CYm85gM", "comment": "Thank you so much for your comments.\nPlease refer the updates 1) and 2) in the above response.\nI am currently implementing SamplePairing in a sub-minibatch granularity. So far, I do not see the significant differences by using smaller granularity of enabling/disabling SamplePairing, e.g. disabling for one mini batch after enabling for four mini batches instead of disabling two epochs after enabling eight epochs. But I am going to add the data with different granularity including the sub-minibatch granularity.", "title": "Author response"}, "Hy-FdzpXG": {"type": "rebuttal", "replyto": "ryOCyetxf", "comment": "Thank you so much for your comments.\nPlease refer the updates in above response on three points you mentioned in the comment.\n\nI like to specially thank the advice on confusion matrix. I have never investigated it.\nOn average, SamplePairing gave improvements in classification of similar classes (e.g. two animals or two vehicles) or different classes (e.g. animal and vehicle). But I am doing further investigation on the characteristics of SamplePairing using confusion matrices.", "title": "Author response"}, "Hy1zDzTmz": {"type": "rebuttal", "replyto": "ryjXhymZM", "comment": "Thank you so much for your comments.\nPlease refer the updates 1) and 2) in above response on two points you mentioned in the comment (using two labels and switching between SamplePairing and regular training).\nI am adding more experiments on the second point (switching), e.g. using different granularity.  I hope I can add more discussion on this point.\n", "title": "Author response"}, "SkqNBf6XG": {"type": "rebuttal", "replyto": "SJn0sLgRb", "comment": "First of all, we greatly thank the reviewers for their valuable comments. Also, we like to thank who made effort to reproduce our results.\nI updated the submission based on the comments from reviewers.\n\nThe major updates are:\n1) I added discussion on mixup, which is proposed in another submission (https://openreview.net/forum?id=r1Ddp1-Rb&;noteId=r1Ddp1-Rb), in related work.\nAlthough mixup does blending two samples as we do in this paper, mixup also blends labels from both samples while we pick only one. \nThere is a blog post by Ferenc Husz\u00e1r (http://www.inference.vc/mixup-data-dependent-data-augmentation/), which points out that using label from one sample will give the same results by reformulating the loss function of mixup.\nI also tested using both labels in our SamplePairing and it did not give significant difference as show in Figure 7 (in Appendix).\n\n2) In this paper, we intermittently disable SamplePairing in 20% of the epochs. I added Figure 6 on how this ratio affects the final results to answer the reviewers' questions. By intermittently disabling SamplePairing, we can get small improvements compared to the case without disabling SamplePairing. But this improvement is minor compared to the improvements by SamplePairing itself; hence the training with SamplePairing is not so sensitive to this (potentially workload dependent) tuning parameter. \n\n3) I added confusion matrices with and without SamplePairing to show how samples in each class are predicted in Figure 8 (in Appendix). \n", "title": "Author response"}, "Hkp3PtC-G": {"type": "rebuttal", "replyto": "ByZ_nVAbf", "comment": "All augmentations (crop, flip, pairing) are per epoch based on random numbers.", "title": "Re: Augmentation per epoch"}, "HJJ1R06-z": {"type": "rebuttal", "replyto": "Hk0XcA6WM", "comment": "In fine tuning part, I just stop applying SamplePairing. The basic data augmentations, drop out etc are still active during the fine tuning phase.", "title": "Re: The fine tuning part"}, "H1BRpQFbM": {"type": "rebuttal", "replyto": "BJZogjdZG", "comment": "In each epoch, we generate one (but not all) sample for each input sample. Since we use random number generator, the generated patches are different for epoch by epoch. The size of the extracted patch (i.e. input of the classifier) is 28x28 for CIFAR, not the original image size of 32x32, as you can see in above network design.", "title": "Re: Re: Re: Clarification"}, "r1zfLuSZf": {"type": "rebuttal", "replyto": "rybe_34-f", "comment": "Thank you so much for your effort for reproducing our results.\n1) I am sorry, but not yet published.\n2) Yes, the baseline uses the flipping and cropping. I will make the paper more clearer on this point.\n3) I use softmax_cross_entropy function provided by Chainer framework. (http://docs.chainer.org/en/stable/reference/generated/chainer.functions.softmax_cross_entropy.html)\n", "title": "Re: Reproducibility "}, "HyMUHnsez": {"type": "rebuttal", "replyto": "SJXiG2oeM", "comment": "Each image is cropped and random flipped differently for each epoch based on random numbers, not only once before training.", "title": "Re: Re: Clarification"}, "H1kIxTqxM": {"type": "rebuttal", "replyto": "r1dM9S5xf", "comment": "In the above network structure, all convolutions are 3x3 size with padding to keep the size. ", "title": "Re: Clarification"}, "r1dM9S5xf": {"type": "rebuttal", "replyto": "SyEr_7ceG", "comment": "Thank you so much for your effort!\n\n> Structure of the network\n(input 28x28x3)\nBatchNorm\nConv 64\nRELU\nBatchNorm\nConv 96\nRELU\nMaxPool 2x2\nBatchNorm\nConv 96\nRELU\nBatchNorm\nConv 128\nRELU\nMaxPool 2x2\nBatchNorm\nConv 128\nRELU\nBatchNorm\nConv 192\nRELU\nMaxPool 2x2\nBatchNorm\nDropOut 40% dropped\nFullConnect 512\nRELU\nDropOut 30% dropped\nFullConnect 10 (100 for CIFAR-100)\nSoftMax\n\n> What fraction of the training data was put aside for the validation set?\nFor CIFAR-10, I used 50,000 images included in data_batch_* for training (except for experiments shown in Figure 5). For validation set, I used 10,000 images in test_batch.\n\n> Was the training set fabricated by fully using the two basic augmentation techniques (e.g. N samples -> 2048N samples)?\nYes. When we test validation images, we extract 28x28 patch from center of the image without ensembling.\n\n> For training on the CIFAR-10 dataset, how many images were used during each SamplePairing epoch and each non-SamplePairing epoch?\nFor each epoch (with or without SamplePairing), all 50,000 training images were fed into the training for CIFAR datasets.\n", "title": "Re: Clarification"}, "BkCNMpTA-": {"type": "rebuttal", "replyto": "SJn0sLgRb", "comment": "I found there is another submission discussing a quite similar technique.\nmixup: Beyond Empirical Risk Minimization\nhttps://openreview.net/forum?id=r1Ddp1-Rb&noteId=r1Ddp1-Rb\n", "title": "related submission in ICLR 2018"}}}