{"paper": {"title": "Deep Variational Canonical Correlation Analysis", "authors": ["Weiran Wang", "Xinchen Yan", "Honglak Lee", "Karen Livescu"], "authorids": ["weiranwang@ttic.edu", "xcyan@umich.edu", "honglak@umich.edu", "klivescu@ttic.edu"], "summary": "A deep generative model for multi-view representation learning", "abstract": "We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA~\\citep{BachJordan05a} to nonlinear observation models parameterized by deep neural networks (DNNs). Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multi-view autoencoders~\\citep{Ngiam_11b}, with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the ``common variables'' underlying both views, extract the ``private variables'' within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings."}, "review": {"ByW6yzaUg": {"type": "rebuttal", "replyto": "H1Heentlx", "comment": "Dear reviewers,\n\nWe have modified our response to your review, acknowledging the related work pointed out by several reviewers, and clarifying our main contributions. \n\nThanks!", "title": "updated response"}, "ry8mZLfEl": {"type": "rebuttal", "replyto": "SkWEBAWEg", "comment": "-- The main contributions of our paper are:\n* We propose a deep multi-view learning model that extends the latent variable model interpretation of linear CCA.\n* We also propose VCCA-private as an extension, which is able to disentangle the shared and private information for multi-view data without strong supervision.\n* Our model provides tight connections to previous work, including the probabilistic interpretation of CCA and multimodal autoencoders.\n* The proposed model is as good as the SOA model in MNIST/XRMB experiment and outperforms SOA model in Flicker experiment. Besides the performance, the VCCA can be trained in an end-to-end style and is relatively easy to optimize.\n\nPreviously, the most successful multi-view deep learning methods are deterministic (CCA, contrastive loss) or undirected graphical models based on RBMs (Srivastava and Salakhutdinov JMLR 2014, Sohn et al., at NIPS 2014). The deterministic methods are not designed for generation. The RBM based models require relatively complicated pipelines for training (layer-wise pre-training, mean-field approximations and MCMCs) and tuning a large number of hyperparameters. \n\n-- Thank you for pointing out related work on inference methods; our contribution and more advanced variational inference/learning techniques (e.g. ,Rezende and Mohamed (2015); Tran et al. (2016)) are complementary. We will be happy to investigate this in our future work.\n\n-- You are correct about \"what matters is the KL regularizer\". In early stages of our experiments, we actually tuned a trade-off parameter for the KL regularizer; deviating from 1 (as the variational lower bound dictates) did not improved the performance. We will rephrase the superficial resemblance of \"sampling procedure\". ", "title": "response to AnonReviewer2"}, "r1iOnHMVx": {"type": "rebuttal", "replyto": "BkKlz-GNe", "comment": "-- Thanks for the references [2-5].  [3] and [4] are quite relevant in that they use the same graphical model and we will cite and discuss them in the next version soon.\n\n-- We improved the result on Flickr simply because we trained the same model longer after the deadline.\n\n-- There was not too much tuning involved to make VCCA work: standard deviation (a scalar) of Gaussian observation models, dropout rate (0.2 is selected for all experiments), and feature dimensionality. For optimization, we have always used Adam with 0.0001 learning rate for 200--300 epochs. We will make our code available and provide complete recipes for replicating our experiments. \n\n-- Dropout was indeed important to remove redundancy in learning useful features in deep variational auto-encoders (Sohn et al., at NIPS 2015). In addition, dropout is a very standard technique for auto-encoder style training to overcome overfitting to the training data. Please note that we set the dropout rate to 0.2 for all the experiments without tuning the rate too much. In summary, we believe the drop-out is a simple and efficient method in deep variational auto-encoder training that also applies to our case.\n\n-- We derive approximate posterior from one of the views mainly due to our multi-view representation learning (other possible derivations are given in last paragraph of page 3): we have large amount of unlabeled multi-view data from which we learn the feature transformation, and the feature transformation is applied to unseen single-view data in downstream tasks. The downstream tasks have little labeled single-view data and thus one could not train a powerful classifier well (e.g., this is the case for flickr, only 25000 images out of 1 million has topic labels). This is a common setting, see Ngiam et al ICML 2012, Srivastava and Salakhutdinov JMLR 2014, Sohn et al NIPS 2014, and\n\nZheng et al. A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data. TPAMI, 2015.\n\nFor the XRMB speech corpus, we had trained powerful discriminative DNN features in this setting, see Section 5 of\nWang et al.  Unsupervised Learning of Acoustic Features via Deep Canonical Correlation Analysis. ICASSP 2015. \nand the DNN features' performance is weaker than that of deep CCA and thus weaker than that of VCCA-private. \n\nWe conducted experiments in the unsupervised feature learning setting in order to make direct comparison with previous work, but it is straightforward to extend our method to semi-supervised setting by combining the objectives of representation learning and downstream task learning.\n\n-- Please note that the objectives of feature learning and downstream task learning shall be correlated but not identical, so the eventual tuning criteria is the downstream validation set performance. This justifies why we also tune the feature dimensionality. As a concrete example, too high a feature dimensionality will be problematic for the training of recognizers (Gaussian mixture model-HMMs)  on XRMB.\n\n-- Our method (as well as several other papers we discuss in related work) is inspired by the probabilistic interpretation of linear CCA, so we believe that the connection is interesting and nontrivial. We hope that our work will stimulate more research on developing a unified framework that explains many previous approaches.\n\n", "title": "responses to AnonReviewer1"}, "HyTUNB8El": {"type": "rebuttal", "replyto": "B1EW3dbEg", "comment": "Thanks for your comments.\n\nWe will consider moving some derivations into the appendix, and rephrase the sentence you mentioned. ", "title": "Response to AnonReviewer3"}, "By5nwHkEg": {"type": "rebuttal", "replyto": "H1Heentlx", "comment": "Dear reviewers,\n\nWe just uploaded a slightly modified version. This version \n\n-- contains the updated results on Flickr and the proposed methods significantly outperform others\n-- reorganized the figures and tables to make the paper more compact\n\nThanks!\n", "title": "revision notes"}, "SJFvDi27g": {"type": "rebuttal", "replyto": "rkvzKM17x", "comment": "Thanks for the comment.\n\nWe chose to parameterize q(z|x) as this feature mapping is what we use during test, so it is very natural to derive the lower bound with this posterior and train it \"end-to-end\" (to match the situation we have for testing).\n\nIt is possible that also parameterizing q(z|x,y) can help improve the performance. A similar approach was taken in the following paper for structured output prediction\nSohn et al. Learning Structured Output Representation using Deep Conditional Generative Models. NIPS 2015.\nwhere the authors derive two variational lower bounds and optimize the convex combination of them.\n\nWe will investigate this approach in future work. \n\n", "title": "choice of approximate posterior"}, "BkaYSi3me": {"type": "rebuttal", "replyto": "rJlfZ2kQl", "comment": "Thanks for the comments.\n\n-- It might be possible to derive the relationship between VCCA and DCCAE (or a similar model) rigorously, but we have not done it so far. One could think of deriving two variational bounds of the data likelihood by parameterizing q(z|x) and q(z|y), optimizing the combination of the two lower bounds while encouraging the two approximate posteriors to agree. This model is similar to DCCAE.\n\n-- The improved model VCCA-private does better than DCCAE in 2 out of 3 experiments.\n\nWe have so far improved the VCCA/VCCA-private performance on MIR-FLICKR significantly, matching previous state-of-the-art result we were aware of for the same setting. This result will be updated in the paper soon. ", "title": "relationship between models"}, "rJlfZ2kQl": {"type": "review", "replyto": "H1Heentlx", "review": "Hi,\nI have a few questions regarding this paper:\nQ1: In the same instructive way in which you compare VCCA to MVAE, would it be possible to compare VCCA to DCCAE? Are the two easy to relate? \n\nQ2: Related to Q1. How come DCCAE outperforms VCCA in most of your experiments? This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.\n\nIn [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.\n\n[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.\n\nThere is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].\n\n[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.\n[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.\n[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.\n[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.\n\nI can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.\n\nHowever, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.\n\nAnother issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.\n\nThe plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.\n", "title": "VCCA and DCCAE", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkKlz-GNe": {"type": "review", "replyto": "H1Heentlx", "review": "Hi,\nI have a few questions regarding this paper:\nQ1: In the same instructive way in which you compare VCCA to MVAE, would it be possible to compare VCCA to DCCAE? Are the two easy to relate? \n\nQ2: Related to Q1. How come DCCAE outperforms VCCA in most of your experiments? This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.\n\nIn [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.\n\n[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.\n\nThere is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].\n\n[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.\n[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.\n[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.\n[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.\n\nI can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.\n\nHowever, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.\n\nAnother issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.\n\nThe plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.\n", "title": "VCCA and DCCAE", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkvzKM17x": {"type": "review", "replyto": "H1Heentlx", "review": "While I can see the advantage of only conditioning on x (instead of x, y) in the encoder for certain applications, doesn\u2019t this reduce the quality of the lower bound? It would be interesting to know by how much. If q(z | x) is needed at test time, one could also train it separately, e.g. by dropping y from q(z | x, y) only after training and fixing p(x, y, z).7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.", "title": "Lower bound and using q(z | x) instead of q(z | x, y)", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1EW3dbEg": {"type": "review", "replyto": "H1Heentlx", "review": "While I can see the advantage of only conditioning on x (instead of x, y) in the encoder for certain applications, doesn\u2019t this reduce the quality of the lower bound? It would be interesting to know by how much. If q(z | x) is needed at test time, one could also train it separately, e.g. by dropping y from q(z | x, y) only after training and fixing p(x, y, z).7\n\nSummary:\nThis paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method\u2019s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.\n\nReview:\nVariational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.\n\nAs the authors point out, \u201cVCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA\u201d. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.\n\nThe derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?\n\nIn Section 3 the authors claim that \u201cif we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data\u201d. This is not correct, a model which hasn\u2019t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.\n\nMinor:\nIn the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.", "title": "Lower bound and using q(z | x) instead of q(z | x, y)", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}