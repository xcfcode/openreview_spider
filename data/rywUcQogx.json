{"paper": {"title": "Differentiable Canonical Correlation Analysis", "authors": ["Matthias Dorfer", "Jan Schl\u00fcter", "Gerhard Widmer"], "authorids": ["matthias.dorfer@jku.at", "jan.schlueter@ofai.at", "gerhard.widmer@jku.at"], "summary": "We propose Differentiable CCA a  formulation of CCA that enables gradient flow through the  computation of the CCA projection matrices.", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated \nlinear projections of two modalities. We propose Differentiable CCA, a \nformulation of CCA that can be cast as a layer within a multi-view \nneural network. Unlike Deep CCA, an earlier extension of CCA to \nnonlinear projections, our formulation enables gradient flow through the \ncomputation of the CCA projection matrices, and free choice of the final \noptimization target. We show the effectiveness of this approach in \ncross-modality retrieval experiments on two public image-to-text \ndatasets, surpassing both Deep CCA and a multi-view network with \nfreely-learned projections. We assume that Differentiable CCA could be a \nuseful building block for many multi-modality tasks.", "keywords": ["Multi-modal learning"]}, "meta": {"decision": "Reject", "comment": "The authors propose to use CCA as a transformation within a network that optimally correlates two views. The authors then back-propagate gradients through the CCA. Promising experimental results on for cross-modality retrieval experiments on two public image-to-text datasets are presented. \n \n The main concern with the paper is the clarity of the exposition. The novelty and motivation of the approach remains unclear, despite significant effort from the reviewers to understand. \n \n A major rewriting of the paper will generate a stronger submission to a future venue."}, "review": {"SyBeWVcSx": {"type": "rebuttal", "replyto": "rywUcQogx", "comment": "Dear reviewers, thank you for your effort and valuable feedback! We \nunderstand that we did not manage to clearly present the central idea of \nour work.\nWe do *not* combine the CCA objective (i.e., maximizing the correlation \nbetween the two hidden representations, the so-called Trace Norm \nObjective in Deep CCA) with another objective. We use CCA as a \ntransformation anywhere within a network that optimally correlates two \nviews, but we do not optimize the network towards maximal correlation as \ndone in Deep CCA. Instead, we apply the CCA transformations in the \nforward pass (not done in Deep CCA) and compute an arbitrary loss \nfunction on the transformed data.\nOur use of CCA can be compared to Batch Normalization (BN): BN provides \na transformation applicable anywhere within a network that normalizes a \nsingle view, and allows backpropagation of gradients through the \nnormalization procedure. Similarly, we backpropagate gradients through \nthe CCA procedure, where the gradients can be derived from any loss \nfunction operating on the optimally-correlated views produced by CCA.\nWe are grateful for all your comments, and will rewrite our manuscript \nto submit it elsewhere.", "title": "Response to reviewer comments"}, "SkfODTamx": {"type": "rebuttal", "replyto": "HJOcE7y7e", "comment": ">> 1. in equation 7, it should be tr((T'T)^0.5) instead of tr(T'T)^0.5 ?\n\nThank you, good catch. We took this from Equation (10) of Andrew et al. \n(2013), which has it wrong. We will fix this in the next revision.\n\n>> 2. in sec 3.1 gradient of CCA, it appears you are computing the\n>> derivative of tr(T'T) or tr(TT') with respect to the data. Maybe this\n>> gives you simpler gradient wrt eigenvectors for symmetric matrices, than\n>> gradient wrt singular vectors for non-square matrices. However, does it\n>> still require eigenvalue decompositions of T'T and TT'?\n\nIn contrast to Deep CCA, we do not compute the derivative of tr(T'T) \nwrt. the data (i.e., the trace norm objective), but the derivative of \nthe projected data wrt. the input data, which requires the derivative of \nthe eigenvectors wrt. the input data. We use the derivative of the eigen \ndecomposition due to its simpler form, and this requires the \neigenvectors and eigenvalues to be known. We do not need to compute the \neigenvalue decompositions of T'T and TT' for that, though, we can still \nuse the SVD of T in the forward pass. We will clarify this in our revision.\n\n>> 3. stochastic optimization for deep CCA have been done in\n>> Weiran Wang, Raman Arora, Nathan Srebro, and Karen Livescu. Stochastic\n>> Optimization for Deep CCA via Nonlinear Orthogonal Iterations. ALLERTON,\n>> 2015. This paper transform the deep CCA problem into a sequence of nonlinear\n>> least squares problem, for which gradients are trivial to derive.\n>> Have you compared with this paper? Also the idea of using running\n>> averages with small minibatches was explored in this paper.\n\nThe mentioned paper is a stochastic version of Deep CCA as introduced in \nAndrew et al. (2013). It tackles the stochastic optimization of the \ntrace norm objective, while our work replaces the trace norm objective \nwith a freely chosen loss function. Thus, this paper does not provide a \nuseful comparison point.\nHowever, we were not aware that Wang et al. also experimented with \nrunning averages and will include this reference in our revision. Thank \nyou for the hint! Note that we did not end up using running averages for \nour results. We found that stochastic optimization of CCA is feasible \nwith large mini-batches (like Wang et al. (2015a;b) cited in our work), \nand works better than running averages (see Appendix B).\n\n>> 4. What is the retrieval objective used in the paper? [...]\n\nThe retrieval objective in our case is cosine distance, or squared \ncosine distance, described in Section 4.2. We initially decided not to \ninclude contrastive or ranking losses, but show that our CCA layer \nallows us to improve over Deep CCA by replacing its trace norm objective \neven with a very simple alternative.\nHowever, our proposal is more than an extension of Deep CCA. Our CCA \nlayer can serve as a replacement for freely learned projections (i.e., \ndense layers) in any multi-view network. For example, we obtained good \nresults combining it with the pairwise ranking loss described in Kiros \net al., 2014. Their loss is also based on cosine distance and a natural \nfit to the proposed CCA layer. As this adds an interesting perspective \non our work, we will add these results in our revised manuscript.\n", "title": "Answer to AnonReviewer3's initial questions"}, "HJOcE7y7e": {"type": "review", "replyto": "rywUcQogx", "review": "I just took a very quick look at the paper (did not go beyond Sec 3).\n\n1. in equation 7, it should be tr((T'T)^0.5) instead of tr(T'T)^0.5 ?\n\n2. in sec 3.1 gradient of CCA, it appears you are computing the derivative of tr(T'T) or tr(TT') with respect to the data. Maybe this gives you simpler gradient wrt eigenvectors for symmetric matrices, than gradient wrt singular vectors for non-square matrices. However, does it still require eigenvalue decompositions of T'T and TT'?\n\n3. stochastic optimization for deep CCA have been done in \nWeiran Wang, Raman Arora, Nathan Srebro, and Karen Livescu. Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations. ALLERTON, 2015. \nThis paper transform the deep CCA problem into a sequence of nonlinear least squares problem, for which gradients are trivial to derive.\nHave you compared with this paper? Also the idea of using running averages with small minibatches was explored in this paper. \n\n4. What is the retrieval objective used in the paper? Another simple (easy to optimize) objective that works well for retrieval is the contrastive (large margin) loss, see for example\nMultilingual Distributed Representations without Word Alignment. Karl Moritz Hermann, Phil Blunsom.\nICLR 2014. \nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. In ICLR 2016.\nHave you compared with this objective?After a second look of the paper, I am still confused what the authors are trying to achieve.\n\nThe CCA objective is not differentiable in the sense that the sum of singular values (trace norm) of T is not differentiable. It appears to me (from the title, and section 3), the authors are trying to solve this problem. However,\n\n-- Did the authors simply reformulate the CCA objective or change the objective? The authors need to be explicit here.\n\n-- What is the relationship between the retrieval objective and the \"CCA layer\"? I could imagine different ways of combining them, such as combination or bi-level optimization. And I could not find discussion about this in section 3. For this, equations would be helpful.\n\n-- Even though the CCA objective is not differentiable in the above sense, it has not caused major problem for training (e.g., in principle we need batch training, but empirically using large minibatches works fine). The authors need to justify why the original gradient computation is problematic for what the authors are trying to achieve. From the authors' response to my question 2, it seems they still use SVD of T, so I am not sure if the proposed method has advantage in computational efficiency.\n\nIn terms of paper organization, it is better to describe the retrieval objective earlier than in the experiments. And I still encourage the authors to conduct the comparison with contrastive loss that I mentioned in my previous comments. ", "title": "some questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJ-aT5ZNg": {"type": "review", "replyto": "rywUcQogx", "review": "I just took a very quick look at the paper (did not go beyond Sec 3).\n\n1. in equation 7, it should be tr((T'T)^0.5) instead of tr(T'T)^0.5 ?\n\n2. in sec 3.1 gradient of CCA, it appears you are computing the derivative of tr(T'T) or tr(TT') with respect to the data. Maybe this gives you simpler gradient wrt eigenvectors for symmetric matrices, than gradient wrt singular vectors for non-square matrices. However, does it still require eigenvalue decompositions of T'T and TT'?\n\n3. stochastic optimization for deep CCA have been done in \nWeiran Wang, Raman Arora, Nathan Srebro, and Karen Livescu. Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations. ALLERTON, 2015. \nThis paper transform the deep CCA problem into a sequence of nonlinear least squares problem, for which gradients are trivial to derive.\nHave you compared with this paper? Also the idea of using running averages with small minibatches was explored in this paper. \n\n4. What is the retrieval objective used in the paper? Another simple (easy to optimize) objective that works well for retrieval is the contrastive (large margin) loss, see for example\nMultilingual Distributed Representations without Word Alignment. Karl Moritz Hermann, Phil Blunsom.\nICLR 2014. \nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. In ICLR 2016.\nHave you compared with this objective?After a second look of the paper, I am still confused what the authors are trying to achieve.\n\nThe CCA objective is not differentiable in the sense that the sum of singular values (trace norm) of T is not differentiable. It appears to me (from the title, and section 3), the authors are trying to solve this problem. However,\n\n-- Did the authors simply reformulate the CCA objective or change the objective? The authors need to be explicit here.\n\n-- What is the relationship between the retrieval objective and the \"CCA layer\"? I could imagine different ways of combining them, such as combination or bi-level optimization. And I could not find discussion about this in section 3. For this, equations would be helpful.\n\n-- Even though the CCA objective is not differentiable in the above sense, it has not caused major problem for training (e.g., in principle we need batch training, but empirically using large minibatches works fine). The authors need to justify why the original gradient computation is problematic for what the authors are trying to achieve. From the authors' response to my question 2, it seems they still use SVD of T, so I am not sure if the proposed method has advantage in computational efficiency.\n\nIn terms of paper organization, it is better to describe the retrieval objective earlier than in the experiments. And I still encourage the authors to conduct the comparison with contrastive loss that I mentioned in my previous comments. ", "title": "some questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}