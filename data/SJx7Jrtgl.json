{"paper": {"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders", "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C.H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "authorids": ["n.dilokthanakul14@imperial.ac.uk", "pmediano@imperial.ac.uk", "m.garnelo-abellanas13@imperial.ac.uk", "matthew.lee13@imperial.ac.uk", "h.salimbeni15@imperial.ac.uk", "kailash.arulkumaran13@imperial.ac.uk", "m.shanahan@imperial.ac.uk"], "summary": "We study a variant of the variational autoencoder model with a Gaussian mixture as prior distribution and discuss its optimization difficulties and capabilities for unsupervised clustering.", "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "keywords": ["Unsupervised Learning", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference."}, "review": {"HyCppHL8l": {"type": "rebuttal", "replyto": "rySYcM6mg", "comment": "Sorry for the late reply. This interesting blog post http://ruishu.io/2016/12/25/gmvae/ has carried out an interesting analysis of model M2 and shown that it is not well suited for clustering tasks.", "title": "reply to comment"}, "Sk5LtSL8e": {"type": "rebuttal", "replyto": "BJC3xsWEe", "comment": "We would like to thank you for your review and the very useful suggestion with regards to the inference model. We addressed a justification of the additional complexity by comparing with GMM in section 4.1. We do find that our model performs worse than adversarial autoencoders and we have included our hypothesis as to why this is the case in section 4.2. With regards to the concern about the results, we confirm that the results we report are testing results. Please also refer to our summary of the paper revision for a more comprehensive overview of the changes we have made in response to all of the reviewers.", "title": "Reply to AnonReviewer2"}, "rJoiuSUUx": {"type": "rebuttal", "replyto": "SkxLJRyMVl", "comment": "Thank you for suggesting an interesting related paper. We have included it in section 3, where we acknowledge previous work on backpropagation through stochastic variables. We emphasise that our choice was made to make the inference model as simple as possible. While we agree that scaling up sideways might not be the best idea as the model scales up linearly with the number of clusters, we believe that stacking GMVAEs on top of each other could in theory increases the effective number of clusters much better. \n\nWe do agree that our old argument about mean-field is not very well explained and not necessarily correct as you have mentioned. As a result, we have approached this problem from a new angle and explained our rationale in section 3.3, 3.4 and in the experiments on the synthetic dataset. Please also refer to our summary of the paper revision for a more comprehensive overview of the changes we have made in response to all of the reviewers.", "title": "Reply to AnonReviewer3"}, "ry_4uHI8g": {"type": "rebuttal", "replyto": "BkE1E-fNx", "comment": "We would like to thank you for the constructive and encouraging review. We hope that our new model without ad-hoc parameters is more attractive from a practical point of view. Please also refer to our summary of the paper revision for a more comprehensive overview of the changes we have made in response to all of the reviewers.", "title": "Reply to AnonReviewer1"}, "H14edSLIe": {"type": "rebuttal", "replyto": "SJx7Jrtgl", "comment": "We would like to thank the reviewers for the constructive reviews which have, indeed, been very useful. We apologize for the late reply as we had to redo the majority of the experiments and rewrite a big part of the paper accordingly. The following is an outline of the major modifications to the paper:\n\n1.) We have changed our inference model and use the approximate posterior suggested by AnonReviewer2. This means the model we now call GMVAE corresponds to what we called GMVAE+ in the old version of the paper. (See section 3.2).\n\n2.) The entire analysis is now specific to this model. We have removed the argument about the mean-field problem which, as argued by AnonReviewer3, might not be strictly correct. Instead, we approach the problem from a different angle and address the problem of over-regularisation (over-pruning) in VAEs. We have high confidence that this argument is both better suited and more grounded. (See section 3.3 and section 3.4).\n\n3.) Following the analysis in section 3.4, we solve the problem with a heuristic called \"minimum information constraint (free-bits)\" that was introduced by Kingma et al. (2016). As a result we do not resort to ad-hoc modifications like the eta-term in the previous version, but introduce a different tuning parameter, lambda, instead. We study this parameter, lambda, on the synthetic dataset and visualize its behaviour. We found that the GMVAE has a high tolerance region for the possible values of lambda and moreover does not require this heuristic for the MNIST experiments at all. Therefore, from a practical point of view we have improved our model and alleviated the concerns raised by AnonReviewer1. (see section 4.1 and 4.2)\n\n4.) The MNIST and SVHN experiments work successfully without any regularisation using the new inference model suggested by AnonReviewer2. As a result, we decided to take out the section on consistency violation that introduced an extra tuning parameter with unintuitive behaviour.\n\n5.) Crucially, we study the problem of over-regularisation in GMVAEs which manifests itself differently from regular VAEs. We explicitly show the behaviour of the \"minimum information constraint\" heuristic that we use to address the problem.\n\nIn conclusion, we have redone all experiments using the new inference model which led to a considerable increase in performance and allowed us to remove all ad-hoc tuning parameters. We show that, although we can train GMVAE using the normal ELBO on the MNIST dataset, the over-regularisation problem persists when trained on our synthetic dataset. Finally, we show that a heuristic introduced to solve the over-regularisation problem in standard VAEs can be used to improve our model as well.\n\nWe hope the reviewers see the intuitions gained from the analysis and the experiments as useful contributions. Although simple, we believe that GMVAEs or deep GMVAEs (stacked GMVAEs) are a very interesting class of model which has the potential to become very powerful as we gain more understanding about the underlying inference process.", "title": "Changes to the paper"}, "rJ4d5O77x": {"type": "rebuttal", "replyto": "HkHPwiJQg", "comment": "Thank you for your comment. We agree that our method could be extended with a hierarchical prior on z; however, as you mentioned, this does introduce more difficulties. In our work we use eta to illustrate the effect of the z-prior term on clustering \u2013 as shown in Figure 2, turning off the effect of z-prior term improves the model\u2019s ability to form separate clusters. With CV regularization, the z-prior term no longer needs to be turned off, thereby highlighting the benefit of information-theoretic regularization. Although we are considering hierarchical priors or alternative priors for future work, we believe that we have shown that CV regularization is beneficial, and empirically it removes the need for the ad-hoc eta.\n\nAs a side note, CV regularisation could be interpreted as imposing an assumption that the Gaussians are not supposed to be overlapping. It would be interesting to make more connection to a sparse prior assumption on z. ", "title": "The ad-hoc parameter"}, "HkHPwiJQg": {"type": "review", "replyto": "SJx7Jrtgl", "review": "Instead of adding the ad-hoc \\eta, wouldn't it be more principled to get a similar effect by putting the assumptions/constraints in the prior? For example, a hierarchical prior on z. I understand that that could introduce additional difficulties, but I was curious to get a comment from the authors.\n\nThis submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). \n\nA general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. \n\nAlso, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the samples in (6).\n\nA negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely \\eta (eq. (3)) and \\alpha (eq. 7). According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them. It also seems that these two parameters interact, since they both regularize z in different ways. This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial. The authors mention that they tune the trade-off between these two regularizers, but I'd be interested in a comment concerning how this is done (what's the space of parameters to search on). In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time.\n\nI really liked the experiments section. It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses. This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets.\n\n\nOverall, I am a little concerned about the practicality of this approach, given the tuning it requires. However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration. Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future. Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.\n", "title": "The ad-hoc parameter", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkE1E-fNx": {"type": "review", "replyto": "SJx7Jrtgl", "review": "Instead of adding the ad-hoc \\eta, wouldn't it be more principled to get a similar effect by putting the assumptions/constraints in the prior? For example, a hierarchical prior on z. I understand that that could introduce additional difficulties, but I was curious to get a comment from the authors.\n\nThis submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). \n\nA general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. \n\nAlso, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the samples in (6).\n\nA negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely \\eta (eq. (3)) and \\alpha (eq. 7). According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them. It also seems that these two parameters interact, since they both regularize z in different ways. This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial. The authors mention that they tune the trade-off between these two regularizers, but I'd be interested in a comment concerning how this is done (what's the space of parameters to search on). In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time.\n\nI really liked the experiments section. It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses. This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets.\n\n\nOverall, I am a little concerned about the practicality of this approach, given the tuning it requires. However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration. Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future. Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.\n", "title": "The ad-hoc parameter", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1PpiRrfe": {"type": "rebuttal", "replyto": "HyoWIt7zg", "comment": "Hi,\n\nThank you for your insightful suggestion. We absolutely agree that using p(z|x,w) instead of q(z|y) is going to give us a better approximate posterior. In fact, we just finished coded this up and ran some experiments. One benefit of using q(z|y) would be, q(z|y) would give us a convenient/cheap way to infer the classification labels. But, not using an approximation would surely out weight this.\n\nFor the synthetic dataset, the modified posterior doesn't change our main results where adjusting eta and alpha is crucial for successful clustering. For MNIST with 1 mc-sample, it improves the best classification result for K = 16 by 4 per cent. No conclusive improvement for the case of K = 10 without extensive experiment. More experiments will be run and we will discuss this in the next version of the paper.", "title": "Advantage of q_{\\phi_z}(z|y) ?"}, "Bk0laLobl": {"type": "rebuttal", "replyto": "H1m3Op_-x", "comment": "Hi Eric, \n\nThank you for your interest! Unfortunately, we haven\u2019t had time to tried Grave's algorithm yet. Although it is not obvious how to calculate the CV term within his framework, it is a possible extension to our work. \n\nWe would like to take this opportunity to emphasise that a different more expressive posterior would not necessarily solve the problem of the ``anti-clustering term'' discussed in section 3.2.2. In our case, the source of problem is not that our posterior is not a GMM but rather the way that the variational posterior is factorised among observations -- i.e. q(Z|X) = \\prod_i q(z_i|x_i) where i indexes observations, z_i is a 'local' latent variable and x_i is an observation. Ideally, we would want Z to be 'global' variable that generates the whole X dataset -- but that would require passing the whole dataset through the recognition network to output just one encoding!\n\nMore obvious solutions would be:\n1.) Structured inference [1]\n2.) Passing the whole dataset through recognition network [2]\n\n[1] M.J. Johnson et al. Composing graphical model with neural networks for structured representations and fast inference (https://arxiv.org/abs/1603.06277)\n[2] H. Edwards, A. Storkey. Towards a Neural Statistician (https://arxiv.org/abs/1606.02185)", "title": "Backprop Through Mixture Samples"}, "H1m3Op_-x": {"type": "rebuttal", "replyto": "SJx7Jrtgl", "comment": "Hi, interesting work.  Just curious: did you try implementing Alex Graves' method for backprop through mixture samples (https://arxiv.org/abs/1607.05690)?  \nBest,\nEric", "title": "Backprop Through Mixture Samples"}, "Hk-P6gGbe": {"type": "rebuttal", "replyto": "SyUPRhgWx", "comment": "Thank you for your interest! In our work the random seed effects weight initialisations. We use Glorot weight initialisation [1] - a standard heuristic - and find this can sometimes be sensitive to seed values; when this is the case the issue can be easily alleviated by using batch normalisation [2] in the recognition model. When deviating from Glorot initialisation we experimentally found that our method is more sensitive to seed values, however, this can be compensated for by batch normalisation. CV regularisation does not address the specific problem that you mention, but it is addressed by the maximisation of the ELBO, as having one cluster containing 2 arcs will have a lower ELBO.\n\n[1] Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics.\n[2] Ioffe, S. and Szegedy, C., 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.", "title": "ELBO cost prefers seperating the arcs, as it gives tighter bound."}, "SyUPRhgWx": {"type": "rebuttal", "replyto": "SJx7Jrtgl", "comment": "Thank you for the interesting paper! I like the idea of adding regularization for improving the interpretability of the model. But I am a bit concerned about the stability of the algorithm. For example, in your synthetic data example (section 4.1), how sensitive is the algorithm w.r.t. the initialization or random seeds etc.? Seems to me that it is likely to learn a cluster containing, say two 2 arcs instead of one. And somehow I don't see why the CV regularization can help avoid that. Thanks a lot! (In fact I am also not sure why this is not a problem for Johnson et. al 2016. I don't see why the model would prefer separating the arcs instead of merging them as a group, maybe mostly due to the regularization caused by finite layers of neural nets?)", "title": "stability of the algorithm?"}}}