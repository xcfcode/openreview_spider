{"paper": {"title": "How much Position Information Do Convolutional Neural Networks Encode?", "authors": ["Md Amirul Islam*", "Sen Jia*", "Neil D. B. Bruce"], "authorids": ["amirul@scs.ryerson.ca", "sen.jia@ryerson.ca", "bruce@ryerson.ca"], "summary": "Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.", "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "keywords": ["network understanding", "absolute position information"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "This paper analyzes the weights associated with filters in CNNs and finds that they encode positional information (i.e. near the edges of the image).  A detailed discussion and analysis is performed, which shows where this positional information comes from.  \n\nThe reviewers were happy with your paper and found it to be quite interesting.  The reviewers felt your paper addressed an important (and surprising!) issue not previously recognized in CNNs."}, "review": {"0MCY0JklOeJ": {"type": "rebuttal", "replyto": "Mb-oPQ1aMx", "comment": "Hi Thomas Brox, thanks for your interesting question. I would like to discuss about this question, but can I ask if you are asking the input of the PEN module is a constant value? The input of the PEN is directly extracted from a pre-trained CNN model, so are you asking if given a natural image X, the multi-level feature from a VGG could be a constant? I can better answer this question if understand it correctly, thanks Sen.", "title": "Answers to Thomas Brox"}, "PaaqsTbMD": {"type": "rebuttal", "replyto": "KLN8PkPA6T", "comment": "Addition to my answer, our work was trying to analyze and understand the backbone by using PEM, instead of designing a PEM to output position information. \n\nCheers\nSen", "title": "Answers to Alberto Bernacchia"}, "KLN8PkPA6T": {"type": "rebuttal", "replyto": "Vt9BTvCtgv", "comment": "Dear Alberto Bernacchia:\n\nThanks for your new questions.\n1) \"analyze the information inside a CNN, do you agree that the choice of the architecture of the PEG is arbitrary?\".\nA: Yes, we analyzed the information inside a CNN about the position information. The choice of PEM could be arbitrary but note that this PEM is only a read-out module. You can replace it with one or more conv layers and your result should be valid.\n\n2)  \"then I choose a PEG that ignores the input, and always gives the pre-defined output. The fact that my input is the activity taken from a CNN does not matter, it can be anything.\"\nA: I assume you are trying to put a PEM on top of the input data directly. Note that we mentioned the source of the position information is from zero-padding, you can have the position information if you put the \"source\" there.\n\nIn a way you can consider the backbone CNN(resnet or vgg) does not matter. On the contrary, however, we were trying to explore if a widely used backbone (resnet or vgg) contains this position information and where does it come from, we \"interpret\" our work as CNN understanding. Because this long-ignored problem is important in computer vision tasks, given that the absolute position can be converted to relative position in theory.\n\nThanks\nSen\n ", "title": "Answers to Alberto Bernacchia"}, "2EjvkJCzrZ": {"type": "rebuttal", "replyto": "FogcyJZsY", "comment": "In addition, the scores on the synthetic images, white, black and noise in Table 1 could be a proof to question the hypothesis \"w = z * m / (s^2 + m^2)\".", "title": "Answers to Alberto Bernacchia"}, "FogcyJZsY": {"type": "rebuttal", "replyto": "FXNRLA98Z", "comment": "Thanks for your further question. We thought about when the model is input-agnostic, w=0. In that case, the model is not able output the pre-define pattern. We did not experience this problem of zero vector in our experiment as well. You might want to check the answer to Reviewer 1, in which we showed the effect of circular padding. If somehow the model was adjusted to \"z * m / (s^2 + m^2)\", the circular padding should improve the performance. Please correct me if I am wrong and help me recall other cases that the model can produce this output without knowing the position information.\n\nRegards\nSen", "title": "Answers to Alberto Bernacchia"}, "KFqvROX01_": {"type": "rebuttal", "replyto": "TI-r_hwtYv", "comment": "Dear Alberto,\n\nThanks for this interesting question and we will further explain our work in this post.\n\n1. I think your understanding about the network architecture is correct, PEM is designed to output a fixed image(the gradient groundtruth) based on the output of a pre-trained model, e.g., VGG or ResNet.\n\n2. Can this model output a fixed image(say a dog) regardless the input? The short answer is it is possible to generate a fixed output, in this case, the learning process should be independent of position information(zero-padding). However, our experiment (Table 5) shows the output is indeed based on the position information delivered by zero-padding. An addition to your question, I personlly believe a model(given enough complexity) could blindly memorize each of the training image to output the desired prediction(say a dog), but I don't think this can give you the output you want on the test set.(https://arxiv.org/abs/1611.03530, ICLR2017).\n\nRegards\nAuthors of paper 781", "title": "Answers to Alberto Bernacchia"}, "SklQXVNkcB": {"type": "review", "replyto": "rJeB36NKvB", "review": "This paper studied the problem of the encoded position information in convolution neural networks. The hypothesis is that CNN can implicitly learn to encode the position information. The author tests the hypothesis with lots of experiments to show how and where the position information is encoded.\n\nClarity:\nThis paper is interesting for me. It tries to understand the encoded position information that is easily ignored by researchers. I like adequate experiments with learned position information and position illustrations.\n\nExperiments:\n1. The paper mainly discussed the zero-padding and found it is the source of position information. How about other padding modes like constant-padding, reflection-padding, and replication-padding?\n\n2. The partial convolution-based padding method [1] (padded regions are masked out) shows that its recognition accuracy is higher than the traditional zero-padding approach. Can you help investigate where the position information comes from for this case?\n\n[1] Partial Convolution based Padding, https://arxiv.org/pdf/1811.11718.pdf.\n\n\nSome of my concerns are well addressed by the author thus I upgrade my score.\n", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 3}, "ryxckH_joB": {"type": "rebuttal", "replyto": "r1xDPX_yqH", "comment": "We thank Reviewer 3 for the detailed feedback and we will further explain the question raised in the comment. \n\nWe think the first question is about initialization, (cold or hot start). We also thought about a longer training procedure for the PosENet because it was trained from scratch. But we found that the training loss does not decrease after the first several iterations, the weight becomes saturated quickly. The training loss of the PosENet converges at 0.084 after the first epoch. Also, all the test losses (on natural images PASCAL-S or synthetic images BLACK, WHITE or NOISE) are the same as the training loss. This suggests that the prediction may be completely independent of the content of images.\n\nWe believe the reason behind this is that zero-padding delivers obvious boundary information, the transition between the zeros padded and the content. As discussed in the answer to Reviewer 1, we believe not all padding strategies can deliver this position information.\n", "title": "Response to reviewer 3"}, "r1gWFEOosS": {"type": "rebuttal", "replyto": "SklQXVNkcB", "comment": "Many thanks for your review and we appreciate your insightful feedback.\n\nIn our paper we discussed the implicit effect of the widely used zero-padding mechanism in CNNs. We believe the strong position information is encoded by the value transition near the boundary, zero to non-zero values. Intuitively, we believe other padding strategies, e.g. reflection or replication padding, are not able to deliver this clear position information.\n\nWe compared the effect of Circular padding implemented in Pytorch with the commonly used zero-padding on the Horizontal (H) setting using VGG16, First row of Table 1 (VGG). The training loss of zero-padding starts from 0.045 and drops to 0.03 in the end. While the loss for circular-padding begins at 0.065 and ends at 0.056, much higher than zero-padding. The results of circular-padding on the PASCAL-S dataset are (SPC 0.381, MAE 0.224). Note that this result is similar to the setting of VGG w/o padding, Table 4 (VGG w/o padding on H). This further validates our hypothesis that the position information is delivered by the value transition of zero-padding.\n\nFor the conv-padding paper, according to Equations (4) and (5), their method essentially still applies zero-padding, which means the position information should be encoded. Their method is actually weighing the output of the convolution based on how many zeros are padded, r(i, j).\n", "title": "Response to reviewer 1"}, "rJlN7fOioB": {"type": "rebuttal", "replyto": "BklLFSzRtB", "comment": "We really appreciate your review and  we\u2019re glad to hear you are pleased with the paper!\n\nPlease let us further clarify the implementation details. We did not remove any pooling layers except the last average pooling layer in the ResNet, which was designed to compress the output in order to feed to a Fully Connected (FC) layer. The pooling layers within each network (convolutional part, sometimes called backbone) have been retained because the weight was trained based on that structure design. It is commonplace to replace the FC layers with conv layers as in most dense labeling tasks.\n", "title": "Response to reviewer 2"}, "BklLFSzRtB": {"type": "review", "replyto": "rJeB36NKvB", "review": "The paper investigates to what degree Convolutional Neural Networks (CNNs) learn to encode positional information.\nRather interesting finding is the not only they do encode this information, but that it is to a large degree function of the padding commonly used in the CNN architectures.\n\nThe problem the paper is looking at is well motivated, the experiments are nicely designed and it includes comprehensive ablation study.\nPrevious and related work seems to be well referenced.\nThe main idea of introducing the PosENet to predict the gradient map is neat, and allows for interesting experiments (e.g. what layers most strongly encode the positional information).\n\nI really enjoyed the paper, the overall quality is high and does not seem to be rushed (no obvious typos or mistakes in the figures/tables).\nI believe this should be an accept.\n\nQ:\nI can understand why you removed the pooling layers, but did you try to run some of your experiments with these as well? How were the numbers effected?", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 2}, "r1xDPX_yqH": {"type": "review", "replyto": "rJeB36NKvB", "review": "This paper studies whether and how position information is encoded in CNNs. On top of VGG and ResNet, it constructs an additional PosENet to recover position information. By analyzing how well PosENet recovers position information, this paper provides several interesting findings: CNNs indeeds encode position information and zero-padding is surprisingly important here.\n\n[Pros]\n\n1. I enjoy reading this paper: probing CNNs is not easy, but it designs experiments in an intuitive way and rigorously performs ablation studies and analysis.\n2. The observations and findings are interesting and helpful to the community.\n\n[Cons]\n\n1. A weakness of this paper is that it ignores the impact of training process while probing PosENet: In Table 1, VGG/ResNet perform much better than PosENet, but it could be because VGG/ResNet is easier to train (kind of fine-tuning PosENet only) than PosENet. Would be nice to show the training curve and train PosENet longer.\n2. Zero-padding seems to play a surprisingly important role in encoding position information (Table 5), but it is still unclear why it is so important and how it helps.\n\nOverall, I think this is a good paper.\n", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 2}}}