{"paper": {"title": "Bidirectionally Self-Normalizing Neural Networks", "authors": ["Yao Lu", "Stephen Gould", "Thalaiyasingam Ajanthan"], "authorids": ["yao.lu@anu.edu.au", "~Stephen_Gould1", "~Thalaiyasingam_Ajanthan1"], "summary": "We theoretically solve the exploding and vanishing gradients problem in neural network training.", "abstract": "The problem of exploding and vanishing gradients has been a long-standing obstacle that hinders the effective training of neural networks. Despite various tricks and techniques that have been employed to alleviate the problem in practice, there still lacks satisfactory theories or provable solutions. In this paper, we address the problem from the perspective of high-dimensional probability theory. We provide a rigorous result that shows, under mild conditions, how the exploding/vanishing gradient problem disappears with high probability if the neural networks have sufficient width. Our main idea is to constrain both forward and backward signal propagation in a nonlinear neural network through a new class of activation functions, namely Gaussian-Poincar\u00e9 normalized functions, and orthogonal weight matrices. Experiments on both synthetic and real-world data validate our theory and confirm its effectiveness on very deep neural networks when applied in practice.", "keywords": []}, "meta": {"decision": "Reject", "comment": "Three knowledgeable referees rate this paper ok but not good enough or borderline positive (4,4,6), and one fairly confident referee rates it borderline positive 6. The referees discussed the authors' responses and, while they considered the idea and some of the theory good, they remain concerned, in particular about the experimental part and generalization discussion. The scores remained unchanged after the discussion. Hence I must reject the article. "}, "review": {"6vxzD-J49xX": {"type": "rebuttal", "replyto": "b7GaFdZXG81", "comment": "We thank the reviewer for the appreciation of our work and the critical reading.\n\n**1. The only (minor) issue I have on the presentation side is the name \u201cbidirectional self-normalizing neural networks\u201d.**  \nWe agree with the reviewer and have changed the name to \u201cbidirectionally self-normalizing neural networks\u201d. Thanks for the correction.\n\n**2. I think that in the end the experimental evaluation demonstrates a major weakness of the proposed approach.**  \nOur experiments are indeed on small-scale and artificial. They by design serve the purpose of verifying our theory empirically. \n\nLinear networks would under-fit the datasets. But in our experimental settings, BSNNs with ELU and SELU achieve almost 100% training accuracy, which is not possible for linear networks. The low testing accuracy is due to over-fitting since the networks have huge capacity (200 layers). As trainability and generalization are two separate issues, we only focus on trainability in this paper.\n\nCurrently, our method is mostly of theoretical interests. However, as often with foundational work, the experiments are on small-scale. We refer the reviewer to the original paper of LSTM [1] and VAE [2], in which only toy experiments were conducted. It took the community many years to get them work on large-scale applications.\n\nTo our knowledge, our work shows for the first time that the vanishing/exploding gradient problem is provably solved for nonlinear neural networks. We hope our theoretical breakthrough can inspire future research in neural network theory and large-scale applications.\n\n[1] Long short-term memory, Hochreiter & Schmidhuber, Neural Computation 1997.  \n[2] Auto-encoding variational bayes, Kingma & Welling, ICLR 2014.\n\n", "title": "The results are not equivalent to linearization"}, "EysfBxWulyA": {"type": "rebuttal", "replyto": "6YbssJTKZiT", "comment": "We thank the reviewer for appreciating our theory and the critical reading!\n\n**1. Unclear relevance due to lacking and small-scale experiments, furthermore, due to missing error bars and significance tests.**  \nOur experiments were designed with the following question in our mind.\nIf the conditions in the theory are mostly satisfied, does the vanishing/exploding gradients problem indeed disappear in practice?\nThe experimental settings largely follow from the dynamical isometry paper [1].\n\nIt is not our intention to achieve superior performance on standard machine learning benchmarks such as UCI, CIFAR-10 and ImageNet. Although it is a valuable goal on its own, it is separate from our goal. \n\nWe thank the reviewer for the experiment suggestions but we found them unsuitable for our theory. For UCI, the neural networks do not need to be deep. In the SNN paper, only 32 layers maximally are needed. The vanishing/exploding gradients problem is not severe for shallow networks. For CIFAR-10 and ImageNet, convolutions and residual connections are employed to achieve superior performance. The two modules are not covered in our theory. \n\nOur experiments are indeed on small-scale and artificial. But they by design serve the purpose of verifying our theory empirically, which is the essence of the experimental part of the paper. \n\nThe total running time of the experiments has more than 100 hours, as the networks have 200 layers of 500 units and we have to run each trial for each activation function (with/without batch norm). Given our limited computational resources, we are unable to provide the error bars and exhaustive hyperparameter tuning. We provide full source code allowing researchers to verify our experimental results.\n\nCurrently, our method is mostly of theoretical interests. However, as often with foundational work, the experiments are on small-scale. We refer the reviewer to the original paper of LSTM [2] and VAE [3], in which only toy experiments were conducted. It took the community many years to get them work on large-scale applications.\n\nTo our knowledge, our work shows for the first time that the vanishing/exploding gradient problem is provably solved for nonlinear neural networks. We hope our theoretical breakthrough can inspire future research in neural network theory and large-scale applications.\n\n[1] Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice, Pennington, Schoenholz, Ganguli, NIPS 2017.  \n[2] Long short-term memory, Hochreiter & Schmidhuber, Neural Computation 1997.  \n[3] Auto-encoding variational bayes, Kingma & Welling, ICLR 2014.\n\n**2. Strong uncommented assumtions**  \nWe are confused by this point. We indeed provided comments in the paper. Please see the paragraph right below the Assumptions \u201cThe above assumptions are not restrictive...\u201d on Page 4.\n\n**3. Neglection of learning dynamics**  \nYes, we agree the analysis of learning dynamics is an important topic.  In Figure 10-13 in the Appendix, we show that the gradients are stable for BSNNs throughout training. We leave the mathematical analysis of the learning dynamics to the future work.\n\n**4. The authors should aim at requiring Eq (4) and (5) to be kept on expectation and not exactly.**  \nYes, this is indeed close to our claim. Please see the following paragraph in the paper.\n\u201cHence, combining Theorems 2 and 3, we proved that bidirectional self-normalization is achievable with high probability if the neural network is wide enough and the conditions in the Assumptions are satisfied. Then by Proposition 1, the gradient exploding/vanishing problem disappears with high probability\u201d\n\n**5. The expectation over E[\\phi(x)] is not more equal zero, which introduces a bias shift. How does this bias shift interact with learning?**  \nThat E[phi(x)] is non-zero  is covered by our theory since E[W*phi(x)] is zero under the uniform distribution of orthogonal W.\n\n**6. Lemma 3: Can you comment on how this bound arises from (Ball, 1997)?**  \nGiven a vector u on a sphere, the spherical gap is the normalized surface area for vectors v on the sphere with dot(u,v) >= epsilon. Therefore, if random vector v is uniformly distributed on the sphere, the spherical gap is the probability that dot(u,v) >= epsilon. Since P(dot(u,v) >= epsilon) = P(dot(u,v) <= -epsilon) due to symmetry, we have P(|dot(u,v)| <= epsilon) = P(dot(u,v) >= epsilon) + P(dot(u,v) <= -epsilon) and therefore Lemma 3 by applying Lemma 2.2 in (Ball, 1997). \n\nThe result is known as \u201cnear orthogonality\u201d for two uniformly distributed random unit vectors. We refer the reviewer to Lemma B.1 in [3] for a similar proof.  \n[3] The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent, Sankararaman et al., ICML 2020.\n\nWe simplified the proof of the main results in the revised paper. The proof is more readable than the previous one. The Lemma 3 in the previous version is no longer needed.", "title": "Focus of the experiments is to verify the theory"}, "nOwQInCYuya": {"type": "rebuttal", "replyto": "pTSLSBagIoe", "comment": "We thank the reviewer for the appreciation of our work and the inspiring questions.\n\n**1. While GPN can be achieved via adding a simple affine transform to the existing activation functions like tanh, does it mean that it is more like an initialization technique? Or it still has the property of fixing deviations in preceding layers**  \nIt is a great question that if the statistical properties or assumptions made in the paper can hold during the training of the networks. We plotted the gradient norm during training in Figure 10-13. It seems the gradients of BSNN are stable throughout training. This might give a hint that the statistical properties are robust.\n\n**2. For the experiments on real-world data, the depth 200 may be too deep for these small datasets, and the accuracy reported is less satisfying compared with networks with 30-50 layers)**  \nThe vanishing/exploding gradients problem is severe only for deep neural networks. For networks of 30-50 layers, our method does not show significant advantages. We admit the experimental settings seem artificial in that they are not tuned to solve a particular application. They were merely designed to verify our theory empirically given the assumptions we made.\n", "title": "Thank you for the appreciation"}, "ISUDe5bVvLc": {"type": "rebuttal", "replyto": "CiZpzKfXQZ", "comment": "We thank the reviewer for the appreciation of our work and the inspiring questions!\n\n**1. It would be good if the authors could provide some intuition for the main contribution**  \nWe provided some intuition of our main contribution in the \u201cSketch of the proofs\u201d paragraph. The shift and scaling of activation allow us to apply the concentration of norm property (Lemma 2). Here, we give the following Python snippet. Running the code will provide some intuition into the theory.\n\n~~~\nimport torch\n\n# Forward norm-preservation \nz = torch.randn(10000)  \nf = 1.4674 * torch.tanh(z) + 0.3885  \nprint(z.norm(), f.norm()) \n\n# Backward norm-preservation \nz = torch.randn(10000) \nf = 1.4674 * (1-torch.tanh(z)**2) \nx = torch.rand(10000) \ny = f * x \nprint(x.norm(), y.norm())\n~~~\n\n**2. How would this work on convolutional networks where one does not typically expect large \u201cwidth\u201d i.e. is this work only of theoretical interest?**  \nCurrently, our theory is developed only for fully connected networks. Additionally, large width convolutional networks have achieved some empirical successes [1]. In the future work, we plan to extend our theory to more sophisticated architectures such as convolutional networks, recurrent networks and networks with skip connections (e.g., ResNet). \n\n[1] Wide Residual Networks, \nZagoruyko & Komodakis, BMVC 2016.\n\n**3. How does this impact generalization?**  \nUnderstanding the relationship between gradient behaviors and generalization is certainly an interesting topic but however beyond the scope of this paper. There are claims that trainability and generalization are separate problems [2]. We only investigate trainability in this paper. We look forward to seeing more work done on this topic by ourselves and the community in the future.\n\n[2] Disentangling Trainability and Generalization in Deep Neural Networks, \nXiao, Pennington, Schoenholz, ICML 2020.\n\n**4. How would you compare with residual networks?**  \nIn our experimental settings, adding residual connections often leads to gradient explosion (with or without BatchNorm). Currently, our theory is developed for fully connected networks and incompatible with residual networks. The reason is that orthogonal matrices and GPN functions are norm-preserving (under the conditions given in the paper) but adding residual connection is not. In the future, we plan to extend our theory to residual networks.\n\n**5. What is the significance of Theorem 1?**  \nTheorem 1 shows the fundamental relationship between a function and its derivative under Gaussian measure. It inspires us to come up with the Gaussian-Poincare Normalization, which has a similar flavor. Additionally, the proof of Proposition 3 relies on Theorem 1.\n\n**6. Theorem 2 statement should say Phi(x) is Gaussian-Poincare normalized?**  \nYes, both Theorem 2 and 3 rely on the Assumptions, as we stated in the paper.", "title": "Thank you for the appreciation"}, "pTSLSBagIoe": {"type": "review", "replyto": "dFBRrTMjlyL", "review": "Summary:\n\nIn this paper, the authors introduce the bidirectional self-normalizing neural networks (BSNN) that preserve the norms in both forward and backward passes. To serve such purpose, a new class of activation functions, GPN, is proposed, which can be obtained via the affine transform of existing activation functions like tanh and SELU. The authors prove that under orthogonal weights and GPN, the norm for both forward and backward passes can be well preserved. Besides, the conclusions are also supported by experiments on synthetic data and real-world data like MNIST and CIFAR 10.\n\n\nStrength:\n\n--The paper is well organized and easy to follow.\n\n--The proofs seem rigorous.\n\n--The experiments on synthetic data and real-world data are consistent with the theoretical prediction.\n\n\nWeakness:\n\n--One major strength of the self-normalizing neural network powered by SELU as well as batch normalization is that, even if the statistical properties deviate from the ideal point at some layers, the succeeding layers can gradually fix the deviation. This property greatly stabilizes the training process even under a high learning rate. On the other hand, methods like dedicated initialization and weight normalization/standardization do not have this property, and previous studies have shown that they are less effective compared with SELU and BN. While GPN can be achieved via adding a simple affine transform to the existing activation functions like tanh, does it mean that it is more like an initialization technique? Or it still has the property of fixing deviations in preceding layers?\n\n--For the experiments on real-world data, the depth 200 may be too deep for these small datasets, and the accuracy reported is less satisfying compared with networks with 30-50 layers. Although I understand that the authors want to demonstrate the performance of self-normalization in very deep models, it would be more convincing if the proposed activation functions are also compared with existing ones like SELU under a more practical depth (e.g., 30-50 layers).\n", "title": "Good Work but Need more Analysis", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "b7GaFdZXG81": {"type": "review", "replyto": "dFBRrTMjlyL", "review": "The paper tackles the problem of vanishing/exploding gradient in neural networks by imposing constraints on the weights and activation functions that ensure consistent Frobenius norm across updates to the weight matrices in all layers of the network.  \n\nThe paper is very well written, straight to follow, theory is well laid out and it is interesting.  The only (minor) issue I have on the presentation side is the name \u201cbidirectional self-normalizing neural networks\u201d.  This phrasing initially made me think the the network is bidirectional\u2026but the network itself is not, it\u2019s the self-normalizing that is bidirectional.   Wouldn\u2019t it be more precise to say \u201cbidirectionally self-normalized neural networks\u201d?\n\nHowever, despite a well outlined theory, I think that in the end the experimental evaluation demonstrates a major weakness of the proposed approach.  Credit to the authors for including Table 2 with evaluation on real datasets, but the generalisation performance there seems to show quite clearly that -GPN variants of the networks are equivalent to linear classifiers.  A quick check of the performance of linear classifier on MNIST and CIFAR10 without augmentation reveals 93% and 41% testing accuracy - pretty close to the performance shown in Table 2.  Sure, -GPNising allows for training a 200-layer model, but it seem to come at the cost of reducing the model to barely more than a linear classifier.  The sacrifice of all non-trivial representational power in order to gain stable learning dynamics is just not worth it.\n", "title": "Nice theoretical treatment, but results equivalent to linearising deep models", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "CiZpzKfXQZ": {"type": "review", "replyto": "dFBRrTMjlyL", "review": "This paper presents an interesting approach to normalizing the signals in a deep network both in the forward pass and in the backward pass with the goal of preventing vanishing and exploding gradients for better training convergence (and perhaps general elegance). \n\nPreviously, this has been partly solved by the use of orthogonal weight matrices (and this completely solves the problem for linear networks). The authors show that additionally a specific modification of the non-linearity (essentially a scale and shift) is enough to guarantee good bi-directional normalization with high probability.  \n\nI did not check the math. It would be good if the authors could provide some intuition for the main contribution: that an appropriate scaling and shifting of the nonlinearity is what is needed. \n\nEmpirical results are demonstrated in essentially toy settings (synthetic, and mnist/cifar with fully connected networks) but these experiments are thorough (but see question below). \n\nMy positive review of the paper is mostly based on viewing this as a natural completion of an existing line of work (around the use of orthogonal weight matrices for normalization), but I worry that this line of work may not lead to practically useful results since the orthogonality constraint is hard to enforce exactly, and it is not clear if this would work for convolution given the requirement for sufficient width. This is why I am hesitant to give it a higher score. \n\nQuestions, mostly around the practicality of the proposed techniques.\n\n1. How would this work on convolutional networks where one does not typically expect large \u201cwidth\u201d i.e. is this work only of theoretical interest?\n\n2. How does this impact generalization? There is reason to believe that mucking with gradients can hurt generalizations. This has been discussed for  adaptive methods (Duchi et al., etc. alluded to the in the introduction) in https://arxiv.org/abs/1705.08292.\n\n3. How would you compare with residual networks? Particularly in terms of run-time and generalization.\n\nMinor:\n\na. What is the significance of Theorem 1?\n\nb. Theorem 2 statement should say $\\phi$ is Gaussian-Poincare normalized?\n", "title": "A completion of sorts for an interesting line of work ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "6YbssJTKZiT": {"type": "review", "replyto": "dFBRrTMjlyL", "review": "Summary: The authors introduce a variant\nof self-normalizing networks (SNNs) that also maintain\nthe norm of the errors during backpropagation. The presented\nBidirectional self-normalizing networks (BSNN)\nhave interesting theoretical properties, but are hardly \nassessed empirically. There are severe flaws in \nthe method evaluation. \n\n\nPros: \na) The mathematical formulations are very clear \nand sound.\nb) The connection between SNNs and works in\nmean-field theory (Poole, Schoenholz) are very\nwell put in place. \nc) Making both the forward- and the backward pass \nself-normalizing appears desirable for NNs.\nd) The work is well embedded into related works. \n\n\nCons: \na) Unclear relevance due to lacking and small-scale experiments, furthermore,\ndue to missing error bars and significance tests. \n\ni) Firstly, SNNs were introduced with relatively large-scale experiments on fully-connected networks. \nIn order to demonstrate improvements due to bidirectional normalization, \nthe suggested activation functions should be compared in that set of experiments. \nThe presented experiments are done with architectures that are extremely far from the current SOTA on CIFAR10, \nwith 46% versus 90% accuracy already reported in 2013 (see ref[2]). The authors\nshould perform experiments on large scale, e.g. CIFAR100 and ImageNet or voice recognition [3], \nwith architectures at or close to the state-of-the-art to demonstrate \nthe broad applicability and relevance of their method. \n\nii) It is unclear whether BSNNs represent an advance at all because \nall presented performance metrics come without repetitions, error bars\nand significance tests. Purported improvements could be just by chance. \nThe authors should use repetitions, present all metrics with confidence\nintervals and perform statistical tests. \n\niii) Conclusions of the experiments are not justified due to limited\nhyperparameter search. The setting of the experiments that activation\nfunctions are compared with the same architecture, same learning rate, \nmomentum term, and batch size strongly limits the conclusions since\nactivation functions could work with slightly different settings. \nThe authors should allow each method (here: activation function) \nto optimize hyperparameters. \n\niv) It is not necessary that almost exact preservation \nof the norm in the backward pass must improve learning, especially,\nbecause it is traded against a bias shift. \nFor standard SNNs, with the SELU activation, there is \na small exploding gradient, which, however leads\nto lower layers learning faster [1]. This could even assist learning.\nThis point should make clear that stringent empirical\nassessment is necessary. Therefore, the authors should provide\na much wider empirical assessment of BSNNs.\n\nb) Strong uncommented assumtions. The presented work requires\northogonal matrices, and several other assumptions. Those assumptions\nare left mostly uncommented and leave unclear how applicable this \ntheory is and how strong it restricts the results and conclusion.\nThe authors should comment on all assumptions and relate it to learned \nneural networks. \ni) The assumption of orthogonal weight matrices is a strong one. \nThis is computationally extremely costly, even in the relaxed way\nof the authors (Section 3.2). The authors should elaborate on this.  \n\nc) Neglection of learning dynamics. Whereas, the derivation of SNNs\ninvolves learning dynamics by showing that even with changes in the\nweights, a fixed points remains, BSNNs neglect learning dynamics. \nEven definition of self-normalization requires exact preservations\nof the norm. The authors should aim at requiring Eq (4) and (5) to \nbe kept on expectation and not exactly. Otherwise, no learned \nnetwork is self-normalizing. \n\n\nMinor:\n\n\nQuestions:\na) The expectation over E[\\phi(x)] is not more equal zero, which introduces\na bias shift. How does this bias shift interact with learning? \nCan you state this difference to SNNs clearer in the manuscript?\nb) Lemma 3: Can you comment on how this bound arises from (Ball, 1997)?\n\n\nJustification of score:\nDespite my many points of concerns, the theory in this paper and the main \nidea are good. I would be willing \nto vote for acceptance if the experimental part is strongly improved. \n\n\nReferences:\n[1] Hoedt, P.J., Hochreiter, S. and Klambauer, G. Characterising activation functions by their backward dynamics around forward fixed points. Critiquing and Correcting Trends in Machine Learning workshop at NeurIPS 2018. \n[2] Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., & Bengio, Y. (2013, May). Maxout networks. In International conference on machine learning (pp. 1319-1327). PMLR.\n[3] Huang, Z., Ng, T., Liu, L., Mason, H., Zhuang, X., & Liu, D. (2020, May). SNDCNN: Self-normalizing deep CNNs with scaled exponential linear units for speech recognition. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 6854-6858). IEEE.\n\n", "title": "Good idea and theory; substandard experimental part.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}