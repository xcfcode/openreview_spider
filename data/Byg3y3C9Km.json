{"paper": {"title": "Learning Protein Structure with a Differentiable Simulator", "authors": ["John Ingraham", "Adam Riesselman", "Chris Sander", "Debora Marks"], "authorids": ["john.ingraham@gmail.com", "adam.riesselman@gmail.com", "cccsander@gmail.com", "deboramarks@gmail.com"], "summary": "We use an unrolled simulator as an end-to-end differentiable model of protein structure and show it can (sometimes) hierarchically generalize to unseen fold topologies.", "abstract": "The Boltzmann distribution is a natural model for many systems, from brains to materials and biomolecules, but is often of limited utility for fitting data because Monte Carlo algorithms are unable to simulate it in available time. This gap between the expressive capabilities and sampling practicalities of energy-based models is exemplified by the protein folding problem, since energy landscapes underlie contemporary knowledge of protein biophysics but computer simulations are challenged to fold all but the smallest proteins from first principles. In this work we aim to bridge the gap between the expressive capacity of energy functions and the practical capabilities of their simulators by using an unrolled Monte Carlo simulation as a model for data. We compose a neural energy function with a novel and efficient simulator based on Langevin dynamics to build an end-to-end-differentiable model of atomic protein structure given amino acid sequence information. We introduce techniques for stabilizing backpropagation under long roll-outs and demonstrate the model's capacity to make multimodal predictions and to, in some cases, generalize to unobserved protein fold types when trained on a large corpus of protein structures.", "keywords": ["generative models", "simulators", "molecular modeling", "proteins", "structured prediction"]}, "meta": {"decision": "Accept (Oral)", "comment": "This paper presents a differentiable simulator for protein structure prediction that can be trained end-to-end. It makes several contributions to this research area. Particularly training a differentiable sampling simulator could be of interest to a wider community.\n\nThe main criticism comes from the clarity for the machine learning community and empirical comparison with the state-of-the-art methods. The authors' feedback addressed a few  confusions in the description, and I recommend the authors to further polish the text for better readability. R4 argues that a good comparison with the state-of-the-art method in this field would be difficult and the comparison with an RNN baseline is rigorously carried out.\n\nAfter discussion, all reviewers agree that this paper deserves a publication at ICLR."}, "review": {"SJlUZjqEim": {"type": "review", "replyto": "Byg3y3C9Km", "review": "Post-rebuttal revision: The authors have adressed my concerns sufficiently. The paper still has issues with presentation, and weak comparisons to earlier methods. However, the field is currently rapidly developing, and comparing to earlier works is often difficult. I believe the Langevin-based prediction is a significant and clever contribution. I'm raising my score to 6.\n\n------\n\nThe paper proposes an end-to-end neural architecture for learning protein structures from sequences. The problem is highly important. The method proposes to use a Langevin simulator to fold the protein \u2018in silico\u2019 from some initial state, proposes numerous tricks for the optimisation, and proposes neural networks to extract information from both the sequence and the fold state (energy function). The system works on internal coordinates, which are conditioned and integrated on the fly. The method seems to perform very well, improving upon their baseline model considerably.\n\nIn spite of the paper being an outstanding work, I have two criticisms about the accessibility and impact of the paper on the broader ICLR audience. In its current form and complexity, the paper feels accessible mostly to a narrow audience.\n\nFirst, the framework proposed in the paper is massive, containing a large amount of components, neural networks, simulators, integrators, optimisation tricks, alignments, profiles, stabilizations, etc. The amount of work done in the manuscript is staggering, but the method is also difficult to understand from reading the main manuscript alone. The 10+ page appendix is critical for understanding (for instance, the appendix reveals that MSA is used to generate more data), and even with it the method is difficult to grasp as a whole. This paper should be presented in a journal form with a presentation not hindered by page limits, while currently one needs to jump between the main text and appendix to get the whole picture. I also wonder if some parts of the system have already been published, and perhaps the presentation could be condensed that way. \n\nSecond, the introduction lists numerous competing methods both on the protein modelling side and on the MCMC vs optimisation side. The paper does not compare to any of these, which is strange, and makes it difficult to assess how much this paper improves upon state-of-the-art. Right now its unclear what is state-of-the-art in general. No bigger context of protein folding is given either, for instance, how well the method fares against purely alignment based approaches, or against purely physics-based simulators. Finally, the experimental section poorly describes how all the pieces of the system affect the final predictions. The discussion on the exploding gradients and dampening is excellent however. The only baseline is one with the simulator replaced by an RNN. There does not seem to be any running time analyses. As such, it is hard to interpret the current system, and it feels like a black box.", "title": "review", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1lRoQMLTX": {"type": "review", "replyto": "Byg3y3C9Km", "review": "Overall this is an important piece of work that deserves publication at ICLR. I recommend to the authors revise their manuscript to make it more accessible to the machine learning community and that they provide better context to allow them to assess the relative quality of the work compared to state of the art results.\n\n# Quality\n\nThe hypothesis that the authors set out to resolve is whether there is an advantage in using an energy function sampled by Langevin dynamics versus simply using a neural network to regress shape from sequence. They construct a flexible deep energy model where the sequence and structure dependent parts are separated in such a way that fast rollouts are possible. They also adapt the learning algorithm to  ensure that long rollouts can be carried out and present a clever trick for integrating internal coordinates efficiently on a GPU. \n\nThe only criticism in terms of quality of work is that it somewhat lacks putting in context with results from the larger community, for example how well does the model compare in terms of speed and accuracy with co-evolutionary approaches? I realise it will not be possible to give a completely fair like to like comparison, but it will help readers put the results in context if they understood, for example, what the average TM score for CASP12 results was, as summarized in this paper for example: https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25423. Similarly, it would be useful to compare the baseline - at least qualitatively - with the results from AlQuraishi et. al. whose model seems very similar in spirit.\n\n# Clarity\n\nI think in terms of clarity, the paper could be improved a little to take into account the audience of ICLR. In particular:\n\n* It may be useful to add a sentence of how profiles have been found to improve secondary structure prediction greatly. Currently the text makes it sound as though they constitute a sort of 'data augmentation', whereas in my opinion they add information compared to the sequence alone. In fact a brief explanation of the importance of homology might help the reader understand the relevance of the hierarchical approach taken to splitting the training set.\n\n* Fig. 2 caption. Could add some information to explain what panel B is showing. I think this would go a long way to explain why both cartesian and internal coordinates are important.\n\n* Fig. 4 second panel. The x axis should be labeled fraction or be numbered 0-100.\n\n* Fig 4. caption. The figure does not have a caption explaining what the graphs are showing. This would be a good place to explain that the colors refer to test sets that overlap with the training set in the full CATH code (black), overlap only in the CAT code (orange) etc. I admit I had found the explanation of the test/train/validation split rather confusing. It is not clear what the validation set is used for, i.e. which hyper-parameters have been tuned on it etc.\n\n* The nature of the loss. The appendix does a good job in describing each term in the loss function, but does not explain how the empirical loss function and the log-likelihood terms are mixed together. \n\n# Originality\n\nThe work is original and is references the relevant literature.\n", "title": "Review", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HylvOjIjCQ": {"type": "rebuttal", "replyto": "r1etop_q0X", "comment": "We apologize and feel that we may have been unclear in how we are framing this work. Our primary goal is to explore the utility of a learned, differentiable simulator in the context of the challenging problem of protein folding and to show that it can have better inductive bias than an approach based directly on angle-prediction (i.e. AlQuraishi and our baseline RNN). To be clear, we do not expect that this method would be competitive at CASP (where all forms of sequence and structure data before a certain date are fair game), as the cost of scaling training limited us to a medium-sized dataset of proteins of limited length (200 AAs) from which many topologies and architectures have additionally been held out.\n\nWe feel that it is still useful to present this novel methodology before it can scale to the challenge of CASP, because we are trying to argue about the inductive bias of simulators and not to make a general claim about deep methods versus conventional methods. Both our method and AlQuraishi\u2019s can create models with hundreds to thousands of atoms in milliseconds to seconds, which is far off from the timescales of physics based approaches (even the very impressive work of Jumper et al measures simulation times on the order of CPU-days), and we consider that sufficiently interesting to motivate research on \u2018deep\u2019 approaches.\n\nWith that in mind, we believe that the RNN baseline is a meaningful comparison and not \u2018artificial\u2019. Within the very recently emerging field of end-to-end models of protein structure, the idea of directly predicting internal coordinates (AlQuraishi) may be considered the other established paradigm (In Anand et al\u2019s currently available manuscript, they focus on structural imputation & generation rather than prediction). Like AlQuraishi\u2019s work, our RNN baseline composes a multilayer, bidirectional LSTM that predicts internal coordinates (in our case, coarse) with a scoring function on the resulting atomic Cartesian structure (in our case, after imputation). If we were to directly retrain the AlQuraishi model on our dataset, there are many possible explanations for performance differences such as different losses, the imputation network, training details, etc. We designed the experiments around our baseline because it allowed us keep those factors constant while replacing only the simulator portion of the model (this is what we meant by \u201cwhere differences in performance come from\u201d). Regarding the methods of Jumper and Krupa, we certainly want to acknowledge their contributions and related ideas but find that the costs of training (much longer simulations per protein) would be very challenging to scale to our dataset of 35k proteins.\n\nTo your other suggestions, we will add per-protein running time statistics and specific schematics for the 1D and 2D neural network modules upon revision. \n\nWe thank you for your patience with this work, and hope that the ICLR community can find value in the methodological and conceptual contributions.\n", "title": "Clarity on framing of work"}, "SkxK8JG507": {"type": "rebuttal", "replyto": "SJlUZjqEim", "comment": "Thank you for your constructive review of this work. We hope that with improved presentation and contextualization, it can be relevant to a broad audience at ICLR.\n\n> In its current form and complexity, the paper feels accessible mostly to a narrow audience.\n> The amount of work done in the manuscript is staggering, but the method is also difficult to understand from reading the main manuscript alone. The 10+ page appendix is critical for understanding \u2026 This paper should be presented in a journal form with a presentation not hindered by page limits\u2026 I also wonder if some parts of the system have already been published, and perhaps the presentation could be condensed that way.\n \nWhile a journal might better accommodate the appendix, we believe that the complete system can be of interest to a general audience at ICLR because it connects recent interesting ideas in machine learning (e.g. differentiable simulators & meta-learning) to a challenging and well-known application domain with novel methodology (transform integrator, stabilization strategies, etc). To make these contributions more accessible, we have simplified the presentation of the model in the main text and added previously missing legends and overview paragraphs. (Lastly, in case its relevant, no parts of the model have previously been published.)\n\n> The paper does not compare to any of these, which is strange, and makes it difficult to assess how much this paper improves upon state-of-the-art. Right now its unclear what is state-of-the-art in general.\n\u2026 The only baseline is one with the simulator replaced by an RNN\n\nWe focus on comparing to end-to-end approaches with a controlled dataset, largely due to the computational challenges of training the differentiable simulator model. We have added a discussion of Advantages and Disadvantages that more explicitly makes the connection to recent end-to-end methods for predicting protein structure in terms of angles (AlQuraishi et al)  and our baseline RNN. We focus on comparing to the RNN baseline because it shares the same loss and data augmentation strategies as our simulator, thus making clearer where differences in performance come from. While we do see that our differentiable simulator model can generalize more effectively to distant folds on our controlled dataset of ~35k proteins < 200AAs, the dataset splitting and significant cost of training (2 months on 2 GPUs) mean that it is difficult to evaluate the approach performance on large proteins. Nevertheless, new models in machine learning with better inductive biases but greater computational demands often get their start via medium-sized controlled datasets.\n\n> There does not seem to be any running time analyses\n \nThanks for pointing this out. We have added a table of qualitative running times in the results section.\n \n> the experimental section poorly describes how all the pieces of the system affect the final predictions\n> it feels like a black box.\n \nWhile we could not afford to perform ablation studies of individual components given the long training time, we believe that structured nature of a differentiable simulator can make it easier to interpret and engineer than purely neural architectures. For example, the Markov Random Field formulation of the energy function means the sequence and structure features can be interpreted separately, and both the efficacy of Langevin dynamics and benefits of alternative coordinate parameterizations to sampling are well-understood phenomena.", "title": "Response to AnonReviewer2"}, "HJlD-yM90X": {"type": "rebuttal", "replyto": "Hkgk3b343m", "comment": "Thank you for your positive comments and constructive suggestions.\n\n> The paper is clearly written, however the description of the method can be confusing. \u2026\nFig. 6 in the appendix helps, however it would be better to have a (perhaps more concise) overview in the main text.\n \nTo improve accessibility, we have simplified explanations of individual components and added overviews to the main text and Figure 6.\n \n> it would be interesting to see whether NEMO outperforms a baseline trained on profile features\n \nWhile it was not clear from the text, the original RNN was indeed trained on profiles, and we apologize for the confusion (Please see our previous comment).\n \n> it would be interesting to see some generated atomic substructures from the imputation network, in particular an analysis of how diverse the generated atom positions are and whether they depend on the local environment.\n \nWe agree that this is an interesting question, though our current model will likely not give an interesting answer since the imputation network is a deterministic mapping (ignoring dropout). That said, all secondary structure calls in the visualizations come directly from hydrogen bonding calls in pymol (default settings) which suggests that model can capture some aspects of (locally) orientation-dependent atom placement.", "title": "Response to AnonReviewer1"}, "rJeaJ0bc0X": {"type": "rebuttal", "replyto": "B1lRoQMLTX", "comment": "We thank the reviewer for the positive comments about the approach and suggestions for how to improve the presentation.\n\n> It may be useful to add a sentence of how profiles have been found to improve secondary structure prediction greatly. Currently the text makes it sound as though they constitute a sort of 'data augmentation', whereas in my opinion they add information compared to the sequence alone.\n \nWe agree that evolutionary profiles add far more information than data augmentation, and have added an explicit point of comparison in the results section to draw the connection to SS prediction.  We have also clarified the distinction between data augmentation and profiles in Appendix B.\n \n> lacks putting in context with results from the larger community, for example how well does the model compare in terms of speed and accuracy with co-evolutionary approaches? \u2026 Similarly, it would be useful to compare the baseline - at least qualitatively - with the results from AlQuraishi et. al. whose model seems very similar in spirit.\n \nWe agree that the paper needs to provide better context in the landscape of methods for protein structure prediction and have tried to address this by adding an \u2018Advantages and Disadvantages\u2019 paragraph to the Results. Since scaling our method to larger datasets of proteins remains difficult with current computational resources, we focus primarily on comparing to other end-to-end approaches, of which an RNN-based angle prediction (of AlQuraishi et al) is the other major approach. Hopefully our updated text and figure legends can clarify this.\n \n> [Fig 2., Fig 4 second panel and caption]\n\nThank you for these suggestions. We have fixed these figure legends to be clearer.\n \n> does not explain how the empirical loss function and the log-likelihood terms are mixed together.\n \nThanks for pointing this out. Our loss involves a simple sum of all terms without weights, and we have added a sentence to clarify this.  \n\n>I admit I had found the explanation of the test/train/validation split rather confusing.It is not clear what the validation set is used for, i.e. which hyper-parameters have been tuned on it etc. \n \nWe have improved the explanation of how we split the dataset hierarchically and temporally to capture different generalization difficulties. We did not explicitly tune hyperparameters on the validation set (in part due to the long training time, for which 200k iterations was what we could afford), but we did allow ourselves to look at the validation set during model development and thus refer to it as such.\n", "title": "Response to AnonReviewer4"}, "Hyxj9pbc0Q": {"type": "rebuttal", "replyto": "Skx-IPBvTQ", "comment": "We thank the reviewer for the extensive comments as well as suggestions for improving the presentation and evaluation.\n\n>Figure. 6 presents a scheme of the entire system, but it lacks details about the different modules, and it is not clear how they interact and how their training together is performed. \n\nWe apologize for the lack of clarity and have added a legend to this figure that walks through the complete sampling process (which is, in turn, backpropagated through).\n\n> The pseudo-code boxes describing Algorithms 1-4, and Table 2 describing the representation are informative and helpful, and more descriptions of this type would help.\n>  what do 'CartesianStep' and 'ClippedInternalStep' mean? where are they described?\n \nWe are glad that these algorithm boxes are helpful, and while we have not made them for 'CartesianStep' and 'ClippedInternalStep', these computations refer to the Langevin Dynamics and Speed Clipping paragraphs in Appendix A.\n \n> I didn't see an Algorithm describing the atomic imputation part.\n \nThe atomic imputation was small and potentially easy to miss, but is defined in Section 2.4 \u2018atomic imputation\u2019. We have modified the formatting and added an overview paragraph to highlight its importance.\n \n> There is also no single place where all the parameters used by the authors\n\nWhile the complexity of the model makes presenting the hyperparameters in table form cumbersome, we intend to release the code which includes hyperparameters as structured objects.\n\n> if and how are constants multiplying them chosen to give lower/higher weights to some of the losses etc\n \nRegarding the loss, we simply sum the individual loss terms and have not explored weighting (owing to the the costly training time). We have clarified this in the text. \n\n> It is not clear to me how good are these results, except that they are shown to be better than a baseline simple model. How well does the author's model compare to other recently suggested end-to-end models? (the authors mention AlQuraishi, Anand&Huang, papers). How do they compare to state-of-the art structure prediction programs? (e.g. CASP winners)?\nI realize giving an automatic end-to-end solution is interesting even if performance is below that of best programs, but still it would be good to know gaps.\nIf such comparisons are less meaningful/not practical to perform this should be argued convincingly. \n \nTo better contextualize our model, we have added an Advantages and Disadvantages discussion as well as an improved explanation of the baseline. The RNN baseline method is similar to the approach of AlQuraishi (though differing in the use of coarse-to-fine reconstruction as well as our loss terms). We focus on comparing to the baseline model because it uses the same loss and imputation network, thus isolating the differences to the simulator itself. Regarding CASP: Although our method was able to scale to training on a database of ~35k protein domains up to length 200 (on 2 GPUs & 2 months), this particular dataset excludes the longer proteins and more diverse templates that would be necessary to be relevant to CASP.\n \n> It would also be useful to add some metrics of running time\n \nWe have added a qualitative table of approximate running times for our methods as well as conventional protein folding approaches.\n \n> typos and inconsistent notations\n\nThank you for pointing these typos out, we believe they are now fixed.", "title": "Response to AnonReviewer3"}, "Skx-IPBvTQ": {"type": "review", "replyto": "Byg3y3C9Km", "review": "The paper proposes a new end-to-end training framework for computational prediction of protein structure from sequence. \nThis is a very important problem and any progress due to new data and/or methods for utilizing may have high impact. \n\nThe paper presents several technical contributions in the modelling and training procedure - for example, automatic transformation between Cartesian and angular coordinates, using Langevin dynamics, and imputation method to get fine atomic coordinates. \n\nThe overall breadth and depth of the methods presented in the paper are impressive. The paper describes a quite complicated systems with multiple modules interacting between them. The paper doesn't describe the system built in enough details, although many of the details are given in the appendix.  \nFigure. 6 presents a scheme of the entire system, but it lacks details about the different modules, and it is not clear how they interact and how their training together is performed. \nThe pseudo-code boxes describing Algorithms 1-4, and Table 2 describing the representation are informative and helpful, and more descriptions of this type would help. \nFor example:    - In Algorithm 3, what do 'CartesianStep' and 'ClippedInternalStep' mean? where are they described? (should have their own boxes/description).  \n\t\t- I didn't see an Algorithm describing the atomic imputation part. \n\t\t- It would be good to add a high-level pseudo-code for the entire end-to-end training algorithm. In it there could be calls to Algorithms 1-4 when needed.  \n\nThere is also no single place where all the parameters used by the authors to achieve their empirical results are presented \n(e.g. learning rates, Gaussian kernel widths, how are random time steps for enforcing Lipschitz condition chosen etc.). \nIn addition, the empirical loss defined in eq. (8) is a sum of 6 different losses. It is not clear how are these very different losses scaled to the same 'units', which ones are more important, \nif and how are constants multiplying them chosen to give lower/higher weights to some of the losses etc. - I guess these choices will have a large effect on the training. \n\nThe authors present generalization results of their trained model in predicting 3D structures from CATH at different generalization level\n(i.e. different similarity levels to the training set proteins). It is not clear to me how good are these results, except that they are shown \nto be better than a baseline simple model. How well does the author's model compare to other recently suggested end-to-end models? \n(the authors mention AlQuraishi, Anand&Huang, papers). How do they compare to state-of-the art structure prediction programs? (e.g. CASP winners)? \nI realize giving an automatic end-to-end solution is interesting even if performance is below that of best programs, but still it would be good to know gaps.\nIf such comparisons are less meaningful/not practical to perform this should be argued convincingly. \nIt would also be useful to add some metrics of running time - it is not clear how computationally heavy and scalable is the author's model and training, compared to other methods. \n\n\nThere are many typos and inconsistent notations which makes it harder for the reader to understand the paper. \nFor example, 'Figure ??' in multiple locations, wrong Figure referenced, using s vs. S for sequence - S is defined as an L*20 matrix but in the appendix there are\n3 indices: s_{i,l,j} and it looks like different sequences in alignment should be denoted s_i. \nEquation for M_{l,j} isn't clear: j is used both as fixed index and index in summation. \nThe indexing in 'orientation vectors' v-hat_ij definition seems off (the formula of base vectors gives 0/0)\n\n\n\n", "title": "Interesting paper - not clear and mature enough ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Hyl_nFNX6m": {"type": "rebuttal", "replyto": "Hkgk3b343m", "comment": "Thank you for your review and positive words about the idea and approach. While we will respond in full later, we wanted to briefly clarify that all RNN models and NEMO results of Fig 4,5 were trained on profiles. The sentence \"We report the results of a sequence-only model in Table 1 and Figure 4\" is a figure-link typo and should instead read \"We also report the results of a sequence-only NEMO model in Table 1 and Figure 9.\" We apologize for the confusion and will make these points clearer upon revision.\n\nIn the meantime, we hope that this can clarify that our main claim about generalization is based on comparing profile-based NEMO to profile-based baselines.", "title": "Quick clarification: RNN baselines are also trained on profiles"}, "Hkgk3b343m": {"type": "review", "replyto": "Byg3y3C9Km", "review": "This paper presents an end-to-end differentiable model (NEMO) for protein structure prediction. I found this paper very interesting and the idea of training the network through the sampling procedure promising. The authors present the challenges and techniques (damping, Lyapunov regularization etc) in detail.\n\nThe paper is clearly written, however the description of the method can be confusing. This stems in part from the many components of the network as well as the fact that the protein is represented using various coordinate systems and features, so that it is not easy to follow which applies at each stage. Fig. 6 in the appendix helps, however it would be better to have a (perhaps more concise) overview in the main text.\n\nIn the evaluation, the NEMO method is compared to a baseline approach using RNNs. While NEMO trained on profile features performs best, the baseline is trained on sequences only. However, it outperforms the NEMO model trained on sequence-only in every category. Therefore, it would be interesting to see whether NEMO outperforms a baseline trained on profile features. Otherwise, I am not certain whether I can follow the conclusion that \"NEMO generalizes more effectively\". Beyond that, it would be interesting to see some generated atomic substructures from the imputation network, in particular an analysis of how diverse the generated atom positions are and whether they depend on the local environment.\n\nOverall, I appreciate the general idea and find the proposed approach very interesting. The contribution could have been stronger with a more detailed evaluation and better presentation.", "title": "Interesting idea with some weaknesses in the evaluation", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ryeM3Sm4im": {"type": "rebuttal", "replyto": "ByeS8UUb5X", "comment": "Thanks for catching the fraction (0-1) / percent (0-100) mislabeling, we will fix it.\n\nIt is correct that we split train and validation based on CATH 4.1 (hierarchical split) and tested on everything that was new to CATH 4.2 (temporal split), with the test set stratified into subsets of varying difficulty, from the very easy to the very difficult, and clearly labelled as such. While we currently stratify by CATH similarity between {test} and {train} (to evaluate more fold types), we can also include results that stratify by CATH similarity between {test} and {train+validation} (at the cost of reduced fold diversity). Since we subject all models and baselines to the same splits either way, all of these are interpretable measures of generalization.", "title": "Test set is stratified"}, "Ske1rVBZcX": {"type": "rebuttal", "replyto": "B1eOXxVb9X", "comment": "Thank you for your question about the training and test splits; we will do our best to clarify briefly here as well as update the manuscript at the next opportunity.\n\nWe very much agree that careful analysis of test and train overlap is one of the key issues when interpreting the results on protein structure prediction and we created our dataset in an attempt to frame this problem in terms of widely used (ie CATH) fold classifications.\n\nTo clarify we *did not* train on all of CATH 4.1 but rather intentionally hierarchically purged the training set at multiple levels of A, T and H. So, after collecting all folds in 4.1 (subject to max length 200 and class 1,2,3), we then randomly selected a subset of A classifications and purged all folds descending from these into the A-level validation set. We repeated this process two more times for T and H. While the specific domain-level splits are contained in a large table not feasible for attaching to an openreview submission, we intend to make these available and will try to summarize the high level held out classifications at next update to the manuscript.\n\nRegarding the middle panel of Figure 4, the x axis is correct and our test set from CATH 4.2 does (naturally) contain some folds of very high (sometimes complete) sequence overlap. However, because we purged the training set (as just described above), the test set also contains many sequences of low/random sequence overlap (left cluster in middle panel) and low classification overlap. The color coding on this scatter plot indicates how close the given test domain is to the training set, where blue means that the ATH classifications were not present in train (column C in table 1), magenta means TH were not present in train (column A), orange means H was not present in train (column T) and gray (column H) means that the full CATH classification *was* present in train.\u00a0The showreel plot contains only folds from the A and T columns (magenta and orange). \n\nWe hope that hierarchical purging approaches to evaluation such as this will be more widely used in the future because they allow testing fold generalization more systematically across thousands of domains (rather than only doing temporal purging).\n\nTo conclude, our main claim is that the model is able to (sometimes) produce reasonable (TM>0.5) predictions for these difficult ATH, TH, and H problems created by our purging process and that it does so more accurately than a baseline that predicts angles directly without a folding process.", "title": "To clarify: Training set is hierarchically purged"}}}