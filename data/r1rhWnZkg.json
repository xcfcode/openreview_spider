{"paper": {"title": "Hadamard Product for Low-rank Bilinear Pooling", "authors": ["Jin-Hwa Kim", "Kyoung-Woon On", "Woosang Lim", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "authorids": ["jnhwkim@snu.ac.kr", "kwon@bi.snu.ac.kr", "quasar17@kaist.ac.kr", "jeonghee.kim@navercorp.com", "jungwoo.ha@navercorp.com", "btzhang@bi.snu.ac.kr"], "summary": "A new state-of-the-art on the VQA (real image) dataset using an attention mechanism of low-rank bilinear pooling", "abstract": "Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the VQA dataset, having a better parsimonious property.", "keywords": ["Deep learning", "Supervised Learning", "Multi-modal learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR."}, "review": {"Hy2qZcxPx": {"type": "rebuttal", "replyto": "r1rhWnZkg", "comment": "1. Appendix C is inserted to discuss about \"the explicit comparison with MCB\". We elaborate the previous conversations in the forum.\n2. In Appendix A, the batch size is 100, not 200. Apology for your unnecessary confusion. The Github repository is also updated.\n3. We update Acknowledgments.", "title": "We add a revision on 20th. "}, "H11e9vxwx": {"type": "rebuttal", "replyto": "H1nTvbePg", "comment": "We will update the pdf soon.", "title": "Thanks!"}, "SkDgtDxPx": {"type": "rebuttal", "replyto": "ByaSXnJvx", "comment": "That's our duty!", "title": "Thank you for your consideration."}, "SkoMwDeDg": {"type": "rebuttal", "replyto": "H1vfqPcIx", "comment": "1. A good point for general discussion with regard to parameter reduction. The study of model parsimony in deep learning is especially important for constrained environments. However, we more focus on to set a strong baseline model with parsimony, simple structure, and better performance for further studies making rooms (gpu memory budget, etc.) for improvements, especially for VQA 2.0, not a dead end study.\n\n2. We think that is a rigorous approach to compare with proposed models. But, your suggestion embraces a critical weakness: the comparison using a *test-dev* split, which can be used for hyperparameter tuning, is not technically rigorous enough to compare with. Notice that our model comparison is performed for a *test-standard* split in Table 2 & 3.", "title": "Rely to the comment"}, "SymHkejUl": {"type": "rebuttal", "replyto": "S1Quww58g", "comment": "1. We do not use GloVe technique, because we think that this is an adhoc to improve the accuracy.  Hence, MCB+Att (64.2) is more appropriate to compare with our model. However, question embedding method is different from each other: MCB uses stacked two LSTMs, whereas MLB uses single GRU. 61.48 comes from our version of MCB, which constrains the other conditions, keeping the rest of the model architecture exactly same, upon the request. Though we do not see it as a decisive evidence of better performance, but we think that an optimal architecture may be required for a proposed method.\n\n2. Except the question embedding above (GRU). In multimodal learning, we swapped compact bilinear pooling with low-rank bilinear pooling. Overall architecture of multimodal learning part is the same.", "title": "Answers to questions:"}, "r1EJXz_Lg": {"type": "rebuttal", "replyto": "r1rhWnZkg", "comment": "Authors present an insight into working of MCB and compare this to hashing of weights as described in (Chen et al 2015). Because the two parameters h_x and h_y follow uniform distribution, the probability of each one assigns to i index is 1/d. However, the bilinear terms are uniformly assigned among d slots, which makes the probability of a bilinear term also 1/d. This is approximation of circular convolution. The compact bilinear pooling, as shown in MCB is thus equivalent to hashing in a 2-dim part of the 3-dim tensor with serialization.\n", "title": "Comment on the revision - theoretical analysis of MCB"}, "HySkE1W8g": {"type": "rebuttal", "replyto": "r1rhWnZkg", "comment": "1. Theoretical analysis on multimodal compact bilinear pooling (MCB) is updated in Section 7.1 and Appendix B. In short, multimodal compact bilinear pooling reduces the number of parameters of two-way interactions in a three-dimensional tensor using a hashing function [1]. Each parameter is shared by NM/d terms in expectation, with the variance of NM(d-1)d^2 (N, M: input sizes, d: pooled representation size). The total number of bilinear terms is NM. Since our method is three-way factorization instead of two-way, the parsimony of our model is better than MCB, and our model can afford to allocate more parameters in the two-way interactions.\n2. In Section 5 Experiments, a description is updated to indicate the details of hyperparameters including preprocessing, question & vision embedding in Appendix A.\n3. The effect of joint embedding size d (800 to 1600) is explored in Table 5 (Appendix A).\n4. We publish our source code in Github repository (https://github.com/jnhwkim/MulLowBiVQA), and mention it in Section 5.\n5. In Section 7, a summarization of related works is updated, and point to Appendix C for details.\n6. Correct minor typos and citation reflecting reviewers\u2019 comments.\n\n[1] Chen, W., Wilson, J. T., Tyree, S., Weinberger, K. Q., & Chen, Y. (2015). Compressing Neural Networks with the Hashing Trick. In 32nd International Conference on Machine Learning (pp. 2285\u20132294). ", "title": "Revision Updated!"}, "HyKOdrM4g": {"type": "rebuttal", "replyto": "SJW_QxMVe", "comment": "1. Maybe you misunderstand our recent comment. We found that compact bilinear pooling performs worse, not low-rank bilinear pooling, keeping rest of the model architecture of ours. (We updated the previous comment to avoid confusing). If it affects your rating, please reconsider it.\n2. Major parameter reduction comes from the difference between the last fully-connected layer as described in Section 7.1. Fukui et al. (2016) should use 16,000-dimension in the middle of networks for their method, compact bilinear pooling.\n3. The VQA dataset has 360K questions for training/validation, and 240K for test. Moreover, for test-standard, we should upload our prediction result to the designated evaluation server with up to five submissions (though we used only two submissions for our paper, one for single model, and the other one for ensemble model). The accuracy variance is empirically around 0.05%. Hence, we believe 0.42% gain is a significant improvement. \nNotice that we attribute a relative huge improvement of the ensemble model from Fukui et al. (2016) in Table 3 to both answer sampling (in our experiment, +0.27%) and data augmentation using visual-genome dataset (in Fukui et al. (2016) +0.70%), which is not used in previous methods. We also use both methods in our ensemble model to compare with Fukui et al. (2016).\n4. MRN is from Kim et al. (2016b), MARN is an experimental model which have an attention mechanism in MRN, and MLB, our proposed model, is a simpler model of MARN, which have no shortcut connection.", "title": "To AnonReviewer1:"}, "SJLrqrzNl": {"type": "rebuttal", "replyto": "rJJE_B-Ve", "comment": "For an experiment of the explicit comparison with compact bilinear pooling, we explicitly substitute *compact bilinear pooling* for *low-rank bilinear pooling*. According to Fukui et al. (2016), we use MCB followed by Signed Square Root, L2-Normalization, Dropout(rate=0.1), and Linear projection from 16,000-dimension to the target dimension (one for attention parameters, one for output). Also, Dropout(rate=0.3) for a question embedding vector. Experiment details are from Fukui et al. (2016)\u2019s implementation from Github repository (https://github.com/akirafukui/vqa-mcb). For test-dev split, our version of MCB (Fukui et al., 2016) model get 61.48% for overall accuracy (yes/no : 82.48%, number : 37.06%, other : 49.07%) vs 65.08% (ours, MLB).\n\nSincerely, we're sorry for your inconvenience.", "title": "Correction for the explanation of the explicit comparison!"}, "rJJE_B-Ve": {"type": "rebuttal", "replyto": "r1rhWnZkg", "comment": "Thank your for sharing your time to review, and we humbly appreciate your efforts.\n\nFor all reviewers:\n\nFor question embedding, we use GRU which have been used in the previous VQA models (Malinowski et al., 2016; Noh et al., 2016a; Kim et al., 2016b). However, there was conflicted results; Malinowski et al. (2016) show that LSTM is slightly better than GRU, and Noh et al. (2016b) revert their previous choice of GRU (Noh et al., 2016a) into 2-layer LSTMs (via personal communication). The 2-layer LSTMs can be also found in Fukui et al. (2016). We cautiously argue that the choice of RNN for question embedding is up to overall architectural design, which is unlike to solely contribute to the performance gain.\n\nWe did not use the concatenation of GloVe word embeddings (Fukui et al, 2016). We are trying to be as simple as possible for model design. This is another reason that we did not take an advantage of question embedding techniques.\n\nFor an experiment of the explicit comparison with compact bilinear pooling, we explicitly substitute *compact bilinear pooling* for *low-rank bilinear pooling*. According to Fukui et al. (2016), we use MCB followed by Signed Square Root, L2-Normalization, Dropout(rate=0.1), and Linear projection from 16,000-dimension to the target dimension (one for attention parameters, one for output). Also, Dropout(rate=0.3) for a question embedding vector. Experiment details are from Fukui et al. (2016)\u2019s implementation from Github repository (https://github.com/akirafukui/vqa-mcb). For test-dev split, our version of MCB (Fukui et al., 2016) model get 61.48% for overall accuracy (yes/no : 82.48%, number : 37.06%, other : 49.07%) vs 65.08% (ours, MLB). The accuracy is not competent as Fukui et al. (2016)\u2019s, especially for other-type. We attribute this result to other hyper-parameters, which should be adjusted to accordingly. Note that this is an explicit comparison with compact bilinear pooling using the same architecture to control other factors, though it maybe not satisfactory to justify the claim. Rather, we emphasis on the parsimonious property of MLB, which reduces the total number of parameters around 25%, achieving the new state-of-the-art result with reasonable margin.  (We are doing additional experiments to response to your requests, and to double-check if there is any missing critical hyper-parameter in the explicit comparison experiment.)", "title": "For question embedding and explicit comparison with compact bilinear pooling"}, "rJIPN8Z4e": {"type": "rebuttal", "replyto": "B1t5kal4e", "comment": "Explanation about the question embedding can be found in Appendix A.1.1, and please refer to our recent comment on that. The output dimension (the number of candidate answers) and joint embedding size is fixed according to Kim et al., (2016b). Our preliminary study shows that the choice of the number of joint embedding size is difficult, since the variance of performance is relatively high, but around 1,000 is fairly good. \n\nFor the multiple-glimpse attention mechanism, we get two different attention probability distributions over 14x14 grids using attention mechanism, and concatenating the two different weighted-sum visual features for each distribution, respectively.", "title": "To AnnoReviewer2:"}, "Bk8DxUZNx": {"type": "rebuttal", "replyto": "SkJVg01Qe", "comment": "Thanks!", "title": "Please refer to our recent comment. "}, "BJ7WlLWNl": {"type": "rebuttal", "replyto": "ryw0a4eNx", "comment": "2.1 Please refer to our recent comment.\n2.2 Equivalent full outer product cannot be done even for modern GPUs. Note that Fukui et al. (2016) did an experiment with fairly small dimensions, which shows that their approximation is better for generalization.\n2.4 With the idea of residual learning, element-wise sum can be interpreted as additive complementary learning with two different models. However, in multimodal learning, two complementary models compete against with each other to fit the desired function, not considering interactive characteristics of multimodality.\n5. You're right, we\u2019ll update that.\nMinor. Early version of Antol et al. (2015) didn\u2019t include Lu et al. (2015)\u2019s model. We\u2019ll update the cite. Thanks!\n", "title": "To AnnoReviewer3:"}, "SkJVg01Qe": {"type": "review", "replyto": "r1rhWnZkg", "review": "Dear Authors,\n\nThe paper stresses that low-rank bilinear pooling outperforms compact bilinear pooling. However, the paper does not explicitly compare the performance of low-rank bilinear pooling vs. compact bilinear pooling, keeping the rest of the model architecture exactly same. It would be great if you could provide experimental results for the same model architecture with the above two types of pooling methods.\n\nThanks.Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.\n\nStrengths:\n\n1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. \n\n2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).\n\n3. The various design choices made in model development have been experimentally verified. \n\nWeaknesses/Suggestions:\n\n1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).\n\n2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? \n\n3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.\n\n4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.\n\n5. In the caption for Table 1, fix the following: \u201chave not\u201d -> \u201chave no\u201d \n\nReview Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.\n", "title": "Explicit comparison of low-rank bilinear pooling with compact bilinear pooling", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJW_QxMVe": {"type": "review", "replyto": "r1rhWnZkg", "review": "Dear Authors,\n\nThe paper stresses that low-rank bilinear pooling outperforms compact bilinear pooling. However, the paper does not explicitly compare the performance of low-rank bilinear pooling vs. compact bilinear pooling, keeping the rest of the model architecture exactly same. It would be great if you could provide experimental results for the same model architecture with the above two types of pooling methods.\n\nThanks.Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.\n\nStrengths:\n\n1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. \n\n2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).\n\n3. The various design choices made in model development have been experimentally verified. \n\nWeaknesses/Suggestions:\n\n1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).\n\n2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? \n\n3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.\n\n4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.\n\n5. In the caption for Table 1, fix the following: \u201chave not\u201d -> \u201chave no\u201d \n\nReview Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.\n", "title": "Explicit comparison of low-rank bilinear pooling with compact bilinear pooling", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkcfaTymg": {"type": "rebuttal", "replyto": "rktVJ9kQe", "comment": "Thanks for sharing your time to review. We sincerely appreciate that. \n\n# Contribution 1: \u201cFirst, we propose low-rank bilinear pooling to approximate full bilinear pooling to substitute compact bilinear pooling.\u201d\n\nTo enable bilinear approximation in deep neural networks, we propose low-rank bilinear pooling, which is effectively and efficiently replaceable with compact bilinear pooling (which is used for multimodal learning in Fukui et al., 2016). \n\nThe recent rise of multiplicative methods (Hadamard product) in deep neural networks makes us rethink this computation as an intermediate module of pooling in neural networks. In this paper, we explicitly discuss the role of \u201cHadamard product\u201d in deep neural networks as low-rank bilinear approximation.\n\nWe cited \u201cAntol et al. 2015 for VQA\u201d as Lu et al., 2015 (it\u2019s Github repository) in our paper. Xu et al. (2015) use multilayer perceptron as an attention model, to optimize with variational lower bound on the marginal log-likelihood of word sequence given image features.\n\n# Contribution 2: \u201cSecond, Multimodal Low-rank Bilinear Attention Networks (MLB) having an efficient attention mechanism using low-rank bilinear pooling is proposed for visual question-answering tasks. MLB achieves a new state-of-the-art performance, and has a better parsimonious property.\u201d\n\nYes, there are many attention models for VQA. Though some of them already used Hadamard product for multimodal representation learning, we argue that this advantage is not merely from Hadamard itself, but from the interpretation of low-rank bilinear pooling. This interpretation leads to, technically, appropriate embeddings for each modality and applying nonlinear activation function. With this, we could propose a novel attention model, MLB. We include a detailed comparison with Fukui et al. (2016) about stability of model structure and use of parameters in Section 7.1.\n\nBy the way, Xu et al. (2015) didn\u2019t use Hadamard product in attention model, only in LSTM. If we missed something, please let us know.\n\n* * * * *\n\nMoreover, we include FAQ via personal communications.\n\n# What is different from the element-wise multiplication model of Fukui et al. (2016)?\n\nSince they do not describe the details of the comparative experiment, we don't know why they couldn't get a better result. If they only substitute MCB with element-wise multiplication, appropriate embeddings for both modalities are missed. This condition leads to weak bilinear approximation as our paper noted. Nonlinear activation function for the pooling may get another advantage for the optimization. \n\n# For the nonlinear funtion, why is tanh?\n\nIn the prelimary experiments, tanh was the best. We believe tanh is a robust choice for learning except image classification tasks using CNNs (in that case, ReLU). Also, you can think about activation functions in various RNNs. ", "title": "Re: Clarify contribution"}, "rktVJ9kQe": {"type": "review", "replyto": "r1rhWnZkg", "review": "Hi,\n\n\n\nIt would be great if you could clarify your contribution.\n\nYou list three in the introduction.\n\"First, we propose low-rank bilinear pooling to approximate full bilinear pooling to substitute compact bilinear pooling. Second, Multimodal Low-rank Bilinear Attention Networks (MLB) having an efficient attention mechanism using low-rank bilinear pooling is proposed for visual question-answering tasks. MLB achieves a new state-of-the-art performance, and has a better parsimonious property. Finally, ablation studies to explore alternative choices, e.g. network depth, non-linear functions, and shortcut connections, are conducted.\"\n\nHowever, as the first, element wise multiplication/Hadamard product, you propose as low-rank bilinear pooling has previously widely been used to combine multiple modalities.\nE.g. Antol et al. 2015 for VQA, or Xu et al's  \"show attend and tell\" for image captioning using similar to this work, attention.\n\nThe second is using attention for VQA, which is also a widely used technique, e.g. Fukui et al 2016 and others. Also in combination with attention, hadamard has been used in Xu et al's  \"show attend and tell\".\n\nI appreciate the extensive experimental evaluation, which is basically the third contribution.\n\nThanks.This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). \nThis formulation is evaluated on the visual question answering (VQA) task together with several other model variants.\n\nStrength:\n1.\tThe paper discusses how the Hadamard product can be used to approximate the full outer product.\n2.\tThe paper provides an extensive experimental evaluation of other model aspect for VQA.\n3.\tThe full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge.\n\nWeaknesses:\n1.\tNovelty: The paper presents only a new \u201cinterpretation\u201d of the Hadamard product which has previously been widely used for pooling, including for VQA.\n2.\tExperimental evaluation:\n2.1.\tAn experimental direct comparison with MCB missing. Although the evaluated model is similar to Fukui et al. several other changes have been made, including question encoding (GRU vs. LSTM), normalization (tanh vs. L2 vs. none). The small difference in performance (0.44% om Table 1) could easily be attributed to these differences.\n2.2.\tAn experimental comparison to the full outer product (e.g. for a lower dimension) is missing. It remains unclear how good the proposed approximation for the full outer product is. While a comparison to MCB is presented this seems insufficient as MCB is a very different model.\n2.3.\tOne of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?\n2.4.\tComparison with other pooling strategies, e.g. elementwise sum instead of elementwise product.\n3.\tNo theoretical analysis or properties of the approximation are presented.\n4.\tThe paper seems to be general at the beginning, but the claim of the benefit of the Hadamard product is only shown experimentally on the VQA dataset.\n5.\tRelated work: The comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental.\n\n\nMinor\n-\tIt is not clear why the Lu et al, 2015 is cited rather than the published paper from Antol et al.\n-\tSect 2, first sentence: \u201cevery pairs\u201d -> \u201cevery pair\u201d\n\n\nSummary:\nWhile the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation. It is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the Hadamard product) but of unrelated aspects which are important to achieve high performance in the VQA challenge.\n\nTo be more convincing I would like to see the following experiments \n-\tComparison with Outer product in the identical model\n-\tComparison with MCB in the identical model\n-\tComparison with elementwise sum instead of elementwise product\n-\tOne of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?\n", "title": "Clarify contribution", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryw0a4eNx": {"type": "review", "replyto": "r1rhWnZkg", "review": "Hi,\n\n\n\nIt would be great if you could clarify your contribution.\n\nYou list three in the introduction.\n\"First, we propose low-rank bilinear pooling to approximate full bilinear pooling to substitute compact bilinear pooling. Second, Multimodal Low-rank Bilinear Attention Networks (MLB) having an efficient attention mechanism using low-rank bilinear pooling is proposed for visual question-answering tasks. MLB achieves a new state-of-the-art performance, and has a better parsimonious property. Finally, ablation studies to explore alternative choices, e.g. network depth, non-linear functions, and shortcut connections, are conducted.\"\n\nHowever, as the first, element wise multiplication/Hadamard product, you propose as low-rank bilinear pooling has previously widely been used to combine multiple modalities.\nE.g. Antol et al. 2015 for VQA, or Xu et al's  \"show attend and tell\" for image captioning using similar to this work, attention.\n\nThe second is using attention for VQA, which is also a widely used technique, e.g. Fukui et al 2016 and others. Also in combination with attention, hadamard has been used in Xu et al's  \"show attend and tell\".\n\nI appreciate the extensive experimental evaluation, which is basically the third contribution.\n\nThanks.This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). \nThis formulation is evaluated on the visual question answering (VQA) task together with several other model variants.\n\nStrength:\n1.\tThe paper discusses how the Hadamard product can be used to approximate the full outer product.\n2.\tThe paper provides an extensive experimental evaluation of other model aspect for VQA.\n3.\tThe full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge.\n\nWeaknesses:\n1.\tNovelty: The paper presents only a new \u201cinterpretation\u201d of the Hadamard product which has previously been widely used for pooling, including for VQA.\n2.\tExperimental evaluation:\n2.1.\tAn experimental direct comparison with MCB missing. Although the evaluated model is similar to Fukui et al. several other changes have been made, including question encoding (GRU vs. LSTM), normalization (tanh vs. L2 vs. none). The small difference in performance (0.44% om Table 1) could easily be attributed to these differences.\n2.2.\tAn experimental comparison to the full outer product (e.g. for a lower dimension) is missing. It remains unclear how good the proposed approximation for the full outer product is. While a comparison to MCB is presented this seems insufficient as MCB is a very different model.\n2.3.\tOne of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?\n2.4.\tComparison with other pooling strategies, e.g. elementwise sum instead of elementwise product.\n3.\tNo theoretical analysis or properties of the approximation are presented.\n4.\tThe paper seems to be general at the beginning, but the claim of the benefit of the Hadamard product is only shown experimentally on the VQA dataset.\n5.\tRelated work: The comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental.\n\n\nMinor\n-\tIt is not clear why the Lu et al, 2015 is cited rather than the published paper from Antol et al.\n-\tSect 2, first sentence: \u201cevery pairs\u201d -> \u201cevery pair\u201d\n\n\nSummary:\nWhile the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation. It is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the Hadamard product) but of unrelated aspects which are important to achieve high performance in the VQA challenge.\n\nTo be more convincing I would like to see the following experiments \n-\tComparison with Outer product in the identical model\n-\tComparison with MCB in the identical model\n-\tComparison with elementwise sum instead of elementwise product\n-\tOne of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?\n", "title": "Clarify contribution", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJc504xbx": {"type": "rebuttal", "replyto": "rkjpmGyge", "comment": "*** We leave the part of personal communication in here to share with reviewers. This posting is delayed due to a system error. ***\n\nOur work is based on Pirsiavash et al. (2009), Lu et al., (2015), and Kim et al., (2016b), in a separate way of your work. However, we admit that your work is noticed recently, and as you've pointed out, it may deserve to be more discussed in the main paper. Since your work shed light on other learning aspects of Hadamard product. We updated a revision of our paper in OpenReview and arXiv. We added\n \n   \u201c\u2026 notably, Wu et al. (2016c) studied learning behavior of multiplicative integration in RNNs with discussions and empirical evidences.\u201d \n\nin the end of Section 3.2, along with correcting typos, polishing few explanations, and formatting references. \n\nWith regard to the \u201c2016c\u201d problem, we cite two papers from Qi Wu, that makes the cite of your paper appear as \"Wu et al. (2016c)\u201d to distinguish the sources by LaTex compiler. I wonder if there is a better solution for this unfortunate case.\n\nThank you for giving us an opportunity to improve the paper.", "title": "Your comments are reflected in the revision."}}}