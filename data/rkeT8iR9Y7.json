{"paper": {"title": "Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep Learning", "authors": ["Cheolhyoung Lee", "Kyunghyun Cho", "Wanmo Kang"], "authorids": ["bloodwass@kaist.ac.kr", "kyunghyun.cho@nyu.edu", "wanmo.kang@kaist.edu"], "summary": "One of theoretical issues in deep learning", "abstract": "Although stochastic gradient descent (SGD) is a driving force behind the recent success of deep learning, our understanding of its dynamics in a high-dimensional parameter space is limited. In recent years, some researchers have used the stochasticity of minibatch gradients, or the signal-to-noise ratio, to better characterize the learning dynamics of SGD. Inspired from these work, we here analyze SGD from a geometrical perspective by inspecting the stochasticity of the norms and directions of minibatch gradients. We propose a model of the directional concentration for minibatch gradients through von Mises-Fisher (VMF) distribution, and show that the directional uniformity of minibatch gradients increases over the course of SGD. We empirically verify our result using deep convolutional networks and observe a higher correlation between the gradient stochasticity and the proposed directional uniformity than that against the gradient norm stochasticity, suggesting that the directional statistics of minibatch gradients is a major factor behind SGD.", "keywords": ["directional statistics", "deep learning", "SNR", "gradient stochasticity", "SGD", "stochastic gradient", "von Mises-Fisher", "angle"]}, "meta": {"decision": "Reject", "comment": "The paper presents a careful analysis of SGD by characterizing the stochastic gradient via von Mises-Fisher distributions. While the paper has good quality and clarity, and the authors' detailed response has further clarified several raised issues, some important concerns remain: Reviewer 1 would like to see careful discussions on related observations by other work in the literature, such as low rank Hessians in the over-parameterized regime, Reviewer 2 is concerned about the significance of the presented analysis and observations, and Reviewers 2 and 4 both would like to see how the presented theoretical analysis could be used to design improved algorithms. In the AC's opinion, while solid theoretical analysis of SGD is definitely valuable, it is highly desirable to demonstrate its practical value (considering that it does not provide clearly new insights about the learning dynamics of SGD)."}, "review": {"Byxlpm08yE": {"type": "rebuttal", "replyto": "SJlXBVN71V", "comment": "\u201cA high-d uniform random unit vector will, with high probability, be orthogonal to vectors living in a subspace. The observations made in the paper may not be capturing such sub-space concentration correctly or at least the claims need to be reconciled with existing observations regarding the Hessian being low rank.\u201d\n\nThe increase in the uniformity of minibatch gradients in the original space implies the increasing uniformity in a lower-dimensional subspace. We however agree with the reviewer that it is desirable to study further the impact of the low-rank Hessian in learning a deep neural network. We leave this to the future. \n\n\n\u201cit will be good to consider the over-parameterized setting where the dimensionality d > number of samples n, which is typical in deep learning settings and where the stochastic gradients cannot span the full space\u201d\n\nWe agree with you that this is a more realistic and interesting setting, and thank you for your suggestion. We however would like to note that we assumed f_{I_i}(w) to be strictly convex to have a unique local minibatch solution p_i(w). In the over-parameterized setting where there are many zero eigenvalues in the hessian, we need a way to replace p_i(w) - w with a vector that characterizes the direction from w to a set of minibatch solutions, which is not trivial. We leave this as a future work.", "title": "Response to AnonReviewer1(additional work needed)"}, "BkgdqbSFAm": {"type": "rebuttal", "replyto": "rkeT8iR9Y7", "comment": "Dear all Reviewers,\n\nthanks for your valuable reviews and comments. According to your suggestions, we have made more discussions and experiments and updated the paper. In particular, we added/changed the following contents:\n\n1) We have clarified the contribution of our paper and added a more detailed review of related work in the Introduction.\n2) We have run experiments to see the effect of batch size on \\kappa in Figure 5 in Section 4.1.\n3) We have discussed the effect of adding batch normalization layers and residual connections in Section 4.2. \n4) We have discussed briefly the estimated values of kappa near the end of training in Section 4.2. Details can be found in Supplemental Material G.\n5) Some unclear parts have been re-written and they are better to read now. \n\nWe hope our responses can help address your concerns and questions. \n", "title": "Submission revised."}, "H1lDUA_jaQ": {"type": "rebuttal", "replyto": "SJlhIp_DaQ", "comment": "\u201cThe experiment results show, by the end of training, all models FNN, DENN and CNN have very large value of \\kappa which is around 10^4. This value implies that the mini-batch gradients distribution is pretty concentrated, and it is contradictory to the statement\u201d\n\nIn order to verify our earlier claim that the accuracy of the kappa estimate grows with respect to the dimensionality (the number of parameters of a neural network in our case), we have run some simulations. In these simulations, we vary the dimensionality, the number of samples and the true, underlying kappa by sampling from the vMF distribution with the designated kappa and estimating the kappa from these samples. We uploaded the plot from the simulation with 10,000 dimensions, which you can check from https://ibb.co/dNycc0 (both x- and y-axes are log10.) Unfortunately we could not easily go over 10,000 dimensions due to the difficulty in sampling from the vMF distribution.\n\nAs can be seen from the uploaded plot, the estimated kappa approaches the true kappa from above as the number of samples increases. When the true kappa is large, the estimation error rapidly becomes zero as the number of samples approaches 3,000. When the true kappa is low (i.e., uniform over the angles), however, the gap does not narrow completely even with 3,000 samples. \n\nWhile fixing the true kappa to 0 and the number of samples to 3,000, we vary the dimensionality to empirically investigate the estimated kappa values. We chose to use 3,000 samples to be consistent with our experiments in the paper. We ran five simulations each and report both mean and standard deviation. See below for the estimates:\n\nd=200k, kappa=3,651.06+-5.10\nd=640k, kappa=11,682.88+-3.57\nd=2M,    kappa=36,526.49+-13.05\n\nWe clearly observe the trend of increasing estimated kappas w.r.t. the dimensions. This suggests that we should not compare the absolute estimated kappas across different network architectures due to the differences in the number of parameters. This agrees well with Cutting et al. [2017] which empirically showed that the threshold for rejecting the null hypothesis of the true kappa being 0 grows with respect to the dimensions.\n\nAt the end of training, DFNN+BN has approximately 2M parameters and the minimum estimated kappa we observed was 6.83 x 10^4, FNN+BN has approximately 640k parameters and the minimum estimated kappas was 2.04 x 10^4, and CNN+BN+Res has approximately 200k parameters and the minimum estimated kappas were 1.56 x 10^4. Considering these in the context of the simulation result above, we cannot say that the underlying directional distribution of minibatch gradients in all these cases at the end of training is not close to uniform.\n\nWe again emphasize that our theory focuses more on the relative decrease of kappa (i.e. the relative increase of directional uniformity) rather than the absolute value of the estimated kappa.\n\n- Cutting et al. [2017] Tests of concentration for low-dimensional and high-dimensional directional data\n\n", "title": "Additional response to AnonReviewer1 re estimated kappa values"}, "SylwdgtPpX": {"type": "rebuttal", "replyto": "SJeiEX3YnX", "comment": "We would like to clarify that we are not claiming to have discovered two-phase dynamics of SGD. As you have correctly pointed out, this behaviour has been known, and even more recently there have been a number of work analyzing this behaviour in deep learning. Li & Yuan (2017) investigated this behaviour by considering a shallow neural network with residual connections and assuming the standard normal input distribution and showed that SGD-based learning has two phases; (1) search and (2) convergence phases. Shwartz-Ziv & Tishby (2017) on the other hand investigated a deep neural network with tanh activation functions and showed that SGD-based learning has (1) drift (empirical error minimization, ERM) and (2) diffusion (representation compression) phases. Chee & Toulis (2018) instead looked at the inner product between successive minibatch gradients and presented transient and stationary phases. Our work investigates the dynamics of SGD-based learning in a perspective different from all these recent works by focusing more on geometry and directional analysis of minibatch gradients. In other words, our work provides yet another perspective on understanding the dynamics of SGD-based learning which is at the core of the recent success of deep learning.\n\nFurthermore, we would like to emphasize that our experiments, unlike most of the previous work, are conducted not only with neural networks that confirm well with theoretical assumptions but also with widely used neural networks (including latest techniques such as deep convolutional networks (Krizhevsky et al., 2015), residual networks (He et al., 2016) and batch normalization (Ioffe & Szegedy, 2015). These experiments revealed that our theoretical analysis applies well when all the latest techniques are used or/and when neural networks are shallow, providing some insight into SGD-based learning of generic neural networks.\n\n\u201cThe paper would be stronger if the authors try to turn this insight into something actionable\u201d\n\nWe strongly agree with you that it is desirable to find a practical algorithm based on the theoretical analysis. We however believe this is out of this paper\u2019s scope, and we leave it as future research.\n\n- Li & Yuan (2017) Convergence analysis of two-layer neural networks with ReLU activation\n- Shwartz-Ziv & Tishby (2017) Opening the black box of deep neural networks via information\n- Chee & Toulis (2018) Convergence diagnostics for stochastic gradient descent with constant learning rate.\n- Krizhevsky et al., (2015) ImageNet classification with deep convolutional neural networks.\n- He et al., (2016) Deep residual learning for image recognition\n- Ioffe & Szegedy (2016) Batch normalization: Accelerating deep network training by reducing internal covariate shift.\n", "title": "Response to AnonReviewer2"}, "SJlhIp_DaQ": {"type": "rebuttal", "replyto": "ByxlCvF92m", "comment": "\n\u201c[1] Section3.3  It is not reasonable to use the same p_i for p_i(w_0^0) and p_i(w_1^0) because p_i(w_0^0) -w_1^0 is definitely not parallel to \\hat g_i(w_1^0).\u201d\n\nWe believe the confusion may have arisen from our example in page 6 (together with Fig. 4b). Specifically, we want to clarify that we did not intend to say that \\frac{\\hat g_i(w_0^{i-1}}{\\| \\hat g_i(w_0^{i-1} \\|} can be replaced with \\frac{p_i-w_0^0}{\\| p_i-w_0^0 \\|}. Instead, our intention was to show that \\sum_{i=1}^3 \\frac{\\hat g_i(w_0^{i-1}}{\\| \\hat g_i(w_0^{i-1} \\|} could be replaced by \\sum_{i=1}^3 \\frac{p_i-w_0^0}{\\| p_i-w_0^0 \\|}, when all the sufficient conditions Corollary 3.1. \n\n\u201c[2] Section 3.3: Assumption \\hat g_i(w_t^{i-1}) [\\approx] \\hat g_i(w_t^0) is not convincing. \u201c\n\nThis is not an assumption we need. In fact, what we need is for the norms to be similar, i.e., \\| \\hat{g}_i(w_t^{i-1})\\| \\approx \\| \\hat{g}_i(w_t^0)\\| for proving Corollary 3.1 (see Appendix D for its proof.) \n\nWe state \\hat g_i(w_t^{i-1}) \\approx \\hat g_i(w_t^0) as one simple possible case of having similar norms of these two minibatch gradients. We will clarify this in the next revision.\n\n\u201c[3] Experiment: Batch size should be consistent with the given assumption in the theoretical part\u201d\n\nThe main theoretical analysis in our paper largely depends on the assumption that the norms of minibatch gradients are similar and not that the size of minibatch is large. We discuss a large size of minibatch as one case in which those norms are similar to each other, although there may be other ways for it to happen. \n\nWe choose a reasonable minibatch size to confirm whether and in which setup our theoretical observation can be confirmed in practice. We however thank you for your suggestion and are running experiments while varying minibatch sizes at the moment. We will update the submission with new results soon.\n\n\u201c[4] The CNN experiment; It is better to add a discussion why the \\kappa increases in the early phase of training.\u201d\n\nOur theoretical analysis makes a few assumptions on the loss function which is induced by the choice of a network architecture, such as the well-behavedness of the loss function. We conjecture our observation that the uniformity of minibatch gradients monotonically growing with a deep convolutional network equipped with residual connections and/or batch normalization is due to the fact that the loss function induced from this kind of network confirms well with the assumptions, as were discussed for instance earlier by Li & Yuan (2017) and Santurkar (2018). This is in contrast to deep networks without these latest techniques.\n\n- Li & Yuan (2017) Convergence analysis of two-layer neural networks with ReLU activation\n- Santurkar et al. (2018) How does batch normalization help optimization? (No, it is not about internal covariate shift)\n\n\u201c[5] The experiment results show, by the end of training, all models FNN, DENN and CNN have very large value of \\kappa which is around 10^4.\u201d\n\nWe would like to point out that the absolute value of $\\kappa$ is not a good indicator of the uniformity due to its dependence on the dimensionality, as was investigated earlier by Cutting et al. Instead, our theoretical analysis and experiments focus on the trend of \\kappa over training.\n\n- Cutting et al. [2017] Tests of concentration for low-dimensional and high-dimensional directional data\n\n\u201c[6] The notations in this paper can be improved\u201d\n\nThanks for the suggestion! We will revise the text to make it clearer.\n\n\n", "title": "Response to AnonReviewer1"}, "SJxhAiOPTX": {"type": "rebuttal", "replyto": "S1lAhxfZam", "comment": "\u201cThe paper didn\u2019t provide an answer on how this study can inform people to improve SGD\u201d\n\nWe strongly agree with you and R2 that it is desirable to find a practical algorithm based on the theoretical analysis. We however believe this is out of this paper\u2019s scope, and we leave it as future research.\n\n\u201cCan the authors provide any theoretical or empirical analysis on why the directional uniformity didn\u2019t increase in deep models like CNN and why it increases when BN and Res are applied?\u201d\n\nOur theoretical analysis makes a few assumptions on the loss function which is induced by the choice of a network architecture, such as the well-behavedness of the loss function. We conjecture our observation that the uniformity of minibatch gradients monotonically growing with a deep convolutional network equipped with residual connections and/or batch normalization is due to the fact that the loss function induced from this kind of network confirms well with the assumptions, as were discussed for instance earlier by Li & Yuan (2017) and Santurkar (2018). This is in contrast to deep networks without these latest techniques.\n\n- Li & Yuan (2017) Convergence analysis of two-layer neural networks with ReLU activation\n- Santurkar et al. (2018) How does batch normalization help optimization? (No, it is not about internal covariate shift)\n", "title": "Response to AnonReviewer4"}, "S1lAhxfZam": {"type": "review", "replyto": "rkeT8iR9Y7", "review": "\nGradient stochasticity is used to analyse the learning dynamics of SGD. It consists of two aspects: norm stochasticity and directional stochasticity. Although the norm stochasticity is easy to compute, it vanishes when the batch size increases. Therefore, it can be hard to measure the learning dynamics of SGD. The paper is motivated by measuring the learning dynamics by the directional stochasticity. Directly measuring the directional stochasticity with the ange distribution is hard, so the paper uses vMF distribution to approximate the uniformity measurement. The paper theoretically studies the proposed directional uniformity measurement. In addition, the experiments empirically show the directional uniformity measurement is more coherent with the gradient stochasticity.\n\n1. As I\u2019m not a theory person, I\u2019m not very familiar with the related work on this line. But the analysis on the directional uniformity is interesting and original. So is the vMF approximation.\n2. The theoretical analysis looks comprehensive and intuitive. And the authors did a reasonably good job on the experiments.\n3. This paper provides some insights that warn people to pay attention to the directions of SGD. But the paper didn\u2019t provide an answer on how this study can inform people to improve SGD. It\u2019s true that the directional uniformity increases over training and it is correlated to the gradient. But what could this bring us remains unstudied.\n4. Can the authors provide any theoretical or empirical analysis on why the directional uniformity didn\u2019t increase in deep models like CNN and why it increases when BN and Res are applied?\n", "title": "The theory looks good but how can it be used?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ByxlCvF92m": {"type": "review", "replyto": "rkeT8iR9Y7", "review": "\nQuality and clarity: good.\n\nOriginality and significance: This paper studies the stochasticity\nof the norms and directions of the mini-batch gradients, to\nunderstand SGD dynamics. The contributions of this paper can be\nsummarized as: a) This paper defines gradient norm stochasticity as\nthe ratio of the variance of the stochastic norm to the expectation\nof the stochastic norm. It theoretically and empirically shows that\nthis value is reduced as the batch size increases b) This paper\nempirically finds that the distribution of angles between\nmini-batch gradient and a given uniformly sampled unit vector\nconverges to an asymptotic distribution with mean 90 degrees, which\nimplies a uniform distribution of the mini-batch gradients. c)\t\nThis paper uses von Mises-Fisher Distribution to approximate the\ndistribution of the mini-batch gradients. By theoretically and\nempirically observing that the estimated parameter \\hat \\kappa\ndecreases during training, they claim that the directional\nuniformity of mini-batch gradients increases over SGD training.\n\nThe idea of measuring the uniformity of mini-batch gradients\nthrough VMF distribution seems interesting. But it is unclear how\nthe study of this stochasticity dynamics of SGD can be related to\nthe convergence behavior of SGD for non-convex problems and/or the\ngeneralization performance of SGD.\n\nThere are additional concerns/questions regarding both theoretical\npart and empirical part:\n\n[1] Section3.3: Assumption that p_i(w_0^0) =p_i(w_1^0) = p_i is not\nreasonable when theoretically comparing \\hat \\kappa(w_1^0) and \\hat\n\\kappa(w_0^0). The concentration parameter \\hat \\kappa(w) should be\nestimated by the sum of the normalized mini-batch gradients \"\\hat\ng_i(w)/||\\hat g_i(w)||\" . Instead of using mini-batch gradient,\nthis paper uses the sum of \"p_i-w\" by assuming that \"p_i(w_0^0) -w\"\nis parallel to \"\\hat g_i(w)\", which is ok. However, when comparing\n\\hat \\kappa(w_0^0) and \\hat \\kappa(w_1^0), we say \\hat\n\\kappa(w_0^0) = h(\\sum p_i(w_0^0) - w_0^0) ) and \\hat \\kappa(w_1^0)\n= h(\\sum p_i(w_1^0) - w_1^0) ). It is not reasonable to use the\nsame p_i for p_i(w_0^0) and p_i(w_1^0) because p_i(w_0^0) -w_1^0 is\ndefinitely not parallel to \\hat g_i(w_1^0).\n\n[2] Section 3.3: Assumption \\hat g_i(w_t^{i-1}) \\hat g_i(w_t^0) is\nnot convincing. With this assumption, the paper writes w_1^0 =\nw_0^0 - \\eta\\sum_i \\hat g_i(w_0^{i-1}) = w_0^0 - \\eta\\sum_i \\hat\ng_i(w_0^0) = w_0^0 - \\eta \\sum_i p_i-w_0^0. These equalities are\nnot persuasive. Because, \\sum_i \\hat g_i(w_0^0) is the full\ngradient g(w_0^0) at w_0^0. In other words, these equalities imply\nthat from w_0^0 to w_1^0 (one epoch), SGD is doing a full gradient\ndescent: w_1^0 = w_0^0 -\\eta g(w_0^0), which is not the case in\nreality.\n\n[3] Experiment: Batch size should be consistent with the given\nassumption in the theoretical part. In theoretical part, \\hat\n\\kappa(w_1^0) < \\hat \\kappa(w_0^0) is based on the assumption that\n|\\hat g_i(wt^{i-1}| \\tat for all i, with *large mini-batch size*.\nBut in the experiment, they prove \\hat \\kappa(w_1^0) < \\hat\n\\kappa(w_0^0) by using small-batch size which is 64. The authors\nshould either provide experiments with large batch size or try to\navoid the assumption of large batch size in theoretical part.\n\n[4] The CNN experiment; It is better to add a discussion why the\n\\kappa increases in the early phase of training.\n\n[5] The experiment results show, by the end of training, all models\nFNN, DENN and CNN have very large value of \\kappa which is around\n10^4. This value implies that the mini-batch gradients distribution\nis pretty concentrated, and it is contradictory to the statement in\nthe introduction which is \"SGD converges or terminates when either\nthe norm of the minibatch gradient vanishes to zeros, or when the\nangles of the mini-batch gradients are uniformly distributed and\ntheir non-zero norms are close to each other''. It is also\ncontradictory to the experiment in 3.2 which implies the mini-batch\ngradient are uniformly distributed after training.\n\n[6] The notations in this paper can be improved, some notations are\nusing \"i\" for batch index, some notations are using \"i\" for one\ndata sample. Some notations in Section 3.3 and 3.1 can be moved to\nSection 2 Preliminaries. It will be clearer to define all the\nnotations in one place.\n\nTypos: -Section 3.1: first paragraph, E\\hat g(w) -> E[\\hat g(w)]; -\nParagraph before Lemma2: \\hat \\kappa increases -> \\hat \\kappa\ndecreases; - Paragraph after Theorem2: double the directions in \"If\nSGD iterations indeed drive the directions the directions of\nminibatch gradients to be uniform\".", "title": "Interesting idea but implications, significance, theoretical analysis and experiments need improvements.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJeiEX3YnX": {"type": "review", "replyto": "rkeT8iR9Y7", "review": "Summary: This work provides an analysis of the directional distribution of of stochastic gradients in SGD. The basic claim is that the distribution, when modeled as a von Mises-Fisher distribution, becomes more uniform as training progresses. There is experimental verification of this claim, and some results suggesting that the SNR is more correlated with their measure of uniformity than with the norm of the gradients.\n\nQuality: The proofs appear correct to me. \n\nClarity: The paper is generally easy to read.\n\nOriginality & Significance: I don't know of this specific analysis existing in the literature, so in that sense it may be original. Nonetheless, I think there are serious issues with the significance. The idea that there are two phases of optimization is not particularly new (see for example Bertsekas 2015) and the paper's claim that uniformity of direction increases as SGD convergence is easy to see in a simple example. Consider f_i(x) = |x-b_i|^2  quadratics with different centers. Clearly the minimum will be the centroid. Outside of a ball of certain radius from the centroid all of the gradients grad f_i point in the same direction, closer to the minimum they will point towards their respective centers. It is pretty clear, then that uniformity goes up as convergence proceeds, depending on the arrangement of the centers.\n\nThe analysis in the paper is clearly more general and meaningful than the toy example, but I am not seeing what the take-home is other than the insight generated by the toy example. The paper would be improved by clarifying how this analysis provides additional insight, providing more analysis on the norm SNR vs uniformity experiment at the end. \n\nPros:\n- SGD is a central algorithm and further analysis laying out its properties is important\n- Thorough experiments.\n\nCons:\n- It is not entirely clear what the contribution is.\n\nSpecific comments:\n- The comment at the top of page 4 about the convergence of the minibatch gradients is a bit strange. This could also be seen as the reason that analysis of the convergence of SGD rely on annealed step sizes. Without annealing step-sizes, it's fairly clear that SGD will converge to a kind of stochastic process.\n\n- The paper would be stronger if the authors try to turn this insight into something actionable, either by providing a theoretical result that gives guidance or some practical algorithmic suggestions that exploit it.\n\nDimitri P. Bertsekas. Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey. ArXiv 2015.", "title": "Contribution not entirely clear", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}