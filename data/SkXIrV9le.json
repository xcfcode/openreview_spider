{"paper": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"], "summary": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "keywords": ["Structured prediction", "Unsupervised Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The strengths and weaknesses pointed out by the reviews were:\n \n Strengths\n Interesting methodology to achieve differentiable translation of an image (R1, R2)\n Graphics-motivation is unique, inspiring (R1)\n \n Weaknesses\n Experiments not clear (R2)\n Novelty is not enough to justify synthetic-only experiments (R1,R2)\n Missing important citations (R3)\n Lacking details, concern with reproducibility (R3)\n \n The authors acknowledged the reviews with a short sentence but did not provide any feedback or revisions.\n \n The AC and PC agree with the reviewers that the paper presents interesting preliminary work which is more suitable for a workshop in its current form."}, "review": {"HkKB6SpOl": {"type": "rebuttal", "replyto": "S16zhMLux", "comment": "We appreciate the reviews and suggestions. We will submit the paper to the Workshop Track.", "title": "Thanks for the reviews."}, "rkjGbyorx": {"type": "rebuttal", "replyto": "HJUQPFZNl", "comment": "Thanks for your review. We will use your suggestions to improve our work.", "title": "reply"}, "HJ2W-1irg": {"type": "rebuttal", "replyto": "rJyMMFbNx", "comment": "Thanks for your review. We will use your suggestions to improve our work.", "title": "reply"}, "S1Tg-yjrg": {"type": "rebuttal", "replyto": "ByfxSqbNx", "comment": "Thanks for your review. We will use your suggestions to improve our work.", "title": "reply"}, "ryRkW1jHg": {"type": "rebuttal", "replyto": "SydvmezVe", "comment": "Thanks for your review. We will use your suggestions to improve our work.", "title": "reply"}, "rJyMMFbNx": {"type": "review", "replyto": "SkXIrV9le", "review": "1. Why does Algorithm 1 include both the delta-convolution and an STN? It seems that one would choose one or the other (Section 2) to implement a translation operator, not use both.\n\n2. In the experiments, it seems that the Convolutional-PUN uses an LSTM, while the STN-PUN uses an RNN. If so, this gives the Convolutional-PUN an unfair advantage. Why wasn't the same base RNN architecture used for both? If both use LSTMs, which is reasonable, the text should be clear that this is the case.\n\n3. Drawing conclusions from results that are not in the main text or even appendix (!) is very sloppy. In one instance (paragraph right before section 4.2) it is claimed that they can be obtained from the source code. In another (third paragraph of section 4.2) it is claimed that the proposed method generates high likelihood frames for longer than the baseline, but this is not shown or quantified anywhere. Can the authors comment on why this was not included even as appendix material?\nThis paper proposes a generative model of videos composed of a background and a set of 2D objects (sprites). Optimization is performed under a VAE framework.\n\nThe authors' proposal of an outer product of softmaxed vectors (resulting in a 2D map that is delta-like), composed with a convolution, is a very interesting way to achieve translation of an image with differentiable parameters. It seems to be an attractive alternative to more complicated differentiable resamplers (such as those used by STNs) when only translation is needed.\n\nBelow I have made some comments regarding parts of the text, especially the experiments, that are not clear. The experimental section in particular seems rushed, with some results only alluded to but not given, not even in the appendix.\n\nFor an extremely novel and exotic proposal, showing only synthetic experiments could be excused. However, though there is some novelty in the method, it is disappointing that there isn't even an attempt at trying to tackle a problem with real data.\n\nI suggest as an example aerial videos (such as those taken from drone platforms), since the planar assumption that the authors make would most probably hold in that case.\n\nI also suggest that the authors do another pass at proof-reading the paper. There are missing references (\"Fig. ??\"), unfinished sentences (caption of Fig. 5), and the aforementioned issues with the experimental exposition.", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SydvmezVe": {"type": "review", "replyto": "SkXIrV9le", "review": "1. Why does Algorithm 1 include both the delta-convolution and an STN? It seems that one would choose one or the other (Section 2) to implement a translation operator, not use both.\n\n2. In the experiments, it seems that the Convolutional-PUN uses an LSTM, while the STN-PUN uses an RNN. If so, this gives the Convolutional-PUN an unfair advantage. Why wasn't the same base RNN architecture used for both? If both use LSTMs, which is reasonable, the text should be clear that this is the case.\n\n3. Drawing conclusions from results that are not in the main text or even appendix (!) is very sloppy. In one instance (paragraph right before section 4.2) it is claimed that they can be obtained from the source code. In another (third paragraph of section 4.2) it is claimed that the proposed method generates high likelihood frames for longer than the baseline, but this is not shown or quantified anywhere. Can the authors comment on why this was not included even as appendix material?\nThis paper proposes a generative model of videos composed of a background and a set of 2D objects (sprites). Optimization is performed under a VAE framework.\n\nThe authors' proposal of an outer product of softmaxed vectors (resulting in a 2D map that is delta-like), composed with a convolution, is a very interesting way to achieve translation of an image with differentiable parameters. It seems to be an attractive alternative to more complicated differentiable resamplers (such as those used by STNs) when only translation is needed.\n\nBelow I have made some comments regarding parts of the text, especially the experiments, that are not clear. The experimental section in particular seems rushed, with some results only alluded to but not given, not even in the appendix.\n\nFor an extremely novel and exotic proposal, showing only synthetic experiments could be excused. However, though there is some novelty in the method, it is disappointing that there isn't even an attempt at trying to tackle a problem with real data.\n\nI suggest as an example aerial videos (such as those taken from drone platforms), since the planar assumption that the authors make would most probably hold in that case.\n\nI also suggest that the authors do another pass at proof-reading the paper. There are missing references (\"Fig. ??\"), unfinished sentences (caption of Fig. 5), and the aforementioned issues with the experimental exposition.", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1T97lP7l": {"type": "rebuttal", "replyto": "HJNRU4Mme", "comment": "Dear reviewer, thanks for your comments. Please, see the answers below:\n\n1) Its not diverging, this is the test set. STN based model overfit much faster and didn't generalize as well as the other models we tested. The only model we could get to generate videos for several time steps without getting blurred was the conv PUN.\n\n2) Note that the conv PUN models uses a single convolution per step, a convolutional LSTM would require many more, making the computational and memory requirements very hard to compare. That being said, we already investigating a scalable version of PUN that uses conv LSTMs, conv LSTM + PUN compositions, etc. The experiments in the present paper focused on modular tests and understanding of the pieces. Extensive hyperparameter search should follow in future work.", "title": "RE: divergence in figure 4"}, "HJNRU4Mme": {"type": "review", "replyto": "SkXIrV9le", "review": "Is there a reason why your stn based model seems to diverge? (fig. 4) What do the training set curves look like? Also, have you considered a LSTM baseline that uses spatial structure like your convolutional pun model? It would help disentangle whether your pun model is helping more or just using spatial information via a convolution is good enough. This paper presents an approach to modeling videos based on a decomposition into a background + 2d sprites with a latent hidden state. The exposition is OK, and I think the approach is sensible, but the main issue with this paper is that it is lacking experiments on non-synthetic datasets. As such, while I find the graphics inspired questions the paper is investigating interesting, I don't think it is clear that this work introduces useful machinery for modeling more general videos.\n\nI think this paper is more appropriate as a workshop contribution in its current form.", "title": "divergence in figure 4", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByfxSqbNx": {"type": "review", "replyto": "SkXIrV9le", "review": "Is there a reason why your stn based model seems to diverge? (fig. 4) What do the training set curves look like? Also, have you considered a LSTM baseline that uses spatial structure like your convolutional pun model? It would help disentangle whether your pun model is helping more or just using spatial information via a convolution is good enough. This paper presents an approach to modeling videos based on a decomposition into a background + 2d sprites with a latent hidden state. The exposition is OK, and I think the approach is sensible, but the main issue with this paper is that it is lacking experiments on non-synthetic datasets. As such, while I find the graphics inspired questions the paper is investigating interesting, I don't think it is clear that this work introduces useful machinery for modeling more general videos.\n\nI think this paper is more appropriate as a workshop contribution in its current form.", "title": "divergence in figure 4", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}