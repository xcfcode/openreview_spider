{"paper": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "summary": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"]}, "meta": {"decision": "Reject", "comment": "This paper suggests that adversarial vulnerability scales with the dimension of the input of neural networks, and support this hypothesis theoretically and experimentally. \n\nThe work is well-written, and all of the reviewers appreciated the easy-to-read and clear nature of the theoretical results, including the assumptions and limitations. (The AC did not consider the criticisms raised by Reviewer 3 justified. The norm-bound perturbations considered here are a sufficiently interesting unsolved problem in the community and a clear prerequisite to solving the broader network robustness problem.) \n\nHowever, many of the reviewers also agreed that the theoretical assumptions - and, in particular, the random initialization of the weights - greatly oversimplify the problem. Reviewers point out that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results, and also does not properly address the issue of gradient masking. "}, "review": {"rJeMUYhzDV": {"type": "rebuttal", "replyto": "SJg52-sfDN", "comment": "Thank you for your comment. Perturbation scaling has been taking into account in our analysis. See paragraph \"Calibrating the threshold \\epsilon to the attack-norm.\" in Sec.2 and see Appendix D.\n\nThe idea is essentially equivalent to what you explain: keep the signal to noise ratio (i.e. perturbation norm over image norm) constant across dimensions. In l-infinity norm, this leads to a constant perturbation threshold \\epsilon_\\infty; in l2-norm, this leads to a threshold epsilon_2 proportional to \\sqrt d; and in lp-norm proportional to d^{1/p}. That is why the x-axis of Figs 1a-c are scaled by d^{1/p}.\n\nNote that our main theorem shows that adversarial damage increases like sqrt d, but only after adjusting the perturbation threshold. \nMore precisely, it shows that |dL(x)|_q scales like d^{1/q - 1/2}. So, with perturbation threshold epsilon_p of lp-attacks scaling like d^{1/p}, we get\n\n\tadv. dam. := max_{|\\delta|_p \\leq \\epsilon_p} L(x+\\delta) - L(x)\n\t\t  \\approx \\epsilon_p |dL(x)|_q\n\t\t  = \\sqrt d,\n\nwhere 1/p + 1/q = 1. \n\n(Note that using p=infty and q=1 has the \"advantage\" to yield a gradient norm |dL|_1 that is proportional to adversarial damage.)", "title": "Dimensional scaling is accounted for"}, "SJg9Wd9j1N": {"type": "rebuttal", "replyto": "H1MzKs05F7", "comment": "We thank all reviewers for their comments and implication in the review process and seize this opportunity for a final recap on our paper and its contributions.\n\nFirst, our work re-emphasises the strong link between small adversarial perturbations and gradient norms. Although this link was known, many still believe that simple gradients cannot explain f. ex. the vulnerability to iterative methods. Our experiment on Fig.2 with PGD trained networks clearly suggests otherwise.\n\nSecond, and more importantly, we are first to point out the prior vulnerability of classical networks, its exact scaling with the input dimension, and its relative independence of the architecture and dataset. We are also first to precisely measure and confirm this dimension-dependence on trained networks, using usual training and real-world, classical datasets. \n\nOur results however do not precisely study *how* prior-vulnerability acts on post usual (robust) training vulnerability. Nor do they prove that no robust training method will ever be able to robustify our current networks. It is not their current ambition. We will clarify this in the final version and rephrase possible over-statements adequately. However, the current results do *suggest* that architectures with less vulnerable priors might be easier to robustify; so much so, that we are now mainly being reproached not to have dug further in these directions. \n\nWe agree that these are thrilling questions. But since the paper is already long, and its afore-mentioned contributions already novel and non-trivial, we see the present controversy about its implications rather as a pro than a cons for publication. The debate attests that our results do indeed yield non-trivial new questions to be answered.\n\nWe therefore call upon all reviewers and the AC, for the final decision, to focus less on what hasn't been done, and more on our actual contributions and the new perspectives that they open for future research.", "title": "Overall summary and clarification of our paper's current primary goal"}, "rkxEtwcsyN": {"type": "rebuttal", "replyto": "Hyl2CLpMyV", "comment": "We thank your for your comment and accurate analysis, how some formulations may have mislead the interpretation of our paper. We will reformulate any possible over-statements adequately for the final version.\n\nWe agree that the new experiments suggest that the prior- and post-training vulnerability are related. However, let us stress that proving this relation was not the ambition of this paper. Our primary goal is simply to point out that a/ the prior vulnerability exists, scales like \\sqrt(d) and is independent of the net-architecture and the dataset; and b/ that the same dimension dependence can be observed after usual training. Our title does not lie on these observations: our empirical experiments do support that vulnerability increases with usual training, which *is* the standard/common/default training method. But, if it helps, we are willing to change our title to \"*Prior* adv. vuln. of n.n. increases with input dim.\".\n\nMore generally, since the paper is already long, and a/&b/ already novel and non-trivial results/observations, we see the present controversy about their implications rather as pro than a cons for publication. The debate attests that our results indeed yield non-trivial new questions for the community to answer.\n\nWe will be happy to include this clarification in our final version.", "title": "Thanks for your comment + clarification of our primary goal"}, "HklcFfmhR7": {"type": "rebuttal", "replyto": "H1MzKs05F7", "comment": "We thank the reviewers for their stimulating comments that motivated the changes below, which strengthen the paper. We hope that this new version fully addresses the reviewers' concerns by highlighting more clearly the value and limitations of our results on prior vulnerability.\n\n(1st update)\n\nOn Fig. 2, we added PGD training with random starts. The new curves corroborate the validity of our first-order analysis. Overall conclusions remain unchanged.\n\n(Latest update)\n\nWe added a paragraph (Sec 5.1) discussing the implications of our results on prior vulnerability. It relies on a new figure (Fig. 4) that reveals an unmistakable discrepancy between the gradient-norm evolution on the training and test sets respectively. This suggests that nets tend to recover their prior gradient properties outside the training sample, which in turn calls for new architectures with smaller prior gradients and strengthens the importance of our analysis of current priors.\n\nThis new figure required to re-do the experiments on the dimension-dependence of vulnerability and gradients (Sec. 4.2), which therefore got updated as well (Fig. 3). To accelerate training, we used upsampled CIFAR-10 datasets, and pushed the previous results on downsampled ImageNet subsets to Appendix E. Conclusions are unchanged.\n\nThe related literature section (\u00a7 On network vulnerability) now mentions reference [1] (pointed out by AnonReviewer3), which links small worst-case perturbations to larger average perturbations.\n\nFinally, we rephrased our conclusion to reflect more accurately the implications and limitations of our results.\n\n[1] Robustness of classifiers: From adversarial to random noise, Fawzi et al., NIPS, 2016", "title": "Summary of paper updates"}, "H1xnecPrC7": {"type": "rebuttal", "replyto": "rklddGv7A7", "comment": "Thank you for your quick reactions.\n\nWe understand that you have different intuitions and preferences than us about this problem. But your own reference [1] suggests that both views (vulnerability to small worst-case perturbations as opposed to larger average perturbations) are essentially equivalent. We will be happy to mention that.\n\nMore generally, do you believe that you should oppose the publication of our paper because you would have written a different one?  In our opinion, a diversity of viewpoints is a good thing.", "title": "Answer regarding adversarial terminology"}, "S1eK78EXR7": {"type": "rebuttal", "replyto": "B1lARktMRm", "comment": "Thank you for your quick acknowledgment of our changes and your new comments.\n\nConcerning:\n\"The authors raise an additional point in their reply, namely that robust models do not generalize to the test set because the gradients are not small for test examples (despite being small for training examples). While I find this possibility intriguing, I don't see it being supported experimentally yet.\"\n\nMany thanks for this question. During the last days, we have run some experiments to assay exactly this point.\n\nWe essentially re-ran the CIFAR-10 experiments (Figs 1&2) with pgd training and tracked the evolution (over training epochs) of the average gradient-norm on training and test sets respectively. The (preliminary) results are unmistakable: there is a clear discrepancy between the gradient-norms on train and test sets. On the training set, they first increase a bit, but then neatly decrease during training; on the test set however, they constantly increase.\nWe also ran these experiments with up-sampled CIFAR-10 images (by copying pixels). The higher the resolution, the bigger the discrepancy between training and test set norms.\nThese observations perfectly fit those of [2] (SOTA robust training reduces adversarial vulnerability on test but not on training set) and the idea that a significant amount of adversarial vulnerability can be explained by large gradients.\n\nWe will add those experiments as soon as they are finished and double-checked (i.e. probably only in the final version, after the decision, since we have no material time to do this well in the coming days). We appreciate the discussion and believe it will substantially strengthen the paper. The discussion (both in content and level) illustrates that this topic is of major interest to the community, and we are confident that the final paper will satisfy your (and our) standards of mature science.\n\n[2] Adversarially Robust Generalization Requires More Data, Schmidt et al., 2018", "title": "Thank you for your comments & new preliminary experiments to strengthen our claim"}, "rklD0Zcn67": {"type": "review", "replyto": "H1MzKs05F7", "review": "The paper studies how the vulnerability of a neural network model depends on its input dimension. The authors prove that for an *untrained* model, randomly initialized with Xavier initialization, the gradient of the loss wrt the input is essentially independent of the architecture and task. This implies that the major factor affecting the norm of that gradient is the input dimension. They then support their argument by experiments measuring the relation between adversarial vulnerability and gradient norm using various *trained* models (including adversarially regularized ones).\n\nI find the main theoretical result interesting. While this is a known fact for the simple case of linear classifiers, extending it to arbitrarily deep networks is a valuable contribution. The proof crucially relies on properties of the specific initialization scheme to show that the gradient does not change too much during backproparagation through the layers. The most significant limitation of the result (which the authors kindly acknowledge) is that this result only holds at initialization. Hence it cannot distinguish between different training methods or between how different architectures evolve during training. Since the situation in adversarial robustness is much more nuanced, I am skeptical about the significance of such statements.\n\nOn the experimental side, the finding that gradient regularization improves adversarial robustness to small epsilon values has been made multiple times in the past (as the authors cite in the related work section). It is worth noting that the epsilon considered is 0.005 in L_inf (1.275/255) which is pretty small. This value corresponds to the \"small-epsilon regime\" where the behavior of the model is fairly linear around the original inputs and thus defenses such as FGSM-training and gradient regularization are effective.\n\nThe authors also perform an interesting experiment where they train models on downsampled ImageNet datasets and find that indeed larger input dimension leads to more vulnerable models.\n\nWhile I find the results interesting, I do not see clear implications. The fact that the vulnerability of a classifier depends on the L1 norm of the input gradient is already known for any locally linear classifier (i.e. deep models too), and it is fairly clear that the L1 norm will have a dimension dependence. The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion. Given that the experimental results are also not particularly novel, I recommend rejection.\n\n[UPDATE]: Given the overall discussion and paper updates, I consider the current version of the paper (marginally) crossing the ICLR bar. I update my score from a 5 to a 6.\n\nMinor comments to the authors:\n-- I think || x ||_* is more clear than |||x||| for the dual norm.\n-- Consider using lambda for the regularization, epsilon is confusing since it is overloaded.", "title": "An interesting approach", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1lSrptgAm": {"type": "rebuttal", "replyto": "BJgK8oPUh7", "comment": "We thank you for your time, review, comments and concerns, which we hope to address in full.\n\n- Concerning: \"This analysis only seems to work for 'well-behaved' models. For models ... apply\"\n\nIndeed, we only analyse differentiable models. First, note that our results already cover many usual networks (not just a small subset). Second, we think that understanding such well-behaved models is a first step towards understanding non-differentiable ones. (For example, some non-differentiable functions can be considered differentiable at a rougher scale. But this opens a whole new research direction, while the text is long enough...)\n\n- Concerning: \"the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.\"\n\nNot all adversarial vulnerability might be first-order, but first-order vulnerability *is* an aspect of vulnerability (not an oversimplification of a problem). If there is first-order vulnerability, then there is vulnerability. Moreover, our results actually suggest that first-order vulnerability and its relation to gradients explains an *essential* part of vulnerability (see Fig 2d, and paragraph \"Validity of first order expansion\").\n\n- Concerning: \"the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones\" & \"It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models\"\n\nWe added experiments with PGD training on CIFAR-10 (see Fig 2 & 6). Our conclusions stay unchanged. The new experiments support our claim that first-order vulnerability plays an essential role.\n\n- Concerning: \"What matters the most is 'why the strongest model is still not robust?' not 'why some weak models are not robust?'\"\n\nWe think that understanding the vulnerability of \"weak\" models (i.e. at initialization or with usual training) may help understanding the vulnerability of SOTA-robustly trained nets. See our post on \"why prior vulnerability matters\".\n\n- Concerning our last sentence:\n\nWe can reformulate it to:\n\"Nevertheless, they show that at least this type of first-order vulnerability is present, common, and firmly rooted *in the priors* of our current network architectures. In future, we may hence want to complement our robust regularisation techniques by new architectures (or architectural building blocks) with less vulnerable priors.\"\n(Anything in that direction would do. We are open to propositions.)", "title": "Specific answers to your comments & concerns"}, "SkgnHhtlAQ": {"type": "rebuttal", "replyto": "H1gaUtVATX", "comment": "We thank you for your expanded reviews, comments, questions and references, which we hope to address in full.\n\n-Concerning: \"The experiments for adversarially trained models in [1] directly contradict the title of this paper\"\n\nSee point 2/ of our overall reply.\n\n- Concerning: \"for some settings of the weights you can show a bound such as is discussed in the paper, but there are other settings (perhaps even initializations) of the weights for which the conclusion will not hold.\"\n\nThe current initialization-methods are used to avoid exploding/vanishing activations at init. Any other initialization would need to solve that issue. See point 3/ of our overall reply.\n\n- Concerning: \"fixing the initialization seems unlikely to buy us much more than what adversarial training achieves, and the experiments in [1] suggest to me the conclusion of this work is limited in scope\"\n\nPlease refer to our overall thread on \"why prior vulnerability matters\" and how it might help understanding and harnessing the vulnerability of (robustly) trained networks.\n\n- Concerning: \"we hit a limit as we increase the epsilon considered for the perturbations\"\n\nEven for small epsilons, our networks are surprisingly vulnerable. If we don't understand the small epsilon vulnerability, then we won't understand big epsilons.\n\n- Concerning: \"it\u2019s not clear to me what actionable insights we can conclude from this work, and how this can be used to improve upon the current SOTA.\"\n\nAgain, please see our thread on \"why prior vulnerability matters\"  and how it may help understanding and harnessing the vulnerability of (robustly) trained networks.\n\n- Concerning: \"it was found that adversarial training eventually gives robustness to the training set, but this robustness does not generalize to the test set [2]... the data distribution.\"\n\nSee 4/ in our overall reply.\n\n- Concerning: \"in [2] it was shown that there is no learning algorithm [that] can become robust to small perturbations, unless that model is trained on significantly more data.\"\n\nPlease refer to our thread on \"why prior vulnerability matters\". As we explain there, to get better generalisation you can either increase your amount of training data, or decrease the complexity of your model, i.e. choose better (non-vulnerable!) priors.\n\n- Concerning assumption H1 and reference [3]:\n\nthe mean field approach of [3] relies on very strong independence approximations, namely, neglecting individual effects and replacing them with overall averaged effects with similar statistics. This amounts to disregarding most correlations. We do believe a mean-field treatment of our approach is possible, but in the end, the mean field approximations are much stronger than our assumption H1, though similar in spirit.\n\n[1] Are adversarial examples inevitable?, 2018\n[3] Deep Information Propagation, Schoenholz et al., 2017", "title": "Specific answers to your comments & concerns"}, "SJxgihtxRm": {"type": "rebuttal", "replyto": "H1eawIzc3X", "comment": "We thank you for your review and your very positive evaluation.\n\nConcerning Fig 4 in Appendix A:\nAppendix A is preliminary work, whose goal is essentially to illustrate how our insights on the prior-vulnerability of neural networks can help us design more robust networks; in this case, by preferring average-poolings over other pooling-operations. But we agree that this section only contains preliminary results, which is why it is in appendix, not main text.\n\nConcerning: \"could the authors comment on possible defences to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)?\"\nPlease refer to point 3/ of our overall reply, which explains what problems arise if we just change the overall weight-size at init.\n\nWe hope that this addresses your small concerns/questions and thank you, once again, for your evaluation.", "title": "Specific answers to your few questions"}, "HJemastx07": {"type": "rebuttal", "replyto": "rklD0Zcn67", "comment": "We thank you for your careful review, and for pointing out that many people do indeed care about small worst-case l_p-perturbations, and why.\n\n- Concerning: \"I am skeptical about the significance of such statements [at initialization].\" & \"While I find the results interesting, I do not see clear implications.\"\n\nPlease refer to our overall thread on, why understanding the vulnerability of priors helps understanding post (robust) training vulnerability. See also point 4/ in our overall reply.\n\n- Concerning: \"While this is a known fact for the simple case of linear classifiers...\"\n\nEven with only linear classifiers, previous published work has predicted a linear increase of vulnerability with input-dimension rather than sqrt(d), because they did not take the dimension-dependance of the weights into account.\n\n- Concerning: \"it is fairly clear that the L1 norm will have a dimension dependence\"\n\nMaybe, but it is all about getting the numbers right. Our predictions correspond to the *exact* increase-rate measured in practice.\n\n- Concerning: \"The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion.\"\n\nThis independence on architecture at initialization shows that, if we want to get non-vulnerable priors, we need to re-think our initialization scheme and/or introduce a new architectural building block. As to why we would want non-vulnerable priors, again, please see our overall thread on the subject.\n\nOnce again, we thank you for your review and hope that our answers may help you to re-evaluate the significance of our results.", "title": "Some specific answers to your comments"}, "rkgD7oKgCm": {"type": "rebuttal", "replyto": "rkxRoqKxRm", "comment": "The reviewers acknowledge that our results --in particular our theorems and their empirical verification in Fig 3-- are novel and interesting. But some of them wonder how understanding the vulnerability of networks at initialization or after usual training helps understanding their vulnerability after (SOTA-) robust training. We try to answer these concerns with the following two points:\n\na/ Why understanding prior-vulnerability of neural networks may partly explain their vulnerability after robust training.\n\nWithout the right priors, even the best learning algorithms fail.\nFor example, if instead of using carefully designed CNNs, we used fully connected (FC) nets for image classification, we'd get much worse accuracies; especially if our FC net had the same amount of neurons than the CNN (although, theoretically, it could learn the same classifier than the CNN). The situation for adversarial vulnerability is the same: of course, smart training algorithms can help to improve robustness. And the fact that the classifiers get more robust shows that they do help up to some extent. But very ill-behaved priors (i.e. naturally vulnerable priors) will certainly harden their task.\n\nThe example pointed out by AnonReviewer3, about recent work [2] showing that adversarial training is efficient on the training set but usually fails to generalise to the test set, may actually be evidence in our favour. Think about FC nets again: they typically get 100% accuracy on the training set, but completely fail to generalise to the test set. The fact that this (essentially) does not happen to CNNs --despite using the same training algorithm!-- shows the importance of choosing well-behaved priors on our architectures/classifiers. Of course, one can argue, as in [2], that with more data, this would not happen, and hence, that adversarial vulnerability is essentially a sample-size problem. But an alternative way to get robust classifiers with the same amount of data might be to use better priors, i.e. more carefully designed architectures. Which brings us to our second point. \n\nb/ Our paper essentially shows that our priors are already adversarially vulnerable and usual training does not escape these priors. But how can we use those insights? What \"actionable\" insight does it give us?\n\nFor one thing, as argued in a/, pointing out the vulnerability of our priors gives the community a clear reason to search for better priors, so as to complement our robustification algorithms. But they give even more: our theorems, with their clear assumptions and proofs, may actually guide this search. We may for example ask: how can I design an architecture/block that escapes our assumptions? Or: what essential parts of the proofs actually \"generate vulnerability\"? Appendix B goes in that direction: noticing that overly large weights are an essential reason for vulnerability, it analyses what happens if we introduce average-pooling layers (that have weights of size 1/d rather than 1/sqrt(d)). This section is still preliminary (which is why it is in appendix), but it illustrates how our results, despite being only on priors, can yield concrete, actionable results to improve adversarial robustness.\n\n[2] Adversarially Robust Generalization Requires More Data, Schmidt et al., 2018\n", "title": "Why prior vulnerability matters (compendium to the overall reply)"}, "rkxRoqKxRm": {"type": "rebuttal", "replyto": "H1MzKs05F7", "comment": "First, we would like to thank all the reviewers and contributors to the discussions. We are happy to see that our study raises so many questions.  We are in fact a bit surprised to be caught in such a storm, especially since nobody seems to disagree about the facts at a technical level...\n\nWe do believe our paper offers the most precise study of the effects of first-order adversarial perturbations; especially, proving independence from network structure, and re-emphasizing the direct relationship with input dimension. The limitations are clearly stated in the text. We do maintain there is a connection with adversarial attacks: if there is a vulnerability at first order, then there is a vulnerability. And we thank AnonReviewer4 for pointing out that many people do care about first-order worst-case perturbations. However, we agree that not all vulnerability comes from first-order phenomena.  \n\nThere seems to be an ongoing debate about the best terminology for the objects studied in our paper. We are happy to term them first-order inputs perturbations, first-order worst-case corruption, first-order adversarial perturbations, or anything similar that makes sense. We will be happy to receive suggestions and edit the text accordingly.  Would that be acceptable?\n\nAs to why our insights on prior vulnerability is relevant to understand the vulnerability of SOTA robust models: as mentioned by AnonReviewer3 citing [2], SOTA models still don't manage to reduce gradients on out-of-training examples (see point 5/ below). This is why we think our study is still highly relevant even after strong adversarial training. (Also see our separate thread on why prior vulnerability matters.)\n\nAs for technical remarks:\n\n1/ More defences closer to SOTA, especially, iterative ones: the version updated today covers PGD iterative defences (see Figs. 2 & 6). The conclusions are unchanged.\n\n2/ Concerning Fig 4b of the later work [1]: Fig 4b of [1] is *after* robust adversarial training; our claims refer to before adversarial training. So [1] does not refute our claims: rather, it confirms that robust adversarial training operates via reducing gradient issues.\nMore precisely, to be comparable across dimensions, the x-axis of Fig 4 of [1] should be rescaled to epsilon / sqrt(d), because their epsilon is measured in l_2-norm rather than l_infty (see our Eq. (3), or their text).  After proper rescaling, Fig 4a shows that naturally trained nets are more vulnerable with growing input-dimension. This confirms our own theorems and experiments (our Fig. 3). After rescaling, the curves of Fig 4b seem to overlap: indeed this suggests that our predicted dimension-dependence vanishes *after* robust adversarial training (rather than usual training). This does not contradict our results, it comforts and completes them: usual training does not escape the prior's vulnerability, and in some situations, ill-behaved priors can be escaped by robust training.\n\n3/ Initialization: most people use He-like initializations for a very good reason: it is essentially the only way to obtain bounded activations at init and be symmetric over the inputs of each unit. Our work thus underlines a conflict between keeping reasonably-valued activations, and having reasonably-sized input gradients. This certainly suggests future studies of other initializations. Such initializations will somehow have to break symmetry between units, and thus introduce implicit priors. Some possibilities would be to favor low frequencies in the image, or to prioritize the learning of some units wrt others by initializing them to larger weights (thus implicitly telling the network to try the large-activation units first, which in effect is a prior on the number of units). We have started to play with this, but this is a whole new research direction, while the present text is long enough...\n\n4/ Concerning \"adversarial training eventually gives robustness to the training set, but this robustness does not generalize to the test set [2]. For these models, the gradients are well behaved local to the training points (...) but the gradients aren\u2019t well behaved for new iid samples from the data distribution.\"\nThis does not contradict our theory/experiments. On the contrary: it appears that outside the training points where gradients have been decreased, the network might keep the gradient properties from its prior, namely, naturally large gradients. This example hence rather shows that our findings on priors and the resulting gradients should be kept in mind.\n\nOnce more, we would like to thank the reviewers for the rich debate. We hope that the technical points have been addressed. We do believe our results are non-trivial and relevant. It seems that the \"adversarial\" terminology raises issues: we are open to consensus suggestions on this point.\n\n[1] Are adversarial examples inevitable, anonymous, under review for ICLR 2019", "title": "Overall answer to reviews"}, "rylBpNERnm": {"type": "review", "replyto": "H1MzKs05F7", "review": "This paper argues that adversarial vulnerability of neural networks increases with input dimension. Theoretical and empirical evidence are given which connect the l_p norm of the gradient of the training objective with the existence of small-worst case l_q perturbations. This connection is made by assuming that the learned function is well approximated by a linear function local to the sampled input x. By making assumptions on the initialization scheme for some simple architectures, the authors show that the l_p norm of the gradient for randomly initialized network will be large, and provide empirical evidence that these assumptions hold after training. These assumptions imply bounds on the typical magnitude of the gradient of the loss with respect to a single input coordinate, this then implies that the overall gradient norm will depend on the input dimension.\n\nI found this paper well written. The mathematical assumptions are presented in a clear, easy to understand manner. Also high level intuition is given around their main theorems which help the reader understand the main ideas. However, I have a  number of concerns about this work.\n\nThe first is, I do not buy the motivation for studying the \"phenomenon\" of small worst-case l_p perturbations. I realize this statement applies to a large body of literature, but since the publication of  [1] we are still lacking concrete motivating scenarios for the l_p action space. I would encourage the authors instead to ask the closely related but more general question of how we can improve model generalization outside the natural distribution of images, such as generalization in the presence of commonly occurring image corruptions [2]. It's possible that the analysis in this work could better our understanding model generalization in the presence of different image corruptions, indeed by making similar linearity assumptions as considered in this work, test error in additive Gaussian noise can be linked with distance to the decision boundary [3,4]. However, this particular question was not explored in this work.\n\nSecond, the work is one of many to relate the norm of the gradient with adversarial robustness (for example, this has been proposed as a defense mechanism in [5,6]). I also suspect that the main theorem relating gradient norm to initialization should easily follow for more general settings using the mean field theory developed by [7,8] (this would be particularly useful for removing assumption H1, which assumes the ReLU activation is a random variable independent of the weights). Overall, I don't see how gradient norms explain why statistical classifiers make mistakes, particularly for more realistic attacker action spaces [9]. Even for \"small\" l_p adversarial examples there seem to be limitations as to how much gradient norms can explain the phenomenon --- for example even max margin classifiers such as SVM's have \"adversarial examples\". Furthermore, adversarial training has been shown to reach a point where the model is \"robust\" locally to training points but this robustness does not generalize to the points in the test set [10]. In fact, for the synthetic data distributions considered in [10], it's proven that no learning algorithm can achieve robustness given insufficient training data.\n\nFinally, the main conclusion of this work \"adversarial vulnerability of neural networks increases with input dimension\" is an overly general statement which needs a much more nuanced view. While experiments shown in [11] support this conclusion for naturally trained networks, it is shown that when adversarial training is applied the model is more robust when the input dimension is higher (see Figure 4 a. and b.). Perhaps the assumptions for Theorem 4 are violated for these adversarially trained models. \n\n1. https://arxiv.org/abs/1807.06732\n2. https://arxiv.org/abs/1807.01697\n3. https://arxiv.org/abs/1608.08967\n4. https://openreview.net/forum?id=S1xoy3CcYX&noteId=BklKxJBF57.\n5. https://arxiv.org/abs/1704.08847\n6. https://arxiv.org/abs/1608.07690\n7. https://arxiv.org/abs/1611.01232\n8. https://arxiv.org/abs/1806.05393\n9. https://arxiv.org/abs/1712.09665\n10. https://arxiv.org/abs/1804.11285\n11. https://arxiv.org/pdf/1809.02104.pdf", "title": "Considered question seems poorly motivated, significance of analysis and conclusions yet to be demonstrated", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1eawIzc3X": {"type": "review", "replyto": "H1MzKs05F7", "review": "The authors provide a compelling theoretical explanation for a large class of adversarial examples.  While this explanation (rooted in the norm of gradients of neural networks being the culprit for the existence of adversarial examples) is not new, they unify several old perspectives, and convincingly argue for genuinely new scaling relationships (i.e. \\sqrt(d) versus linear in d scaling of sensitivity to adversarial perturbations versus input size). They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures, and provide thorough empirical evidence of their work.\n\nI can honestly find very little to complain about in this work--the prose is clear, and the proofs are correct as far as I can tell (though I found Figure 4 in the appendix (left panel) to not be hugely compelling.  More data here would be great!)\n\nAs much of the analysis hinges on the particularities of the weight distribution at initialization, could the authors comment on possible defenses to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)?", "title": "A solid contribution to the study of adversarial examples.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJgK8oPUh7": {"type": "review", "replyto": "H1MzKs05F7", "review": "This paper analyzes the relationship between \"adversarial vulnerability\" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found.\n\nThe paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis.\n\nThe theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons:\n- This analysis only seems to work for \"well-behaved\" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.\n- One very important special case of the point above: the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes.\n\n\nOn the empirical results, the authors made a few interesting observations, for example the close correspondence between \"Adv Train\" and \"Grad Regu\" models. \nMy concern is that the experiments were done on a narrow range of models, which only have \"weak\" adversarial training / defenses.\nAdversarial robustness is hard to achieve. What matters the most is \"why the strongest model is still not robust?\" not \"why some weak models are not robust?\" \nIt is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\\infty robustness benchmark.\nWithout comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper.\nFor example, re: the last sentence in the conclusion: \"They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques.\" The reasoning is not obvious to me given the current evidence shown in the paper.\n\n[MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083", "title": "interesting work but with limited applicability and significance demonstrated", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}