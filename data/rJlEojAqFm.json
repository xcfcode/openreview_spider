{"paper": {"title": "Relational Forward Models for Multi-Agent Learning", "authors": ["Andrea Tacchetti", "H. Francis Song", "Pedro A. M. Mediano", "Vinicius Zambaldi", "J\u00e1nos Kram\u00e1r", "Neil C. Rabinowitz", "Thore Graepel", "Matthew Botvinick", "Peter W. Battaglia"], "authorids": ["atacchet@google.com", "songf@google.com", "pmediano@imperial.ac.uk", "vzambaldi@google.com", "janosk@google.com", "ncr@google.com", "thore@google.com", "botvinick@google.com", "peterbattaglia@google.com"], "summary": "Relational Forward Models for multi-agent learning make accurate predictions of agents' future behavior, they produce intepretable representations and can be used inside agents.", "abstract": "The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them. Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments. Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions. Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. \nAs more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary. Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial.", "keywords": ["multi-agent reinforcement learning", "relational reasoning", "forward models"]}, "meta": {"decision": "Accept (Poster)", "comment": "\npros:\n- interesting application of graph networks for relational inference in MARL, allowing interpretability and, as the results show, increasing performance\n- better learning curves in several games\n- somewhat better forward prediction than baselines\n\ncons:\n- perhaps some lingering confusion about the amount of improvement over the LSTM+MLP baseline\n\nMany of the reviewer's other issues have been addressed in revision and I recommend acceptance."}, "review": {"HylpLcxbTm": {"type": "review", "replyto": "rJlEojAqFm", "review": "\nThis paper used graph neural networks to do relational reasoning of multi-agent systems to predict the actions and returns of MARL agents that they call Relational Forward Modeling. They used RFM to analyze and assess the coordination between agents in three different multi-agent environments. They then constructed an RFM-aumented RL agent and showed improved training speeds over non relational reasoning baseline methods. \n\nI think the overall approach is interesting and a novel way to address the growing concern of how to access coordination between agents in multi-agent systems. I also like how they authors immediately incorporated the relational reasoning approach to improve the training of the MARL agents. \n\nI wonder how dependent this approach is to the semantic representation of the environment. These semantic descriptions are similar to hand crafted features and thus will require some prior knowledge about the environment or task and will be harder to obtain on more difficult environment and tasks. \n\nWill this approach work on continuous tasks? For example, the continuous state and action space of the predator-prey tasks that use the multi-agent particle environment from OpenAi. \n\nI think one of the biggest selling points from this paper is using this method to assess the coordination/collaboration between agents (i.e. the social influence amongst agents). I would have liked to see\nmore visualizations or analysis into these learned representations. The bottom row of Figure 3 shows that \"when stags become available, agents care about each other more than just before that happens\". While this is very interesting and an important result, i think that this allows one to see what features of the environment (including other agents) are important to a particular agents decision making but it doesn't really answer whether the agents are truly coordinated, i.e. whether there are any causal dependencies between agents. \n\nFor the RFM augmented agents, I like that you are able to train the policy as well as the RFM simultaneously from scratch, however, it seems that this requires you to only train a single agent in the multi-agent environment. If I understand correctly, for a given multi-agent environment, you first pre-trained A2C agents to play the three MARL games and then you paired one of the pre-trained (expert) agents with the RFM-augmented learning agents during training. This seems to limit the practicality and usability of this method as it requires you to have pre-trained agents that have already solved the task. I would like to know why the authors didn't try to train two (or four) RFM-augmented agents from scratch together. When you use one of the agents as a pre-trained agent, this might make the training of the RFM module a bit easier since you have at least one agent with a fixed policy to predict actions from.  It could be challenging when trying to train both RFM modules on two learning agents as the behaviors of learning agents are changing over time and thus the learning might be unstable. \n\nOverall, i think this is an interesting approach and especially for probing what information drives agents' behaviors. However, I don't see the benefit of the RFM-augmented agent provides. It's clearly shown to learn faster than non RFM-augmented agents (which is good), however, unless I'm mistaken, the RFM-augmented agent requires a pre-trained agent to be able to learn in the first place. \n\n--edit:\nThe authors have sufficiently addressed my questions and concerns and have performed additional analysis.  My biggest concern of weather or not the RFM-augmented agent was capable of learning without a pre-trained agent has been addressed with additional experiments and analysis (Figure 8). \n\nBased on this, i have adjusted my rating to a 7. \n\n", "title": "Relational Forward Models for Multi-Agent Learning provides a new tool for assessing coordination in MARL and can improve MARL training speeds. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJgBS8y0nX": {"type": "review", "replyto": "rJlEojAqFm", "review": "RELATIONAL FORWARD MODELS FOR MULTI-AGENT LEARNING\n\nSummary: Model free learning is hard, especially in multi-agent systems. The authors consider a way of reducing variance which is to have an explicit model of actions that other agents will take. The model uses a graphical structure and the authors argue it is a) interpretable, b) predicts actions better and further forward than competing models, c) can increase learning speed.\n\nStrong Points:\n-\tThe main innovation here is that the model uses a graph conv net-like architecture which also allows for interpretable outputs of \u201cwhat is going on\u201d in a game.\n-\tThe authors show that the RFM increases learning speed in several games\n-\tThe authors show that the RFM does somewhat better at forward action prediction than a na\u00efve LSTM+MLP setup and other competing models\n\nWeak Point\n-\tThe RFM is compared to other models in predicting forwards actions but is not compared to other models in Figure 5, so it is not clear that the graphical structure is actually required to speed up learning. I would like to see these experiments added before we can say that the RFM is adding to performance.\n-\tRelated: The authors argue that an advantage of the RFM is that it is interpretable, but I thought a main argument of Rabinowitz et. al. was that simple forward models similar to the LSTM+MLP here were also interpretable? If the RFM does not improve learning above and beyond the LSTM+MLP then the argument comes down to more accurate action prediction (ok) and more interpretability (maybe) which is less compelling.\n\nClarifying Questions\n-\tHow does the 4 player Stag Hunt work? Do all 4 agents have to step on the Stag together or just 2 of them? How are rewards distributed? Is there a negative payoff for Hunting the stag alone as in the Peysakhovich & Lerer paper?\n-\tRelated: In the Stag Hunt there are multiple equilibria, either agents learn to get plants (which is safe but low payoff) or they learn to Hunt (which is risky but high payoff). Is the RFM leading to more convergence to the Hunting state or is it simple leading agents to learn the safe but low payoff strategies faster?\n- The choice of metric in Figure 2 (# exactly correct prediction) is non-standard (not saying it is wrong). I think it would be good to also see a plot of a more standard metric such as loglikelihood of the model's for each of X possible steps ahead. It would help to clarify where the RFM is doing better (is it better at any horizon or is it just able to look further forward more accurately than the competitors?)\n\n\n", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rke5oY6H0X": {"type": "rebuttal", "replyto": "BJgBS8y0nX", "comment": "Hello,\n\nThank you again for taking the time to review for ICLR and for your insightful feedback.\n\nFollowing your suggestion, we have updated the text to include a more thorough discussion of Rabinowitz et al 2018 in the Related Work section. We highlighted that Rabinowitz et al.\u2019s ToM net focuses on single agent RL and on entire behavioral motifs as opposed to an entity-relation interpretable model of each action and event. Similarly, we pointed to Fig. 7 more prominently in the text; this figure contains an additional experiment showing that onboard RFM modules accelerates agents learning to a larger extent than a non-relational MLP + LSTM based module. Finally, we included our thinking for our choice of model-performance metric in the main text and directed the reader to Fig. 10 where, for completeness, we report the next-action classification accuracy of each model.\n\nWe think these additions will help the reader put our work in the context of existing methods, appreciate the problems in which relational models might be a preferred choice, as well as provide more complete performance assessment measures.\n\nWith only two days left in the rebuttal period we wanted to make sure we have dispelled your concerns. Please do let us know if anything else needs to be further clarified.\n\nThank you", "title": "Response to AnonReviewer1 -- 2"}, "HkxhF8j8TX": {"type": "review", "replyto": "rJlEojAqFm", "review": "This paper studies predicting multi-agent behavior using a proposed neural network architecture. The architecture, called a relational forward model (RFM) is the same graph network proposed by Battaglia et al., 2018, but adds a recurrent component. Two tasks are define: predict the next action of each agent, and predict the sum of future rewards. The paper demonstrates that RFMs outperform two baselines and two ablations. The authors also show that edge activation magnitudes are correlated with certain phenomenons (e.g. an agent walking towards an entity, or an entity being \u201con\u201d or \u201coff\u201d). The authors also show that appending the output of a pre-trained RFM to the state of a policy can help it learn faster.\n\nOverall, this paper presents some interesting ideas and is easy to follow, but the significance of the paper is not clear. The architecture is a rather straightforward extensions of previous work, and using graph networks for predictive modeling in multi-agent settings has been examined in the past, making the technical contributions not particularly novel. Examining the correlation between edge activation magnitudes and certain events is intriguing and perhaps the most novel aspect of this paper, but it is not clear how or why this information would be useful. There a few unsubstantiated claims that are concerning. There are also some odd experimental decisions and results that should be addressed.\n\nFor specific comments:\n\n1. Why would using a recurrent network help (i.e. RFM vs Feedforward)? Unless the policies are non-Markovian, the entire prediction problem should Markovian. I suspect that most of the gains are coming from the fact that the RFM method simply has more parameters than the Feedforward method (e.g. it can amortize some of the computation into the recurrent part of the network). Suggestion: train a Feedforward model that has more parameters (with appropriate hyperparameter sweeps) to see if this is the cause. If not, provide some analysis for why \u201cmemories of the relations between entities\u201d would be any more beneficial than simply recomputing those relations.\n2. The other potential reason that the recurrent method did better is that policy actions are highly correlated (e.g. because agents move in straight lines to locations). If so, then recurrent methods can outperform feedforward methods without having to learn anything about what actually causes policies to move in certain directions. Suggestion: measure the correlation between consecutive actions. If there is non-trivial correlation, than this suggests that RFM does better than Feedforward (which is basically prior work of Battaglia et. al.) is for the wrong reasons.\n3. If I understand the evaluation metric correctly, for each rollout, it counts how many steps from the beginning of the rollout match perfectly before the first error occurs. Then it averages this \u201cminimum time to failure\u201d across all evaluation rollouts. If this is correct, why was this evaluation metric chosen? A much more natural metric would be to just compute the average number of errors on a test data-set (and if this is what is actually reported, please update the description to disambiguate the two). The current metric could be very deceptive:  Methods that do very well on states around the initial-state distribution but poorly near the end of trajectories (e.g. perfectly predicts the actions in the first 10 steps, but then resorts to random guessing for the last 99999 time steps) will outperform methods that have lower average error rate (e.g. a model that is correct 50% of the time). Suggestion: change the metrics to average number of errors, or report both, or provide a convincing argument why this metric is meaningful.\n4. Unless I misunderstood, the results in Section 2.2.3 seem spurious and the claims seem unsubstantiated. For one, if we look at Equations (1) and (2), when we average over s_a1 and s_a2, they should both give the same average for R_a1. Put another way: the prune graph should (in theory) marginalize out s_a2. On average, its expected output should be the same as the output of the full graph (after marginalizing out s_a1 and s_a2). Obviously, it is possible to find specific rollouts where the full graph has higher value than the prune graph (and it seems Figure 4 does this), but it should equally be possible to find rollouts where the opposite is true. I\u2019m hoping I misunderstood this section, but otherwise this seems to invalidate all the claims made in this section.\n5. Even if concern #4 is addressed, the following sentence would still seem false: \u201cThis figure shows that teammates\u2019 influence on each other during this time is beneficial to their return.\u201d The figure simply shows predictions of the RFM, and not of the ground truth. Moreover, it\u2019s not clear what \u201cteammates\u2019 influence\u201d actually means.\n6. The comparison to NRI seems rather odd, since that method uses strictly less information than RFM.\n7. For Section 3, is the RFM module pretrained and then fine-tuned with the new policy? If so, this gives the \u201cRFM + A2C\u201d agent extra information indirectly via the pretrained weights of the RFM module.\n8. I\u2019m not sure what to make of the correlation analysis. It is not too surprising that there is some correlation (in fact, it\u2019d be quite an interesting paper if the findings were that there wasn\u2019t a correlation!), and it\u2019s not clear to me how this could be used for debugging, visualizations, etc. If someone wanted to analyze the correlation between two entities and a policy\u2019s action, it seems like they could directly model this correlation.\n\nSome minor comments:\n - In Figure 3C, right, why isn\u2019t the magnitude 0 at time=1? Based on the other plots in Figure 3c, it seems like it should be 0.\n - The month/year in many of the citations seems odd.\n - The use of the word \u201cvalence\u201d seems unnecessarily flowery and distracting.\n\nMy main concern with this paper is that it is not particularly novel and the contribution seems questionable. I have some concerns over the experimental metric and Section 2.2.3, but even if that is clarified, it is not clear how impactful this paper would be. The use of a recurrent network seems unnecessary, unjustified, and not analyzed. The analysis of correlations is interesting, but not particularly compelling or surprising. And lastly, the RFM-augmented results are not very strong.\n\n--\n\nEdit: After discussing with the authors, I have changed my rating. The authors have adjusted some of the language, which I previously thought overstated the contributions and was misleading. They have added a number of experiments which valid the claim that their method is proposing a reasonable way of measuring collaboration. I also realized that I misunderstood one of the sections, and I encourage the authors to improve the presentation to (1) present the significance of the experiments more clear, (2) not overstate the results, and (3) emphasize the contribution more clearly.\n\nOverall, the paper presents convincing evidence that factors in a graph neural networks do capture some notion of collaboration. I do not feel that the paper is particularly novel, but the experiments are thorough. Furthermore, their experiments show that adding an RFM module to an agent consistently helps (albeit not by much). Given that the multi-agent community is still trying to decide how to best quantify and use metrics for collaboration, I find it difficult to access the long-term impact of this paper. However, given the thoroughness of the experiments and analysis, I suspect that this will be valuable for the community and deserves some visibility.", "title": "Review of Relational Forward Models for Multi-Agent Learning", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SygcOsjn67": {"type": "rebuttal", "replyto": "ryg9NANi6Q", "comment": "8a) Thank you for your suggestion, we have changed the paragraph in our introduction that outlines our contribution as follows:\n\n[...] Perhaps more importantly, they produce intermediate representations that support the social analysis of multi-agent systems: we use our models to propose a new way to characterize what drives each agent's behavior, track when agents influence each other, and identify which factors in the environment mediate the presence and valence of social interactions. [...]\n\nSimilarly we modified the motivation of our experiments in Sec. 2.2.2 as follows:\n\nWe propose the Euclidean norm of a message vector (i.e., $\\|e'_k\\|$) as a measure of the influence a sender entity, $v_{s_k}$, has on a receiver, $v_{r_k}$. We validate this suggestion in Fig.~\\ref{fig:edges} (top row), where [...]\n\n8b) Thank you, we are happy to hear you recognize the value of our work and we are glad you agree that Figure 9 (sorry about the typo!) conveys the information that only relative changes, and not raw values are meaningful.\n\n8c) Thank you for the thoughtful suggestion. We\u2019ve given similar ideas a lot of thought and can share some insights.\n\nFirst, we penalize edge activations during training, precisely to encourage the edges to only convey useful information for prediction. As a result of this, if the state s_a2 is not predictive of the action a_a1, the model will learn to suppress the edge activation. In other words, if the derivative you propose is small, the edge norm should be small.\n\nIn the ideal case, we would have ground-truth data of the effect of s_a2 on a_a1 to validate this. While we do not have ground-truth data for agent-agent influence (see below), we do have them for object-agent influence (Fig. 3 top-row). In this case, we find that objects (stags and apples) with large edge norms are informative about the direction that the agent subsequently travels, while objects with small edge norms are not informative in the same way. Thus the magnitude of these edges are good proxies for measuring object-agent influence.\n\nBeing able to validate that the agent-agent edge norm correlates with this derivative turns out to be technically complex, even in these relatively small environments. For one, in the case of the apples and stags above, the ground-truth influence of objects on agents can be estimated through their attractive effect on the agent (i.e. one knows the action to measure correlation against); while the effect of one agent\u2019s state on another agent\u2019s action can be much more intricate (e.g. I\u2019m going for this apple, so you\u2019d better not). Another complexity is that the derivative measure you propose needs to be integrated over plausible alternative values for s_a2. Choosing a space of counterfactuals, or averaging over a proposal distribution q(s_a2), brings its own challenges. We\u2019re actually working on a similar idea in spirit at the moment (though it\u2019s out of the scope of this work), and we look forward to sharing our results in a later paper when they are ready. \n\n", "title": "Re: Response to AnonReviwer4 3/3 "}, "ryeSysshT7": {"type": "rebuttal", "replyto": "BJxryqVsTQ", "comment": "Q) I also fail to see why removing some edges [...]\n\nThe RFM indeed receives both s_a1 and s_a2 as input, but it\u2019s computing both R_a1 and R_a2 at the same time. The signal path for the pruned-graph estimator is such that s_a1 information is only routed to predict R_a1, and s_a2 information is only routed to predict R_a2. \n\nThis can be checked by looking at the Graph Net formulas in Eq. 1 in the paper. Suppose, for simplicity, that there are only 3 vertices:\n\nA1 (agent 1) with attributes (x, y, action, N/A)\nA2 (agent 2) with attributes (x, y, action, N/A)\nS1 (stag 1) with attributes (x, y, N/A, available/unavailable),\n\n2 edges: \nE_1: receiver: A1, sender A2, no attributes\nE_2: receiver: A1, sender S1, no attributes,\n\nand that globals are empty.\n\nWe want to use our graph net to predict the return of agent 1 denoted as A_1\u2019 (to highlight that this will be an updated node attribute). From Eq. 1 in the paper:\n\nE_1\u2019 = PHI_E(A_1, A_2)\nE_2\u2019 = PHI_E(A_1, S_1)\nA_1\u2019 = PHI_V[PHI_E(A_1, A_2) + PHI_E(A_1, S_1)]\n\n(In the last line we used the fact that that RHO_E--->V is just a sum over the senders A_2 and S_1 of the PHI_Es).\nIf we remove edge 1 (receiver A_1, sender A_2) then\n\nA_1\u2019 = PHI_V[PHI_E(A_1, S_1)]\n\nAnd S_2 does not enter the calculations.\n\nWe have also fixed some confusing notation in Eq. 2 and 3 to highlight that M is a function approximator rather than a probability distribution.\n", "title": "Re: Response to AnonReviewer4 2/3 2/2"}, "rJe9bjs36X": {"type": "rebuttal", "replyto": "BJxryqVsTQ", "comment": "Hi, thank you for getting back to us so quickly and for working with us to make sure we put our work out there in a timely fashion.\n\nMaybe we finally understand where the confusion comes from: our choice of example with Mary and John and their presence had spurious consequences and we sincerely apologize for this. The variable Y in that example corresponds to Mary\u2019s presence/absence, whereas the variable Y in the paper itself corresponds to the *state* of the agent (e.g. its position, previous action, ...). The fact that you deduce from \u201call agents are always present in this environment\u201d the conclusion that \u201cY is always set to 1\u201d (which is not actually the case) might stem directly from this specific choice of example. For the sake of avoiding further misleading statements, let us ground the discussion back in the game we considered.\n\nWe hope these observations will bring things back into focus:\n1. The \u201cstate of the teammate\u201d, s_a2, does not denote its presence or absence, but rather all the agent node attributes (position and last action). When we remove s_a2 from the equation we do not \u201cremove the agent\u201d, but simply make information about its position and last action unavailable to the model, M. In light of this, \u201caveraging over Y\u201d makes sense even when \u201cthe agent is always there\u201d, because Y contains the agent\u2019s position and last action, rather than an indicator variable denoting its presence.\n\n2. During training, our model has access to examples from all situations (both when using the full graph and the pruned graph). However, when computing the average \u201cEffect on Return\u201d in Fig. 4, we restrict ourselves to situations when a stag was *consumed*.\nIn this case the model with the full graph knows that both agents are on the stag (or near to it, in the time steps leading to the consumption event); the full graph model correctly predicts that both agents will collect a reward of 10. On the other hand the model with the pruned graph does not *know for sure* that the second agent is near or on the stag, so it predicts a lower reward. We measure the difference between these two estimates (one with observed s_a2 and one averaging over an implicit posterior on s_a2) and call it the \u201cvalue of the actual social context\u201d.\n\n3.You are absolutely correct that if we were to average over *all* situations we would find that the difference between the two estimators is close to 0 (in practice, we find that this difference is less than 5% of the total reward collected by the agents). However, in the analysis of this section, we are not averaging over all situations. Instead, we average only over a specific set of situations: for Fig. 4 middle and right panel, we only consider times when a stag is about to be *consumed*.\n", "title": "Re: Response to AnonReviewer4 2/3 1/2"}, "Byxww9-qT7": {"type": "rebuttal", "replyto": "SkeCbN1KpX", "comment": "Thank you for your question!\n\nWe tried to match models for capacity (with the exception of NRI which has about 3x more parameters than other models because of its autoencoder connectivity estimator). The raw number of parameters (as reported by the TensorFlow checkpoint loader) were as follows for the RFM, FeedForward and MLP + LSTM:\n\n(RFM, CoopNav) ---> 61194\n(RFM, CoinGame) ---> 63134\n(RFM, StagHunt) ---> 63134\n\n(FeedForward, CoopNav) ---> 60240\n(FeedForward, CoinGame) ---> 65140\n(FeedForward, StagHunt) ---> 65140\n\n(MLP + LSTM, CoopNav) ---> 59307\n(MLP + LSTM, CoopNav) ---> 66718\n(MLP + LSTM, StagHunt) ---> 71803\n\nWe don\u2019t think a discrepancy in the number of parameters of less than 3% can account for the difference in performance we observe.\n\nWe have added a comment to the paper (\u201cWe matched all models for capacity\u2026\u201d).\n", "title": "Re: Response to AnonReviewer4 1/3"}, "H1gmKi-56m": {"type": "rebuttal", "replyto": "rygAtEgF6Q", "comment": "Thank you for raising these concerns.\n\n8a) Learning coordinated behavior is a goal of multi-agent reinforcement learning. Apart from the coarse game-theoretic definition, coordinated behavior is abstractly defined as inter-dependent behavior towards a common objective. Yet this does not give clear direction on how to actually *measure* whether a set of policies acts in a coordinated manner or not. The way we structure the RFM model allows us to do this directly. This can be useful to any researcher that wants to design either tasks or algorithms that lead to coordinated behavior: we now have a way to measure whether they got it or not. In turn, this will assist the MARL community in building better algorithms, and better tasks. \n\nHere\u2019s a concrete example. Suppose we want to design the next generation of house-cleaning robots. In particular, we\u2019re designing a pair of robots, one of whom operates a dustpan, and one operates a brush. As engineers designing this system, we would want to ensure that the agents are actually learning a coordinated solution to the dusting problem. Our method could allow us to identify whether a particular pair of policies is actually achieving this by fitting a powerful non-linear predictive model that goes well beyond simple correlation and co-occurrence. Moreover, we could use it to find the situations for which the robots\u2019 behavior is most inter-dependent, and use this to design training environments or curricula to increase the learning pressure for such behavior.\n\nWe have added some clarifying description in the introduction of the paper:\n\n\u201cAlongside the challenges of learning coordinated behaviors, there are also the challenges of measuring them. In learning-based systems, the analysis tools currently available to researchers focus on the functioning of each single agent, and are ill-equipped to characterize systems of diverse agents as a whole. Moreover, there has been little development of tools for measuring the contextual inter-dependence of agents' behaviors in complex environment, which will be valuable for identifying the conditions under which agents are successfully coordinating.\u201d\n\n8b) You're correct that the absolute magnitudes (e.g. 2.7 and 3.2) are not meaningful in and of themselves. What matters is the relative values. The change in magnitude is certainly statistically significant, as can be seen from the error bars. The edges from stags to agents change even more than this when the stags respawn, as shown in Figure 3b.\n\nWith respect to your suggestion that it's not surprising that when the input changes, the activation of hidden units changes too: this is not a given! In particular, we now show in Figure 8 that these results do not show up when stags are not relevant for coordination (i.e. their presence does not mediate coordinative behaviors). \n\nWe added a comment in Section 2.2.2 to the effect that the raw numbers are not intrinsically meaningful, and that one should instead consider comparisons between edge norm values or rank order of edge norm values.\n\n8c) We agree that \"explains\" and \"how\" claim too much ground. We have changed this sentence to \"Our models enable a characterization of what drives each agent's behavior, tracking when agents influence each other, and identifying factors in the environment which mediate the presence and valence of social interaction.\". With respect to your last point, the analyses in Figure 3 and 9 study the presence, and Figure 4 studies the valence.  \n", "title": "Re: Response to AnonReviwer4 3/3"}, "Syes3cZcp7": {"type": "rebuttal", "replyto": "HJeL-oyF6Q", "comment": "Thank you for taking the time to clarify your question.\n\n4) This statement is incorrect: \"the prune graph is only trained on data collected when the other agent is present, correct?\". The pruned-graph estimator and the full-graph estimator are *both* trained on *all* data. The pruned graph just never gets to know the state of the other agent when making its predictions.\n\nThere\u2019s perhaps a subtlety here, in case you missed it before: all agents are always present in this environment; the difference between the full-graph estimator and the pruned-graph estimator is that one is given access to the state s_a2, and the other is not. In the John/Mary case, the state s_a2 corresponds to the presence/absence of Mary. So your statement that the prune graph \u201cseems to directly model E[X | Mary=1]\u201d is incorrect: it\u2019s directly modelling p(X), where Mary could be 0 or 1 (but it never gets to know what the value truly is). \n\nAs more detail, both the pruned-graph estimator and the full-graph estimator are produced by a single graph neural network. Thus M is the same model in the two equations. We only have one neural network, which is trained to predict agent 1's return both using the full graph (i.e. knowing the actual state of a2) and the pruned graph (i.e. not knowing the actual state of a2). During training we randomly drop out all edges between teammates. At test time, when can then compute the full-graph estimate by using all edges, and the pruned-graph estimator by dropping out edges between teammates. \n\nWe provide this information in the paragraph beginning \u201cWe ran this experiment\u2026\u201d. However, we realize from your responses that we did not communicate this clearly, so we have re-written this section by including the following:\n\nWe note that within this setup, both the pruned-graph estimator and the full-graph estimator are produced by a single graph neural network. This network is trained to predict agent 1's return both using the full graph (i.e.\\ knowing the actual state of $a_2$) and the pruned graph (i.e.\\ not knowing the actual state of $a_2$). During training we randomly drop out edges between teammates (to ensure that both full graph and pruned graph are in-distribution for $M$). At test time, we then compute the full-graph estimate by using all edges, and the pruned-graph estimator by dropping out edges between teammates.\n\nnew question) In the pruned graph, there are no indirect paths between the two agents through the graph. In other words, there are no ways of sending information from s_a2 (from the current input or in the past) through other entities to the node corresponding to a1. There does remain the possibility that the network could infer *something* about s_a2 from the environment state (which we denote z in the main text). For example, if an apple at a particular location was consumed 5 time steps ago, then the network could effectively determine that agent a2 was within a 5-step radius of that location. We make this comment in the footnote at the bottom of pg 8.\n\n", "title": "Re: Response to AnonReviewer4 2/3 "}, "rklQ8hTOT7": {"type": "rebuttal", "replyto": "HylpLcxbTm", "comment": "Thank you for your insightful questions and comments on the submission.\n\nWe hope we have addressed your questions below. In particular, we have addressed your major criticism (#4 below): RFM-augmented agents do not require pre-trained agents to be able to learn in the first place. Given that this is not a limitation of the method (and we show experiments demonstrating this), we hope you will revise your rating accordingly.\n\n1)  Is this approach dependent on having semantic representations of the environment?\n\nYes, at the moment, the approach we describe is dependent on having such a semantic representation. Learning such representations purely from perceptual input is a field of active research. For example, there has been some success in relational reasoning from pixels (e.g. Santoro et al, 2017; Watters et al, 2017; Barrett et al, 2018; Zambaldi et al, 2018), though little attempt has been made to interrogate these systems to uncover the semantics of the intermediate representations (which is something we leverage for our analysis). We do not try to solve the problems of learning semantic representations from pixels here, but we anticipate that as progress is made in this domain, we will be to transfer it over to build the next generation of models.\n\n2)  Would this work in continuous settings?\n\nActually, the methods we use originated in continuous settings. For example, graph nets have been used to model the dynamics of interacting particles (Battaglia et al, 2016). In multi-agent settings, both VAIN (Hoshen, 2017) and NRI (Kipf et al, 2018) have been applied to model behavior in continuous domains (soccer and basketball, respectively). We did not explicitly test the RFM model in continuous domains in this submission, but we have no reason to believe that it would not work.", "title": "Response to AnonReviewer3 1/2"}, "S1l7Eh6d6m": {"type": "rebuttal", "replyto": "HkxhF8j8TX", "comment": "Thank you for a very thorough and thoughtful review.\n\n0)  The architecture is a rather straightforward extensions of previous work, and using graph networks for predictive modeling in multi-agent settings has been examined in the past, making the technical contributions not particularly novel. Examining the correlation between edge activation magnitudes and certain events is intriguing and perhaps the most novel aspect of this paper, but it is not clear how or why this information would be useful. There a few unsubstantiated claims that are concerning. There are also some odd experimental decisions and results that should be addressed.\n\nWe agree that components of the model are drawn from previous work, and the improvements to this part of the architecture are incremental. However, our two major novel contributions are elsewhere. The first, as the reviewer points out, is in using this model for analysis of multi-agent behavior. This is useful for teasing apart patterns of influence in complex situations; for instance, we can use this method to answer AnonReviewer3\u2019s questions about whether (and when) agents are coordinating with each other. Such analysis can also assist with the evaluation of different multi-agent algorithms to determine whether they are producing desirable policies, or to more finely dissect the cooperative or competitive behaviors that a task induces.\n\nOur second contribution is to integrate the RFM model into agents, which we show assists with their decision-making. This provides a measurable improvement over baselines in multi-agent tasks.\n\n\n1)  Why would using a recurrent network help (i.e. RFM vs Feedforward)? Unless the policies are non-Markovian, the entire prediction problem should Markovian. I suspect that most of the gains are coming from the fact that the RFM method simply has more parameters than the Feedforward method (e.g. it can amortize some of the computation into the recurrent part of the network). Suggestion: train a Feedforward model that has more parameters (with appropriate hyperparameter sweeps) to see if this is the cause. If not, provide some analysis for why \u201cmemories of the relations between entities\u201d would be any more beneficial than simply recomputing those relations.\n\nBy the nature of the problem, we do expect a priori that a stateful RFM should outperform a stateless one, for two reasons. First, in all cases, the agents that the RFMs were modelling were themselves stateful. If the agents are making any functional use of their memory (which we anticipate in the general case), then the RFM would benefit from taking advantage of previous relations between entities. Second, while CoopNav and StagHunt are fully observed, CoinGame is not, as the episode-specific reward for each agent\u2019s coins are known to the agents themselves, but not to their teammates (or the RFM). This latent variable has to be inferred from teammates\u2019 history of actions, since its value is often aliased within a single observation. From the results in Figure 2, we indeed find that the recurrent RFM outperforms the feedforward RFM the most when modelling behavior in CoinGame.\n\n\n2)  The other potential reason that the recurrent method did better is that policy actions are highly correlated (e.g. because agents move in straight lines to locations). If so, then recurrent methods can outperform feedforward methods without having to learn anything about what actually causes policies to move in certain directions. Suggestion: measure the correlation between consecutive actions. If there is non-trivial correlation, than this suggests that RFM does better than Feedforward (which is basically prior work of Battaglia et. al.) is for the wrong reasons.\n\nWe don\u2019t think this alternative hypothesis explains the improved performance of the recurrent model, for two reasons. First, the feedforward RFM includes the most recent previous action in its inputs, allowing it to take advantage of correlations between consecutive actions. Second, any autocorrelation structure in an action sequence is either a consequence of autocorrelation in the MDP state (in which case a feedforward RFM should be able to reproduce it), or it is due to statefulness of the agent (in which case a recurrent RFM is necessary; see our argument above). Either which way, the RFM model will indeed learn what actually causes policies to move in certain directions.", "title": "Response to AnonReviewer4 1/3"}, "SyemfhaO6Q": {"type": "rebuttal", "replyto": "HkxhF8j8TX", "comment": "3)  If I understand the evaluation metric correctly, for each rollout, it counts how many steps from the beginning of the rollout match perfectly before the first error occurs. Then it averages this \u201cminimum time to failure\u201d across all evaluation rollouts. If this is correct, why was this evaluation metric chosen? A much more natural metric would be to just compute the average number of errors on a test data-set (and if this is what is actually reported, please update the description to disambiguate the two). The current metric could be very deceptive:  Methods that do very well on states around the initial-state distribution but poorly near the end of trajectories (e.g. perfectly predicts the actions in the first 10 steps, but then resorts to random guessing for the last 99999 time steps) will outperform methods that have lower average error rate (e.g. a model that is correct 50% of the time). Suggestion: change the metrics to average number of errors, or report both, or provide a convincing argument why this metric is meaningful.\n\nWe have now provided an alternative metric in Figure 10 (next-step action classification accuracy) which shows the same qualitative results.\n\nThe reason we use the particular metric in the main text is that is gives us a measure of how long the model remains useful. In particular, we are learning a simulator of the agents dynamics; the metric gives an indication of how many steps one can simulate before making a mistake. There most likely isn\u2019t a perfect metric that covers all bases, and in particular alternative rollout metrics are hard to define after the model makes a mistake, since the ground-truth observations and predictions no longer match. Nonetheless, between this and the new Figure 10 we believe there\u2019s a strong case that the RFM-based model is better. \n\n\n4)  Unless I misunderstood, the results in Section 2.2.3 seem spurious and the claims seem unsubstantiated. For one, if we look at Equations (1) and (2), when we average over s_a1 and s_a2, they should both give the same average for R_a1. Put another way: the prune graph should (in theory) marginalize out s_a2. On average, its expected output should be the same as the output of the full graph (after marginalizing out s_a1 and s_a2). Obviously, it is possible to find specific rollouts where the full graph has higher value than the prune graph (and it seems Figure 4 does this), but it should equally be possible to find rollouts where the opposite is true. I\u2019m hoping I misunderstood this section, but otherwise this seems to invalidate all the claims made in this section.\n\nWe think we\u2019ve identified the cause of the misunderstanding. You\u2019re correct that if one averaged over *all* situations (i.e. if one marginalized out both s_a1 and s_a2, and z as well), then the two estimators should give the same results. In other words, the full-graph estimator and the pruned-graph estimator should have the same mean value. But here we\u2019re not averaging over all situations. We\u2019re picking a particular *class* of situations. \n\nTo give an example: in the middle of Figure 4, each point represents a particular distribution of situations q_t(s_a1, s_a2, z). These situations are defined by the time t until stag consumption. These distributions, q_t, are themselves not equal to the overall marginal distribution p(s_a1, s_a2, z). Thus when you compute the expected value of the full-graph estimator under q_t, and the expected value of the pruned-graph estimator under q_t, you get different quantities.\n\nHere\u2019s an analogy: imagine we build two models for John\u2019s heart rate. One model includes more factors than the other model. They agree on the average value though: marginalizing over all circumstances, John\u2019s heart rate averages at 80bpm. The richer model also includes a particular factor: when Mary is in the room, John\u2019s heart rate goes up by 10bpm. If we marginalize over situations when Mary is present, the richer model estimates John\u2019s average heart rate to be 90bpm, while the smaller model estimates it as 80bpm. The two models may still agree in expectation over *all* situations (though, as you intuit, the richer model would have to make up for this somehow by having John\u2019s heart rate being lower on average when Mary is absent).   \n\nOverall, this allows us to measure whether an interaction is overall \u201cgood\u201d or \u201cbad\u201d for the agents.\n\n(Some extra info if it helps: the left panel of Figure 4 shows the differences from a single episode, for illustration purposes. We compute the quantities in the middle and right panels over 10 randomly-chosen episodes from the test set).", "title": "Response to AnonReviewer4 2/3"}, "r1xuajaOT7": {"type": "rebuttal", "replyto": "HkxhF8j8TX", "comment": "5)  Even if concern #4 is addressed, the following sentence would still seem false: \u201cThis figure shows that teammates\u2019 influence on each other during this time is beneficial to their return.\u201d The figure simply shows predictions of the RFM, and not of the ground truth. Moreover, it\u2019s not clear what \u201cteammates\u2019 influence\u201d actually means.\n\nGood catch. We cannot actually compute the ground truth. We have rephrased this to: \u201cThus the model estimates that teammates' specific interactions during this time are beneficial to their return.\u201d\n\n6)  The comparison to NRI seems rather odd, since that method uses strictly less information than RFM.\n\nNRI has access to the same set of information as the RFM. In comparison however, NRI actively discards information: it infers a connectivity map from the past trajectory, and may choose to do inference using any number of edges. This might actually confer advantages if the ground-truth process is indeed sparse and relatively stationary. Overall, NRI is a good method and it has many advantages. Our results nevertheless demonstrate that these design decisions are less well-suited to the patterns of multi-agent interaction in these environments. \n\n7) For Section 3, is the RFM module pretrained and then fine-tuned with the new policy? If so, this gives the \u201cRFM + A2C\u201d agent extra information indirectly via the pretrained weights of the RFM module.\n\nThe on-board RFM is trained *from scratch* alongside the policy network. There is no pre-training.\n\n\n8)  I\u2019m not sure what to make of the correlation analysis. It is not too surprising that there is some correlation (in fact, it\u2019d be quite an interesting paper if the findings were that there wasn\u2019t a correlation!), and it\u2019s not clear to me how this could be used for debugging, visualizations, etc. If someone wanted to analyze the correlation between two entities and a policy\u2019s action, it seems like they could directly model this correlation.\n\nMeasuring the emergence of coordinated behavior in multi-agent systems is an important open problem in this field (see also AnonReviewer3\u2019s comment to this effect). Especially in the case of *learning* systems, assessing whether or not agents are able to coordinate, what drives their behavior, whether they learn to help or hinder one another and how they modify their behavior in response to changes in the environment are all crucial aspects of multi-agent analysis that we struggle to quantify. Here we show that learning relational models of multi-agent systems might be a good place to look.\n\nWith respect to the particular suggestion that one could directly model correlations between entities and a policy\u2019s actions, we wish it were that simple! The influence of one agent\u2019s state on another\u2019s behavior can be highly contextual, so one would need to factor in the state to tease apart the appropriate effect. This amounts to fitting a parameterized model of the interaction, which is precisely what we\u2019re doing here. \n\n\n\n9) Some minor comments\na. In Figure 3C, right, why isn\u2019t the magnitude 0 at time=1? Based on the other plots in Figure 3c, it seems like it should be 0.\nAgents tend to rapidly move away from fruit or stags they just consumed. The probability of respawning is small (0.05) so there will be nothing there for some time (on average). This means that there is some information that a recently-consumed entity can provide to the agent that just consumed it: when the entity is adjacent to the agent, its chances of respawning are lower than otherwise. This predictive information drops sharply after the first step as agents will move towards available entities (which might be anywhere) rather than directly away from unavailable ones (see also Figure 3 top row the two right panels).\n\nb. The month/year in many of the citations seems odd.\nWe have fixed this.\n\nc. The use of the word \u201cvalence\u201d seems unnecessarily flowery and distracting.\n\u201cValence\u201d is a standard technical term from psychology, denoting the attractiveness or aversiveness of a stimulus (e.g. Frijda, 1986).\n\nReferences\nFrijda, N. H. (1986). The emotions. Cambridge University Press.\n", "title": "Response to AnonReviwer4 3/3"}, "H1lg2cadaX": {"type": "rebuttal", "replyto": "Hyx9Q__Q67", "comment": "Thank you for your comments. \n\nWe take it as a good sign that you see the ideas as obvious a posteriori. To the best of our knowledge, though, no one has actually explored them. When others have modelled the dynamics of multi-agent systems (e.g. NRI, VAIN, ToMnet), they have not attempted to integrate these models into the agents themselves. Conversely, in papers that do put models of opponents in MARL, these models are not relational, and they model goals in agents identical to oneself (e.g. Raileanu et al. 2018), policy representations or Q-values (He et al. 2016), but not future actions. In a similar manner, no one has developed this method to interpret multi-agent behavior to the extent that we do here. The event-based analysis and value-based analyses are novel applications of this framework. \n\nThank you for pointing out that we were missing a citation to Scarselli, we included it the revised manuscript.\n", "title": "Response to AnonReviewer2"}, "Byx8_qTuaX": {"type": "rebuttal", "replyto": "HylpLcxbTm", "comment": "3)  Are the agents truly coordinated? Can we measure causal influence between agents?\n\nThis is a good question. There are many potential definitions of coordination. From a game theoretic perspective, the agents have definitely found a coordinative equilibrium. They coordinate to consume stags, which is reflected in their overall return. Another sense of coordination is whether agents\u2019 behaviors are mutually interdependent in service of a common goal. The RFM analysis in the bottom of Figure 3 demonstrates that it is statistically appropriate to describe the agents\u2019 behavior as mutually interdependent when stags are present. The common goal, of course, is stag consumption. A final sense might be whether there are ground-truth causal influences between agents. We note that the RFM approach we pursue here is not designed to answer causal questions, per se: it is a statistical fit to time-series data, and does not traffic directly with interventions or counterfactuals. We are currently exploring such possibilities in ongoing research. If there\u2019s an additional sense of coordination that you\u2019re interested, please feel free to suggest a specific experiment to falsify the conjecture that we\u2019re picking up on something other than coordination here.\n\nAs a further test of whether the agents are coordinated in these ways, we ran two additional experiments (1) where there is no scope for coordination between the agents and (2) when no interdependent behavior is required. For additional experiment (1) We trained deep RL agents on a modified version of Stag Hunt where stags yielded no reward at all (i.e. the only rewards are for consuming apples). We then trained the RFM on rollouts of these agents\u2019 behavior. In contrast to the standard case, we found that the edge norms between agents was not modulated by the appearance of a stag (Figure 9a). In additional experiment (2) we obtained similar results in a version of the environment where stags could be consumed by single agents, without the need for coordination (Figure 9b).\n\n4)  Does the RFM-augmented agent require pre-trained agents?\n\nNot at all. We only chose to include this experiment previously to isolate the benefit of including the RFM in the augmented agent and because this situation is relevant to artificial agnets learning to act in an environment shared with human experts. However, the RFM-augmented agent can be trained in just the same way alongside learning agents too, with similar benefits. We have included this in Figure 8 in a revised version of the manuscript.\n\nWe note that when the RFM-augmented agent is trained alongside teammates which are also learning agents, the relative benefit of the RFM on the agent\u2019s return is smaller in magnitude than when this agent is trained alongside expert teammates. We suspect the reason for this is that the RFM model is initially modeling the behavior of untrained teammates, and there are fewer opportunities for rewarded coordination. Since the teammates are learning at roughly the same rate as the RFM-augmented agent, the RFM only has a chance to provide useful information later on during training.\n\nReferences:\nBattaglia et al, (2016). Interaction networks for learning about objects, relations and physics. NIPS.\nSantoro, et al. (2017). A simple neural network module for relational reasoning. NIPS.\nWatters, et al. (2017). Visual interaction networks: Learning a physics simulator from video. NIPS.\nBarrett, et al. (2018). Measuring abstract reasoning in neural networks. arXiv:1807.04225.\nZambaldi, et al. (2018). Relational Deep Reinforcement Learning. arXiv:1806.01830.", "title": "Response to AnonReviewer 3 2/2"}, "rJeJTtTd6X": {"type": "rebuttal", "replyto": "BJgBS8y0nX", "comment": "Thank you for your insightful questions and suggestions on the submission.\n\nWe hope we have addressed your questions below. In particular, we have addressed your two major weak points: the graphical structure of the embedded RFM is indeed helpful for speeding up learning; and the type and degree of interpretability differs substantially from previous work. We hope you will revise your rating accordingly.\n\n\n1)  Can we augment agents with models other than the RFM (e.g. MLP + LSTM)?\n\nYes! We have performed your suggested experiments, which we show in Figure 7. Indeed, the RFM-augmented agents outperform the MLP+LSTM-augmented agents (i.e. those with forward models that are non-relational).\n\n2)  How does interpretability compare with ToMNet (Rabinowitz et al, 2018)?\n\nThis is a good question. To begin, interpretability is not a binary property of a model; there are a number of advantages that RFMs offer over the ToMNet construction in Rabinowitz et al, 2018. Most importantly, the ToMNet embeds sequences of behavior as points in an unstructured, high-dimensional Euclidean space, relying on the inherent structure of the data (and optionally an information bottleneck) to yield interpretable representations. In contrast, RFMs shape behavioral representations through the structure of entities and relations (via the graph net). These representations are very natural for humans to interface with, as they conform to representation schemas of human cognition (Spelke & Kinzler, 2007). Moreover, the representations of the RFM also allow us to easily ask directed questions about how different entities influence agent behavior (e.g. Figure 3), which is not something that the ToMnet enables. \n\nWe also note that the ToMNet was designed as a single-agent forward model, while RFMs naturally scale to the multi-agent setting. This allows us to augment agents with the RFM module, which is not something pursued in this previous work.\n\n3)  Clarifications about Stag Hunt\n\n4 players - do all 4 need to step on the stag to capture it? No, only 2\nIs there a negative reward for hunting alone? No\nDoes including the RFM leads to more Stags being captured? We have only made qualitative observations, but anecdotally, the RFM-augmented agents go for stags like maniacs.\n\n4)  Choice of metric in Figure 2.\n\nWe have now provided an alternative metric in Figure 10 (next-step action classification accuracy) which shows the same qualitative results.\n\nThe reason we use the particular metric in the main text is that is gives us a measure of how long the model remains useful. In particular, we are learning a simulator of the agents dynamics; the metric gives an indication of how many steps one can simulate before making a mistake. Alternative rollout metrics are hard to define beyond the first mistake when the true environment and the predictions have diverged. There most likely isn\u2019t a perfect metric that covers all bases, and in particular alternative rollout metrics are hard to define after the model makes a mistake, since the ground-truth observations and predictions no longer match. Nonetheless, between this and the new Figure 10 we believe there\u2019s a strong case that the RFM-based model is better.  \n\nReferences:\nSpelke & Kinzler. (2007). Core knowledge. Developmental science, 10(1), 89-96.", "title": "Response to AnonReviewer1"}, "Hyx9Q__Q67": {"type": "review", "replyto": "rJlEojAqFm", "review": "This paper proposes to use graph neural networks in the scenario of multi-agent reinforcement learning (MARL). It tackles two current challenges, learning coordinated behaviours and measuring such coordination.\n\nAt the core of the approach are graph neural networks (a cite to Scarselli 2009 would be reasonable): acting and non-acting entities are represented by a graph (with (binary) edges between acting-acting and acting-nonacting entities) and the graph network produces a graph where these edges are transformed into a vectorial representation, which then can be used by a downstream task, e.g. a policy algorithm (as in this paper) that uses it to coordinate behavour. Because the output of the graph network is a structurally identical graph to the input, it is possible to interpret this output.\n\nThe paper is well written, the main ideas are clearly described. I'm uncertain about the novelty of the approach, at least the way the RFM is utilized in the policy is a nice idea (albeit, a-posteriori, sounds straight forward in the context of MARL). Similarly, using the graph output for interpretations is an obvious choice). Nevertheless, showing empirically that the ideas actually work gives the paper a lot of credibility for being a stepping stone in the area of MARL.", "title": "Review Relational Forward Models for Multi-Agent Learning ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}