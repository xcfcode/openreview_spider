{"paper": {"title": "Predictive Attention Transformer: Improving Transformer with Attention Map Prediction", "authors": ["Yujing Wang", "Yaming Yang", "Jiangang Bai", "Mingliang Zhang", "Jing Bai", "Jing Yu", "Ce Zhang", "Yunhai Tong"], "authorids": ["~Yujing_Wang1", "~Yaming_Yang1", "pku_bjg@pku.edu.cn", "zml24@pku.edu.cn", "jbai@microsoft.com", "yujing02@iie.ac.cn", "~Ce_Zhang1", "~Yunhai_Tong1"], "summary": "We propose a novel transformer architecture, PA-Transformer, which leverages convolution-based attention prediction and achieves state-of-the-art performances on both natural language processing and image classification. ", "abstract": "Transformer is a ubiquitous model for natural language processing and has also attracted wide attentions in other domains such as computer vision. The self-attention maps, learned independently for each layer, are indispensable for a transformer model to encode the dependencies among input tokens, however, learning them effectively is still a challenging problem. In this paper, we address this problem and propose a novel approach to improve self-attention through supplementary prediction modules. The underlying assumption is that the attention structures in the current layer should not be completely independent from those in the previous layer. Instead, we model their dependencies via a chain of prediction models that take previous attention maps as input to predict the attention maps of a new layer through convolutional neural networks. Specifically, we propose Predictive Attention Transformer and obtain significant performance gains for various kinds of tasks on top of multiple state-of-the-art models. On GLUE benchmark, the average performances of BERT-Base and BERT-Large are lifted by 4.1 and 2.5 points respectively. For machine translation, it improves the BLUE score of a vanilla Transformer consistently on IWSLT'14 De-En dataset with different model sizes. For ImageNet classification, we achieve significant improvement over a strong backbone model with comparable capacity.", "keywords": ["Transformer", "Convolution", "Attention Map"]}, "meta": {"decision": "Reject", "comment": "Multiple reviewers point out the interesting improvement to mix attention maps at different layers via convolution based prediction modules. This module is sufficient to show improvements only on encoder side while comparing to concurrent work Synthesizer.\nHowever, the novelty of the work is limited as compared to other papers and the results though improved did not convince the reviewers fully to gain a strong accept.\n"}, "review": {"LkSsjLSXnXM": {"type": "rebuttal", "replyto": "YQVjbJPnPc9", "comment": "1. We add evaluation results on RoBERTa-Large and T5-Base, which verify the superiority of PA-BERT on more pre-trained models. Impressively, PA-T5-Base outperforms Synthesizer on the same backbone model without the need of pre-training from scratch again. We also want to address that the conclusion of Synthesizer and our paper is not conflicting, but collaboratively confirming. Synthesizer argues that the current self-attention is not that useful, and a synthesized attention map even improve the performance for some cases.  PA-Transformer proposes a general strategy to improve the generalization ability of self-attention. We think there is a large chance that we can obtain a future improvement by composing Synthesizer and PA-Transformer together, and this sheds new light on a promising direction. \n\n2. We add empirical results on ImageNet with multiple model capacities, including ResNet-34, -50 and -101. In addition, an ablation study for ResNet-34 on ImageNet is updated in the latest version (Table 6). \n\nAA-ResNet-34           74.33\n\nAA-ResNet-34 with PA-style Residual Connection        74.36\n\n1-Layer PA-AA-ResNet-34                                                  74.90\n\n2-Layer PA-AA-ResNet-34                                                  74.35\n\nWe can see that residual connections only improve the performance slightly, but adding one convolutional prediction layer brings much more benefit. To summarize, the experimental results on ImageNet prove the effectiveness of convolutional layers for predicting attention patterns.   \n\n3. The papers are polished following reviewers' suggestions. For short, we provide more insightful explanations, case studies, result numbers and formal conclusions.  \n", "title": "Revision Summary"}, "dYszi1w7cs7": {"type": "rebuttal", "replyto": "mEz5bdrZOqp", "comment": "Dear reviewer,\n\nThanks for the helpful feedback! We fully understand your concerns, and would like to give a bit more explanation and ablation results. \n\nThe most important argument in this paper is that an explicit modeling of token-token dependency patterns based on the previous attention map is useful. As we have shown in the ablation study, removing the convolutional module in attention prediction leads to a significant performance decay (see PA-Transformer v.s. Transformer with Residual Connection). Of course, one can use CNN, FNN or other models (even another self-attention layer) to capture the general dependency patterns in a 2D attention map. Because CNN is the most common choice for modeling 2D inputs, we leverage CNN in this paper for an initial attempt. We think the major contribution of this paper is to point out a new direction and encourage more advanced modeling of attention maps in the future.  \n\nWe agree with you that it would be more insightful to add more ablation settings. We show more ablation results on the SNLI dataset below (average of 5 runs). The empirical analysis on more datasets will be provided in the next version. \n\nTransformer: 83.22\n\nPA-Transformer: 84.63      (with 3*3 convolution kernel) \n\nRes-Transformer: 82.37\n\nThis setting adds residual connections between adjacent transformer blocks.  It does not work, at least for this dataset. \n\nPA-Transformer only Residual Connection: 83.81\n\nThis setting replaces CNNs in the PA-Transformer by direct residual connections. It performs better than vanilla Transformer, but decay obviously from PA-Transformer with 3*3 convolution kernel. \n\nPA-Transformer Synthesized Input: 83.21\n\nThis setting uses a global trainable synthesized attention map as the input of PA-Transformer, instead of using the attention map of the previous layer.  \n\nPA-Transformer FNN: 83.92\n\nThis setting replaces CNNs in PA-Transformer by FNNs (analogous to 1*1 convolution). It has substantial improvement over vanilla Transformer, but decay relatively based on PA-Transformer with 3*3 convolution. This shows the advantage of capturing local dependency patterns with a 3*3 convolution kernel. \n\nPA-Transformer 5*5 CNN: 83.54\n\nThis setting replaces 3 * 3 convolution with 5 * 5 convolution. The performance decay is perhaps due to the overfitting of a large kernel.\n\nAs shown by the results above, using a dedicated model to capture attention patterns generally improves the performance, and 3*3 CNN achieves the best result on the SNLI dataset. \n\nMoreover, it should be noticed that the Conv-Transformer backbone model in our paper already adopts dense residual connections (similar to DenseNet) as described in the paper. Therefore, the improvement on top of Conv-Transformer truly comes from a better modeling of attention patterns. We will keep exploration along this direction and try to provide theoretical analysis in the future. \n", "title": "More explanation and ablation results"}, "SKJIwAvXDP7": {"type": "rebuttal", "replyto": "tomzp9D_dtj", "comment": "Dear reviewer, \nSorry, previously we might have a misunderstanding of your question and did not target it well. Here we explain how our method can be applied to a decoder. \n\nAs the decoder cannot foresee succeeding tokens, we need a special mask. For example, when modeling the dependency  patterns for token i, we can not see the dependency pairs (i, i+1) or (i+1, i), but we can see the pairs (i, i-1), (i, i-2) or (i-1, i-2). These unmasked pairs serve as input to the convolution to help a better modeling of the dependency patterns around token i. For a vanilla transformer, we only need to mask (i, i+1). This is the major difference, and a way to solve the causal problem. \n\nWe have implemented a version of this decoder, which got a marginal improvement (+0.1 BLUE) on De-En machine translation dataset compared to PA only encoder setting. We mainly select benchmarks for encoder evaluation, as the goal of this paper is to claim the contribution and improvement for text and image representation. We will provide extensive empirical study for the decoder in future works. \n\nAt last, we really appreciate your time for the insightful discussions and suggestions no matter what your final decision is. Thank you very much! \n", "title": "Answer your question about decoder"}, "-HeKlmM1KVi": {"type": "review", "replyto": "YQVjbJPnPc9", "review": "The idea proposed in the paper is simple - \"predict\" future attention weights using past attention weights. A 2D CNN is used to mix previous N layer's attention maps. Strictly, this is also not \"predicting\" but instead generating\". There is no supervised loss here. \n\nThe authors introduce a PA-Transformer model. The idea is to use previous attention maps to augment future attention weights. A stack of N previous attention weights is modeled with 2D CNN to generate future attention maps. The idea of predicting attention weights (or generating them) is not new (see https://arxiv.org/abs/2005.00743). The difference here is that there is a 2D CNN to model relationships between N previous layers. This is somewhat a pretty incremental extension of the Synthesizer-Transformer model. \n\nThere is also insufficient convincing evidence that using previous layer's attention to generate future attention weights is beneficial. \n\nI think the experiments are lacking. The experiments on GLUE are only comparing against BERT (the least the authors could do is to compare side-by-side with at least a few other models). Machine translation datasets are tiny and ablation studies are unconvincingly run on SST and SNLI. The authors only run experiments using a preloaded checkpoint of BERT and do not apply their architecture to actually pretrain BERT which is also one weakness of this work. Hence, the paragraph beginning with \"pretraining\" is misleading\". The results on GLUE are also weak and could be a result of variance over the existing BERT model. \n\nThe authors should also discuss how this can be implemented in a decoder setting since the current setup will disable causal attention.\n\nOverall, I recommend a clear rejection. I think the key selling point and hypothesis behind this paper (using prev N layer attention) is not well supported. Experimental settings are also weak and there are insufficient convincing experiments to feel that this architecture is doing something useful. ", "title": "Not convincing experiments and insufficient novelty.", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ZDCm4TuJTSC": {"type": "rebuttal", "replyto": "8-Yx5MkuEYB", "comment": "Thanks again for the detailed discussion! It is really helpful for us to improve this work.\n\nHere we add a clarification of the decoder part. In this paper, we focus on the improvement of encoders. All models use the same vanilla transformer for the decoder network (in T5 and machine translation experiments) to obtain a fair comparison. PA-Transformer has shown a solid improvement although only the encoder network is replaced. The idea of PA-Transformer is also applicable to the decoder network, which is considered as future work. Given consistent improvements on multiple tasks and domains for the encoder (including large-scale image classification), we think this work should be impactful in the current form. We will provide more experimental results with respect to decoder and Synthesizer in a later version. \n", "title": "Clarify the decoder"}, "TS8M5EHlDcx": {"type": "review", "replyto": "YQVjbJPnPc9", "review": "\n**Summary:**\n \nThis paper proposed a modification to the classical transformer architecture and demonstrated significant performance gain on multiple benchmark tasks in both natural language processing and computer vision. Specifically, the authors propose to introduce a convolution-based attention map prediction module, so the dependencies of attention maps across different layers can be captured. With the extensive experiments, the proposed modification is quite effective on improving the model's performance.\n\n\n**Reasons for score:**\n\nThe idea of bridging attention maps across layers in Transformers is intuitive, as later layers can benefit from dependency structures learned from earlier layers. The model is validated on several benchmarks in both NLP and CV, and show some consistent improvements over baselines. However there are many unclear expressions and claims in the paper, and some ablations are missing which might be critical to better understand the module. Besides the paper definitely needs more proof-reading.\n\n\n**Pros:**\n\n1. The idea of treating multi-headed attention maps as multi-channel images is interesting, and the proposed convolution-based attention prediction module is a natural choice under such settings.\n2. The proposed methods are validated with extensive experiments, and the performance gain is consistent and quite significant. This shows the effectiveness of the proposed approach.\n \n**Cons:**\n\n1. When applying transformer architecture to images, the image of shape $H \\times W \\times C$ is flattened as $X \\in R^{N\\times C}$ where $N=H \\times W$. However for regular images the resolution is quite large, e.g., ImageNet is usually used in 224x224, then the N would be ~50k. In Section 4.2.1 it seems even for CIFAR it will be OOM (out of memory), but this is not mentioned for experiments with ImageNet in Section 4.2.2.\n2. On Page 2 the authors claim that \"... experimental results demonstrate the superiority of PA-Transformer in terms of accuracy, memory cost and computational efficiency ...\" but I was only able to see the accuracy improvements; did I miss the experiments for computational efficiency? If it refers to the #Params and #FLOPs in e.g. Table 2, I'm actually curious: according to Table 1, Transformer and PA-Transformer has <0.01K (if not identical) FLOPs. Could you explain how this is calculated? Because if we count multiplications and additions as FLOPs, I think the PA module will definitely introduce more than 10 (=0.01K) FLOPs. \n3. The authors claim several times (e.g. last sentence of Page 3) that self-attention module could \"dedicate itself to incorporate layer-specific knowledge into *residual* attention maps\". It seems arguable since in most cases the self-attention is dominating the generated attention map ($\\alpha$ is usually small). Also, I'm wondering if the 0-layer PA in Table 5 corresponds to a direct skip-connection, i.e. simply copying the $A_{\\text{pre-logits}}$ over to next layer. In fact I think it's a quite important ablation experiment, e.g. replacing \"predicted attention\" with \"attention from previous layer\", and may worth showing the results on other tasks too (e.g. on CIFAR and ImageNet).\n4. Just curious: the $\\alpha$ seems to have large variations across datasets, e.g. on CIFAR it is set to 0.01 but on SNLI and ImageNet it is set to 0.5. In addition to empirical validation results, is there any explanations for this?\n5. In Table 1 and Table 2 since each experiments are replicated for five times, it's better to show the standard deviation (confidence interval) together with the mean value.\n\n**Questions during rebuttal period:**\n\nI've listed my questions in the cons section, and hopefully can be addressed during the rebuttal.\n  \n\n**Some typos and minor issues:**\n\n-- The phrase \"except for\" is misspelled as \"expect for\" in several places (e.g., Page 5, 7th line of Section 4.1.1 \"Models\", Page 7, 2nd line of Section 4.2.1 Settings). \n-- Multiple typos: \"ResetNet-50\" -> \"ResNet-50\", \"Noe\" -> \"Note\", etc. This paper needs more polishing.", "title": "Adding convolution-based attention prediction module to Transformer to capture cross-layer dependencies", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "0ckHRz8VuWc": {"type": "rebuttal", "replyto": "E2ngssr6WG8", "comment": "Thanks for your careful reading and helpful comments. \n\n1 & 2.The results on GLUE reported in this paper are on the test set, and all tasks are fine-tuned individually. All models follow the same experimental protocol, except for Synthesizer. We check the descriptions in the Synthesizer paper and find that it co-trains all the tasks and reports the results on the dev set. Therefore, we also report the dev results on T5-Base architectures for a direct comparison. The average GLUE scores are listed below, with details updated in Table 2 in the latest version. PA-T5-Base outperforms Synthesizer-T5-Base on the dev set although it is only fine-tuned individually without benefiting from co-training of multiple tasks. We sincerely thank you for the comments which help us to find the discrepancy of experimental settings. Since we do not find the open source code of Synthesizer and the time is limited, we have to leave more controlled experiments to future work. \n\nT5-Base (Dev): 83.5\n\nSynthesizer-T5-Base (Dev): 84.1\n\nPA-T5-Base (Dev): 84.4\n\n-------------------------------\n\nT5-Base (Test): 83.2\n\nPA-T5-Base (Test): 84.5\n\n3. Maybe we used a misleading word previously. Actually, Synthesizer can be applied to the computer vision domain. What we mean is that the original paper does not provide results in computer vision tasks, so we are not sure if a synthesized attention map still works well for tasks like image classification. It needs much time to implement a Synthesizer for image-related tasks and conduct sufficient experiments. Thus, a comparison in computer vision domain is considered as future work. \n\nBesides, the ablation results of residual connections and predictive convolutional layers on ImageNet are updated in Table 6 in the latest version. \n", "title": "Answers"}, "VEg9zo1hDhb": {"type": "rebuttal", "replyto": "TS8M5EHlDcx", "comment": "1. For ImageNet experiments, we choose AA-ResNet (Attention Augmented Convolutional Networks, ICCV 2019) as the backbone model, which only applies attention augmentation on top layers to avoid OOM problems. PA-AA-ResNet strictly follows its architecture and only replaces the self-attention layers as PA-based ones. \n\n2. Yes, the memory and computation costs are indicated by #Params and #FLOPS respectively. We calculate them by tensorflow code directly. (reference https://stackoverflow.com/questions/45085938/tensorflow-is-there-a-way-to-measure-flops-for-a-model). We appreciate you for pointing out the mismatch in #FLOPS! We also found this problem after paper submission and have fixed a bug in the code. The new metrics of flops have been updated in the revised version, and the conclusion is not affected. \n\n3. Res-Transformer (0-layer PA) in Table 5 is exactly the ablation baseline for direct skip connection. The result proves the main claim in this paper: a residual connection is beneficial, and a chain of convolutional predictive layers brings additional advantages. We also perform ablation experiments on ImageNet and the results have been updated in the latest version. \n\n4. Although the optimal alpha is small for some datasets, the performance lifts are obvious and stable. The value of alpha does not necessarily indicate the relative importance, but also covers some normalization issues. Originally we just utilized a standard residual connection. However, we find that the convolution-predicted attention sometimes has a sharp distribution, which harms the performance for some datasets. Therefore, we leverage an hyper-parameter \u201calpha\u201d for the residual summation, which achieves a stable improvement for all datasets after hyper-parameter tuning on the validation set. In future work, we would like to investigate ways to get rid of this hyper-parameter while maintaining the superior performance. \n\n5. We have updated Table 1 to include standard deviation of five runs, except for the machine translation experiments where we used a fixed seed following the baseline implementation. For Table 2, we run each experiment only once because the models are finetuned from pre-trained checkpoints and the performances are quite stable. \n\nThanks again for pointing out some errors and typos in our paper. Your comments are really helpful for us to polish the paper content. We have corrected them in the revised version. Looking forward to having more discussions with you. \n", "title": "Address the cons and make a revision accordingly"}, "0qDrGg14JGM": {"type": "rebuttal", "replyto": "-HeKlmM1KVi", "comment": "Thanks for mentioning a related work, \u201cSynthesizer\u201d. It is a concurrent work to ours which is also in the reviewing process. More importantly, the key novelties of Synthesizer and our PA-Transformer model are different. The Synthesizer paper develops a strategy to synthesize attention maps and argues that explicit token-token interaction is not that important. However, in this paper, we argue that we can improve the effectiveness of inter-token relationships through convolutional neural networks with residual connections. This claim is well supported by the ablation study on multiple datasets. We also perform ablation experiments on large-scale ImageNet dataset to make the conclusion more convincing, and the results are updated in Table 6 in the latest version. We have empirically compared PA-Transformer with Synthesizer on the GLUE benchmark and verified the superiority of PA-Transformer.  \n\nFrom the experimental perspective, we add more results to address your concerns (listed below). Hope these results can solve your concerns. We are open for more discussions if there are still questions or concerns from your side.  \n\n1. For the BERT-style fine-tuning experiments, we test more model backbones, including RoBERTa-Large and T5-base (see Table 2 in the revised version). The results show consistent improvement against vanilla baselines. In addition, the performance on T5-base is better than Synthesizer, although Synthesizer is pre-trained from scratch but PA-Transformer is only fine-tuned by the target task. This shows another practical advantage of PA-Transformer: getting superior performances without the need of pre-training from scratch again. \n\n2. For the ImageNet dataset, we include additional results with more model capacities (ResNet-34, ResNet-101) in the revised version.  Our idea is proven to be general for both NLP and CV domains, so we can expect its impacts to more tasks and domains in the future. For Synthesizer, we are currently not sure if it is applicable to the computer vision domain.\n\n", "title": "Clarify the novelty and add more convincing results"}, "13C7i0iSHqV": {"type": "rebuttal", "replyto": "XLTc-sefSjl", "comment": "1. We think the improvement can be largely explained by two factors: (1) a residual connection to facilitate attention map learning; (2) a convolutional module to learn generalized patterns of inter-token relationship. In a vanilla Transformer, the self-attentions in each layer all learned independently.  Assembly, a residual connection on the attention map will help the succeeding layer to take the attention knowledge from previous layers. Moreover, there may be common patterns and transition rules shared across different attention maps. We hypothesis that a convolutional layer on a (n*n) attention map could leverage the locality of a relationship matrix and improves the performance of Transformer by generating better inter-term relationships explicitly. Using a convolutional induction bias is reasonable because nearby terms may have similar relationships. Table 5 shows empirical support of this explanation. Res-Transformer (0-layer PA) uses only residual connections, which already shows stable improvement on various datasets. Furthermore, adding one or two convolutional layers demonstrates additional benefits consistently. \n\n2. As we have addressed above, residual connection is one of the reasons for performance improvement, but not all. As shown in Table 5, we empirically prove that the convolutional layers effectively learns a generalized function to produce better inter-token relationships based on existing attention maps.\n \n3. We try to give more in-depth analysis through case studies for both text and image application. \n(1) In Figure 2, we visualize related attention maps for the last layer in BERT-Base and PA-BERT-Base models for a case of grammar check. The sentence is \u201cMary tried John to go abroad.\u201d In Figure 2(a), BERT focuses on verbs and stop signs, leading to a misclassification. In contrast, PA-BERT learns to attend to the relationships between \u201ctried\u201d and \u201cJohn\u201d, which correctly captures the error part and gives a correct answer. The design of PA-Transformer to generalize inter-token relationships through convolutional inductive bias seems to be beneficial in this kind of scenario. More details are explained in Appendix C.3.1.\n(2) In Figure 1, we visualize a case of image classification. Again, PA-Conv-Transformer has a clearer attention map to show the horse than Conv-Transformer. As shown in the third column at Figure 1(b), the generated attention maps from convolutional modules highlights local key areas, so that self-attention could focus on complementary global information and collaboratively produce a better attention map. More details can be found in Appendix C.3.2.  \n \n4. Thanks for pointing out the miss of conclusion section. We have added more insight explanations, case study, and conclusion sections (some of them originally located in the appendix) in the 9-page main content. Hope this will make the paper more clear and solve your major questions and concerns. We are very pleased to have a further discussion with you if you have further comments or suggestions to our work. \n", "title": "Explain motivations and provide in-depth analysis"}, "IN_7RoUPm9D": {"type": "rebuttal", "replyto": "qcAYZpe-h7", "comment": "Thanks for your informative comments and suggestions for our work. We\u2019d like to address your major concerns and questions as follows. \n\n1. Multi-branch networks concatenate the output representations from transformer and convolutional encoders, where the convolution encoder takes text sequences as input. PA-Transformer, instead,  takes the n * n attention map as input for the convolutional layer, which is by design to model the general patterns of inter-term relationships explicitly. Through extensive experiments, we show that both designs are complementary and PA-Transformer still achieves significant improvement when applied to a multi-branch network architecture that already combines CNN and Transformer (see  PA-Conv-Transformer v.s. Conv-Transformer). The architecture of PA-Conv-Transformer is shown in Figure 4 in the appendix. \n\n2. The goal of the convolutional layer is to learn a generalized function that generates better attention maps based on the previous knowledge. We do not use a predictive loss directly because there is no ground truth for the best attention map. Alternatively, we expect the convolutional layer to be guided by the task-specific loss indirectly. If there is enough data, this goal is achievable and the empirical results have proved this design.\n\n3. We put the source code on Github using a new account for anonymity requirements. We have provided exemplar scripts for reproducing PA-BERT on GLUE benchmark and PA-AA-ResNet on ImageNet dataset. \nhttps://github.com/a-MLer/pa-transformer.\n\n4. For baseline of residual connection, one can refer to Table 5 in the Analysis section, where Res-Transformer (equals to 0-layer PA) is the setting using only residual connections without convolutional layers. We can see the residual connection itself shows benefits as expected, while adding one or two convolutional layers further improves the performance by a large margin. This indicates that the convolutional module effectively learns the generalized pattern of inter-token relationships, which is the major contribution of this paper. \n", "title": "Clarify the cons and provide more results and analysis in the revised version"}, "qcAYZpe-h7": {"type": "review", "replyto": "YQVjbJPnPc9", "review": "This paper proposes a novel approach to improve self-attention through by bridging the attention maps from different layers via a chain of convolution-based prediction modules.  In particular, it proposes to augment the existing works on Transformer through supplementary prediction modules by CNN-based attention prediction layers.  The main contribution of this paper is the introduction of CNN-based attention prediction to enhance model predictions. Empirical studies are performed to show the superiority of the proposed model PA-Transformer over several SOTA approaches on NLP and image classification tasks. \n\nReasons for score: \n \n I like the idea of a chain of attention prediction to learn attention dependencies from the previous block. My major concern is about the clarity of the paper. Hopefully, the authors can address my concern in the rebuttal period. \n\nPros: \n \n1. The paper addresses one of the most important issue of transformer: attention dependencies cross blocks or layers. For me, the problem itself is real and practical. \n2. The proposed predictive attention transformer (PA-Transformer) is novel for capturing the attention dependencies transformer layers and address the problem of the self-attention maps learned independently for each layer. The design for using the PA-Transformer to tasks of NLP and image classification is reasonable and interesting. \n3. This paper provides comprehensive experiments, including both NLP and image classification results, to show the effectiveness of the proposed framework.  \n\nCons: \n \n1. The paper claims that Multi-channel is one of the first works to take attention maps as multi-channel images for explicit modelling in the section of Introduction. It is better to clarity this point and give the difference between Multi-channel and multi-branch on method section.\n2. Why does the paper call the proposed model as predictive attention? From my understanding, it is a type of residual connection for attention. Are the attention results used to predict some kind of tasks in intermediate layers?\n3. Is the source codes available to reproduce the work? \n4. It would be more convincing if the authors can provide a set of experiments about a baseline just using residual connection to bridge the layers, instead of CNN-based attention map prediction module, in the rebuttal period. \n\nQuestions during rebuttal period: \n \nPlease address and clarify the cons above \n", "title": "Predictive Attention Transformer: Improving Transformer with Attention Map Prediction", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "XLTc-sefSjl": {"type": "review", "replyto": "YQVjbJPnPc9", "review": "\n\nThis paper's main topic is the enhancement of Transformer models for improving performance in a task-independent way.\n\n\nThis paper first points out that the attention maps are trained independently among layers in the Transformer models.\nThe authors then hypothesize that the performance could be improved by integrating an additional module for estimating attention maps.\nTherefore, they propose a method that yields the attention maps based on the lower layer's attention maps.\n\nThe experimental results on several different datasets from different domains show that the proposed method consistently improves the performance.\nThey are somewhat surprising results.\n\n\nThe following are the questions and concerns of this paper.\n\n\n1,\nThe proposed method seems rather strange; why can the proposed method yield better attention map predictions?\n\nI understand such a phenomenon if we provide the correct attention maps for model training.\nHowever, it seems that the proposed method does not require any additional information on correct attention maps. \nThis is the largest mystery for me about this method.\nPlease elaborate on what architecture or mechanism enables the proposed model to provide better attention maps theoretically or empirically to support the authors' claim?\nIf I did not miss something, there are no clear explanations about it.\n\n\n\n2, \nthe source of the effectiveness:\nThis additional module seems to also work as a sort of skip or short cut connections between layers.\nIn my feeling, the performance gain could be just the direct linking between attention mechanisms in each layer and not be caused by a better attention map prediction.\nAs a recent common knowledge in the community, the correct attention map can be, but not necessarily, a strong correlation to performance.\nPlease reveal the actual source of the performance gain to prove the correctness of the authors' claim.\nOtherwise, there may be a risk of providing the wrong knowledge to the community.\n\n\n\n3, \nRelated to the above two questions, this paper's main concern is that this paper does not provide more in-depth analyses of the proposed method that tell model behaviors or characteristics.\nThere are no intuitive and motivational examples of what kind of situation the proposed method successfully works.\n\n\n\n\n4,\nThe current version does not have the \"Conclusion\" section, which most scientific papers have.\nOf course, there is no rule that the paper always needs to have a Conclusion section.\nHowever, I would like to know why the authors decided not to provide the Conclusion section.\n\n\nI am willing to change my score if I got reasonable answers for all the questions and concerns written in the above reviews.\n\n\n", "title": "unclear why the proposed method improves performance", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}