{"paper": {"title": "Overcoming the Disentanglement vs Reconstruction Trade-off via Jacobian Supervision", "authors": ["Jos\u00e9 Lezama"], "authorids": ["jlezama@fing.edu.uy"], "summary": "A method for learning image representations that are good for both disentangling factors of variation and obtaining faithful reconstructions.", "abstract": "A major challenge in learning image representations is the disentangling of the factors of variation underlying the image formation.  This is typically achieved with an autoencoder architecture where a subset of the latent variables is constrained to correspond to specific factors, and the rest of them are considered nuisance variables. This approach has an important drawback: as the dimension of the nuisance variables is increased, image reconstruction is improved, but the decoder has the flexibility to ignore the specified factors, thus losing the ability to condition the output on them.  In this work, we propose to overcome this trade-off by progressively growing the dimension of the latent code, while constraining the Jacobian of the output image with respect to the disentangled variables to remain the same.  As a result, the obtained models are effective at both disentangling and reconstruction.  We demonstrate the applicability of this method in both unsupervised and supervised scenarios for learning disentangled representations. In a facial attribute manipulation task, we obtain high quality image generation while smoothly controlling dozens of attributes with a single model. This is an order of magnitude more disentangled factors than state-of-the-art methods, while obtaining visually similar or superior results, and avoiding adversarial training.", "keywords": ["disentangling", "autoencoders", "jacobian", "face manipulation"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a new way to tackle the trade-off between disentanglement and reconstruction, by training a teacher autoencoder that learns to disentangle, then distilling into a student model. The distillation is encouraged with a loss term that constrains the Jacobian in an interesting way. The qualitative results with image manipulation are interesting and the general idea seems to be well-liked by the reviewers (and myself).\n\nThe main weaknesses of the paper seem to be in the evaluation. Disentanglement is not exactly easy to measure as such. But overall the various ablation studies do show that the Jacobian regularization term improves meaningfully over Fader nets. Given the quality of the results and the fact that this work moves the needle in an important (albeit hard to define) area of learning disentangled representations, I think would be a good piece of work to present at ICLR so I recommend acceptance."}, "review": {"SyxL9vbI27": {"type": "review", "replyto": "Hkg4W2AcFm", "review": "This paper proposed a novel approach for learning disentangled representation from supervised data (x as the input image, y as different attributes), by learning an encoder E and a decoder D so that (1) D(E(x)) reconstructs the image, (2) E(D(x)) reconstruct the latent vector, in particular for the vectors that are constructed by mingling different portion of the latent vectors extracted from two training samples, (3) the Jacobian matrix matches and (4) the predicted latent vector matches with the provided attributes. In addition, the work also proposes to progressively add latent nodes to the network for training. The claim is that using this framework, one avoid GAN-style training (e.g., Fader network) which could be unstable and hard to tune. \n\nAlthough the idea is interesting, the experiments are lacking. While previous works (e.g., Fader network) has both qualitative (e.g., image quality when changing attribute values) and quantitative results (e.g., classification results of generated image with novel combination of attributes), this paper only shows visual comparison (Fig. 4 and Fig. 5), and its comparison with Fader network is a bit vague (e.g., it is not clear to me why Fig. 5(e) generated by proposed approach is \u201cmore natural\u201d than Fig. 5(d), even if I check the updated version mentioned by the authors' comments). Also in the paper there are five hyperparameters (Eqn. 14) and the center claim is that using Jacobian loss is better. However, there is no ablation study to support the claim and/or the design choice. From my opinion, the paper should show the performance of supervised training of attributes, the effects of using Jacobian loss and/or cycle loss, the inception score of generated images, etc. \n\nI acknowledge the authors for their honesty in raising the issues of Fig. 4, and providing an updated version. ", "title": "Need more quantitative experiments to justify the claims. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1gqJD2u2X": {"type": "review", "replyto": "Hkg4W2AcFm", "review": "Summary: The paper proposes a method to tackle the disentanglement-reconstruction tradeoff problem in many disentangling approaches. This is achieved by first training the teacher autoencoder (unsupervised or supervised) that learns to disentangle the factors of variation at the cost of poor reconstruction, and then distills these learned representations into a student model with extra latent dimensions, where these extra latents can be used to improve the reconstructions of the student autoencoder compared to the teacher autoencoder. The distillation of the learned representation is encouraged via a novel Jacobian loss term that encourages the change in reconstructions of the teacher and student to be similar when the latent representation changes. There is one experiment for progressive unsupervised disentangling (disentangling factor by factor) on MNIST data, and one experiment for semi-supervised disentangling on CelebA-HQ.\n\nPros:\n- I think the idea of progressively capturing factors of variation one by one is neat, and this appears to be one of the first successful attempts at this problem.\n- The distillation appears to work well on the MNIST data, and does indeed decrease the reconstruction loss of the student compared to the teacher.\n- The qualitative results on CelebA-HQ look strong (especially apparent in the video), with the clear advantage over Fader Networks being that the proposed model is a single model that can manipulate the 40 different attributes, whereas Fader Nets can only deal with at most 3 attributes per model.\n\nCons:\n- There are not enough quantitative results supporting the claim that the model is \u201ceffective at both disentangling and reconstruction.\u201d The degree of disentanglement in the representations is only shown qualitatively via latent interpolation, and only for a single model. Such qualitative results are generally prone to cherry-picking and it is difficult to reliably compare different disentangling methods in this manner. This calls for quantitative measures of disentanglement. Had you used a dataset where you know the ground truth factors of variation (e.g. dSprites/2D Shapes data) for the unsupervised disentangling method, then the level of disentanglement in the learned representations could be quantified, and thus your method could be compared against unsupervised disentangling baselines. For the semi-supervised disentanglement example on CelebA, you could for example quantify how well the encoder predicts the different attributes (because there is ground truth here) e.g. report RMSE of the y_i\u2019s on a held out test set with ground truth. A quantitative comparison with Fader Networks in this manner appears necessary. The qualitative comparison on a single face in Figure 5 is nowhere near sufficient.\n- There is quantitative evidence that the reconstruction loss decreases when training the student, but here it\u2019s not clear whether this quantitative difference makes a qualitative difference in the reconstructions. Getting higher fidelity images is one of the motivations behind improving reconstructions, so It would be informative to compare the reconstructions of the teacher and the student on the same image.\n- In the CelebA experiments, the benefit of student training is not visible in the results. In Figure 5 you already show that the teacher model gives decent reconstructions, yet you don\u2019t show the reconstruction for the student model (quantitatively you show that it improves in Figure 3b, but again it is worth checking if it makes a difference visually). Also it\u2019s not clear whether Figure 4 are results from the student model or the teacher model. I\u2019m guessing that they are from the student model.\n- These quantitative results could form the basis of doing ablation studies for each of the different losses in the additive loss (for both unsupervised & semi-supervised tasks). Because there are many components in the loss, with a hyperparameter for each, it would be helpful to know what losses the results are sensitive to for the sake of tuning hyperparameters. This would be especially useful should I wish to apply the proposed method to a different dataset.\n- I think the derivation of the Jacobian loss requires some more justification. The higher order terms in the Taylor expansion in (2) and (3) can only be ignored when ||y_2 - y_1|| is small compared to the coefficients, but there is no validation/justification regarding this.\n\nOther Qs/comments:\n- On page 5 in the last paragraph of section 3, you say that \u201cAfter training of the student with d=1 is finished, we consider it as the new teacher\u201d. Here do you append z to y when you form the new teacher?\n- On page 6 in the paragraph for prediction loss, you say \u201cThis allows the decoder to naturally \u2026. of the attributes\u201d. I guess you mean this allows the model to give realistic interpolations between y=-1 and 1?\n- bottom of page 6: \u201cHere we could have used any random values in lieu of y_2\u201d <- not sure I understand this?\n- typo: conditionnning -> conditioning\n- I would be inclined to boost the score up to 7 if the authors include some quantitative results along with more thorough comparisons to Fader Networks\n\n************ Revision ***********\nThe authors' updates include further quantitative comparisons to Fader Networks and ablation studies for the different types of losses, addressing the concerns I had in the review. Hence I have boosted up my score to 7.", "title": "Idea is neat and qualitative results are impressive, but the paper is quite lacking in quantitative results and comparisons to other methods.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyxiD3BG0Q": {"type": "rebuttal", "replyto": "r1gqJD2u2X", "comment": "Thank you very much for your detailed review.\n\nWe answer each item below.\n\n> \"There are not enough quantitative results [...]\"\n\nWe added quantitative comparisons for both the unsupervised and supervised\ntasks.  The quantitative measure consists in evaluating, via an external\nclassifier, how well the latent units condition the specified factor of\nvariation in the generated image.  \n\nIn the MNIST example we measure how well the first two latent units can\nmanipulate the digit class in the images generated by the student models.  The\nresults are presented in the new Table 1, showing that the student with Jacobian\nsupervision obtains a better trade-off between disentanglement and reconstruction.\n\nIn the facial attribute manipulation task we used a pre-trained attribute\nclassifier provided by the authors of Fader Networks. Using the classifier, we\nmeasure if by manipulating the latent unit corresponding to one attribute we can\nchange the presence or absence of that attribute in the generated image. We do\nthis for all attributes and for all images in the test set. The results are\nshown in Table 2 and Figure 4.\n\nFor comparison, we trained two Fader Networks models to manipulate all\nattributes. The training did not converge and the resulting manipulation and\nreconstruction performance is inferior to our method. Besides the quantitative\ncomparison, this can also be seen qualitatively in the new Figures 7 and 8.\n\n\n> \"[...] compare the reconstructions of the teacher and the student on the same\n  image.\"\n\nWe added a new figure to the appendix showing a comparison between the\nreconstructions obtained by the teacher and by the student (new Figure 7). It\nshows that the student model is better at reconstructing fine image details.  The\ncomparison also includes a Fader Networks model trained to manipulate multiple\nattributes, and show that its reconstruction is distorted.\n\n> \"Also it\u2019s not clear whether Figure 4 are results from the student model or\n  the teacher model. [...]\"\n\nSorry for this lack of clarity. Figure 4 shows results by the student model\ntrained with Jacobian supervision. We clarified this in the manuscript.\n\n> \"[...]  ablation studies for each of the different losses [...]\"\n\nWe added ablation studies for both unsupervised and supervised tasks in the new\nsection A.3 in the appendix (page 14).  Unless otherwise noted, the weighs of\nthe losses were found by evaluation on separate validation sets.\n\n> \"[...] The higher order terms in the Taylor expansion in (2) and (3)\n  can only be ignored when ||y_2 - y_1|| is small [...]\"\n\nIndeed, because of the higher order terms, even assuming (5) and (6) hold, (7)\nis only an approximation. Note however that the norm of the approximation error\nin (7) is that of the difference between the higher order terms of the teacher\nand the student, namely ||o^T(||y_2-y_1||) - o^S(||y_2-y_1||)||. This might be\nlower than the individual higher order terms, especially if both decoders\nrespond similarly to variations in $y$.  Currently, our justification is mainly\nempirical. We also considered weighing the loss by a factor reciprocal\nto ||y_2-y_1||, to give less importance to pairs of samples for\nwhich ||y_2-y_1|| is large.  Another option we contemplated is, for the Jacobian\nsupervision, to consider a blurred version of the student, so that it has the\nlow resolution of the teacher. The formulation still holds and this would also\nmake (6) easier to enforce. In informal experiments we observed no significant\nadvantage w.r.t. the current approach, which is simpler.  We\nleave these possible avenues of improvement as future work.\n\n> \"[...] you say that \u201cAfter training of\n  the student with d=1 is finished, we consider it as the new teacher\u201d. Here do\n  you append z to y when you form the new teacher?\"\n\nYes this is correct. We clarified this in the text. \n\n> On page 6 in the paragraph for prediction loss, you say \u201cThis allows the\n  decoder to naturally \u2026\" of the attributes\u201d. I guess you mean this allows the\n  model to give realistic interpolations between y=-1 and 1?\n\nWe intended to say that we do not require the prediction to be binary values, as\nif we used the cross-entropy loss, but any real value. Thus, the decoder can\nread the amount of attribute variation from this variable, and not only if the\nattribute is present or not.\n\n> \"[...] \u201cHere we could have used any random values in lieu of y_2\u201d [...]\"\n\nWe wanted to say that the $y$ part in the fabricated latent code could be\nrandom, but instead we sample it from the data (copy from another sample).  \nWe clarified this in the text.\n\n> \"typo: conditionnning -> conditioning\"\n\nThank you.\n\n> \"I would be inclined to boost the score up to 7 if the authors include some\n  quantitative results along with more thorough comparisons to Fader Networks\"\n\nThank you. We hope the additional quantitative and qualitative results can\nconvince you of the superior performance of our method with respect to Fader\nNetworks, for multiple attributes manipulation.\n", "title": "Authors' response"}, "rJlI1wrzAX": {"type": "rebuttal", "replyto": "SyxL9vbI27", "comment": "Thank you very much for reviewing our work.\n\n\nTo address your main concern, we added quantitative comparisons by using external\nclassifiers to assess the conditioning of the disentangled factors. \n\nWe believe the new quantitative results strongly support our two main claims:\n1) Our model outperforms Fader Networks by achieving better reconstruction and\n   multiple attribute manipulation.\n2) Once a disentangling teacher model has been obtained, the proposed Jacobian\n   loss allows to add latent units that help improving the reconstruction while\n   maintaining the disentangling.\n\n\nWe address each of your concerns below.\n\n\n> \"e.g., it is not clear to me why Fig. 5(e) generated by proposed approach is\n\u201cmore natural\u201d than Fig. 5(d)\" \n\nWe realize that this is a very subjective remark so we removed this claim from\nthe image caption.  The intent of Fig. 5 is to show that even for single\nattribute manipulation and reconstruction, our proposed method performs similar\nor better than Fader Networks. For multiple attributes, a Fader Network model\ndoes not converge and has a poorer reconstruction and attribute manipulation\nperformance. Besides the new quantitative results in Table 2 and Figure 4, this\nis also shown qualitatively in the new Figures 7 and 8 in the appendix.\n\n> \"Also in the paper there are five hyperparameters (Eqn. 14) and the center\nclaim is that using Jacobian loss is better. However, there is no ablation study\nto support the claim and/or the design choice.\"\n\nWe show quantitatively in the new Table 2 and Figure 4 that using the Jacobian\nsupervision performs better than the cycle-consistency loss, in terms of the\ndisentanglement versus reconstruction trade-off. To measure the disentangling\nperformance of the models, we manipulate the latent variables aiming to change\nthe presence or absence of each attribute, and check with an external classifier\nthat the attribute is indeed changed. We used a pre-trained classifier provided\nby the authors of Fader Networks.\n\n> \"From my opinion, the paper should show the performance of\nsupervised training of attributes, the effects of using Jacobian loss and/or\ncycle loss, the inception score of generated images, etc.\"\n\nWe included ablation studies in the appendix (new Section A.3, page 14). These\nshow the separate and combined use of Jacobian and cycle-consistency losses for\ntraining the student (Table 5). Their combination actually works OK. For the\nsake of simplicity we keep only the Jacobian loss, and the cycle-consistency\nloss is only used to train the disentangling by the teacher.\n\nNote that by using an external classifier, the measure we obtain is in some\nsense similar to an inception score.\n", "title": "Authors' response"}, "HklRMVBzA7": {"type": "rebuttal", "replyto": "BJxo72kq3X", "comment": "Thank you very much for reviewing our work.\n\nWe chose MNIST for the unsupervised disentangling experiment because the two\nprincipal factors of variation are related to the digit class and thus it served\nas a very good pedagogic example.\n\nTo address your first concern, we conducted further experiments for the\nunsupervised disentanglement on the Street View House Numbers (SVHN)\ndataset. The results are shown in the appendix (Section A.5, page 17).  In this\ncase, the two principal factors are related to the shading of the digit image\nand not to the class.  However, we found that later in the progressive discovery\nof factors of variation, the algorithm learns factors that are quite related to\nthe digit class (ninth and tenth factors). Then, the final student model is able\nto manipulate the class of the digit while approximately maintaining the style\nof the digit (Figure 11).\n\nTo address your second concern, we added quantitative experiments for the\nunsupervised example of Section 3 (new Table 1). These were obtained by using an\nexternal MNIST classifier to assess the digit class manipulation. The results\nshow that the Jacobian supervision indeed allows a more advantageous traversing\nof the disentanglement versus reconstruction trade-off.\n\nFinally, we also added quantitative results for the CelebA experiments, showing\nthe advantage of our method with respect to Fader Networks (new Table 2 and\nFigure 4).\n", "title": "Authors' response"}, "S1lLdzSG0X": {"type": "rebuttal", "replyto": "Hkg4W2AcFm", "comment": "We thank the reviewers for their constructive comments which helped us\nto significantly improve our submission.\n\n\nWe did the following modifications to address the reviewers concerns:\n\n1) We addressed the lack of quantitative results, which was an\nimportant concern shared among all reviewers. By using external classifiers on\nthe generated images, we were able to assess the degree of disentangling and\nconditioning of the models and thus we were able to consistently quantify their\ntrade-off between disentanglement and reconstruction.\n\nWe believe the resulting quantitative results further support our approach.  In\nparticular, we quantitatively demonstrate superior performance to Fader\nNetworks in the facial attribute manipulation task.\n\n2) We extended the unsupervised experiments by including results on the SVHN\n   dataset (Section A.5 in the appendix, page 17).\n\n3) We added further qualitative comparison with Fader Networks on image\n   reconstrucion and attributes manipulation (Section A.4 in the appendix, page\n   15).\n\n4) We added ablation studies for the different components in the loss functions\n   (Section A.3 in the appendix, page 14).\n\n5) We replaced Figure 3(b) by a more informative graph showing the traversal of\n   the disentanglement-reconstruction trade-off in the new Figure 4.\n\n\n\nBesides the modifications suggested by the reviewers, we also did the following\nchanges:\n\n6) We made minor modifications to the manuscript aiming to improve our\n   exposition.\n\n7) We use a model with different hyperparameters in Figure 1 and we corrected\n   the values of two hyperparameters in the model of Section 4.\n\n8) We added one missing reference (Burgess et al., 2018, NIPS workshops).\n\n9) We moved  Table 3 to the appendix.\n", "title": "Revised version"}, "BJxo72kq3X": {"type": "review", "replyto": "Hkg4W2AcFm", "review": "The paper aims to learn an autoencoder that can be used to effectively encode the known attributes/ generative factors and this allows easy and controlled manipulation of the images while producing realistic images.\n\nTo achieve this, ordinarily, the encoder produces latent code with two components y and z where y are clamped to known attributes using supervised loss while z is unconstrained and mainly useful for good reconstruction. But his setup fails when z is sufficiently large as the decoder can learn to ignore y altogether. Smaller sized z leads to poor reconstruction.\n\nTo overcome this issue, the authors propose to employ a student teacher training paradigm. The teacher is trained such that the encoder only produces y and the decoder that only consumes y. This ensures good disentanglement but poor reconstruction. Subsequently, a student autoencoder is learned which has a much larger latent code and produces both y and z. The y component is mapped to the teacher encoder\u2019s y component using Jacobian regularization.\n\nPositives:\nThe results of image manipulation using known attributes is quite impressive. The authors propose modifications to the Jacobian regularization as simple reconstruction losses for efficient training. The approach avoids adversarial training and thus is easier to train.\n\nNegatives:\nUnsupervised disentanglement results are only shown for MNIST. I am not convinced similar results for unsupervised disentanglement can be obtained on more complex datasets. Authors should include some results on this aspect or reduce the emphasis on unsupervised disentanglement. Also when studying this quantitative evaluation for disentanglement such as in beta-VAE will be nice to have.\n\nTypos:\npage 3: tobtain -> obtain\npage 5: conditionning -> conditioning ", "title": "Nice results on image manipulation", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rygfJ69oqm": {"type": "rebuttal", "replyto": "B1xizrM-q7", "comment": "(1) In equation (1) y_i refers to an arbitrary dimension in the input space of the\ndecoders. Both T and S decoders have the same input space for the specified\nvariables, namely $\\mathds{R}^k$.  In the paper we use the superscript when we\nwant to indicate the value was produced by one of the encoders.\n\n(2) Please refer to our answer to item (5) below for a quantitative\ncomparison. Yes, epoch 0 in Fig.1 (d) corresponds to the teacher. We will\nclarify it.\n\n(5) We quantified the level of disentanglement as follows: we evaluated how well\nthe first two hidden variables ($k$=2), maintain the encoding of the digit class\nin the student models. We take two images of different digits from the test set,\nfeed them to the encoder, swap their corresponding 2D subpart of the latent code\nand feed the fabricated latent codes to the decoder. We then run a pre-trained\nMNIST classifier in the generated image to see if the class was correctly\nswapped.\n\n| model                             | $d$ | recons. MSE | swaps OK |\n|---------------------------------+-----+---------------+-----------|\n| teacher                             |   0 |     3.66e-2 |    80.6% |\n| student w/ Jac. sup. (*) |  14 |     1.38e-2 |    57.2% |\n| student wo/ Jac. sup.     |  14 |     1.12e-2 |    32.0% |\n| student wo/ Jac. sup      |  10 |     1.40e-2 |    41.4% |\n|---------------------------------|------|--------------|------------|\n| random weights             |  14 |     1.16e-1 |     9.8% |\n\nWe observe that at the same level of reconstruction performance (~1.4e-2), the\nstudent with Jacobian supervision maintains a better disentangling of the class\n(under this metric) than the student without it. We will include a figure\nshowing that the reconstruction-disentanglement trade-off traversed by varying\n$d$ is indeed more advantageous for our model. Note that the first two variables\ndo not encode perfectly the digit class. This advantage in the trade-off is much\nlarger in the application of Section 4.\n\n(*) Note: this model was trained with $\\lambda_{diff} = 0.1$ instead of $1.0$ as\nthe one currently in the paper. The figure will be updated for this model.\n\n(4) We evaluated the disentangling measure (described in (5)), on the\nMNIST test set, for the student with Jacobian supervision:\n\n| xcov weight | $d$ | recons. MSE | swaps OK |\n|-------------+-----+-------------+----------|\n|        1e-3 |  14 |     1.38e-2 |    57.2% |\n|        1e-2 |  14 |     1.46e-2 |    56.3% |\n|        1e-1 |  14 |     1.49e-2 |    56.6% |\n\n(6) Thank you for remarking this important point.  In this paper we use the\nword disentangling to refer to both aspects:\n\na) each latent unit in the specified part is sensitive to one generative factor\nb) the value of each of these latent units conditions the generated output such\nthat it varies the corresponding generative factor\n\nWe will clarify this in the manuscript and revise the text to make sure it is\ncoherent.\n\n(7) See item (8)\n\n(8) We evaluated quantitatively how well the output is conditioned to the specified\nfactors, similarly to the procedure described in item (5). To do this, for each\nimage in the CelebA test set, we tried to flip each of the 32 disentangled\nattributes, one at a time (e.g. eyeglasses/no eyeglasses). We did the flipping\nby setting the latent variable y_i to sign(y_i)*-1*\\alpha, with \\alpha >0 a\nmultiplier to exaggerate the attribute, found in a separate validation set for\neach model (\\alpha=40 for all).\n\nTo verify that the attribute was indeed flipped in the generated image, we used\nan external classifier trained to predict each of the attributes. We used the\nclassifier provided by the authors of Lample et al. (2017), which was trained\ndirectly on the CelebA dataset.\n\nThe results are as follows:\n\n| model                           |  $d$ | flips OK | recons. MSE |\n|------------------------------+--------+------------+---------------|\n| teacher                        | 2048  |    73.1% |     1.82e-3 |\n| student w/ Jac. sup.   | 8192 |    72.2% |     1.08e-3 |\n| student wo/ Jac. sup. | 8192 |    42.7% |     1.04e-3 |\n|------------------------------+--------+------------+---------------|\n| Lample et al., 2017     | 2048 |    43.1% |     3.08e-3 |\n| random weights         | 2048 |    20.2% |     1.01e-1 |\n\nAt approximately the same reconstruction performance, the student with Jacobian\nsupervision is significantly better at flipping attributes than the student\nwithout it. \n\nWe also trained a Fader Networks model (Lample et al., 2017) with the same\nhyperparameters and training epochs as our teacher model. The result suggests\nthat the adversarial discriminator acting on the latent code harms the\nreconstruction and that the conditionning is worse than with our teacher model.\n\n(9) We will add to the appendix the result of trying the same experiment as in\nFigure 4, but using the student model without Jacobian supervision. It will be\nclear from this experiment that the latter cannot effectively control most of\nthe attributes.\n", "title": "thank you for your interest"}}}