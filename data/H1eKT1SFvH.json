{"paper": {"title": "Towards Effective 2-bit Quantization: Pareto-optimal Bit Allocation for Deep CNNs Compression", "authors": ["Zhe Wang", "Jie Lin", "Mohamed M. Sabry Aly", "Sean I Young", "Vijay Chandrasekhar", "Bernd Girod"], "authorids": ["mark.wangzhe@gmail.com", "lin-j@i2r.a-star.edu.sg", "msabry@ntu.edu.sg", "sean.i.young@stanford.edu", "vijay@i2r.a-star.edu.sg", "bgirod@stanford.edu"], "summary": "", "abstract": "State-of-the-art quantization methods can compress deep neural networks down to 4 bits without losing accuracy. However, when it comes to 2 bits, the performance drop is still noticeable. One problem in these methods is that they assign equal bit rate to quantize weights and activations in all layers, which is not reasonable in the case of high rate compression (such as 2-bit quantization), as some of layers in deep neural networks are sensitive to quantization and performing coarse quantization on these layers can hurt the accuracy. In this paper, we address an important problem of how to optimize the bit allocation of weights and activations for deep CNNs compression. We first explore the additivity of output error caused by quantization and find that additivity property holds for deep neural networks which are continuously differentiable in the layers. Based on this observation, we formulate the optimal bit allocation problem of weights and activations in a joint framework and propose a very efficient method to solve the optimization problem via Lagrangian Formulation. Our method obtains excellent results on deep neural networks. It can compress deep CNN ResNet-50 down to 2 bits with only 0.7% accuracy loss. To the best our knowledge, this is the first paper that reports 2-bit results on deep CNNs without hurting the accuracy.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete. However, the main concern is that the paper is very similar to a recent work by the authors, which is not cited."}, "review": {"rJlLBT1Yjr": {"type": "rebuttal", "replyto": "H1eKT1SFvH", "comment": "\nWe thank all reviewers for their careful reviews, insightful comments and feedback on our paper. The draft has been revised accordingly. The revised draft has been uploaded. The main revisions are summarized below:\n\n-\tWe cited the ICIP paper mentioned by Reviewer 1 and discussed the differences with the ICIP paper in the related work section. We added the results of the ICIP paper in Table 1 and compared our method with the ICIP paper in the experiment section. (response to Reviewer 1)\n-\tWe added a section (section 6.1) to clarify the computational complexity of our method. We made a comparison of the amount of arithmetic operations to the equally quantized method in section 6.1. (response to Reviewer 2)\n-\tThe optimization method is elaborated in section 4.2. A figure is also added to show an example of the optimization method (Fig. 3). (response to Reviewer 2)\n-\tWe discussed the relationship between the variances and the bitrates in the supplementary material. (response to Reviewer 3)\n\nHere we summarize the main differences with the ICIP paper mentioned by Reviewer 1:\n\n-\tOur ICLR submission shows the additivity property of the output error and provides a mathematical derivation for the additivity property. \n-\tOur quantization framework differs from the ICIP paper in two-fold. First, we adopt a dead zone to the quantization function of weights. Second, we provide a scheme to support the retraining for both quantized weights and activations.\n-\tOur work demonstrates that the optimal bit allocation solved by our method has very positive impacts on inference speed, which has been verified by the hardware simulation experiments.\n-\tBy combining the dead-zone quantization and STE based retraining with the optimal bit allocation strategy, our quantization framework achieves state-of-the-art results. To the best of our knowledge, our work is the first to report 2-bit results on deep neural network ResNet-50 without hurting the accuracy on ImageNet.\n\nBest,\nAuthors\n", "title": "Summary of Revisions"}, "SkgvAjJKsr": {"type": "rebuttal", "replyto": "rJx2sPqAFS", "comment": "We revised the draft based on the comments:\n-\tWe added section 6.1 to clarify the computational complexity of our method. \n-\tWe made a comparison of the amount of arithmetic operations to the equally quantized method in section 6.1. \n-\tThe optimization method is elaborated in section 4.2. A figure is also added to show an example of the optimization method (Fig. 3). \nThe updated version has been uploaded.\n", "title": "Revisions to the Draft"}, "rJehZskYoB": {"type": "rebuttal", "replyto": "HJeAzEJQqS", "comment": "We revised the draft based on the comments: \n-\tWe cited the ICIP paper and discussed the differences with the ICIP paper in the related work section. \n-\tWe added the results of the ICIP paper in Table 1 and compared our method with the ICIP paper in the experiment section. \nThe updated version has been uploaded. \n", "title": "Revisions to the Draft"}, "B1etb9GNjr": {"type": "rebuttal", "replyto": "rylTYKApFB", "comment": "\nThank you for the careful reviews and for the comments. We answer your questions below.\n\nQ1: Do the authors mean to conclude that layers with large number of weights hold a lot of redundancy and don't have a significant impact on the overall accuracy of the model? This needs to be clarified further.\n\nWe empirically found that the layers having a larger number of weights receive lower bitrates (and vice-versa). The reason could be the values of the variances of the layers. According to the classical Lagrangian rate-distortion formulation, the optimal bit allocation follows a rule,\n\nRate = G( - 1 / sigma^2 ), \n\nwhere G(.) is a strictly increasing function and sigma^2 is the variance of the variables. Based on the rule above, the layers with larger variances receive larger bitrates (and vice-versa). \n\nWe calculated the variances of layers for two deep networks ResNet-50 and MobileNet-v1 (see Table 1 and Table 2 below). The results show that the layers with a smaller number of weights typically have a larger variance, and thus these layers receive larger bitrates. We added a paragraph and a figure in the supplementary material to discuss the relationship between variances and bitrates. The draft has been updated accordingly. Thank you for this good question.\n\nTable 1 \u2013 Variances of Weights across Layers on ResNet-50\n+-------------------------+----------+----------+----------+-----------+------------+\n|      Layer Index      |      5     |      15   |     25   |      35     |      45     |\n+-------------------------+----------+----------+----------+-----------+------------+\n| #Weights (10^5)   |  0.16   |    1.47  |   2.62  |    2.62    |    23.6    |\n+-------------------------+----------+----------+----------+-----------+------------+\n|    Variance (10^3) |  1.33    |    0.59  |   0.51   |    0.37   |    0.11    |   \n+-------------------------+----------+----------+----------+-----------+------------+\n|        Bitrate             |    4.2    |    3.5   |     3.3   |     2.8    |     1.2      |\n+-------------------------+----------+----------+----------+-----------+------------+    \n\nTable 2 \u2013 Variances of Weights across Layers on MobileNet-v1\n+-------------------------+----------+----------+----------+-----------+------------+\n|      Layer Index      |      2     |      8     |    14     |     20     |      26     |\n+-------------------------+--------- +----------+----------+-----------+------------+\n| #Weights (10^7)   |   0.29  |   1.15   |   4.61   |   4.61    |     9.22   |\n+-------------------------+----------+----------+----------+-----------+------------+\n|      Variance            |   7.83  |    0.49  |  0.54   |   0.23    |    0.07     |\n+-------------------------+----------+----------+----------+-----------+------------+\n|       Bitrate              |   7.9    |   5.38   |   5.86   |  5.29     |    4.1       |\n+-------------------------+----------+----------+----------+-----------+------------+\n", "title": "Response to Reviewer #3 Questions"}, "SyeB8pofiS": {"type": "rebuttal", "replyto": "Skg_McpoPH", "comment": "Thank you for your interest in this paper and for your comments. Below we answer your questions.\n\n\nQ1: Uniformity of quantization\n\nDead zone quantization can also support integer arithmetic if we set the values of the first negative and the first positive quantization centroids as k * delta, where k is an integer value and delta is the length of quantization interval. By doing this, every quantization centroid becomes an integral multiple of delta and we can use the integral multiplier for integer arithmetic.\n\nFor example, if we set k = 2, the corresponding quantization centroids are:\n\n ... , -n * delta , ... , -3 * delta , -2 * delta , 0 , 2 * delta , 3 * delta , ... , n * delta , ...\n\nand the integral multipliers \"... , -n , ... , -3 , -2 , 0 , 2 , 3 , ... , n , ...\" can be used for integer arithmetic.\n\nWe will introduce how to apply integer arithmetic with dead zone quantization and update the paper accordingly. Thank you for this good point.\n\n\nQ2: Computational complexity vs. memory requirements\n\nWe calculated the amount of arithmetic operations of our method required to perform a single inference on ResNet-50, and did a comparison with the equally quantized method (please see our response to Q1 of Reviewer #2). The results show that our method has fewer operations than the equally quantized method at 4 bits and 6 bits. While, at 2 bits, our method has 1.4x more operations than the equally quantized method.  \n\nIn practice, the inference time on hardware devices is constrained by both compute and memory access. The Pareto-optimal bit allocation tends to allocate fewer bits per weight for layers that have a lot of weights. As a result, it helps to reduce the corresponding memory-access time which in turn reduces compute idle time and improves the overall inference speed. The simulation results on the Google TPU platform show that our method is 1.5x faster than the equally quantize method on ResNet-50 at 2 bits. Please see our response to Reviewer #2 for the details. \n", "title": "Response to Question 1 and 2"}, "SkxS-gdzjH": {"type": "rebuttal", "replyto": "BklwCkdGoH", "comment": "\nQ2: I wish the actual optimization part briefly mentioned in section 4.2 could be elaborated more. It is a crucial part but somewhat understated. \n \nThank you for this suggestion. We will elaborate section 4.2 to show the optimization steps in more details. We will respond to this comment again once we finish the revision.\n \n\nQ3: I also wonder what\u2019s the effect or limitation of using MSE for this optimization, where cross-entropy is a more suitable choice. I know that the objective function in eq 5 is just to find the best combination of bit allocations per layer, but still, the error space might not be the best for this classification problem. \n \nWe agree that MSE does not directly optimize accuracy and thus may not be the best choice for classification problem. The reason we choose MSE as the measurement of the quantization impact is mainly because it ensures that the additivity property of output error holds, from both empirical observations and mathematical derivations (as shown in the draft), and the additivity property is essential for Pareto condition. Besides, optimization with MSE not only supports classification tasks but also can be applied to any other tasks like object detection and semantic segmentation where regression loss is also required.\n\n+-----------------------+-----------+----------+-----------+-----------+\n |            size          |  4 bits   | 6 bits  |  8 bits   | 10 bits  | \n+-----------------------+-----------+----------+-----------+-----------+ \n |  cross-entropy  |    41.3   |  43.6    |   46.0    |   57.0    |\n+-----------------------+-----------+----------+-----------+-----------+\n |   MSE                  |   63.6    |   70.8   |    70.9   |   70.9    |\n+-----------------------+-----------+----------+-----------+-----------+    \n\nCross-entropy is a more suitable choice for classification, but our empirical observations show that it is not compatible with the Pareto optimal bit allocation framework. The table above shows the results on MobileNet-v1 when replacing MSE in Eq. 3 with cross-entropy for optimization. One can see that there is a noticeable accuracy drop using cross-entropy in the optimization framework. We also observed that the additivity property doesn\u2019t hold anymore if we use cross-entropy as the measurement. From the mathematical point of view, it is unclear whether or not the additivity property is still valid for metrics beyond MSE, we would like to leave it for future study. Thank you for this insight.", "title": "Response to Reviewer #2 Comments (part 2)"}, "BklwCkdGoH": {"type": "rebuttal", "replyto": "rJx2sPqAFS", "comment": "Thank you for the careful reviews and for the comments. We answer your questions below.\n \n\nQ1: First of all, the paper is not about 2bit quantization. It seeks an \u201caverage\u201d 2bit quantization. \u2026 Is it really more efficient to do multiplication-and-addition between 2 bit weights and 5 bit input (the output of the previous activation) than between 4bit weights and 4bit input?\n\nWe did a comparison of the computational complexity of our method and the equally quantized method on ResNet-50. We calculated the number of arithmetic operations of both methods required to perform single inference.\n\nSpecifically, we define a 32-bit multiplication/addition operation as one operation. To count the number of operations of the mixed-precision computation (e.g., 3 bit weight and 5 bit input), we follow the protocol defined by MicroNet challenge (https://micronet-challenge.github.io/scoring_and_submission.html) and consider the resolution of an operation to be the maximum bit-width of the 2 operands of this operation. For example, a multiplication operation with one 3-bit and one 5-bit operand will count as 5/32 of an operation.\n \n+-------------------------+-------------------+-------------------+--------------------+\n|            size              |        2 bits       |        4 bits       |         6 bits        |\n+-------------------------+-------------------+-------------------+--------------------+\n| equally quantized|   3.6 x 10^8    |   7.2 x 10^8    |    10.8 x 10^8  |\n+-------------------------+-------------------+-------------------+--------------------+\n|     our method      |   5.1 x 10^8    |   7.0 x 10^8    |    9.4 x 10^8     |\n+-------------------------+-------------------+-------------------+--------------------+\n \nThe table above shows the amount of operations required when weights and activations are compressed to 2 bits, 4 bits and 6 bits respectively. Our unequally quantized method has less amount of operations than the equally quantized method when weights and activations are compressed to 4 bits and 6 bits on average. While, at 2 bits, our method has 1.4x more operations than the equally quantized method. \n \nOn the other hand, the amount of operations does not imply equivalent inference speed in practice, as the processing of deep networks on hardware devices is constrained by both compute and memory access. We would like to reiterate that our method is effective to reduce the memory access time and thus provide higher inference rate compared to the equally quantized method, particularly for memory-bound hardware platforms where data movements are much slower and less energy efficient than compute. This is achieved by Pareto-optimal bit allocation which tends to allocate fewer bits per weight for layers that have a lot of weights. Thus, given fixed bandwidth, more weights can be loaded from off-chip memory to on-chip memory when processing the layers with a lot of weights, which in turn reduces compute idle time and improves the overall inference rate.\n \nTo verify the point above, we simulated the inference speed on Google TPU v1 at 2 bits for both equally and unequally quantized methods with ResNet50. As existing hardware does not well support mixed-precision operations, we assume the weights and activations with unequal bit-widths are fetched from off-chip memory, then decoded to fixed 8-bit stream and fed to compute unit that supports 8-bit multiplications (e.g. TPU). The simulation results on Google TPU platform show that our method is 1.5x faster than the equally quantize method.\n\nWe will add a paragraph to clarify the computational complexity of our method and the equally quantized method. \n\n\n\n\n", "title": "Response to Reviewer #2 Comments "}, "Hkx3g0MZoS": {"type": "rebuttal", "replyto": "HJeAzEJQqS", "comment": "\nThank you for your reviews and for pointing out the ICIP paper. We notice that the ICIP paper was posted on IEEE Xplore website on 22 Sept, which is 3 days prior to the ICLR deadline (Sept 25). According to the ICLR 2019 reviewer guideline, \u201cno paper will be considered prior work if it appeared on arxiv, or another online venue, less than 30 days prior to the ICLR deadline\u201d. We believe that our submission meets the ICLR regulations and rules.\n\nOur ICLR submission has substantial differences with the mentioned ICIP paper including the theoretical analysis, methods and insights, and experimental results. Moreover, with the new compression framework, the ICLR submission achieves 2-bit quantization results on deep architecture ResNet-50. To our best knowledge, this is the first work that reports 2-bit results without hurting the accuracy. Below we summarize the main differences with the ICIP paper:\n\n(1) Our ICLR submission provides a mathematical derivation for the additivity property. With two reasonable assumptions, we demonstrate that the additivity property holds for any neural networks which are continuously differentiable in the layers.\n\n(2) Our quantization framework differs from the ICIP paper in two-fold. First, we adopt a dead zone to the quantization function of weights. Second, we apply the straight-through estimator (STE) to perform back-propagation on the retraining stage for both quantized weights and activations. The ICIP paper uses the simple uniform quantizer and the framework does not provide a scheme to support the retraining for quantized weights and activations. However, as we illustrated in the experiment section, dead zone and STE retraining are critical for improving the accuracy.\n\n(3) In our ICLR submission, we reveal that the pattern of Pareto-optimal bit allocation across layers has positive impacts on neural network inference rate in practice. It tends to allocate fewer bits per weight for layers that have a lot of weights, which helps to reduce memory-access time which in turn reduces compute idle time and improves the overall inference rate. We verified this point by designing hardware simulation experiments on Google TPU v1 platform. Results show that the Pareto-optimal bit allocation improves the inference rate on ResNet50 by 1.5x compared to its equal bit allocation counterpart.\n\n(4) Combined the dead-zone quantization and STE based retraining with the optimal bit allocation strategy, our quantization framework achieves state-of-the-art result on deep neural network ResNet-50 at 2 bits. To the best of our knowledge, this is the first work that reports 2-bit results without hurting the accuracy. The ICIP paper can only compress ResNet-50 down to 4 bits and the accuracy drops significantly at 2 bits.\n\nWe will change our ICLR draft accordingly, and then upload it to the review website. We would also like to answer any other questions.\n", "title": "Clarification - the Main Differences with the ICIP paper"}, "rylTYKApFB": {"type": "review", "replyto": "H1eKT1SFvH", "review": "Very good paper that studies the error rate of low-bit quantized networks and uses Pareto condition for optimization to find the best allocation of weights over all layers of a network. The theoretical claims are strongly supported by experiments, and the experimental analysis covers state-of-the-art architectures and demonstrates competitive results. The paper in addition also analyzes the inference cost of their approach (in addition to the accuracy results), and shows positive results on ResNet and MobileNet architectures.\n\nThe paper primarily shows that the mean squared error of the final output of a quantized network has the additive property of being equal to the sum of squared errors of the outputs obtained by quantizing each layer individually. Although there is no reason why this should be case, experimental results from the authors on AlexNet and VGG-16 validate this. Based on this assumption, the authors then use a Lagrangian based constrained optimization to minimize the sum of squared errors of outputs when individual weights/activations are quantized, with the constraint being the total bit budget for weights and activations. The authors show that this can be optimized under the Pareto condition easily.\n\nThe experimental section is quite detailed and covers the popular architectures instead of toy ones. The accuracy results compared to other 2-bit and 4-bit approaches are competitive. It's also nice to see analysis of inference cost where unequal bitrate allocation performs better than other methods.\n\nThe authors show that given the constrained optimization, layers that have a large number of weights receive lower bitrates and vice-versa. While it makes sense that this would contribute to stronger inference speedup compared to methods with either equal bitrate allocation across layers or those that allocate higher bitrate to layers with large number of weights, it's not entirely clear why the optimization would produce this allocation in the first place. Do the authors mean to conclude that layers with large number of weights hold a lot of redundancy and don't have a significant impact on the overall accuracy of the model? This needs to be clarified further.", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 2}, "rJx2sPqAFS": {"type": "review", "replyto": "H1eKT1SFvH", "review": "This work nicely proposes a new theoretically-sound unequal bit allocation algorithm, which is based on the Lagrangian rate-distortion formulation. Surprisingly, the simple Lagrange multiplier on the constraints leads us to the convenient conclusion that the rate distortion curves for the weight quantization and the activation quantization have to match. Based on this conclusion, the authors claim that their search for the best bit allocation strategy is with a less complexity.\n\nI found this paper interesting and enjoyed reading it. However, I wish the paper could address some issues that are a little bit confusing to me. \n\nFirst of all, the paper is not about 2bit quantization. It seeks an \u201caverage\u201d 2bit quantization. It means that some weights in some layers can be quantized with higher or lower bits per weight.  Same story goes on for the activation quantization. I don\u2019t exactly know the implication of this, but it seems that the hardware implementation of a convolution layer could be either too complicated to benefit from this quantization scheme, or doesn\u2019t really improve the efficiency of, say 4bit quantization for all layers. Is it really more efficient to do multiplication-and-addition between 2 bit weights and 5 bit input (the output of the previous activation) than between 4bit weights and 4bit input? I\u2019m not a hardware person, but this part needs to be clearly addressed. Storage-wise, lowering the bitrate might be a clear benefit (I guess)\n\nI wish the actual optimization part briefly mentioned in section 4.2 could be elaborated more. It is a crucial part but somewhat understated. \n\nI also wonder what\u2019s the effect or limitation of using MSE for this optimization, where cross-entropy is a more suitable choice. I know that the objective function in eq 5 is just to find the best combination of bit allocations per layer, but still, the error space might not be the best for this classification problem. ", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "HJeAzEJQqS": {"type": "review", "replyto": "H1eKT1SFvH", "review": "This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete. My main concern is regarding the related work and experimental validation being incomplete, as they don't mention a very recent and similar work published in ICIP19 https://ieeexplore.ieee.org/document/8803498: \"Optimizing the bit allocation for compression of weights and activations of deep neural networks\". A reference in related work as well as a comparison in experimental validation would be necessary  and the novelty of this work is rather weak given the above mentioned 2019 publication.", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 3}, "BJl5neg1_H": {"type": "rebuttal", "replyto": "Sygq4eUhDH", "comment": "Thanks for your interest in our work. Apologies that we didn't have enough time to clean up our code before the deadline, also, we felt it is necessary to double-check the quantized models and redo evaluations again to make sure all the results reported here are reproducible.\n\nFinally, we have uploaded the evaluation code and quantized models to the dropbox link provided earlier. Kindly let us know if there is any question regarding the code.", "title": "Codes are available"}}}