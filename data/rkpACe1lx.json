{"paper": {"title": "HyperNetworks", "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"], "summary": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper contains an interesting idea, and after the revision of 3rd Jan, the presentation is clear enough as well. (Although I find it now contains an odd repetition where related work is presented first in section 2, and then later in section 3.2)."}, "review": {"B1cNrHKPl": {"type": "rebuttal", "replyto": "B1_ZyAV4g", "comment": "Hi, AnonReviewer4,\n\nThanks for revising your rating from 6 -> 8, based off the revised paper we have submitted on Jan 2, 2017.\n\nThe review process have helped us strengthen the presentation of our results in the revision of the paper.\n\nRegards,\n\nDavid\n", "title": "Response to AnonReviewer4 score revision."}, "SyMbS0uSg": {"type": "rebuttal", "replyto": "B1_ZyAV4g", "comment": "Hi, AnonReviewer4,\n\nThanks for your review.  We agree with your points about the writing style of the paper.  Based on your feedback, and also the feedback from the other reviewers, we have significantly rewritten the paper, shortened it and presented only the RNN part of the paper, so that it is more focused, self-contained and consistent. We\u2019re happy to change the title of the paper to more RNN-focused, if that\u2019s something the reviewer feels the right thing to do.  The parameter count is also included in the machine translation section as requested. Please take a look and let us know your thoughts!", "title": "Response to AnonReviewer4, Revised paper."}, "H1atK0dSg": {"type": "rebuttal", "replyto": "r15r5JDre", "comment": "Thanks for the review!\n\nWe have revised the paper to focus on the RNN methodology and results, to keep it inline with the conference format as the reviewers suggested, and have thus taken out the CNN section.  When we were starting out with this work, we originally started with CNNs.  But as you have pointed out, when we enforced weight-sharing across layers of a deep convnet, we found it difficult to match existing baseline results.  The insight from CNNs lead us to explore applying the opposite methodology, and to allow weight-sharing for RNNs instead, to see if we can improve baseline RNNs.\n\nWe hope this type of methodology can be applied to other types of RNNs going forward, and serve as a useful way to train RNNs to have context-dependent parameters.  We have released the code on github, and refactored it recently, to encourage others to use this technique for their own projects.\n", "title": "Response"}, "Hkbt4R_rl": {"type": "rebuttal", "replyto": "HkwR93bEg", "comment": "Hi, AnonReviewer2,\n\nThanks for the review!  We agree that the style of the paper suffers from a lack of focus.  We have rewritten the paper significantly to focus on the RNN part. We\u2019re happy to change the title of the paper to more RNN-focused, if that\u2019s something the reviewer feels the right thing to do.  Please take a look and let us know your feedback.\n\nAs for the point you mentioned about how HyperLSTM\u2019s improvements over LSTM come mainly from increasing the number of parameters, we tried to demonstrate in the Character PTB experiment, and also in the Handwriting Generation experiment, that the HyperLSTM can outperform conventional LSTMs with a lower parameter count.  For example, in the Char PTB experiment, we found that HyperLSTM with 1000 units can outperform an LSTM with 1250 units, and for the similarly in the handwriting generation experiment, the HyperLSTM with 900 hidden units outperform significantly the vanilla LSTM with 1000 units.  We hope that with the rewritten version that focuses on the RNN, it is easier to follow and digest the experimental results.\n\nFor your question regarding the softmax layer size:  For the MT experiment, the model does use a considerably large softmax layer size (32K for the GNMT architecture), although less than the 100K you mentioned.  We did not find it challenging for the HyperLSTM Cell to generate weights for the main LSTM.  The HyperLSTM Cell only had to generate a weight scaling vector of size 1000 (number of units of the main LSTM), which is not directly related to the size of the softmax layer.  We have expanded the MT section in the revised version of the paper, to try to emphasize the applicability of HyperLSTM to large-scale architectures such as GNMT.\n", "title": "Response to AnonReviewer2, Revised paper."}, "ryGnXAuBx": {"type": "rebuttal", "replyto": "SJVe59xVx", "comment": "Hi, AnonReviewer3,\n\nThanks for the review.  I appreciate the effort and detail you have put into writing the review and personally found it educational.  We took your suggestion to refocus the paper on the RNN section and have rewritten it with that in mind, leaving out the CNN section, and expanded on the Machine Translation section which was a bit lacking as you mentioned.\n\nWe have also followed your advice and added a related approaches section to describe the relation and differences with multiplicative RNNs and the other related works with the HyperRNN, in addition to adding the Second Order RNN work to the related works section.  In addition, we have improved style of the writing to make it more self contained and less dependent on the appendices, as you have outlined in the review.  Would appreciate it greatly if you can help us review the revised version of our paper, and provide us feedback on the improvements.", "title": "Response to AnonReviewer3, Revised paper."}, "BJEVMKSQg": {"type": "rebuttal", "replyto": "S1BAH31Xg", "comment": "Hi, thanks for the useful pointers.  For the RNN case, we have made an ongoing effort to try to improve the weight augmentation approach with low-rank matrix approximation, specifically using the HyperRNN to generate an additive (or multiplicative) low-rank 'fudge' to existing weight matrix of the main RNN.  I found that the current row-multiplicative method in eq 7-8 still offers the best performance while keeping computational performance relatively efficient, compared to low-rank factorized versions.  It may be possible to combine both approaches to squeeze out more performance at the expense of computational time / additional memory.\n\nFor section 3.1, your description is an elegant way to formulate the approach, by having the embedding vector z which depends on both j and i.\n\nWe have not tried to beat the SOTA results on CIFAR-10, as currently I found it more challenging to train much larger versions of a hypernetwork for CNN.  I'm actually trying out an approach of using hypernetworks for model distillation (https://arxiv.org/abs/1503.02531), which may help cope with some of the challenges.  For example, taking a pre-trained ensemble of ResNets, and recording the logits produced for the training set, and then training the hypernetwork version of the same ResNet to learn the logits.  I think learning soft-labels will help with both the training stability and performance.  This is still an ongoing effort for the future work.", "title": "re: questions"}, "S1BAH31Xg": {"type": "review", "replyto": "rkpACe1lx", "review": "To reduce the num. of parameters: in a linear element, low-rank factorization is often used, aka. linear bottleneck, often with mean or batch normalization.\nHave the authors tried such technique? E.g. low-rank factorized representation of K^j.\nFor RNNs, Gated Recurrent Convolutional Units have also been tried in the literature.\n\nIn section 3.1:\n  the authors argue against the one-layer hyper network. However using embedding vector z which depends on j (depth) and i (num of input, 1..N_{in}) would be also an option.\n  In eq 2, W_i*z^j + b_i := z_i,j would exactly result in this, and would not necessarily increase the num. of parameters -> no need for two layers.\n\nCould the authors outperform the best result (or achieve the same) in Table 2 with less parameters, e.g. <=3.77%, <13.3M ?\nAlthough the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.\nUnfortunately, as the results show, the authors could not get better results with less parameters.\nHowever, the proposed structure with even more number of parameters shows significant gain e.g. in LM.\n\nThe paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent.\nE.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.\n\nCould the authors provide the num. of trainable parameters for Table 6?\n\nProbably presenting less results could also improve the readability.\nOnly marginal accept due to the writing style.\n", "title": "questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1_ZyAV4g": {"type": "review", "replyto": "rkpACe1lx", "review": "To reduce the num. of parameters: in a linear element, low-rank factorization is often used, aka. linear bottleneck, often with mean or batch normalization.\nHave the authors tried such technique? E.g. low-rank factorized representation of K^j.\nFor RNNs, Gated Recurrent Convolutional Units have also been tried in the literature.\n\nIn section 3.1:\n  the authors argue against the one-layer hyper network. However using embedding vector z which depends on j (depth) and i (num of input, 1..N_{in}) would be also an option.\n  In eq 2, W_i*z^j + b_i := z_i,j would exactly result in this, and would not necessarily increase the num. of parameters -> no need for two layers.\n\nCould the authors outperform the best result (or achieve the same) in Table 2 with less parameters, e.g. <=3.77%, <13.3M ?\nAlthough the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.\nUnfortunately, as the results show, the authors could not get better results with less parameters.\nHowever, the proposed structure with even more number of parameters shows significant gain e.g. in LM.\n\nThe paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent.\nE.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.\n\nCould the authors provide the num. of trainable parameters for Table 6?\n\nProbably presenting less results could also improve the readability.\nOnly marginal accept due to the writing style.\n", "title": "questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyE4lBCMg": {"type": "rebuttal", "replyto": "B1hyYPsfl", "comment": "For the static hypernetworks described in Section 3, and in eq(1), the embedding vectors of each layer (z^j) are also learned parameters, and they are learned end-to-end during training of the image classification task along with the rest of the model.  So as you described, we could instead choose to represent each z^j by a one-hot, say determined by the layer index, and map z^j to the embedding space with an embedding matrix and learn that matrix end-to-end during training.  This would be equivalent to learning the embedding vectors directly.\n\nFor the run-time complexity, in the CNN case described in section 3, the extra run-time required will be in the order of the feeding the set of embedding vectors (z^j) through the hypernetwork to produce the set of kernels K^j.  During inference, the set of kernels K^j can be computed just once by the hypernetwork, and stored to be used as many times as required by the main network, so in that sense the run-time complexity is largely dominated by the main network as you mentioned.  During training, however, the hypernetwork will still be utilized at each forward and backward pass of every mini-batch, but in practice, we notice that the training time per mini-batch is ~ only 10% slower than the normal convnet case for MNIST and CIFAR10 experiments, as most of the complexity still resides in the main network.\n\nIn the RNN case, you are correct that the run-time complexity is larger than conventional RNNs, as the hypernetwork is operated at each timestep.\n\nPlease let us know if you have any more questions or concerns, thanks.", "title": "re: layer embedding and run-time complexity"}, "B1hyYPsfl": {"type": "review", "replyto": "rkpACe1lx", "review": "Can you explain how do you obtain the layer embedding vector like z^j in eq(1)? Do you map the layer index j represented by a one-hot vector to z^j like word index to a word vector in language modeling?\n\nIs It correct that the run-time complexity of deep CNNs generated by the hyperNetwork is the same as the conventional main network, as the hypernetwork will be discarded, while for RNNs with hypernetworks, the run-time complexity is larger than conventional RNNs, as the hypernetwork should also operate for each time step?This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.    \n\n--pros\n\nThis work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.\n\n--cons\n\nThe paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters. However, in the running time, the computational complexity is the same as the standard main network for static networks, such as ConvNet, and the computational cost is even larger for dynamic networks such as LSTMs. The improvements of hyperLSTMs over conventional LSTM and its variants seem mainly come from increasing the number of model parameters.\n\n--minor question,\n\n The ConvNet and LSTM used in the experiments do not have a large softmax layer. For most of the word-level tasks for either LM or MT, the softmax layer could be more than 100K. Is it going to be challenging for the hyperNetwork generate large number of weights for that case, and is it going to slowing the training down significantly?\n", "title": "layer embedding and run-time complexity", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkwR93bEg": {"type": "review", "replyto": "rkpACe1lx", "review": "Can you explain how do you obtain the layer embedding vector like z^j in eq(1)? Do you map the layer index j represented by a one-hot vector to z^j like word index to a word vector in language modeling?\n\nIs It correct that the run-time complexity of deep CNNs generated by the hyperNetwork is the same as the conventional main network, as the hypernetwork will be discarded, while for RNNs with hypernetworks, the run-time complexity is larger than conventional RNNs, as the hypernetwork should also operate for each time step?This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.    \n\n--pros\n\nThis work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.\n\n--cons\n\nThe paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters. However, in the running time, the computational complexity is the same as the standard main network for static networks, such as ConvNet, and the computational cost is even larger for dynamic networks such as LSTMs. The improvements of hyperLSTMs over conventional LSTM and its variants seem mainly come from increasing the number of model parameters.\n\n--minor question,\n\n The ConvNet and LSTM used in the experiments do not have a large softmax layer. For most of the word-level tasks for either LM or MT, the softmax layer could be more than 100K. Is it going to be challenging for the hyperNetwork generate large number of weights for that case, and is it going to slowing the training down significantly?\n", "title": "layer embedding and run-time complexity", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}