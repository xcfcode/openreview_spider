{"paper": {"title": "Implicit Normalizing Flows", "authors": ["Cheng Lu", "Jianfei Chen", "Chongxuan Li", "Qiuhao Wang", "Jun Zhu"], "authorids": ["~Cheng_Lu5", "~Jianfei_Chen1", "~Chongxuan_Li1", "~Qiuhao_Wang1", "~Jun_Zhu2"], "summary": "We generalize normalizing flows, allowing the mapping to be implicitly defined by the roots of an equation and enlarging the expressiveness power while retaining the tractability.", "abstract": "Normalizing flows define a probability distribution by an explicit invertible transformation $\\boldsymbol{\\mathbf{z}}=f(\\boldsymbol{\\mathbf{x}})$. In this work, we present implicit normalizing flows (ImpFlows), which generalize normalizing flows by allowing the mapping to be implicitly defined by the roots of an equation $F(\\boldsymbol{\\mathbf{z}}, \\boldsymbol{\\mathbf{x}})= \\boldsymbol{\\mathbf{0}}$. ImpFlows build on residual flows (ResFlows) with a proper balance between expressiveness and tractability. Through theoretical analysis, we show that the function space of ImpFlow is strictly richer than that of ResFlows. Furthermore, for any ResFlow with a fixed number of blocks, there exists some function that ResFlow has a non-negligible approximation error. However, the function is exactly representable by a single-block ImpFlow. We propose a scalable algorithm to train and draw samples from ImpFlows. Empirically, we evaluate ImpFlow on several classification and density modeling tasks, and ImpFlow outperforms ResFlow with a comparable amount of parameters on all the benchmarks.", "keywords": ["Normalizing flows", "deep generative models", "probabilistic inference", "implicit functions"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "This is a clear accept. Solid and timely work extending normalizing flows to implicitly defined mappings. Convincing presentation. Supported by all four reviewers. Best paper in my batch. Has the potential to spark further developments in the field. I recommend to feature this paper as a spotlight."}, "review": {"pqwZD5Dojuj": {"type": "review", "replyto": "8PS8m9oYtNy", "review": "Paper summary:\n\nThe authors propose ImpFlow, an implicit normalizing flow defined around solving the equation x + g_x(x) = z + g_z(z) where g_x and g_z have Lipschitz < 1.\n\nSolving z given x, or x given z, gives the forward and inverse passes of ImpFlow. Both directions require solving a root finding problem, hence this model is implicit in both directions, contrasting with prior works (e.g. ResFlow) where at least one direction is always explicit.\n\nImpFlow is equivalently a composition of ResFlow and the inverse of a ResFlow. \n\nIn the context of expressiveness, the forward pass a ResFlow is (1 + L)-Lipschitz while, importantly, the inverse is (1 / (1 - L))-Lipschitz, for some L < 1. Since ImpFlow makes use of the inverse of ResFlows in its construction, it can model arbitrary Lipschitz transformations. \n\nIn the context of likelihood evaluation and training, the estimators from ResFlows can be used and the main difference is the use of implicit function theorem to differentiate through the root finding procedure. Authors claim the total compute cost is comparable, though solving the root finding problem does seem to introduce some overhead, going from 3.189s to 4.462s (~40% increase) for each iteration of training on CIFAR-10.\n\nStrong points:\n\nThe use of the inverse of ResFlows in both directions of a normalizing flow is interesting, and result of having higher Lipschitz is convincing. \n\nIn addition to the well-written series of lemmas and theorems, the additional capacity of ImpFlow over ResFlow is nicely illustrated on a 1D example.\n\nExperiments indicate ImpFlow is slightly more performant in log-likelihood than ResFlows for the same number of parameters.\n\nWeak points:\n\nIs being implicit really useful? Outside of the Lipschitz requirement in this particular formulation of ImpFlows using the inverse of ResFlows, it's not clear whether having implicit formulations in both directions of a normalizing flow is advantageous. In addition to the use of a general title and method name advocating for implicit formulations, the authors do claim that \"bijections with explicit forward mapping only covers a fraction of the broad class of invertible functions suggested by the first requirement, which may limit the model capacity\" in the introduction. This ideally should be accompanied by a reference or rephrased to be a conjecture.\n\nThe experiments do not compare to other existing normalizing flows outside of ResFlows. The numbers reported in Table 2 (for tabular experiments) are worse than the baselines reported in Table 1 of Papamakarios et al. \"Masked Autoregressive Flow for Density Estimation\" while the results on CIFAR10 (Table 3) are closer to Real NVP (2016) than models like Glow or FFJORD (2018). This is likely simply due to the use of smaller models than those used previously, but if so, perhaps the authors could comment a bit more on the problems for scalability of ImpFlows. A ~40% additional time cost is mentioned once when compared to ResFlows, and an extra usage of GPU memory cost is only mentioned in the Appendix. The other parts of the paper are rather well done, but I'd liked to have seen explicit diagnostics of ImpFlows. For instance: \n  - How many function evaluations were used for solving the implicit formulation? \n  - Are results sensitive to the value of epsilon (for convergence of fixed point) hyperparameters? (A problem for implicit formulations is controlling the numerical error. Is this a problem for ImpFlows?)\n  - Since the Lipschitz of ResFlows grows exponentially, does the empirical difference between ImpFlow and ResFlow shrink as model sizes increase?\n\nSince the differences between ResFlow and ImpFlow are rather small (e.g. 0.01~0.02 bits/dim on CIFAR10), it'd be good to have some standard deviation across random seeds for all experiments.\n\nAdditional comments:\n\nThe citation under Lemma 1, \"Chen et al. (2019, Lemma 2)\" should probably be \"Behrmann et al. (2019, Lemma 2)\". (I assume the authors meant to refer to the bi-Lipschitz bounds derived in Behrmann et al.?)\n\nPost-discussion:\n\nI thank the authors for responding swiftly to all of my main concerns and for providing additional experimental results. I'm happy that authors have promised standard deviations and comparisons to ResFlow in the newly updated experiments for the camera-ready version, and I have adjusted my rating accordingly. I think this paper makes a very solid contribution to the normalizing flows literature.", "title": "review", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "f5H89oEBI9Y": {"type": "rebuttal", "replyto": "pZqqi33k1bD", "comment": "We'd like to thank the reviewer for the interest in our work and the positive updated comments. Below we address the reviewer's comments.\n\n#### Q1: About the implicit formulation.\nThanks for the constructive discussion. In our initial version, we choose to term our method as the more general \"implicit normalizing flows\" because we think the implicit view itself has some insights. Though we mainly discussed the implicit version of residual flows, we conjecture the general implicit bijection family has more to explore. We understand and agree with the reviewer's point, and will rephrase our statements more rigorously in our camera-ready version. \n\n#### Q2: Comparable ResFlows with deeper networks.\nThanks for the suggestion. We will add it to our camera-ready version due to the time limitation of the discussion period.\n \n#### Q3: About the time costs and function evaluation numbers of ImpFlows.\nThanks for the suggestion. We've added a full table of c=0.5,0.6,0.7,0.8 and 0.9 to compare the computation costs between ImpFlows and ResFlows, as shown in the updated version of Table 5 and Table 6. Although the computation overhead of ImpFlows increases as $c$ becomes large, the main conclusion remains the same: The training time of ResFlows is still comparable with ResFlows (40%\\~80%). However, the inference time gap is much smaller (10%\\~20%), since ImpFlow's additional overhead of fixed-point solver is marginal, relative to the overhead of approximating the log determinant. And the sampling time of ImpFlows is faster (40%\\~60%) than that of ResFlows, as half of the blocks are already aligned in the sampling direction. Fast sampling is particularly desirable since it is the main advantage of flow-based models over autoregressive models. For the function evaluation numbers, we've corrected the number we reported for $c=0.5$, and the numbers of function evaluations for forward and backward are 8.2 and 13.5. And for $c=0.9$, the numbers are 13.9 and 28.4, respectively. This result is consistent with the reviewer's points. However, we note that this does not affect the conclusion, because this overhead is comparable.\n\n#### Q4: About the sampling method.\nThanks for pointing it out. We use the same sampling code as the released version of ResFlows, for the fair comparison. We've made it more clear in our updated version in Appendix C.2.\n\n#### Q5: Do the authors believe Broyden's method is helping in reducing the number of network evaluations?\nThis is a very good question. Empirically, we find that the fixed-point iteration method needs more number of function evaluations, but for some big $\\epsilon_f$, it sometimes costs less time than the Broyden's method (due to the cost for approximating the inverse Jacobian). However, during the training phase, using Broyden's method is helpful for the precision of the fixed-point solution because we can set a small $\\epsilon_f$. And the fixed-point iteration method sometimes cannot achieve such precision and may cost much more time and the number of function evaluations. However, for the sampling phase, we do not need a high precision solution because the image data is discrete. So we simply use fixed-point iterations with a little bigger tolerance.\n\n", "title": "Thanks for the suggestions!"}, "cgpJ1CZq18A": {"type": "review", "replyto": "8PS8m9oYtNy", "review": "Summary: The paper introduces an invertible transformation implicitly via the roots of an equation. As a result, they claim that this implicit transformation is in theory a superset of Residual Flows. Further, they show improved performance empirically in NLL on CIFAR10 in varying settings.\n\nReview:\nThe paper introduces a novel an interesting method to parametrize invertible transformations via an implicit. The method in its current form can actually be interpreted as two Residual Blocks, but where the inverse parametrisation of the second block is used in the forward direction (as the authors show in Eq. 6). Nevertheless, I find the proposed perspective interesting and a promising future direction. The authors give detailed proofs of the flexibility of their method. The authors also propose a more memory efficient solution for training.\n\nWeaknesses: Experimentally there are some unanswered questions.\n- As Implicit flows require iterative solutions during training, they will most likely be slower. Even if ImpFlows are slower, it is really necessary to clearly highlight this difference in computation time. Further, ImpFlows are probably even faster during sampling as half of the Residual Blocks are now aligned with the sampling direction.\n- The new gradient computation from Eq (16) is only very briefly discussed in the paper. Since the authors claim gains in memory efficiency, these results need to be backed up empirically.\n- Thm 1 can also be derived from the perspective of two stacked residual blocks (second inverted). It would help readers to already introduce that perspective here. \n\nIf the authors address the points raised in Weaknesses, I will consider raising my score. \n\nMinor comments:\nTypos. The paper has some typos and in many cases a spell checker could correct these. For instance: Inverible (Sec 3.2) Esitamated (algorithm 1.).\nRephrasing (sec. 4.2). \"Note that we define R_1 = R\" -> \"By definition of Eq. 4 and Eq. 5, R_1 = R\"\nThe overload in Algorithm 1 is somewhat difficult to parse, consider changing \"g(z) = F (z, x; theta)\"\n\nAfter rebuttal:\nI am satisfied with the reply of the authors and I have raised my score to 7.", "title": "Good idea, some important details missing.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "8EvQq3FaM5C": {"type": "rebuttal", "replyto": "pqwZD5Dojuj", "comment": "We'd like to thank the reviewer for the positive comments and acknowledgment of our contribution. Below we address the reviewer's comments\n\n#### Q1: Is being implicit really useful?\n\nThe implicit formulation is useful. Its relaxed Lipschitz constraint has real empirical improvements, leading to SOTA results on some tabular datasets (please refer to our updated paper). For instance, on the HEPMASS dataset, our 20-block ImpFlow achieves a SOTA log-likelihood (-13.95), which improves considerably than the previous SOTA (-15.09) for normalizing flow models, to the best of our knowledge.\n\nThanks for the suggestion on writing. We will rephrase the claims in the final version. In fact, our ImpFlow itself is a bijection that cannot be represented by an explicit forward mapping of neural networks. Furthermore, recently implicit methods have shown great promise in deep learning. They achieve more modeling flexibility by incorporating logical reasoning [1] and also improved memory consumption [2]. While exploring more implicit bijections is beyond the scope of this paper, these positive results strengthen our belief that implicit flows are a promising direction to research.\u00a0\n\n#### Q2: The experiments do not compare to other existing normalizing flows outside of ResFlows.\n\nWe've updated the results of tabular datasets and included the mentioned baselines. The original results are not strong due to the shallow architecture. Please see Table 2 of the updated version. We trained a 20-block ImpFlow on tabular datasets and achieved SOTA results on GAS and HEPMASS datasets.\n\n#### Q3: How many function evaluations were used for solving the implicit formulation?\nWe evaluated the single-batch computation costs (running time and number of function evaluations) for ResFlow and ImpFlow in our updated version. The number of function evaluations for forward and backward are 10.7 and 22.7, respectively. Please refer to Table 4, and Table 5 in Appendix.C.2 for details.\n\n#### Q4: Are results sensitive to the value of epsilon (for convergence of fixed point) hyperparameters?\n\nEmpirically, our model is not sensitive to $\\epsilon_f$ in a fair range ($10^{-8}$ to $10^{-2}$). Please refer to Appendix.C.3 in our updated version.\n\n#### Q5: Since the Lipschitz of ResFlows grows exponentially, does the empirical difference between ImpFlow and ResFlow shrink as model sizes increase?\n\nWe include a new experiment varying the depth. ImpFlows are always better than ResFlows. The performance gain becomes smaller as the model grows deeper since they all converge to a performance upper bound (which is related to the network structure, activation functions, and hyperparameters). Nevertheless, ImpFlow requires fewer blocks than ResFlow to achieve the same likelihood. Please refer to Appendix.D.3 and Figure 5 in our updated version.\u00a0\n\n#### Q6: It'd be good to have some standard deviation across random seeds for all experiments.\n\nThanks for the suggestion. Due to the limited time of the discussion period, we will add it in our camera-ready version.\n\n#### Q7: Typos.\n\n\nThanks for pointing it out. We've fixed the typos in our updated version.\n\n=====================================================================\n\n[1]. Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In International Conference on Machine Learning, pp. 6545\u20136554, 2019.\n\n\n[2]. Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural Information Processing Systems (NeurIPS), 2019.", "title": "Thanks for the positive comments and acknowledgment of our contribution"}, "r9IuD9HiLeM": {"type": "rebuttal", "replyto": "S8iQLY4TX8Q", "comment": "We'd like to thank the reviewer for the positive comments and acknowledgment of our contribution. Below we address the reviewer's comments.\n\n#### Q1: I would like to see some analysis on how a Lipschitz function influences the Lipschitzness of the inverse.\n\nThanks for the suggestion. We've added it in Appendix.B, Lemma 7. And the bound is indeed $L<(1+c)/(1-c)$, as suggested by the reviewer.\n\u00a0\n#### Q2: About the results of tabular datasets.\n\nWe've updated the results of tabular datasets and included the baselines. The original results are not strong due to the shallow architecture. Please see Table 2 of the updated version. We trained a 20-block ImpFlow on tabular datasets and achieved SOTA results on GAS and HEPMASS datasets.\n\n#### Q3:  I suggest that you should additionally have a variant of ResFlow with the same execution time in addition to the number of parameters as a corresponding ImpFlow.\n\nThanks for the suggestion. We will add a result under the same execution time in our camera-ready version.\n\n#### Q4: Typos.\n\nThanks for the suggestion. we've fixed the typos.\n\n", "title": "Thanks for the positive comments and acknowledgment of our contribution"}, "yvSjAd-Y98L": {"type": "rebuttal", "replyto": "4aO1sjSt7N", "comment": "We'd like to thank the reviewer for the positive comments and acknowledgment of our contribution. Below we address the reviewer's comments.\n\n#### Q1: You should mention that everything is proved in the appendix\n\nThanks for the suggestion. We've mentioned it at the beginning of Sec.4.2. in our updated version.\n\n#### Q2: Clarify how you selected the number of steps (L).\n\nThanks for the suggestion. We include a new experiment varying the depth (steps). ImpFlows are always better than ResFlows and require fewer blocks than ResFlow to achieve the same likelihood. Please refer to Appendix.D.3 and Figure 5 in our updated version.\u00a0\n\n#### Q3: You should mention other sota NF architectures in Table 2.\n\nWe've updated the results of tabular datasets and included the baselines. The original results are not strong due to the shallow architecture. Please see Table 2 of the updated version. We trained a 20-block ImpFlow on tabular datasets and achieved SOTA results on GAS and HEPMASS datasets.\n\n#### Q4: You could maybe change the statement of Corollary 1.\n\nThanks for the suggestion. We've changed the statement of Thm.2 and Cor.1 to use the notation of $\\mathcal{F}_2$.\n\n#### Q5: Typos.\nThanks for the suggestion. We've fixed the typos in our updated version.\n\n", "title": "Thanks for the positive comments and acknowledgment of our contribution"}, "HA98qMXrmVo": {"type": "rebuttal", "replyto": "cgpJ1CZq18A", "comment": "We'd like to thank the reviewer for the positive comments and acknowledgement to our novelty and significance. Below we address the reviewer's comments.\n\n#### Q1:  It is really necessary to clearly highlight this difference in computation time.\u00a0\nThanks for the suggestion. We decomposed the single-batch computation costs for ResFlow and ImpFlow in our updated paper. There are three different settings: training, inference (density computation), and sampling. The training time of ImpFlow (4.152s) is ~40% longer than ResFlow (2.910s). However, the inference time gap is much smaller (2.905s vs 2.656s), since ImpFlow's additional overhead on fixed-point solver (0.445s) is marginal, relative to the overhead of approximating the log determinant (2.370s).\u00a0\n\nMoreover, ImpFlow's sampling time is ~40% FASTER than ResFlow, as half of the blocks are already aligned in the sampling direction. Fast sampling is particularly desirable since it is the main advantage of flow-based models over autoregressive models. Please refer to Appendix.C.2, Table 4, and Table 5 for details.\u00a0\n\n#### Q2: Why Eqn. (16) is memory efficient?\nWe suspect there could be some misunderstandings. By \"memory efficient\", we mean that our algorithm does not need to store the intermediate results of the Broyden's iterations. We've changed our discussion under Eqn.(16) by adding \"the quasi-Newton iterations of the forward pass\" to clarify the statement.\n\nDetails: In the forward pass, ImpFlows need to solve the fixed-point by the quasi-Newton iterations. Every iteration is like \"$z^{[i+1]}=G^{[i]}(z^{[i]}; \\theta)$\", and after $L$ steps we get $z^* \\approx z^{[L]}$. In the backward pass, we need to compute the loss gradient w.r.t. $\\theta$. By backpropagation, we firstly have the loss gradient w.r.t $z^{[L]}$. If we do not use Eqn.(16), we need to backpropagate along $z^{[L]},z^{[L-1]}\\cdots, z^{[1]}$ to compute the gradient w.r.t. $\\theta$. Therefore, we need to save all the $L$ intermediate results $z^{[1]},\\cdots, z^{[L]}$ during the forward pass. This costs $O(L)$ GPU memory. Instead, if we use Eqn.(16), we only need to save $z^{[L]}$ during the forward pass, and this costs only $O(1)$ GPU memory, which is independent of the iteration steps in the forward pass and treats the root solvers (such as the Broyden's method) as a black-box.\n\n#### Q3: Thm. 1 can also be derived from the perspective of two stacked residual blocks (second inverted)\nThanks for the suggestion. We've added this discussion under Thm. 1 in our updated version.\n\n#### Q4: Typos and statement of Algorithm 1.\nThanks for the suggestion. We've fixed the typos and changed the statement of Algorithm 1 in the updated version.\n", "title": "Thanks for the positive comments and acknowledgement to our novelty and significance"}, "S8iQLY4TX8Q": {"type": "review", "replyto": "8PS8m9oYtNy", "review": "# Summary\nThe authors concerns the question of how expressive invertible functions can be constructed. Their ansatz is the defining an invertible layer implicitly, using the root of an equation. While this approach is more general, they employ residual flows (ResFlows) to formulate a particular realisation of such an equation, calling the model ImpFlow. They show that the resulting function space is strictly richer than that of ResFlows. They further demonstrate how ImpFlows can be trained and evaluated. Empirically, ImpFlows outperform ResFlows on all considered tasks.\n\n# Strong and weak points\n## Pros:\n- Implicit functions are a new way to formulate invertible functions\n- The proposed formulation using ResFlows is clearly presented\n- ImpFlows are strictly more expressive than ResFlows, allowing arbitrary Lipschitz constants\n- Experiments highlight the improvements\n- In general: very concise and well-guiding writing\n\n## Cons:\n- Implicit functions are more expensive to evaluate than explicit functions (here, +50% in execution time)\n\n# Recommendation\nThe development of expressive invertible functions is key in applications that involve invertible functions, like density estimation using normalising flows. This paper proposes a novel framework to formulate such invertible expressive functions implicitly. The results are not game-changing, but consistently outperform its closest relative. Together, this is a solid work that should clearly be accepted.\n\n# Questions\n- I would like to see some analysis on how a Lipschitz function influences the Lipschitzness of the inverse. In particular, in Section 6 you could mention the relation between the \"Lipschitz coefficient\" c and the achievable Lipschitz constants of an ImpFlow layer. I naively would guess the bound $L < (1 + c) / (1 - c)$.\n- Can you give an intuition how large the improvements on the density estimation datasets are? I checked [paperswithcode](https://paperswithcode.com/sota/density-estimation-on-uci-power) on the POWER dataset and the models are clearly outperformed by other methods. To be explicit: I don't require that every new architecture has to beat the state of the art in all possible tasks, especially when it comes with a fresh idea and thorough theory. But can you give your best guess about why other approaches achieve significantly better results?\n- When comparing ImpFlow and ResFlow, I suggest that you should additionally have a variant of ResFlow with the same *execution time* in addition to *number of parameters* as a corresponding ImpFlow. I think the tradeoff involved is not only between quality and the number of parameters to store, but the also execution time of a model.\n", "title": "Very good submission, minor points remain open", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "4aO1sjSt7N": {"type": "review", "replyto": "8PS8m9oYtNy", "review": "# Summary\nThis work is about a new architecture for normalizing flows inspired by implicit neural networks. In particular, the authors show that a specific implicit neural network build from residual blocks defines a bijective map. From this insight, they show how to efficiently train such architecture by estimating the log Jacobian and solving the inverse problem related to the implicit architecture. On top of this achievement, the authors thoroughly study the attractiveness of their architecture compared to i-res-flow. They demonstrate analytically and empirically that implicit normalizing flows are strictly more expressive than i-res-flow.\n\n# Major comments\n## Pros:\nOverall the paper is well written and pleasant to read. The idea is novel and is well introduced. Moreover, the paper provides a theoretical justification for the expressivity gain made by the newly introduced architecture with respect to I-res-flow. The experimental results are good although not defining a new standard in density estimation tasks. \n\n## Cons:\n1) As I said the paper is well written overall. However, I took quite some time to fully understand section 4 (in particular 4.2), I think this section should be clarified. After taking the time, everything appears convincing but I think the flow could be improved. I checked the proofs in 4.2 and everything seems correct, great job! You should mention that everything is proved in the appendix, I was not sure it was the case the first time I read it.\n2) Regarding the experiments I have two concerns. First, you provide only results about one specific value of L without stating how you chose it. It feels like you selected it to show the performance gain brought by your method, it may be fine but you should at least mention it clearly. Else you should perform a full architecture search both for resflow and for impflow and show the best results of each architecture. My second concern is about table 2, I think you should mention other sota NF architectures (such as the ones based on monotonic transformation) and maybe discuss a possible explanation for the large gap.\n\n## What could be done to address my comments\n1)  You could maybe directly state that $\\mathcal{R}_2 \\subset_\\neq \\mathcal{F}_2 = \\mathcal{I}$ instead of corollary 1.\n2) Clarify how you selected the number of steps. I saw you mention that you copy the experimental setting of Chen et al. 2019. However, they use 200 steps for the 2D grid and do not provide results for the UCI dataset (or I missed it maybe).\n\n# Minor comments\np3: \"inverible\".\n4.1: $J_f$ inconsistent notation wrt (1).\ndef 2: missing . at the end of the equation.\n\"2-norm\": do you mean $L_2$ norm?\n\"in 1-D input\" -> \"In the 1D input\"\nD.4: \"Tablular\"", "title": "Overall a nice work!", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}