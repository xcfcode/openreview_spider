{"paper": {"title": "Cost-Sensitive Robustness against Adversarial Examples", "authors": ["Xiao Zhang", "David Evans"], "authorids": ["xz7bc@virginia.edu", "evans@virginia.edu"], "summary": "A general method for training certified cost-sensitive robust classifier against adversarial perturbations", "abstract": "Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations. These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications. We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others. We encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness. Our experiments on simple MNIST and CIFAR10 models with a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy.", "keywords": ["Certified robustness", "Adversarial examples", "Cost-sensitive learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper studies the notion of certified cost-sensitive robustness against adversarial examples, by building from the recent [Wong & Koller'18]. Its main contribution is to adapt the robust classification objective to a 'cost-sensitive' objective, that weights labelling errors according to their potential damage. \nThis paper received mixed reviews, with a clear champion and two skeptical reviewers. On the one hand, they all highlighted the clarity of the presentation and the relevance of the topic as strengths; on the other hand, they noted the relatively little novelty of the paper relative [W & K'18]. Reviewers also acknowledged the diligence of authors during the response phase. The AC mostly agrees with these assessments, and taking them all into consideration, he/she concludes that the potential practical benefits of cost-sensitive certified robustness outweight the limited scientific novelty. Therefore, he recommends acceptance as a poster. "}, "review": {"HJg7nLR0n7": {"type": "review", "replyto": "BygANhA9tQ", "review": "The authors define the notion of cost-sensitive robustness, which measures the seriousness of adversarial attack with a cost matrix. The authors then plug the costs of adversarial attack into the objective of optimization to get a model that is (cost-sensitively) robust against adversarial attacks.\n\nThe initiative is novel and interesting. Considering the long history of cost-sensitive learning, the proposed model is rather ad-hoc for two reasons:\n\n(1) It is not clear why the objective should take the form of (3.1). In particular, if using the logistic function as a surrogate for 0-1 loss, shouldn't the sum of cost be in front of \"log\"? If using the probability estimated from the network in a Meta-Cost guided sense, shouldn't the cost be multiplied by the probability estimate (like 1/(1+exp(...))) instead of the exp itself? The mysterious design of (3.1) makes no physical sense to me, or at least other designs used in previous cost-sensitive neural network models like\n\nChung et al., Cost-aware pre-training for multiclass cost-sensitive deep learning, IJCAI 2016\nZhou and Liu, Training cost-sensitive neural networks with methods addressing the class imbalance problem, TKDE 2006 (which is cited by the authors)\n\nare not discussed nor compared.\n\nUpdate: I thank the authors for providing updated information in the Appendix discussing about other alternatives. While I still think it worth comparing with other approaches (as it is still not clear whether Khan's approach is regarded as state-of-the-art for *general* cost-sensitive deep learning), I think the authors have sufficiently justified their choice.\n\n(2) It is not clear why the perturbed example should take the cost-sensitive form, while the original examples shouldn't (as the original examples follow the original loss). Or alternatively, if we optimize the original examples by the cost-sensitive loss, would it naturally achieve some cost-sensitive robustness (as the model would naturally make it harder to make high-cost mistakes)? Those issues are yet to be studied.\n\nUpdate: I thank the authors for providing additional experiments on this part.\n", "title": "interesting initiative, ad-hoc model", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byx0aQAY0X": {"type": "rebuttal", "replyto": "ByeXwZZzCm", "comment": "Thank you for your consideration, we have included our discussions on the choices of cost matrices in Appendix D. ", "title": "Thank you for your suggestion."}, "ByxW5QAYCX": {"type": "rebuttal", "replyto": "HJxSIK7VRX", "comment": "Madry et al., (2018) is based on robust training against adversarially generated images devised via PGD attacks, which is not targeted for certifiable robustness. Thus, investigation on how to make PGD-based robust training cost-sensitive is beyond the scope of our work. \n\nTo the best of our knowledge, before the submission of our paper there are only two proposed certifiable robust training methods: one is Wong & Kolter (2018) and the other one is [1]. Compared with Wong & Kolter (2018), [1] is only applicable to neural networks with two layers, thus we focus our experiments on Wong & Kolter (2018) that is more general. Recently, [2] extends the method of [1] to arbitrary number of neural network layers, thus it would be interesting to study whether our approach is applicable to the robust model developed in [2]. \n\nReference:\n[1] Raghunathan, et al., Certified Defenses against Adversarial Examples. https://arxiv.org/abs/1801.09344\n[2] Raghunathan, et al., Semidefinite relaxations for certifying robustness to adversarial examples. https://arxiv.org/abs/1811.01057\n", "title": "Thank you for suggestions, but there is a misunderstanding on Madry et al. (2018) being a certified robust learning model."}, "B1xUE7Rt0m": {"type": "rebuttal", "replyto": "rJgLQKrMRX", "comment": "1. Comparison with other alternatives\n(a) We have added an equivalent form of the cost-sensitive CE loss for standard classification as in (B.1) in Appendix. The derivation of (B.1) simply follows the definition of the cross entropy loss and the modified softmax outputs y_n as defined in (11) of [1]. Our robust classifier is basically applying the same techniques on the guaranteed robust bound to induce cost-sensitivity for the adversarial setting.\n\n(b) Indeed, [1] introduces other cost-sensitive loss including MSE loss and SVM hinge loss besides the cost-sensitive CE loss. However, they only evaluated the cost-sensitive CE loss in their experiments, as argued in [1] that CE loss usually performs best among the three loss functions for multiclass image classification. Thus, we consider cost-sensitive robust optimization based on CE loss as the most promising approach.\n\n(c) We have cited [1] in Section 3.2 of the main paper in the revised pdf.\n\n(d) Please refer to Appendix C for the discussions of existing related work on cost-sensitive learning with neural networks, and explanations on why choosing to incorporate cost information into the cross entropy loss, instead of other loss functions.\n\n(e) The proposed robust training objective adapts cost-sensitive CE loss to the adversarial setting, and the cost-sensitive CE loss is aligned with the idea of minimizing the Bayes risks (see equation (1) in MetaCost). More specifically, it is proved in Lemma 10 of [1] that the cost-sensitive CE loss is c-calibrated, or more concretely, there exists an inverse relationship between the optimal CNN output and the Bayes cost of the t-th class.\nTherefore, minimization of the cost-sensitive CE loss will lead to classifier that has risks closer to the optimal Bayes risks.\n\n2. As requested, we have added in Appendix C to survey related works on cost-sensitive learning for non-adversarial settings and explain the reasoning behind the techniques we choose.\n\n3. Quoting from the reviewer, \u201cWhat happens if the original examples are also evaluated/optimized cost-sensitively\u201d, if you are referring to robustness of standard cost-sensitive learning method, this is what the experiment in Appendix B.3 tests. The results show naive cost-sensitive learning does not lead to cost-sensitive robustness.\n\nReference\n[1]. Khan, et al., Cost-Sensitive Learning of Deep Feature Representations from Imbalanced Data. https://arxiv.org/abs/1508.03422", "title": "Please read Appendix B.3 and Appendix C in the revised pdf"}, "ryxEPIvKhm": {"type": "review", "replyto": "BygANhA9tQ", "review": "** review score incremented following discussion below **\n\nStrengths:\n\nWell written and clear paper\nIntuition is strong: not all source-target class pairs are as beneficial to find adversarial examples for \n\nWeaknesses:\n\nCost matrices choices feel a bit arbitrary in experiments\nCIFAR experiments still use very small norm-balls\n\nThe submission builds on seminal work by Dalvi et al. (2004), which studied cost-sensitive adversaries in the context of spam detection. In particular, it extends the approach to certifiable robustness introduced by Wong and Kolter with a cost matrix that specifies for each pair of source-target classes whether the model should be robust to adversarial examples that are able to take an input from the source class to the target (or conversely whether these adversarial examples are of interest to an adversary).\n\nWhile the presentation of the paper is overall of great quality, some elements from the certified robustness literature could be reminded in order to ensure that the paper is self-contained. For instance, it is unclear how the guaranteed lower bound is derived without reading prior work. Adding this information in the present submission would make it easier for the reader to follow not only Sections 3.1 and 3.2 but also the computations behind Figure 1.b. \n\nThe experiments results are clearly presented but some of the details of the experimental setup are not always justified. If you are able to clarify the following choices in your rebuttal, this would help revise my review. First, the choice of cost matrices feels a bit arbitrary and somewhat cyclical. For instance, binary cost matrices for MNIST are chosen according to results found in Figure 1.b, but then later the same bounds are used to evaluate the performance of the approach. Yet, adversarial incentives may not be directly correlated with the \u201chardness\u201d of a source-target class pair as measured in Figure 1.b. The real-valued cost matrices are better justified in that respect. Second, would you be able to provide additional justification or analysis of the choice of the epsilon parameter for CIFAR-10? For MNIST, you were able to improve the epsilon parameter from epsilon=0.1 to epsilon=0.2 but for CIFAR-10 the epsilon parameter is identical to Wong et al. Does that indicate that the results presented in this paper do not scale beyond simple datasets like MNIST?\n\nMinor comments:\n\n\nP2: The definition of adversarial examples given in Section 2.2 is a bit too restrictive, and in particular only applies to the vision domain. Adversarial examples are usually described as any test input manipulated by an adversary to force a model to mispredict.\nP3: typo in \u201coptimzation\u201d \nP5: trade off -> trade-off \nP8: the font used in Figure 2 is small and hard to read when printed.\n", "title": "review", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJlIr-JG0X": {"type": "rebuttal", "replyto": "SygPPR_l0X", "comment": "We don\u2019t see any intrinsic reason why the class transformation difficulty is correlated with adversarial value, but the actual value and difficulty should depend on the application. The results in Table 1 show that the cost-sensitive robustness can harden both \u201ceasy\u201d (4->9, robust error reduces from 10.08% to 1.02%) and \u201chard\u201d (0->2, robust error reduces from 0.92% to 0.38%) - the improvement is bigger for the \u201ceasy\u201d transformation, but even after the cost-sensitive robustness hardening, it remains slightly \u201ceasier\u201d than the \u201chard\u201d transformation in the overall robust model.  \n\nFor the MNIST classes, there is no correlation between the adversarial value (in the toy check fraud motivation) and transformation difficulty, since adversarial value is directional and semantically different digits can look more similar than far apart ones. For a more realistic security application, it would be desirable to define the classes in such a way that the valuable adversarial transformations are also the hardest ones to achieve.\n", "title": "Adversarial incentives and transformation hardness"}, "B1e-WxMi6Q": {"type": "rebuttal", "replyto": "HJg7nLR0n7", "comment": "We\u2019ve added an Appendix B.3 to the revised paper that addresses the question you raised about whether standard cost-sensitive loss trained on original examples would improve cost-sensitive robustness. The results from our experiments show that standard cost-sensitive loss does not result in a classifier with cost-sensitive robustness.", "title": "Additional experiments regarding cost-sensitive learning"}, "HklCJkMs6X": {"type": "rebuttal", "replyto": "ryxEPIvKhm", "comment": "We hope the following explanations address your questions:\n\n1. Regarding the choice of the cost matrices\nOur goal in the experiments was to evaluate how well a variety of different types of cost matrices can be supported. MNIST and CIFAR-10 are toy datasets, thus defining cost matrices corresponding to meaningful security applications for these datasets is difficult. Instead, we selected representative tasks and designed cost matrices to capture them. Our experimental results show the promise of the cost-sensitive training method works across a variety of different types of cost matrices, so we believe it can be generalized to other cost matrix scenarios that would be found in realistic applications.\n\nIt is a good point that the cost matrices that were selected based on the robust error rates in Fig 1B are somewhat cyclical, but it does not invalidate our evaluation. We use the \u201chardness\u201d of adversarial transformation between classes only for choosing representative cost matrices, and the robust error results on the overall-robustness trained model as a measure for transformation hardness. Further, the transformation hardness implied by the robust error heatmap is generally consistent with intuitions about the MNIST digit classes (e.g., \u201c9\u201d and \u201c4\u201d look similar so are harder to make robust to transformation), as well as  with the visualization results produced by dimensional reduction techniques, such as t-SNE [1]. \n\n2. Regarding the choice of epsilon for CIFAR-10\nIn our CIFAR-10 experiments, we set epsilon=2/255, the same experimental setup as in [2]. Our proposed cost-sensitive robust classifier can be applied to larger epsilon for CIFAR-10 dataset, and similar improvements have been observed for different epsilon settings. In particular, we have run experiments on CIFAR-10 with epsilon varying from {2/255, 4/255, 6/255} for the single seed task. The comparison results are reported in Figure 5(b), added to the revised PDF. These results support the generalizability of our method to larger epsilon settings.\n\n[1] Maaten and Hinton, Visualizing Data using t-SNE. http://www.jmlr.org/papers/v9/vandermaaten08a.html\n[2] Wong, et al., Scaling Provable Adversarial Defenses. https://arxiv.org/abs/1805.12514\n", "title": "Thank you for your positive and constructive comments"}, "SklzwA-oaQ": {"type": "rebuttal", "replyto": "B1lNPh9c3Q", "comment": "Thank you for your review. Please see our responses below.\n\n1. Concern regarding the novelty\nThe review correctly notes that the method we use to achieve cost-sensitive robustness is a straightforward extension to the training procedure in Wong & Kolter (2018). The novelty of our paper lies in the introduction of cost-sensitive robustness as a more appropriate criteria to measure classifier\u2019s performance, and in showing experimentally that the cost-sensitive robust training procedure is effective. Previous robustness training methods were designed for overall robustness, which does not capture well the goals of adversaries in most realistic scenarios. We consider it an advantage that our method enables cost-sensitive robustness to be achieved with straightforward modifications to overall robustness training.\n\n2. Limitation in data scale\nWe agree with the reviewer that certified robustness methods, including our work, are a long way from scaling to interesting models. All previous work on certified adversarial defenses has been limited to simple models on small or medium sized datasets (e.g., [1-3] below), but there is growing awareness that non-certified defenses are unlikely to resist adaptive adversaries and strong interest in scaling these methods. The method we propose and evaluate for incorporating cost-sensitivity in robustness training is generic enough that we expect it will also work with most improvements to certifiable robustness training. So, even though our implementation is not immediately practical today, we believe our results are of scientific interest, and the methods we propose are likely to become practical as rapid progress continues in scaling certifiable defenses. \n\n\n[1] Wong and Kolter, Provable defenses against adversarial examples via the convex outer adversarial polytope. https://arxiv.org/abs/1711.00851\n[2] Raghunathan, et al., Certified Defenses against Adversarial Examples. https://arxiv.org/abs/1801.09344\n[3] Wong, et al., Scaling Provable Adversarial Defenses. https://arxiv.org/abs/1805.12514\n", "title": "Novelty is cost-sensitive robustness"}, "H1llJV-z6X": {"type": "rebuttal", "replyto": "HJg7nLR0n7", "comment": "Thank you for your review. Your comments about the model being ad hoc stem from a few misunderstandings, which we hope to clarify:\n\n1. Justification of training objective (3.1)\nThe design of (3.1) is not ad hoc, but follows from previous cost-sensitive learning work such as MetaCost, and is inspired by the cost-sensitive CE loss (see equation (10) of [1] for a detailed definition). To be specific, class probabilities for cost-sensitive CE loss are computed by multiplying the corresponding cost and then normalizing the result vector. As a result, transformations that induce larger cost will receive larger penalization by minimizing the cost-sensitive CE loss. We neglected to include this explanation in the paper, and will revise it to make this clear. \n\nFor the first question, moving the sum of cost in front of \u201clog\u201d is unreasonable because the loss for each seed example will not be a negative log-likelihood term as in the case of cross-entropy. We can check the sanity of the objective by examining whether it reduces to standard CE loss if we set C = 1*1^\\top-I. For the second question, we indeed multiply the probability estimates by the cost, but the result vector has to be normalized before plugging into the cross entropy loss. Thus, the sum of cost will appear in front of the \u201cexp\u201d term.\n\n2. Comparison with other alternative designs\nThe cost-sensitive neural network models you mentioned are only demonstrated to be effective in the non-adversarial settings, whereas we show that our proposed classifier is effective in the adversarial setting. Thus, comparing our method with theirs is not appropriate, since it is unclear whether such alternative cost-sensitive models can be adapted and remain effective in the adversarial setting. Even if they can be adapted, it is still not the main focus of our paper, as our main goal is to show that our proposed classifier achieves significant improvements in cost-sensitive robustness in comparison with models trained for overall robustness.\n\n3.  Why are the original examples are not in cost-sensitive form?\nThe training objective (3.1) is constructed for maximizing both cost-sensitive robustness and standard classification accuracy, and allows us to use the alpha hyperparameter to control the weighting between these goals. Thus, the first term in (3.1) doesn\u2019t involve cost-sensitivity. We regard the standard classification accuracy as an important criteria for measuring classifier performance. Besides, the cost matrix for misclassification of original examples might be different from the cost matrix of adversarial transformations. For instance, misclassifying a benign program as malicious may still induce some cost in the non-adversarial setting, whereas the adversary may only benefit from transforming a malicious program into a benign one. In a scenario where the model is cost-sensitive regardless of adversaries, it could make sense to incorporate a cost-sensitive loss function as the first term also, but we have not explored this and are focused on the adversarial setting where cost-sensitivity is with respect to adversarial goals.\n\n4. What if we only optimize the original examples by cost-sensitive loss\nGiven the vulnerability of deep learning classifiers against adversarial examples, we highly doubt that if we only optimize the original training by the cost-sensitive loss it would achieve significant cost-sensitive robustness (this expectation is based on how poorly models trained with the goal of overall accuracy do at achieving overall robustness). To be more convincing, we are running an experiment to test the robustness of a standard cost-sensitive classifier and will post the results soon.\n\nReference\n[1]. Khan, et al., Cost-Sensitive Learning of Deep Feature Representations from Imbalanced Data. https://arxiv.org/abs/1508.03422\n", "title": "Objective justification"}, "B1lNPh9c3Q": {"type": "review", "replyto": "BygANhA9tQ", "review": "The paper introduces a new concept of certified cost-sensitive robustness against adversarial attacks. A cost-sensitive robust optimization formulation is then proposed for deep adversarial learning. Experimental results on two benchmark datasets (MNIST, CIFAR-10) are reported to show the superiority of the proposed method to overall robustness method, both with binary and real-value cost matrices. \n\nThe idea of cost-sensitive adversarial deep learning is well motivated. The proposed method is clearly presented and the results are easy to access. My main concern is about the novelty of the approach which looks mostly incremental as a rather direct extension of the robust model (Wong & Kolter 2018) to cost-sensitive setting. Particularly, the duality lower-bound based loss function and its related training procedure are almost identical to those from (Wong & Kolter 2018), up to certain trivial modification to respect the pre-specified misclassification costs. The numerical results show some promise. However, as a practical paper, the current empirical study appears limited in data scale: I believe additional evaluation on more challenging data sets can be useful to better support the importance of approach. \n\nPros: \n\n- The concept of certified cost-sensitive robustness is well motivated and clearly presented.\n\nCons:\n\n-  The novelty of method is mostly incremental given the prior work of (Wong & Kolter 2018).\n- Numerical results show some promise of cost-sensitive adversarial learning in the considered settings, but still not supportive enough to the importance of approach.\n\n", "title": "An incremental paper that straightforwardly applies cost-sensitive loss to robust adversarial learning.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}