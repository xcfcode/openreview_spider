{"paper": {"title": "Super-Resolution via Conditional Implicit Maximum Likelihood Estimation", "authors": ["Ke Li*", "Shichong Peng*", "Jitendra Malik"], "authorids": ["ke.li@eecs.berkeley.edu", "shichong.peng@mail.utoronto.ca", "malik@eecs.berkeley.edu"], "summary": "We propose a new method for image super-resolution based on IMLE. ", "abstract": "Single-image super-resolution (SISR) is a canonical problem with diverse applications. Leading methods like SRGAN produce images that contain various artifacts, such as high-frequency noise, hallucinated colours and shape distortions, which adversely affect the realism of the result. In this paper, we propose an alternative approach based on an extension of the method of Implicit Maximum Likelihood Estimation (IMLE). We demonstrate greater effectiveness at noise reduction and preservation of the original colours and shapes, yielding more realistic super-resolved images. ", "keywords": ["super-resolution"]}, "meta": {"decision": "Reject", "comment": "The main novelty of the paper lies in using multiple noise vectors to reconstruct the high resolution image in multiple ways. Then, the reconstruction with minimal loss is selected and updated to improve the fit against the target image. The most important control experiment in my opinion should compare this approach against the same architecture with only with m=1 noise vector (i.e., using a constant noise vector all the time). Unfortunately, the paper does not include such a comparison, which means the main hypothesis of the paper is not tested. Please include this experiment in the revised version of the paper.\n\nPS: There is another high level concern regarding the use of PSNR or SSIM for evaluation of super-resolution methods. As shown by \"Pixel recursive super resolution (Dahl et al.)\" and others, PSNR and SSIM metrics are only relevant in the low magnification regime, in which techniques based on MSE (mean squared error) are very competitive. Maybe you need to consider large magnification regime in which GAN and normalized flow-based models are more relevant."}, "review": {"Skg5JP42k4": {"type": "rebuttal", "replyto": "Bkl9w6GnkE", "comment": "Yes, we have tried using m=1, but found that this resulted in blurrier images because not allowing the net to output multiple possibilities essentially forces it to predict the mean of the different possibilities. We'll include this result in the camera-ready. ", "title": "Yes, using multiple noise vectors is important"}, "rkgIZHPj0X": {"type": "rebuttal", "replyto": "SJeKkSMYs7", "comment": "Thank you for your review. We have updated our paper to include a discussion of other methods that use a multi-stage architecture in section 2.3. To generate the upper noise vector, we generated samples with the noise input to the lower sub-network fixed and then perform kNN search among this pool of samples. Using hierarchical sampling during training improves the results at test time compared to vanilla sampling using the same number of samples, because it functions as if more samples were generated using vanilla sampling at training time, which in the context of IMLE results in improved performance. We tried performing all computation on the GPU, but ran into memory issues with our implementation (which may be an issue specific to our implementation). It\u2019s possible that projection using a random Gaussian matrix could introduce distortions, but we also tried using the original features without projection and observed no significant difference in the results (which can also be explained theoretically by the Johnson-Lindenstrauss lemma). The low-resolution input is generated by applying Gaussian blur and subsampling. In our experience, LPIPS focuses more on the high-level semantics and less on the low-level details, and so does not correlate well with human judgement in the context of super-resolution. We conducted human evaluation to eliminate possible biases that can be introduced by the choice of evaluation metric. To generate different samples from the same input image for SRGAN, we trained a SRGAN model where we add a second input to the generator containing random noise. We found that even when with random noise as input, SRGAN cannot generate multi-modal results due to mode collapse. As per your suggestion, we have updated the paper to include comparisons to [f][g][h][i] in the appendix and found that our method outperforms these methods in terms of image quality. ", "title": "Response to your review"}, "H1x-h4wiA7": {"type": "rebuttal", "replyto": "SJxh_DXN2Q", "comment": "Thank you for your review. To train the sub-networks, we first train the sub-network for the lower input resolution; then we add the second sub-network on top and train both sub-networks jointly. The feature space is pre-trained and fixed in our setting. \n\nAt test time, given a low-resolution input image, we randomly generate one noise input for each sub-network and feed all inputs into the network to get the super-resolved output. The variation exhibited by different samples corresponds to different plausible ways to super-resolve ambiguous regions of the input image. To productionize such a network in real-time systems, we can use established techniques for model compression and binarization. Our approach is as easy to productionize as GAN-based methods because the sampling procedure for our method and GAN are the same at test time (only the training algorithm differs). Multi-modality could be a problem in this case because it is important in various applications to have control over which mode we want to output; for example, if we\u2019d like to super-resolve all frames in a video, we need to make sure the same mode is selected consistently across all frames, so that the blurry regions are super-resolved in the same way in all frames. To choose one specific mode in a conscious way, we can simply choose a noise input that results in a super-resolved image that we prefer and use this (fixed) noise input for all low-resolution input images. \n\nIn our paper, we focused on a more challenging setting than that is typically considered in the literature, where the input image is of a relatively low resolution (64x64). We chose this setting because most existing methods already perform very well when the input is of a higher resolution, and so the differences between different methods are easier to discern under a more challenging lower-resolution setting. Because the limitations of SRGAN are more perceptible under this setting, SRGAN does not outperform bicubic interpolation by a large margin; however, SRIM does outperform it by a fairly large margin. ", "title": "Response to your review"}, "rJgGtNwsRQ": {"type": "rebuttal", "replyto": "HkeCTOqY2X", "comment": "Thank you for your review. We note that the existing datasets commonly used in the super-resolution literature for evaluation are quite small (they typically contain <=100 images), whereas ImageNet is a lot bigger and can provide more reliable results. Common super-resolution testing datasets are also typically used in a high-resolution setting (i.e. fairly high-resolution inputs are fed into the super-resolution algorithm), whereas our experiments are conducted with 64x64 inputs, which can contain 3-6x fewer pixels than the inputs that are used with common datasets. Because the typical inputs are at higher resolutions, the typical setting is easier and so differences between different methods are harder to discern, which is why we performed evaluation under a more challenging setting. However, as per your request, we have updated our paper to include a comparison of our method to SRGAN on the commonly used dataset of Set14 in the appendix and found that both methods perform comparably, but note that this comparison is less informative because all methods perform well on Set14. We have also updated our paper to include comparisons to more recent methods in the appendix, including EnhanceNet, SFT network, EDSR network and RDN. We found that our method outperforms these methods in terms of image quality. ", "title": "Response to your review"}, "HkeCTOqY2X": {"type": "review", "replyto": "HklyMhCqYQ", "review": "- Summary\nThis paper proposes a method based on implicit maximum likelihood estimation for single-image super-resolution. The proposed method aims at avoiding common artifacts such as high-frequency noise and shape distortion. The proposed method shows better performance than SRGAN in terms of PSNR, SSIM, and human evaluation of realism on the ImageNet dataset.\n\n- Pros\n  - The proposed method shows better performance than SRGAN in terms of PSNR, SSIM, and human evaluation.\n  - The selection of the evaluation methods is appropriate. In the field of image super-resolution tasks, both signal accuracy (e.g., PSNR) and perceptual quality (e.g., human evaluation) are important.\n\n- Cons\n  - The experiments are conducted thoroughly in the ImageNet, but the selection of the dataset is not appropriate. It would be better to apply the proposed method to other datasets which are used recent papers.\n  - Also, the selection of the methods to be compared is not appropriate. It would be better to provide recent state-of-the-art methods and compare the proposed method with them.\n\nThe proposed approach is interesting and promising, but the selection of the methods and datasets to be compared is not appropriate.", "title": "Interesting approach, but experiments are not appropriate", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJxh_DXN2Q": {"type": "review", "replyto": "HklyMhCqYQ", "review": "This paper proposes a technique to find a maximum-likelihood estimate of the super-resolved images under latent variables without computing it. Paper is mostly clearly written and except for some sections, it provides enough details. The work is original enough but might need some improvement or more explanation in experiments/result section.\n\nPros:\n-The idea seems to be original enough, simple and easy to implement.\n-A nice follow-up of the recent work in NN search and Implicit maximum likelihood estimation. \n-Many details that could be helpful for further research in the area are given.\n\nCons:\n-Regarding methodology, an unclear point in the paper is how different networks trained according to algorithm 1. Is each sub-network trained separately? Is the visual perception based feature space pre-trained and fixed, or is it jointly retrained with the super-resolution network? \n\n-Another critical point is post-training, particularly the way learned parameters are used could be explained better: Given a super-resolution model f, how the super-resolution of a single image is performed? What is the sampling variation? How likely such a network can be productionized in real-time systems (e.g., digital displays or embedded systems)? How does the proposed approach compared to GAN based methods with regards to that? Is multi-modality a problem in this case? Any way to choose one specific mode in a conscious way?\n\n-My main concern about the paper is the results section: Authors perform both large-scale offline comparison (imagenet) and a small subset human evaluation. The results in human evaluation need some explanation. This comparison is identical to several previous 1-1 comparisons performed in literature and almost every single such comparison it has been found that state of the art techniques (e.g., 10+ years of super-resolution algorithms) significantly outperform bicubic interpolation. However, Table 2 in the paper suggests that both SRGAN and SRIM barely beats bicubic interpolation. For example, authors in https://arxiv.org/pdf/1209.5019.pdf showed that a relatively older supervised technique beat bicubic 90% of the time. There seems to be some explanation needed here: Is it the sample size? Are the samples from both SRIM and SRGAN very variable?\n", "title": "Paper is written well and except for some sections, it provides enough details. The work is original enough but might need some improvement or more explanation in experiments/result section.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJeKkSMYs7": {"type": "review", "replyto": "HklyMhCqYQ", "review": "Quality: The overall quality of this paper is good. It adopts a simple but novel idea on SISR and shows clear improvement against existing method (e.g., SRGAN). \n\nClarify: This paper is well written and easy to follow. It shows a clear motivation for adopting the implicit probabilistic model.\n\nOriginality: To the best of my knowledge, this paper is the first work to learn multi-modal probabilistic model for SISR.\n\nSignificance: While the results can be further improved (still look a bit blurred), this paper shows an interesting and important direction to learn better mappings for SISR.\n\nPros:\n+ The writing is clear.\n+ The proposed method is well motivated and easy to understand.\n+ The experimental results include both objective and subjective evaluations.\n\nCons:\n- The two-stage architecture is similar to the following generative models and SR methods. It\u2019s suggested to discuss them as well.\n[a] Denton, E. L., Chintala, S., & Fergus, R. \u201cDeep generative image models using a\ufffc laplacian pyramid of adversarial networks\u201d. NIPS, 2015.\n[b] Karras, T., Aila, T., Laine, S., & Lehtinen, J. \u201cProgressive growing of gans for improved quality, stability, and variation\u201d. ICLR 2018.\n[c] Lai, W. S., Huang, J. B., Ahuja, N., & Yang, M. H. \u201cDeep laplacian pyramid networks for fast and accurate super-resolution.\u201d CVPR 2017.\n[d] Wang, Y., Perazzi, F., McWilliams, B., Sorkine-Hornung, A., Sorkine-Hornung, O., & Schroers, C. \u201cA Fully Progressive Approach to Single-Image Super-Resolution.\u201d. CVPR Workshops 2018.\n\n- In the hierarchical sampling (section 2.4), it\u2019s not clear how to generate the upper noise vector \u201cconditioned on the lower noise vector\u201d. \n\n- The hierarchical sampling seems to improve the efficiency of training. I wonder does it affect the results of testing?\n\n- In the implementation details (section 2.5), I don\u2019t understand why you need to transfer the the feature activations from GPU to CPU? I think all the computation can be done on GPU for most common toolboxes. Projecting the activations to a lower dimension with a \u201crandom Gaussian matrix\u201d sounds harmful to the results.\n\n- How do you generate the low-resolution images? Are you using bicubic downsampling or other approaches? This detail should be clarified.\n\n- While the evaluation with PSNR and SSIM is a reference to show the quality, many literatures already show that PSNR and SSIM are not correlated well with human perception. It is suggested to also evaluate with some perceptual metrics, e.g., LPIPS [e].\n[e] Zhang, R., Isola, P., Efros, A. A., Shechtman, E., & Wang, O. \u201cThe unreasonable effectiveness of deep features as a perceptual metric.\u201d CVPR 2018.\n\n- In Figure 7, how do you generate different results from the same input image for SRGAN? From my understanding, SRGAN doesn\u2019t take any noise vector as input and cannot generate multi-modal results.\n\n- I feel that the comparison with only SRGAN is not enough. There are some GAN-based SR methods [f][g]. It\u2019s also suggested to compare with MSE-based state-of-the-art SR algorithms [h][i].\n\n[f] Sajjadi, M. S., Sch\u00f6lkopf, B., & Hirsch, M. \u201cEnhancenet: Single image super-resolution through automated texture synthesis.\u201c ICCV 2017.\n[g] Wang, X., Yu, K., Dong, C., & Loy, C. C. \u201cRecovering realistic texture in image super-resolution by deep spatial feature transform.\u201d CVPR 2018.\n[h] Lim, B., Son, S., Kim, H., Nah, S., & Lee, K. M. \u201cEnhanced deep residual networks for single image super-resolution.\u201d CVPR Workshops 2017.\n[i] Zhang, Y., Tian, Y., Kong, Y., Zhong, B., & Fu, Y. \u201cResidual dense network for image super-resolution.\u201d CVPR 2018.\n\n", "title": "A super-resolution method with with fewer visual artifacts than the SRGAN method.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HyxcMnORcX": {"type": "rebuttal", "replyto": "Bylsy6q997", "comment": "Thanks for your clarification. EnhanceNet is similar to SRGAN in terms of perceptual quality but has higher reconstruction error, as shown by Figure 2 in the referenced paper. SFTGAN uses a semantic segmentation model to predict the categories of objects and therefore uses auxiliary supervision in the form of segmentation masks. Our method does not use such supervision, and so cannot be directly compared. ProGAN is somewhat lesser known; thanks for bringing it to our attention. We ran their model and found that the results exhibit similar types of artifacts as those of SRGAN. ", "title": "Response to your clarification"}, "rJlQGqnY97": {"type": "rebuttal", "replyto": "rJgtXKEv9Q", "comment": "Thanks for your comment. As is standard in machine learning, the training and test sets are from the same collection of images (ImageNet) to eliminate any possibility of biases in the evaluation results due to domain shift. We chose to use ImageNet for training because that was used by SRGAN, and this choice mandated the use of ImageNet for testing. \n\nNevertheless, as per your suggestion, we evaluated our method on BSD100 and found the results were comparable to those on ImageNet: SSIM was 0.7254 (compared to 0.7153 on ImageNet) and PSNR was 26.39 (compared to 25.36 on ImageNet). ", "title": "Response to your comment"}, "ByxSpthFcX": {"type": "rebuttal", "replyto": "SkxSWW0mqX", "comment": "Thanks for your comment. We note that the referenced paper reports on the performance of methods submitted to a recently concluded ECCV workshop challenge. Because the methods were only released a week before the submission deadline, whose code and implementation details remain unavailable in many cases, we weren\u2019t able to compare to these methods. We do note that Figure 2 in the referenced paper shows that SRGAN is one of the best available methods in terms of visual quality at the time the challenge was conducted. ", "title": "Response to your comment"}}}