{"paper": {"title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning", "authors": ["Maximilian Igl", "Gregory Farquhar", "Jelena Luketina", "Wendelin Boehmer", "Shimon Whiteson"], "authorids": ["~Maximilian_Igl1", "~Gregory_Farquhar1", "~Jelena_Luketina1", "~Wendelin_Boehmer1", "~Shimon_Whiteson1"], "summary": "We find that transient non-stationarity can worsen generalization in reinforcement learning and propose a method to overcome this effeect.", "abstract": "Non-stationarity can arise in Reinforcement Learning (RL) even in stationary environments. For example, most RL algorithms collect new data throughout training, using a non-stationary behaviour policy. Due to the transience of this non-stationarity, it is often not explicitly addressed in deep RL and a single neural network is continually updated. However, we find evidence that neural networks exhibit a memory effect, where these transient non-stationarities can permanently impact the latent representation and adversely affect generalisation performance. Consequently, to improve generalisation of deep RL agents, we propose Iterated Relearning (ITER). ITER augments standard RL training by repeated knowledge transfer of the current policy into a freshly initialised network, which thereby experiences less non-stationarity during training. Experimentally, we show that ITER improves performance on the challenging generalisation benchmarks ProcGen and Multiroom.", "keywords": ["Reinforcement Learning", "Generalization"]}, "meta": {"decision": "Accept (Poster)", "comment": "There is a substantial contribution in identifying novel questions/issues, as this paper certainly does. Neither I nor the reviewers have seen this issue of transient non-stationary before, and the authors make a compelling case for it, especially in the supervised setting with the CIFAR experiments. It is less compelling through the RL experiments. As such, this paper is likely to inspire new work within the field. To me, Figure 1 is the most interesting aspect of the whole paper.\n\nThe initial approach by the authors is questionable in its effectiveness, and is likely to be improved by others in the future. Some of the results in Figure 3 are questionable, especially when you look at the individual curves in Figure 8. So overall, this means that the authors have identified a truly novel issue, and proposed an initial method that is just okay.  They've done a nice job investigating this in a supervised setting, and need to push further in the RL setting.\n\nThe question is whether the novel contribution of the problem outweighs that the algorithm and its evaluation could use improvement.  The reviewers debated this in the discussion, with points on both sides, but the novelty of the question/issue (even if the investigation could use work) is likely to inspire further work in this direction.\n\nOther notes:\nThe authors could have evaluated the (impractical) version of their algorithm proposed in the first paragraph of Section 4.2. This would inform 1) whether their parallel training approximation is close to the optimal algorithm, and 2) whether the optimal (impractical) algorithm is capable of improving generalization significantly. If the latter is true, it would leave open a huge avenue of investigation to find better approximate solutions."}, "review": {"UYpUHiTBmM1": {"type": "rebuttal", "replyto": "FU75JdoYQ4g", "comment": "While we cannot provide theoretical guarantees, we provide empirical evidence in multiple experiments that re-learning can improve generalisation in the situations discussed where transient non-stationarity negatively affected generalisation. \n\nThe architectures and optimizers we use are standard in the community and cover multiple cases: our SL experiments use a (adapted) ResNet with SGD while our RL experiments use Adam with either a CNN architecture for the gridworlds or the widely used Impala architecture for ProcGen.\nConsequently, we expect our results to be applicable to many settings that are of relevance to the community. \n\nLastly, as noted in our related work section, re-learning is used very differently in the context of continual or life-long learning: While in those cases one tries to prevent forgetting of early data, in our case forgetting the impact of earlier data is precisely our goal.\n", "title": "-"}, "P07OlAWoKKx": {"type": "rebuttal", "replyto": "Qun8fv4qSby", "comment": "Dear Reviewers,\n\nthank you for your thoughtful reviews! \n\nWe have updated the paper to incorporate your feedback. Apart from smaller modifications, the two main changes are\n* An additional experiment on a new environment (Sokoban) in section 5.2 to provide more RL results. (Some seeds are still running, explaining the sudden change in standard error at around 1.4e8) \n* A more detailed explanation of the supervised learning experiments in section 5.4 (previously 5.3), including a new schematic figure (fig. 4) explaining the training setup. \n\nWe've also responded to your reviews individually and are looking forward to any further comments, questions or feedback.", "title": "Updated Paper"}, "zxZ-xP0Ov51": {"type": "rebuttal", "replyto": "JdNx_mlInBy", "comment": "Thank you for your review.\n\nWe agree that off-policy and especially offline RL is both an interesting application and might facilitate more in-depth analysis by allowing one to modify the state-distribution, similar to our SL experiments and we are planning to explore this in future work.\n\nThank you also for the imitation learning suggestion! It\u2019s something we hadn\u2019t considered so far. \n\n", "title": "Thank you for your review"}, "cVHhxW7hjm": {"type": "rebuttal", "replyto": "i5xYGGV8pO2", "comment": "Thank you for your review! \n\nWe agree that Figure 5 (middle) is quite packed with information. We updated the corresponding description in the text and added a schematic figure for the experimental procedure, hoping it provides more clarity. We\u2019re very grateful for any additional feedback/ideas. \n\n> While the method being presented is quite simple, it also seems a bit ad-hoc. For instance, it would be nice if the teacher-student distillation could be supported by some convergence guarantees, e.g. by showing that (under some circumstances) replacing the teacher by the student does not increase the loss.\n\nThank you for the suggestion, it\u2019s something we plan to explore in future work. We expect it to be quite tricky as the observed effect of non-stationarity wouldn\u2019t be present for simple (e.g. linear) function approximators while generalization in typical neural networks isn\u2019t well understood yet.  \n\n> The set of benchmarks considered in the paper is a good proof-of-concept, but additional experiments with different environments and base-algorithms is necessary to better judge the merits of the approach.\n\nWe have added an additional experiment on the Sokoban environment, which also utilizes procedural generation of levels. \n", "title": "Thank you for your review"}, "M2GZUoY8rPB": {"type": "rebuttal", "replyto": "vF2ZzSgMQXW", "comment": "Thank you for your review! We provide some clarification below.\n\n> The new algorithm ITER doubles the computational costs.`\n\nWhile this is indeed the case (however only during the distillation phase!), this additional computation can be performed in parallel and as such the runtime is not doubled. We also do not require any additional environmental steps, which is often the more important bottleneck: in many scenarios, some extra computation by the learner is likely a small price to pay for improved generalisation. In our experiments, the runtime was about 1.3-1.5 times as high for the large ProcGen models, which we expect could be brought down even further by running the distillation step on a separate GPU. \n\n> Experiment results on Procgen is fair but not significant.\n\nThe magnitude of improvements is in a comparable range to other methods that are evaluated on the ProcGen benchmarks (e.g. Igl et al.: Generalization in RL with Selective Noise Injection; Lasking et al.: RL with Augmented Data; Raileanu et al.: Automatic Data Augmentation for Generalization in DLR; Cobbe et al.: Phasic Policy Gradient; Jiang et al.: Prioritized level replay).\nThe absolute magnitude of improvement that is possible is therefore as much a feature of the environment as of the algorithm: generalization and its improvement on ProcGen are hard. The important point is that ITER does improve generalization, therefore providing evidence that the discussed non-stationarity impacts generalization. \nAlso please note that the magnitude of improvement is greater on Multiroom and Sokoban [results added in paper revision] , which are complex environments (despite being visually slightly simpler). \n\n> Experiments on Page 7 are trying to further illustrate the mechanism for such a phenomenon. However, this subsection is not very clear.\n\nWe have updated this section (including a new schematic figure explaining the training setup)  and are grateful for any additional feedback on clarity or how it could be improved. \n\nThank you for your other comments, we\u2019ve incorporated them in the updated draft. \n", "title": "Thank you for your review"}, "5mFUvnQAr9Q": {"type": "rebuttal", "replyto": "Fbm5bPAM8LM", "comment": "Thank you for your review! We provide some clarifications below.\n\n> The authors do not clearly define the generalization which the relearning aims to improve. It may depend on the neural network architecture and environment and so on. However, presuming that the paper reports the best gain possible, it is hard to accept that ITER always improves generalization.\n\nWe refer to generalisation to environment states not yet seen during training (we emphasise this now more in the beginning of section 4). Unlike many commonly used RL benchmarks, our experiments rely on procedural generation of tasks. This creates a sufficient diversity of states to make generalisation a meaningful performance bottleneck, and in the case of ProcGen allows us to explicitly measure a generalisation gap by separating levels used for training and testing, similar to the use of a validation set in supervised learning.\n\nWe expect our approach to improve performance in environments where such generalisation is important, either because testing encounters states that are guaranteed to be unseen (like ProcGen) or the agent is faced with a very large number of diverse states (like Multiroom, or Sokoban [results added in paper revision] in which each episode has a different layout).\n\nGood generalisation can also become important when diverse states arise from noise in transition dynamics, or from broad starting state distributions. However, we wouldn\u2019t expect our method to improve performance in near deterministic environments like Atari games.\n\n> Although the underlying intuition sounds delightful, the understanding on the non-stationarity is insufficient. In particular, the authors design a setup of \"supervised\" learning to show importance of non-stationarity. However, it is a weak evidence on the importance of non-stationarity in \"reinforcement\" learning. Indeed, the gain of using ITER is not significant.\n\nOur fundamental insight is that certain kinds of non-stationarity interfere with the SGD training of neural networks and can degrade their generalization performance, which should be independent of whether the network is used in a SL or RL setup. \nThis is easiest shown in a supervised learning setup as it has fewer confounding factors. Furthermore, there is no reason this should not, in principle, translate to RL. The improved performance through ITER provides evidence that this is indeed the case and non-stationarity in RL is impeding generalization (at least in environments such as those described above). \n\nWe agree that research aiming at better understanding of which type of non-stationarity in RL is the main problem for generalization is an exciting direction for future work. However, we believe this is out of scope of this paper as such research is quite challenging and we already show (a) that such a problem exists and (b) how to alleviate it during training, which we believe is a substantial contribution.\nFurther ablation studies in RL are tricky because independently modifying for example the state distribution, causes the policy updates to be off-policy, thereby introducing significant confounding factors.\n\nThe magnitude of improvements is in a comparable range to other methods that are evaluated on the ProcGen benchmarks (e.g. Igl et al.: Generalization in RL with Selective Noise Injection; Lasking et al.: RL with Augmented Data; Raileanu et al.: Automatic Data Augmentation for Generalization in DLR; Cobbe et al.: Phasic Policy Gradient; Jiang et al.: Prioritized level replay).\nThe absolute magnitude of improvement that is possible is therefore as much a feature of the environment as of the algorithm: generalization and its improvement on ProcGen are hard. The important point is that ITER does improve generalization, therefore providing evidence that the discussed non-stationarity impacts generalization. \nAlso please note that the magnitude of improvement is greater on Multiroom and Sokoban, which are complex environments (despite being visually slightly simpler). \n\n> I have a concern on the little gain, which is shown by the comparison between PPO vs. PPO+ITER with the same hyperparameters. A fair comparison may use the best hyerparameters for each of PPO and PPO+ITER. Or, at least, there need comparisons with different hyperparameters to claim consistent improvement.\n\nTo find hyperparameters, we first tune them for optimal performance with PPO. For ITER, we then keep these PPO hyperparameters fixed and only tune t_distill (all other hyper-parameters were kept fixed after some preliminary experiments). Consequently, independent tuning of hyperparameters could therefore only improve PPO+ITER, not PPO. \nWe chose not to independently tune PPO+ITER for computational reasons and to prevent overfitting of hyperparameters for our method. \n\n", "title": "Thank you for your review"}, "i5xYGGV8pO2": {"type": "review", "replyto": "Qun8fv4qSby", "review": "The paper deals with a relevant issue. The simplified supervised learning setting is a good way of looking at the issue of non-stationarity in isolation and it makes a compelling case that neural networks optimized by SGD can have generalization issues in settings where the data distribution changes over time, even after the data distribution converges. The solution proposed by the paper is simple and can be applied to most off-the-shelf RL algorithms. My main criticism would be that the hybrid objective feels rather ad-hoc and that the proposed method could use a bit more theoretical justification. Also, since the issue being tackled here is quite general to RL, a wider set of benchmarks and base-algorithms (in addition to PPO) would be necessary to get a better picture.\nRegardless, I believe the community would benefit from the inclusion of this paper. \n\n### Pros\n\n* The paper is well-written. The story is easy to follow and well-motivated.\n* The central problem that is being tackled is highly relevant.\n* I like that the problem is illustrated in a supervised-learning setting, which allows to investigate without the noise of a typical RL setup.\n* The proposed modification is simple and can be applied to a wide-range of algorithms.\n* The \"legacy feature\" effect is an interesting phenomenon and the additional experiments inspecting it are useful.\n\n### Cons\n\n* In Figure 5, the middle panel is a bit hard to parse. I don't know how to improve this but in its current state, too many variables are being presented at once.\n* While the method being presented is quite simple, it also seems a bit ad-hoc. For instance, it would be nice if the teacher-student distillation could be supported by some convergence guarantees, e.g. by showing that (under some circumstances) replacing the teacher by the student does not increase the loss.\n* The set of benchmarks considered in the paper is a good proof-of-concept, but additional experiments with different environments and base-algorithms is necessary to better judge the merits of the approach.", "title": "This paper tackles the problem of distribution shift in the context of reinforcement learning, e.g. resulting from gradual exploration of the state-space or optimization of the critic. The main idea is that this kind of non-stationarity can have a lasting negative effect on generalization, even after the data distribution has converged. The paper proposes a solution which is based on iteratively distilling the current model into a new model that is optimized from scratch.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "vF2ZzSgMQXW": {"type": "review", "replyto": "Qun8fv4qSby", "review": "This paper investigates an interesting problem that transient non-stationarity can affect the generalization of the neural network. This paper first conducts experiments on a supervised learning task to illustrate that transient non-stationarity can lead to degenerated performance on testing set. Then, the paper proposes an RL algorithm called ITER to avoid the negative impact of such non-stationarity.\n\nStrengths: \n+ This paper observes a novel problem that may appear in RL and designs a new algorithm to prevent such a problem.\n+ This paper investigates this problem through experiments on supervised learning tasks.\n\nWeaknesses:\n- The new algorithm ITER doubles the computational costs.\n- Experiments on Page 7 are trying to further illustrate the mechanism for such a phenomenon. However, this subsection is not very clear.\n- Experiment results on Procgen is fair but not significant.\n\nMinor comments:\n- In Section 2, the advantage function is defined as A^\\pi(s,a,s') but later used as A^\\pi(s,a) without additional explaination.\n- f in $\\mathcal{D}_{f,m}$ is not explained when it first appears. I guess it is the ratio of the modification.", "title": "Official Blind Review #3", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Fbm5bPAM8LM": {"type": "review", "replyto": "Qun8fv4qSby", "review": "The paper proposes a mechanism of re-learning (ITER; iterative re-learning) to handle issue from non-stationarity in RL. Some gain of using the proposed method is experimentally presented. My major concern is the limited understanding of the non-stationarity issue and ITER. \n\nPros)\n- The authors propose a new approach to resolve the non-stationarity issue in RL. The intuition itself makes a sense as human often performs such relearning processes to escape from the local minimum. \n\n- Some gain of ITER is empirically demonstrated in various setting.\n\nCons)\n- Although the underlying intuition sounds delightful, the understanding on the non-stationarity is insufficient. In particular, the authors design a setup of \"supervised\" learning to show importance of non-stationarity. However, it is a weak evidence on the importance of non-stationarity in \"reinforcement\" learning. Indeed, the gain of using ITER is not significant.\n\n- The authors do not clearly define the generalization which the relearning aims to improve. It may depend on the neural network architecture and environment and so on. However, presuming that the paper reports the best gain possible, it is hard to accept that ITER always improves generalization.\n\n- I have a concern on the little gain, which is shown by the comparison between PPO vs. PPO+ITER with the same hyperparameters. A fair comparison may use the best hyerparameters for each of PPO and PPO+ITER. Or, at least, there need comparisons with different hyperparameters to claim consistent improvement.\n", "title": "Limited understanding on the non-stationarity in RL and benefit from ITER", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "JdNx_mlInBy": {"type": "review", "replyto": "Qun8fv4qSby", "review": "This paper presents empirical evidence that non-stationarity data typical in deepRL settings can affect the intermediate representation of deep neural network and affect testing performance. The paper is easy to read and the authors provide experiments to support the their observations and claims. Overall I think this is a good paper and in the following I suggest some good to have additions.\n\n(1) The examples for the supervised learning setting clearly demonstrates the impact of non-stationary data. However, given that this is inspired by the problems under DRL setting, it will be interesting to do more analysis of this effect on some DRL tasks. For example, an analysis for offline RL might be a good setting to study this effect.\n\n(2)Imitation learning algorithm like Dagger might be another good example to demonstrate the effect of nonstationarity. The data under the Dagger setting is also changing overtime and it will be interesting to see how it affects the student policy.\n\n(3) The RL experiment is mainly done in the on policy (PPO) settings. Some experiments with off policy RL setting might be useful, and the effect of the non-stationarity might be more pronounced as well.", "title": "interesting paper analyzing how non-stationarity affects generaliztion", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}