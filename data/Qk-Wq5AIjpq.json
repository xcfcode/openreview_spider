{"paper": {"title": "PAC Confidence Predictions for Deep Neural Network Classifiers", "authors": ["Sangdon Park", "Shuo Li", "Insup Lee", "Osbert Bastani"], "authorids": ["~Sangdon_Park1", "lishuo1@seas.upenn.edu", "~Insup_Lee1", "~Osbert_Bastani1"], "summary": "We propose a novel algorithm for constructing predicted classification confidences for DNNs that comes with provable correctness guarantees, and demonstrate how our predicted confidences can be used to enable downstream guarantees in two settings.", "abstract": "A key challenge for deploying deep neural networks (DNNs) in safety critical settings is the need to provide rigorous ways to quantify their uncertainty. In this paper, we propose a novel algorithm for constructing predicted classification confidences for DNNs that comes with provable correctness guarantees. Our approach uses Clopper-Pearson confidence intervals for the Binomial distribution in conjunction with the histogram binning approach to calibrated prediction. In addition, we demonstrate how our predicted confidences can be used to enable downstream guarantees in two settings: (i) fast DNN inference, where we demonstrate how to compose a fast but inaccurate DNN with an accurate but slow DNN in a rigorous way to improve performance without sacrificing accuracy, and (ii) safe planning, where we guarantee safety when using a DNN to predict whether a given action is safe based on visual observations. In our experiments, we demonstrate that our approach can be used to provide guarantees for state-of-the-art DNNs.", "keywords": ["classification", "calibration", "probably approximated correct guarantee", "fast DNN inference", "safe planning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper provides a method for constructing PAC confidence scores for pre-trained deep learning classifiers. The reviewers were all positive about the paper.\n\nPros:\n- Has provable guarantees on the reliability of the prediction. Such guarantees are quite desirable in practice.\n- The problem of neural network uncertainty is important and timely problem, especially in safety-critical applications.\n- The method is simple and well-motivated.\n- Strong empirical performance.\n- Interesting applications to fast DNN inference and safe planning.\n\nCons:\n- Lack of generalization guarantees-- the guarantees in the paper only hold on the training set; but in practice, performance in test is what's important.\n- Only a handful of baselines tested against, most of which (if not all) were naive."}, "review": {"_W0xIiVPqa": {"type": "review", "replyto": "Qk-Wq5AIjpq", "review": "This is a paper that focusses on the timely and important problem of uncertainty quantification for the predictions of deep neural network classifiers. The authors propose constructing calibrated outputs that have provable correctness guarantees, using PAC-style arguments. \n\nThe authors also demonstrate how this framework can be utilized for computationally efficient predictions by combining a smaller, faster albeit less accurate model with a larger, more accurate model, utilizing the latter only when the former is less confident. For this to work, one needs good guarantees on the DNN's estimates of its confidence -- and creating such guarantees forms the crux of the paper.\n\nPros:\n+ Paper is well written\n+ Important and timely problem, motivating arguments are well constructed\n+ Paper appears to be mathematically sound though I did not check all the proofs in the appendix.\n\nCons\n+ One of the crucial assumptions is that the data during test time will be from in-distribution. While I understand it is hard and maybe impossible to provide any guarantees for out-of-distribution data,  it is important to realize that one of the most common ways in which DNNs can fail when deployed in safety critical systems is when faced with predicting from an out-of-distribution data. So it is unclear how  practically applicable the proposed method is. \n\n+ If somehow, one is always guaranteed to work within in-distribution data, then the authors should compare other methods that improve calibration (but don't have guarantees) with the proposed method, both in terms of calibration and computational efficiency in the fast-model/slow-model approach.\n\n+ Also I do not see any calibration performance in the experimental results. While I understand the proposed method is using histogram binning and is not a new calibration method per se, these results should be included. That is,show how  accuracy correlates with the softmax predictions (on the test set) using proposed technique.\n\n+ Since the proposed method provides stronger UQ than other methods, I would also like to see accuracy-vs-coverage curves for benchmark datasets, and compare this to such curves for the baseline (where one thresholds on the winning softmax scores).\n\nOverall, this is a very worthwhile line of work, and I feel the paper has merit, but given that some important results are missing, as it stands, does not meet the bar for acceptance. \n\n== Post rebuttal update ==\n\nSee my response to author's rebuttal below. In light of new experimental results, I feel this now meets the bar of acceptance at ICLR, and hence  updating my score to 6.", "title": "Method to provide prediction guarantees of DNNs, but some crucial experimental results are missing.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "GQK1pFFLkDN": {"type": "review", "replyto": "Qk-Wq5AIjpq", "review": "In this work, the authors provide a method for a posteriori calibration of DNN uncertainty with emphasis on constructing a classifier that has PAC uncertainty guarantees. The authors define a \u2018calibrated\u2019 probability prediction to be one such that given (for example) an image labeled as a cat, that the probability assigned to the class label \u2018cat\u2019 by the predictor is equivalent to the probability that the classifier correctly predicts images from the class \u2018cat.\u2019 The authors seek to create a \u2018provably\u2019 correct classifier under iid data assumptions. \n\nAccounting for and re-calibrating the poor uncertainty of neural networks is a central problem for deep learning especially when operating in safety-critical domains and so the work is certainly attempting to attack a worthwhile problem.\n\nOne concern I have about the theory laid out by the authors is that they place no conditions on \\hat{f}. That is, one could have a classifier which has arbitrarily bad uncertainty calibration. Take for example a predictor \\hat{f} which assigns \\hat{p}(x) = 0.9 to every input \u2018x\u2019 regardless of its ultimate accuracy on the class, then given a new input with unknown label, it is not clear to me exactly how the framework would use the intervals to improve the uncertainty of this classifier, especially given that the class of this new point is unknown. I would appreciate if the authors could help me understand this case as I think it is indicative of a misunderstanding on my part. \n\nAnother concern is the fact that getting well calibrated Clopper-Pearson intervals with good statistical guarantees takes a non-trivial number of samples and it appears this would scale with the number of classes. Thus, for a task like ImageNet, the authors inference model would require the set Z to be quite large (quoted at 20000). It seems that considering this overhead would greatly slow the average inference time. Is this overhead considered in figure 2a? I think this is a OK trade-off to make when in safety-critical scenarios, but then the authors give \u201cfast inference\u201d as one of their primary applications, it seems like a bit more discussion of this may be warranted. \n\nThe authors use the word \u201cprovably\u201d correct in a couple places (page 2 and 3) where the correct term they should use is \u201cprobably\u201d or PAC. Saying that a statistical guarantee is \u201cprovably\u201d correct is an over claim and in my view this should be corrected.\n\nMy last concern is not so much with what the authors have presented, just in the fact that the limitation is not clearly stated. When using statistical guarantees such as those given under the PAC framework, the iid assumption is almost always necessary. Yet, it can be limiting in both the case of planning and image classification, especially when we are considering safety-critical applications. In such applications, one often considers worst-case scenarios and in these cases the iid assumption usually does not hold. Thus, the PAC guarantees in this paper would seemingly be invalidated. For example, adversarial examples may be out of the support of the data distribution, but are still valid inputs. In the presence of adversarial examples the uncertainty guarantees presented here are rendered void. Similarly, there is often concept drift in RL and control problems. Ultimately, I don\u2019t think this is only a minor mark against the work, and one that can be overlooked given that such cases are at least clearly stated as limitations of the approach.\n\n\nPost-Rebuttal Comment: \n\nI would like to thank the authors for thoughtfully answering my concerns and questions. I think the small modifications made in response to my comments have made the paper much easier to understand and I think the work is well presented and positioned. Ultimately, I have increased my score to Accept on the basis that I no longer have any major criticism of the work. I do hope the authors can make a more prominent note about appendix F1 in the main text as I think it is an interesting and important thing to highlight for those who may be skeptical. ", "title": "Certainly interesting, but perhaps needs to address limitations more clearly", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "q3YEoKeozkP": {"type": "rebuttal", "replyto": "Qk-Wq5AIjpq", "comment": "Dear reviewers, \n\nWe have updated our paper based on your feedback. List of changes:\n\n* (clarity) We did our best to enhance overall clarity of the paper\n* (experiment) We added evaluation and comparison results on calibration performance (Section 5.1 and Appendix G.1)\n* (experiment) We added additional baselines in fast DNN inference (e.g., Figure 3)\n* (experiment) We added additional baselines in safe planning (e.g., Figure 4)\n* (proof) We added an optimality proof of the greedy algorithm in fast DNN inference for the case M=2 in Theorem 3\n* (discussion) We added additional discussions mentioned in the response (across paper and in Appendix F)\n\nPlease let us know if you have any additional concerns. \n", "title": "Update for the Common Response to Comments by all reviewers"}, "00lpsrVKp4B": {"type": "rebuttal", "replyto": "_W0xIiVPqa", "comment": "***Comment 4***: Since the proposed method provides stronger UQ than other methods, I would also like to see accuracy-vs-coverage curves for benchmark datasets, and compare this to such curves for the baseline (where one thresholds on the winning softmax scores).\n\n***Response 4 (updated)***: We have added an accuracy-coverage plot for our approach along with baselines in Figure 7 in Appendix G.", "title": "Update for Response to Comments by Reviewer"}, "0QgDyUSyBub": {"type": "rebuttal", "replyto": "GQK1pFFLkDN", "comment": "Thanks for the valuable discussion. The following are the answers to your concerns. Let us know if further clarification is needed. \n\n---\n\n**Comment 1**: One concern I have about the theory laid out by the authors is that they place no conditions on $\\hat{f}$. That is, one could have a classifier which has arbitrarily bad uncertainty calibration. Take for example a predictor $\\hat{f}$ which assigns $\\hat{p}(x) = 0.9$ to every input $x$ regardless of its ultimate accuracy on the class, then given a new input with unknown label, it is not clear to me exactly how the framework would use the intervals to improve the uncertainty of this classifier, especially given that the class of this new point is unknown.\n\n**Response 1**: In fact, ours can be applied to any $\\hat{f}$. If $\\hat{f}$ has poor calibration, then our algorithm will simply rescale the probabilities to be very uncertain (as is the case with the histogram binning algorithm underlying our approach). Intuitively, the interval we produce captures the uncertainty in the calibrated probabilities. In the reviewer\u2019s example, all the examples will fall in a single bin B. Then, histogram binning will rescale the confidence of this bin is the empirical accuracy of examples in this bin -- e.g., around $\\hat{p}(x) = 0.5$ if $\\hat{f}$ is random. Our contribution is to additionally provide a confidence interval around this value. We will clarify this point in our paper.\n\n---\n\n**Comment 2**: ...Thus, for a task like ImageNet, the author's inference model would require the set Z to be quite large (quoted at 20000). It seems that considering this overhead would greatly slow the average inference time. Is this overhead considered in figure 2a?\n\n**Response 2**: Our approach constructs the intervals for each bin during design time. They do not need to be re-computed for new test instances. In particular, the overhead is negligible and O(1) (i.e., simply a table lookup for the bin containing the predicted probability). This overhead is considered in Figure 2c when measuring CPU and GPU times. We will clarify this point in our paper. \n\n---\n\n**Comment 3**: The authors use the word \u201cprovably\u201d correct in a couple places (page 2 and 3) where the correct term they should use is \u201cprobably\u201d or PAC. Saying that a statistical guarantee is \u201cprovably\u201d correct is an over claim and in my view this should be corrected.\n\n**Response 3**: We meant \u201cprovably correct\u201d in the sense of \u201cprovable PAC guarantee\u201d. To avoid confusion, we will change the term \u201cprovably\u201d to \u201cPAC\u201d throughout our paper.\n\n---\n\n\n**Comment 4**: My last concern is not so much with what the authors have presented, just in the fact that the limitation is not clearly stated. When using statistical guarantees such as those given under the PAC framework, the iid assumption is almost always necessary. Yet, it can be limiting in both the case of planning and image classification, especially when we are considering safety-critical applications. In such applications, one often considers worst-case scenarios and in these cases the iid assumption usually does not hold. Thus, the PAC guarantees in this paper would seemingly be invalidated. For example, adversarial examples may be out of the support of the data distribution, but are still valid inputs. In the presence of adversarial examples the uncertainty guarantees presented here are rendered void. Similarly, there is often concept drift in RL and control problems. Ultimately, I don\u2019t think this is only a minor mark against the work, and one that can be overlooked given that such cases are at least clearly stated as limitations of the approach.\n\n**Response 4**: We will explicitly state the assumption in our introduction and describe its limitations in terms of applications to reinforcement learning, concept drift, and adversarial examples. We agree that providing theoretical guarantees for out-of-distribution data is an important direction; however, we believe that our work is an important stepping stone towards this goal. See the \u201cCommon Response\u201d for a more detailed discussion of future directions.\n", "title": "Response to Comments by Reviewer"}, "gA7BphVSLSS": {"type": "rebuttal", "replyto": "JNohqagqTjT", "comment": "Yes, our bound is a generalization bound -- it says that the confidence intervals are valid for new examples drawn from the test distribution (assuming it equals the training distribution). In particular, it says that with probability at least $1 - \\delta$, we construct confidence intervals that are valid for all x.\n", "title": "Response to Comment 3 clarification"}, "sDuJ7aJO40W": {"type": "rebuttal", "replyto": "QUprAhhIOjs", "comment": "**Response to Comment1**: Yes, at training time, the agent is allowed to go to unsafe states. In particular, when collecting calibration sets (i.e., $Z'$ and $W'$), we collect a rollout that includes the first unsafe state. We believe this is a reasonable strategy -- in practice, RL is usually used to train a policy in simulation that is to be deployed on a real robot. Our goal is to ensure that the policy is at least safe with respect to the model -- for instance, currently, we cannot even guarantee that a trained policy does not run into a wall in the simulator. We will update our paper to clarify that we are not addressing the sim-to-real gap (except to the extent that it can be modeled as noise in the simulator).", "title": "Response on Policy"}, "ZL92VYJxDp_": {"type": "rebuttal", "replyto": "2fohPotrdSc", "comment": "Thanks for the swift comments! The following includes the responses of your additional comments. Please feel free to let us know if further clarification is needed. \n\n---\n\n**Comment 1**: still unclear on this - it seems like the safe planning problem is off-policy planning. Regardless of whether or not you enter an unsafe state, isn't this still a different distribution over trajectories?\n\n**Response 1**: To clarify, we assume that the learned policy is the same at both training time and test time, so it is not exactly an off-policy planning problem. The only thing that changes is that we override the learned policy if we decide it is no longer safe to use. The distribution over states is identical up until (and including) this event. After this event, the robot safely comes to a stop, so it is guaranteed to be safe even though the states visited are now from a different distribution than the training distribution.\n\n---\n\n**Comment 2**: mentioned this also in the general comment, but I'm interested in understanding how well they transfer to the test set. Does it generally take a lot of data? Do they converge quickly? Not looking for a full proof but some discussion would be pertinent.\n\n**Response 2**: Non-exact intervals (i.e., ones that are only asymptotically correct and do not have finite-sample guarantees) converge at a rate of $1/\\sqrt{n}$. While we do not know of analytical bounds on the rate of convergence of CP intervals, we expect that they would converge at the same rate, since asymptotically they should be the same.\n\n---\n\n**Comment 4**: You say that histogram binning satisfies the desired error bound due to the way it is constructed - what does this mean?\n\n**Response 4**: What we mean is that histogram binning satisfies the desired error bound asymptotically. In particular, if infinitely many samples are collected to construct the histogram binning, then the value in each bin is the same as the true confidence -- i.e., $\\hat{p}(x) = c_{\\hat{f}}(x)$; thus histogram binning is well-calibrated in the sense of Definition 1. In this case, the histogram binning can satisfy a desired error. However, this property relies on having an infinite number of samples, which does not hold in practice.\n\n\n\n\n\n", "title": "Response of the additional comments (1/2)"}, "e2CTPwRqtk": {"type": "rebuttal", "replyto": "km1PF3H6C0T", "comment": "We appreciate the reviewer\u2019s value comments. For the \u201cOther feedback\u201d, we will directly update them on the paper.\n\n---\n\n**Comment 1**: I am confused about the application of this method to safe planning. In particular, it seems to me like the proposed intervals only hold their PAC guarantee when the test-time distribution matches the training distribution. However, this will not be the case in the safe planning setting as I understand it, since the observed trajectories are drawn from a different policy than the one which will be implemented in the world\n\n**Response 1**: The reason that our approach works is we assume that as soon as the robot reaches an unsafe state, it permanently switches to the backup policy. Thus, our recoverability predictor never encounters an out-of-distribution sample. If it were to switch back to the learned policy, then as you point out, our PAC guarantee would no longer be satisfied. We will do our best to improve the clarity of this point in our paper.\n\n---\n**Comment 2**: With respect to these test set questions, it would be nice to see a little discussion in the paper of how these guarantees transfer from training set to the test distribution\n\n**Response 2**: Our guarantees transfer to the test distribution if it is identical to the training/validation distributions. We believe that extending our results to handling covariate shift is an important direction for future work. See the \u201cCommon Response\u201d for a more detailed discussion of future directions.\n\n---\n**Comment 3**: I would like to see more explanation of the proofs in the appendix, right now they are a little too compact for me\n\n**Response 3**: We will add more detailed explanations to the proofs. \n\n---\n**Comment 4**: Experimental baselines raise some questions for me. First, I need more explanation on the histogram binning baseline beyond the one sentence given. Second, the authors state this baseline \u201cdoes not satisfy the desired error\u201d \u2013 but I\u2019m not sure why we would expect it to, since that baseline was not tuned to any sort of error level. Finally, I would like to see na\u00efve threshold baselines in the safe planning setting for more cautious values than 0.5 \u2013 since that is more aligned with the goals of safety.\n\n**Response 4**: \n*Histogram binning*: This algorithm calibrates the predictions of $\\hat{f}$ by sorting the calibration examples $(x, y^*)$ into bins $B_i = [b_i, b_{i+1})$ based on their predicted probability $\\hat{p}(x) \\in B_i$. Then, for each bin, it computes the empirical probability $p_i = Pr[\\hat{y}(x) = y^*]$ (i.e., the empirical counterpart of Eq. 2 in our paper). Finally, it returns predicted probability $p_i$ for all future test instances $x$ such that $\\hat{p}(x) \\in B_i$.\n\n*Baseline quality*: Histogram binning asymptotically satisfies the desired error bound due to the way it is constructed and the definition of calibration in Eq. 2. However, the estimation error of the bin probability p_i causes it to violate the desired guarantee. In contrast, our approach accounts for the estimation error, enabling us to satisfy the desired guarantee with high probability.\n\n*The threshold of na\u00efve threshold baselines*: We will add results with more cautious and adaptive threshold (i.e., $1 - \\epsilon$).\n\nWe will clarify all of the aforementioned points in our paper.\n\n\n---\n**Comment 5**: In your PAC definition, we could just always output [0, 1] to satisfy it. Therefore, when framing the goals of your method, you should be a little more clear about exactly what you want from a PAC prediction.\n\n**Response 5**: Thank you for pointing this out. Our goal is to minimize the size of the interval $\\hat{C}$ while satisfying the PAC constraint. We will clarify this point in our problem formulation section.\n\n---\n**Comment 6**: A number of notation errors throughout which are important to fix for clarity and neatness, and a decent amount of lack of precision in language throughout which is important to fix for clarity. See Other Feedback\n\n**Response 6**: We appreciate the detailed feedback, and will enhance clarity based on the feedback. \n\n\n---\n**Comment 7**: Line below Eq 5 confuses me: you say you exit at m if \\hat{y}_m correctly classified an example also correctly classified at \\hat{y}_M. But how can you know this without doing inference to the last layer?\n\n**Response 7**: Sorry for the confusion. Yes, we need to do inference in the last layer to know this fact with certainty. But, if the confidence of $\\hat{y}_m$ is sufficiently high (using our confidence intervals to be conservative), then $\\hat{y}_m = y^*$ with high probability. In this case, $\\hat{y}_M$ either correctly classifies or misclassifies the same example: if the example is misclassified, it does not hurt the relative error, and if it is correctly classified, we have $\\hat{y}_m = y^* = \\hat{y}_M$ with high probability. We will clarify this point in our paper.\n\n\n\n\n\n\n\n", "title": "Response to Comments by Reviewer (1/2)"}, "QMzPqU6RB8F": {"type": "rebuttal", "replyto": "Qk-Wq5AIjpq", "comment": "Dear reviewers, \n\nWe appreciate your valuable feedback. The following includes changes we are making, and discussion on limitations and possible extensions of our approach to handle out-of-distribution test examples.\n\n\n**Changing list**\n- (clarity) We will enhance overall clarity of the paper\n- (experiment) We will add evaluation and comparison results in calibration performance \n- (experiment) We will add additional comparison results in fast DNN inference \n- (experiment) We will add additional comparison results in safe planning\n- (proof) We will add an optimality proof of the greedy algorithm in fast DNN inference, along with the corresponding discussion\n- (discussion) We will add additional discussions mentioned in the response---We are still working on updating the paper to address the reviewer comments, and will update our paper as soon as possible.\n\n\n\n**Discussion on out-of-distribution cases**\n\n*Related comments:*\n- *Comment 1*: One of the crucial assumptions is that the data during test time will be from in-distribution. While I understand it is hard and maybe impossible to provide any guarantees for out-of-distribution data, it is important to realize that one of the most common ways in which DNNs can fail when deployed in safety critical systems is when faced with predicting from an out-of-distribution data. So it is unclear how practically applicable the proposed method is.\n- *Comment 2*: My last concern is not so much with what the authors have presented, just in the fact that the limitation is not clearly stated. When using statistical guarantees such as those given under the PAC framework, the iid assumption is almost always necessary. Yet, it can be limiting in both the case of planning and image classification, especially when we are considering safety-critical applications. In such applications, one often considers worst-case scenarios and in these cases the iid assumption usually does not hold. Thus, the PAC guarantees in this paper would seemingly be invalidated. For example, adversarial examples may be out of the support of the data distribution, but are still valid inputs. In the presence of adversarial examples the uncertainty guarantees presented here are rendered void. Similarly, there is often concept drift in RL and control problems. Ultimately, I don\u2019t think this is only a minor mark against the work, and one that can be overlooked given that such cases are at least clearly stated as limitations of the approach.\n- *Comment 3*: With respect to these test set questions, it would be nice to see a little discussion in the paper of how these guarantees transfer from training set to the test distribution\n\n*Common response*:\n\nWe also believe that providing theoretical guarantees for out-of-distribution data is an important direction; however, we believe that our work is an important stepping stone towards this goal. In particular, to the best of our knowledge, we do not know of any existing work that provides theoretical guarantees on calibrated probabilities even for the in-distribution case.\n\nOne possible direction is to use our approach in conjunction with covariate shift detectors (e.g., [1]). Alternatively, it may be possible to directly incorporate ideas from recent work on calibrated prediction with covariate shift [2] or uncertainty set prediction with covariate shift [3, 4]. In particular, we can use importance weighting q(x)/p(x), where p(x) is the training distribution and q(x) is the test distribution, to reweight our training examples, enabling us to transfer our guarantees from the training set to the test distribution. The key challenge is when these weights are unknown. In this case, we can estimate them given a set of unlabeled examples from the test distribution [2], but we then need to account for the error in our estimates. We will add discussion on this future direction to our paper.\n\n[1] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6lkopf, A. Smola. A kernel two-sample test. JMLR 2012.\n\n[2] S. Park, O. Bastani, J. Weimer, and I. Lee. Calibrated Predictions with Covariate Shift via Unsupervised Domain Adaptation. AISTATS 2020\n\n[3] M. Cauchois, S. Gupta and A. Ali and J. C. Duchi. Robust Validation: Confident Predictions Even When Distributions Shift. 2020 \n\n[4] R. F. Barber, E. J. Candes, A. Ramdas, R. J. Tibshirani. Conformal prediction under covariate shift. NeurIPS 2019.\n", "title": "Common Response to Comments by all reviewers"}, "yIrR4afOtuJ": {"type": "rebuttal", "replyto": "km1PF3H6C0T", "comment": "\n\n**Comment 8**: Is there a reason why the greedy approach to Fast DNN inference you take is desirable? Be more clear about why you chose this and if it is optimal somehow or not\n\n**Response 8**: Based on your suggestion, we have proved that the current greedy approach is in fact optimal. Intuitively, we are always better off (in terms of inference time) by classifying more inputs using the faster model. We will add a discussion to our paper and a proof of optimality to the appendix.\n\n\n---\n**Comment 9**: Defn 1: should this be a nearness constraint rather than equality? We have multiple examples x in a bin each with their own p-hat, and so not all of them can be equal to c(x)\n\n**Response 9**: We note that $c_{\\hat{f}}$ in Definition 1 is *different* from $c_{\\hat{f}}$ in Eq. 1. Instead, we are re-defining $c_{\\hat{f}}$ to be the average probability for $x$\u2019s in bin $B_k$, where $k = \\kappa_{\\hat{f}(x)}$. Thus, by definition, $c_{\\hat{f}}(x)$ is the same for all $x$ in this bin. We apologize for the confusion, and will clarify this point in our paper.", "title": "Response to Comments by Reviewer (2/2)"}, "Jtnccc79Cxr": {"type": "rebuttal", "replyto": "_W0xIiVPqa", "comment": "Thank you for the valuable comments. We have done our best to address them below; please let us know further clarification is needed. \n\n---\n\n**Comment 1**: One of the crucial assumptions is that the data during test time will be from in-distribution. While I understand it is hard and maybe impossible to provide any guarantees for out-of-distribution data, it is important to realize that one of the most common ways in which DNNs can fail when deployed in safety critical systems is when faced with predicting from an out-of-distribution data. So it is unclear how practically applicable the proposed method is.\n\n**Response 1**:We also believe that providing theoretical guarantees for out-of-distribution data is an important direction; however, we believe that our work is an important stepping stone towards this goal. See the \u201cCommon Response\u201d for a more detailed discussion of future directions.\n\n---\n\n**Comment 2**: If somehow, one is always guaranteed to work within in-distribution data, then the authors should compare other methods that improve calibration (but don't have guarantees) with the proposed method, both in terms of calibration and computational efficiency in the fast-model/slow-model approach.\n\n**Response 2**: We note that we have already compared with histogram binning in our experiments. For the computational efficiency experiments (Figure 2), our proposed approach helps to satisfy the downstream guarantee whereas histogram binning does not. We will add comparisons to temperature scaling as well; we are also happy to add other approaches as suggested by the reviewer.\n\n---\n\n**Comment 3**: Also I do not see any calibration performance in the experimental results. While I understand the proposed method is using histogram binning and is not a new calibration method per se, these results should be included. That is,show how accuracy correlates with the softmax predictions (on the test set) using proposed technique.\n\n**Response 3**: Our approach does not provide calibrated predictions that are directly comparable to histogram binning, since it is providing lower and upper bounds on the bin probabilities. Nevertheless, we will add comparisons in terms of reliability diagrams for histogram binning where we include our confidence intervals around the predicted probability for that bin, as well as comparisons to uncalibrated probabilities and temperature scaling. This comparison allows us to check whether the proposed approach is good in terms of ECE---i.e., the induced intervals pass the diagonal of the reliability diagram most of the time. We note that our confidence intervals need to be remapped to make sense in the reliability diagram, so we refer to the intervals shown as \u201cinduced intervals\u201d. We will add a detailed description in the paper.\n\n---\n\n**Comment 4**: Since the proposed method provides stronger UQ than other methods, I would also like to see accuracy-vs-coverage curves for benchmark datasets, and compare this to such curves for the baseline (where one thresholds on the winning softmax scores).\n\n**Response 4**: Could you clarify the meaning of \u201caccuracy-vs-coverage curves for benchmark datasets\u201d? It would be also great if we could have an example of a paper that has the related experiments.\n", "title": "Response to Comments by Reviewer"}, "km1PF3H6C0T": {"type": "review", "replyto": "Qk-Wq5AIjpq", "review": "Summary: This paper proposes a method for obtaining probably-approximately correct (PAC) predictions given a pre-trained classifier. The PAC intervals are connected to calibration, and take the form of confidence intervals given the bin a prediction falls in. They demonstrate and explore two use cases: applying this technique to get faster inference in deep neural networks, and using the PAC predictor to do safe planning. Experiments in both of these cases show improvements in speed-accuracy or safety-accuracy tradeoffs, as compared to baselines.\n\nI\u2019m recommending acceptance since the idea seems useful and well-argued both conceptually and experimentally. However, the paper needs some work in terms of clarification of the key ideas \u2013 with this clarification I can raise my score.\n\nStrong points: \n-\tThe proposed method provides a provable guarantee on the reliability of a pre-trained methods prediction, which is a very nice property to have in the reliability/safety problem.\n-\t This approach is a simple but good idea, seems grounded in a good motivation and the explored use cases are informative and interesting. \n-\tExperimentally, the method shows improvements over a na\u00efve baselines, and demonstrate that it can obey a given error or safety threshold in practice, an important property\n\nWeak points + Clarifications:\n-\tI am confused about the application of this method to safe planning. In particular, it seems to me like the proposed intervals only hold their PAC guarantee when the test-time distribution matches the training distribution. However, this will not be the case in the safe planning setting as I understand it, since the observed trajectories are drawn from a different policy than the one which will be implemented in the world\n-\tWith respect to these test set questions, it would be nice to see a little discussion in the paper of how these guarantees transfer from training set to the test distribution\n-\tI would like to see more explanation of the proofs in the appendix, right now they are a little too compact for me\n-\tExperimental baselines raise some questions for me. First, I need more explanation on the histogram binning baseline beyond the one sentence given. Second, the authors state this baseline \u201cdoes not satisfy the desired error\u201d \u2013 but I\u2019m not sure why we would expect it to, since that baseline was not tuned to any sort of error level. Finally, I would like to see na\u00efve threshold baselines in the safe planning setting for more cautious values than 0.5 \u2013 since that is more aligned with the goals of safety.\n-\tIn your PAC definition, we could just always output [0, 1] to satisfy it. Therefore, when framing the goals of your method, you should be a little more clear about exactly what you want from a PAC prediction.\n-\tA number of notation errors throughout which are important to fix for clarity and neatness, and a decent amount of lack of precision in language throughout which is important to fix for clarity. See Other Feedback\n-\tLine below Eq 5 confuses me: you say you exit at m if \\hat{y}_m correctly classified an example also correctly classified at \\hat{y}_M. But how can you know this without doing inference to the last layer?\n-\tIs there a reason why the greedy approach to Fast DNN inference you take is desirable? Be more clear about why you chose this and if it is optimal somehow or not\n\nOther feedback:\n-\t- Some precision in language could be improved throughout \u2013 for instance \u201cby using the accurate model only if the confidence of the accurate one is underconfident\u201d on p2 doesn\u2019t really make sense\n-\t-p2: \u201ca naively trained DNN is not reliable\u201d what does reliable mean here?\n-\t-above eq 2, should this be \\kappa_x ?\n-\t-Defn 1: should this be a nearness constraint rather than equality? We have multiple examples x in a bin each with their own p-hat, and so not all of them can be equal to c(x)\n-\t-Is there a reason why these intervals should be defined as continuous rather than possibly a disjoint set?\n-\t-should make it clearer from the start that this is defined for post-hoc classifiers, you\u2019re not learning these intervals directly\n-\tEq 4 \u2013 the bold theta-hat here is different from the one defined in the line above\n-\tNeed more explanation on \u201cThe following expression is equivalent due to the relationship between the Binomial and Beta distributions\u201d p. 4\n-\tIn the definition of C-hat, you\u2019re overloading x on the right side of the given sign. Can you use x\u2019 or something?\n-\tIn the \u201cimportant case\u201d below Thm 1, can you clarify \u2013 this is the mean right?\n-\tProblem formulation in Fast DNN Inference: should the \\hat{y}_i be \\hat{y}_m? I feel like i is not scoped here\n-\tYou define d_m below the problem formulation but it isn\u2019t in the formulation itself.\n-\tTop of p5, you train the network in the \u201cstandard way\u201d \u2013 this is not clear. Do the gradients at the lower levels flow back through the earlier layers? Or are they stopped and the only gradients are from the final prediction task at the last layer? Either could make sense to me\n-\tShould define more carefully what the \u201ccomposed classifier\u201d refers to\n-\tClarify how rollouts work \u2013 will you observe unsafe states? Sometimes in safe planning you assume you don\u2019t actually observe the unsafe states in training but it looks like you need that\n-\tBottom of p6: what is Z\u2019? Not sure what Z is \u2013 is it ordered pairs? Why is the second element always 1?\n-\tP13: why is this an upper bound? It\u2019s hard to parse but it looks equal to the expression in 10 at first glance \u2013 it\u2019s the appendix so please explain further. You can also note that the E_t are disjoint by definition\n-\tAlso please add comments about the appendix figures? I have no idea what they are\n\u2003\n", "title": "Good idea, good motivation/use cases, needs some clarification + cleaning up", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}