{"paper": {"title": "Lifelong Learning by Adjusting Priors", "authors": ["Ron Amit", "Ron Meir"], "authorids": ["ronamit@campus.technion.ac.il", "rmeir@ee.technion.ac.il"], "summary": "We develop a lifelong learning approach to transfer learning based on PAC-Bayes theory, whereby priors are adjusted as new tasks are encountered thereby facilitating the learning of novel tasks.", "abstract": "In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks. Under the assumption that future tasks are related to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task. We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework. Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks. We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples. ", "keywords": ["Lifelong learning", "Transfer learning", "PAC-Bayes theory"]}, "meta": {"decision": "Reject", "comment": "The author's revisions addressed clarity issues and some experimental issues (e.g., including MAML results in the comparison). The work takes an original path to an important problem (transfer learning, essentially). There is a question of significance, and this is due to the fact that the empirical comparisons are still very limited. The task is an artificial one derived from MNIST. I would call this \"toy\" as well. On this toy task, the approach isn't that much different from MAML, which is not in of itself a problem, but it would be interested to have a less superficial discussion of the differences.\n\nThe authors mention that they didn't have time for a larger empirical study. I think one is necessary in this case because the work is purposing a new learning algorithm/framework, and the question of its potential impact/significance is an empirical one."}, "review": {"SJAvHP8BM": {"type": "rebuttal", "replyto": "rJX9THLHf", "comment": "We thank the area chair for the helpful comment.\nIndeed there was a problem with the constant, please see our response to AnonReviewer1.\n \nP.S.\nSince the submission of the revised paper we added more experiments that demonstrate the meta-learning  performance in varied task-environments and with different number of training-tasks.\nWe would be happy to include those in a revised version if possible.", "title": "Response to Area Chair"}, "SkBYVPISz": {"type": "rebuttal", "replyto": "Sk3Jqd7rz", "comment": "Indeed there was a mistake in the constant multiplying the KL divergence of the hyper-distributions.\nUsing the corrected code the results of both methods (varitional and PAC-Bayes) are quite similar  (0.9% for LAP-M, 0.75% for LAP-S and 0.85% for LAP-VB).\n", "title": "We thank the reviewer for the helpful  comment."}, "HyJlQlQWf": {"type": "review", "replyto": "rJUBryZ0W", "review": "The paper considers multi-task setting of machine learning. The first contribution of the paper is a novel PAC-Bayesian risk bound. This risk bound serves as an objective function for multi-task machine learning. A second contribution is an algorithm, called LAP, for minimizing a simplified version of this objective function. LAP algorithm uses several training tasks to learn a prior distribution P over hypothesis space. This prior distribution P is then used to find a posterior distribution Q that minimizes the same objective function over the test task. The third contribution is an empirical evaluation of LAP over toy dataset of two clusters and over MNIST.\n\nWhile the paper has the title of \"life-long learning\", the authors admit that all experiments are in multi-task setting, where\nthe training is done over all tasks simultaneously. The novel risk bound and LAP algorithm can definitely be applied to life-long setting, where training tasks are available sequentially. But since there is no empirical evaluation in this setting, I suggest to adjust the title of the paper. \n \nThe novel risk bound of the paper is an extension of the bound from [Pentina & Lampert, ICML 2014]. The extension seems to be quite significant. Unlike the bound of [Pentina & Lampert, ICML 2014], the new bound allows to re-use many different PAC-Bayesian complexity terms that were published previously. \n\nI liked risk bound and optimization sections of the paper. But I was less convinced by the empirical experiments. Since \nthe paper improves the risk bound of [Pentina & Lampert, ICML 2014], I expected to see an empirical comparison of LAP and optimization  algorithm from the latter paper. To make such comparison fair, both optimization algorithms should use the same base algorithm, e.g. ridge regression, as in [Pentina & Lampert, ICML 2014]. Also I suggest to use the datasets from the latter paper.  \n\nThe experiment with multi-task learning over MNIST dataset looks interesting, but it is still a toy experiment. This experiment  will be more convincing with more sophisticated datasets (CIFAR-10, ImageNet) and architectures (e.g. Inception-V4, ResNet). \n\nMinor remarks:\nSection 6, line 4: \"Combing\" -> \"Combining\"\nPage 14, first equation: There should be \"=\" before the second expectation.", "title": "Interesting risk bound but empirical evaluation is not convincing", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1IHf4oVf": {"type": "rebuttal", "replyto": "B1l2Xr8Ef", "comment": "We appreciate your thorough review which greatly contributes to our work.\nRegarding the name 'lifelong', we followed the definition of Pentina & Lampert.\nHowever, we agree that the name might be misleading and we will change it to 'meta-learning' in future submissions.", "title": "Authors response"}, "H1qBI-5gG": {"type": "review", "replyto": "rJUBryZ0W", "review": "I personally warmly welcome any theoretically grounded methods to perform deep learning. I read the paper with interest, but I have two concerns about the main theoretical result (Theorem 1, lifelong learning PAC-Bayes bound).\n* Firstly, the bound is valid for a [0,1]-valued loss, which does not comply with the losses used in the experiments (Euclidean distance and cross-entropy). This is not a big issue, as I accept that the authors are mainly interested in the learning strategy promoted by the bound. However, this should clearly appear in the theorem statement.\n* Secondly, and more importantly, I doubt that the uaw of the meta-posterior as a distribution over priors for each task is valid. In Proposition 1 (the classical single-task PAC-Bayes bound), the bound is valid with probability 1-delta for one specific choice of prior P, and this choice must be independent of the learning sample S. However, it appears that the bound should be valid uniformly for all P in order to be used in Theorem 1 proof (see Equation 18). From a learning point of view, it seems counterintuitive that the prior used in the KL term to learn from a task relies on the training samples (i.e., the same training samples are used to learn the meta-posterior over priors, and the task specific posterior).  \n\nA note about the experiments:\nI am slightly disappointed that the authors compared their algorithm solely with methods learning from fewer tasks. I would like to see the results obtained by another method using five tasks. A simple idea would be to learn a network independently for each of the five tasks, and consider as a meta-prior an isotropic Gaussian distribution centered on the mean of the five learned weight vectors.\n\nTypos and minor comments:\n- Equation 1: \\ell is never explicitly defined.\n- Equation 4: Please explicitly define m in this context (size of the learning sample drawn from tau).\n- Page 4, before Equation 5: A dot is missing between Q and \"This\".\n- Page 7, line 3: Missing parentheses around equation number 12.\n- Section 5.1.1, line 5: \"The hypothesis class is a the set of...\"\n- Equation 17: Q_1, ... Q_n are irrelevant.\n\n=== UPDATE ===\nI increased my score after author's rebuttal. See my other post.", "title": "Interesting algorithm based on a theoretical study, but the main theorem might contain a flaw", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyY2Sogef": {"type": "review", "replyto": "rJUBryZ0W", "review": "The author extends existing PAC-Bayes bounds to multi-task learning, to allow the prior to be adapted across different tasks. Inspired by the variational bayes literature, a probabilistic neural network is used to minimize the bound. Results are evaluated on a toy dataset and a synthetically modified version of MNIST. \n\nWhile this paper is well written and addresses an important topic, there are a few points to be discussed:\n\n* Experimental results are really week. The toy experiment only compares the mean of two gaussians. Also, on the synthetic MNIST experiments, no comparison is done with any external algorithms. Neural Statistician, Model-Agnostic Meta-Learning and matching networks all provide decent results on such setup. While it is tolerated to have minimal experiments in a theoretical papers, the theory only extends Pentina & Lampert (2014). Also, similar algorithms can be obtain through variational-bayes evidence lower bound. \n\n* The bound appears to be sub-optimal. A bound where the KL term vanishes by 1/n would probably be tighter. I went in appendix to try to see how the proof could be adapted but it\u2019s definitively not as well written as the rest of the paper. I\u2019m not against putting proofs in appendix but only when it helps clarity. In this case it did not.\n\n* The paper is really about multi-task learning. Lifelong learning implies some continual learning and addressing the catastrophic forgetting issues. I would recommend against overuse of the lifelong learning term.\n\nMinors:\n* Define KLD\n* Section 5.1 : \u201cto toy\u201d\n* Section 5.1.1: \u201cthe the\u201d\n", "title": "Well written paper, with very week experiments.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1LX97Czz": {"type": "rebuttal", "replyto": "HyJlQlQWf", "comment": "* We added a discussion in the introduction about the distinction from multi-task learning  (section 1 - first paragraph). \nThere is a clear difference from multi-task, since in lifelong learning  the goal is to acquire knowledge (prior) that when transferred to new tasks facilitates good learning.\nWhile we call this transfer  setup \u201clifelong learning \u201d (as in Pentina and Lampert\u2019s work), it can also be called \u201clearning-to-learn\u201d. But \u2018multi-task learning\u2019 is inappropriate because of the different goals and outcome of learning (a prior for learning tasks vs. solutions to given tasks).\n\n* We added an experimental  comparison to a learning objective which is based on Pentina and Lampert\u2019s main theorem.  As can be seen in section 6, this bound  leads to far worse empirical results. We believe that  using our theorem leads to better performance since it is a tighter bound.\n\n* Due to  technical difficulties and lack of time we cannot provide a high quality multiple data-set evaluation at this time.\nHowever, we did add a comparison to competitive recent  approach - Model-Agnostic Meta-Learning (MAML, Finn, Abbeel, and Levine. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv preprint arXiv:1703.03400 (2017).)   (see section 6). \n", "title": "We thank the reviewer for the helpful comments, and address the  specific points below."}, "H1zptXAzM": {"type": "rebuttal", "replyto": "H1qBI-5gG", "comment": "* We added a comment about the bounded loss issue   (see end of section 2.2).  Indeed,  this is not a big issue since - theoretically, we can claim to bound a truncated version of the loss, and empirically the losses are almost always smaller than one.\n\n* Thank you for pointing out the delicate issue about our main Theorem. We have rewritten the proofs using a different technique, which clarifies the points made by the reviewer and, in fact, leads to improved bounds (see section 3.2 for overview and 8.1 for full proof). \nIn the new formulation, each task bound holds for all hyper-posteriors and all posteriors, so it is valid to optimize both using the same samples.\nNote that our new theorem deviates significantly in both proof technique and behavior from that in Pentina and Lampert\u2019s work.  \n\n* In section 6, we added the experiment you suggested and several other methods  which use all the training tasks, including:   \n1. Using the bound from  Pentina and Lampert\u2019s work as a learning objective, \n2. Using an objective derived from variational methods and hierarchical generative models and 3. A recent  method - Model-Agnostic Meta-Learning (MAML, Finn, Abbeel, and Levine. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv preprint arXiv:1703.03400 (2017).) .  \n\n", "title": "We thank the reviewer for the helpful comments, and address the  specific points below."}, "r1-wtXRzz": {"type": "rebuttal", "replyto": "HyY2Sogef", "comment": "* The toy example (section 5) was meant only for visualization of the setup. \nIn the revised version we separated it from the experimental results section.\n\nIn the experimental results part (section 6) we added a comparison to the recently introduced Model-Agnostic Meta-Learning (MAML, Finn, Abbeel, and Levine. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv preprint arXiv:1703.03400 (2017).) .\n\nWe also addressed the comparison to variational methods  which maximize the evidence lower bound (section 6).  Actually, such methods can be seen as minimizing a bound on the generalization error, but with a complexity terms of the  KLD  between posterior and prior, which is less tight than the bounds in the paper.  We compared the results of  such an objective ( which is referred to as LAP-KLD in section 6) and showed that it performed much worse.\n\n* We rewrote the proof - hopefully it is clearer.  (see section 3.2 for overview and 8.1 for the full proof).\nIn section 8.2  we also added a bound in which the the KL term can vanish at a rate of 1/m (number of samples) if the empirical error is low. For the number of tasks, n, we preferred to keep the 1/n for simplicity and because this term is less important for the LAP algorithm.\n\n* We added a discussion in the introduction (section 1 - first paragraph) about the distinction from continual learning and from multi-task learning. We hope this clarifies our choice of paper title. There is a clear difference from multi-task learning, since the goal in our work is to acquire knowledge (prior) that, when transferred to new tasks, facilitates learning with low generalization error, rather than using multiple tasks collaboratively to aid each task in the given set of tasks.\n\n", "title": "We thank the reviewer for the helpful comments, and address the  specific points below."}}}