{"paper": {"title": "INVASE: Instance-wise Variable Selection using Neural Networks", "authors": ["Jinsung Yoon", "James Jordon", "Mihaela van der Schaar"], "authorids": ["jsyoon0823@gmail.com", "james.jordon@wolfson.ox.ac.uk", "mihaela.vanderschaar@eng.ox.ac.uk"], "summary": "", "abstract": "The advent of big data brings with it data with more and more dimensions and thus a growing need to be able to efficiently select which features to use for a variety of problems. While global feature selection has been a well-studied problem for quite some time, only recently has the paradigm of instance-wise feature selection been developed. In this paper, we propose a new instance-wise feature selection method, which we term INVASE. INVASE consists of 3 neural networks, a selector network, a predictor network and a baseline network which are used to train the selector network using the actor-critic methodology. Using this methodology, INVASE is capable of flexibly discovering feature subsets of a different size for each instance, which is a key limitation of existing state-of-the-art methods. We demonstrate through a mixture of synthetic and real data experiments that INVASE significantly outperforms state-of-the-art benchmarks.", "keywords": ["Instance-wise feature selection", "interpretability", "actor-critic methodology"]}, "meta": {"decision": "Accept (Poster)", "comment": "This manuscript proposes a new algorithm for instance-wise feature selection. To this end, the selection is achieved by combining three neural networks trained via an actor-critic methodology. The manuscript highlight that beyond prior work, this strategy enables the selection of a different number of features for each example. Encouraging results are provided on simulated data in comparison to related work, and on real data.\n\nThe reviewers and AC note issues with the evaluation of the proposed method. In particular, the evaluation of computer vision and natural language processing datasets may have further highlighted the performance of the proposed method. Further, while technically innovative, the approach is closely related to prior work (L2X) -- limiting the novelty. \n\nThe paper presents a promising new algorithm for training generative adversarial networks. The mathematical foundation for the method is novel and thoroughly motivated, the theoretical results are non-trivial and correct, and the experimental evaluation shows a substantial improvement over the state of the art."}, "review": {"BkehMR6shm": {"type": "review", "replyto": "BJg_roAcK7", "review": "This paper proposes an instance-wise feature selection method, which chooses relevant features for each individual sample. The basic idea is to minimize the KL divergence between the distribution p(Y|X) and p(Y|X^{(s)}). The authors consider the classification problem and construct three frameworks: 1) a selector network to calculate the selection probability of each feature; 2) a baseline network for classification on all features; 3) a predictor network for classification on selected features. The goal is to minimize the difference between the baseline loss and predictor loss.\n\nThe motivation of the paper is clear and the presentation is easy to follow. However, I have some questions on the model and experiments:\n\n1. How is Eq. (5) formulated? As the selector network does not impact the baseline network, an intuition regarding Eq. (5) is to maximize the predictor loss, which seems not reasonable. It seems more appropriate to use an absolute value of the difference in Eq. (5). Some explanation for the formulation of Eq. (5) would be helpful.\n\n2. The model introduces an extra hyper-parameter, $\\lambda$, to adjust the sparsity of selected features. I was curious how sensitive is the performance w.r.t. this hyper-parameter. How is $\\lambda$ determined in the experiments?\n\n3. After the selector network is constructed, how are the features selected on testing data? Is the selection conducted by sampling from the Bernoulli distribution as in training or by directly cutting off the features with lower probabilities?\n", "title": "Instance-wise feature selection", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1lgbw8K3X": {"type": "review", "replyto": "BJg_roAcK7", "review": "This paper proposes a new instance-wise feature selection method, INVASE. It is closely related to the prior work L2X (Learning to Explain). There are three differences compared to L2X. The most important difference is about how to backpropagate through subset sampling to select features.  L2X use the Gumbel-softmax trick and this paper uses actor-critic models.\n\nThe paper is written well. It is easy to follow the paper. The contribution of this paper is that it provides a new way,  compared to L2X, to backpropagate through subset sampling in order to select features. The authors compare INVASE with L2X and several other approaches on synthetic data and show outperforming results. In the real-world experiments, the authors do not compare INVASE with other approaches. \n\nRegarding experiments, instance-wise feature selection is often applied on computer vision or natural language process applications, where global feature selection is not enough. This paper lacks experiments on CV or NLP applications. For the MAGGIC dataset, I expect to see subgroup patterns. The patterns that authors show in Figure 2 are very different for all randomly selected 20 patients. The authors do not explain why it is preferred to see very different feature patterns for all patients instead of subgroup patterns.\n\nI have questions about other two differences from L2X, pointed by the authors. First, the selector function outputs a probability for selecting each feature \\hat{S}^\\theta(x). In the paper of L2X, it also produces a weight vector w_\\theta(x) as described in section 3.4. I think the \\hat{S}^\\theta(x) has similar meaning as w_\\theta(x) in L2X. In the synthetic data experiment, the authors fix the number of selected features for L2X so that it forces to overselect or underselect features in the example of Syn4. Did the author try to relax this constraint for L2X and use w_\\theta(x) in L2X to select features as using \\hat{S}^\\theta(x) in INVASE? \n\nSecond, I agree with the authors that L2X is inspired by maximizing mutual information between Y and X_S and INVASE is inspired by minimizing KL divergence between Y|X and Y|X_S. Both intuitions lead to similar objective functions that INVASE has an extra term \\log p(y|x) and \\lambda ||S(x)||. INVASE is able to add a l_0 penalty on S(x) since it uses the actor-critic models. For the \\log p(y|x) term, as the author mentioned, it helps to reduce the variance in actor-critic models. This \\log p(y|x) term is a constant in the optimization of S(x). In Algorithm 1, 12, the updates of \\gamma does not depend on other parameters related to the predictor network and selector network. Could the authors first train a baseline network and use it as a fixed function in Algorithm 1? I don't understand the meaning of updates for \\gamma iteratively with other parameters since it does not depend on the learning of other parameters. Does this constant term \\log p(y|x) have other benefits besides reducing variance in actor-critic models?\n\nI have another minor question about scaling. How does the scaling of X affect the feature importance learned by INVASE?\n\nNote: I have another concern about the experiments. Previous instance-wise variable selection methods are often tested on CV or NLP applications, could the authors present those experiments as previous works?", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1xeFW7oT7": {"type": "rebuttal", "replyto": "rkxFiB2Tom", "comment": "Thank you for the insightful comments.\n\nA1: It is not true that fast training of the predictor network can lead to a suboptimal selector network. Even when the predictor network is fully trained after each selector network update, the selector network can converge optimally. However, because the input distribution of the predictor network changes with each update of the selector network, the predictor network will have to update after each selector update. It is therefore not possible for the predictor network to converge until after the selector network has converged. Therefore, there are no stability issues caused by using the same learning rates for each network.\n\nA2: INVASE+.py is the code corresponding to the implementation found in this paper. INVASE.py corresponds to the same implementation but without the baseline (i.e. just the selector and predictor networks). In practice we found both to perform similarly, but the derivation of INVASE+ is a little more natural, and as such we used it for the paper.\n\nWe have since changed the names in the repository to INVASE and INVASE- (so that now INVASE is indeed the implemented method and INVASE- is the method without the baseline). We hope this alleviates the confusion.", "title": "RE: Simple idea, but good results"}, "SylXL-XspQ": {"type": "rebuttal", "replyto": "H1lgbw8K3X", "comment": "Thank you for the insightful comments.\n\nA1: We performed extensive experiments in the synthetic setting on all methods (we both reproduced and extended the settings from L2X). In addition to this, results for semi-synthetic data (where the underlying features are from real data but the label is generated synthetically) can be found in the Appendix on page 16. It is necessary to perform experiments on synthetic data if we wish to be able to compare the TPR and FDR of the different methods since we require knowledge of the ground truth relevant features.\n\nFor the real-world results, our focus was on qualitative results (believing we had already demonstrated the methods efficacy in the synthetic - and in the appendix the semi-synthetic - settings). We will move the semi-synthetic results to the main body of the paper to make clear that we have demonstrated the performance in this setting.\n\nFor the real-data experiment in which we report prediction performance, we have extended our results to include the other approaches. We use the same predictive model as the INVASE predictor network (to allow a fair comparison) but use only the selected features of each approach. As can be seen in the below table (for the PLCO dataset), INVASE does significantly outperform the other approaches. Detailed results will be added to the revised manuscript.\n\n----------------------------------------------------------------------------------------------------\n         Labels           |                  5-year                 |                  10-year                |\n         Metrics         |      AUROC     |    AUPRC    |      AUROC     |     AUPRC    |\n----------------------------------------------------------------------------------------------------\n        INVASE           |     0.637         |     0.329      |       0.673        |       0.506    |\n           L2X               |     0.558         |     0.170      |       0.583        |       0.365    |\n          LIME             |     0.597         |     0.183      |       0.601        |       0.374    |\n        Shapley          |     0.614         |     0.194      |       0.615        |       0.381    |\n        Knockoff        |     0.619         |     0.230      |       0.658        |       0.475    |\n           Tree             |     0.632         |     0.269      |       0.655        |       0.469    |\n          SCFS             |     0.632         |     0.231      |       0.632        |       0.444    |\n          LASSO          |     0.623         |     0.218      |       0.656        |       0.467    |\n----------------------------------------------------------------------------------------------------\n\nA2: Our method can definitely be applied to CV or NLP, though in the paper we focus on what we believe to be an equally important application where global feature selection is not enough, i.e. medicine.\nWe will provide qualitative results in the Appendix of the revised manuscript using the Kaggle Dog vs Cat dataset (https://www.kaggle.com/c/dogs-vs-cats).\n\nA3: The results shown in figure 2 for the MAGGIC dataset are entirely qualitative. We are not suggesting that the patterns shown are preferred (or expected) but rather showing that when we use INVASE to discover features for MAGGIC, we find that the patterns are different (though, if you look at, for example, patients 9, 10 and 11 we see a similar pattern for all 3). To us, this simply reinforces the fact that instance-wise feature selection is necessary - if MAGGIC did indeed only contain subgroup patterns then we would expect INVASE to pick these out (as it does in the synthetic and semi-synthetic experiments where, for example in Syn4, Syn5 and Syn6, there are two distinct subgroups).", "title": "RE1: Review"}, "r1eVBWms6Q": {"type": "rebuttal", "replyto": "H1lgbw8K3X", "comment": "\nA4: The main problem in doing this for L2X is in the training stage (not the testing stage). As can be seen in the equations to compute V values (page 4 end of the left column of L2X paper), they must provide some k to train with which is, in general, unknown in real-world datasets (because we don\u2019t know how many features are relevant in the real-world datasets). The weights w(X) are optimized according to a specific feature selection strategy during training, using them in a different strategy during testing would not make sense, as they are no longer optimized for this strategy. While intuitively possible, consider that, due to the way they\u2019ve been trained, the weights w(X) are expected to \u201cspit out\u201d k features. Because of this, it might be that the weights for the unselected k features are essentially random (but lower than the selected k features). We have no reason to believe that the weights beyond the selected k features would be meaningful (since during training the method only ever selected precisely k features).\n\nWe have, however, conducted an experiment in the Syn4 and Syn5 settings with 100 featurs in which we directly use w(X) and threshold it to select features. As can be seen below, the results are significantly worse than for INVASE and the large increase in FDR is indeed consistent with the fact that the weights beyond the top k are not well-disciplined. We will clarify this in the revised manuscript. Note that the published code of the L2X paper is also forced to select k features in both the training and testing stages.\n----------------------------------------------------------------------------------------\n         Datasets             |               Syn4            |             Syn5              |\n         Thresholds         |      TPR     |    FDR    |      TPR    |     FDR     |\n----------------------------------------------------------------------------------------\n      L2X      |      0.1      |      87.4     |     93.5   |     79.5     |     95.3    |\n      L2X      |      0.3      |      69.9     |     83.8   |     77.2     |     77.1    |\n      L2X      |      0.5      |      69.8     |     64.1   |     66.4     |     84.6    |\n      L2X      |      0.7      |      59.1     |     61.2   |     54.4     |     65.7    |\n      L2X      |      0.9      |      52.7     |     44.8   |     51.2     |     50.5    |\n             INVASE           |       66.3    |      40.5   |     73.2     |     23.7    |\n----------------------------------------------------------------------------------------\n\nA5: The baseline network does not have to be trained iteratively with the other networks, however in actor-critic models it typically is. This is because the baseline is used in some sense to \u201cnormalize\u201d the predictor network. For this reason, it is therefore good to have the baseline and predictor at a similar \u201clevel of convergence\u201d. However, the performance differences are marginal between the two methods, and so we found that it was not important which training method we used.\n\nA6: The scaling of X is not important. At no point do we multiply the feature vector (X) by the \u201cimportance weights\u201d. The weights are used to obtain a binary mask vector which is then multiplied (element-wise) with the feature vector. As such, the unselected features end up being 0 and the selected features retain their original value.", "title": "RE2: Review"}, "Hke-ey7oTQ": {"type": "rebuttal", "replyto": "BkehMR6shm", "comment": "Thank you for the insightful comments.\n\nA1: Equation (5) is the difference between the cross-entropies of the predictor and baseline networks. The first term (-sum_y log f_i^\\phi (x^(s), s)) is the cross-entropy of the predictor network and the second term (-sum y log f_i^\\gamma (x)) is the cross-entropy of the baseline network. The loss in equation (5) is defined as the \u201cfirst term \u2013 second term\u201d. The selector network is trained to minimize this, not maximize it. Note that the baseline network is introduced to reduce the variance of this quantity, and not as a term that the selector network can change (this is a standard technique used in the actor-critic literature).\n\nAlso note that if the baseline network term (the second term) in equation (5) is removed, then we simply end up with the predictor loss defined in the \u201cPredictor Network\u201d section (l_1).\n\nIf instead we were to use absolute value, then when the baseline network loss is larger than the predictor network loss, the method would actually be trying to maximise the predictor network loss (which we do not want).\n\nIt is important to note that we are not trying to minimize the difference between the predictor and baseline losses - we are using the baseline to reduce the variance of the overall loss and we are simply trying to minimize the predictor loss.\n\nA2: As can be seen in page 13 (subsection \u201cDetails of INVASE\u201d), we explain that \u201cWe use cross-validation to select lambda among {0.1,0.3,0.5,1,2,5,10}\u201d. We select the lambda which maximizes the predictor accuracy in terms of AUROC. We will clarify this in the revised manuscript. Below, we give the results for various values of lambda in the Syn4, Syn5, and Syn6 settings. More detailed results will be added to the revised manuscript.\n--------------------------------------------------------------------------------------------------------------------------------------------------\n               Datasets             |                   Syn4                   |                   Syn5                   |                   Syn6                   |\n--------------------------------------------------------------------------------------------------------------------------------------------------\n   Lambda / Metrics (%)  |         TPR        |      FDR        |         TPR        |      FDR       |         TPR        |      FDR        |         \n--------------------------------------------------------------------------------------------------------------------------------------------------\n                 0.1                      |       98.0         |      94.3        |         90.0        |      93.4      |         99.2        |        92.3     |\n                 0.3                      |       93.7         |      87.9        |         84.2        |      88.9      |         96.9        |        86.7     |\n                 0.5                      |       99.0         |      43.1        |         88.3        |      50.6      |         99.6        |        31.7     |\n                   1                       |       66.3         |      40.5        |         73.2        |      23.7      |         90.5        |        15.4     |\n                   2                       |         0.0         |       0.0         |         25.4        |       4.1       |         67.1         |         3.6      |\n                   5                       |         0.0         |       0.0         |          7.5         |       2.7       |          7.6          |         2.5      |\n                  10                      |         0.0         |       0.0         |          0.0         |       0.0       |          0.0          |         0.0      |         \n--------------------------------------------------------------------------------------------------------------------------------------------------     \n\nA3: As can be seen in the GitHub code (anonymously published on https://github.com/iclr2018invase/INVASE), on testing data, we select the features whose selection probabilities are larger than 0.5. (see line 225 in INVASE-.py and line 274 in INVASE.py) We will clarify this in the revised manuscript.", "title": "RE: Instance-wise feature selection"}, "rkxFiB2Tom": {"type": "review", "replyto": "BJg_roAcK7", "review": "In the paper, the authors proposed a new algorithm for instance-wise feature selection. In the proposed algorithm, we prepare three DNNs, which are predictor network, baseline network, and selector network. The predictor network and the baseline networks are trained so that it fits the data well, where the predictor network uses only selected features sampled from the selector network. The selector network is trained to minimize the KL-divergence between the predictor network and the baseline network. In this way, one can train the selector network that select different feature sets for each of given instances.\n\nI think the idea is quite simple: the use of three DNNs and the proposed loss functions seem to be reasonable. The experimental results also look promising.\n\nI have a concern on the scheduling of training. Too fast training of the predictor network can lead to the subotpimal selection network. I have checked the implementations in github, and found that all the networks used Adam with the same learning rates. Is there any issue of training instability? And, if so, how we can confirm that good selector network has trained?\n\nMy another concern is on the implementations in github. The repository originally had INVASE.py. In the middle of the reviewing period, I found that INVASE+.py has added. I am not sure which implementations is used for this manuscript. It seems that INVASE.py contains only two networks, while INVASE+.py contains three networks. I therefore think the latter is the implementation used for this manuscript. If this is the case, what INVASE.py is for?\nI am also not sure if it is appropriate to \"communicate\" through external repositories during the reviewing period.", "title": "Simple idea, but good results", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}