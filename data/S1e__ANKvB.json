{"paper": {"title": "Molecular Graph Enhanced Transformer for Retrosynthesis Prediction", "authors": ["Kelong Mao", "Peilin Zhao", "Tingyang Xu", "Yu Rong", "Xi Xiao", "Junzhou Huang"], "authorids": ["mkl18@mails.tsinghua.edu.cn", "masonzhao@tencent.com", "tingyangxu@tencent.com", "yu.rong@hotmail.com", "xiaox@sz.tsinghua.edu.cn", "joehhuang@tencent.com"], "summary": "", "abstract": "With massive possible synthetic routes in chemistry, retrosynthesis prediction is still a challenge for researchers. Recently, retrosynthesis prediction is formulated as a Machine Translation (MT) task. Namely, since each molecule can be represented as a Simpli\ufb01ed Molecular-Input Line-Entry System (SMILES) string, the process of synthesis is analogized to a process of language translation from reactants to products. However, the MT models that applied on SMILES data usually ignore the information of natural atomic connections and the topology of molecules. In this paper, we propose a Graph Enhanced Transformer (GET) framework, which adopts both the sequential and graphical information of molecules. Four different GET designs are proposed, which fuse the SMILES representations with atom embedding learned from our improved Graph Neural Network (GNN). Empirical results show that our model signi\ufb01cantly outperforms the Transformer model in test accuracy.", "keywords": []}, "meta": {"decision": "Reject", "comment": "Several approaches can be used to feed structured data to a neural network, such as convolutions or recurrent network. This paper proposes to combine both roads, by presenting molecular structures to the network using both their graph structured and a serialized representation (SMILES), that are processed by a framework combining the strenth of Graph Neural Network and the sequential transformer architecture.\n\nThe technical quality of the paper seems good, with R1 commenting on the performance relative to SOTA seq2seq based methods and R3 commenting on the benefits of using more plausible constraints. The problem of using data with complex structure is highly relevant for ICLR.\n\nHowever, the novelty was deemed on the low side. As a very competitive conference, this is one of the key aspects necessary for successful ICLR papers. All reviewers agree that the novelty is too low for the current (high) bar of ICLR. "}, "review": {"B1xi4igOjS": {"type": "rebuttal", "replyto": "rygfa6a_Fr", "comment": "Thank you for your review.  \n(1) For retrosynthesis prediction,  our work is the first attempt to utilize both the graphical and sequential information of molecules.  We fully investigate four organization forms of graph neural network (GNN) with Transformer and significantly improve the prediction performance in many metrics, i.e.,  accuracy and chemical invalid rate. Besides, the GNN we used is also well-designed to further improve the performance, which is more powerful to learn the representation of atoms.  \n\n(2) We also thank for your directional suggestion  (we will consider the chemistry conference).\n\n(3) The decoder of our model is vanilla Transformer's decoder, thus it's a sequence generative model. The molecular graph information is only used in the encoder phase, since we cannot get the whole molecular graphs of the reactants when decoding. However, how to directly generate a molecular graph not a SMILES sequence is just what we are working on, and we will add the related work about graph Autoencoders and Graph generative models later.  Thank you very much.\n\n(4) We report the result of Rule-based Expert System, LSTM+Attention and Similarity by their original paper, not our re-implementation. We just re-implement the Transformer model's result since there have been 4 papers using Transformer to solve retrosynthesis, while they reported different results. So we had to re-implement it by ourselves. Actually, their results are closed to ours, and our GET is also obviously better than them.", "title": "Author Response #4"}, "S1esjRzdir": {"type": "rebuttal", "replyto": "H1gIxC_m9H", "comment": "We thank the reviewer for the feedback and comments. Our replies are as follow.\n\n-\"This paper provide no novelty with respect to deep learning method. It is just a combination of sequence transformer and graph neural network \"\n\nOur work is the first attempt to fuse the graphical and sequential information of molecules to solve the retrosynthesis prediction task. We take the application of deep learning to retrosynthesis in novel directions.  We fully investigate four organization forms of graph neural network (GNN) with Transformer and significantly improve the prediction performance in many metrics, i.e.,  accuracy and chemical invalid rate.  Besides, the GNN we used is also well-designed to further improve the performance, which is more powerful to learn the representation of atoms.  \nWe note that the successful combination of GNN and Transformer model is meaningful since the molecule has graph structure naturally and sequential structure artificially. We think that how to jointly utilize these two forms' information effectively is important in this area, while it has not been explored in literature.  Our work just fills this gap and achieves good results.\n", "title": "Author Response #1"}, "ryl3TdMOsr": {"type": "rebuttal", "replyto": "HygJspHTFB", "comment": "Thanks for your constructive suggestions and questions. We appreciate your approval of the meaning of our work. Our replies are as follow:\n1. Sorry for not providing the error bar in the original paper.  Here we provide the error bar of GET-LT1 (We experiment 3 times with different random seeds (41, 42, 43) ).  \n\nModel  | top-1 | top-3 | top-5 | top-10   \n---------------------------------------------------------------------------------------------\nGET-LT1 (our) |  59.1\u00b10.062 | 73.4\u00b10.276 | 76.4\u00b10.187 |  78.7\u00b10.179\n\nThe above is our current  result. We ensemble 10 GET-LT1 models with different parameters by averaging the models\u2019 probability vectors, i.e., the output vector of the decoder, to decide the generated SMILES character at each time step. By this way, we further improve the accuracy.\n\n2. Yes, bootstrapping Transformer model with multiple non-canonical SMILES for the same input molecules can improve performance. Someone has done this experiment. (It can be find in  https://github.com/kheyer/Retrosynthesis-Prediction) . With multiple valid SMILES, the model can learn the grammar of SMILES more easily and achieve better results.\n\n4. Compared with GET-LT2, GET-LT1 utilizes the atom information;  Compared with GET-LG, GET-LT1's learned representations are more determined by the Transformer and we think this is why it is better.  Since the decoder is a \"sequence decoder\", it may be better to give the \"sequential encoder\"(i.e., Transformer encoder) more weights while using the graphical encoder (i.e., Graph Encoder) as an enhancement. Thus, letting the representations first pass through the graph encoder and  then pass through the Transformer encoder may be better; Compared with GET-CT,  fusing two-level's embeddings in a serial way while not in a parallel way (i.e., concatenation) can get better representations.\n\n5. Yes, for the USPTO-50K dataset, our model's generative ability is the best. \n\n6. We can get the top-N results with N beam size. As N become larger, the model's generative confidence for the top-N's result decrease, i.e., top-1 > top-2 > top-3 > top-4 > top-5. Larger N will result in more molecules which are generated with less confidence, so the invalid rate will go up.\n\n7. We provide the concrete parameters of each model in our code. https://github.com/papercodekl/ICLR2020_MGET.  The training script is \npython -u train.py -data data2/seqdata -save_model experiments/checkpoints2/model -seed 42  -gpu_ranks 0 -save_checkpoint_steps 10000 -keep_checkpoint 20 -train_steps 500000 -param_init 0  -param_init_glorot -max_generator_batches 32 -batch_size 4096 -batch_type tokens -normalization tokens -max_grad_norm 0  -accum_count 4 -optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam -warmup_steps 8000  -learning_rate 2 -label_smoothing 0.0 -report_every 1000 -layers 4 -rnn_size 256 -word_vec_size 256 -encoder_type transformer -decoder_type transformer -dropout 0.1 -position_encoding -share_embeddings -global_attention general -global_attention_function softmax -self_attn_type scaled-dot -heads 8 -transformer_ff 2048.\n\nAnd we will supplement the parameter settings of each model in the paper later.\n\n8.  The  astrices (*) means the best result among template-free methods.\n9. Sorry, its our negligence.\n10. Yes, I mean sigmoid, sorry for my spelling mistake.\n11. k means the k-th head. We refer to GAT (Velickovic et al. 2018\u00b4) to write this symbol. Maybe it's better to write as a subscript. \n\n12. SELFIES is a new proposed representation, thank you very much for telling us this informative paper and we will test our model with SELFIES later. \n", "title": "Author Response #3"}, "rygfa6a_Fr": {"type": "review", "replyto": "S1e__ANKvB", "review": "\nThis paper focuses on the retrosynthesis prediction problem which to my understanding is the factorization of a target molecule into simpler structures. Previously, retrosynthesis prediction has been tackled as a language translation problem by using a Seq2Seq algorithm called \"Transformer\". This sequential approach was possible because molecules can be expressed as strings using the following format: Simplified Molecular-Input Line-Entry System (SMILES).\n\nDespite its good performance, language translation methods ignore the graphical structure of molecules. This paper proposes to add a graph neural network in front of the Seq2Seq/Transformer network to exploit the graphical structure, hence the name \u201cGraph Enhanced Transformer (GET)\u201d. The Graph Neural Network considered in this work also uses an attention mechanism similarly to Graph Attention Networks.\n\nThe main contribution is the addition of a Graph Neural Network to a Seq2Seq model for retrosynthesis prediction which provides state of the art results.\n\nFrom a machine learning / representation learning perspective, I do not consider the paper is innovative enough. Even so, the paper shows strong results for retrosynthesis prediction, I guess it would be a good fit in a chemistry conference.\n\n\nThings to improve:\n\nThere are plenty of works related to graph Autoencoders and Graph generative models that are not mentioned in the related work. It would be great to dedicate a section for them. Here some examples: \n\n- Li, Y., Vinyals, O., Dyer, C., Pascanu, R., & Battaglia, P. (2018). Learning deep generative models of graphs.\n\n-Simonovsky, M., & Komodakis, N. (2018, October). Graphvae: Towards generation of small graphs using variational autoencoders.\n\n-De Cao, N., & Kipf, T. (2018). MolGAN: An implicit generative model for small molecular graphs.\n\n-You, J., Ying, R., Ren, X., Hamilton, W. L., & Leskovec, J. (2018). Graphrnn: Generating realistic graphs with deep auto-regressive models.\n\nAll experiments are compared to author\u2019s re-implementations of other works. It would be interesting to directly report other work accuracies.\n", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 3}, "HygJspHTFB": {"type": "review", "replyto": "S1e__ANKvB", "review": "~The authors propose an enhancement to the transformer architecture that takes molecule graph structure into account.~\n\nI applaud the authors work on making more physically plausible machine learning constraints. However, I feel like this work is incremental and does not vastly improve SoA.\n\nFor all tables, what are the error bars on the accuracy?\n\nThere are multiple possible SMILES strings for any unique molecule (which includes the canonical SMILES string.) Does bootstrapping your Transformer model with multiple non-canonical SMILES for the same input molecules improve performance?\n\nMany seq2seq molecules allow an attention mechanism on the input sequence while decoding, and that seems like this would be useful for this data. What would the impact of this be?\n\nDo you have any explanation why GET-LT1 is your top performing model?\n\nDoes your model generalize to unseen molecules or reactions better than previous methods?\n\nIn Table 5, why does the % invalid SMILES go up with more beams? I would figure that larger beam sizes would result in more valid generated molecules?\n\nIn Table 4, please report the number of parameters for each model.\n\nIn Table 2, what are the astrices (*) in the GET-LT1 row?\n\nIn Table 3, you should bold the Similarity model for the top-5 and top-10 accuracy.\n\nIn equation 9, do you mean \u201csigmoid\u201d?\n\nIn equation 8, should the k superscript be a subscript?\n\nHow would this model perform with SELFIES representation of small molecules, which are more robust representation [Krenn et al., 2019]?\n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "H1gIxC_m9H": {"type": "review", "replyto": "S1e__ANKvB", "review": "This paper proposed Graph Enhanced Transformer(GET) to combine the graphical and sequential representations of the molecule to improve the retrosynthesis prediction performance. Experiments indicated that the proposed model outperforms state-of-the-art Seq2Seq-based methods on USPTO-50K dataset, and showed ability in reducing invalid SMILES rate.\n\nTwo main comments: \n\n1. This paper provide no novelty with respect to deep learning method. It is just a combination of sequence transformer and graph neural network (using RDKit(Landrum, 2016) to transform a SMILES into the molecular graph). The decoder is the same as vanilla Transformer to generate SMILE string output. \n\n2. The writing can be improved. For instance, in the caption of Figure 1 - \"somehow to be transformed\" ...  plus a few other places have wording issues like this.  \n\n \n\n\n", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 4}}}