{"paper": {"title": "Deep Learning with Sets and Point Clouds", "authors": ["Siamak Ravanbakhsh", "Jeff Schneider", "Barnabas Poczos"], "authorids": ["mravanba@cs.cmu.edu", "bapoczos@cs.cmu.edu", "jeff.schneider@cs.cmu.edu"], "summary": "Parameter-sharing for permutation-equivariance and invariance with applications to point-cloud classification.", "abstract": "We introduce a simple permutation equivariant layer for deep learning with set structure. This type of layer, obtained by parameter-sharing, has a simple implementation and linear-time complexity in the size of each set. We use deep permutation-invariant networks to perform point-could classification and MNIST digit summation, where in both cases the output is invariant to permutations of the input. In a semi-supervised setting, where the goal is make predictions for each instance within a set, we demonstrate the usefulness of this type of layer in set-outlier detection as well as semi-supervised learning with clustering side-information.", "keywords": ["Deep learning", "Structured prediction", "Computer vision", "Supervised Learning", "Semi-Supervised Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "This paper studies neural models that can be applied to set-structured inputs and thus require permutation invariance or equivariance. After a first section that introduces necessary and sufficient conditions for permutation invariance/equivariance, the authors present experiments in supervised and semi-supervised learning on point-cloud data as well as cosmology data.\n \n The reviewers agreed that this is a very promising line of work and acknowledged the effort of the authors to improve their paper after the initial discussion phase. However, they also agree that the work appears to be missing more convincing numerical experiments and insights on the choice of neural architectures in the class of permutation-covariant. \n \n In light of these reviews, the AC invites their work to the workshop track. \n Also, I would like to emphasize an aspect of this work that I think should be addressed in the subsequent revision.\n \n As the authors rightfully show (thm 2.1), permutation equivariance puts very strong constraints in the class of 1-layer networks. This theorem, while rigorous, reflects a simple algebraic property of matrices that commute with permutation matrices. It is therefore not very surprising, and the resulting architecture relatively obvious. So much so that it already exists in the literature. In fact, it is a particular instance of the graph neural network model of Scarselli et al. '09 (http://ieeexplore.ieee.org/abstract/document/4700287/) when you consider a complete graph, which has been used in the setup of full set equivariance for example in 'Learning Multiagent communication with backpropagation', Sukhbaatar et al NIPS'16; see also 'Order Matters: sequence to sequence for sets', Vinyals et al. https://arxiv.org/abs/1511.06391. \n The general question of how to model point-cloud data, or more generally data defined over graphs, with neural networks is progressing rapidly; see for example https://arxiv.org/abs/1611.08097 for a recent survey.\n \n The question then is what is the contribution of the present work relative to this line of work. The authors should answer this question explicitly in the revised manuscript, either with a new application of the model, or with theory that advances our understanding of these models, or with new numerical applications."}, "review": {"B1QElZ3Ue": {"type": "rebuttal", "replyto": "rk3j7Fi8l", "comment": "Thanks for your comments! We have added a paragraph explaining the relation of Chen et al.\u201914 to our work. Scattering features are permutation invariant, however, they cannot be used for semi-surpervised learning where we need \u201cequivariance\u201d. Their other requirement seems to be an underlying graph structure to guide the partitioning of the nodes. Also note that we believe the scrambled MNIST results in their paper is produced by permuting the pixels with an \u201cidentical\u201d permutation matrix for all instances and their result on this dataset is not permutation-invariant by our definition. They only intend to show that they are able to find the underlying pixel neighborhood and leverage it in supervised learning (We intended to compare our method against their method on this task before we noticed this issue.)\n\nWe have also added another baseline for the red-shift improvement experiments from the original redMaPPer catalog, per your request.\n", "title": " "}, "BkCk4pcUe": {"type": "rebuttal", "replyto": "HJF3iD9xe", "comment": "We thank all reviewers for their comments! A commonl issue in reviews was regarding the disconnect between our general treatment of invariance and experimental results. The other major issue was regarding the clarity of the first part. \n\nTo resolve both issues we have removed our general treatment and focused on \"deep learning with sets and point-clouds\", incorporating all reviewer feedback on this part. We welcome any further comments on the revised version.", "title": "Major Revision"}, "HJIvka5Ix": {"type": "rebuttal", "replyto": "S1cGMcb4g", "comment": "Thanks for your feedback! \nTo remove the gap between our general formalism and the experimental treatment that was limited to sets and point-clouds, we have extensively revised the paper, solely focusing on the application of deep permutation-equivariant/invariant models. We have added new experimental results and clarified the formalism for sets. \n", "title": "Re:A promising work!"}, "HJ5Gk69Ie": {"type": "rebuttal", "replyto": "H1bCVY-4x", "comment": "Thank you very much for your feedback! We appreciate the time you have spent on our paper and based on your comments we have extensively rewritten the paper. We have removed all the abstract and general treatments of invariance and focused solely on the case of sets and point-clouds. We have included more experimental results and baseline evaluations as well. Any further feedback is appreciated.\n", "title": "Re:interesting topic, but hard to follow for a non-expert"}, "SJJj0h58e": {"type": "rebuttal", "replyto": "BkY_LYbNe", "comment": "Thank you very much for your feedback! Based on your comments, we have extensively revised the paper. In particular, we have removed the general treatment of invariances from this paper in favour of clarity and focused solely on the case of permutation-invariance/equivariance for set structure. \n\nRegarding the choice of set-layer, a new theorem in our paper shows that parameter-sharing of our set-layer is the only way of achieving permutation-equivariance in \u201cstandard\u201d neural network layers. \n\nWRT baselines: We have added new experiments as well as more baseline results for \u201call\u201d settings to address your concern. \n\nWRT composition: in the revised paper, we show that functional composition preserves permutation-equivariance.\n\nWRT connections to symmetric function theory: the permutation-invariant function in our case is indeed a so-called \u201csymmetric function\u201d. We have added citations for this. However, symmetric function theory is concerned with polynomial forms while we study symmetric functions in the form of abstract neurons.\n", "title": "Re:Interesting formalization of invariance in neural networks, but too abstract and weak experimental results"}, "HkOIA258g": {"type": "rebuttal", "replyto": "HyLUaojHx", "comment": "Dear Andrew, thank you for your feedback. We have basically rewritten the paper, focusing only on sets and point-clouds. We do agree with you that treatment of point-cloud data is highly useful in many modern applications and hope that our work motivates researchers in robotics and vision to directly employ point-cloud data.\n\nWRT concerns regarding design of the layer: We have added a theorem showing that our parameter-sharing is the only mechanism to achieve permutation-equivariance in \u201cstandard\u201d neural network layers. Your suggestion of sharing a FC layer between the instances in the set is what we refer to as set-pooling and only works for supervised settings.\n\n\nWRT your concern regarding the invariance to spatial transformations in the point-cloud data: great observation! We are aware of this. If in any application a reference point is needed in the point-cloud, it can be added as another point (possibly outside the boundary of the cloud) to fix an origin.\n", "title": "Re:Excellent choice of topic, but more work is needed for the results to be actionable."}, "H1ZpBsZVe": {"type": "rebuttal", "replyto": "ry6mm9-4l", "comment": "Thanks for your follow up! \n\nIn the case of convolution, \"permutation over relations\" does not result in the final translation equivariance. That is the commutative summation plays no role here. What does the trick for 2D-grid is the fact that individual functions f^{i,j}(x_{S}) defined on the neighborhood of (i,j) pixel, share their parameters -- i.e. f^{i',j'} = f^{i,j} = f_{theta} for all 0<i,j<N associated with pixels; see figure 2(a)-left.\n\nIn general there are two factors at play in producing invariances of a structure: 1) invariance to permutation over relations in the structure S; 2) parameter-sharing across functions f(x_S) for x \\in \\mathcal{X}, also encoded in the structure S.\n\n  \n\n", "title": "Re: Invariance to structure"}, "SyIAEIgEe": {"type": "rebuttal", "replyto": "r1Gx5XxVl", "comment": "\nThank you for your follow up!\n\nThe product structure is indeed central in our approach to handling of structure. According to our partial ordering of structures, they appear between two extreme cases of \u201call\u201d structure --that requires using fully connected layer-- and \u201cnull\u201d structure of the mini-batch. Note that the set-structure (handled by set-invariant layer) is not at either extremes of this partial order.  \nNow, to handle the product of any structures (not limited to \u201cnull\u201d, \u201call\u201d or \u201cset\u201d structures), one could obtain the product (as defined on page 3) and proceed by defining the parameter-sharing of the layers according to eq(1). A faster \u201cheuristic\u201d is to handle one structure at a time, which as our extreme example 4.3 shows could lead to trouble. \n\nLet me clarify where and why we use this heuristic:\n\nWe present three sets of experiments on 1) point-cloud-classification; 2) face outlier detection and; 3) semi-supervised learning with sets. The product-structures in experiments (1) and (3) are the product of \u201cnull\u201d, \u201cset\u201d and \u201call\u201d structures, which only require using the set-invariant layer of section 5.1. Here, note that multiple input channels correspond to the \u201call\u201d structure in the product structure. \n\nHowever, in the face outlier detection experiment (2), we also have the 2D grid structure that requires using the parameter-sharing of the 2D-convolution layer. Here the complete product structure is: \n\nS = null \\times set \\times all \\times 2D-Grid\n\nWhere \u201cnull\u201d structure reflects the mini-batch, set structure is due to having a set of 16 face images, \u201call\u201d structure is due to multiple input channels (e.g. R, G, B) and 2D-grid reflects the structure of 2D image. To correctly handle this product structure, we need to use the parameter-sharing implied by the resulting product. However, for handling the product of 2D-grid and set structures, we could not use the efficient implementation of convolution layer. That is why we use the heuristic of section 4.1 for handling one structure at a time: first we define multiple convolution-pooling layers to capture the spatial/grid structure and then use the set-invariant layer of section 4.1 to handle the product of \u201cnull\u201d, \u201call\u201d and \u201cset\u201d structures (details are in Appendix D.1.). As our result for face outlier detection indicates, handling one structure at a time, works well here. \n\nHere, while one could use the heuristic of section 4.1 followed by some other technique (as you suggest) to reduce the \u201cinformation loss\u201d, for best results one might as well avoid the heuristic of section 4.1 altogether.\n\nAlthough we repeatedly acknowledge the fact that our experiments are limited to the set structure, our framework is more general; demonstrated by deriving other types of layers as special cases. We believe this alternative approach to structure based on parameter-sharing is useful on its own -- for example our derivation of graph convolution is much simpler and gives complementary view to current derivation. We are currently investigating applications of this framework for more complex structures such as nested sets and graph of graphs for multi-resolution handling of point-clouds and graphs. We agree with you that it would be very useful to be able to automatically infer the structure from data, however we have no idea for an efficient solution at the moment.\n\nI hope this addresses your questions and comments. ", "title": "Re: Clarifications"}, "rJPInXkQl": {"type": "rebuttal", "replyto": "SJf7U6CGx", "comment": "When the invariance is not \"minimal\" (as defined in section 4), one could lose discriminative information. The only place this lack of minimal invariance comes up in our paper is as an easy way to handle the product structure. Example 4.3 gives an extreme scenario to see the possible issues with using this trick with product structures. This inadvertent loss of information does not happen with correct (but possibly cumbersome) way of handling a product structure using minimally invariant layer (of eq. 1).\n\nThis means the set-invariat layer (as well-as other structured discussed, such as graph-convolution) is minimally invariant -- e.g.,\n the output of point-cloud classification is indeed invariant to the permutation of points. You are correct in relating the set structure to graph structure. However, note that our proposed set-layer, eq(4), is different from what you get from graph-convolution in using the max operation and its linear (rather than quadratic) complexity.\n", "title": "Re: Set structure questions"}, "SJf7U6CGx": {"type": "review", "replyto": "HJF3iD9xe", "review": "If I understand correctly, one issue with the invariance to permutation of the graph is that one can loose discriminative information (example 4.3). Might it be possible to recover the loss of information due to the averaging?\n\nI understand why one needs a set structure for the set anomaly detection, but not for the point cloud detection. Indeed, in the anomaly detection case, the objective of the problem is to seprate the set E of faces into E\\{x} and {x} where x is the anomaly, and E\\{x} is invariant by permutations. But, in the point cloud detection, I would have expected the final objective to be invariant by any permutations of the points. I guess I am doing a logical error, could you correct me? Is it to avoid discriminability issues? Why did you chose this graph? In this case, can we interpret your model as a graph convolution, with the edges at x being {(x,x'),x'\\neq x}?\n\nIf the relational dependencies are not given, could they be learned?\n\nThanks.Pros : \n- New and clear formalism for invariance on signals with known structure\n- Good numerical results\n\nCons :\n- The structure must be specified.\n- The set structure dataset is too simple\n- There is a gap between the large (and sometimes complex) theory introduced and the numerical experiments ; consequently a new reader could be lost since examples might be missing\n\nBesides, from a personal point of view, I think the topic of the paper and its content could be suitable for a big conference as the author improves its content.  Thus, if rejected, I think you should not consider the workshop option for your paper if you wish to publish it later in a conference, because big conferences might consider the workshop papers of ICLR as publications. (that's an issue I had to deal with at some points)", "title": "Set structure questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1cGMcb4g": {"type": "review", "replyto": "HJF3iD9xe", "review": "If I understand correctly, one issue with the invariance to permutation of the graph is that one can loose discriminative information (example 4.3). Might it be possible to recover the loss of information due to the averaging?\n\nI understand why one needs a set structure for the set anomaly detection, but not for the point cloud detection. Indeed, in the anomaly detection case, the objective of the problem is to seprate the set E of faces into E\\{x} and {x} where x is the anomaly, and E\\{x} is invariant by permutations. But, in the point cloud detection, I would have expected the final objective to be invariant by any permutations of the points. I guess I am doing a logical error, could you correct me? Is it to avoid discriminability issues? Why did you chose this graph? In this case, can we interpret your model as a graph convolution, with the edges at x being {(x,x'),x'\\neq x}?\n\nIf the relational dependencies are not given, could they be learned?\n\nThanks.Pros : \n- New and clear formalism for invariance on signals with known structure\n- Good numerical results\n\nCons :\n- The structure must be specified.\n- The set structure dataset is too simple\n- There is a gap between the large (and sometimes complex) theory introduced and the numerical experiments ; consequently a new reader could be lost since examples might be missing\n\nBesides, from a personal point of view, I think the topic of the paper and its content could be suitable for a big conference as the author improves its content.  Thus, if rejected, I think you should not consider the workshop option for your paper if you wish to publish it later in a conference, because big conferences might consider the workshop papers of ICLR as publications. (that's an issue I had to deal with at some points)", "title": "Set structure questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}