{"paper": {"title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "authors": ["Jianhe Yuan", "Zhihai He"], "authorids": ["yuanjia@missouri.edu", "hezhi@missouri.edu"], "summary": "Defense against adversarial attacks.", "abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "keywords": ["Adversarial Defense", "Adversarial Attack"]}, "meta": {"decision": "Reject", "comment": "This paper presents a method to defend neural networks from adversarial attack. The proposed generative cleaning network has a trainable quantization module which is claimed to be able to eliminate adversarial noise and recover the original image. \nAfter the intensive interaction with authors and discussion, one expert reviewer (R3) admitted that the experimental procedure basically makes sense and increased the score to Weak Reject. Yet, R3 is still not satisfied with some details such as the number of BPDA iterations, and more importantly, concludes that the meaningful numbers reported in the paper show only small gains, making the claim of the paper less convincing. As authors seem to have less interest in providing theoretical analysis and support, this issue is critical for decision, and there was no objection from other reviewers. After carefully reading the paper myself, I decided to support the opinion and therefore would like to recommend rejection. \n"}, "review": {"B1erP1osoS": {"type": "rebuttal", "replyto": "rJlzPUN9jS", "comment": "\nWe really appreciate your valuable comment! \n\nBPDA is an attack method. In the original paper of BPDA, they provided results on MINST, CIFAR-10, and ImageNet. Here is the reason that we only had BPDA defense results on the CIFAR-10. (1) We found that only one defense paper had results on MINST, so we did not provide comparisons on MINST. (2) For the ImageNet, it is too huge and extremely time consuming for us to run all of these experiments. (3) In recently defense papers, only the STL method from CVPR 2019 provided results on BPDA, which we have included it in the paper and our algorithm significantly outperforms it. \u00a0\nWe used the standard attack-defense evaluation package called AdverTorch which provides implementations of major attack methods, including BPDA.\n\nWe set 10 attack iterations as the BPDA baseline setting. In addition, the large iterations and large epsilon evaluation of BPDA attacks and are also presented in Figure 3 and Figure 4, which show the proposed method is able to effective defend BPDA attacks and outperform other methods. \n\nWe did used the adversarial samples for training the generator and discriminator. But we did not re-train the target classifier and made no modification to the target classifier.", "title": "Responses to Reviewer 3 comment"}, "BylE_J6vsr": {"type": "rebuttal", "replyto": "ryea03dwoB", "comment": "We really appreciate your insightful comment! \n\nWe forgot to plot more points on the curve. Yes, even with epsilon = 0.2, the accuracy with BPDA attack becomes 1.35%, far below 10%, according to our evaluation. However, with PGD at epsilon=0.5, our accuracy is 12.63%, which is slightly above 10% as you mentioned. This is because our nonlinear transform layer has disrupted the gradient propagation behavior of PGD. Because of this, the attacked image by PGD is messed up, instead of being totally random. When we set the epsilon to 0.6, the accuracy drops to near 10%. This shows BPDA is more effective than PGD with our defense. Hope this has addressed your concern. We will update the figure in the paper shortly to include these additional points on the curve, plus the large-epsilon BPDA curve. Thank you very much!", "title": "Responses to Reviewer 3 comment "}, "S1eBQkXmjr": {"type": "rebuttal", "replyto": "HJgwJxIzsB", "comment": "Dear Reviewer 3, as suggested by you, we have updated the paper to include the large-iterations and large-epsilon PGD attacks of our method. We also compare our algorithm against PNI and vanilla adv. training method. Please see Figure 4. We can see that our method is able to surve the large-iteration PGD attacks and significantly outperform the other two methods. Also, our method performs much better than the other two methods during large-epsilon attacks with epsilon going to 1, as you suggested. Please review. Thank you!", "title": "Responses to Reviewer 3 comment"}, "B1xYZtPzoB": {"type": "rebuttal", "replyto": "HJgwJxIzsB", "comment": "Dear Reviewer, Thank you so much for the quick response and appreciate your valueable suggestion! We are updating the paper to include these two figures, as you suggested. We will upload the paper soon and let you know once it is updated. ", "title": "Responses to Reviewer 3 comment"}, "ryeM4izzsB": {"type": "rebuttal", "replyto": "BJgs00VTKS", "comment": "As we can see from our paper, we followed exactly the same evaluation procedure and used the same datasets as the paper mentioned by the reviewer. Following existing papers recently published in ICLR/CVPR/ICCV/ECCV 2017-2019, we used the advTorch standard package to generate all attacks on our method.\n\nFigure 3 shows the large number of BPDA iterations. Due to the page limitation, we did not include the figure for larger number of PGD iterations, since BPDA is a more powerful attack method than PGD. \n\nWe could not include the figure for attacks with large epsilon since this figure is not very critical and many recent papers chose not to include it due to page limitations. \n\nFollowing recently published paper, we have included the most comprehensive performance comparison results in the paper. We have demonstrated  that our method significantly outperforms existing state-of-the-art methods.", "title": "Responses to Reviewer 3 comment"}, "rklNHwMGsS": {"type": "rebuttal", "replyto": "SkxWHoIRYH", "comment": "We sincerely thank the Reviewer for the positive feedback and high recommendation of our paper!\n\nThanks for pointing out this! In this paper, we use the benchmark datasets and compare our results against the results published in existing papers. \n\nUnfortunately, not all published papers provided complete results for both of these datasets. For example, some papers did not have results on SVHN. In  this case, we left them empty. \n\nWe chose not to re-implement existing methods since it will be very hard to re-produce their exact results, which might lead to unfair performance comparisons.  \n\nDuring our experiments, we tried our best to compare with as many methods as possible if they have published results on the dataset.", "title": "Response to Reviewer 4"}, "rJgRrNMfoB": {"type": "rebuttal", "replyto": "SygPDfaXcr", "comment": "We sincerely thank Reviewer 1 for the positive and encouraging comments!\n\nThe reviewer suggests that we need to provide theoretical analysis or proof why the proposed deep neural network defense method is working. We must admit that this is really hard. The deep learning research community is still working very hard to establish theoretical analysis or proof for deep neural networks, which is however is very challenging. \n\nHowever, our proposed method is based on data-driven deep learning. All the defense networks are trained based on the loss functions. So, if the loss function converges, the proposed method is achieving the target performance. \n\nFollowing existing state-of-the-art methods published in recent ICLR/CVPR/ICCV/ECCV papers, we use the benchmark their datasets and standard evaluation protocols to demonstrate that our proposed method is significantly outperforming existing methods.  \n\nFor the experiments, we have results on two datasets, CIFAR-10 and SVHN with two different attack modes: white-box attack and black-box attacks. Sorry for the confusion. We will better organize these experimental results.\n\nFor the attacks, we follow the standard procedure used in all existing methods. Specifically, we use advTorch evaluation package to generate all attacks. ", "title": "Response to Reviewer 1 Comments"}, "BJgs00VTKS": {"type": "review", "replyto": "SkxOhANKDr", "review": "This paper proposes a method for adversarial defense based on generative cleaning.\n\nThe paper does not follow any of the best practices for evaluating adversarial robustness, e.g. in these two papers:\n\"On Evaluating Adversarial Robustness\" https://arxiv.org/abs/1902.06705\n\"Obfuscated Gradients Give a False Sense of Security\" https://arxiv.org/abs/1802.00420\n\nFor instance the paper does not use a large number of PGD iterations (10 is too small) and does not check that accuracies go to zero for large epsilon (an important sanity check to reveal gradient masking). In the one place where a larger number of attack iterations is used (100 for BPDA) the gap with adversarial training mostly vanishes.\n\nIn the absence of these best practices it is impossible to assess the validity of the results, so the paper should be rejected.", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 4}, "SkxWHoIRYH": {"type": "review", "replyto": "SkxOhANKDr", "review": "This paper proposes a new method to defend a neural network agains adversarial attacks (both white-box and black-box attacks). By jointly training a Generative Cleaning Network with quantized nonlinear transform, and a Detector Network, the proposed cleans the incoming attacked image and correctly classifies its true label. The authors use state-of-the-art attack methods on various models, and the proposed model consistently outperforms all baseline models, even dramatically outperforming them for some specific attack methods.\n\nComment:\nIs there a reason the authors did not test the same set of attack methods for SVHN as they did for CIFAR-10?", "title": "Official Blind Review #4", "rating": "8: Accept", "confidence": 1}, "SygPDfaXcr": {"type": "review", "replyto": "SkxOhANKDr", "review": "This paper developed a method for defending deep neural networks against adversarial attacks based on generative cleaning networks with quantized nonlinear transform. The network is claimed to recover the original image while cleaning up the residual attack noise. The authors developed a detector network, which serves as the dual network of the target classifier network to be defended, to detect if the image is clean or being attacked. This detector network and the generative cleaning network are jointly trained with adversarial learning so that the detector network cannot find any attack noise in the output image of generative cleaning network. The experimental results demonstrated that the proposed approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. \n\nA few comments: \n\n1. It does not provide theoretical reasons why the prosed method can defend against those attacks. \n\n2. The experiments are a bit messy and the attacks' setup need to improve. \n\n3. The proposed defense showed only empirical results against the target attack. It seems to provide no theoretical / provable guarantees. \n\n\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}}}