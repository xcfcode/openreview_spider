{"paper": {"title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "authors": ["Yue Xing", "Qifan Song", "Guang Cheng"], "authorids": ["xing49@purdue.edu", "qfsong@purdue.edu", "chengg@purdue.edu"], "summary": "", "abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.", "keywords": ["Data Interpolation", "Multiplicative Constant", "W-Shaped Double Descent", "Nearest Neighbor Algorithm"]}, "meta": {"decision": "Reject", "comment": "The authors show that data interpolation in the context of nearest neighbor algorithms, can sometime strictly improve performance. The paper is poorly written for an ICLR audience and the added value compared to extensive prior work in the area is not clearly demonstrated."}, "review": {"SygzwYFssr": {"type": "rebuttal", "replyto": "rklTS2CAKS", "comment": "The goal of this work is not to pursue the rate of convergence of interpolated-NN (which has been done in Belkin (2018) and Xing (2019). ) Our main theorem provides the EXACT MSE and Regret, rather than rate result up to unknown multiplicative constants. Therefore, it is not a comparable result to Belkin (2018) and Xing (2019), but a sharper improvement. Traditional kNN, interpolated-NN and the OWNN (Samworth, 2012) are all rate optimal, but our result enable us to rigorously compare the three on the multiplicative constant level (as described in Section 3.4).\n \nThe study of multiplicative constant, beyond the rate of convergence, provides more subtle insights. For example, Samworth (2012) claimed that the OWNN is the optimal weight choice by proving its multiplicative constant is the smallest; Locally-weighted NN (Cannings, et al, 2019) calculated the exact Regret to prove that pointwise local weighting scheme is better than a uniform choice of k.  More reference of studies on multiplicative constant is provided in the reference list.\n\nOur work, beyond the results of Belkin (2018) and Xing (2019), characterizes how the performance of interpolated-NN changes with respect to the interpolated level (i.e., gamma) and reveals a \u201cdouble descent\u201d phenomenon in NN algorithm which echoes many recent studies for over-parametrized models. This new insight is the most important message we would like to deliver to the readers, rather than the convergence rate results. Another insight is that a proper interpolation may be viewed as a form of regularization (in terms of slightly reducing bias) that improves the predictive power of optimal kNN. \n \n Reference:\n[1] Cannings, T. I., Berrett, T. B. and Samworth, R. J. (2019) Local nearest neighbour classification with applications to semi-supervised learning. Ann. Statist., to appear.\n[2] Cannings, T. I., Fan, Y. and Samworth, R. J. (2019) Classification with imperfect training labels. Biometrika, to appear.\n[3] Samworth, R. J. (2012) Optimal weighted nearest neighbour classifiers. Ann. Statist., 40, 2733-2763. DOI: 10.1214/12-AOS1049.\n[4] Sun, Will Wei, Xingye Qiao, and Guang Cheng. \"Stabilized nearest neighbor classifier and its statistical properties.\" Journal of the American Statistical Association 111.515 (2016): 1254-1265.\n[5] Duan, Jiexin, Xingye Qiao, and Guang Cheng. \"Distributed Nearest Neighbor Classification.\" arXiv preprint arXiv:1812.05005 (2018).", "title": "To Reviewer 1&3"}, "HJxqBtYijS": {"type": "rebuttal", "replyto": "rJeZ9nU15S", "comment": "The goal of this work is not to pursue the rate of convergence of interpolated-NN (which has been done in Belkin (2018) and Xing (2019). ) Our main theorem provides the EXACT MSE and Regret, rather than rate result up to unknown multiplicative constants. Therefore, it is not a comparable result to Belkin (2018) and Xing (2019), but a sharper improvement. Traditional kNN, interpolated-NN and the OWNN (Samworth, 2012) are all rate optimal, but our result enable us to rigorously compare the three on the multiplicative constant level (as described in Section 3.4).\n \nThe study of multiplicative constant, beyond the rate of convergence, provides more subtle insights. For example, Samworth (2012) claimed that the OWNN is the optimal weight choice by proving its multiplicative constant is the smallest; Locally-weighted NN (Cannings, et al, 2019) calculated the exact Regret to prove that pointwise local weighting scheme is better than a uniform choice of k.  More reference of studies on multiplicative constant is provided in the reference list.\n\nOur work, beyond the results of Belkin (2018) and Xing (2019), characterizes how the performance of interpolated-NN changes with respect to the interpolated level (i.e., gamma) and reveals a \u201cdouble descent\u201d phenomenon in NN algorithm which echoes many recent studies for over-parametrized models. This new insight is the most important message we would like to deliver to the readers, rather than the convergence rate results. Another insight is that a proper interpolation may be viewed as a form of regularization (in terms of slightly reducing bias) that improves the predictive power of optimal kNN. \n \n Reference:\n[1] Cannings, T. I., Berrett, T. B. and Samworth, R. J. (2019) Local nearest neighbour classification with applications to semi-supervised learning. Ann. Statist., to appear.\n[2] Cannings, T. I., Fan, Y. and Samworth, R. J. (2019) Classification with imperfect training labels. Biometrika, to appear.\n[3] Samworth, R. J. (2012) Optimal weighted nearest neighbour classifiers. Ann. Statist., 40, 2733-2763. DOI: 10.1214/12-AOS1049.\n[4] Sun, Will Wei, Xingye Qiao, and Guang Cheng. \"Stabilized nearest neighbor classifier and its statistical properties.\" Journal of the American Statistical Association 111.515 (2016): 1254-1265.\n[5] Duan, Jiexin, Xingye Qiao, and Guang Cheng. \"Distributed Nearest Neighbor Classification.\" arXiv preprint arXiv:1812.05005 (2018)", "title": "To Reviewer 1&3"}, "B1eGR_toiS": {"type": "rebuttal", "replyto": "rJeZ9nU15S", "comment": "Thanks for pointing out our writing problem. \n\nThe theorem 1 in the first submission shall serve as an important intermediate (i.e lemma) result. The corollaries derived from theorem 1 is actually our main point. We adjusted the displays of main theorems in a revision submission to enhance the readability. \n", "title": "To Reviewer 2&3"}, "rkx3udtsiS": {"type": "rebuttal", "replyto": "rJl9DU5MoH", "comment": "Thanks for pointing out our writing problem. \n\nThe theorem 1 in the first submission shall serve as an important intermediate (i.e lemma) result. The corollaries derived from theorem 1 is actually our main point. We adjusted the displays of main theorems in a revision submission to enhance the readability. ", "title": "To Reviewer 2&3"}, "B1eNXtKjiH": {"type": "rebuttal", "replyto": "rklTS2CAKS", "comment": "In Belkin (2018), they technically only obtains a suboptimal bound for classification. And from their theorem, even if it is an optimal rate, it is only sufficient to state that \"interpolated-NN is not hurt by interpolation\". Our work, by proving that interpolated NN yields a smaller multiplicative constant, asserts that \"interpolation helps improve the performance\".", "title": "To Reviewer 1"}, "rJl9DU5MoH": {"type": "review", "replyto": "Ske5UANYDB", "review": "The paper studies theoretical perspective of double descent phenomenon for the interpolated K-NN classifier.\n\nThe paper is works in several interesting directions and gives theoretical reasoning to how interpolated K-NN could exhibit the double descent phenomenon. They give theoretical justifications albeit with strong assumptions.\n\nI think the paper is a good paper. However, I have concerns with the presentation quality of the paper. It is very tough to get through the paper till the end. \n\nIn my view, it would have been an Accept if the paper was well written.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "rklTS2CAKS": {"type": "review", "replyto": "Ske5UANYDB", "review": "This paper is about interpolation schemes in the particular case of the\nNearest Neighbor algorithm. The authors investigate the bene\ft, mainly\ntheoretical, of the proposed interpolation. They study minimax rates of\nthe proposed interpolated-NN for both classi\fcation and regression. The\nstatistical stability of the Interpolated-NN is adressed.\nThe paper is easy to understand and correctly written. Nevertheless,\nIt is a particular case of ( \\Over\ftting or perfect \ftting? Risk bounds for\nclassi\fcation and regression rules that interpolate\", Belkin, M., Hsu, D.,\nand Mitra, P. (2018a)) with an explicit interpolation schemes given by the\neuclidien distance power gamma. It appears as an application of the above\npaper which brings only few theoretical advantages and not enough to justify,\nalthough intuitive, the choice of these weights. Only few discussions and\nno comparison to others bounds (as those in the above paper) of the main\ntheorem are given. The paper is too much incremental from the papers of\nBelkin et al. (2018) and Xing et al. (2018) and the bene\fts of the proposed\ninterpolation are limited. Furthermore, the empirical performance of the\ninterpolate-NN is clearly not convincing and show no signi\fcant practical\nadvantages of the proposed method. As the goal of the paper is clearly\ntheoretical, the 'real data analysis' part is not necessary in my opinion.", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 4}, "rJeZ9nU15S": {"type": "review", "replyto": "Ske5UANYDB", "review": "This paper studies the interpolated k-nearest neighbors algorithm from a theoretical perspective. Specifically, it studies how the performance of the algorithm is affected by reweighting the k nearest neighbors according to their relative distance. This regime has been considered in prior work, particularly Belkin et al. (2018).\n\nUnder various niceness conditions, the paper proves error bounds for interpolated k-nearest neighbors for both regression (i.e. squared loss) and classification (i.e. 0-1 loss after thresholding).\n\nOverall, I have the impression that this paper contains interesting ideas, but the presentation is very poor. It should be revised and resubmitted before it can be accepted.\n\nIn particular, the paper does not make its contribution clear. The main theorem only appears on page 4 and the reader must consult the appendix to see the definition of all the terms that appear in the theorem. I have no idea how to interpret the (complicated) expression in the theorem. The theorem needs to be explained in intuitive terms. More context needs to be given by comparing the main theorem to prior works (which I am not familiar with). \n\n\n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 2}}}