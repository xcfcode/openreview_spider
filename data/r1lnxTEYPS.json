{"paper": {"title": "Detecting Out-of-Distribution Inputs to Deep Generative Models Using Typicality", "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Balaji Lakshminarayanan"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "matsukaw@deshaw.com", "ywteh@google.com", "balajiln@google.com"], "summary": "We propose detecting out-of-distribution inputs to deep generative models via a goodness-of-fit test based on the model entropy.", "abstract": "Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data [Nalisnick et al., 2019; Choi et al., 2019].  We posit that this phenomenon is caused by a mismatch between the model's typical set and its areas of high probability density.  In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed [Bishop, 1994].  To determine whether or not inputs reside in the typical set, we propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods.  The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated.  We report experiments showing that our procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. [2019].", "keywords": ["Deep generative models", "out-of-distribution detection", "safety"]}, "meta": {"decision": "Reject", "comment": "This paper tackles the problem of detecting out of distribution (OoD) samples. To this end, the authors propose a new approach based on typical sets, i.e. sets of samples whose expected log likelihood approximate the model's entropy. The idea is then to rely on statistical testing using the empirical distribution of model likelihoods in order to determine whether samples lie in the typical set of the considered model. Experiments are provided where the proposed approach show competitive performance on MNIST and natural image tasks.\n\nThis work has major drawbacks: novelty, theoretical soundness, and robustness in settings with model misspecification. Using the typicality notion has already been explored in Choi. et al. 2019 (for flow-based model), which dampers the novelty of this work. The conditions under which the typicality notion can be used are also not clear, e.g. in the small data regime. Finally, the current experiments are lacking a characterization of robustness to model misspecification. Given these limitations, I recommend to reject this paper.\n"}, "review": {"HJep8WdqtH": {"type": "review", "replyto": "r1lnxTEYPS", "review": "I've read the authors' rebuttal and other reviews; I'd like to keep my score as is. My main concerns are the novelty of the work, the theoretical soundness of the method for small data settings and its robustness in settings with model misspecification. \n\n#################################\n\nThe paper proposes a new approach based on the notion of typical set in probability and tackles the challenging problem of detecting OOD using deep generative models. The main claim of the paper is that assigning high likelihood to OOD samples in DGMs is due to the mismatch between model\u2019s typical set and its high probability density areas. \n\nI liked the idea of proposing a hypothesis testing approach for finding OOD samples generated from a model; however, my main concern is that the approach has some major practical limitation that the authors have also rightly mentioned in their discussion. It seems that even with a hypothesis testing tool for OOD detection, the model capacity and other properties of the model are more fundamental and critical for OOD detection in DGMs. In other words, how this tool can be useful in practice if the models are misspecified and how robust is the tool with respect to model properties. This major limitation has not been addressed in the experiments. \n\nThis paper, does a good job in finding the OOD data points if the likelihood histograms do not overlap using the typicality notion. However, this idea had already been proposed and explored in Choi. et al. 2019 (although for a flow-based model). This makes the technical novelty of the work less significant. \n\nOverall, I think the paper needs some improvement in terms of discussing the robustness of the test with respect to model properties; otherwise, it is just another typicality set explanation of why DGMs may produce high likelihood values for OOD samples which has already been mentioned in previous work. ", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}, "B1gx24IkqS": {"type": "review", "replyto": "r1lnxTEYPS", "review": "This paper is concerned with how to determine whether a set of data points are from a given distribution. It uses the so-called typical set to transform the problem to determining whether the data points lie in the typical set of the given distribution. It proposes a statistical test using the empirical distribution of model likelihoods to determine whether inputs lie in the typical set if the considered model. \n\nThe motivation of the work is very clear, and the paper is well organized. The basic idea of using the typical set to check whether given data points are from a given distribution seems sensible, as guaranteed by Theorem 2.1.\n\nMy concern is about the performance of the proposed method compared to alternatives. First, a standard approach to the considered problems seems to be the two-sample tests (or its approximations or variations), so it would be desirable to compare the typical set-based approach with the two-sample test approaches theoretically. In particular, given that you have to allow some error when using typical sets, what is exactly the advantage of the proposed approach?  Second, according to the empirical results (Section 5), the proposed method does not seem to clearly outperform alternatives such as KS-test. In this case, a better justification of the reliability of the proposed approach would be helpful.\n\nI acknowledge I read the authors' response and other reviews and would like to keep my original rating. (I agree that the t-Test and KS-test were probably first used by the authors, but at the same time it is natural to adopt them; that is why I considered them as baselines.)", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "rJe18aQpKH": {"type": "review", "replyto": "r1lnxTEYPS", "review": "Thanks for the authors for your detailed reviews.\n\nMy major concerns about the proposed method are whether \"the typicality set\" could be faithfully applied in the small data regime. The authors point me to the interesting Figure 4, which shows that it basically achieves converged performance when $m = 50$ or smaller numbers for some problems. I think this experiment is a strong support for the proposed method. \n\nHowever, I don't agree that the M=1 Gaussian case acts as a strong support for the method. As I said, for some other wired distribution, it is difficult to interpret what the M=1 Typicality set becomes. \n\nThe authors also clarify the difference between different baselines. \n\nOverall, I will increase my score to \"Weak Accept\".\n\n##########################\n\nRecent works have shown that out-of-distribution samples can have higher likelihoods than in-distribution samples for some generative models. To explain this phenomenon and to tackle the problem for OOD detection, this paper adopts \"typical sets\" for identifying in-distribution samples. Specifically, a \"typical set\" is a set of examples whose expected log likelihood approximate the model's entropy. For a Gaussian distribution, the paper finds that a single point typical set locates exactly in the \\sqrt{d} radius, which is usually favored over the high-likelihood origin. Then the paper uses the \"typical set\" to perform OOD for a batch of examples. Empirically they demonstrate competitive performance over MNIST and natural image tasks. \n\nTypical set seems natural for out-of-distribution detection. An important property is that, if one draws a large number of independent samples from the distribution, it is very likely that these samples belong to the typical set (basically Theorem 2.1).  However,  for small n, this property doesn't hold anymore, which leaves here a questionmark whether \"Typical set\" can be used for OOD detection in small n regime. As the author argues, for Gaussian distribution when n=1 the typical locations are those \\sqrt{d} radius points. But this doesn't justify the \"Typical set\". If the distribution is some non-Gaussian wired distribution, the typical locations doesn't seem to make sense at all.\n\nFollowing the previous argument above, the Typical set method requires to perform OOD for a batch of examples. In contrast, the Annulus method can be directly applied to one single test example. \n\nEmpirically, the Typically set doesn't demonstrate obvious advantages compared to the baselines. For both MNIST and natural image tasks, it seems that all methods behave similarly. For comparing such big tables, I would recommend adding a column showing the average ranks among all methods. Beyond that, standard OOD tasks usually evaluate methods using AUROC and AUPR (Hendrycks and Gimpel, 2017). Is it possible to also include such metrics ?\n\nTheorem 2.1 is confusing. It is beneficial to define what P is, and verbally state what the theorem conveys. ", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}, "HkegdfbwoB": {"type": "rebuttal", "replyto": "B1gx24IkqS", "comment": "Thank you for your feedback, R3.  We are glad that you found the work\u2019s motivation \u201cvery clear\u201d and the paper \u201cwell organized.\u201d  Hopefully we can dispel your reservations below.\n\n1. Performance in Comparison to Baselines:  Firstly, let us clarify that the only relevant baseline from the goodness-of-fit-testing literature is the Kernelized Stein Discrepancy (KSD).  As stated in Sec 3.1, we are aware of no other GoF test that can be widely applied across all types of deep generative models.  As for our test vs KSD, they perform roughly the same in all cases except for the PixelCNN trained on FashionMNIST.  KSD is unable to detect MNIST as OOD, and our test is unable to detect NotMNIST as OOD.  Yet an additional factor that differentiates the two is runtime: KSD is drastically slower, requiring an $\\mathcal{O}(dM^{2})$ evaluation time (with M representing the batch size) in addition to the cost of computing derivatives through the model.  Our method is $\\mathcal{O}(M)$ after the likelihoods have been computed from the model.  With runtime considered, the results clearly favor our method.\nSecondly, the t-Test and KS-test baselines are not really \u2018competitors\u2019 as they are (i) proposed by us and (ii) closely based on our typicality test.  The t-test is just the typicality test with $\\epsilon = 0$, and the KS-test is comparing all moments, whereas our typicality test is comparing just the first.  Thus, these tests should perform comparably!  The fact that the KS-test and ours perform so similarly can be seen as positive evidence for our method since it validates that the first moment (i.e. entropy) is truly the critical one for testing OOD.  \nThirdly, the Maximum Mean Discrepancy (MMD) baseline is not a `'valid' competitor as it is performing a two-sample test, not a GoF test.  We provide MMD only as a reference point to see how testing against $p^{*}$ compares to testing against $p_{\\theta}$.  Lastly, the annulus method [Choi et al., 2019] is simply what our test reduces to in the special case of the $(\\epsilon, M=1)$-typical set for isotropic Gaussians (which we state on p 6).\n\n2.  \u201cA standard approach to the considered problems seems to be the two-sample tests\u201d: This is incorrect.  Two-sample tests assume the setting $q$ vs $p^{*}$, with $p^{*}$ being the inaccessible underlying generative process.  Rather, we are testing  $q$ vs $p_{\\theta}$, with $p_{\\theta}$ being a model to which we have access.  Yet we do report an MMD baseline in the experiments in anticipation of readers wondering how a two-sample test would perform in the same setting.  We find it to perform comparably except in the same setting mentioned for KSD above: for the PixelCNN trained on FashionMNIST, MMD is unable to detect MNIST as OOD, and our test is unable to detect NotMNIST as OOD.  Note that this performance was only able to be achieved when we derived the MMD kernel from the generative model.  Otherwise, performance was strictly worse than our test.   Moreover, MMD is much more expensive to compute as it is $\\mathcal{O}(MNd)$ (as mentioned on p 6).    \n\n3.  \u201cGiven that you have to allow some error when using typical sets, what is exactly the advantage of the proposed approach?\u201d:  We are not sure what you mean exactly by \u201callow for some error.\u201d  But to summarize our contributions once more: (1) we propose typicality as an explanation for the OOD phenomenon in deep generative models, (2) derive a GoF test widely applicable across all deep generative model classes and with better runtime than KSD, (3) show that this test, despite it using only the first moment of the empirical likelihoods (which makes runtime cheap), is able to detect the OOD set in many of the cases reported by [Nalisnick et al., 2019].    \n\nAgain, thanks for taking the time to read our paper.  We look forward to further discussion.", "title": "Response to R3"}, "HkgW0N-woB": {"type": "rebuttal", "replyto": "rJe18aQpKH", "comment": "Thank you for taking the time to read our paper, R1.  We are glad that you found our approach \u201cnatural for out-of-distribution detection.\u201d  Please consider the following rebuttals to your critiques.\n\n1. Small $M$ Behavior:  You are correct in that Thm 2.1 only holds for sufficiently large $M$, and therefore it is not obvious that a typicality-based test behaves correctly in the small-$M$ regime.  Firstly, we emphasize that GoF testing for deep generative models is still a completely open problem even in the large-M regime.  As KSD is the primary competitor and scales as $\\mathcal{O}(M^{2})$, even a procedure that performs well for large M is a contribution.  And as can be seen in Figure 4, our test works near perfectly for $M>50$, and this prompted us to focus the main experimental section on the $M \\in \\{2, 10, 25 \\}$ regime.  \nNow returning to the main point, we have two pieces of evidence for the test\u2019s correctness for small $M$.  The first is theoretical: examining the Gaussian case, the $(\\epsilon=0, M=1)$-typical set is $\\mathcal{A}_{0}^{1}[N(x;\\mu, \\sigma)] = \\{x \\mid || x - \\mu ||_{2} = \\sigma \\sqrt{d} \\}$, the shell of radius $\\sigma \\sqrt{d}$.  This is quite a narrow region of the support, which speaks to the inapplicability of Thm 2.1.  Yet as we are given only one test instance, such a restriction seems to be the most appropriate choice.  It is clearly better than picking the mode, for instance.  Our second piece of evidence is experimental: Our test performs reasonably well in the $M=10$ regime and in cases can achieve near perfect OOD classification for $M=2$.  In the case of Glow trained on SVHN, 98% of CIFAR-10 batches and 100% of ImageNet batches were correctly classified at $M=2$.  Hence, our test must be checking sensible regions of data space or such good results could not be possible.  In turn, we find the claim that \u201cthe typical locations doesn't seem to make sense at all [for small $M$]\u201d completely speculative.  Can you please provide some reasoning in support of your claim?   \n\n2.  Performance in Comparison to Baselines:  Firstly, let us clarify that the only relevant baseline from the goodness-of-fit-testing literature is the Kernelized Stein Discrepancy (KSD).  As stated in Sec 3.1, we are aware of no other GoF test that can be widely applied across all types of deep generative models.  As for our test vs KSD, they perform roughly the same in all cases except for the PixelCNN trained on FashionMNIST.  KSD is unable to detect MNIST as OOD, and our test is unable to detect NotMNIST as OOD.  Yet an additional factor that differentiates the two is runtime: KSD is drastically slower, requiring an $\\mathcal{O}(dM^{2})$ evaluation time (with M representing the batch size) in addition to the cost of computing derivatives through the model.  Our method is $\\mathcal{O}(M)$ after the likelihoods have been computed from the model.  With runtime considered, the results clearly favor our method.\nSecondly, the t-Test and KS-test baselines are not really \u2018competitors\u2019 as they are (i) proposed by us and (ii) closely based on our typicality test.  The t-test is just the typicality test with $\\epsilon = 0$, and the KS-test is comparing all moments, whereas our typicality test is comparing just the first.  Thus, these tests should perform comparably!  The fact that the KS-test and ours perform so similarly can be seen as positive evidence for our method since it validates that the first moment (i.e. entropy) is truly the critical one for testing OOD.  \nThirdly, the Maximum Mean Discrepancy (MMD) baseline is not a `valid\u2019 competitor as it is performing a two-sample test, not a GoF test, which is the topic of our paper.  We provide MMD only as a reference point to see how testing against $p^{*}$ compares to testing against $p_{\\theta}$.  Lastly, the annulus method [Choi et al., 2019] is simply what our test reduces to in the special case of the $(\\epsilon, M=1)$-typical set for isotropic Gaussians (which we state on p 6).\n\n3.  \u201cThe Typical set method requires to perform OOD for a batch of examples. In contrast, the Annulus method can be directly applied to one single test example.\u201d:  Pitting the annulus method vs our typicality test is a false dichotomy.  The annulus method (as mentioned above) is a special case of our test, and the only reason it can be used for one-sample is that it\u2019s assuming the $(\\epsilon, M=1)$-typical set.   \n\n4.  AUROC metrics:  As far as we are aware, AUROC metrics in the literature are computed for point-wise rejection rules.  Thus they would not be comparable to our hypothesis testing / batch-wise methodology, which we believe to be the first of its kind for deep generative models.  Yet, we thank the reviewer for the suggestion, and we will look into adding ROC-based metrics to our revised draft.  \n\n5.  Thm 2.1:  Thank you for the feedback.  We will add more discussion of the intuition.\n\nAgain, thanks for sharing your thoughts.  We look forward to further discussion.", "title": "Response to R1"}, "SyxuDmbwsS": {"type": "rebuttal", "replyto": "HJep8WdqtH", "comment": "Thank you for your comments, R2.  We are glad to hear that you \u201cliked the idea of proposing a hypothesis testing approach\u201d and appreciate that the method \u201cdoes a good job in finding the OOD data points.\u201d  We hope to address your doubts below.\n\n1. Novelty w.r.t. Annulus Method:  It is incorrect to state that Choi et al. [2019] have previously \u201cexplored\u201d a typicality-based solution.  Their annulus method is what our test reduces to in the *special case* of the $(\\epsilon, M=1)$-typical set for isotropic Gaussians (which we state on p 6).  Choi et al. make no mention of the general entropy-based definition of typicality (our Def 2.1).  Nor do they give any methodology for testing the $(\\epsilon, M>1)$-typical set in Gaussians, let alone any other class of deep generative model (eg PixelCNN, VAEs).   Previous to our work, the question of how to test for typicality *in every class of deep generative model except Gaussian flows* was completely wide open and unaddressed.  \n\n2. Behavior Under Misspecified Models:  While model misspecification is certainly a concern when building generative models, the topic of our paper is not model building.  Rather, we focus on testing a given, pre-trained generative model.  Or to be precise, we are testing $q$, some unknown distribution we observe only through samples, vs $p_{\\theta}$, the pre-trained model.  Model misspecification tests $p^{*}$, the distribution of the training data, vs $p_{\\theta}$ (or $p_{\\theta}$ vs $p_{\\theta\u2019}$).  Clearly these are much different settings, making an analysis of misspecification well out-of-scope for our paper.\n\nAgain, thank you for taking the time to read our draft.  We look forward to further discussion of these points.", "title": "Response to R2"}, "S1eMVHqnYS": {"type": "rebuttal", "replyto": "BkgdQXF3tS", "comment": "Thanks for the clarification.  I think we understand your point now (but please correct us if not).\n\nAs for at what value of $M$ does the AEP kick-in, it must do so at $M > 50$ since we observe that type-I and type-II error are essentially nonexistent in nearly all cases tested.  For $M \\le 50$, it\u2019s hard to say.  Please let us know if you have any ideas about how to directly quantify the transition.\n\nYet you do make a good point about Thm 2.1 in the small $M$, small $\\epsilon$ regime.  Theorem 2.1 indeed does not apply in this regime, and the sentence that you quoted has the potential of being interpreted too generally.  Yet we believe it\u2019s important to emphasize (for other readers, at least) that just because Thm 2.1 doesn\u2019t apply here, that doesn\u2019t mean that the procedure is ill-behaved.  Returning to the isotropic Normal example once more, the $(\\epsilon=0, M=1)$-typical set is $\\mathcal{A}_{0}^{1}[N(x;\\mu, \\sigma)] = \\{x \\mid || x - \\mu ||_{2} = \\sigma \\sqrt{d} \\}$, the shell of radius $\\sigma \\sqrt{d}$.  Clearly this set is a poor summary of what we expect to be generated by $N(x;\\mu, \\sigma)$, which speaks to the inapplicability of Thm 2.1.  But since we are testing only one sample ($M=1$), it makes sense that our procedure would become conservative, only letting the test pass if $\\tilde{x}$ falls in a very restricted region.  We will incorporate this discussion into the paper during the next revision.  Thanks again for the discussion. ", "title": "Thanks for the Clarification"}, "Bye1z-83Fr": {"type": "rebuttal", "replyto": "rJglnNe2FH", "comment": "Thanks for continuing the discussion, Shengyu.  We agree that the AEP cannot be used when $M$ is small.  However, we are not sure of exactly what you're asking in your last statement.  Are you referring to Thm 2.1: $P(\\mathcal{A}^{N}_{\\epsilon}[p(x)]) > 1 - \\epsilon$ (with $N$ now serving for $M$)?  ", "title": "Requesting Clarification"}, "BJg0bvGoKH": {"type": "rebuttal", "replyto": "HkgtMunUFB", "comment": "Hi Shengyu, \n\nThanks for taking the time to read our paper and share your thoughts.  It\u2019s much appreciated.\n\nI\u2019m not sure that we follow your line of reasoning about the small-$M$ regime.  Let\u2019s return to the $M=1$ Gaussian example but now with d=1 (with $d$ denoting dimensionality) so that there\u2019s no dimension-wise factorization to consider.  In this case the test would simplify to: $ \\mid \\sigma^{2} - (x - \\mu)^{2} \\mid \\le 2 \\sigma^{2} \\epsilon$.  Intuitively this expression is testing if $x$\u2019s distance from the mean is roughly $\\sigma$, with the bound relaxing as a function of $\\sigma$.  This seems like reasonable behavior to us.  One interesting stress test to consider is the case of $x \\approx \\mu$, which could easily happen in the $d=1$ case.  We\u2019d then have to set $\\epsilon \\ge \\frac{1}{2}$ for the bound to hold.  Yet still $\\epsilon = 1/2$ does not seem like an unreasonably high setting for $\\epsilon$ that would signal that our test is problematic.\n\nIn general, we don\u2019t agree that \u201cit is quite strange to consider only one or few samples when talking about typicality.\u201d  You seem to imply that $\\epsilon$ must be set large, but this is not the case.  Rather, it\u2019s just that the criterion for what is a \u2018typical\u2019 batch of samples becomes more stringent.  To see this, consider the general bound $ \\mid \\frac{1}{M} \\sum_{m=1}^{M} -\\log p(x_{m}) - \\mathbb{H}[p]  \\mid \\le \\epsilon$ (Equation 3).  For small $M$, this means that those few samples must approximate the entropy very well for the test to pass.  In the Gaussian example (for general $d$), it must be that all $x_{m}$\u2019s fall very close to the annulus radius.  For large $M$, a few samples can be far from the annulus just so long as the mean log density is still close to the entropy---hence the small-$M$ case being \u2018more stringent\u2019 as there is more contribution from any given sample.  \n\nAs for your comment on the proof, we appreciate you taking the time to read even the appendix.  However, we don\u2019t quite agree that the subset condition is equivalent to saying the distributions $p$ and $q$ have the same entropy.  Consider the case $p=N(\\mu=-1000, \\sigma=1)$ vs $q=N(\\mu=1000, \\sigma=1)$.  These distributions have the same entropy, but clearly $\\mid \\mathbb{E}_{q}[ -\\log p(x)] - \\mathbb{H}[p]  \\mid >> 0$.\n\nLastly, we do indeed have results for $M > 25$.  Please see Appendix E.3, Figure 4 for results showing when $M$ is as large as $150$.  Our procedure achieves near perfect discrimination of the OOD set on almost all of the dataset pairs (which, of course, is due to the AEP kicking in, as you mention). Since our method achieved excellent OOD performance for high-values of $M$, we thought the more interesting and practically relevant experimental question was to understand how OOD detection changes as $M$ is decreased.  That\u2019s why we focused on $M \\le 25$ in the main experimental section.  We will update the text to clarify this.   ", "title": "Author Response"}}}