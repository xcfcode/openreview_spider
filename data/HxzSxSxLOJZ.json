{"paper": {"title": "ResNet After All: Neural ODEs and Their Numerical Solution", "authors": ["Katharina Ott", "Prateek Katiyar", "Philipp Hennig", "Michael Tiemann"], "authorids": ["~Katharina_Ott1", "prateek.katiyar@de.bosch.com", "~Philipp_Hennig1", "~Michael_Tiemann1"], "summary": "We explain why some Neural ODE models do not permit a continuous-depth interpretation after training and how to fix it.", "abstract": "A key appeal of the recently proposed Neural Ordinary Differential Equation (ODE) framework is that it seems to provide a continuous-time extension of discrete residual neural networks. \nAs we show herein, though, trained Neural ODE models actually depend on the specific numerical method used during training.\nIf the trained model is supposed to be a flow generated from an ODE, it should be possible to choose another numerical solver with equal or smaller numerical error without loss of performance.\nWe observe that if training relies on a solver with overly coarse discretization, then testing with another solver of equal or smaller numerical error results in a sharp drop in accuracy. \nIn such cases, the combination of vector field and numerical method cannot be interpreted as a flow generated from an ODE, which arguably poses a fatal breakdown of the Neural ODE concept.\nWe observe, however, that there exists a critical step size beyond which the training yields a valid ODE vector field. \nWe propose a method that monitors the behavior of the ODE solver during training to adapt its step size, aiming to ensure a valid ODE without unnecessarily increasing computational cost.\nWe verify this adaption algorithm on a common bench mark dataset as well as a synthetic dataset. \n", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "The paper considers whether Neural ODEs have a valid interpretation as an ODE, showing that such an interpretation is not correct unless the discretization is chosen properly.  This is important, given interest in Neural ODEs as models as well as they way they will be used, both for problems involving physical/temporal data as well as more generally.  The paper proposes an algorithm for adapting integration step-size during training to partially address this issue, and empirical results are shown.  There was a detailed discussion between reviewers and authors which led to improvements.  The authors should also discuss the relationship of their work with https://arxiv.org/abs/2008.02389, which makes a similar point, in the final version."}, "review": {"7Gca-Le9ank": {"type": "review", "replyto": "HxzSxSxLOJZ", "review": "- Paper makes a good contribution by pointing an intrinsic flaw in the NeuralODE technique. The problem is that even with an  error accruing step size, the results of a NerualODE can be good, leading to a false belief that the ODE used in the construct represents the phenomena, but instead it is the dynamic behaviour arising from the mixture of the ODE and the solver that separates the classes well. \n- However, the proposed solution does not seem convincing. It seems like a work in progress. The solution is proposed in algorithm 2,4,6 which should have been algorithm 1,2,3. \n- It is not made clear how solvers would be able to use this algorithm.\n-The  algorithm is not nicely constructed.  Putting a function like \"calculate accuracy higher order solver();\" in an algorithm without fully describing what it does , is not advised. \n- Figures are not illustrative, there is too much clutter. I believe a point could be made with the same amount of figures but with less clutter.\n- Based upon the contribution made by the authors, it seems appropriate that their results are published right now. \n\n\n\n", "title": "Good contribution but algorithm is not sound", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "D184PiAkMJQ": {"type": "review", "replyto": "HxzSxSxLOJZ", "review": "**Summary** \nThe authors show that Neural ODEs exploit the ODE-solver used for training to realize a dynamical system that violates the ODE vector field property of non-overlapping trajectories. The authors conclude that NODEs are not real ODEs, hence the paper's title \"ResNet after all.\". To avoid such behavior, the authors propose to monitor the accuracy metrics using a finer ODE solver and decrease the solver's step size if a discrepancy between the two different stepsize accuracies is observed.\n\n**comments** \nWhile this paper's claims and experiments are relatively narrowly focused, the overall conclusion and proposed solution are clear.\nHowever, I see a fundamental issue with the assumption of fixed stepsize solvers. The main reason why using adaptive stepsize solvers is to avoid such a problem of choosing the right stepsize. Consequently, I expect the described problem to be more elegantly\nsolved using a lower relative tolerance value of a dynamic stepsize solver.\n\nIn algorithm 2, there is a variable called test_acc. The term \"test_acc\" is overloaded in this context, and it can refer to the test-set accuracy or the training accuracy under a higher-order ODE solver.  If the authors refer to the training accuracy under a higher-order ODE solver (which I assume), please change the variable's name. ", "title": "An Investigation of the discrete dynamics of Neural ODEs", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "y7DGdAve4-Z": {"type": "rebuttal", "replyto": "PRCDuDHAZO", "comment": "Unfortunately, the revision period is ending soon. Reviewer 2, are there any last concerns/questions that you want us to address? We might not be able to improve the manuscript further until revision deadline, but we could try to incorporate them into a potential camera-ready version.\n\nFurthermore, have we addressed your concerns adequately to maybe improve your judgement of our paper?", "title": "End of revision period fast approaching"}, "VlNf4akI95u": {"type": "review", "replyto": "HxzSxSxLOJZ", "review": "This paper empirically studies whether Neural ODEs have a valid ODE interpretation. The authors show that a Neural ODE model does not necessarily represent a continuous dynamical system if the discretization of the numerical method is too coarse. Indeed, this is a widely overlooked issue that has been largely ignored in the Neural ODE community. To address this issue, the authors propose a novel adaptive step size scheme. \n\nReasons for my score: Overall, I vote for marginally above acceptance threshold. The presented ideas and results are very interesting and relevant for the community. It is important to better understand if and when a Neural ODE has a valid ODE interpretation. I am willing to increase my score if the authors can address my concerns during the rebuttal period. \n\nPros: \n-------\n+ The work addresses a crucial aspect of Neural ODEs that is particularly important in the context of scientific and robotics applications. That is, because here one is often not only interested in the predictive accuracy of the model, but also whether the model has a valid ODE interpretation. \n\n+ The authors show several illustrating examples that demonstrate how the Neural ODE is affected by the specific solver configuration used for training.\n\n+ The adaptive step size scheme is simple yet effective. The experiments clearly demonstrate the advantage of this scheme. \n\n\nCons: \n-------\n\n- It would be good if you apply your adaptive step size scheme to an actual scientific problem, where it actually matters that the ODE has a valid ODE interpretation. It is not intuitive why an ODE interpretation is relevant for computer vision tasks. \n\n- An extended set of results in Section 3.1 would help to better understand the performance of the proposed algorithm. For instance, how does the results change if you use RK4 for testing, instead of midpoint (this should only be marginally more expensive). Also, what happens if you train with midpoint and then use RK4 for testing. Next, can you also add results for MNIST or FMNIST in Table 1 in order provide an additional set of experiments.\n\n- The Neural ODE block that you consider is very shallow. I assume, that it should be possible to achieve about 75% accuracy on CIFAR10 using a state of the art Neural ODE block. \n\n- The Appendix is poorly formatted. Typically, I would expect that Figures are embedded into a descriptive text. It would be nice if you provide at least some discussion for why you provide these Figures and what we can learn from it (in addition to the captions). Also, it is not clear to me why you are presenting a `'preliminary tolerance adpation algorithm'. Are you proposing to use this algorithm, or is this an idea for a future work that still needs to be tested and improved? Typically, I would not expect to see any preliminary results in a conference paper.  \n\n- The authors miss to discuss how their works relates to some recent theoretical results [1,2,3].\n\n- The title of the paper seems not fitting, i.e., what do mean by `ResNet after all'? You do not discuss ResNets in much detail in this paper.\n\n- Please provide code in order to reproduce the results. \n\nMinor comments:\n-------\n\n* Some parts of the paper are unclear. For instance, the authors do not establish the relationship between step size and number of steps. This should be discussed somewhere below Eq. (3). On page 7 you say `that 'after a pre defined number of steps (we chose k= 50)'. I assume here you refer to the number of iterations?\n\n* Some of the Figures are crowded and difficult to parse. For instance, there is much going on in Figure 4. First, it would help if you increase the width of the plots (there is lot's of white space on the left and right of the figure). Further, it would help if you reduce the content slightly. Finally, a legend would be very much appreciated. \n\n* I think, it sounds better to use 'adaptation' instead of 'adaption'.\n\n\nReferences\n-----\n\n[1] Bo, Lijun, Agostino Capponi, and Huafu Liao. \"Deep Residual Learning via Large Sample Mean-Field Optimization: Relaxed Control and Gamma-Convergence.\" arXiv:1906.08894 (2019). \n[2] Thorpe, Matthew, and Yves van Gennip. \"Deep limits of residual neural networks.\" arXiv preprint arXiv:1810.11741 (2018).\n[3] W. E, J. Han, and Q. Li, \"A mean-field optimal control formulation of deep learning,\" Research in the Mathematical Sciences, vol. 6, no. 1, p. 10, 2019.", "title": "Review for Neural ODEs and Their Numerical Solution", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "lwcCjq1FWB": {"type": "rebuttal", "replyto": "HxzSxSxLOJZ", "comment": "As suggested by Reviewer 4 we have added additional experiments to our paper. These experiments are:\n\n- Experiments on MNIST: Fixed step solver experiments, adaptive step size solver experiments, step adaptation algorithm\nwith Euler as train solver and Midpoint as test solver.\n- Step adaptation algorithm with Euler as train solver and rk4 as test solver on CIFAR10 and Sphere2.\n- Step adaptation algorithm with Midpoint as train solver and rk4 as test solver on CIFAR10 and Sphere2.\n\nThe experimental results can be found in the Supplementary Material Section B.", "title": "Revision 3"}, "lmW1kIw3qoa": {"type": "rebuttal", "replyto": "PRCDuDHAZO", "comment": "Thank you for your interest in the problem and all the interesting questions.\n\nWe have added a new section to the appendix including new analysis plots and a new plot for Lady W.s fan.\n\nBoth trajectory crossing and Lady W.'s fan contribute to the global error.\nHeuristically, there exists a step size where the sensitivity of the global error \nis smaller than the sensitivity of the downstream layer.\nTherefore, there should exist an h* such that for all h<h* the accuracy no longer changes.\nWe discuss this at the end of Section 2.2 but maybe to emphasize this point more.\nOverall, we are unaware of any theoretical statement supporting our heuristic statements.\nThe difficulty we see is that one has to not only look at the theory of an individual IVP but of \nseveral IVPs. \nWe have looked into this and are not aware of any theoretical work in this area.\nOverall there are many subtle effects and not every model violation shows in the test accuracy.\nFor example if we have inter-class crossings between trajectories, this does not change the \ntest accuracy.\n\n\nDoes this answer some of your questions? Are there any specific points you would like us to clarify?\nAdditionally, are there any effects we should provide more detail on in our paper?\n", "title": "Re: Clarifying reviewer 2 theoretical and empirical questions"}, "Vqm1VAlfnj5": {"type": "rebuttal", "replyto": "HxzSxSxLOJZ", "comment": "We have incorporated further improvements and uploaded a new draft of our paper.\n\nThese improvements include:\n- We improved the layout of the figures by making them wider and adding\nlegends as suggested by Reviewer and Reviewer 4 and Reviewer 3. \nAdditionally, we reduced the amount of data shown in each figure to make them clearer. \n- We added a descriptive text to the Supplementary Material and improved the\noverall layout.\n- We added a new section to the appendix in answer to Question 2 of Reviewer 2.\n- We have added the suggested additional reference.\n- Changed the title to: ResNet After All? Neural ODEs and Their Numerical Solution\n\nWe have started experiments, as suggested by Reviewer 4, and hope to \nadd additional experimental results soon.\n", "title": "Revision 2"}, "-5eKHv2W0cC": {"type": "rebuttal", "replyto": "7HZcoXu7oqy", "comment": "Thank you for your interest in our paper. Indeed the work you mention is related to our work, specifically the \nresults in Table 7. We will add [1] to our discussion, thank you for the suggestion. \n\nThe reason our model achieves much lower accuracy than [1] is due to the simple architecture of our model.\nFor example the model in [1] consists of multiple ODE blocks, whereas our model consists of only a single ODE block.\n \n\n[1] Zhuang, Juntang, et al. \"Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE.\" \narXiv preprint arXiv:2006.02493 (2020).", "title": "Re: Juntang Zhuang"}, "-H4t6-rVDxl": {"type": "rebuttal", "replyto": "HxzSxSxLOJZ", "comment": "We have incorporated first improvements and uploaded a new draft of our paper.\n\nThese improvements include:\n\n- We worked on improving the clarity of the text in Section 2 specifically with regard\nto Lady W.'s fan. We have also added ticks and tick labels to the axis of Figure 2. \nReviewer 2 does this help you in regard to your Question 3? Additionally, we will try\nto improve this Section further by introducing additional figures.\n- We have improved the algorithm by using more precise notation\n and we hope this makes it clearer how the proposed algorithm can\nbe used in practice as suggested by Reviewer 3 and Reviewer 1. \n- We have added the references suggested by Reviewer 3 to our related work Section.\n- We fixed all the typos spotted by Reviewer 2, included the minor improvements suggested \nby Reviewer 4 and fixed the algorithm numbering as noted by Reviewer 3.\n- We changed the caption of Figure 1, as suggested by Reviewer 3.\n\nWe are looking forward to your feedback, and we will continue working on the improvements \nwe promised (specifically improving the clarity of the figures, improving the Appendix, adding an\nexample to answer Reviewer 2's Question 2 ).", "title": "Revision 1"}, "-o2M2cXCgSR": {"type": "rebuttal", "replyto": "D184PiAkMJQ", "comment": "We thank the reviewer for their comments.\n\n1. **Adaptive step methods solve the problem:** We would like to thank the reviewer for this comment. We would like to point out that our work considers both adaptive and fixed step solvers (see Figure 4 (c) and (d) where adaptive step size methods where used for training the model.) Adaptive step size methods do not solve the problem described in the paper. Adaptive step size methods control *the local error* with a control signal typically specifying a desired number of significant digits. In this setting, the user has already done two things: run a preliminary analysis how many digits are required to likely get a qualitatively correct numerical solution and b) understood how much accuracy is required in the solution. In Neural ODEs, the required numerical accuracy depends on the downstream layers that change per gradient step. That is, the global error prerequisite of the numerical solver potentially changes with every gradient step. While there always exists a low enough tolerance such that the adaptation issue does not occur, this low enough tolerance may be prohibitively small in practice and is certainly leaving runtime efficiency on the table. We will try to make this point clearer in the paper.\n2. We thank the reviewer for this suggestions and we will change the variable name to improve clarity.", "title": "Re: Reviewer 1"}, "r40Ni7g-w2a": {"type": "rebuttal", "replyto": "7Gca-Le9ank", "comment": "We thank the reviewer for their comments.\n\n1. **Maturity of proposed solution** We agree that the proposed solution is a first simple heuristic to approach the problem. It may not provide overwhelming success, but it does solve the problem. We hope that this helps the community to understand what the intrinsic flaw is all about and that it is solvable in general, thus leading to further research on this problem.\n\n   We would like to understand in more detail why you did not find the algorithm convincing such that we can improve our work or add some discussion. We will fix the latex bug concerning the numbering of the algorithm - thank you for spotting this!\n\n2. We will adapt the algorithm such that it represents the full training loop.\nWe would like to point out that the presented algorithm is not part of the solver but outside of\nthe solver and can thus be applied to any solver.\nWe would like to ask the reviewer to point out where we should provide additional clarification.\n\n3. The presentation of the adaption algorithm can indeed be improved. We will add additional details on the `calculate_accuracy_higher_order_solver()` function in the text and/or the appendix. We will also work on improving the overall clarity of the algorithm.\n\n4. We promise to improve the visualization on the experiments. We will do this by incorporating the suggestions by reviewer 4 minor comment 2. We are open to concrete suggestions on how to reduce the clutter in our figures.\n\nWe hope to have a first revision including the above changes by the weekend.\nWe are currently running additional experiments and we will only update the figures\nafter we have collected all results. This revision will take a bit longer.", "title": "Re: Reviewer 3"}, "FZIll4EaW5M": {"type": "rebuttal", "replyto": "VlNf4akI95u", "comment": "We thank the reviewer for their helpful comments.\n\n1. **Application to scientific problem:** We agree that it is currently unclear when a valid ODE interpretation would be required and that it might not be relevant to computer vision tasks yet. We believe that the Neural ODE model class---as opposed to ResNets---is only beginning to develop its full potential. For instance, [1] discusses that Neural ODEs as a model class might improve robustness. Therefore, we believe our work to be relevant beyond the immediate justification.\n\n That being said, we can try to run experiments, if you have a concrete application in mind?\n\n2. **Extended results:** The suggestions by the reviewer for extending the experiments with the step adaption algorithm are interesting. We agree that the step adaption algorithm should also work for the proposed cases.If time permits we will run additional experiments with the step adaption algorithm. We will start additional experiments for mnist and add them to the paper as soon as they are finished. We cannot promise these results until the end of the rebuttal but we hope that everything finishes in time.\n3. Please see  our answer to Reviewer 2 point 5.. Additionally, to improve the performance of a \"single\"  ODE-block one could add additional augmented dimensions [2] (with the same number of parameters they achieve 60.6% on CIFAR-10).\n4. We will work on reformatting the appendix and add additional explanatory text. \"preliminary algorithm\": We have called this algorithm preliminary as the results have been below our expectations compared to the relative success in the fixed step case. The problem of continuous vs. discrete semantics also appears for adaptive methods but the simple heuristic algorithm we present does not seem to work as well as expected for adaptive methods. We want to include the results for the tolerance adaptions algorithm nonetheless as we think they emphasize our overall findings. We are aware that the presented algorithm is only the first step of many in the direction of solving this problem and we hope that the community will continue to tackle this challenge. We will remove the word preliminary as it is unfitting as pointed out by the reviewer.\n5. We thank the author for point us towards these publications and we will add these papers to our discussion.\n6. We would like to point out that Reviewer 1 seems to find the title quite fitting. Nevertheless, we are open to suggestions. One option would be the title we chose for a previous version of this paper: \"When are Neural ODEs proper ODEs?\"\n7. We are currently applying for clearance of the code from our institution. We hope to achieve this before the end of the discussion phase, but we promise to release the code eventually.\n\n## Minor comments\n1. We thank the reviewer for pointing out this detail. We will add an explanation for the relationship between step size and number of steps in the introduction. We will also fix the wording on page 7 (number of steps -> number of iterations).\n2. We will incorporate the helpful suggestions of the reviewer. We will increase the width of the figures and we will also add a legend to make the figures clearer.\n3. We thank the reviewer for this suggestion and we will adapt the paper to use the word\"adaptation\".\n\nWe hope to have a first revision including all points of clarity by the weekend. A revision including improved figures and, resources permitting, experiments will take a bit longer.\n\n## References\n[1] On Robustness of Neural Ordinary Differential Equations, Yan et al, 2020\n\n[2] Augemented Neural ODEs, Dupont, Doucet, Teh, 2020\n\n", "title": "Re: Reviewer 4"}, "SAu2VSh56bP": {"type": "rebuttal", "replyto": "7vCSfEs3_Hj", "comment": "We would like to thank the reviewer for their detailed comments.\n\n1. **Number of crossings:** We thank the reviewer for this interesting question. We will get back to you if we find a solution.\n\n2. **Interplay of solver and vector field** We would like to work on answering this interesting question and therefore we ask the reviewer to please clarify a few details. Particularly, we kindly ask the reviewer to clarify what *analytical form of an ODE* is referring to. We believe that reviewer is either referring to that the analytical form of the right side of ODE f(z) is known or that the analytical solution to the ODE is known. We cannot think of a scenario where one would use a numerical solver for an ODE with known analytical solution, so we currently assume the reviewer considers the case of a right-hand side with known analytic properties.\n\n   What can be said at this point: Finding a step size h* for which no crossings occur does not guarantee that no crossings will occur for all step sizes below this step size h*. However, for each problem, there exists some sufficiently small step size h_s such that for all h < h_s, no crossings do occur. I.e., an empirical first occurence of no crossings is not a sufficient condition for small enough step size, but a small enough step size always exists for which even smaller step sizes will produce no crossings.\n\n   Continuous right-hand sides are also not sufficient to eliminate the problem of trajectory crossings. We can construct a synthetic corner case highlighting these problems which we will add in the appendix  (space permitting in the main text).\n\n3. **Lady Windemere's Fan:** We will provide additional explanations concerning Lady W.'s fan see also our answer\nto question 3 below. We would like to know whether your question refers to the specific ODE presented in Eq. (4) or problems\nwhere Lady W.'s fan can be observed in general? Note that Lady W.s fan is a problem *independent* of the trajectories crossing problem. In this case, valid ODE semantics are maintained, but the trained model still crucially depends on the discrete solver dynamics. We will try to make this clearer in the main text.\n\nWe hope to have a first revision including the above changes by the weekend.\nThe addition of the synthetic corner case might take a bit longer, though.\n", "title": "Re: Reviewer 2 Theoretical and empirical questions"}, "rce3BZrgkUo": {"type": "rebuttal", "replyto": "7vCSfEs3_Hj", "comment": "## Questions and Clarification requests\n1. We thank the reviewer for this question. For Figure 1 and 2, there are no true underlying dynamics.The Neural ODE model is only given the classification task and has to find some dynamics which solve the problem. In combination with the classifier, many different vector fields might be possible. We will clarify this in the main text.\n\n2. We thank the reviewer for pointing this out. The difference in the images is due to different scaling of the axis which were chosen such that the final position of all points is shown. We will add units to the axis of Figure 2 (a) and (b) to make this clearer.\n\n3. Our descriptions of Lady W.'s fan indeed needs further explanation - we will expand section 2.2. Lady W.'s fan does not refer to a specific problem, but how the local error gets accumulated into the global error. Lady Windemere's fan does not guarantee solving the XOR problem. But the example used in Section 2.2 is aimed towards showing that error accumulation can lead to dynamics which solve the XOR problem, even if the analytic solution to the ODE does not. The ODE we present in section 2.2. corresponds to a flow with increasing ellipsoids and an increase in the rotational speed for this problem. We discovered this model based on the knowledge that the precision of the solver influences how the rotational speed of the ellipsoids is resolved.\n\n4. \"Do you have any ideas of what directions you might head in in terms of regularising neural ODEs so that they manage to learn continuous semantics, even when trained at larger step-sizes?\" - We thank the reviewer for this interesting question. We do not have any precise ideas yet but restricting the Lipschitz constant of the Neural ODE to below 1 avoids crossing trajectories [1]. Additionally, forcing the model to learn simpler dynamics could reduce the critical step size (as done for example in [2]). We will add this discussion to the main text of the paper.\n\"In particular, why did you go in the direction of an adaptive optimization algorithm, instead of, say, training the neural ODE with a randomly chosen step-size each iteration or even step?\" The idea of our algorithm is to keep the number of steps as small as possible. The idea of reviewer to use random step sizes is interesting. Random step sizes might not provide the wanted gradient information, as too large step sizes drive the system to discrete dynamics. Therefore, if the variance in the steps is too large, we believe that training might become difficult.\n\n 5. **Low accuracy on cifar:**  The performance of our model is due to the simple architecture chosen for our experiments. To improve performance, other work uses a deeper classifier, an upstream downsampling block and often even multiple ODE blocks. We did  not want to use an upstream block, a deeper downstream classifier block and multiple ODE blocks, as we want to maximize the contribution of the ODE block. We will add this to the explanation in the main text\n\n ## Typos and minor edits\nThank you for spotting these and we will fix them.\n\n ## References\n [1] Invertible Residual Networks, Behrmann et al., ICML, 2019\n\n [2] Learning differential equations that are easy to solve, Kelly et al., arXiv, 2020", "title": "Re: Reviewer 2 Question and Clarification Requests"}, "i4zEL5XJEuh": {"type": "rebuttal", "replyto": "HxzSxSxLOJZ", "comment": "We thank the reviewers for their detailed and insightful comments. Reviewers 2, 3 and 4 seem to agree that our paper presents an important issue with high relevance to the Neural ODE community. Reviewers 2 and 4 raise concerns regarding the experimental coverage and the theoretical underpinning, leading to hesitation whether this submission should be accepted now or at a future conference. Reviewer 1 is skeptical about the generality of the presented problem.\n\nWe will try to address the reviewers' concerns within the rebuttal period and we will try to convince you that the work is indeed ready to be published now. To this end, we will improve the clarity and presentation and we will also try to add missing experiments.\n\nFor individual feedback, please refer to the comments below your respective reviews.", "title": "Re: all"}, "7vCSfEs3_Hj": {"type": "review", "replyto": "HxzSxSxLOJZ", "review": "Paper summary:\n\nThe paper demonstrates how neural ODE models generating features for downstream tasks (or simply modelling trajectories) may rely on the discreteness of integration methods to generate features and thus fail in the exact ODE limit of integration step-size going to zero. The paper highlights particular failure modes, such as the discreteness of integration methods allowing for qualitative differences like overlapping trajectories (impossible for the exact solution of an autonomous ODE) compared to exact solutions, or quantitative differences like the accumulated error of a numerically integrated ODE resulting in useful features for downstream tasks. The paper empirically demonstrates the phenomenon that low training losses can be achieved for a range of integration methods and integration step-sizes, but that, of these models, the ones robust to changes in integration method and decreases in integration step-sizes at test time are those trained below a certain (empirically determined) integration step-size threshold. This is attributable to models trained with lower integration step-sizes maintaining features that are qualitatively the same as or quantitatively close to those features produced by the same model with smaller integration step-sizes. The paper proposes an algorithm for adapting integration step-size during training so that the resulting neural ODE model is robust to changes in integration method and integration step-size at test time. The algorithm is empirically demonstrated to achieve the same performance as grid search (for similar numbers of function evaluations).\n\n------------------------------------------\nStrengths and weaknesses:\n\nI liked the paper as it raised an important question of whether and when we should interpret neural ODEs as having continuous semantics and gave a few examples of failure cases. The results of the step-size adaptive algorithm were also promising (it matched grid search but with less work). Further, the paper was clearly written and easy to understand.\n\nHowever, as it stands, I\u2019m assigning a score of 5. I like the paper and think that it would be a good workshop paper but is not ready for the main conference. The reason for this is that the theoretical part of the paper is mostly qualitative, whilst the experiments are not extensive enough to make up for the qualitative theoretical justification. If one of these two areas were to be improved, I would be happy to increase my score. To be concrete, here are examples of theoretical and empirical questions whose answers (just one would do) would increase the paper\u2019s score for me:\n\n1)\tHow can we mathematically describe when numerically integrated trajectories cross over in terms of the time over which the ODE is integrated and on the initial separation of the trajectories?\n\n2)\tSuppose we are integrating an ODE for which we have the analytic form. Are there additional behaviours we need to watch out for? For example, after passing below a step-size where we transition from crossing trajectories to non-crossing trajectories, is it possible to transition back to crossing trajectories as we continue to decrease the step-size? Or can we rule out this case, for example, in the case of a f being continuous in the equation z\u2019(t) = f(z)?\n\n3)\tFor Lady Windermere\u2019s Fan with the true dynamics, at what step-size does trajectory overlap cease to occur (assuming a minimum initial separation of trajectories and fixed time period)? And if we instead attempt to learn Lady Windermere\u2019s Fan with a neural ODE, at what step-size does the neural ODE start to be robust against test-time decreases in step-size? How does this latter step-size compare to the former step-size? \n------------------------------------------\nQuestions and clarification requests:\n\n1)\tWhat was the true underlying model for figures 1 and 2?\n\n2)\tWhy are the classifier decision boundaries different in figures 2a and 2b? I thought that you trained a neural ODE with h_train = 1/2 and then tested this model for both h_test = 1/2 and h_test = 1/4. \n\n3)\tI didn\u2019t understand the connection between Lady Windermere\u2019s Fan and the XOR problem. Does running Lady Windermere\u2019s Fan on R^2 with an XOR labelling lead to trajectory end points that are linearly separable? If so, how did you discover this?\n\n4)\tYou mention at the end of section 2.2 that \u201cThe current implementation of Neural ODEs does not ensure that the model is driven towards continuous semantics as there are no checks in the gradient update ensuring that the model remains a valid ODE nor are there penalties in the loss function if the Neural ODE model becomes tied to a specific numerical configuration.\u201d Do you have any ideas of what directions you might head in in terms of regularising neural ODEs so that they manage to learn continuous semantics, even when trained at larger step-sizes? In particular, why did you go in the direction of an adaptive optimization algorithm, instead of, say, training the neural ODE with a randomly chosen step-size each iteration or even step?\n\n5)\tWhy was the CIFAR-10 classification accuracy (~55%)? Previous work on neural ODEs has obtained accuracy in the 80 -95% range. Is this just due to the limited expressiveness of the upstream classifier, cf. \u201cFor all our experiments, we do not use an upstream block f_u similar to the architectures proposed in Dupont et al. (2019). We chose such an architectural scheme to maximize the modeling contributions of the ODE block.\u201d \n\n------------------------------------------\nTypos and minor edits:\n\n- Write Initial Value Problem (IVP) on first usage of IVP.\n- Fig.2 caption \u2013 \u201cThe model was trained \u2026, we used \u2026\u201d -> \u201cThe model was trained \u2026, and we used \u2026\u201d\n- Page 8, Conclusion, line 3 \u2013 \u201c\u2026 an continuous\u2026\u201d -> \u201c\u2026 a continuous \u2026\u201d", "title": "Review of \"RESNET AFTER ALL: NEURAL ODES AND THEIR NUMERICAL SOLUTION\"", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}