{"paper": {"title": "Finding Winning Tickets with Limited (or No) Supervision", "authors": ["Mathilde Caron", "Ari Morcos", "Piotr Bojanowski", "Julien Mairal", "Armand Joulin"], "authorids": ["mathilde@fb.com", "arimorcos@gmail.com", "bojanowski@fb.com", "julien.mairal@inria.fr", "ajoulin@fb.com"], "summary": "Finding winning tickets does not require much supervision or data.", "abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n", "keywords": ["Lottery Tickets Hypothesis", "Self-Supervised Learning", "Deep Learning", "Image Recognition"]}, "meta": {"decision": "Reject", "comment": "The paper studies finding winning tickets with limited supervision. The authors consider a variety of different settings. An interesting contribution is to show that findings on small datasets may be misleading. That said, all three reviewers agree that novelty is limited, and some found inconsistencies and passages that were hard to read: Based on this, it seems the paper doesn't quite meet the ICLR bar in its current form. "}, "review": {"BkxyHdzhsr": {"type": "rebuttal", "replyto": "SJeuh5z19B", "comment": "1) Following the reviewer's comment, we report in Appendix E tables the exact accuracies in each setting. We report mean and standard errors for our experiments which we run with 3 (ImageNet and Places) or 6 (CIFAR) different seeds. We thank the reviewer for this recommendation and for helping us improving the clarity and robustness of the comparison.\n\n2) The \u201cRandom - adjusted\u201d baseline is not obtained by applying the pruning mask to randomly initialized weights. In the following lines, we motivate and clarify this baseline and have included this explanation in the paper updated version.\nWe find that deep architectures (VGG-19 or ResNet-18 for example) trained on CIFAR-10 are naturally sparse (~80% of the weights are zeroed at convergence). Pruning a network at rates below its level of natural sparsity without impacting the performance is trivial because the network is already sparse. Indeed, we found that in the random global pruning baseline (which can remove non zero weights), pruning at rates below the natural sparsity of the network degrades accuracy, while pruning of weights that are already zeroed has no effect. Experiments performed with pruning rates below the natural level of sparsity of the network (~80%) are uninformative. Inconveniently, this performance gap carries over to higher pruning rates (in which we are interested in) and can lead to misleading interpretations. The random adjusted baseline removes this effect by first pruning the weights that naturally converge to zero after training. Then, we randomly mask the remaining non-zeroed weights to get different final desired pruning rates. The remaining non-masked weights are randomly initialized. This baseline therefore corrects for the natural sparsity present in CIFAR-10 networks. \n\nRegarding the random initialization remark, Liu et al. indeed show in Figure 7.a (unstructured iterative pruning) that starting from randomly reinitialized weights works well on deep architectures (VGG-16 and ResNet-50) on CIFAR-10 when pruned at rates below ~90%. This is consistent with the observations of Frankle et al. (2019) in the Appendix A of their paper. Indeed, Frankle et al. (2019) also show that up to a certain level of sparsity, training the subnetwork from its original weights or from random re-initialization gives comparable performance. However, for more extreme pruning rates (>90%), resetting the subnetwork to its original weights gives better performance than random re-initialization. In this work, we follow up on the work of Frankle et al. (2019) and Morcos et al. (2019) that both provide empirical evidence that in the regime of large datasets or high pruning rates, starting from a particular set of weights instead of random initialization is critical to reach high accuracy.\nFor completeness though, we take into account the remark of the reviewer and have included in Appendix G results with random re-initialization for winning tickets found with labels or with RotNet self-supervised task on both ImageNet and CIFAR-10. \nOn ImageNet, consistently with the experiments of Frankle et al. (2019) we observe that resetting the weights accordingly is crucial to get high accuracy. Indeed, on both ResNet-50 and AlexNet, for labels, the subnetworks that are reset to their weights early in training (dark blue plain line) perform significantly better than subnetworks randomly re-initialized (dark blue dashed line). Interestingly, this is not the case for RotNet winning tickets: starting from original weights (pink plain line) gives only a very slight boost (or even no boost at all) of performance compared to randomly re-initialization (pink dashed line). Overall, labels or rotnet subnetworks perform in the same ball park when randomly re-initialized, but using the original weights gives a large boost of performance for labels but not for rotnet. Thus, it suggests that the information carried by the pruned mask itself is similar for labels and rotnet subnetworks but the weights of the rotnet winning tickets are not as good starting points as the weights from labels winning tickets. We thank the reviewer for suggesting this experiment; it gives interesting insights on the difference of performance between labels and rotnet winning tickets.\nOn CIFAR-10, up to a certain level of sparsity that roughly corresponds to the natural level of sparsity of the network, using random re-initialization or weights \u2018early in training\u2019 gives similar performance. However, for more extreme pruning rates, using a particular set of weights gives significantly better performance than random re-initialization.\n\n3) We chose not to vary the dataset size on CIFAR-10 because it is already small. However, following the reviewer recommendation we include results with CIFAR-10 in Appendix F.\n\nOverall, we hope that our updated version of the paper along with our comments provide clarifications about our experimental settings and reinforce the validity of our results.\n", "title": "Reply to review #1"}, "rJgexFfhir": {"type": "rebuttal", "replyto": "SkxRhOSTFB", "comment": "We agree with the reviewer that our contribution is not methodological but mainly experimental.\n\nAlso, we agree that some of our observations might be intuitively expected and reasonable, though we note that the fact that winning tickets transfer between similar datasets with labels (as shown in Morcos et al., 2019) does not necessarily imply that transfer from self-supervised tasks should be possible. Based on previous results, it is entirely plausible that winning tickets are dependent on labels (i.e., p(y|x) vs. p(x)). \nFurthermore, we argue that, even if these results are expected, confirming these intuitions with rigorous experimentations as we propose in our paper is still important, as noted by R2 (\u201ckind of expected, but it is still good that this paper provide solid experimental results to verify this\u201d). \n\n\u201cI also don't see a practical benefit beyond transfer learning setup.\u201c: As a byproduct, the label-agnostic winning tickets also allow to study the transferability of winning tickets between different tasks, which has a concrete practical benefit. Indeed, similarly to the motivation of Morcos et al., if winning tickets can transfer between tasks, then they can be reused across a variety of problems, thus dispensing the need for generating new winning tickets for each new task.\n\n\u201cgiven that lottery tickets are transferable (Morcos paper) it is really not that surprising\u201d: The datasets used in the paper of Morcos et al. were pretty similar to one another, and the fact that we can transfer between similar supervised tasks does not suggest that we should be able to transfer from self-supervised to supervised. Our work provides insights regarding the dependence of winning tickets on p(x) vs. p(y|x).\n\n\"I was surprised to not see pseudo-labeling or consistency training\": For our semi-supervised experiment, we choose to focus solely on the semi-supervised technique introduced in the paper \"S4L: Self-Supervised Semi-Supervised Learning\" of Zhai et al. (ICCV 2019) because it yields better performance compared to VAT or pseudo-labeling on ImageNet (see Table 1. from their paper).\n", "title": "Reply to review #3"}, "BJxzhLG3jr": {"type": "rebuttal", "replyto": "B1gdk_E5cH", "comment": "We thank the reviewer for this positive feedback. We did not experiment on the particular scenario of multi-task learning with limited amount of data; we agree that this is an interesting problem for future work.\n", "title": "Reply to review #4"}, "SJl5O8GnjS": {"type": "rebuttal", "replyto": "ryxRdy869S", "comment": "We thank the reviewer for this constructive and thoughtful feedback.\n\u201cThis undermines the *bold* claim in the abstract\u201d: The remark about the bold claim is a fair point, and we have updated the paper with this caveat accordingly.\n\n\u201cThis paper raises the issue of ill-definedness of \u201cearly in training\u201d, but did not provide a solution.\u201c: We agree that the fact that we do not provide a solution to the problem of late resetting is slightly disappointing. Yet, this is not the main focus of our paper.\n\n\u201cthe ability to exactly perserve the accuracy while pruning the weights\u201c: We emphasize that our primary aim is to better understand lottery tickets rather than just get good performance. In particular, we are interested in whether the winning ticket initializations derived from data with little or no supervision outperform subnetworks initialized randomly. Our finding that these winning tickets do in fact outperform random tickets suggests that the properties of winning ticket initializations which lead to better optimization are largely independent of labels, and rather mostly rely on p(x) (though we do note, as the reviewer pointed out, that the inclusion of labels does lead to better winning tickets, though not by much). \n\n\u201cI feel that the novelty of this paper is limited, and do not provide much new insights.\u201c\u201d:  Please see our general comment for more detail on the novelty of our work and why the insights we generated are relevant to future work on the lottery ticket hypothesis. \n", "title": "Reply to review #2"}, "H1gzfIG2or": {"type": "rebuttal", "replyto": "SJx_QJHYDB", "comment": "We thank the reviewers for taking the time to provide detailed and thoughtful comments. This constructive feedback has been helping us improving our submission.\n\nOur contribution is essentially experimental and we were pleased to see that overall, the reviewers found our experimental results to be \u201csolid and provide more understandings of the lottery ticket hypothesis\u201d (R2) and assessed that we have \u201cconducted extensive experiments on three open questions and results prove [ours] assumptions\u201d (R4).\nYet, reviewer 1 is concerned by the robustness of our experimental setup and we address his or her concerns in our reply and in the updated version of the paper.\n\nThe main caveat from the reviewers relates to the lack of novelty (R2: \u201cthe novelty of this paper is limited\u201d; R3: \u201cimmediate followup on Morcos et al.\u201d, \u201cfairly obvious\u201d). They also question the interest and practical value of our study (R2: \u201cdo not provide much new insights\u201d; R3: \u201c I also don't see a practical benefit\u201d).\n\n*Novelty.*\nTo the best of our knowledge, we propose the first study of the lottery ticket hypothesis in the context of limited access to samples and labels*.* Our experiments are fairly extensive: we generate winning tickets on ImageNet for several different settings (2 different self-supervision losses, 4 different sizes of dataset and 4 different number of classes, semi-supervision) at 14 different pruning rates ranging from 20% to 99.9%, thus covering both moderate and extreme sparsity.\nWe are the first paper addressing the lottery ticket hypothesis with a majority of our experiments conducted on ImageNet,\nwhile showing that conclusions on smaller datasets may be misleading. Our experiments show indeed that deep networks trained on CIFAR-10 are naturally sparse, making conclusions potentially incorrect.\n\nMoreover, our findings are different from Morcos et al., who show that winning tickets can transfer between different datasets with a common domain (natural images) trained on the same task (labels classification). The fact that we can transfer between similar supervised tasks does not suggest that we should be able to transfer from self-supervised to supervised tasks. Also, it does not guarantee that winning tickets found with only 10 classes (out of 1000) transfer well to full ImageNet for example. Besides, even if these results were expected somehow, it would still be essential to verify these with rigorous experiments, as we propose in our paper.\n\n\n*Motivation - why does it matter ?*\nIn our submission, we aim to better understand the properties of winning tickets. Indeed, a better understanding of winning ticket properties might enable faster winning ticket generation and thus allow for concrete applications in fields such as network compression or initialization. We propose an extensive series of experiments investigating winning ticket generation with limited access to labels and samples in order to isolate and assess the dependance in p(x) and p(y|x) of the winning tickets.\nAs a byproduct of this design, the label-agnostic winning tickets also allow to study the transferability of winning tickets between different tasks, which has a concrete practical benefit. Indeed, similar to the motivation of Morcos et al., if winning tickets can transfer between tasks, then they can be reused across a variety of problems, thus dispensing the need for generating new winning tickets for each new task.\n", "title": "Global comment"}, "SkxRhOSTFB": {"type": "review", "replyto": "SJx_QJHYDB", "review": "This paper studies the problem of finding sparse networks in a limited supervision setup. The authors build on the lottery ticket work of Frankle & Carbin and investigate the validity of their idea when one has few or no labels. This work is an immediate followup on Morcos et al. who investigated the transferability of lottery tickets.\n\nThis work is more observational rather than algorithmic or theoretical. Authors study various small sample/label setups where network sparsification works well. \n\nMain contribution is Section 4.1 where self-supervision is investigated. However given that lottery tickets are transferable (Morcos paper) it is really not that surprising that semisupervised learning algorithms will do a decent job as well. I also don't see a practical benefit beyond transfer learning setup.\n\nSection 4.2 essentially sweeps through supervised problem parameters such as reducing sample size, adding noise etc and . The main application seems to be extracting lottery tickets faster by downsampling the data however this aspect is again fairly obvious. \n\nIn short, unfortunately, this paper doesn't cut it for ICLR. As improvements, I would recommend adding standard semi-supervised training techniques to their comparison. I was surprised to not see pseudo-labeling or consistency training (e.g. virtual adversarial training).", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 4}, "SJeuh5z19B": {"type": "review", "replyto": "SJx_QJHYDB", "review": "In the original lottery ticket paper, it points out that training the pruned architecture from scratch with initial weight can achieve the same performance compared to fine-tuning it. This work further discuss this phenomenon when data or label is not enough. It is good to see the few-data/label can still provide a comparable results. But some experiment\u2019s results and its setting are confusing, while also makes me concerned about the conclusion solidness.\n\n1) Usually in classification task (especially in cifar10 dataset), 0.5% to 1% accuracy could be a huge gap between two models. For example, in the original \u201cLottery Ticket Hypothesis\u201d paper, using initial weight only has roughly 0.5% improvement compared to random initialization. But the figures in this paper do not contain a zoom-in details for each line, make me hard to distinguish the performance between each setting. If the author does not provide a detailed version, it will look like theses model have the same performance, which is actually wrong. The author should either plot a zoom-in figure especially when the pruning ratio larger than 50% or give a Table with accuracy of each setting. And it is better to complete the figure with several random seed and plot the error bar to avoid randomness.\n\n2) Does the \u201cRandom - adjusted\u201d item in Figure. 1 mean the correctly pruning architecture with random initialization? In \"rethinking the value of network pruning\", Liu et al. points that in the large learning rate setting (lr=0.1, which is also your setting), random initialization can achieve the same performance compared to the lottery ticket. In my perspective, I want to see whether few-data/label also works on random initialization instead of lottery tickets. I expect the author to explain the \u201cRandom -adjusted\u201d experiment setting clearly in the response and I suggest the author to discuss the\nrandom initialization part specifically.\n\n3) Figure.3 only shows the \u201cvarying dataset size\u201d experiments on ImageNet. The experiments on cifar10 is lacked. The author should complete this part in the response.", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "B1gdk_E5cH": {"type": "review", "replyto": "SJx_QJHYDB", "review": "In this paper, the authors try to provide empirical answers to several important open questions on winning tickets. They conduct most of experiments on ImageNet and results show that winning ticket is robust, and few data samples can also obtain good winning tickets.\n\nGenerally, the paper has conducted extensive experiments on three open questions and results prove their assumptions.\n\nAs describe in page 7, lottery tickets are sensitive to data distributions. I\u2019m wondering, whether there will be winning ticket for multi-task learning with limited data each task? Will this be helpful in distilling the model?", "title": "Official Blind Review #4", "rating": "6: Weak Accept", "confidence": 1}, "ryxRdy869S": {"type": "review", "replyto": "SJx_QJHYDB", "review": "This paper empirically studies the lottery ticket hypothesis with limited or no supervision. First, the authors use self-supervised learning to generate winning tickets, showing that \"good\" (reasonable) winning tickets can be found without labels. Second, the authors show that finding \"good\" (reasonable) winning tickets can be accelerated by a factor 5 on ImageNet by using only a subset of the data. The authors also argue that using large datasets is important to study lottery tickets, since deep networks trained on CIFAR-10 are natually sparse, making conclusions potentially misleading.\n\nThe experimental results are rich and provide more understanding of winning ticket generation with limited or no supervision. The results on self-supervised learning task (including the layer-wise pruning results) and a subset of training dataset are reasonable and kind of expected, but it is still good that this paper provide solid experimental results to verify this. As the paper observed, \"none of the tickets found with limited access to labels and or data matches the accuracy of tickets found with all the labeled data when considering moderate pruning rates (more than 10% of unpruned weights)\non ImageNet. Indeed, we consistently observe a decrease in performance compared to the full overparametrized network as soon as we prune the network.\" In this sense, winning tickets are certainly label and data dependant. This undermines the *bold* claim in the abstract that \"we provide a positive answer to both questions, by generating winning tickets with limited access to data, or with self-supervision\". From my perspective, the ability to exactly perserve the accuracy while pruning the weights (see the flat regions of \"Lables\" curves in Figure 1,2,3,4,5) is the interesting part of the lottery ticket hypothesis. We have several different ways to achieve a descreased accuracy with a smaller network, the dynamics there may be a mixture of the lottery ticket hypothesis and standard model pruning, which needs more careful experiment design to separate different dynamics.\n\n\"using large datasets is important to study lottery tickets, since deep networks trained on CIFAR-10 are natually sparse, making conclusions potentially misleading.\" \"The definition of \u201cearly in training\u201d is somehow ill-defined: network\nweights change much more for the first epochs than for the last ones.\" These two messages are important to future study of the lottery ticket hypothesis. This paper raises the issue of ill-definedness of \u201cearly in training\u201d, but did not provide a solution. \n\nOverall, I found that the experimental results in this paper are solid and provide more understandings of the lottery ticket hypothesis. However, I feel that the novelty of this paper is limited, and do not provide much new insights. Therefore, it does not reach the bar of being published at ICLR, from my perspective. Therefore, I say \"Weak Reject\".", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 1}}}