{"paper": {"title": "Training independent subnetworks for robust prediction", "authors": ["Marton Havasi", "Rodolphe Jenatton", "Stanislav Fort", "Jeremiah Zhe Liu", "Jasper Snoek", "Balaji Lakshminarayanan", "Andrew Mingbo Dai", "Dustin Tran"], "authorids": ["~Marton_Havasi1", "~Rodolphe_Jenatton3", "~Stanislav_Fort1", "~Jeremiah_Zhe_Liu1", "~Jasper_Snoek1", "~Balaji_Lakshminarayanan1", "~Andrew_Mingbo_Dai1", "~Dustin_Tran1"], "summary": "We show that a deep neural network can be trained to give multiple independent predictions simultaneously, which results in a computationally efficient ensemble model.", "abstract": "Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance  can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant runtime cost. In this work, we show a surprising result:\nthe benefits of using multiple predictions can be achieved 'for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100,  ImageNet, and their out-of-distribution variants compared to previous methods.", "keywords": ["Efficient ensembles", "robustness"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a simple but effective method to obtain ensembles of classifiers (almost) for free. \nEssentially you train one network on multiple inputs to predict multiple outputs. The authors show that this leads to surprisingly diverse networks - without a significant increase in parameters - which can be used for ensembling during test time. \nBecause of its simplicity, I can imagine that this approach could become a standard trick in the \"deep learning tool chest\". \n\n-AC"}, "review": {"_Px3yjPuMoE": {"type": "review", "replyto": "OGg9XnKxFAH", "review": "Summary: The ensemble method MIMO is proposed in this paper to reduce the inference delay and keep the prediction diversity. Only one model with sufficient capacity exists in this method while multiple implicit subnets are embedded in the parent model. Each subnet has individual I/O, so only one forward pass of the parent model is needed to process all subnets and make the ensemble.\n\nQuality: Medium to high. Pros: 1) The dissections of the subnets are impressive. They design the experiments to survey the loss plane, the parameter/activation projection of different subnets. 2) With the proposed training paradigm and proper test setting, the accuracy improvement can be seen and also uncertainty estimation. 3) They report the SOTA results of accuracy, uncertainty, and robustness on various datasets and their OOD variants when considering the inference latency. Cons: 1) They only report the inference time of one sample, but the total computation costs (e.g. MACs or FLOPS) are omitted. 2) The multiple branch networks are not only used in ensemble and broader usages exist. I know at least three works which involves multi-branch architecture (output-wise or layer-wise) in robustness or knowledge distillation and some of them are also using the ensemble of multiple predictions. The lack of citations in the related work seriously declines the quality of this work.\n\nClarity: High. Pros: 1) The method framework has a brief and clear explanation (Figure 1). 2) The training methods are conveyed in every detail, including some techniques like \u201cinput repetition\u201d. 3) Some critical plots reporting accuracy using error bars or box plots to display the performance variance. Cons: Some format mistakes of symbols are existing. For example, the authors mix the usage of the normal and italic font of \u201cx\u201d when referring to samples in different expressions; the chaotic usage can even happen in the same equation for the first one of Section 3.3.\n\nOriginality: Low to medium. Pros: This method successfully uses the multi-branch architecture to reduce the inference delay in ensemble, which is a rather novel idea. Cons: As mentioned above, many similar multi-branch architectures have been used in different but related topics. Consider all of these works, the originality of this work has to be downgraded.\n\nSignificance: Low to medium. Pros: The shining points of this work are making ensemble for \u201cfree\u201d. They do decrease the inference delay significantly. Cons: 1) The authors attempt to distract our attention on the computational costs of this MIMO architecture and try to make an illusion that it\u2019s convenient in computing. The latency is decreased at the cost of batch size. Other ensemble methods have a longer delay, but they can process a batch of images. This method fills the batch with the same image which implicitly decreases the batch size. Furthermore, no measurement is used on computation costs or other related aspects. I think it\u2019s deceptive and tricky. 2) As shown above, the originality of this work is not as solid as their experiments. \n\n[ Detailed comments]\n1. What are the structural considerations for the related work of Section 5 not to be explained in Section 2?\n2. In Figure 6, the \u2018#\u2019 marked on the abscissa of (b) is redundant\n\n", "title": "Limited novelty", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "515r0KUngN": {"type": "rebuttal", "replyto": "_Px3yjPuMoE", "comment": "Thank you again R3 for your detailed feedback. We have incorporated them in our revision. If you still have concerns, please let us know. This is the last day for us to reply so we hope we can address any lingering questions.", "title": "Addressing the concerns"}, "iKo7vopEvl-": {"type": "rebuttal", "replyto": "_Px3yjPuMoE", "comment": "Thank you for the detailed feedback. We are addressing the two main concerns, originality and computational costs, followed by the minor concerns.\n\n__Main concern - Originality:__ The core contribution of the paper is that by using a multi-input multi-output approach, we can train independent subnetworks within a network and obtain diverse predictions. This addresses the main drawback of previous multi-headed approaches [1,2]: their subnetworks\u2019 predictions highly correlate when using only a single input, as shown in Section 3-4. Since in our setup, independence is ensured by the use of multiple inputs, MIMO also eliminates the need for architectural changes (such as in [1]) which further reduces computational costs. To the best of our knowledge, the idea of using multiple inputs for this purpose is novel, and as we show, it comes with significant computational advantages.\n\nWe discuss related work on multi-branch architectures in Section 5. We are keen to expand on this. Could you point us towards the works that we missed?\n\n__Main concern - Computational costs:__ In the paper, we claim that MIMO\u2019s computational cost is equivalent to a standard deep neural network at test time. We support this claim by measuring and reporting the inference delay for all models. In Table 1, 2 and 3 the prediction time column refers to the time it takes to do inference on a single image (ms/example). This metric is calculated over a batch of images---64 for CIFAR10/100 and 128 for ImageNet---and then averaging. The inference delay of MIMO is identical to standard deep neural networks, since they both require a single forward pass for evaluation.\n\nTo further address the concern, the number of FLOPS for a ResNet28-10/CIFAR-10 is 10,559M and for MIMO (M=3) it is 10,561M (less than 1% difference). The only additional computational cost is processing the extra inputs and the extra outputs.\n\nRegarding batching, MIMO is fully compatible with batch evaluation. A single input to MIMO has the shape [W, H, MC], where W is the width of the image, H is the height and C is the number of color channels. The third dimension is formed by concatenating the M input images along the channel axis (in the supplied source code, this is done on line 90 in cifar_model.py). At evaluation time, to evaluate MIMO on a batch of 64 images, we first tile the images M times along the channels axis, forming a tensor of shape [64, W, H, MC], execute the 64 forward passes required for MIMO and report the results.\n\nWe do not believe that our results are deceptive, let us know in what ways MIMO requires clarification.\n\n__Minor concerns:__\nNotation - we use bold characters to denote random variables and italic to denote their instatiations. Thank you for pointing this out, we are going to clarify this in the paper.\n\nRelated works section - we decided to place our related works section after the experiments so that the flow of the paper is uninterrupted. In our opinion, the method is easier to understand this way.\n\nIf our reply addressed some of your concerns, please consider updating your score.\n\n[1] Lee, Stefan, et al. \"Why M heads are better than one: Training a diverse ensemble of deep networks.\" arXiv preprint arXiv:1511.06314 (2015).\n\n[2] Tran, Linh, et al. \"Hydra: Preserving ensemble diversity for model distillation.\" arXiv preprint arXiv:2001.04694 (2020).\n\nEdited:\nUpdated the number of FLOPS. Our results are in agreement with these results: https://github.com/osmr/imgclsmob/blob/master/chainer_/README.md (WRN-28-10)", "title": "Addressing the concerns"}, "ChQldYeCZ7s": {"type": "rebuttal", "replyto": "OGg9XnKxFAH", "comment": "In response to the reviews, we made a few changes to the paper.\n* We clarified where the 1% increase in model parameters come from and also included numbers for FLOPs (Reviewers 3 and 4)\n* We clarified the source of independence in the introduction (Reviewer 1)\n* We clarified our notation with respect to random variables and their instantiations (Reviewer 4)", "title": "Rebuttal Edits"}, "lGwRno3n60": {"type": "rebuttal", "replyto": "QmT9IAdEHfO", "comment": "Thank you for the review, we are glad that you found our work interesting and impactful.\n\nTo address the main concern, the 1% increase in model parameters comes from the extra parameters needed in the first and last layers of MIMO. The first hidden layer requires M times more weights for the M inputs and the output layer also requires M times more weight for the M outputs. Of course, the number of extra weights is architecture dependent, but to give an example, in the case of M=3/ResNet28-10/Cifar-10 they account for 1% additional model parameters. We are clarifying this point in the paper.\n\nWe report results on three commonly used image classification benchmarks, Cifar-10, Cifar-100 and ImageNet, with 10, 100 and 1000 classes respectively. It would be interesting to see further datasets and tasks, but we leave this for future works.\n", "title": "Addressing the concerns"}, "16tgzLWjm27": {"type": "rebuttal", "replyto": "5o6eaFZLv_", "comment": "Thank you for the detailed feedback, we are glad that you found our work interesting. We would like to address each weakness in order.\n\nWe briefly touched on the relationship between MIMO and BatchEnsemble in our related works section and we are expanding on this a little bit. The reason that MIMO has better diversity is that the subnetworks in MIMO learn to ignore the inputs to the other subnetworks, whereas in BatchEnsemble, it is quite likely that the subnetworks share each other's features due to the high level of parameter sharing.\n\nThe key to independence is that the subnetworks do not share features, since features derived from one input contain no useful information for classifying another. As a result, the subnetworks learn a more compact, independent representation for each input, and they learn to classify their corresponding inputs while ignoring the other inputs. We are updating the introduction to bring more attention to this key detail.\n\nBeyond the intuition for independence, we present two forms of empirical analysis to support our claims. First, we look at the loss landscape of the networks and find that the subnetworks converge to different local minima in weight space (Figure 3) and in function space (Figure 4, t-SNE plot). Second, by analysing the conditional variance of the activations within the network, we show that the subnetworks separate within the network and do not share features (Figure 4). We agree that it would be interesting to theoretically analyze the emergence of independence, but leave this for future work.\n", "title": "Addressing the concerns"}, "hDgCzaHQRBU": {"type": "rebuttal", "replyto": "0FluA_PN3gR", "comment": "Thank you for the detailed review, we are glad that you found the paper interesting.\n\nRegarding the first reservation, we investigate theoretical questions in the regression case where we show that the advantage of MIMO boils down to a bias-variance tradeoff. This analysis showed that MIMO is particularly effective when the network has excess capacity that MIMO can make use of. We leave further theoretical analysis for future works.\n\nIn terms of practical advantage, MIMO significantly improves robustness without increasing the computational costs. We agree that if the test-time computational requirements allow for using a deep ensemble, MIMO might not be the optimal choice, however, when the test-time compute is limited, such as the case in self-driving cars, or large-data applications, MIMO offers considerable advantages over using a standard deep neural network.\n\nIndeed, at test time we repeat the same input to obtain M predictions. This can occur during training (with a small probability), so this is not out-of-distribution for the model. The model behaves as expected, because the subnetworks learn to ignore each other\u2019s inputs during training (since the other inputs provide no useful information for classifying their own input). \n\nWe can also permute the inputs. What we see is that they each try to classify their own inputs and ignore the others. Since the subnetworks operate independently, they give different predictions for the same input.\n", "title": "Addressing the concerns"}, "0FluA_PN3gR": {"type": "review", "replyto": "OGg9XnKxFAH", "review": "This paper proposes to train a single network with M input examples and M corresponding predictions, and the M input examples are mixed to produce the M corresponding predictions. Although only a single network is learned, it implicitly consists of multiple sub-networks due to the nature of multiple inputs and multiple outputs in training. In testing, the single testing example can be replicated M times as inputs, so that M outputs are produced by the trained network. The multiple outputs enable efficient ensembing for robust prediction. \n\nI find the proposed idea very clever. The empirical results on toy data and real data are interesting and compelling. It demonstrates that a single network has the capacity to contain multiple sub-networks, which is an interesting discovery in itself. \n\nI do have two main reservations. The first one is the lacking of some basic, not necessarily rigorous, theoretical formulation and analysis. The second one is about its practical potential. Apparently M has to be rather small, due to the limited capacity of a single network. Its advantage over training multiple networks may not be dramatic. \n\nI am also a bit concerned that the network is trained on M independent examples (although the proposed method does allow for occasional identical examples), but is tested on M identical copies of the same testing example. \n\nI am also unclear about the nature of mixing M independent training examples, and the effect of doing that. That is why I feel some theoretical understanding is needed. \n\nWhat if we permute the M training examples and check the difference of the corresponding outputs? \n", "title": "A clever idea and interesting empirical results", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "5o6eaFZLv_": {"type": "review", "replyto": "OGg9XnKxFAH", "review": "SUMMARY:\nThis paper describes a multi-input multi-output (MIMO) strategy for training several subnetworks inside a same and single neural network for robust prediction. The approach consists in jointly training the heads to make predictions for their corresponding inputs. The strategy is simple and demonstrates strong results. The experimental study reveals that subnetworks functionally behave as independent networks, hence resulting in a strong and robust ensemble. \n\nSTRENGTHS:\n- The experimental study is very thorough. I really appreciated the investigation in Section 3, which indeed convincingly shows that the subnetworks behave as independent. \n- The method is compared on standard benchmarks, across a wide range of metrics. Experimental results show better performance than single forward pass methods.  Performance is reasonable with respect to a simple solution consisting in training an actual ensemble of 4 networks.\n- The paper is well written and easy to follow.\n\nWEAKNESSES:\n- I believe the approach to be original, but its similarities/differences with other multi-input multi-output (such as BatchEnsemble) could have been discussed much further to better appreciate the originality of MIMO.\n- Although independence between subnetworks is shown empirically, I cannot help but wonder how/why it emerges from the architecture! This is an exciting phenomenon that ought to be better understood. Nevertheless, I believe the actual independence of subnetworks should be nuanced at places (e.g., in the title or in the abstract), as this highly depends on the capacity of architecture and on the problem to solve -- as shown in the experiments themselves. \n- Some (hypothetical) theoretical explanations regarding the emergence of independence would have made the paper stronger, although I realize this would be a whole new paper by itself.  ", "title": "A simple and effective way to train a single network as an ensemble of networks", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "QmT9IAdEHfO": {"type": "review", "replyto": "OGg9XnKxFAH", "review": "Authors assessed how these subnetworks can be as diverse as independently trained networks. The contribution of this paper is in proposing an approach to improve uncertainty estimation and robustness with minor changes (1 percent) to the number of parameters and compute cost. \n\nSTRENGTHS: \nTraining multiple independent subnetworks within a network, with minimal increase in number of parameters. \nThe use of MIMO makes this approach simple, while it can be evaluated in a single forward pass.  \n\nCONCERNS:\nThe authors claim that the benefits of using multiple predictions can be achieved \u2018for free\u2019, while their proposed model increases the number of parameters (even though by 1 percent)\nThe paper has examined the accuracy and disagreement of the subnetworks, but a detailed evaluation on number of parameters is missing (i.e. where the 1% increase in parameters comes from).\nAn experiment on more diverse datasets would be also helpful, such as OpenImages. \n", "title": "This paper presents an approach to use a multi-input multi-output configuration for training multiple subnetworks with independent tasks. The authors claim that by ensembling the predictions (output of subnetworks) they can improve model robustness without additional computational cost. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}