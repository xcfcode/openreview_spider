{"paper": {"title": "Area Attention", "authors": ["Yang Li", "Lukasz Kaiser", "Samy Bengio", "Si Si"], "authorids": ["liyang@google.com", "lukaszkaiser@google.com", "bengio@google.com", "sisidaisy@google.com"], "summary": "The paper presents a novel approach for attentional mechanisms that can benefit a range of tasks such as machine translation and image captioning.", "abstract": "Existing attention mechanisms, are mostly item-based in that a model is trained to attend to individual items in a collection (the memory) where each item has a predefined, fixed granularity, e.g., a character or a word. Intuitively, an area in the memory consisting of multiple items can be worth attending to as a whole. We propose area attention: a way to attend to an area of the memory, where each area contains a group of items that are either spatially adjacent when the memory has a 2-dimensional structure, such as images, or temporally adjacent for 1-dimensional memory, such as natural language sentences. Importantly, the size of an area, i.e., the number of items in an area or the level of aggregation, is dynamically determined via learning, which can vary depending on the learned coherence of the adjacent items. By giving the model the option to attend to an area of items, instead of only individual items, a model can attend to information with varying granularity. Area attention can work along multi-head attention for attending to multiple areas in the memory. We evaluate area attention on two tasks: neural machine translation (both character and token-level) and image captioning, and improve upon strong (state-of-the-art) baselines in all the cases. These improvements are obtainable with a basic form of area attention that is parameter free. In addition to proposing the novel concept of area attention, we contribute an efficient way for computing it by leveraging the technique of summed area tables.", "keywords": ["Deep Learning", "attentional mechanisms", "neural machine translation", "image captioning"]}, "meta": {"decision": "Reject", "comment": "although the idea is a straightforward extension of the usual (flat) attention mechanism (which is positive), it does show some improvement in a series of experiments done in this submission. the reviewers however found the experimental results to be rather weak and believe that there may be other problems in which the proposed attention mechanism could be better utilized, despite the authors' effort at improving the result further during the rebuttal period. this may be due to a less-than-desirable form the initial submission was in, and when the new version with perhaps a new set of more convincing experiments is reviewed elsewhere, it may be received with a more positive attitude from the reviewers."}, "review": {"Sye91YISeV": {"type": "rebuttal", "replyto": "B1ga9sDNxN", "comment": "While accuracy gains are not always significant, the improvements from area attention are pretty consistently seen across most conditions. While the baseline models were well tuned by previous work, we didn\u2019t particularly tune each model to better work with area attention. We believe some hyperparameter tuning would result in better accuracy. For example, we acquired some new results (29.74 for En-De and 41.5 for En-Fr) recently by just tuning on the maximum area size. Tuning other hyperparameters such as attention dropout ratio can help realize area attention\u2019s full potential.", "title": "Thanks for your further comments"}, "BklOdLLreV": {"type": "rebuttal", "replyto": "rylq26xWeV", "comment": "Re: Motivation\nWe agree that attention visualization would be a good way to show, which we should add to the paper. We feel the need to support areas or ranges has been well demonstrated in a collection of previous works that are suggested by Reviewer1 and discussed by us in the Related Work section. Area attention we proposed here provides a simple and effective solution for enabling areas. We will add concrete examples to the paper.\n\nRe: Significance\nOn one hand, we agree that Transformer_big is a strong baseline and our gain is not always significant. On the other hand, we feel area attention provides a simple solution that can be easily applied to many tasks and models that lead to better accuracy. We want to point out that while the baseline models (Transformer_Base & Big) were extensively tuned for the NMT tasks, we did not particularly tune model conditions for area attention. We simply used all the hyperparameters that were tuned for the baselines for area attention experiments. In our recent experiments, by tuning on the maximum area size, we found area attention achieved even better accuracy, e.g., 29.74 for En-De and 41.5 for En-Fr, which enable a larger gain over the baselines. There are other hyperparameters to tune such as attention dropout ratio, which can be crucial for area attention to realize its potential. Please note that the basic form of area attention (Eq.3) requires no additional parameters and the feature combination version of area attention (Eq.5-9) uses very few additional parameters: less than 0.03% for Transformer Big.\n", "title": "Thanks for your further comments"}, "ryxwGrUrxE": {"type": "rebuttal", "replyto": "HygTAER-gE", "comment": "Thank you for your encouraging remarks. We agree for some conditions our accuracy gain is not as significant. However, the accuracy improvement has been very consistent across most model conditions when area attention is used. We currently simply used the hyperparameters tuned for the baseline models. We believe some tuning of hyperparameters for area attention can result in better accuracy. In our recent experiments, area attention achieved 29.74 for En-De and 41.5 for En-Fr (a larger margin than previously reported in the revision) by simply tuning on the maximum area size. Tuning on other hyperparameters such as attention dropout ratio can further realize its potential. ", "title": "Thanks for your further comments"}, "SkxW5QBK3m": {"type": "review", "replyto": "rygp3iRcF7", "review": "\nI have several concerns about this paper.\n\n[originality]\nSome important related studies are missing.\n\n# Related studies about the perspective of \u201carea\u201d.\nThe consecutive position in sequence is often referred to as \u201cspan\u201d in NLP filed, which is identical to what the authors call \u201carea\u201d in this paper.\nThen, the idea of utilizing spans currently becomes a very popular in NLP field. We can find several papers, \ne.g.,\nWenhui Wang, Baobao Chang, \u201cGraph-based dependency parsing with bidirectional lstm\u201d, ACL-2016.\nMitchell Stern, Jacob Andreas, Dan Klein, \u201cA Minimal Span-Based Neural Constituency Parser\u201d, ACL-2017.\nKenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer, \u201cEnd-to-end Neural Coreference Resolution\u201d, EMNLP-2017.\nNikita Kitaev, Dan Klein, \u201cConstituency Parsing with a Self-Attentive Encoder\u201d, ACL-2018.\n\nSimilarly, there are several related studies in image processing field,\ne,g.,\nMarco Pedersoli, Thomas Lucas, Cordelia Schmid, Jakob Verbeek, \u201cAreas of Attention for image captioning\u201d, ICCV-2017\nQuanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo, \u201cImage Captioning with Semantic Attention\u201d, CVPR-2016.\n\n# Related studies about the perspective of \u201cstructured attention\u201d. \nSeveral papers about structured attention have already been proposed, \ne.g.,\nYoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush. \u201cStructured Attention Networks\u201d, ICLR-2017.\nVlad Niculae, Mathieu Blondel. \u201cA Regularized Framework for Sparse and Structured Neural Attention\u201d, NIPS-2017.\n\n\nI think the authors should explain the relations between their method and the methods proposed in the above listed papers.\n\n\n[significance]\n# Concern about experimental settings\nThe experimental setting for NMT looks unnormal in the community.\nCurrently, most of papers use sentences split in subword units rather than character units. I cannot find a reason to select the character units. I think the authors should report the effectiveness of the proposed method on the widely-used settings.\n\n\n# computational cost\nThe authors should report the actual calculation speed by comparing with the baseline method and the proposed method.\nIn Sec. 2.2, the authors provided the computational cost. \nI feel that the cost of O(|M|A) is still enough large and that can unacceptably damage the actual calculation speed of the proposed method.\n\n\n\nOverall, the proposed method itself seems to be novel and interesting.\nHowever, in my opinion, writing and organization of this paper should be much improved as a conference paper. I feel like the current status of this paper is still ongoing to write.\nThus, it is a bit hard for me to strongly recommend this paper to be accepted. \n\n\n\n", "title": "Some important related studies are missing.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HygeYLjd27": {"type": "review", "replyto": "rygp3iRcF7", "review": "[Summary]\nPaper \u201cAREA ATTENTION\u201d extends the current attention models from word level to \u201carea level\u201d, i.e., the combination of adjacent words. Specifically, every $r_i$ adjacent words are first merged into a new item; next a key and the value for this item is calculated based on Eqn.(3 or 7) and Eqn. (4), and then the conventional attention models are applied to these new items. The authors work on (char level) NMT and image captioning to verify the algorithm. \n\n[Details]\n1.\tIn the abstract, \u201c\u2026 Using an area of items, instead of a single, we hope attention mechanisms can better capture the nature of the task \u2026\u201d, can you provide an example to show why \u201can area of items\u201d can \u201cbetter capture the nature of the task\u201d? In particular, you need to show why the conventional attention mechanism fails.\n2.\tIn this new proposed framework, how should we define the query for each area including multiple items like words? For example, in Figure 1, what is the query for $n$-item areas where $n=1,2,3$.\n3.\tTwo different kinds of keys are proposed in Eqn. (3) and Eqn. (7). Any comparison between them?\n4.\tI am not convinced by the experimental results.\n(4a) On WMT\u201914 En-to-Fr and En-to-De, we know that \u201ctransformer_big\u201d can achieve better results than the three settings shown in Table 1 & 2. The results of using transformer_big are not reported. Besides, it is not necessary to use the \u201ctiny\u201d setting for En-to-{De, Fr} translation considering the data size.\n(4b) It is widely adopted to use token-level neural machine translation. It is not convincing to work on char-level NMT only. Also, please provide the results using transformer_big setting.\n(4c) There are no BLEU scores for the LSTM setting. Note that comment (4b) and (4c) are also pointed by anonymous readers.\n(4d) It is really strange for me to \u201ctrained on COCO and tested on Flickr\u201d (See the title of Table 4). It is not a common practice in image captioning literature. Even if in (Soricut et al., 2018), the authors report the results of training of COCO and test on COCO (the Table 5). Therefore, the results are not convincing. You should train on COCO and test on COCO too.\ne.\tWhat if we use different area size? I do not find the study in this paper.\n\n[Pros & Cons]\n(+) A new attempt of the attention model that tries to build the attention beyond unigrams.\n(-) Experiments are not convincing.\n(-) The motivation is not strong.\n", "title": "Experiments are not convincing", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rkeQcjb9AX": {"type": "rebuttal", "replyto": "HygeYLjd27", "comment": "Thank you for your detailed comments. We have carefully addressed all your questions in the revision. Please see the Summary of Changes and the revised paper for details. We here respond to each of your questions. \n\nRe: \"1. can you provide an example to show why \u201can area of items\u201d can \u201cbetter capture the nature of the task\u201d? In particular, you need to show why the conventional attention mechanism fails.\"\n\nWe miscommunicated our motivation in the original paper. Our motivation is to allow a model to attend to information with varying granularity. Conventional attention mechanisms attend to items with a predefined and fixed granularity, e.g., a word piece of a character in translation or a cell in a fixed grid for image captioning. With area attention, the unit of attention can be a combination of a group of adjacent items. For example, multiple cells in an image grid can make a car that can be more efficient for the word \u201ccar\u201d in the caption to attend to. We have clarified our motivation throughout the paper.\n\nRe: \"2. In this new proposed framework, how should we define the query for each area including multiple items like words? For example, in Figure 1, what is the query for $n$-item areas where $n=1,2,3$.\"\n\nThere is no special requirement on queries. A query can be the same as in a regular attention. For example, a word in a target sentence can be a translation of a multi-word phrase (an area) in a source sentence. \n\nRe: \"3. Two different kinds of keys are proposed in Eqn. (3) and Eqn. (7). Any comparison between them?\"\n\nWe added a comparison of these two forms of keys in Token-Level Translation Tasks (see Table 1 & 2). Overall, keys with feature combination (Eqn.5-9) generally performs better than the basic form Eqn.3, although the basic form already outperforms the baselines in most conditions.\n\nRe: \"(4a) (4b)\"\n\nWe added Token-Level Translation tasks in the revised paper (see Section 4.1). We also added a comparison with \u201cTransformer_Big\u201d. Again, area attention consistently improves these models over all the conditions. In particular, area attention achieved BLEU 29.68 on EN-DE that improved upon the previous result of BLEU 28.4 reported for Transformer Big in the original Transformer paper. For EN-FR, area attention also outperformed the baseline, although it didn\u2019t match the previous result on EN-FR. We think it\u2019s due to the use of a different batch size and training steps.\n \nRe: \"(4c) There are no BLEU scores for the LSTM setting. Note that comment (4b) and (4c) are also pointed by anonymous readers.\"\n\nWe reported the BLEU scores for LSTM as well. All the models are tested for both token and character-level translation tasks.\n\nRe: \"(4d) It is really strange for me to \u201ctrained on COCO and tested on Flickr\u201d (See the title of Table 4). It is not a common practice in image captioning literature. Even if in (Soricut et al., 2018), the authors report the results of training of COCO and test on COCO (the Table 5). Therefore, the results are not convincing. You should train on COCO and test on COCO too.\"\n\nWe added COCO40 test results (from the official COCO challenge website) in the revised paper (see Table 5). Area attention significantly improved the original model on both CIDEr and ROUGE-L metrics.\n\nRe: \"What if we use different area size? I do not find the study in this paper.\"\n\nThis is a great question. We reported the effect of different area sizes for image captioning tasks, e.g., 2x2 versus 3x3. We explored which layers in Transformer can benefit from area attention and found area attention helps more when it\u2019s used in lower layers. We speculate that this is because each position is well equipped with contextual information due to self attention in latent layers and area attention might not be able to help much. In contrast, the lower layers can benefit a lot from area attention. We also found a large area size does not necessarily improve accuracy. It is worth tuning the max area size as a hyper parameter to achieve even better results for specific problems.", "title": "Responses to Reviewer3's comments"}, "S1xGY8W5CQ": {"type": "rebuttal", "replyto": "H1gTcC3xnQ", "comment": "Thank you very much for your feedback. We have revised the paper substantially and here address each of your concerns.\n\nRe: \u201cAttention mechanisms are designed to focus on a single item\u201d\nThis is indeed miscommunication in our original submission. What we intend to convey is that regular attention mechanisms are designed to focus on individual items, i.e., the granularity of attention is each item. Such granularity is predetermined and fixed, e.g., a character or a word-piece. In contrast, area attention allows a model to attend to information with varying granularity by dynamically grouping adjacent items. For example, it can be a group of adjacent regions on an image that forms an object or a \"super pixel\", or multiple word pieces that form a phrase. The difference with regular attention is that we do not have to decide what the proper unit is. Rather, we let the model pick on the right level of aggregation of items or raw features. Such granularity or aggregation is acquired through learning. We have clarified this point throughout the paper.\n\n# the gains on BLEU and perplexity are limited. \nWe reported BLEU scores on all the translation tasks including those previously with perplexity. We also added COCO40 Official test results for the image captioning tasks. Overall, for translation tasks, while the margin is not as large as we hoped, the accuracy gain with area attention is quite consistent throughout most conditions. Particularly on Token-level EN-DE translation, area attention achieved BLEU 29.68 that improved upon the state of the art results with a big margin. For image captioning, the accuracy gain is significant for both in-domain and out-of-domain tests.\n\nWe have improved the paper in many ways. Please see the complete list of changes we made in the Summary of Changes and the revised paper for details.", "title": "Responses to Reviewer2's comments"}, "BJeGDE-9AQ": {"type": "rebuttal", "replyto": "SkxW5QBK3m", "comment": "Thank you so much for the thorough feedback that really strengthens the paper. We have conducted additional experiments and revised the paper to address these points. We here respond each point in your review.\n\nRe: originality\nThanks for bringing up a great collection of previous works, which helped us better position our work in the literature. We have added the Related Work section to clarify the relationship of area attention with each previous work you suggested.\n\nRe: Concern about experimental settings\nWe added token-level translation experiments as requested. The results are summarized in Table 1 & Table 2. Area attention consistently outperformed regular attention on token-level translation on all the conditions as well. There is particularly a significant accuracy gain on EN-DE tasks.\n\nRe: computational cost\nWe reported the actual calculation speed of area attention in comparison to regular attention for two major model configurations (Transformer Base and Big) in section 4.1.1. Briefly, for Transformer Base model, on 8 NVIDIA P100 GPUs, each training step took 0.4 seconds for Regular Attention, 0.5 seconds for the basic form of Area Attention (Eq.3 & Eq.4), 0.8 seconds for Area Attention using multiple features (Eq.9 & Eq.4). \n\nWe have made a number of other improvements to the paper. Please see the summary of changes and the revised paper for details. ", "title": "Responses to Reviewer1's comments"}, "rygBF0ec0X": {"type": "rebuttal", "replyto": "rygp3iRcF7", "comment": "We thank the anonymous reviewers and readers for their thoughtful feedback and encouraging remarks. We have substantially improved the paper by making the following changes in the revision.\n\n1. Motivation & Goals\nClarified the purpose of area attention throughout the paper. Area attention allows a model to attend to information with learned, varying granularity and levels of aggregation, which is in contrast to existing attention mechanisms focus on items with predetermined fixed granularity. Please read the revision for details.\n\n2. Related Work\nAdded a Related Work section to clarify the relationship of area attention with each previous work suggested by Reviewer1 and readers.\n\n3. Presentation\nIncreased the clarity of the writing throughout the paper, particularly improved the Pseudo code.\n\n4. Experiments\n* Token-Level Translation\nAdded a section for token-level translation experiments as requested by the reviewers and readers. The results are summarized in Table 1 & 2. Area attention consistently outperformed regular attention on token-level translation across all the conditions. In particular, it improved upon the state-of-art result on EN-DE with a significant margin.\n\n* Actual Cost\nReported the actual calculation speed of area attention in comparison to regular attention for two major model configurations (Transformer Base and Big) in section 4.1.1.\n\n* Comparing Area Attention Keys\nAdded a comparison of the two forms of area attention keys: the parameter free version (Eq.3) and the feature combination version (Eq.5-9) on all the token-level translation tasks (see Table 1 & 2).\n\n* BLEU for LSTM\nReported BLEU scores for LSTM translation tasks as well (see Table 2 & 4).\n\n* Transformer Big\nAdded experiments with Transformer Big where area attention has also shown consistent improvements.\n\n* Tests on COCO\nAdded COCO40 official tests for in-domain image captioning (see Table 5). Area attention outperformed the benchmark model with a significant margin.\n", "title": "Summary of Changes"}, "H1gTcC3xnQ": {"type": "review", "replyto": "rygp3iRcF7", "review": "I prefer the idea of using some statistics (such as variances) of multiple items for attention. \nThis direction may lead to better attention units for future works. \n\nI do not fully understand the argument, \"Attention mechanisms are designed to focus on a single item in the entire memory\". \nIn my understanding, the attention formulation has no mathematical bias to focus on a single item. \nI have been working on the enterprise NMT for years, and observed many cases where the attention weights concentrate in a few (not a single), tokens. \nDo you have any comments? \n\nCould you show some concise examples that we really need to attend multiple (adjacent) items to boost the performance? \nFor example, in char-based machine translation case, we can mimic the area attention with the wordpiece + token-wise NMT.  \nFor the image case, the adjacent area looks like a \"super pixel\". \n\nIt is unfortunate to observe that the gains on BLEU and perplexity are limited. \nSince the authors do not provide any statistical tests, or a confidence interval of the scores, \nI cannot be sure these gains are truly significant. \nFrom my experiences +1.0 BLEU score is often insignificant in NMT experiments (BLEU variance is high in general). \n\nSummary\n+ A new variant of attention, allowing attention to asses statistics of multiple items (such as variances) is interesting\n- Claims are not so much convincing for the need of attending multiple adjacent items. \n- Gains in experiments are limited. \n", "title": "A few concerns", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1e0m61nom": {"type": "rebuttal", "replyto": "BJe3HncKoQ", "comment": "Yes. We will make this point clear in the revision. Thanks for these suggestions that strengthen the contribution of our paper.", "title": "Thanks!"}, "r1gjJpJ_oQ": {"type": "rebuttal", "replyto": "ryxUM6rPi7", "comment": "Thanks again for your further comments. Please see our responses here.\n\nRe: #1\nWe did not intend to argue that focusing on a single item is always bad. Rather, area attention is simply to give the model the option to attend to a range of items that are structurally adjacent when needed. Please notice that area attention subsumes single-item attention rather than excluding it. For example, area attention with max_area_size=5 includes all the areas with size 1-5, where area_size=1 is effectively single-item attention. \n\nAgain, we will clarify our point regarding attention convergence in the revision. To answer your question, we did observe that, for example, mean entropy started with 5.0 at the initial stage of training for some layers and then dropped to less than 1.8 within the first 20K iterations. That said, we want to clarify that attention convergence is not the issue we are tackling, which is what it should be doing as you pointed out. Rather, our main motivation with area attention is to give the model more options when attending to items.\n\nRe: #4\nWe have run experiments on token-level translation, and found area attention outperformed the transformer baselines in most conditions as well. In particular, for the Transformer (Base Model) that you are questioning, area attention achieved BLEU scores: 28.17 (ende) 39.22 (enfr). All these BLEU scores are higher than what were previously reported in the Transformer paper (ende: 27.3 and enfr: 38.1). The performance gain is simply achieved by just using the parameter-free version of Area Attention. We will add a section and a table in the revision to report the token-level performance of area attention with Transformer.\n\nRe: #5\nYour example is assuming each item (character) is not carrying its positional information. For Transformer, this is not the case because each item encodes its position in the sequence (see Sec 3.5: Positional Encoding in the Transformer paper https://arxiv.org/pdf/1706.03762.pdf). For LSTM, this is also less of an issue because attention is applied to the output of an LSTM layer which already captures order information to a certain extent.\n\nThat said, we see your point, and we could have used a sequence model to encode each area to directly capture the order of items in the area. However, one important advantage of area attention is that it is fast and takes constant time to compute (key, value) for each area due to the use of summed area table, and its basic form (Eq.3 & 4) is parameter free.\n\nRe: #6\nWe reported perplexity to see the relative trend when area attention is in use, which showed that area attention often improved over regular attention in LSTM. That said, we will report back the BLEU scores for LSTM. \n\nRe: #7\nWe will cite the Image Transformer paper. It uses regular multi-head attention and solves a different task on conditional image generation.\n\nRe: #8\nYes. We will release the code as we implemented area attention directly based on the open source Tensor2Tensor library (https://github.com/tensorflow/tensor2tensor), where benchmark Transformer models and tasks are implemented, which guarantees a solid comparison with regular attention in Transformer.\n\nOverall:\nWe clarify that our motivation with area attention is to give a model more options when attending to items. Area attention subsumes regular-attention rather than excluding it as explained earlier. Even the parameter-free version of area attention has outperformed regular attention on both character-level and token-level translation tasks, and image captioning tasks. We will add new experimental results to the revision.\n", "title": "Response on motivation, token-level performance and other points"}, "H1lSNXPF57": {"type": "rebuttal", "replyto": "BJlczK-Mcm", "comment": "Thank you for bringing up this previous work that is indeed relevant. The paper focused on image captioning and proposed two nice methods for attending to object regions on images, where both use a special network to infer regions to attend. In contrast, our method examines all possible areas with summed area table for fast computation. The basic form of area attention we proposed is parameter free. We also intend to propose area attention as a general mechanism beyond captioning tasks. We will cite and discuss the paper in the revision.", "title": "Will cite & discuss the related work"}, "Sygs7-Y-5X": {"type": "rebuttal", "replyto": "rJxRSjwg9X", "comment": "Thank you for your interest in reading the work. We will make the pseudo code more readable and release the source code that is written in TensorFlow. Our experiments were conducted based on the original Transformer implementation released in Tensor2Tensor (https://github.com/tensorflow/tensor2tensor).", "title": "Code"}, "Bkges0dWqX": {"type": "rebuttal", "replyto": "H1l0Ups3YQ", "comment": "Yes! Thanks for catching this. We will fix it in the revision.", "title": "Good suggestion"}, "B1eyP2OZc7": {"type": "rebuttal", "replyto": "HJxsDJ2jtQ", "comment": "Thank you for bringing up these questions. We briefly clarify them here and will address them further in the revision.\n\nRe: Question #1\nWe agree that the peakiness of softmax depends on data and tasks. Our statement about softmax convergences is mostly empirical, from both previous work and our own observation. For example, the early attention work (https://arxiv.org/pdf/1409.0473.pdf) by Bahdanau, Cho & Bengio showed that it is often a single or very few items that are receiving most attention probability (see Figure 3 in the paper). Similar phenomena were reported in the Transformer work (https://arxiv.org/pdf/1706.03762.pdf) (In Figure 3, each row that uses color density to represent attention distribution is mostly white). In our own experiments, we found the entropy of the attention probability distribution tends to decrease rapidly, which indicates that the attention probability distribution is towards more deterministic rather than evenly distributed as the training proceeds. That said, we will clarify that our statement is empirical and cite the literature.\n\nRe: Question #2\nThanks for pointing out the issue. There should be a transpose over relu(). Alternatively it should be relu(\u00b5i + \u03c3i + ei; \u03b8) x W_d. We will correct this in the revision.\n\nRe: Question #3\nWe explored both standard deviation and variance in the early experiments from which we did not see a noticeable difference. In \u03c6, the sum of the three is projected before ReLU. That said, we agree it makes more sense to use standard deviation rather than variance, and we will run full experiments on standard deviation and report back.\n\nRe: Question #4\nOur motivation to study area attention on character-level instead of token-level comes from the fact that there are simply more areas to attend to on characters. Since many sentences only have a few tokens, it is intuitively less clear why attending to areas should help in that case. Having said that, area attention is a general framework that is applicable in all cases (e.g., image captioning as we presented), in the worst case performing similarly to plain attention. We will run token-level experiments as well and report back.\n\nRe: Question #5\nThe overall attention distribution will be different, even though the representation for that specific area is the same. This is because area attention allows overlapping areas. The change in the order of items will cause these items to be picked up by different areas. For example, assume there is a sequence with six items: A, B, C, D, E, and F. Say Area 1 contains A, B and C; Area 2 contains C and D; and Area 3 contains D, E and F. Reordering C and D in Area 2 will not change Area 2\u2019s representation. However, the reordering will leave Area 1 with A, B and D, and Area 3 with C, E and F.", "title": "Clarifications"}}}