{"paper": {"title": "Deep Biaffine Attention for Neural Dependency Parsing", "authors": ["Timothy Dozat", "Christopher D. Manning"], "authorids": ["tdozat@stanford.edu", "manning@stanford.edu"], "summary": "", "abstract": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with\nbiaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark\u2014outperforming Kiperwasser & Goldberg (2016) by 1.8% and 2.2%\u2014and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.\n", "keywords": ["Natural language processing", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The area chair disagrees with the reviewers and actually thinks this is a very important contribution. This paper has the potential to be have huge impact as it sets a new state of the art on a 20+ year old benchmark with a simple model. The simplicity of the model is what is so impressive, because it resets how people think about syntactic parsing as a task. The contributions in this paper have the potential to unleash a series of model simplifications in a number of areas and the area chair therefore strongly suggests accepting the paper.\n \n It is true that the techniques used in this paper are not new inventions but rather a careful combination of ideas proposed in other places. However, in the area chair's opinion this is a substantial contribution. There are lots of ideas out there and knowing which ideas to pick and how to combine them is very valuable. Showing that a simple model can beat more complicated models advances our understanding much more than a new technique that adds unnecessary additional complexity. The focus on novel models in academia is too big, leading to a proliferation of models that nobody needs."}, "review": {"rJeA5xrSVN": {"type": "rebuttal", "replyto": "S1gMiPAK74", "comment": "This is not a question about the version of the parser model or its results presented in the ICLR 2017 paper, but rather about the second version of the parser run on the CoNLL 2017 UD shared task data in a second CoNLL paper.\n\nNevertheless, we stand by those results and they should be reproducible. However, the code has been edited quite a bit since the shared task, so there certainly could be a problem. I see you've also opened an issue on the Parser-v2 GitHub https://github.com/tdozat/Parser-v2/issues \u2013 which seems a better place for this\u2026.", "title": "Not actually a problem with this paper / version of the system"}, "S1gMiPAK74": {"type": "rebuttal", "replyto": "Hk95PK9le", "comment": "When replicating the results of the CoNLL 2017 shared task from the models here https://nlp.stanford.edu/data/CoNLL17_Parser_Models/ , I ran into an issue as I could not get the same performance as is reported in the paper. My steps in reproducing, I showcase English LinES treebank:\n\n    download ud-treebanks-conll2017 that was distributed among participants of CoNLL 2017 shared task\n    download respective model from https://nlp.stanford.edu/data/CoNLL17_Parser_Models/\n    download test sets ud-test-v2.0-conll2017\n    tag test set file \"en_lines.conllu\" in ud-test-v2.0-conll2017 by the downloaded model \"English-LinES-Tagger\"\n    parse tagged in step 4 test set file \"en_lines.conllu\" located in \"English-LinES-Tagger\" by downloaded model \"English-LinES-Parser\"\n    evaluate by the conll17_ud_eval.py script with the gold data and parsed result.\n\nThe result is different than the one reported in the paper. So what could have gone wrong?", "title": "Issue with replicating the results?"}, "rJXjf6UUe": {"type": "rebuttal", "replyto": "S1iorxrNg", "comment": "Thank you for your generally positive but still realistic feedback!", "title": "Thanks!"}, "ryJw3c37e": {"type": "rebuttal", "replyto": "By8iA97Xe", "comment": "A good read! It looks like that paper proposes essentially the same graph-based parser as Kiperwasser & Goldberg (2016), using bidirectional LSTMs to generate a recurrent vector for each word/tag pair and then training a traditional attention mechanism to predict the right head for each dependent. The main architectural contribution of our paper is an improved (we argue) attention mechanism that Zhang et al (2016) don't explore--but we'll definitely update the paper to cite them!", "title": "Most related to Kiperwasser & Goldberg"}, "By8iA97Xe": {"type": "rebuttal", "replyto": "Hk95PK9le", "comment": "The authors should take a look at this paper, where they used a very similar model\nhttps://arxiv.org/abs/1606.01280\n", "title": "A very Related Work"}, "H1gFdgZQg": {"type": "rebuttal", "replyto": "HyftUlXGg", "comment": "We haven't had a chance to re-run the experiments on the CTB without tag dropout yet, so we don't have evidence for or against that hypothesis at the moment. Another team of researchers ran our model using word embeddings trained on larger corpora and found that the UAS was about the same but the LAS was much better, so it may in fact be that the other hypothesis we put forth--that the word embeddings we used weren't very good--is a bigger factor than we thought.\n\nUpdate: we just re-ran the CTB experiments without tag dropout, and got UAS of 90.03% and LAS of 86.42%, which is more in line with what we'd expect. So dropping gold tags hurts LAS but not UAS.", "title": "We haven't re-run the CTB experiments yet"}, "Hy5TSl-7g": {"type": "rebuttal", "replyto": "BJhal_ozx", "comment": "I believe K&G are discussing something slightly different at that point in the paper--in the previous sections, they describe their model as literally concatenating the head and dependent vectors when doing the matrix multiplications, which can't be efficiently done through vectorization, and requires n^2 multiplications of W * concat(x1, x2). However, because W * concat(x1, x2) can be rewritten as W1x1 + W2x2, we can avoid the quadratic number of multiplications by computing W1x1 and W2x2 separately and then summing them for each token-token pair. This requires n^2 additions but only 2n multiplications, rather than requiring n^2 concatenations and n^2 multiplications. In our discussion of memory complexity, we assume this is vectorized for even more efficiency--after computing W1x1 and W2x2 (which are both (n x d)-dimensional), we expand their first and second dimensions (making them (n x 1 x d)- and (1 x n x d)-dimensional) and do a broadcast sum, creating the (n x n x d)-dimensional vector. Doing this without vectorization wouldn't reduce the memory complexity, because each of the n^2 d-dimensional hidden states would need to be cached for computing the gradient, so the O(n^2*d) memory complexity is unavoidable.", "title": "Some confusion regarding the K&G speed improvement"}, "SyzhlgZXx": {"type": "rebuttal", "replyto": "B1n127GMl", "comment": "You're right that our model has a higher average-case memory complexity because of the label classifier, but the longest sentence in the PTB is about 140 tokens long, giving our approach a better worst-case memory complexity on this dataset. So at both models' highest memory-usage points in training, our model will be more efficient, which is more important when rationing out GPU memory.", "title": "The longest sentence is the bottleneck"}, "rJ7xOJZXl": {"type": "rebuttal", "replyto": "SyAQYRCfg", "comment": "In terms of the model architecture (as opposed to hyperparameters), yes, the main difference is the scorer--they use an MLP that takes as input the concatenation of the dependent's recurrent output and the head's recurrent output and produces as output a linear value representing the score, whereas our model changes the scorer to build distinct representations for the dependent token and the potential head tokens (using MLPs), and then uses a biaffine operation to generate the score. Another small difference between the models is the training objective--K&G use a hinge-loss objective whereas we use traditional softmax cross-entropy objective, but we didn't test to see how much affect this had on performance, and we don't suspect it would have changed much.", "title": "The main architectural difference is the scorer"}, "H1ktB1bXg": {"type": "rebuttal", "replyto": "H1XVc_1Qg", "comment": "Biaffine attention bears a closer relationship to traditional classification with a fixed number of categories. With traditional classification, the vector of scores is defined according to an affine transformation over the input vector x (or r, the topmost recurrent state in our paper): Wx + b, where W is (c x d) and b is (c x 1). In this context, the bias vector b bears a clear relationship between the prior probability of each class P(class), and the linear transformation Wx is closely related to the likelihood of each class given the input, P(class | x). Biaffine attention straightforwardly extends this traditional approach to classification over a variable number of classes (here, words in the sentence). We need W to have the same number of rows as there are classes (that is, it still needs to have shape (c x d), even though we don't know a priori what c is), which we can do by representing each class with a vector z and defining W to be the product of ZU, where Z is (c x d) and U is (d x d). When we do the multiplication, ZU winds up being (c x d), which is what we need in order to make it analogous to fixed-class classification. Similarly, the bias vector b needs to be (c x 1), even when c varies from run to run--we can achieve this using the same matrix Z of class vectors, and multiplying it by a (d x 1) vector w. The resulting vector Zw is (c x 1), which is what we need the bias term to be. When we look at biaffine attention this way, we can see the clear relationship between Wx + b and (ZU)x + (Zw)--the two components (Zw) and (ZU)x both capture P(class) and P(class | x), respectively. In our case, the vector x is the vector representation of the token whose head we want to find, and the matrix of class vectors Z is the representation of each token in the sentence that it can attend to, the number of which changes based on how long the sentence is. The intuition behind the label classifier is similar; we want a scorer with components that clearly relate to P(label) (the prior probability of each label), P(label | head) (the probability of a head taking dependents of a certain type), P(label | dependent) (the probability of a dependent taking a certain class), P(label | head, dependent) (the probability of a label based on properties of both head and dependent taken together). Our label classifier x1Ux2 + W1x1 + W2x2 + b has this property--each term contributes something to the score that clearly correlates with an important part of the probability that we want to capture.\n\nThis shows how biaffine attention can be interpreted as variable-class classification and related to fixed-class classification. What's the relationship between traditional attention and fixed-class classification? With traditional attention, we concatenate the input vector x with a class-specific vector z and feed the result into an MLP (which is the same for all classes) with a single linear output. So, if we wanted to do traditional classification in the style of traditional attention, we would define the score of a class as U*relu(W1x + W2z + b), which simplifies to U*relu(Wx + z) since W2, z, and b don't depend on the input, where z is a class-specific vector but U and W are the same for all classes. While there's no reason this shouldn't work, it's clearly somewhat bizarre and considerably more complex than the alternative of just using Wx + b. It also lacks interpretability in the sense that you can't point to an individual component and tell what part of the probability it contributes to (e.g. there's no single component that clearly captures P(class)).\n\nSo in that sense, biaffine attention is intuitively a simple extension of fixed-class classification to variable-class classification, and comes with the same level of interpretability. I hope that answers your question!", "title": "Relationship between attention and classification"}, "H15N7lxXe": {"type": "rebuttal", "replyto": "rJs6FmGzl", "comment": "You're absolutely right that we probably should have cited Kiperwasser & Goldberg (2016) in the introduction, or even in the abstract, and we'll update this in the next version of the paper. However, we don't claim that this is the first parser that uses attention as a graph-based parser--we mention a couple times that this work is building off of that previous research, and we in fact do discuss Kiperwasser & Goldberg's parser and the relationship between ours and that one in some detail on page 3 of the paper. The discussion of the utility of such attention-based parsers is meant to draw a contrast with transition-based neural parsers, which use parsing-specific mechanisms and which were the standard in the neural parsing literature before KG--it wasn't meant to suggest that using attention for parsing represents a new innovation of the paper. Rather, the main innovations are the new parser and label scorers--which we found to outperform the scorers used in the KG parser--and a discussion of other hyperparameter choices that allowed us to improve on that parser even further and beat the previous SOTA by a fairly wide margin. These hyperparameter choices could be useful for other researchers looking to improve on ours or the KG parser as well.", "title": "We cite and discuss Kiperwasser & Goldberg (2016)"}, "H1XVc_1Qg": {"type": "review", "replyto": "Hk95PK9le", "review": "Other than the computational benefits of the biaffine attention, why should I expect it to work better than the traditional attention mechanism. In other words, what is the intuition? Is the learned attention interpretable?This is primarily an engineering paper. The authors find a small architectural modification to prior work and some hyperparameter tuning which pushes up the state-of-the-art in dependency parsing in two languages.\n\nThe architecture modification is a biaffine attention mechanism, which was inspired work in neural machine translation by Luong et al. (2015). The proposed attention model appears to be a win-win: better accuracy, reduced memory requirements, and fewer parameters.\n\nThe performance of the model is impressive, but how the performance is achieved is not very impressive. I do not believe that there are novel insights in the paper that will generalize to other tasks, nor does the paper shed light on the dependency parsing tasks (e.g., does biaffine attention have a linguistic interpretation?).\n", "title": "Intuition for biaffine attention", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJJU3DWEl": {"type": "review", "replyto": "Hk95PK9le", "review": "Other than the computational benefits of the biaffine attention, why should I expect it to work better than the traditional attention mechanism. In other words, what is the intuition? Is the learned attention interpretable?This is primarily an engineering paper. The authors find a small architectural modification to prior work and some hyperparameter tuning which pushes up the state-of-the-art in dependency parsing in two languages.\n\nThe architecture modification is a biaffine attention mechanism, which was inspired work in neural machine translation by Luong et al. (2015). The proposed attention model appears to be a win-win: better accuracy, reduced memory requirements, and fewer parameters.\n\nThe performance of the model is impressive, but how the performance is achieved is not very impressive. I do not believe that there are novel insights in the paper that will generalize to other tasks, nor does the paper shed light on the dependency parsing tasks (e.g., does biaffine attention have a linguistic interpretation?).\n", "title": "Intuition for biaffine attention", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyAQYRCfg": {"type": "review", "replyto": "Hk95PK9le", "review": "My main question is the difference between the authors approach with K&G. The score for the arc (computed from parent child bilinear form) is used the same way in K&G?The paper brings the new STOA in PTB dependency parsing. The numbers are very impressive.\n\nBuilt upon the framework of K&G parser, this improvement is achieved by mainly two things -- (1) the paper replace the original scorer using bilinear scorer and make a difference between the head of modifier representation (2) the hyperparameter tuning in the ADAM trainer.\n\nAlthough I think the bilinear modification make some sense intuitively, I don't think this contribution alone is strong enough for a conference publication. The authors did not show a good explanation of why this approach works better in this case nor did the author show this modification is generally applicable in any other tasks. ", "title": "questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ry1J89-4l": {"type": "review", "replyto": "Hk95PK9le", "review": "My main question is the difference between the authors approach with K&G. The score for the arc (computed from parent child bilinear form) is used the same way in K&G?The paper brings the new STOA in PTB dependency parsing. The numbers are very impressive.\n\nBuilt upon the framework of K&G parser, this improvement is achieved by mainly two things -- (1) the paper replace the original scorer using bilinear scorer and make a difference between the head of modifier representation (2) the hyperparameter tuning in the ADAM trainer.\n\nAlthough I think the bilinear modification make some sense intuitively, I don't think this contribution alone is strong enough for a conference publication. The authors did not show a good explanation of why this approach works better in this case nor did the author show this modification is generally applicable in any other tasks. ", "title": "questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJhal_ozx": {"type": "rebuttal", "replyto": "B1n127GMl", "comment": "I think the claim and comparison of memory complexity with traditional approach(Kiperwasser & Goldberg (2016)) may be wrong.\nAs mentioned in Kiperwasser & Goldberg (2016) section 5 under \"Speed improvements\", the memory complexity should be O(n^2 + d*n).\nConsidering labels, the complexity becomes O(c*n^2 + d*c*n). While, because of storing tensor U, the complexity of the proposed biaffine \nshould be O(d^2 + n^2 + d*n) without labels, and O(c*d^2 + c*n^2 + d*c*n) with labels.\nSince d is often much larger than n in practice, the memory complexity of the proposed method is larger than the traditional method.\nAm I right?", "title": "Memory Complexity"}, "rJs6FmGzl": {"type": "review", "replyto": "Hk95PK9le", "review": "I think that what proposed in the paper is an extension of Kiperwasser & Goldberg (2016). The main (and only?) difference is how to score word pairs. So the point of \"using general-purpose neural network components...\" (abstract) is not innovative at all. In fact, the authors should cite to K&G paper for this point. The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing. The proposed function is biaffine in the sense that it's a combination of a bilinear score function and a bias term playing a role as prior. The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB.\n\nThe paper is very well written with impressive experimental results and analysis. However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015). Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model \"deeper\", whereas adding linear bias terms isn't confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine). \n\nI think that this paper is more proper for NLP conferences. ", "title": "related to Kiperwasser & Goldberg (2016)", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1iorxrNg": {"type": "review", "replyto": "Hk95PK9le", "review": "I think that what proposed in the paper is an extension of Kiperwasser & Goldberg (2016). The main (and only?) difference is how to score word pairs. So the point of \"using general-purpose neural network components...\" (abstract) is not innovative at all. In fact, the authors should cite to K&G paper for this point. The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing. The proposed function is biaffine in the sense that it's a combination of a bilinear score function and a bias term playing a role as prior. The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB.\n\nThe paper is very well written with impressive experimental results and analysis. However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015). Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model \"deeper\", whereas adding linear bias terms isn't confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine). \n\nI think that this paper is more proper for NLP conferences. ", "title": "related to Kiperwasser & Goldberg (2016)", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}