{"paper": {"title": "YellowFin and the Art of Momentum Tuning", "authors": ["Jian Zhang", "Ioannis Mitliagkas", "Christopher Re"], "authorids": ["zjian@cs.stanford.edu", "ioannis@iro.umontreal.ca", "chrismre@cs.stanford.edu"], "summary": "YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.", "abstract": "Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.", "keywords": ["adaptive optimizer", "momentum", "hyperparameter tuning"]}, "meta": {"decision": "Reject", "comment": "This paper asks when SGD+M can beat adaptive methods such as Adam, and then suggests a variant of SGD+M with an adaptive controller for a single learning rate and momentum parameter.  There is are comparisons with some popular alternatives.  However, the bulk of the paper is concerned with a motivation that didn't convince any of the reviewers."}, "review": {"ryUK51INM": {"type": "rebuttal", "replyto": "BJsMnO6mG", "comment": "Dear Carlos,\n\nThanks for the clarification. Rescaling the learning rate as per your suggestion, on multiple experiments, has the following consequences:\n\n-- As expected, training is sped-up compared to using YF\u2019s LR un-adjusted and just setting momentum to 0. In some cases, like the one you conduct experiment on, YF is only marginally better (or the same) than YF-minus-momentum-plus-rescaling. \n\n-- Signs of instability start showing. Essentially, trading momentum for higher learning rate when we do the suggested rescaling, makes the performance curves behave more unstable (figures below).\n\n\nOverall, in a number of examples, the original, momentum based YF tuner demonstrates better validation metrics and better stability than the suggested rescaling rule with zero momentum. \n\nE.g. YellowFin rule with non-zero momentum can demonstrate better validation perplexity in the constituency parsing model. (https://github.com/AnonRepository/YellowFin_Pytorch/blob/master/plots/parsing_test_perp.pdf).\n\nIn the following example of ResNext on CIFAR10, YellowFin rule with non-zero momentum can also demonstrate more stable validation accuracy (https://github.com/AnonRepository/YellowFin_Pytorch/blob/master/plots/cifar_smooth_test_acc.pdf). \n\nAs the third example, using the momentum-based tuner, we are able to boost the learning rate (as described in Appendix J4) to get even better performance. In this example, we use learning rate factor 3.0 to increase the learning rate on ResNext for CIFAR10 (as what we did in the experiments in appendix J4), YellowFin rule with non-zero momentum gives *both* observably higher and more stable validation accuracy than the suggested rescaling rule (https://github.com/AnonRepository/YellowFin_Pytorch/blob/master/plots/cifar_smooth_test_acc_lr_fac_3.pdf ).\n\nWe appreciate the feedback on this important point! We will be adding this discussion to our manuscript and, would also be happy to add an acknowledgement for your suggestion.\n\nBest regards,\nThe authors\n", "title": "YellowFin with non-zero momentum can demonstrate better and more stable performance"}, "HyuhIWYez": {"type": "review", "replyto": "SyrGJYlRZ", "review": "This paper proposes a method to automatically tuning the momentum parameter in momentum SGD methods, which achieves better results and fast convergence speed than state-of-the-art Adam algorithm.\n\nAlthough the results are promising, I found the presentation of this paper almost inaccessible to me.\n\nFirst, though a minor point, but where does the name *YellowFin* come from?\n\nFor the presentation, the motivation in introduction is fine, but the following section about momentum operator is hard to follow. There are a lot of undefined notation. For example, what does the *convergence rate* mean (what is the measurement for convergence)? And is the *optimal accelerated rate* the same as *convergence rate* mentioned above? Also, what do you mean by *all directions* in the sentence below eq.2?\n\nThen the paper talks about robustness properties of the momentum operator. But: first, I am not sure why the derivative of f(x) is defined as in eq.3, how is that related to the original definition of derivative?\n\nIn the following paragraph, what is *contraction*? Does it have anything to do with the paper as I didn't see it in the remaining text?\n\nLemma 2 seems to use the spectral radius of the momentum operator as the *robustness*. But how can it describe the robustness? More details are needed to understand this.\n\nWhat it comes to Section 3, it seems to me that the authors try to use a local quadratic approximation for the original function f(x), and use the results in last section to find the optimal momentum parameter. I got confused in this section because eq.9 defines f(x) as a quadratic function. Is this f(x) the original function (non quadratic) or just the local quadratic approximation? If it is the local quadratic approximation, how is it correlated to the original function? It seems to me that the authors try to say if h and C are calculated from the original function, then this f(x) is a local quadratic approximation? If what I think is correct, I think it would be important to show this.\n\nAlso, the objective function in SingleStep algorithm seems to come from eq.13, but I failed to get the exact reasoning.\n\nOverall, I think this is an interesting paper, but the presentation is too fuzzy to get it evaluated.", "title": "YellowFin and the Art of Momentum Tuning", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1RZJ1cxG": {"type": "review", "replyto": "SyrGJYlRZ", "review": "The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin). They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications.\n\n\nI found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading:\nBased on the analysis of 1-dimensional problems, the authors design a framework and an algorithm that  supposedly ensures accelerated convergence. There are two major problems with this approach:\n\n-First: Exploring 1-dim functions is indeed a nice way to get some intuition. Yet,  algorithms that work in the 1-dim case do not trivially generalize to high dimensions, and such reasoning might lead to very bad solutions.\n\n-Second: Accelerated GD does not benefit over GD in the 1-dim case. And therefore, this is not an appropriate setting to explore acceleration.\nConcretely, the definition of the generalized condition number $\\nu$, and relating it to the standard definition of the condition number $\\kappa$, is very misleading. This is since $\\kappa =1$ for 1-dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case.\nHowever, $\\nu$ might be much larger than 1 even in the 1-dim case.\n\n\nRegarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension).\nAnd as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions.\nThus, it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed.\nThe authors mention that their experiments were done without tuning or with very little tuning, which is very mysterious for me.\n\nIn contrast to the theoretical part, the experiments seems very encouraging. Showing YF to perform very well on several deep learning tasks without (or with very little) tuning. Again, this seems a bit magical or even too good to be truth. I suggest the authors to perform a experiment with say a qaudratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition.\n", "title": "Misleading and shaky theoretical motivation/approach", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJ0CHgbbM": {"type": "review", "replyto": "SyrGJYlRZ", "review": "[Apologies for short review, I got called in late. Marking my review as \"educated guess\" since i didn't have time for a detailed review]\n\nThe paper proposes an algorithm to tune the momentum and learning rate for SGD. While the algorithm does not have a theory for general non-quadratic functions, experimental validation is extensive, making it a worthy contribution in my opinion. I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here.", "title": "Worthy contribution", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "SkdSe3zNz": {"type": "rebuttal", "replyto": "SyrGJYlRZ", "comment": "Dear readers and reviewers,\n\nwe have updated our manuscript during the rebuttal period in addition to our response to the official reviews. Specifically, we:\n\n1. performed a significant rewrite of Sections 2 and 3 to make exposition of our ideas much more clear.\n\n2. we added discussion in section 3.1 on how our tuning rule in equ(8) intuitively generalizes to multiple dimensions.\n\n3. we addressed a number of reviewers comments and suggestions on clarification. \n\nBest regards,\nThe authors", "title": "Updated manuscript"}}}