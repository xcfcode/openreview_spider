{"paper": {"title": "Online Learning Rate Adaptation with Hypergradient Descent", "authors": ["Atilim Gunes Baydin", "Robert Cornish", "David Martinez Rubio", "Mark Schmidt", "Frank Wood"], "authorids": ["gunes@robots.ox.ac.uk", "rcornish@robots.ox.ac.uk", "david.martinez2@wadh.ox.ac.uk", "schmidtm@cs.ubc.ca", "fwood@robots.ox.ac.uk"], "summary": "", "abstract": "We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "All reviewers agreed that, despite the lack of novelty, the proposed method is sound and correctly linked to existing work. As the topic of automatically learning the stepsize is of great practical interest, I am glad to have this paper presented as a poster at ICLR."}, "review": {"H1pbs28kG": {"type": "review", "replyto": "BkrsAzWAb", "review": "SUMMARY:\n\nThe authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent. The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique.\n\n\nGENERAL IMPRESSION:\n\nOne central problem of the paper is missing novelty. The authors are well aware of this. They still manage to provide added value.\nDespite its limited novelty, this is a very interesting and potentially impactful paper. I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods.\n\n\nCRITICISM:\n\nThe experimental evaluation is rather solid, but not perfect. It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum. However, it is not clear why the method is tested only on a single data set: MNIST. Since it is entirely general, I would rather expect a test on a dozen different data sets. That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \\alpha_0 and \\beta.\n\nThe extensions in section 5 don't seem to be very useful. In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem. Analyzing the actual adaptive algorithm would be very interesting. In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method.\n\n\nMINOR POINTS:\n\npage 4, bottom: use \\citep for Duchi et al. (2011).\n\nNone of the figures is legible on a grayscale printout of the paper. Please do not use color as the only cue to identify a curve.\n\nIn figure 2, top row, please display the learning rate on a log scale.\n\npage 8, line 7 in section 4.3: \"the the\" (unintended repetition)\n\nEnd of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?\n", "title": "good, but not perfect", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1jLC23Jf": {"type": "review", "replyto": "BkrsAzWAb", "review": "The authors consider a method (which they trace back to 1998, but may have a longer history) of learning the learning rate of a first-order algorithm at the same time as the underlying model is being optimized, using a stochastic multiplicative update. The basic observation (for SGD) is that if \\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t), then \\partial/\\partial\\alpha f(\\theta_{t+1}) = -<\\nabla f(\\theta_t), \\nabla f(\\theta_{t+1})>, i.e. that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update w.r.t. the learning rate \\alpha.\n\nI have seen this before for SGD (the authors do not claim that the basic idea is novel), but I believe that the application to other algorithms (the authors explicitly consider Nesterov momentum and ADAM) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization).\n\nThe experiments are well-presented, and appear to convincingly show a benefit. Figure 3, which explores the robustness of the algorithms to the choice of \\alpha_0 and \\beta, is particularly nicely-done, and addresses the most natural criticism of this approach (that it replaces one hyperparameter with two).\n\nThe authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one. This appears to be a promising approach, and bringing it back to the attention of the machine learning community is valuable.", "title": "Somewhat weak novelty, but well written, complete, and potentially impactful.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJ6v0V9ef": {"type": "review", "replyto": "BkrsAzWAb", "review": "\nThis paper revisits an interesting and important trick to automatically adapt the stepsize. They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize. Such simple trick alleviates the effort in tuning stepsize, and can be incorporated with popular stochastic first-order optimization algorithms, including SGD, SGD with Nestrov momentum, and Adam. Surprisingly, it works well in practice.\n\nAlthough the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick, considering their performance, I vote for acceptance. But before that, there are several issues need to be addressed. \n\n1, the derivation of the update of \\alpha relies on the expectation formulation. I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick. \n\n2, The derivation of the multiplicative rule of HD relies on a reference I cannot find. Please include this part for self-containing. \n\n3, As the authors claimed, the Maclaurin et.al. 2015 is the most related work, however, they are not compared in the experiments. Moreover, the empirical comparisons are only conducted on MNIST. To be more convincing, it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet. \n\nMinors: \n\nIn the experiments results figures, after adding the new trick, the SGD algorithms become more stable, i.e., the variance diminishes. Could you please explain why such phenomenon happens?", "title": "interesting idea, but weak experiments", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hy8WTMFmf": {"type": "rebuttal", "replyto": "H1pbs28kG", "comment": "> One central problem of the paper is missing novelty. The authors are well aware of this. They still manage to provide added value. Despite its limited novelty, this is a very interesting and potentially impactful paper.  I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods.\n\nThank you very much for your evaluation and encouraging words.\n\n> The experimental evaluation is rather solid, but not perfect. It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum. However, it is not clear why the method is tested only on a single data set: MNIST. Since it is entirely general, I would rather expect a test on a dozen different data sets. That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \\alpha_0 and \\beta.\n\nPlease note that we provide experimental evaluation on a non-MNIST data set, specifically CIFAR-10 (Section 4.3 on page 8 and Figure 2 on page 7).\n\n> The extensions in section 5 don't seem to be very useful. In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem. Analyzing the actual adaptive algorithm would be very interesting. In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method.\n\nWe agree with your assessment that the analysis in Section 5.1 is significantly restricted and this is a limitation of the current paper. There remains much to be done in this respect, and a theoretical convergence analysis is a highly desired future work. Please note that a convergence analysis of the technique in the multidimensional quadratic case is available in a separate work, which we will highlight prominently in the de-anonymized final revision of the paper.\n\n> MINOR POINTS\n\nThank you for pointing these out, we will fix them in the final revision.\n", "title": "Thank you"}, "S1WP2GFQz": {"type": "rebuttal", "replyto": "r1jLC23Jf", "comment": "> I have seen this before for SGD (the authors do not claim that the basic idea is novel), but I believe that the application to other algorithms (the authors explicitly consider Nesterov momentum and ADAM) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization). \n\n> The experiments are well-presented, and appear to convincingly show a benefit. Figure 3, which explores the robustness of the algorithms to the choice of \\alpha_0 and \\beta, is particularly nicely-done, and addresses the most natural criticism of this approach (that it replaces one hyperparameter with two).\n\nThank you very much for your evaluation and your encouraging feedback.\n\nFigure 3 was produced with exactly the purpose that you described, and we are very glad that this was noticed and found useful.\n\n> The authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one. This appears to be a promising approach, and bringing it back to the attention of the machine learning community is valuable.\n\nWe agree that a theoretical convergence analysis is a highly desired future work and is a limitation of the current paper. We also agree with the assessment that the approach appears promising and therefore we would like to bring it to the attention of the larger community.\n", "title": "Thank you"}, "HJcZnfFXM": {"type": "rebuttal", "replyto": "BJ6v0V9ef", "comment": "Thank you for your encouraging evaluation and for the improvements suggested.\n\n> 1, the derivation of the update of \\alpha relies on the expectation formulation. I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick.\n\nWe do not have theoretical results about the effect of the minibatch size and gradient variance on the hypergradient descent (HD) algorithm. Considering that the reviewer was potentially referring to experimental evidence, we will make sure to include experimental results with varying minibatch sizes in an appendix in the final revision of this paper.\n\n> 2, The derivation of the multiplicative rule of HD relies on a reference I cannot find. Please include this part for self-containing.\n\nThank you for pointing this out. The mentioned reference for the multiplicative HD rule is now made accessible online, and can be located with a Google search of the title.\n\n> 3, As the authors claimed, the Maclaurin et.al. 2015 is the most related work, however, they are not compared in the experiments. Moreover, the empirical comparisons are only conducted on MNIST. To be more convincing, it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet.\n\nAs you point out, Maclaurin et al. (2015) is a highly related work, which introduces the term \u201chypergradient\u201d and similarly performs gradient-based updates of hyperparameters through a reversible higher-order automatic differentiation setup. \n\nHowever, note that in the approach in Maclaurin et al. (2015) a regular optimization procedure is truncated to a fixed number N of \u201celementary\u201d iterations (such as N = 100 in the paper), at the end of which the derivative of an objective is propagated all the way through this N inner optimization iterations (the \u201creversibility\u201d trick introduced in the paper is for making this possible in practice), and the resulting hypergradient is used in an outer optimization of M \u201cmeta\u201d iterations (such as M=50 in the paper). Our technique, in contrast, is an online adaptation of a hyperparameter (in particular, the learning rate) at each iteration of optimization, and does not perform derivative propagation through an inner optimization that consists of many iterations. The techniques are thus not directly comparable as competing alternatives. For instance, it is not straightforward to replicate our learning rate trajectory through the VGGNet/CIFAR-10 experiment of 78125 iterations (Figure 2 on page 7, rightmost column) in the reversible learning algorithm due to (1) uninformative gradients beyond a few hundred iterations (see Section 4 \u201cLimitations\u201d in Maclaurin et al. 2015) and (2) potentially prohibitive memory requirements. Having said this, we believe that it would be interesting to compare the behavior of our algorithm for the initial 100 iterations with the 100-iteration learning-rate schedules reported in Maclaurin et al. (2015) and we intend to add such an experiment in the appendix in the final revision of the paper.\n\n> Moreover, the empirical comparisons are only conducted on MNIST. \n\nPlease note that the paper does report non-MNIST empirical comparisons, specifically CIFAR-10 (Section 4.3 on page 8 and Figure 2 on page 7).\n\n> Minors: In the experiments results figures, after adding the new trick, the SGD algorithms become more stable, i.e., the variance diminishes. Could you please explain why such phenomenon happens?\n\nAs far as we can observe, the variance does not diminish, and the method behaves in a similar way to how regular SGD does with a good choice of the learning rate, as for example 10e-2 in the case of logistic regression. We would be interested in looking into this more carefully if you could point us to an experiment/figure where this behavior with SGD happens.\n\nThank you once more for all these constructive comments and suggested additions that allow us to improve the paper.\n", "title": "Thank you"}, "H1PN17VXz": {"type": "rebuttal", "replyto": "B1wQhWzGM", "comment": "Thank you very much for your time and for reporting your results. This sort of validation is extremely valuable for us and the community.\n\nFollowing the decision notification, we will make a repository public with the full code in Python (including the plotting codes that we used for producing the plots in the paper). We will also add information about the hardware setup that was used for running the presented experiments.", "title": "Thank you"}, "BydQzcHxf": {"type": "rebuttal", "replyto": "r1HU3l1kf", "comment": "Hi, both are very interesting potential applications! \n\nI think an application to non-stationary data, where the learning rate varies on the fly as new data comes in, would be very interesting indeed. We will keep this in mind. \n\nWe're also looking at adaptive filter theory.\n\nThank you very much for the pointers.", "title": "Thank you"}, "S1sZVWMlz": {"type": "rebuttal", "replyto": "rkaXMT-lz", "comment": "You only need a logaritmic number of iterations to shift your current learning rate to another value, instead of a linear number of them. We have also seen in practice that with good hyperparameters for both implementations, the multiplicative rule adapts faster. There is also a theoretical reason that comes from the formal derivation of the rule that suggests that the multiplicative rule makes more sense than the additive one.", "title": "why multiplicative adaptation is in general faster than the additive adaptation\uff1f"}}}