{"paper": {"title": "JAKET: Joint Pre-training of Knowledge Graph and Language Understanding", "authors": ["Donghan Yu", "Chenguang Zhu", "Yiming Yang", "Michael Zeng"], "authorids": ["~Donghan_Yu2", "~Chenguang_Zhu1", "~Yiming_Yang1", "~Michael_Zeng1"], "summary": "A joint pre-training framework which models both the knowledge graph and text and can easily adapt to unseen knowledge graphs in new domains during fine-tuning", "abstract": "Knowledge graphs (KGs) contain rich information about world knowledge, entities, and relations. Thus, they can be great supplements to existing pre-trained language models. However, it remains a challenge to efficiently integrate information from KG into language modeling. And the understanding of a knowledge graph requires related context. We propose a novel joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text while the language module generates context-aware initial embeddings for entities and relations in the graph. Our design enables the pre-trained model to easily adapt to unseen knowledge graphs in new domains. Experimental results on several knowledge-aware NLP tasks show that our proposed framework achieves superior performance by effectively leveraging knowledge in language understanding.", "keywords": ["Pre-training", "Knowledge Graph", "Language Understanding", "Graph Neural Network"]}, "meta": {"decision": "Reject", "comment": "Four knowledgeable referees reviewed this paper; one reviewer (weakly) supports accept and other three indicate reject. Even with the rebuttal, all reviewers (including positive reviewer) have concerns on unconvincing experimental results (due to missing baselines for instance). I basically agree on negative reviews that this submission fails to have enough quality considering the high standard of ICLR."}, "review": {"LvIG7D9u75L": {"type": "review", "replyto": "SOVSJZ9PTO7", "review": "This paper presents an approach to jointly pre-train language models and representations for knowledge graphs. In particular, natural language texts (English Wikipedia) are used to train context representations, while knowledge graphs (Wikidata) train entity representations (and both depend on each other). Experiments show that the approach outperforms baseline methods on several natural language understanding tasks: few-shot relation classification, knowledge graph question answering, and entity classification. \n\nThe presented approach looks very promising, however, it also leaves several doubts. One natural ablation question is whether we could simply include the knowledge graph as plain text in training only a language model (via a similar pre-training and fine-tuning)? So, whether the additional hybrid structure involving graph convolution networks is actually necessary? (Maybe the gain in accuracy is only due to the additional information that is available in the knowledge graph?) Another natural question is why the authors have not made further experiments on other datasets and knowledge graphs? Does this approach only work for English Wikipedia and Wikidata (maybe because these two are matching extraordinarily well)? Finally, I would have expected further experimental comparisons to related approaches.      \n\nAfter rebuttal: I'm also still not convinced by the experimental evaluation. For this reason, I slightly downgraded my overall rating.", "title": "An interesting paper", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "c1qNlfvgadK": {"type": "rebuttal", "replyto": "K23eeH5o90D", "comment": "**4. About comparison to existing knowledge-enhanced language models**\n\nWe conducted an additional experiment on the FewRel dataset using one strong knowledge-enhanced pre-training model KnowBERT [1], which is also pretrained on English Wikipedia corpora and Wikidata KG. We fine-tune the pre-trained model provided in its official codebase. The result below demonstrates the effectiveness of our proposed co-training framework. We\u2019ve included the result in the revised version of our paper.\n\n| FewRel   | 5-way-1-shot | 5-way-5-shot | 10-way-1-shot |\n|----------|--------------|--------------|---------------|\n| KnowBERT |     86.2     |     90.3     |      77.0     |\n| JAKET    |     87.4     |     92.1     |      78.9     |\n\n[1] Knowledge enhanced contextual word representations (EMNLP 2019)", "title": "Continued response to Reviewer 1"}, "K23eeH5o90D": {"type": "rebuttal", "replyto": "dknl2hx5_CD", "comment": "Dear Reviewer 1, we really appreciate your valuable feedback.\n\nFirst, we want to summarize the main contributions of our paper:\n\n1. We propose a novel knowledge-language co-pretraining framework, JAKET, where a knowledge module and a language module mutually assist each other for more effective semantic analysis.\n2. We use a two-stage language model to solve the cyclic dependency problem between two modules.\n3. We employ an entity context embedding memory with scheduled updates which speeds up the pre-training by 15x.\n4. JAKET can easily adapt to unseen knowledge graphs in the finetuning phase.\n5. Experiments show that JAKET outperforms strong baseline methods on the tasks of few-shot relation classification, question answering, and entity classification.\n\nHere are the replies to each of your question/comment:\n\n**1. For the question \u201cHow would fine-tuning work on knowledge graphs that do not have entity descriptions?\u201d:**\n\nThere are multiple ways to obtain entity descriptions: find related entries in Wikidata (https://www.wikidata.org/) or Wiktionary (https://www.wiktionary.org/); retrieve sentences in the corpus containing the entity. In any case, the goal is to obtain better entity representations by placing them into context. Finally, the backup is to use the entity name itself. \n\n**2. For the question \u201cWhat does M in RoBERTa+GNN+M stand for? Is it memory?\u201d:**\n\t\nYes, M stands for Memory. More precisely, it means that the memories for entity and relation context embedding are computed by RoBERTa on their description text. In comparison, the memories of RoBERTa+GNN are randomly initialized.\n\n**3. For the comment on \u201ccomputational overhead that adding a KG module\u201d:**\n\nGood suggestion! The computation of the KG module is much less than the LM module. For BERT-base or RoBERTa-base, the number of inference computation flops (#flops) over each sequence (length N=128) is over 22 billion [1, 2]. Here, we compute the number of flops of the KG module:   \nThe sequence length is N=128, and the hidden dimension is H=768. The number of entities in a sequence is usually less than N/5. The number of sampled neighbors per entity is r=10. And the number of layers of the GNN based KG module is L=2. \nIt follows that the #flops of KG module is about N/5 \\* r^L \\* 2H^2 \u2248 3 billion, less than 1/7 of LM computation. If we set r=5, the #flops can be further reduced to about 1/30 of LM computation.     \n\nDuring pre-training, another computation overhead is entity context embedding memory update (Sec. 3.5): \nFirstly, the number of entities is about 3 million and the update step interval is 500. Thus for each step on average the model processes the description text of 3e6/500=6e3 entities. Secondly, the length of description text is 64, much smaller than the length of input text 512, and we only use LM1 (the first half of LM module) for entity context embedding generation, which saves half of the computation time compared to using the whole LM module. Thirdly, the embedding update only requires forward propagation, costing only half of computation compared to training process which requires both forward and backward propagation. Thus, generating context embedding of 6k entities consumes about the same number of flops as training 6000\\*64/(512\\*2\\*2) \u2248 200 input texts, much smaller than the batch size 1024. In short, the entity context embedding memory update only costs 200/1024\u22481/5 additional computation. We can also set a larger update step interval to further reduce the computation cost. Note this computation overhead only exists during pre-training, since entity embedding memory is not updated when fine-tuning.\n\nWe've included the discussion in our revised paper. For the concern about performance improvement, we think this depends on the tasks: For the tasks that rely much on the semantic information of the input text itself (which can be well captured by pure LM), the improvement can be relatively small [3, 4]. For the tasks which rely much on KG information like semi-supervised entity classification, our model performs much better than the pure language model like RoBERTa. \n\n\n[1] MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices (ACL 2020)     \n[2] TinyBERT: Distilling BERT for Natural Language Understanding (EMNLP 2020)    \n[3] Knowledge enhanced contextual word representations (EMNLP 2019)    \n[4] ERNIE: Enhanced Language Representation with Informative Entities (ACL 2019)", "title": "Response to Reviewer 1"}, "-n7E7zMibuV": {"type": "rebuttal", "replyto": "f15ClNZIVdq", "comment": "Dear Reviewer 3, we really appreciate your valuable feedback.\n\nFirst, we want to summarize the main contributions of our paper:\n\n1. We propose a novel knowledge-language co-pretraining framework, JAKET, where a knowledge module and a language module mutually assist each other for more effective semantic analysis.\n2. We use a two-stage language model to solve the cyclic dependency problem between two modules.\n3. We employ an entity context embedding memory with scheduled updates which speeds up the pre-training by 15x.\n4. JAKET can easily adapt to unseen knowledge graphs in the finetuning phase.\n5. Experiments show that JAKET outperforms strong baseline methods on the tasks of few-shot relation classification, question answering, and entity classification.\n\nHere are the replies to each of your question/comment:\n\n**1. For the question about the difference between our model JAKET and K-BERT:**\n\nThanks for bringing this up. There are two main differences between JAKET and K-BERT: 1. K-BERT only injects KG triplets during fine-tuning, while JAKET jointly learns embeddings for entities and tokens during both pre-training and fine-tuning. 2. K-BERT represents the entities and relations by surface form tokens (i.e. names), while JAKET utilizes their description texts, which contain much more semantic information. We\u2019ve made this clear in the revised paper.\n\n**2. For the question \u201cWhat is the relation text or description\u201d:**\n\nThe relation description is similar to entity description, which is a piece of text describing the relation. For example, the relation \u201cinstance of\u201d in Wikidata has a provided description \u201cthat class of which this subject is a particular example and member\u201d. This will provide richer information about the relation beyond its surface name. Experiments show that using LM such as RoBERTa to initialize relation embedding using the description text can improve the performance on downstream tasks like MetaQA by 2~3%.\n\n**3. About comparison with other knowledge pretrained LM models.**\n\nWe conducted an additional experiment on the FewRel dataset using one strong knowledge-enhanced pre-training model KnowBERT [1], which is also pretrained on English Wikipedia corpora and Wikidata KG. We fine-tune the pre-trained model provided in its official codebase. The result below demonstrates the effectiveness of our proposed co-training framework. We\u2019ve included the result in the revised version of our paper.\n\n| FewRel   | 5-way-1-shot | 5-way-5-shot | 10-way-1-shot |\n|----------|--------------|--------------|---------------|\n| KnowBERT |     86.2     |     90.3     |      77.0     |\n| JAKET    |     87.4     |     92.1     |      78.9     |\n\n**4. For the suggestions about more baselines:**\n\nThanks for suggesting these reasonable baselines. For the KGQA task, it usually requires a pipeline of methods including question embedding, sub-graph retrieval, graph modeling, answer selection, etc. We test our model\u2019s capability on the subtask of generating question representation. Thus we only compare with models that focus on sentence representation generation like RoBERTa. For the comparison with other KG embedding methods and a separate pre-training framework, we\u2019re unable to provide ablation results due to the limited time of rebuttal. But we\u2019ll include them in the final version of our paper.\n\n**5. For the question about \u201cEffect of number of steps in graph embedding update and also training time\u201d**\n\nWe tried different updating intervals and found the current schedule can achieve the lowest and most stable pre-training loss. \nThe update of entity context embedding memory takes only 1/6 of the pre-training time. We explain that in the following calculation:     \nFirstly, the number of entities is about 3 million and the update step interval is 500. Thus for each step on average, the model processes the description text of 3e6/500=6e3 entities. Secondly, the length of the description text is 64, much smaller than the length of input text 512, and we only use LM1 (the first half of LM module) for entity context embedding generation, which saves half of the computation time compared to using the whole LM module. Thirdly, the embedding update only requires forward propagation, costing only half of computation compared to training process which requires both forward and backward propagation. Thus, generating context embedding of 6k entities consumes about the same number of flops as training 6000\\*64/(512\\*2\\*2) \u2248 200 input texts, much smaller than the batch size 1024. In short, the entity context embedding memory update only costs 200/1024\u22481/5 additional computation. Note this computation overhead only exists during pre-training, since entity embedding memory is not updated when fine-tuning.\n\n[1] Knowledge enhanced contextual word representations (EMNLP 2019)", "title": "Response to Reviewer 3"}, "zrnrFh_V4zx": {"type": "rebuttal", "replyto": "qcir1OVe4kw", "comment": "Dear Reviewer 4, we really appreciate your valuable feedback.\n\nFirst, we want to summarize the main contributions of our paper:\n\n1. We propose a novel knowledge-language co-pretraining framework, JAKET, where a knowledge module and a language module mutually assist each other for more effective semantic analysis.\n2. We use a two-stage language model to solve the cyclic dependency problem between two modules.\n3. We employ an entity context embedding memory with scheduled updates which speeds up the pre-training by 15x.\n4. JAKET can easily adapt to unseen knowledge graphs in the finetuning phase.\n5. Experiments show that JAKET outperforms strong baseline methods on the tasks of few-shot relation classification, question answering, and entity classification.\n\nHere are the replies to each of your question/comment:\n\n**1. For the issue about \u201cun-justified design like entity embedding update and neighbor sampling\u201d**\n\nThanks for the suggestion. Due to the limited time of rebuttal, we\u2019re unable to rerun pre-training to provide ablation results. But we\u2019ll include them in the final version of our paper. \n\n**2. For the question \u201cfor knowledge graphs, ... do the link prediction instead?\u201d**\n\nLink prediction can be easily combined into our framework and is definitely worth trying. For the two pre-training tasks in our paper, entity category prediction has been demonstrated to be effective in pre-training graph neural networks [1]. Relation type prediction can be regarded as one type of link prediction, as it also masks one element (i.e. relation) of a triplet (<e1, [MASK], e2>) and predicts the masked element. \n\n**3. For the question \u201cbaseline \u2026 pretrain the KG embedding and BERT embedding separately, and concat/merge them ...\u201d.**\n\nThis is a good baseline which we\u2019ll include in the final paper. We add a comparison with one strong knowledge-enhanced PLM KnowBERT [2], which pre-computes the KG embedding and combines them with token embeddings in BERT instead of jointly training them. The result of the FewRel dataset shows that our model outperforms KnowBERT. This further demonstrates the reason we combine the pre-training of knowledge graph and language modeling: i) the knowledge graph can provide information about related entities and relations for entities in the text; ii) the language model can provide contextual information of entity nodes/relation edges in the knowledge graph based on description text. In this way, both modules can benefit from each other by leveraging richer information.\n\n| FewRel   | 5-way-1-shot | 5-way-5-shot | 10-way-1-shot |\n|----------|--------------|--------------|---------------|\n| KnowBERT |     86.2     |     90.3     |      77.0     |\n| JAKET    |     87.4     |     92.1     |      78.9     |\n\n**4. For question about \u201cwhy approaches like VRN in MetaQA paper are not \u2018fair\u2019\u201d**\n\nVRN or PullNet is a pipeline specifically designed for Question answering over KG, including question embedding, sub-graph retrieval, graph modeling, answer selection, etc. In comparison, our model is a generalized framework for natural language understanding. In the case of MetaQA, we test our model\u2019s capability on the subtask of generating question representation. Thus we only compare with models that focus on sentence representation generation like RoBERTa.\n\n**5. For the question \u201cwhy use entity classification tasks? ... Why is the gap so large?\u201d**\n\nFirst, we want to clarify that the task is under fair comparison since the downstream Wikidata KG is disjoint with the pre-training Wikidata KG, i.e. both the entities and labels are not seen during pre-training. Second, semi-supervised node classification is also an important NLP task and has been intensively studied recently [3, 4]. The gap is large because this task relies more on the structure information of KG, which can not be captured by vanilla language models like RoBERTa. Similar gap has also been shown in [3, 4].\n\n[1] Strategies for Pre-training Graph Neural Networks (ICLR 2020)     \n[2] Knowledge enhanced contextual word representations (EMNLP 2019)     \n[3] Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2016)    \n[4] Modeling Relational Data with Graph Convolutional Networks (ESWC 2018)      \n", "title": "Response to Reviewer 4"}, "qdmtuOw36lQ": {"type": "rebuttal", "replyto": "LvIG7D9u75L", "comment": "Dear Reviewer 2, We really appreciate your valuable feedback.\n\nFirst, we want to summarize the main contributions of our paper: \n1. We propose a novel knowledge-language co-pretraining framework, JAKET, where a knowledge module and a language module mutually assist each other for more effective semantic analysis.\n2. We use a two-stage language model to solve the cyclic dependency problem between two modules.\n3. We employ an entity context embedding memory with scheduled updates which speeds up the pre-training by 15x. \n4. JAKET can easily adapt to unseen knowledge graphs in the finetuning phase.\n5. Experiments show that JAKET outperforms strong baseline methods on the tasks of few-shot relation classification, question answering, and entity classification.\n\nHere are the replies to each of your question/comment:\n\n**1. For the question \u201c... include the knowledge graph as plain text in training only a language model?\u201d**\n\nConverting a knowledge graph to plain text is one way to combine knowledge with language modeling. However, since KGs represent facts in a topological way, conversion to text will inevitably lose this structural information. Thus, we use the graph neural network to model the topological information via neighborhood aggregation and the contextual information via initial node embeddings from description text.\n\n**2. For the question \u201c... not made further experiments on other datasets and knowledge graphs? \u2026 only work for English Wikipedia and Wikidata?\u201d:**\n\nIn this paper we conducted experiments on three different tasks: few-shot relation classification, question answering over KG and entity classification, with both KG used during pre-training and novel KG with unseen entities and relations. We are also planning to apply JAKET to other tasks and languages in the future. Note that the JAKET architecture we propose is a framework that can adapt any corpus and related knowledge graph. The design of the framework decouples the source of knowledge from the model. This has been shown by replacing Wikidata with movie KG in the experiments on the MetaQA dataset. \n\n**3. For the suggestion about \u201cfurther experimental comparisons to related approaches\u201d:**\n\nGood suggestion. We conducted an additional experiment on the FewRel dataset using one strong knowledge-enhanced pre-training model KnowBERT [1], which is also pretrained on English Wikipedia corpora and Wikidata KG. We fine-tune the pre-trained model provided in its official codebase. The result below demonstrates the effectiveness of our proposed co-training framework. We\u2019ve included the result in the revised version of our paper. \n\n| FewRel   | 5-way-1-shot | 5-way-5-shot | 10-way-1-shot |\n|----------|--------------|--------------|---------------|\n| KnowBERT |     86.2     |     90.3     |      77.0     |\n| JAKET    |     87.4     |     92.1     |      78.9     |\n\t\n[1] Knowledge enhanced contextual word representations (EMNLP 2019)\n\n", "title": "Response to Reviewer 2"}, "mrQ_6seJDJP": {"type": "rebuttal", "replyto": "SOVSJZ9PTO7", "comment": "We thank the reviewers for their valuable feedback. Based on the questions and suggestions, we made the following content revisions in our paper:\n\n1. Add KnowBERT [1] results on the FewRel dataset in Table 1 on Page 7. Add analysis between KnowBERT and JAKET in the third paragraph of Section 4.2.\n2. Add computation analysis in Appendix A.2. \n3. Add difference discussion with K-BERT [2] in the third paragraph of Section 2.\n\n[1] Knowledge enhanced contextual word representations (EMNLP 2019)\n[2] K-BERT: Enabling Language Representation with Knowledge Graph (AAAI 2020)", "title": "Paper Revision"}, "dknl2hx5_CD": {"type": "review", "replyto": "SOVSJZ9PTO7", "review": "This work proposes a method for joint pre-training of knowledge graph and text data which embeds KG entities and relations into shared latent semantic space as entity embeddings from text. The proposed model JAKET consists of two main parts: a language module and a knowledge module. The model is pre-trained on a collection of tasks: entity category prediction, relation type prediction, masked token prediction and masked entity prediction. The proposed framework enables fine-tuning on knowledge graphs which are unseen during pre-training.\n\n\nOverall, I believe that the work on knowledge-enhanced language models to be an interesting and important area of research. However, I believe the paper is not ready for publication in its current form as (i) the demonstrated improvements obtained by pre-training with the added KG module seem minor compared to the computational overhead of having to compute entity and relation embeddings using GNNs; and (ii) experimental comparison to some relevant prior work is missing.\n\nQuestions/comments for the authors:\n\n1. One of the drawbacks of the proposed method is that it assumes entity descriptions to always be available, which might be the case for Wikidata, but it is not usually the case with e.g. standard knowledge graph completion datasets WN18 and FB15k. How would fine-tuning work on knowledge graphs that do not have entity descriptions?\n2. What does M in RoBERTa+GNN+M stand for? Is it memory?\n3. The improvements over a pure language model on few-shot relation classification and KGQA are minor, especially given the computational overhead that adding a KG module entails. The authors should include a discussion on computational overhead of having a KG module vs a pure language model.\n4. The experimental comparison to existing knowledge-enhanced language models from Section 2 is missing.\n", "title": "A joint KG and language pre-training model", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "f15ClNZIVdq": {"type": "review", "replyto": "SOVSJZ9PTO7", "review": "The paper proposed a pretraining framework for language models and knowledge graphs. As authors mentioned in the paper, many approaches recently focused on this topic, with different variations with respect to entity embedding, initialization, over-parametrization, masked language model, fine-tuning task and etc.\n\nNovelty: The authors mentioned entity embeddings from previous works in this area mainly are computed by external resources and are injected  to the model or they are learned as parameters of the model. What about K-BERT? Doesn\u2019t seem to have these problems. The paper doesn\u2019t provide a comprehensive comparison with K-Bert. I think the novelty of the work may need to be further justified.\nAlso, what is the relation text or description? What kind of description do the relations have? Is it useful to use Roberta for relation embedding initialization?\n\nExperiments: \nThe paper proposes a few baselines and compares the architecture with those baselines. However, there are several other works in this area. Why not comparing with those related approaches?  As an example comparison with K-BERT?\n\nIt would be also interesting to have more baselines without language models (like the GNN in Entity Classification task)? For example, just initialize the knowledge graph by a pretrained language model and evaluate the knowledge graph on questions answering? Or even have a GNN on top of that without language model? Comparison with other knowledge graph embedding approaches on Wikipedia? Also, what if we separately pretrain knowledge graph and language model. How does affect the experiments?\n\nThere is a lack of experiments to show the Effect of number of steps in graph embedding update and also training time?\n", "title": "JAKET Review", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "qcir1OVe4kw": {"type": "review", "replyto": "SOVSJZ9PTO7", "review": "# Summary\n\nThis paper proposed a new language modeling pretraining method that leverages the knowledge graph information. Specifically, the paper replaces the entity embedding in one hidden layer of BERT context embedding, with the corresponding graph attention embedding that is obtained from the knowledge graph. The pretraining tasks contain not only the language related tasks (like predicting masked tokens), but also the knowledge graph tasks like entity classification or relation type prediction. Experiments on few-shot learning tasks, question answering and entity classification show better performance over other pretraining counterparts. \n\n\n# Pros\n\n- The motivation of combining knowledge graphs and unstructured text data for pretraining is valid. \n- The joint training architecture is interesting. \n- The improvement over other kg+lm or purely lm baselines is consistent. \n\n\n# Cons\n\n- Some of the designs need justification.\n- Some of the baselines/ablations might be missing. \n- The experiments have some flaws.\n\n\n# Details\n\nOverall I lean towards accepting the paper, if my concerns can be properly addressed. \n\nI like the idea of having a joint training method for both the kg embedding and text embedding. Although it encounters additional implementation/computation difficulties, it entangles the two sets of information in a coherent way. Also the pretraining has been done on a large scale, with a reasonable size of knowledge graph as the backbone. I think the paper has demonstrated a nontrivial contribution to the field. \n\nThere are several potential issues with the current paper:\n\n1. It seems the paper sacrifices too much on the efficiency, while having many heuristic approximations/designs that are un-justified. For example, a) how much would the entity embedding update affect the fine-tuning; b) how does the neighborhood sampling affect the knowledge graph embedding in the context of BERT training; \n\n2. It is not sure how each individual pretraining method affects the quality of the embedding. Also for knowledge graphs, a commonly used embedding learning method is the link prediction, where one samples positive/negative <e1, r, e2> triplets for learning. The entity category prediction or relation type prediction is less common. Would it be helpful to do the link prediction instead? \n\n3. An important baseline is missing, where one first pretrain the KG embedding and BERT embedding separately, and concat/merge them with downstream fine-tuning tasks. I\u2019d like to see the results on the three tasks in the experiments. \n\n4. For table 2 it would be nice to include the knowledge graph based approaches that don\u2019t leverage the pretraining, including the pullnet results, or the results that come with original metaQA paper. I don\u2019t get the comment at the bottom of page 8 why approaches like VRN in MetaQA paper are not \u2018fair\u2019, as these models actually use less information. \n\n5. For table 1 and 2, the improvement over RoBERTa seems marginal. However for table 3 the gap is huge. Could the authors provide explanations for \n1) why use entity classification tasks? It is somewhat unfair, as the pretraining of the proposed method already involved with the entity classification tasks (although in a transductive setting); Also as I mentioned in 2, link prediction would be a more preferable task for evaluating the quality of the knowledge graph embedding; \nand 2) why is the gap so large, is it due to the issue in 1)? \n\n# Questions \n\nI\u2019d like to hear the answers to my questions above. \n\n# Improvement\n\nI highly encourage the authors to include the necessary baselines in experiments, as suggested above. \n", "title": "Review for JAKET", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}