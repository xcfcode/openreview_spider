{"paper": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "summary": "", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"]}, "meta": {"decision": "Reject", "comment": "The paper introduces a variant of the variational autoencoder (VAE) for probabilistic non-negative matrix factorization. The main idea is to use a Weibull distribution in the latent space. There is agreement among the reviewers that the paper is technically sound and well written, but that it lacks in motivation and demonstration of utility of the proposed method.\nAll the reviewers think the approach is not particularly novel and somewhat incremental. The main issue is that the empirical evaluation of the algorithm is also quite limited. Specifically, it should have been compared with Bayesian NMF. Many papers have addressed Bayesian NMF with variational inference (Cemgil; Fevotte & Dikmen; Hoffman, Blei & Cook) like in VAE. Experimentally, Bayesian NMF and the proposed PAE-NMF could easily be quantitatively compared on matrix completion tasks. Overall, there was consensus among the reviewers that the paper is not ready for publication.\n"}, "review": {"rkx-tS5FCm": {"type": "rebuttal", "replyto": "SJl1lttvAQ", "comment": "Thank you for the reply to our response.\n\nComment 1. For the update of W, it would be better to perform optimization in a constrained non-negative sparse than to force the negative values to zero.\n\nResponse: In previous work on using autoencoders for NMF (not published yet but we intend to publish sometime soon) we investigated the use of multiplicative updates for the W matrix, which guarantees to keep the W matrix non-negative. We did not see any major differences in results when performing that and when doing a gradient descent followed by setting negative terms back to zero, which is also faster than the multiplicative update method. \n\nComment 2. Since the L2 loss corresponds to a Gaussian model, the probability for v to be negative is not zero, which is again a little bit unpleasant if the data are inherently non-negative.\n\nResponse: By construction both h and W are non-negative so it is not possible for the reconstruction to be negative. Within the minimum description length approach, the probability distribution of the error just specifies the message length for sending the errors. The more accurate the distribution the shorter the message length. We could spend more time understanding the distributions of errors, but this would in our judgment give us little gain and take focus away from the main message of the paper (In a full MDL approach we would have to transmit the parameters of the distribution for errors. If we use complicated distributions this can lead to an increase in the description length. It is an interesting, but non-trivial task to study methods for communicating errors, but given the page restrictions it does not seem appropriate in this submission). We would like to point out that nothing \"unpleasant\" happens. The algorithm is perfectly stable. If we used, for example, a truncated Gaussian this would again lead to an L2 loss with an additional constant factor that would not change anything.\n\n\nComment 3. The advantages of the proposed method need to be better demonstrated in the updated version of this paper. \nResponse: we have added a paragraph to the motivation into our new manuscript which we will upload. There are two obvious comparisons to be made for PAE-NMF: 1) with NMF and 2) with probabilistic forms of NMF:\n1st comparison with NMF: it is clear where the advantages lie: we can now sample from the distribution, get a probabilistic link between inputs and latent variables, and because of the KL-divergence term have an automatic way of regularising the method. \n2nd comparison with probabilistic NMF: here the advantages of our method are more subtle but they are effectively equivalent to the advantages of a VAE over other probabilistic methods, which are explained in \u201cAuto-Encoding Variation Bayes\u201d by Kingma and Welling. \n\nAs ever with NMF it is difficult to conclusively prove that one solution is better than another empirically as there is no ground truth for most real world data sets. However, there are important choices in running NMF that gives significantly different results. This paper proposes a method that introduces a principled self-regularisation scheme that encodes the uncertainty in the model through a probability distribution on h. The main argument for doing this is a logical argument, although as much as possible we have tried to support this empirically. We believe our method is novel and deserves to be known. Superficially it may seem that what we are proposing provides nothing different from what a Bayesian approach gives. However, as we argue in the updated paper, Bayesian approaches suffer from requiring a good prior, but in most applications it is very unclear how one would obtain such a prior. This would force the use of model selection techniques, but these would seem to be extremely expensive as it would require finding the full posterior and then require the optimisation of hyper-parameters. We believe our approach provides substantial advantages over any other method that we are aware of.", "title": "Further rebuttal from authors"}, "BklPXKoVh7": {"type": "review", "replyto": "BJGjOi09t7", "review": "The paper is generally well-written (lacking details in some sections though). My main criticism is about the lack of motivation for nonnegative VAE and lack of comparison with NMF.\n\nComments:\n- the motivation of the proposed methodology is not clear to me. What is the interest of the proposed auto-encoding strategy w.r.t NMF ? There is no experimental comparison either. Besides the probabilistic embedding (which exists in NMF as well), is there something PAE-NMF can do better than NMF ? There is probably something, but the paper does not bring a convincing answer.\n- the paper missed important references to nonnegative auto-encoders, in particular:\nhttps://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf\n- the review of probabilistic NMF works is limited, see e.g.,\nhttps://paris.cs.illinois.edu/pubs/smaragdis-spm2014.pdf\n- more details are needed about inference in Section 2.4\n\nMinor comments:\n- the notations z and h are sometimes confusing, what about using h every where ?\n- it\u2019s not clear to me how the first term in (1) is equal to the second term in (2", "title": "lacking motivation and comparisons", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rylhntPTs7": {"type": "review", "replyto": "BJGjOi09t7", "review": "This paper replaces the Gaussian latent variables in standard VAE with the Weibull distribution and therefore presents a VAE solution to nonnegative matrix factorization, under squared Euclidean distance loss function. The literature review summarizes four related work very well. The adopted Weibull distribution provides a tractable inverse CDF function and analytical form of the KL divergence, facilitating VAE inference. In particular, the effects of the entropy term are discussed in detail.  Experiments illustrate the generated data from the model,  the learned part-based dictionaries, and the distributions of latent variables from similar data points.  \n\nQuestions: \n\n1. What is the updating rule for W_f? Is it multiplicative?  In Sec 2.4, The value of W_f is kept to be nonnegative by \"setting negative terms to zero\". Does it mean once one entry is set to zero, it would never be positive in the sequential gradient steps? \n\n2. Although the proposed model is claimed to be probabilistic, the L2 loss function in equation (2) implies that the data generated from the model could be negative. How would the proposed approach handle other loss function of NMF such as KL (e.g., under Poisson assumption)?  \n\n3. The nonnegative variant sounds interesting, but the experimental results are quite limited. It is unclear how the proposed approach would compare to other probabilistic NMF models and algorithms, or the standard VAE as a generative model. It seems the proposed method can do as good as NMF or VAE in some aspects. This begs the question of when would the proposed approach be superior to others and in what aspect? \n\nMinor: \nIn some places where the parameter are constrained to be nonnegative, it would be more clear to use notations such as R_+ instead of R.  ", "title": "Experimental evaluations are mostly qualitative", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1xMbCCbsX": {"type": "review", "replyto": "BJGjOi09t7", "review": "TITLE\nA VARIATIONAL AUTOENCODER FOR PROBABILISTIC NON-NEGATIVE MATRIX FACTORISATION\n\nREVIEW SUMMARY\n\nWell written, interesting new idea, modest technical contribution, limited demonstration.\n\nPAPER SUMMARY\n\nThe paper presents an approach to NMF within a variational autoencoder framework. It uses a Weibull distribution in the latent space. \n\nQUALITY\n\nThe work appears technically sound except for minor typos. \n\nCLARITY\n\nOverall the paper is a pleasure to read. Only the presentation of the standard vae could be more clear.\n\nORIGINALITY\n\nThe method is (to my knowledge) novel. \n\nSIGNIFICANCE\n\nI think this paper is a significant contribution. I feel I have learned something from reading it, and am motivated to try out this approach. I believe there should be a wide general interest. The technical contribution is perhaps somewhat modest, as the paper fairly straightforwardly includes non-negativity in a vae setting, but I think this is a good idea. The demonstration of the algorithm is also quite limited - I would have enjoyed seeing this applied to some more reaslistic, practical problems, where perhaps the quantification of uncertaincy (which is one of the main benefits of a vae-based nmf) would come more directly into play. \n\nFURTHER COMMENTS\n\npage 3\n\nThe presentation of the VAE objective is a bit oblique. The statement \"they require a different objectiv function\" is not wrong, but also not very precise. The equality in eq. (2) is incorrect (I assume this is meant to be a stochastic approximation, i.e. the expectation over q approximated by sampling?)\n\n\"with \\hat v  the reconstructed vector\" Not clear. I assume \\hat v is reconstructed from a sample from q given v ?\n\nThere is a typo in eq. 3. The first factor in the second to last term should be (lambda_1/\\lambda_2)^(k_2)\n\n", "title": "Well written, interesting new idea, modest technical contribution, limited demonstration.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}