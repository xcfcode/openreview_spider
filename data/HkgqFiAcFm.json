{"paper": {"title": "Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications", "authors": ["Carson Eisenach", "Haichuan Yang", "Ji Liu", "Han Liu"], "authorids": ["eisenach@princeton.edu", "h.yang@rochester.edu", "ji.liu.uwisc@gmail.com", "hanliu.cmu@gmail.com"], "summary": "", "abstract": "Many complex domains, such as robotics control and real-time strategy (RTS) games, require an agent to learn a continuous control. In the former, an agent learns a policy over R^d and in the latter, over a discrete set of actions each of which is parametrized by a continuous parameter. Such problems are naturally solved using policy based reinforcement learning (RL) methods, but unfortunately these often suffer from high variance leading to instability and slow convergence. Unnecessary variance is introduced whenever policies over bounded action spaces are modeled using distributions with unbounded support by applying a transformation T to the sampled action before execution in the environment. Recently, the variance reduced clipped action policy gradient (CAPG) was introduced for actions in bounded intervals, but to date no variance reduced methods exist when the action is a direction, something often seen in RTS games. To this end we introduce the angular policy gradient (APG), a stochastic policy gradient method for directional control. With the marginal policy gradients family of estimators we present a unified analysis of the variance reduction properties of APG and CAPG; our results provide a stronger guarantee than existing analyses for CAPG. Experimental results on a popular RTS game and a navigation task  show that the APG estimator offers a substantial improvement over the standard policy gradient.", "keywords": ["reinforcement learning", "policy gradient", "MOBA games"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper introduces a new variance reduced policy gradient method, for directional and clipped action spaces, with provable guarantees that the gradient is lower variance. The paper is clearly written and the theory an important contribution. The experiments provide some preliminary insights that the algorithm could be beneficial in practice. "}, "review": {"Hyl_lXQF3X": {"type": "review", "replyto": "HkgqFiAcFm", "review": "Summary\n\nThis paper derives a new policy gradient method for when continuous actions are transformed by a\nnormalization step, a process called angular policy gradients (APG). A generalization based on\na certain class of transformations is presented. The method is an instance of a \nRao-Blackwellization process and hence reduces variance.\n\n\nDetailed comments\n\nI enjoyed the concept and, while relatively niche, appreciated the work done here and do believe it has clear applications. I am not convinced that the measure theoretic perspective is always\nnecessary to convey the insights, although I appreciate the desire for technical correctness. Still,\nappealing to measure theory does reduces readership, and I encourage the authors to keep this in\nmind as they revise the text.\n\nGenerally speaking it seems like a lot of technicalities for a relatively simple result:\nmarginalizing a distribution onto a lower-dimensional surface.\n\nThe paper positions itself generally as dealing with arbitrary transformations T, but really is \nabout angular transformations (e.g. Definition 3.1). The generalization is relatively \nstraightforward and was not too surprising given the APG theory. The paper would gain in clarity\nif its scope was narrowed. \n\nIt's hard for me to judge of the experimental results of section 5.3, given that there are no other \nbenchmarks or provided reference paper. As a whole, I see APG as providing a minor benefit over PG.\n\nDef 4.4: \"a notion of Fisher information\" -- maybe \"variant\" is better than \"notion\", which implies there are different kinds of Fisher information \nDef 3.1 mu is overloaded: parameter or measure?\n4.4, law of total variation -- define \n\n\nOverall\n\nThis was a fun, albeit incremental paper. The method is unlikely to set new SOTA, but I appreciated\nthe appeal to measure theory to formalize some of the concepts.\n\n\nQuestions\n\nWhat does E_{pi|s} refer to in Eqn 4.1?\nCan you clarify what it means for the map T to be a sufficient statistic for theta? (Theorem 4.6)\nExperiment 5.1: Why would we expect APG with a 2d Gaussian to perform better than a 1d Gaussian\non the angle?\n\n\nSuggestions\n\nParagraph 2 of section 3 seems like the key to the whole paper -- I would make it more prominent.\nI would include a short 'measure theory' appendix or equivalent reference for the lay reader.\n\nI wonder if the paper's main aim is not actually to bring measure theory to the study of policy\ngradients, which would be a laudable goal in and of itself. ICLR may not in this case be the right\nvenue (nor are the current results substantial enough to justify this) but I do encourage authors to\nconsider this avenue, e.g. in a journal paper.\n\n= Revised after rebuttal =\n\nI thank the authors for their response. I think this work deserves to be published, in particular because it presents a reasonably straightforward result that others will benefit from. However, I do encourage further work to\n1) Provide stronger empirical results (these are not too convincing).\n2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.\n", "title": "Fun, albeit incremental paper", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1eabnIj67": {"type": "rebuttal", "replyto": "Hyl_lXQF3X", "comment": "Thank you for the time and effort spent reviewing our paper, and for the detailed suggestions. Below we repeat the questions/comments from the review and respond to each in turn.\n\n\u201cThe paper positions itself generally as dealing with arbitrary transformations T, but really is about angular transformations (e.g. Definition 3.1). The generalization is relatively straightforward and was not too surprising given the APG theory. The paper would gain in clarity if its scope was narrowed.\u201d\n\nOur MPG framework not only supports the angular transformation but also covers the recently proposed clipped transformation in CAPG [Fujita and Maeda, 2018]. The theoretical result is tighter than the one in [Fujita and Maeda, 2018], and it supports general transformations instead of only clipped actions.\n\n\"I am not convinced that the measure theoretic perspective is always necessary to convey the insights, although I appreciate the desire for technical correctness.\" / \"Generally speaking it seems like a lot of technicalities for a relatively simple result: marginalizing a distribution onto a lower-dimensional surface.\"\n\nWe agree that the measure theoretic approach is not always necessary (indeed for angular actions, it is not needed), but it is necessary for a very common scenario -- clipped actions. Researchers and practitioners both almost always clip actions when using policy gradient algorithms for robotics control environments (read: MuJoCo tasks). Recently, a reduced variance method was introduced by Fujita and Maeda (2018) for clipped action spaces. Their algorithm is also a member of the marginal policy gradients family and our theoretical results for MPG significantly tighten the existing analysis of that algorithm. \n\n\n\"It's hard for me to judge of the experimental results of section 5.3, given that there are no other benchmarks or provided reference paper. As a whole, I see APG as providing a minor benefit over PG.\"\n\nFor the results in Section 5.3, the issue is that currently, there are no benchmark environments for directional control. We anticipate that in the future this may change (e.g. console and PC games often have directional controls).\n\n\u201cWhat does E_{pi|s} refer to in Eqn 4.1?\u201d\n\nThe expectation is taken with respect to the policy \\pi conditioned on the current state s (s here is arbitrary, but fixed). Stated differently, we are taking the expectation with respect to the distribution $\\pi(\\cdot | s,\\theta)$.\n\n\u201cCan you clarify what it means for the map T to be a sufficient statistic for theta? (Theorem 4.6)\u201d\n\nWe have now removed this part of the statement because we are no longer absolutely certain of its correctness, and because it is not used anywhere else in the paper.\n\n\u201cExperiment 5.1: Why would we expect APG with a 2d Gaussian to perform better than a 1d Gaussian on the angle?\u201d\n\nBecause using a 1D Gaussian requires either (1) clipping the angle to [0,2\\pi) before execution in the environment and making updates using the clipped output or (2) using the sampled angle for updates and perform the clipping in the environment. In the first case, this approach is asymmetric in that does not place similar probability on $\\mu_{\\theta}(s) - \\epsilon$ and $\\mu_{\\theta}(s) + \\epsilon$ for $\\mu_{\\theta}(s)$ near to $0$ and $2\\pi$. In the second case, this requires approximating a periodic function. We include both these reasons at the start of Section 3.\n\n\nLastly, thank you for the concrete suggestions:\n\"Def 4.4: \"a notion of Fisher information\" -- maybe \"variant\" is better than \"notion\", which implies there are different kinds of Fisher information \nDef 3.1 mu is overloaded: parameter or measure?\n4.4, law of total variation -- define \"\n\nWe have addressed these and uploaded a new draft to reflect the changes. For the last suggestion, we currently define the law of total variance(variation) in the preliminaries so we did not repeat the definition in Section 4.4. We now write \"law of total variance\" instead of \"law of total variation\" to avoid any ambiguity.", "title": "Authors' Response to Reviewer 3"}, "SJlQqs8spQ": {"type": "rebuttal", "replyto": "H1gSPxyC3Q", "comment": "Thank you for the time and effort spent reviewing our paper. We mostly agree with your characterization of our work, but we think there are two important points we perhaps did not sufficiently emphasize in our paper and that we would like to mention:\n\n(1) There are other existing tasks and algorithms that fall into the marginal policy gradients framework. For example, researchers and practitioners both almost always clip actions when using policy gradient algorithms for robotics control environments (read: MuJoCo tasks). Recently, a reduced variance method was introduced by Fujita and Maeda (2018) for clipped action spaces. Their algorithm is also a member of the marginal policy gradients family and our theoretical results for MPG significantly tighten the existing analysis of their algorithm.\n\n(2) To the best of our knowledge, our work is the first to apply such variance reduction techniques to RL.\n\nTo summarize, our work consists of two components: (a) a new algorithm for directional control and (b) a variance reduction framework that can be applied to directional action space and clipped action spaces. While directional action spaces are not very common at this time, clipped action spaces are extremely common. We also anticipate that in the future, many additional environments will be available that feature directional actions (many console or PC games, for example). For these reasons, we feel that our work is not incremental at all, and is actually quite novel.\n", "title": "Authors' Response to Reviewer 2"}, "S1eUaqUjTQ": {"type": "rebuttal", "replyto": "SJgG0gkChX", "comment": "Thank you for the time and effort spent reviewing our paper. We are glad you liked the paper. We want to emphasize one point that we perhaps did not highlight enough in our paper: there are other existing algorithms that fall into the marginal policy gradients framework. Specifically, researchers and practitioners both almost always clip actions for use in robotics control environments (read: MuJoCo tasks). Recently, a reduced variance method was introduced by Fujita and Maeda (2018) for clipped action spaces. Their algorithm is also a member of the marginal policy gradients family and our theoretical results for MPG significantly tighten existing analyses of variance reduction that can be achieved for clipped actions.\n\nTo respond to your question, yes it is possible (e.g. the example given above), but their is no general procedure that we know of to derive such methods. Rather, this would be done on an action space by action space basis", "title": "Authors' Response for Reviewer 3"}, "SJgG0gkChX": {"type": "review", "replyto": "HkgqFiAcFm", "review": "In this paper the authors proposed a new policy gradient method, which is known as the angular policy gradient (APG), that aims to provide provably lower variance in the gradient estimate. Here they presented a stochastic policy gradient method for directional control. Under the set of parameterized Gaussian policies, they presented a unified analysis of the variance of APG and showed how it theoretically outperform (in terms of having lower variance) than other state-of-the art methods. They further evaluated the APG algorithms on a grid-world navigation domain as well as the King of Glory task, and showed that the APG estimator significantly out-performs the standard policy gradient.\n\nIn general I think this paper addressed an important issue in policy gradient in terms of deriving a lower variance gradient estimate. In particular the authors showed that under the parameterized marginal distribution, such as the angular Gaussian distribution, the corresponding APG estimate has a lower variance estimate than that of CAPG. Furthermore, I also appreciate that they evaluated these results in realistic experiments such as the RTS game domains. \n\nMy only question is on the possibility of deriving realistic APG algorithms beyond the class of angular Gaussian policy. In terms of the layout of the paper, I would also recommend including the exact algorithm pseudo-code used in the main paper.\n", "title": "Comprehensive analysis and evaluated algorithms on realistic experiments.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1gSPxyC3Q": {"type": "review", "replyto": "HkgqFiAcFm", "review": "This paper introduces policy gradient methods for RL where the policy must choose a direction (a.k.a., the navigation problem).\n\nMapping techniques from \"non-directional\" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big). The authors propose to sample directly on the sphere, using the fact that the likelyhood of an angular Gaussian r.v. has *almost* a closed form and its gradient can almost be computed, up to some normalization term (the integral which is constant in the standard Gaussian case).\n\n\nThis can be seen as a variance reduction techniques.\n\nThe proofs are not too intricate, for someone used to variance reduction (yet computations must be made quite carefully).\n\n\nThe result is coherent, interesting from a theoretical point of view and the experiment are somehow convincing. The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...\n", "title": "Limited setting of directional RL, but interesting approach and results.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}