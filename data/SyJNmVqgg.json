{"paper": {"title": "Neural Data Filter for Bootstrapping Stochastic Gradient Descent", "authors": ["Yang Fan", "Fei Tian", "Tao Qin", "Tie-Yan Liu"], "authorids": ["v-yanfa@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "tie-yan.liu@microsoft.com"], "summary": "We propose a reinforcement learning based teacher-student framework for filtering training data to boost SGD convergence.", "abstract": "Mini-batch based Stochastic Gradient Descent(SGD) has been widely used to train deep neural networks efficiently. In this paper, we design a general framework to automatically and adaptively select training data for SGD. The framework is based on neural networks and we call it \\emph{\\textbf{N}eural \\textbf{D}ata \\textbf{F}ilter} (\\textbf{NDF}). In Neural Data Filter, the whole training process of the original neural network is monitored and supervised by a deep reinforcement network, which controls whether to filter some data in sequentially arrived mini-batches so as to maximize future accumulative reward (e.g., validation accuracy). The SGD process accompanied with NDF is able to use less data and converge faster while achieving comparable accuracy as the standard SGD trained on the full dataset. Our experiments show that NDF bootstraps SGD training for different neural network models including Multi Layer Perceptron Network and Recurrent Neural Network trained on various types of tasks including image classification and text understanding.", "keywords": ["Reinforcement Learning", "Deep learning", "Optimization"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The authors propose a meta-learning algorithm which uses an RL agent to selectively filter training examples in order to maximise a validation loss. There was a lot of discussion about proper training/validation/test set practices. The author's setup seems to be correct, but the experiments are quite limited. Pro - interesting idea, very relevant for ICLR. Con - insufficient experiments. This is a cool idea which could be a nice workshop contribution."}, "review": {"SkW0n51Dg": {"type": "rebuttal", "replyto": "HyoMSTSVl", "comment": "Dear Reviewer,\n\nPlease check the new paper version. Basically:\n\n1) Using validation set for reward computation is justifiable and common in the literature. Please refer to our (detailed) response point 7 to the first reviewer. \n2) We have removed the term `plain SGD' in the paper to avoid misunderstandings. In fact we are using Adadelta + No data filtration as baseline.\n3) We have added a new experiment on mnist.\n\nHope all these can relieve your concerns listed in your original reviews and provide a new basis of your view towards our work. \n\nThank you again for the comments.\n\nBest,\nFei ", "title": "The new version"}, "By2XU91wx": {"type": "rebuttal", "replyto": "SyJNmVqgg", "comment": "Dear All Reviewers,\n\nWe have updated a new version, which: 1) adds a new experiment on mnist dataset to further verify the effectiveness of our proposed NDF algorithm; 2) makes a more dedicated and clearer description towards baseline method, REINFORCE v.s. Actor-Critic and validation setup.\n\nWe hope the new version can remove your previous concerns towards this work, even it is approaching the end of rebuttal period (we feel it sorry to submit it late).\n\nThanks again for all of your valuable comments and suggestions to this work!\n\nBest,\nFei", "title": "A New Version"}, "B1Jc2DILx": {"type": "rebuttal", "replyto": "SyJNmVqgg", "comment": "Dear Reviewer, \n\nThank you very much for your positive comments and scores!  As stated before, we are using Adadelta as the basic optimizer, rather than simple SGD. Very sorry for the misunderstanding brought by the term `Plain SGD\u2019 in the paper. It just means `Adadelta without any data filtration'. In addition we are working on new datasets and conducting more qualitative analysis.\n", "title": "Reply for reviewer2"}, "HyQPb6eIe": {"type": "rebuttal", "replyto": "BkHr2k2Bl", "comment": "Dear Reviewer:\n\nPlease check our response 7) to AnonReviewer1's review, on validation set up. Thanks!\n\nBest,\nFei", "title": "Clarification on Point 2"}, "HyfTgTl8l": {"type": "rebuttal", "replyto": "rktOx2WNl", "comment": "We thank you so much for your professional and valuable comments, which will definitely help to improve the paper.  We\u2019d like to make some clarifications on this work:\n1.\tWe have updated the paper according to your suggestions on citation formatting, and clarified clearly on the first-step forward pass. Please check the latest version. Thanks again!\n\n2.\tActually, from the first submitted version, we have stated that all the curves are obtained from averaged results of five repeated runs. Please refer to the paragraph right below Figure 3. Maybe the formatting (fairly long distance between this statement and Figure 2) makes you unaware of such statement and we feel very sorry.\n\n3.\tAs to the usage of optimization algorithms Adam and RMSProp, please refer to points 1) of our reply to reviewer 3.  Basically, there are two justifications of using Adadelta: 1). Adadelta is widely used in RNN-related tasks and there is no evidence showing that it is inferior to Adam/RMSProp; 2) We\u2019re able to use Adadelta to obtain even better classification accuracy on IMDB dataset, compared with the numbers reported in previous works. Of course, we will test Adam and RMSProp in the future work.\n\n4.\tFor the difference of REINFORCE and Actor-Critic, such as update frequency and reward form, we would like to make the following clarifications: 1) Our goal is to boost the convergence of the whole training process, from the very beginning to convergence. Therefore, in our MDP formulation (in the beginning of section 2), we define each episode as the whole training process, aiming to taking `future\u2019 reward into consideration in each training step. 2)  According to our knowledge, standard REINFORCE algorithm uses one-time observation (obtained through Monto-Carlo sampling) to approximate true reward, and usually the reward is observed at the end of each episode. Thus, REINFORCE in most previous work does not update at each single step, given future rewards have not been obtained yet. 3) Given 1) and 2), we have to use different rewards for REINFORCE and Actor-Critic, since the validation accuracy in each step (used in A-C) reflects the convergence speed, and the A-C policy will get updated immediately in each step. However, for REINFORCE, the final accuracy, and the sum of accumulated immediate accuracy, are roughly the same for different episodes, which means pure validation accuracy numbers cannot reflect the quality of different policies.   This is exactly the motivation of our designed rewards for REINFORCE, given that it can reflect convergence speed.\n\n5.\tWe use Adam to optimize the policy. The missing statement has been added in the updated version.\n\n6.\tSorry we are not quite clear about what you mean by `policy adapting\u2019. As we stated in the paper, there are several steps in each of our experiments: in the first step, we trained both the policy network and the RNN classification network; in the second step, we fixed the policy network and trained a RNN classification network using the policy network; finally, we tested the learnt RNN network on the test set.\n\n7.\tFor the validation dataset selection, again we thank you very much for pointing out the `four-disjoint\u2019 dataset separation principles. Here are several of our points:\na.\tYes, the validation set is usually used for hyper-parameter search, but not limited to hyper-parameter search.  For example, in neural network training, people frequently use validation accuracy to perform learning rate scheduling, e.g, halving the learning rate when validation accuracy drops. In such a scenario, as far as we know, there is no `extra\u2019 validation set used. In other words, validation set is used for both TRAINING CONTROL and hyperparameter search.   In fact, in this scenario, all hyper-parameter configurations are still treated equally \u2013 they are both under the same training protocols (e.g., halving learning rate when validation accuracy is dropped) and the chosen of the best hyper-parameter is fair.\n\nb.\tWe in fact follows previous hyper-parameter configuration suggested in [2] as above, such as RNN model size, learning rate scheduling, to make sure the public classification accuracy is obtained. That is to say, hyper-parameter configuration is fixed in our experiment. \n\nc.\tFor the paper from Google [3], there are several statements such as \u201cTraining the network \u2013 the \u201dchild network\u201d \u2013 on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller\u201d, \u201cAt convergence, this child network will achieve an accuracy R on a held-out dataset. We can use this accuracy R as the reward signal and use reinforcement learning to train the controller.\u201d, \u201cOn each dataset, we have a separate held-out validation dataset to compute the reward signal.\u201d and \u201cThe validation set has 5,000 examples randomly sampled from the training set, the remaining 45,000 examples are used for training\u201d. Once you dig into details, you can find that from these four statements, they do NOT distinguish the term \u2018validation\u2019 and \u2018held-out validation\u2019 \u2013 in fact, the reward they used in REINFORCE are from such a (held-out) validation dataset with size 5,000, i.e., [3] and our paper use the validation set in the same way.\n\nOverall thanks again for your detailed suggestions. We are constantly improving the paper and hope the new version and our clarifications can address your concerns.\n\n\n\n\n", "title": "Thank you for your detailed comments."}, "Sy1QF_hre": {"type": "rebuttal", "replyto": "BkHr2k2Bl", "comment": "Oh We are quite sorry for using the term `plain SGD' in the figure, that leads to your misunderstandings. The `plain' here means there is no data filtration used, rather than the `straightforward' SGD.  In fact it is the Adadelta optimizer without any data filtration.\n\nWe are updating the paper to remove such a concern.", "title": "In fact Adadelta"}, "ryzKsYoSg": {"type": "rebuttal", "replyto": "HyoMSTSVl", "comment": "Thanks for your feedbacks. We are afraid you made several misunderstandings about our work.\n\n1) We used Adadelta as the optimizer, rather than plain SGD as you stated. This has been clearly stated from the first version of our submission. In fact, Adadelta is commonly used in RNN-related tasks, such as the RNN-Search Neural Machine Translation model [1]. In addition, using Adadelta, we can reproduce, or get even better results than previous reported classification accuracy on using LSTM for IMDB dataset (i.e., >88.0%, check Table 2 of [2], and our Figure 2.). Therefore, it is sufficient to leverage Adadelta as the black-box optimizer for this particular task.\n\n2) We do not agree with you on your doubt of using validation accuracy as features for training. In fact, as long as the test data is unseen, one may design multiple ways of using validation dataset to control training process,  such as early stopping training, tuning learning rate, etc.  From that sense,  we in fact act in the similar way: validation accuracy is simply used as one of the features for controlling training.\n\nIn fact, you may observe other works that leverage validation accuracy as the reward for RL to better design training strategies, such as [3],  also submitted to ICLR 2017.\n\nWe agree with you that other tasks are necessary to better demonstrate the effectiveness of our algorithm. However, some misleading points may lead to your unfair overall judgement of this work.\n\n[1] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.\n[2] Dai A M, Le Q V. Semi-supervised sequence learning[C]//Advances in Neural Information Processing Systems. 2015: 3079-3087.\n[3] Zoph, Barret, and Quoc V. Le. \"Neural architecture search with reinforcement learning.\" arXiv preprint arXiv:1611.01578 (2016).\n", "title": "Clarifications on some misunderstandings"}, "HkTzqFqVl": {"type": "rebuttal", "replyto": "Hka1p3HVx", "comment": "Dear Reviewer:\n\nWe are sorry that you may neglect/misunderstanding our paper as well as our response. In fact, we are using Adadelta in our experiments and never use Plain SGD. We've stated it very clearly from the first version of the paper. We will formalize our formal response to give a further explanation.\n\nThanks!", "title": "We are not using Plain SGD"}, "ByMCP717x": {"type": "rebuttal", "replyto": "B12fmRTze", "comment": "Thanks for your detailed questions!\n1)\tFor margin value, we follow widely used definition (e.g. ref 1) for multi-class classification, that is for a hypothesis h and training instance (x,y),   the margin is defined as h(x,y) -  \\max_{y\u2019\\neq y} h(x,y). In our scenario, h(x,y) indicates the conditional probability induced by h, i.e., h(x,y)= P_h(y|x).\n\n2)\tOnce a mini-batch data D arrives, its state features is computed at once. Note here very tiny extra computations are added, compared with standard feedforward/backpropagation process, since the feedforward results in computing state features can be re-used in backpropagation.  \n\n3)\tThank you very much for the suggestions of further analyzing the behaviors of curriculum learning.  In fact, we did observe some interesting results on using CL for DNN training, such as some training stability issues. Due to the emphasis of this paper, we have not included this part. We are creating some toy (and maybe corrupted) dataset to further investigate the behaviors of CL.\n\n4)\tWe have not obtained enough gains on MNIST dataset yet. Our conjecture is that the noise in MNIST may be less compared with text tasks which includes human judgments, and the room for improvement on such tiny dataset may be limited. However, another reason might be that more training efforts are necessary and the experiments are still undergoing. We will keep updating the manuscript for these extra experiments.\n\nWe have improved the manuscript to clarify some of your questions such as margin value definition and feature computation process. Thank you again! \n\n1.\tCortes C, Mohri M, Rostamizadeh A. Multi-Class Classification with Maximum Margin Multiple Kernel[C]//ICML (3). 2013: 46-54.\n", "title": "Reply for reviewer 1's questions"}, "Sk_b5m1Xx": {"type": "rebuttal", "replyto": "BJaR-oaMl", "comment": "Thanks for your questions. In fact, our designed NDF framework can be applied to any of your mentioned optimization algorithms such as adagrad/adadelta/plain sgd\u2026 All these algorithms just act as a `black box\u2019 for updating neural network status (weights), which corresponds the evolution of environment in reinforcement learning. Our strategy aims to provide `useful\u2019 input (i.e., data) to these optimizers. Therefore, these optimizations algorithms are not `baselines\u2019, but are our `customers\u2019.\n\nWe can conduct some additional experiments based on other optimization algorithms, which means `one optimization algorithm, one Figure 2\u2019 and that seems somewhat not necessary at status :)\n\nThank you again for your question! \n", "title": "Reply for reviewer 3's questions"}, "B12fmRTze": {"type": "review", "replyto": "SyJNmVqgg", "review": "- You make the comparison with curriculum learning. Is it something you have observed? Could you/did you design a toy experiment showing that? (If not I think it would be an awesome addition to the paper).\n- are \"Features to represent the combination of both data and model\" computed each time when feeding the state representation to the actor? (which is expensive) Or do you use the previous run's computations? (which would mean you need to go over your whole dataset at least once before using your method) Or I'm missing something.\n- What is the margin you refer to? The distance between the decision boundary and the point? That seems an odd choice in high dimensions.\n- Did you try this approach on simpler, or toy datasets? (like MNIST, or as I said in my first question, some toy dataset where curriculum learning drastically helps)\n\nThanks!This work proposes to augment normal gradient descent algorithms with a \"Data Filter\", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.\n\n\nStylistic comment, please use the more common style of \"(Author, year)\" rather than \"Author (year)\" when the Author is *not* referred to or used in the sentence.\nE.g. \"and its variants such as Adagrad Duchi et al. (2011)\" should be \"such as Adagrad (Duchi et al., 2011)\", and  \"proposed in Andrychowicz et al. (2016),\" should remain so.\n\nI think the paragraph containing \"What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered.\" should be clarified. What is \"seeing\"? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.\n\n\nThere are a few choices in this work which I do not understand:\n\nWhy wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).\n\nSimilarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.\n\n\nI have a few qualms with the experimental setting:\n- is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me.\n- with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.\n- it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all.\n- the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).\n- in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization.\n- how is the \"pseudo-validation\" data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.\n\n\nOverall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results.\n", "title": "Some questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rktOx2WNl": {"type": "review", "replyto": "SyJNmVqgg", "review": "- You make the comparison with curriculum learning. Is it something you have observed? Could you/did you design a toy experiment showing that? (If not I think it would be an awesome addition to the paper).\n- are \"Features to represent the combination of both data and model\" computed each time when feeding the state representation to the actor? (which is expensive) Or do you use the previous run's computations? (which would mean you need to go over your whole dataset at least once before using your method) Or I'm missing something.\n- What is the margin you refer to? The distance between the decision boundary and the point? That seems an odd choice in high dimensions.\n- Did you try this approach on simpler, or toy datasets? (like MNIST, or as I said in my first question, some toy dataset where curriculum learning drastically helps)\n\nThanks!This work proposes to augment normal gradient descent algorithms with a \"Data Filter\", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.\n\n\nStylistic comment, please use the more common style of \"(Author, year)\" rather than \"Author (year)\" when the Author is *not* referred to or used in the sentence.\nE.g. \"and its variants such as Adagrad Duchi et al. (2011)\" should be \"such as Adagrad (Duchi et al., 2011)\", and  \"proposed in Andrychowicz et al. (2016),\" should remain so.\n\nI think the paragraph containing \"What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered.\" should be clarified. What is \"seeing\"? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.\n\n\nThere are a few choices in this work which I do not understand:\n\nWhy wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).\n\nSimilarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.\n\n\nI have a few qualms with the experimental setting:\n- is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me.\n- with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.\n- it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all.\n- the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).\n- in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization.\n- how is the \"pseudo-validation\" data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.\n\n\nOverall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results.\n", "title": "Some questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJaR-oaMl": {"type": "review", "replyto": "SyJNmVqgg", "review": "Does authors have other baselines that plain SGD? Dropout, Adam, etc..?Final review: The writers were very responsive and I agree the reviewer2 that their experimental setup is not wrong after all and increased the score by one.  But I still think there is lack of experiments and the results are not conclusive. As a reader I am interested in two things, either getting a new insight and understanding something better, or learn a method for a better performance. This paper falls in the category two, but fails to prove it with more throughout and rigorous experiments. In summary the paper lacks experiments and results are inconclusive and I do not believe the proposed method would be quite useful and hence not a conference level publication. \n\n--\nThe paper proposes to train a policy network along the main network for selecting subset of data during training for achieving faster convergence with less data.\n\nPros:\nIt's well written and straightforward to follow\nThe algorithm has been explained clearly.\n\nCons:\nSection 2 mentions that the validation accuracy is used as one of the feature vectors for training the NDF. This invalidates the experiments, as the training procedure is using some data from the validation set.\n\nOnly one dataset has been tested on. Papers such as this one that claim faster convergence rate should be tested on multiple datasets and network architectures to show consistency of results. Especially larger datasets as the proposed methods is going to use less training data at each iteration, it has to be shown in much larger scaler datasets such as Imagenet.\n\nAs discussed more in detail in the pre-reviews question, if the paper is claiming faster convergence then it has to compare the learning curves with other baselines such Adam. Plain SGD is very unfair comparison as it is almost never used in practice. And this is regardless of what is the black box optimizer they use. The case could be that Adam alone as black box optimizer works as well or better than Adam as black box + NDF.", "title": "Are there any other baseline results?", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HyoMSTSVl": {"type": "review", "replyto": "SyJNmVqgg", "review": "Does authors have other baselines that plain SGD? Dropout, Adam, etc..?Final review: The writers were very responsive and I agree the reviewer2 that their experimental setup is not wrong after all and increased the score by one.  But I still think there is lack of experiments and the results are not conclusive. As a reader I am interested in two things, either getting a new insight and understanding something better, or learn a method for a better performance. This paper falls in the category two, but fails to prove it with more throughout and rigorous experiments. In summary the paper lacks experiments and results are inconclusive and I do not believe the proposed method would be quite useful and hence not a conference level publication. \n\n--\nThe paper proposes to train a policy network along the main network for selecting subset of data during training for achieving faster convergence with less data.\n\nPros:\nIt's well written and straightforward to follow\nThe algorithm has been explained clearly.\n\nCons:\nSection 2 mentions that the validation accuracy is used as one of the feature vectors for training the NDF. This invalidates the experiments, as the training procedure is using some data from the validation set.\n\nOnly one dataset has been tested on. Papers such as this one that claim faster convergence rate should be tested on multiple datasets and network architectures to show consistency of results. Especially larger datasets as the proposed methods is going to use less training data at each iteration, it has to be shown in much larger scaler datasets such as Imagenet.\n\nAs discussed more in detail in the pre-reviews question, if the paper is claiming faster convergence then it has to compare the learning curves with other baselines such Adam. Plain SGD is very unfair comparison as it is almost never used in practice. And this is regardless of what is the black box optimizer they use. The case could be that Adam alone as black box optimizer works as well or better than Adam as black box + NDF.", "title": "Are there any other baseline results?", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}