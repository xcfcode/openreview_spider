{"paper": {"title": "Noise against noise: stochastic label noise helps combat inherent label noise", "authors": ["Pengfei Chen", "Guangyong Chen", "Junjie Ye", "jingwei zhao", "Pheng-Ann Heng"], "authorids": ["~Pengfei_Chen1", "gy.chen@siat.ac.cn", "kourenmu@gmail.com", "~jingwei_zhao1", "~Pheng-Ann_Heng1"], "summary": "SGD noise induced by stochastic label noise helps escape sharp minima and prevents overconfidence, hence can mitigate the effects of inherent label noise and improve generalization.", "abstract": "The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect, previously studied in optimization by analyzing the dynamics of parameter updates. In this paper, we are interested in learning with noisy labels, where we have a collection of samples with potential mislabeling. We show that a previously rarely discussed SGD noise, induced by stochastic label noise (SLN), mitigates the effects of inherent label noise. In contrast, the common SGD noise directly applied to model parameters does not. We formalize the differences and connections of SGD noise variants, showing that SLN induces SGD noise dependent on the sharpness of output landscape and the confidence of output probability, which may help escape from sharp minima and prevent overconfidence. SLN not only improves generalization in its simplest form but also boosts popular robust training methods, including sample selection and label correction. Specifically, we present an enhanced algorithm by applying SLN to label correction. Our code is released.", "keywords": ["Noisy Labels", "Robust Learning", "SGD noise", "Regularization"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "The paper studies the effect of explicitly introducing stochastic label noise into SGD updates, showing both theoretically and empirically that this can improve model performance on datasets with \"inherent\" label noise. The intuition is that this helps the model escape sharp local minima, where predictions may be overconfident. \n\nReviewers broadly found the work to be conceptually and theoretically interesting, and the empirical results are promising. The paper is thus well-posed to be of broad interest to the community."}, "review": {"S6Kx7Z5-rQl": {"type": "review", "replyto": "80FMcTSZ6J0", "review": "Summary:\nmitigate inherent label noise\nThis paper studies the effect of applying SGD noise on mitigating the inherent label bias which is common in real-world datasets. It introduces stochastic label noise (SLN), a variant of SGD noise induced by controllable label noise. It formalizes connections between SLN and two other existing SGD noise variants (Proposition 1-3). With such propositions, it shows that SLN can help the model to avoid sharp minima and prevent overconfidence (Claim 1-2). The experiments show that SLN helps improve generalization than baseline methods and can be further used to boost robust training methods on CIFAR10, CIFAR100, CLOTH-1M under five different types of noise settings. Apart from vanilla SLN, it further proposes momentum model (MO) and label correction (LC). Combining them together with SLN can further boost test accuracy for label-correction.\n\n\n################################################\n\nReasons for score:\nThe paper is overall very well-written and gives theoretical insight on the connections of different SGD noise variants. It further provides comprehensive experiments both qualitatively and quantitatively to validate the effectiveness of its proposed SGD noise variant, SLN. The results show that SLN can simplify parameter tuning, producing superior results on label-correction without additional computational overheads.\n\n################################################\n\nPros:\n\n+very well-written and easy to follow in general\n\n+great connection with as well as comprehensive discussions of related work\n\n+comprehensive experiments along with good visualization\n\n+the proposed method gives better performance without overhead and can be used to enhance existing methods\n\nCons:\n\n-might be better to also give empirical evidence to support the claim of helping escaping sharp minimums. e.g. a visualization of gradient landscapes for CE and SLN, respectively.\n\n\n################################################\n\nQuestions:\n\n-As I mentioned in the cons, is there any empirical evidence of escaping sharp minima you observed to further support the theoretical finding?\n\n-I see that in Fig 5, you give a qualitative visualization of using different sigma. Did you also do any quantitative ablation study on the hyperparameter sigma? How sensitive the results would be by choosing different sigma?\n\n################################################\n\nPost Rebuttal Update: the authors have well addressed my concerns, in particular (1) the additional visualization gives a good qualitative empirical evidence supporting the claim that SLN helps escaping sharp minima. (2) the search process for the hyperparameter $\\sigma$ is very reasonable and makes the usage of SLN practical. I will keep my initial assessment and vote for accepting this paper.\n", "title": "Nice formalization of SGD noises variants with intuitive theoretical justification and comprehensive empirical results", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "XK4wQLuYw8l": {"type": "rebuttal", "replyto": "QlZVu9Lnsiq", "comment": "Yes, we do have a guideline for tunning $\\sigma$. Given a new dataset with unknown noise, we suggest quickly search the best $\\sigma$ (denoted as $\\sigma_{0}$) based on the binary search using the validation accuracy throughout training.  1) If we observe a decrease of validation accuracy at late stage of training, it implies overfitting and $\\sigma<\\sigma_{0}$. 2) Otherwise, we have $\\sigma\\geq\\sigma_{0}$. Based on 1) and 2), we can conduct a binary search for $\\sigma$, which can quickly find the best $\\sigma$ especially if one would like to search $\\sigma$ in a very detailed range. Finally, we would like to emphasize that tuning the hyperparameter for SLN is easy compared with many baselines since we simply need to tune $\\sigma$. And in our paper, we achieve good results by simply tuning $\\sigma$ in {0.1, 0.2, 0.5, 1}.  \n\nPreviously, both $z_f$ and SLN are not adopted in learning with noisy labels. Our originality includes adopting the techniques in combating noisy labels and notably, the analysis of the effects of SGD noise variants, which are supported by the empirical results and visualizations. Based on our analysis, $z_f$ also has the effects of escaping sharp minima; hence it endows robustness. SLN is better in terms of reducing overconfidence. ", "title": "To Reviewer 5. Thanks for your feedback! Thanks for appreciating the additional results and visualizations!"}, "vBmuXT8PHWV": {"type": "rebuttal", "replyto": "4pFzv_FYZjh", "comment": "Looking forward to your updates.\n\nSincerely,  \nPaper568 Authors.\n", "title": "To Reviewer 2. Thanks for your positive feedback on the updated paper. Please feel free to discuss if there is anything else to clarify."}, "hrJCNdyhg-": {"type": "review", "replyto": "80FMcTSZ6J0", "review": "**Summary** The paper tackles the problem of training under noisy labels. It proposes adding random  zero-mean Gaussian noise to the labels during training. It is shown that such a noise induces a variable gradient noise which adaptively increases 1) when the learnt network function has higher curvature around training points and 2) when the output prediction has lower entropy (higher confidence) for the training points. It is claimed that the former property helps avoiding sharper minima which generally improves generalization. The second property avoids overfitting to noise in similar fashion to label smoothing. \n\n**Quality** The paper is well written. The \u201crelated works\u201d section is quite thorough but concise when covering the field of learning under label noise. Certain parts such as the tSNE plot (Figure 5) and the time-vs-accuracy plot (Figure 6) do not seem to be central to the paper and are not informative. Instead, certain parts could have been discussed more thoroughly (see the detailed technical comments below).\n\n**Clarity** The claims and contributions are generally clear from the paper. The reasoning behind certain claims could be better clarified. Also, details are missing on the hyperparameter optimization of the baselines. Details come below.\n\n**Originality** The method is close to other works in the analysis of noisy gradients for better generalization as well as the usage of label smoothing and random label perturbation [a] for generalization and learning under label noise. However, to the knowledge of the reviewer, this work combines the ideas of the two directions in a coherent and original way.\n\n**Significance** The experiments are done on several setups and using 5 different independent runs for the baselines and three variants of the proposed method. The results show significant and consistent improvements. However, certain experiments could be added to better support the detailed claims as opposed to merely reporting best final numbers.\n\n**Major technical comments**\n\n*Experiments*\n1. Interesting and informative side experiments including 1) the separation of noisy and correctly labeled data when using SLN compared to label smoothing, and 2) the strength of SLN compared to unnoisy cross entropy loss indicating the suitability of SLN in identifying correctly labelled samples (low-loss regime) and correcting noisy labels (high-loss regime).  \n2. 5 different synthetic noise types are used for the experiments on CIFAR10 and CIFAR100.\n3. the paper has two clear claims regarding the sharpness of the found local minimum and on overconfident predictions. While the experiments show clear improvements of the results across various settings and compared to different baselines, the connection of the improvements to the claims remain largely unsubstantiated. As such, the paper is missing direct experiments supporting the claims and/or shedding some light on them. Some suggestions are as follows:\n\n     3.1. implement the noise as in proposition 3 i.e., directly applying the noise to the gradient. Then, one can modify the noise to decouple the two components and demonstrate individual contributions.\n\n     3.2. quantitatively analyze the jacobian of the learnt network function and/or the sharpness of the local minimum with and without the added noise and for the different kinds of noise to directly investigate the first claim.\n4. the performance of some of the baselines are low. Are the baselines reimplemented? How are the general training hyperparameters (learning rate, weight decay, batchsize, etc) and method-specific hyperparameters optimized? Is there a different set of optimized hyperparameters per baseline? What are them?\n5. reading the end part of section 3.3 it seems that in SLN+MO+LC the label correction starts only after the full convergence of a SLN-only training. In light of this, how is that the increase in time complexity of SLN+MO+LC is negligible in Table 1? What is the stopping criteria for the initial training and then retraining with LC?\n\n*Theory*\n1. The paper misses to cite a relevant paper [a] that also randomizes the labels for avoiding overfitting and demonstrates better generalization. Regarding this, the paper should clearly acknowledge [a] when it comes to claiming the novelty of the noisy-label approach and also when it discusses the advantages of perturbing labels -- [a] discusses similarities to ensemble approaches. That being said, I believe the paper has enough originality on top of [a]: for instance [a] replaces provided labels with independent noise, does not experiment on learning under noisy labels and, the gradient noise analysis of the paper is complementary to [a]. \n2. When correcting the labels using SLN, why is the weight of the given label increases as the sample loss increases? Shouldn\u2019t it be the opposite based on figure 3? I found that there are some discussions provided in appendix C. However, as this goes against the previously published work it deserves more formal discussion and corresponding experiments in the main paper.\n3. From what I understand proposition 3 shows that functions that are smoother at training points will receive lower variance. If so, formal discussions are missing to connect smooth functions and flat minima.\n4. As the training continues the loss tends to get smaller by getting the function closer to the given one-hot labels at the training data points. From proposition 3 it is argued that as this happens the noise in the gradient increases. This raises a caveat regarding convergence. A theoretical discussion and/or empirical observation are needed to study the convergence. For instance, does the variance of the model increase towards the end of the training or does it actually converge to a solution that is robust to the gradient noise (remains approximately unchanged in the functional space)?\n\n**Overall** In the reviewer\u2019s view, the paper has clear merits in bridging between the theory of gradient noise and label smoothing for learning under label noise, both empirically and theoretically. However, it can benefit from more clarity and additional informative experiments to better understand the effect of the proposed noise.\n\n[a] \"DisturbLabel: Regularizing CNN on the Loss Layer\", CVPR 2016\n\n**Post Rebuttal Update** \nThe authors address many of the concerns, 1) [a] is properly acknowledged in the revised version and novelty is not claimed on additional label noise in the text, 2) while quantitative studies are still absent for claims on sharp vs. flat minima, qualitative results are provided for convergence to \"flatter\" minima  3) connections between smooth functions and generalization is discussed 4) answers and updates regarding complexity and convergence are *somewhat* convincing. Thus, I am willing to increase the score from 6 to 7 and confidence from 3 to 4 as I believe the paper provides relevant and interesting theoretical arguments.", "title": "The paper is original and brings consistent improvements. However, the connection of the claims to the performance improvement remain empirically unsubstantiated. Furthermore, the paper can improve its clarity.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "cBNLqS4SQcC": {"type": "rebuttal", "replyto": "4pFzv_FYZjh", "comment": "Yes, the work (Achille & Soatto, 2018) supports our method. We update the paper and add a discussion in the first paragraph of Section 3.2.\n\nFor our SLN, we analyze and illustrate the effect of leading to flat minima and show better generalization performance. The work (Achille & Soatto, 2018) supports our method because it theoretically bridges the gap between flat minima and better generalization performance: 1) Flat minima have low information between the weights and the data (Section 4.3 and Proposition 4.3 in their paper); 2) low information reduces overfitting (Section 4 Eq. (2) and the discussions in their paper). Thanks for pointing out the methods (Harutyunyan et al., 2020;\nXie et al., 2020) that follow the finding in Achille & Soatto (2018). We cite them in the updated paper.\n\n[Achille & Soatto, 2018] Emergence of Invariance and Disentanglement in Deep Representations, JMLR, 2018.  \n[Harutyunyan et al., 2020] Improving Generalization by Controlling Label-Noise Information in Neural Network Weights. ICML, 2020.  \n[Xie et al., 2020] Artificial Neural Variability for Deep Learning: On Overfitting, Noise Memorization, and Catastrophic Forgetting. arXiv 2020.  ", "title": "Thanks for acknowledging that our additional results support the claim well. Thanks for pointing out the reference, which further supports our method."}, "Ub34nuxINS": {"type": "rebuttal", "replyto": "7rU7TtgTBAw", "comment": "Thanks for the comments.\n\n1. **More experimental explorations on the effectiveness of SLN.**  \nYes, SLN is a rather flexible method that is promising to improve existing methods. In the updated paper, we add Table 4 in Appendix C. We integrate SLN with three methods, including Co-Teaching, SIGUA, DivideMix, showing that SLN consistently improves these methods.  \nWe compare all methods in a fair setting: training the same backbone wide ResNet-28-2 for 300 epochs without learning rate change. Though a more powerful backbone and subtle learning rate schedule may yield better results, the additional results in Table 4 should be sufficient to verify the effectiveness of SLN: the methods obtain consistent improvement when integrated with SLN.\n\n2. **The clean sample ratio w.r.t different loss intervals after the first few epochs.**  \nAccording to the memorization effect (Arpit et al., 2017), DNNs tend to learn simple and correct patterns first before memorizing noise. Therefore, after the first few epochs, we still expect a higher clean sample ratio for the small-loss part. However, since the prediction accuracy is low after the first few epochs, there shall be many correctly or wrongly labeled samples with wrong predictions, i.e., the overall blue and green regions should be larger compared with the converged model.\n[Arpit et al., 2017] A closer look at memorization in deep networks. ICML 2017.\n\n3. **The superiority of SLN to conventional label smoothing (LS).**  \nSLN is not simply a new strategy of label smoothing. Please refer to Section 3.2 and Figure 3 in the updated paper, where we present a direct comparison between SLN and LS. The effects of SLN are very different from LS.   \n$\\bullet$ LS introduces a fixed and biased modification on labels to reduce overconfidence. Throughout the training, the labels are fixed.  \n$\\bullet$ SLN induces SGD noise that helps 1) escaping from sharp minima and 2) preventing overconfidence. The effects rely on random perturbations induced in each training step, as justified by Propositions 1-3 and Claims 1-2.  \nFor example, around a sharp minimum, SLN yields SGD noise with high variance and \u2018random directions\u2019 so that the noise helps escape. While for LS, the fixed labels do not induce dynamic perturbations so that the training always follows the direction of gradient descent.", "title": "To Reviewer 3. We present more results and discussions according to the comments."}, "II9mKzBcSxo": {"type": "rebuttal", "replyto": "80FMcTSZ6J0", "comment": "The analysis and empirical results presented in this paper should interest not only the subfield of learning with noisy labels but also a wide area of optimization/regularization. The additional visualizations of sharp/flat minima are very interesting and support our claims very well. The major updates in the paper are as follows.\n\n1. **Figure 2 in Section 3.2: visualizations of sharp/flat minima.**  \nWe visualize loss landscapes of converged models to verify our claim of escaping from sharp minima. The loss is averaged on all training samples. The landscapes are visualized using the technique in Li et al. (2018), which perturbs the model parameters in two directions on a normalized scale. Notably, the results exactly fit our analysis. (a): The model trained with CE converges to a sharp minimum. (b): Training with Eq. (1) yields a minimum with a higher loss, yet it is still sharp. (c)&(d): Consistent with our analysis, the model trained with Eq. (2) or Eq. (3) (SLN) converges to a flat minimum.  \n[Li et al., 2018] Visualizing the loss landscape of neural nets. NeurIPS 2018.\n\n2. **Table 4 in Appendix C: more results showing that SLN improves existing robust learning methods.**  \nWe integrate SLN with three methods, including Co-Teaching, SIGUA, DivideMix. We do not expect vanilla SLN to achieve state-of-the-art results compared with many integrated methods. Still, SLN can be a promising option in the family of robust learning methods. It can improve existing methods, as appreciated by Reviewer 3.  \nWe compare all methods in a fair setting: training the same backbone wide ResNet-28-2 for 300 epochs without learning rate change. Though a more powerful backbone and subtle learning rate schedule may yield better results, the additional results in Table 4 should be sufficient to verify the effectiveness of SLN: the methods obtain consistent improvement when integrated with SLN. \nWe summarize the results here.  \n\n    |    |  Symmetric  |  Asymmetric  |  Dependent  |  Open-Set  |\n    | :----: | :----: |  :----: |  :----: |  :----: |\n    Co-Teaching | 82.37\u00b10.32 | 79.61\u00b11.06 | 76.69\u00b10.66 | 85.21\u00b10.35 |\n    SLN-Co-Teaching | 84.22\u00b10.43 | 87.79\u00b10.17 | 80.37\u00b10.22 | 90.37\u00b10.32 |\n    **Improvement** | **+1.85** | **+8.18** | **+3.68** | **+5.16** |\n    SIGUA | 83.76\u00b10.67 | 78.24\u00b11.41 | 76.67\u00b10.97 | 86.70\u00b10.62 |\n    SLN-SIGUA | 84.27\u00b10.41 | 87.65\u00b10.94 | 80.09\u00b10.68 | 90.38\u00b10.24 |\n    **Improvement**  | **+0.51** | **+9.41** | **+3.42** | **+3.68** |\n    DivideMix | 90.38\u00b10.34 | 87.88\u00b10.45 | 82.21\u00b10.37 | 90.49\u00b10.62 |\n    SLN-DivideMix | 90.87\u00b10.28 | 89.31\u00b10.39 | 82.86\u00b10.41 | 91.65\u00b10.59 |\n    **Improvement** | **+0.49** | **+1.43** | **+0.65** | **+1.16** |\n3. **Figure 6 in Section 4: the ablation study on $\\sigma$.**  \nSLN simply requires tuning $\\sigma$, which is tuned in {0.1, 0.2, 0.5, 1}. On CIFAR10 and CIFAR-100, following Zhang & Sabuncu (2018), we use 5k noisy samples (10% of the training data) to tune $\\sigma$. On Clothing1M, following the standard setting (Patrini et al., 2017), we use the clean validation set to tune $\\sigma$.  \nWe add Figure 6 to show an ablation study on $\\sigma$ on CIFAR-10 by changing the value around the best one found in our original experiments. Notably, the figure implies that we can even achieve better results than reported in Table 1 with a more detailed hypermarameter search.\n\n4. **The convergence.**  \nWe add Appendix E to discuss the convergence. The visualizations of loss landscapes (Figure 2 in the updated paper) show that **the model trained with SLN converges to a solution that has small SGD noise**. Firstly, the center point on the visualized landscape (i.e., the loss of the given model) is a local minimum. From Figure 2 (d), we observe that the minimum has the following properties.  \n$\\bullet$ The gradient around the minimum is small since it is flat.  \n$\\bullet $ The predictions \u201cdo not approach one-hot labels\u201d because the loss at the local minimum is high. As shown in Figure 3, the prediction probabilities are much lower than 1.  \nWith the above two properties, Proposition 3 implies that around the flat minimum illustrated in Figure 2 (d), the noise on gradients is small. Therefore, the model converges in the local flat minimum.\n\n5. **Please refer to the individual response for more detailed discussions.**", "title": "[Paper updated]. Common response to all the reviewers. Thanks for the very detailed constructive comments. We polish the paper with additional empirical results and discussions according to the comments."}, "t85ooWyO9jd": {"type": "rebuttal", "replyto": "S6Kx7Z5-rQl", "comment": "Thanks for the comments.\n\n1. **Visualizations of landscapes.**  \nWe add Figure 2 with visualizations of sharp/flat minima. The figure supports our claim very well. The model trained with SLN converges to a flat minimum, while the model trained with CE converges to a sharp minimum. Please refer to the common response and Figure 2 in the updated paper for more details.\n\n2. **Ablation study on the hyperparameter $\\sigma$.**  \nWe add Figure 6 to show an ablation study on $\\sigma$ on CIFAR-10 by changing the value of $\\sigma$ around the best value found in our original experiments. Note that in the original submission, $\\sigma$ is tuned in {0.1, 0.2, 0.5, 1}. On CIFAR10 and CIFAR-100, following Zhang & Sabuncu (2018), we use 5k noisy samples (10% of the training data) to tune $\\sigma$. On Clothing1M, following the standard setting (Patrini et al., 2017), we use the clean validation set to tune $\\sigma$.", "title": "To Reviewer 5. We present more empirical results and visualizations accordingly."}, "ctV7QohQrpN": {"type": "rebuttal", "replyto": "hlsx5-pGMDY", "comment": "Thanks for the comments.\n\n1. **More results to verify the effectiveness of SLN.**  \nIn the updated paper, we add Table 4 in Appendix C. We integrate SLN with three methods, including Co-Teaching, SIGUA, DivideMix, showing that SLN consistently improves these methods. We do not expect vanilla SLN to achieve state-of-the-art results compared with many integrated methods. Still, SLN can be a promising option in the family of robust learning methods. It can improve existing methods, as appreciated by Reviewer 3. Table 4 verifies this point. Please refer to the common response for a summary of the results.\n\n2. **Illustrations of sharp/flat minima.**  \nWe add Figure 2 with visualizations of sharp/flat minima. The figure supports our claim very well. The model trained with SLN converges to a flat minimum, while the model trained with CE converges to a sharp minimum. Please refer to the common response and Figure 2 in the updated paper for more details.\n\n3. **The convergence.**  \nPlease refer to the common response.\n\n4. **How to tune the standard deviation $\\sigma$.**  \nWe briefly discuss the hyperparameter optimization in Section 4 and provide a detailed discussion in Appendix B. $\\sigma$ is tuned in {0.1, 0.2, 0.5, 1}. On CIFAR10 and CIFAR-100, following Zhang & Sabuncu (2018), we use 5k noisy samples (10% of the training data) to tune $\\sigma$. On Clothing1M, following the standard setting (Patrini et al., 2017), we use the clean validation set to tune $\\sigma$.  \nMoreover, we add Figure 6 in Section 4 to show an ablation study on $\\sigma$ on CIFAR-10.", "title": "To Reviewer 2. We present additional empirical results accordingly and further clarify our analysis and claims."}, "EMraREksp0Q": {"type": "rebuttal", "replyto": "hrJCNdyhg-", "comment": "Thanks for the comments. We move the t-SNE plot and the time-vs-accuracy plot to the appendix and discuss certain parts more thoroughly according to the comments (see the detailed response below).\n\nExperiments\n\n1. No question.\n\n2. No question.\n\n3. **Direct experiments supporting the claims and/or shedding some light on them.**  \n$\\bullet$ For the sharp/flat minima, we add visualizations of landscapes in Figure 2. The figure supports our claim very well. More details can be found in the common response and the updated paper. The model trained with noise in Eq. (2) or Eq. (3) converges to flat minima.  \n$\\bullet$ For preventing overconfidence, histograms in Figure 3 (Figure 2 in the original submission) show that the prediction probabilities on both noisy and correctly labeled data are reduced.  \n\n4. **The hyperparameter optimization.**  \nWe update the paper and describe the hyperparameter optimization in detail in Appendix B. On CIFAR-10 and CIFAR100, all methods are reimplemented and fairly compared with 1) the same general training hyperparameters and 2) method-specific hyperparameters tuned. Following Zhang & Sabuncu (2018), we use 5k noisy samples (10% of the training data) to tune hyperparameters. On Clothing1M, published results share the same backbone ResNet-50. Hence, we easily implement the methods by following the suggested hyperparameters. The result of DivideMix is reproduced from its official implementation. More details can be found in Appendix B.\n\n5. **The stopping criteria for the initial training and then retraining with label-correction (LC).**  \nYes, we should introduce LC after the convergence of a SLN-only training. The increase in time complexity of SLN+MO+LC is negligible because we apply LC at the 250th epoch without tuning and all models are still trained for 300 epochs in total. The reasons are: 1) we find that the training accuracy does not increase much in the last 50 epochs (indicating convergence); 2) we can avoid increasing the time complexity.\n\n\nTheory\n\n1. **A relevant paper [a] \"DisturbLabel: Regularizing CNN on the Loss Layer\", CVPR 2016.**  \nWe update the paper to acknowledge [a] in the related works, the method section and when discussing the effects. Yes, our originality includes analysis of SGD noise variants and the effects, and experiments in learning with noisy labels.\n\n2. **The weight in label-correction (LC).**  \nWe polish the discussion in Appendix D to make it clearer. Figure 10 provides empirical justifications. In summary, the reasons are as follows.  \n$\\bullet$ For small-loss samples, we have $S \\approx y$. LC does not affect these samples much regardless of the weight.  \n$\\bullet$ Samples that can benefit from LC have large-loss, but for these samples, a higher loss does not mean that it requires a higher weight on the prediction S (an example is provided in the updated paper).  \n$\\bullet$ There exist large-loss samples for which label-correction can be harmful because the prediction accuracy is not 100%.  \nTherefore, we add small weights on the prediction S for large-loss samples to correct the labels slightly. More detailed discussions can be found in Appendix D.\n\n3. **Smooth functions and flat minima.**  \nYes, functions that are smoother at training points will receive a lower variance. Still, the smoothness is characterized w.r.t. change in model parameters $\\theta$ rather than the input x, because the gradient is w.r.t. $\\theta$ rather than x. Around a flat minimum, the gradient of loss w.r.t. $\\theta$ is small. The loss is averaged on batches of samples in training and averaged on the whole training set when we visualize the loss landscapes in Figure 2.\n\n4. **The convergence.**  \nPlease refer to the common response.", "title": "To Reviewer 1. According to the comments, we present more empirical results to verify the claims and polish the discussions to improve clarity."}, "hlsx5-pGMDY": {"type": "review", "replyto": "80FMcTSZ6J0", "review": "This paper studies learning robust models with noisy labels. The authors argue that a specific SGD noise induced by stochastic label noise (SLN) can mitigate the effect of label noise. But the common SGD noise cannot achieve this. Then they apply the proposed SLN induced SGD noise to the existing label-correction methods for noisy-label learning and provide some experimental results.\n\nPros:\n\n-The paper provides an interesting view of SGD noise in the lens of noisy labels. They claim that common SGD noise does not endow much robustness against label noise, but using a variant SGD noise by label perturbations can improve the generalization and boost existing robust training methods.\n\nCons:\n\n-Learning with noisy labels is a hot research area as reviewed in the related work section. It seems that the selected baselines in the experiments are not representative and state-of-the-art methods. For example, Yu et al. (2019) improves co-teaching Han et al. (2018) and should be compared instead. \n\nAnd some representative regularization based methods should also be compared:\n\n-Mixup: Beyond Empirical Risk Minimization, Zhang et at., ICLR 2018\n\n-Virtual adversarial training: a regularization method for supervised and semi-supervised learning, Miyato et al., TPAMI 2019,\n\n-SIGUA: Forgetting May Make Learning with Noisy Labels More Robust, Han et al., ICML 2020\n\nsince they are more related to the essence of training networks as claimed in the paper. \n\n-In Claim 1 and 2, it is said that with SGD noise induced by SLN training is difficult to converge in some cases. How to guarantee convergence of the proposed algorithm? Some convergence analysis under reasonable assumptions may be helpful.\n\n-It is claimed in the paper that training without SGD noise under label noise can converge to sharp minima, and SLN helps escape from the sharp minima. It is not very intuitive. Could the authors explain more on it, maybe adding some citations or experimental results could be helpful.\n\n-It is still unclear to me how to tune the standard deviation sigma in practice, which should be an important factor that affects the performance.\n\n-The clarity of the paper could be improved, for example, adding brief proof sketches to the theorems may help for better understanding.\n\nOverall, the paper provides some interesting analysis of SGD noise and label noise, but many unclear parts need to be clarified.", "title": "Interesting work on SGD noise and label noise, but many unclear parts need to be clarified", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "7rU7TtgTBAw": {"type": "review", "replyto": "80FMcTSZ6J0", "review": "This paper studies the noisy stochastic gradient descent algorithm in noisy label learning. Concretely, the authors added Gaussian noise on the labels rather than the gradient itself. By comparing different noisy SGD algorithms, the authors demonstrate that the proposed SLN algorithm not only help the model to escape from sharp local minima, but also help it to be over confidential. Finally, the authors demonstrate that SLN outperforms some classical SGD methods.\n\nPros:\n- The proposed idea is novel to me. Theoretically, the proposed SLN framework jointly enjoys the ability to escape from sharp saddle points and make the prediction smooth. The authors provide a new perspective to develop robust learning algorithms.\n- It seems that SLN method can be integrated with many state-of-the-art noisy label learning models.\n- The experimental results in Figure 3 is very promising. The small loss samples are generally clean, which may help improve the performance of many sample-selection based approaches.\n\nCons:\n- I have one main concern. While Figure 3 shows very good results, I noticed that quantitative results are far away from state-of-the-art models. Compared to state-of-the-art models, such as DivideMix, SLN demonstrates far lower accuracy. For example, on CIFAR-10, Asymmetric noisy with 40% noise, the accuracy of DivideMix is 92~93.4% and SLN-MO-LC is 87.85%. Although state-of-the-art performance is not the most essential for me, I think the authors require more experimental exploration. Since SLN is a rather flexible method, it can be integrated with many state-of-the-art models and I believe the performance would be competitive or at least at the same level as the SOTA models. \n\nMinor comments:\n- While Figure 3 shows the clean sample ratio of converged models w.r.t different loss intervals, what would it be after the first few epochs?\n- From my point of view, SLN is actually a new strategy of label smoothing (or not?). May the authors explain the superiority of SLN to conventional label smoothing methods?\n\nOverall, I think this paper brings an interesting idea to the community, but the experiments are not enough for me.\n", "title": "Very interest idea, but the empirical studies are not sufficient", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}