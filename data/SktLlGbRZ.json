{"paper": {"title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "authors": ["Judy Hoffman", "Eric Tzeng", "Taesung Park", "Jun-Yan Zhu", "Phillip Isola", "Kate Saenko", "Alyosha Efros", "Trevor Darrell"], "authorids": ["jhoffman@eecs.berkeley.edu", "etzeng@eecs.berkeley.edu", "taesung_park@berkeley.edu", "junyanz@berkeley.edu", "isola@eecs.berkeley.edu", "saenko@bu.edu", "efros@eecs.berkeley.edu", "trevor@eecs.berkeley.edu"], "summary": "An unsupervised domain adaptation approach which adapts at both the pixel and feature levels", "abstract": "Domain adaptation is critical for success in new, unseen environments.\nAdversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.\nRecent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at  mapping images between domains, even without the use of aligned image pairs.\nWe propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.\nCyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.  Our model can be applied in a variety of visual recognition and prediction settings.\nWe show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.", "keywords": ["domain adaptation", "unsupervised learning", "classification", "semantic segmentation"]}, "meta": {"decision": "Reject", "comment": "I concur with two of the reviewers: the work is somewhat incremental in terms of technical novelty (it's effectively CycleGANs for domain adaptation with a couple of effective tricks) and the need/advantage of the cycle consistency loss is not demonstrated sufficiently. The only solid ablation evidence seems to the the SVHN-->MNIST experiment from post-submission; I would personally like to see this kind of empirical proof extended much further (the fact that Shrivastava et al.'s method doesn't work well on GTA-->Cityscapes is not itself proof that cycle consistency is needed). With more empirical evidence I can see this paper being a good candidate for a computer vision conference like CVPR or ICCV."}, "review": {"S1Elwq_xf": {"type": "review", "replyto": "SktLlGbRZ", "review": "This paper proposed a domain adaptation approach by extending the CycleGAN with 1) task specific loss functions and 2) loss imposed over both pixels and features. Experiments on digit recognition and semantic segmentation verify the effectiveness of the proposed method.\n\nStrengths:\n+ It is a natural and intuitive application of CycleGAN to domain adaptation. \n+ Some of the implementation techniques may be useful for the future use of CycleGAN or GAN in other applications, e.g., the regularization over both pixels and features, etc.\n+ The experimental results are superior over the past.\n+ The translated images in Figure 6 are amazing. Could the authors show more examples and include some failure cases (if any)?\n\nWeaknesses:\n- The presentation of the paper could be improved. I do not think I can reproduce the experimental results after reading the paper more than twice. Many details are missing and some parts are confusing or even misleading.  As below, I highlight a few points and the authors are referred to the comments by Cedric Nugteren for more suggestions.\n\n-- Equation (4) is incorrect.\n-- In the introduction and approach sections, it reads like a big deal to adapt on both the pixel and feature levels. However, the experiments fail to show that these two levels of adaptation are complementary to each other. Either the introduction is a little misleading or the experiments are insufficient. \n-- What does the \u201cimage-space adaptation\u201d mean?\n-- There are three fairly sophisticated training stages in Section 4.2. However, the description of the three stages are extremely short and ambiguous. \n-- What are exactly the network architectures used in the experiments?\n\n- The technical contribution seems like only marginal innovative. \n- The experiments adapting from MNIST to SVHN would be really interesting, given that the MNIST source domain is not as visually rich as the SVHN target. Have the authors conducted the corresponding experiments? How are the results? \n\nSummary:\nThe proposed method is a natural application of CycleGAN to domain adaptation. The technical contribution is only marginal. The results on semantic segmentation are encouraging and may motivate more research along this direction. It is unfortunate that the paper writing leaves many parts of the paper unclear. \n\n=========================================\nPost rebuttal:\n\nThe rebuttal addresses my first set of questions. The revised paper describes more experiment details, corrects equation (4), and clarifies some points about the results. \n\nThis paper applies the cycle consistent GAN to domain adaptation. I still think the technical contribution is only marginally innovative. Nonetheless, I do not weigh this point too much given that the experiments are very extensive. \n\nThe rebuttal does not answer my last question. It would be interesting to see what happens to adapt from MNIST to SVHN, the latter of which contains more complicated background than the former. \n", "title": "Novelty incremental, results encouraging, writing could be improved", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SyFscqngM": {"type": "review", "replyto": "SktLlGbRZ", "review": "This paper essentially uses CycleGANs for Domain Adaptation. My biggest concern is that it doesn't adequately compare to similar papers that perform adaptation at the pixel level (eg. Shrivastava et al-'Learning from Simulated and Unsupervised Images through Adversarial Training' and Bousmalis et al - 'Unsupervised Pixel-level Domain Adaptation with GANs', two similar papers published in CVPR 2017 -the first one was even a best paper- and available on arXiv since December 2016-before CycleGANs). I believe the authors should have at least done an ablation study to see if the cycle-consistency loss truly makes a difference on top of these works-that would be the biggest selling point of this paper. The experimental section had many experiments, which is great. However I think for semantic segmentation it would be very interesting to see whether using the adapted synthetic GTA5 samples would improve the SOTA on Cityscapes. It wouldn't be unsupervised domain adaptation, but it would be very impactful. Finally I'm not sure the oracle (train on target) mIoU on Table 2 is SOTA, and I believe the proposed model's performance is really far from SOTA.\n\nPros:\n* CycleGANs for domain adaptation! Great idea!\n* I really like the work on semantic segmentation, I think this is a very important direction\n\nCons:\n* I don't think Domain separation networks is a pixel-level transformation-that's a feature-level transformation, you probably mean to use Bousmalis et al. 2017. Also Shrivastava et al is missing from the image-level papers.\n* the authors claim that Bousmalis et al, Liu & Tuzel and Shrivastava et al ahve only been shown to work for small image sizes. There's a recent work by Bousmalis et al. (Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping) that shows these methods working well (w/o cycle-consistency) for settings similar to semantic segmentation at a relatively high resolution. Also it was mentioned that these methods do not necessarily preserve content, when pixel-da explicitly accounts for that with a task loss (identical to the semantic loss used in this submission)\n* The authors talk about the content similarity loss on the foreground in Bousmalis et al. 2017, but they could compare to this method w/o using the content similarity or using a different content similarity tailored to the semantic segmentation tasks, which would be trivial.\n* Math seems wrong in (4) and (6). (4) should be probably have a minus instead of a plus. (6) has an argmin of a min, not sure what is being optimized here. In fact, I'm not sure if eg you use the gradients of f_T for training the generators?\n* The authors mention that the pixel-da approach cross validates with some labeled data. Although I agree that is not an ideal validation, I'm not sure if it's equivalent or not the authors' validation setting, as they don't describe what that is.\n* The authors present the semantic loss as novel, however this is the task loss proposed by the pixel-da paper.\n* I didn't understand what pixel-only and feat-only meant in tables 2, 3, 4. I couldn't find an explanation in captions or in text\n\n\n=====\nPost rebuttal comments:\nThanks for adding content in response to my comments. The cycle ablation is still a sticky point for me, and I'm still left not sure if cycle-consistency really offers an improvement. Although I applaud your offering examples of failures for when there's no cycle-consistency, these are circumstantial and not quantitative.  The reader is still left wondering why and when is the cycle-consistency loss is appropriate. As this is the main novelty, I believe this should be in the forefront of the experimental evaluation. ", "title": "Great problem and idea, but without adequate experiments that show that cycle-consistency is the cause of the improvement", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S14j0RTxM": {"type": "review", "replyto": "SktLlGbRZ", "review": "This paper proposes  a natural extension of the CycleGAN approach. This is achieved by leveraging the feature and semantic losses to achieve a more realistic image reconstruction. The experiments show that including these additional losses is critical for improving the models performance.  The paper is very well written and technical details are well described and motivated. It would be good to identify the cases where the model fails and comment on those. For instance, what if the source data cannot be well reconstructed from adapted target data? What are the bounds of the domain discrepancy in this case? ", "title": "This paper extends the previous work on CycleGAN by coupling it with adversarial adaptation approaches. The extension includes a new feature and semantic loss in the overall objective of the CycleGAN. While this extension is straightforward, it is novel. The experimental validation is extensive and clearly shows the benefits of the proposed extension. ", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "B1upY6WmM": {"type": "rebuttal", "replyto": "SyFscqngM", "comment": "Thank you for your comments. We have included new experiments and text edits per your suggestion.\n\nHigher performing semantic segmentation models\n======================================\nFirst, we added a new experiment for GTA->CityScapes adaptation with a newer semantic segmentation model. Again, we found that for this experiment, feature space adaptation alone provided a large improvement (21 mIoU -> 31 mIoU), pixel adaptation alone resulted in a substantial improvement (21->37 mIoU) and finally, combining feature space with pixel space adaptation provided the largest performance (21->39). \n\nCycle Ablation\n===========\nWe added a new ablation experiment to the SVHN->MNIST setting where the cycle loss is removed while the semantic loss remains. This version was still susceptible to label flipping and understandably failed at the task of reconstruction (see Figure 3b).\n\nComparison to other Pixel Level Approaches\n==================================\nWe ran Shrivastava et al (see Appendix A.2) in the GTA->CityScapes scenario and found that the model was not able to accurately capture the transfer problem, resulting in performance below the original source model.\n\nWe added a citation to the new Bousmalis et al. (2017a) paper on robotic grasping (pg 1 Introduction). Those images are indeed higher resolution than the prior work, but they still do not match the resolution of the dashcam driving images and have significantly lower variation and complexity. In general, optimizing pixel transfer methods with high resolution images remains a challenging problem. Our approach provides one solution by which additional regularization through the pixel cycle loss encourages transfer. We would like to clarify that the comment we made about prior pixel level approaches which \u201cmay not necessarily preserve content\u201d was intended as a potential criticism of pixel based approaches in general, not specifically about Bousmalis et al. (2017b). In fact, in the related work section we explicitly mention that Bousmalis et al. (2017b) uses a content similarity loss on the foreground mask. This is a privileged version of our semantic consistency loss as it requires a known foreground mask on target data. We do not claim to be the first to introduce the use the a task classifier to preserve content. Instead we introduce a model which does pixel transfer through a cycle loss for low level preservation and a semantic loss for preserving semantics in a large domain shift scenario (when all pixels must change significantly). \n\nText Edits\n=======\nThank you for noticing the error in Equation (4). We have updated the text to accurately reflect our description and implementation. In addition, we have added semantic consistency to our new Figure 2 to clarify the use of this objective. \n\nEquation (6) defines the full CyCADA objective and Equation (7) presents the optimization problem.\n\nAppendix A.1 describes architectures, training procedures, and implementation details needed to reproduce our experiments. \n\nWe have revised the method section to clarify the pixel vs feature level transfer which is ablated in the experiments section. In addition the new Figure 2 should offer further clarity.\n", "title": "New semantic segmentation experiments, comparison to Shrivastava et al, cycle ablation, text improvements"}, "SJGM5T-Xz": {"type": "rebuttal", "replyto": "S1Elwq_xf", "comment": "Thank you for your comments. We have made a number of modifications to our manuscript based on your feedback. First, thank you for noticing the error in Equation 4. We have updated it to accurately reflect our description and implementation (our new figure 2 should also clarify its use). We have modified the explanation of image/pixel space adaptation vs feature space adaptation within the main method description and provided headers to guide the reader. We have also added an appendix with an implementation section specifying the network architectures and describing the training procedures. We will release our code, data and models upon publication. We have also followed many of the suggestions from Cedric Nugteren as you have pointed out (please see our response there for the detailed list of changes). \n\nWe would like to clarify that our results show that independently pixel space and feature space adaptation offer performance improvement over no adaptation across all experiments. When combined they provide anywhere from equivalent (as in USPS<->MNIST) to marginal improvement (GTA->CityScapes), to *significantly* better performance (SVHN->MNIST) than either approach alone.  Thus, we propose using both components together.", "title": "Revision: Text clarifications and implementation details in appendix"}, "S1bP5aZQG": {"type": "rebuttal", "replyto": "S14j0RTxM", "comment": "Thank you for your positive feedback and suggestion to study the errors from the model. We have included a section to our appendix illustrating the confusion matrices for the largest domain shift of our digit experiments -- SVHN -> MNIST. In this case we find certain error types are resolved after adaptation while others still remain. Confusion between visually similar classes, such as 1s and 7s, is difficult to resolve without target labels.\n\nIn addition, we have included additional experiments and made updates to further clarify details within our manuscript based on the suggestions from the other reviewers. ", "title": "New revision and analysis for digit experiments"}, "HyQBYTbXz": {"type": "rebuttal", "replyto": "Sy-YBUn1G", "comment": "Thank you for your interest and suggestions. We have addressed Cedric\u2019s comments above. In addition, we have made changes to our method section to clarify the distinction between pixel and feature space adaptation. The new Appendix 6.1 discusses the training procedure which indicates which components are updated in each phase. Equation (4) has been fixed. ", "title": "Revision addresses comments."}, "SyqyFTWmM": {"type": "rebuttal", "replyto": "BJxW87myM", "comment": "Thank you for your suggestions on how to improve the presentation of our algorithm. We have incorporated them into our revised manuscript. Changes are specified below.\n\n* Figure 2: we include a new version of this figure with semantic consistency and feature level transfer together with explicit discriminator blocks.\n\n* Moved (-) outside the expectation in Equation (1)\n\n* Fixed Equation (4)\n\n*Made explicit that the source model is pretrained and fixed. Also see implementation details in the Appendix which reinforces this.\n\n*Pixel and Feature adaptation are clarified in the method section as well as in the appendix implementation section.\n\n* Indeed, we do assume that the label space remains unchanged before and after transfer. In fact, that is exactly what the semantic consistency loss enforces.\nNew semantic segmentation results with the DRN-26 architecture results in higher performance overall. Our findings remain the same.", "title": "Clarifications in new revision"}}}