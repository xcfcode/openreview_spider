{"paper": {"title": "Surprisal-Driven Feedback in Recurrent Networks", "authors": ["Kamil Rocki"], "authorids": ["kmrocki@us.ibm.com"], "summary": "In this paper, we add surprisal as additional input to RNN , which take into account past error information when making new predictions. We extend SOTA on character-level language modelling, achieving 1.37 bits/char on wikipedia dataset.", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.", "keywords": ["Unsupervised Learning", "Applications", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "Based on the feedback, I'm going to be rejecting the paper on the following grounds:\n 1. Results are not SOTA as reported.\n 2. No real experiments other than cursory experiments on Hutter prize data.\n 2. Writing is very poor.\n \n However, just for playing devil's advocate, to the reviewers, I would like to point out that I am in agreement with the author that dynamic evaluation is not equivalent to this method. The weights are not changed in this model, as far as I can see, for the test set. Surprisal is just an extra input to the model. I think the reviewers were puzzled by the fact that at test time, the actual sequence needs to be known. While this may be problematic for generative modeling, I do not see why this would be a problem for language modeling, where the goal of the model is only to provide a log prob to evaluate how good a sequence of text is. Long before language modeling started being used to generate text, this was the main reason to use it - in speech recognition, spelling correction etc.."}, "review": {"Bka0xjVNx": {"type": "rebuttal", "replyto": "r1pQ4-zNe", "comment": "Next-step prediction will likely be used more in the future if model-based RL will be more popular, as many (e.g. Lecun and Bengio) suspect.\n\nEDIT: Although, thinking about it more, since a model in RL is mostly useful for planning, I guess it might not be so useful there.  I think a better application might be compression via entropy encoding.", "title": "Online prediction will be useful for model-based RL"}, "H1-uddlNx": {"type": "rebuttal", "replyto": "SkfVKMe7e", "comment": "In the 'adaptive zoneout' paper, we also tried the algorithm on 'linux source' data - http://olab.is.s.u-tokyo.ac.jp/~kamil.rocki/data/\nWe thought it would be more challenging than enwik8 because it requires more generalization and less memorization, the algorithm works remarkably well.", "title": "linux dataset"}, "r1dQS_gEe": {"type": "rebuttal", "replyto": "rJ9apDymx", "comment": "Regarding points 1 and 2, please see my comments above. Please let me know if they answer your questions. Regarding 3) the notation is simply accumulation - that is instead of writing 'x' += y, we wrote x = x + y", "title": "Thank you"}, "Hyn1VulVx": {"type": "rebuttal", "replyto": "SkfVKMe7e", "comment": "Hello,\n\n1. We tried SF with zoneout (https://arxiv.org/abs/1606.01305) which improved performance and recently we also implemented adaptive zoneout which relies on the feedback information (https://arxiv.org/pdf/1610.07675.pdf) which set new SOTA on enwik8 dataset. It shows that SF is very effective as a driving signal for regularizing network activations.\n\nRegarding batch or layer norm, we haven't tried it yet, but it might be possible to apply it and further improve the result. We tried to keep the changes to the algorithm as minimal as possible, so SF requires just a few lines of code changed. Batch and layer normalization also make inference problematic as far as I understand the algorithm.\n\n2. So far we only experimented with character-level text prediction currently we are implementing neural-turing like algorithm with surprisal-driven controller, but we don't have any publicly available results yet.\n\n3. In the adaptive-zoneout paper which I mentioned - https://arxiv.org/pdf/1610.07675.pdf, we visualized how surprisal can affect gate activations. We believe that the reason this works so well on language modeling is the fact that some portions of text are just unpredictable by nature (like dates or numbers, we know that it's a number, but it would be pointless to try to predict document id), so we learn this fact and simply don't 'pay attention' to those fragments. Without suprisal-feedback, the network would try to memorize everything and possibly could overwrite some more general rules, which would lead to overfitting.", "title": "Hello"}, "ryuxbOg4e": {"type": "rebuttal", "replyto": "r1GuklzXx", "comment": "Surprisal-feedback is not comparable to 'dynamic evaluation'. \n\nThe concept might be familiar, however the way they operate is fundamentally different. \n\nSurprisal-feedback 'plugs in' previous prediction into the inputs. No changes to the original learning algorithm are needed. It is exactly the same as vanilla 'RNN'/'LSTM'. We don't even give it exact information about the mistake, just the sum of surprises (a single scalar). \n\nDynamic evaluation requires changes to the algorithm. The backpropagate errors suring the test phase which is conflict with the idea of splitting data into train/test set. We give allow the network to change during the test phase, which might lead to (a) overfitting, (b) lack of information how it actually works for other datasets (c) which network should we use at the end - the weights will change.\n\nThe main difference is that in surprisal-feedback LSTM, the network learns how to utilize the surprisal magnitude, whereas dynamic evaluation directly modifies the weights by backpropagation. Therefore, SF learns the 'algorithm' to use surprisal and can generalizes to the test set or any other unseen data. We never actually 'tell' the network how to use surprisal.\n\nPlease take a look at the source code provided and see that the algorithm is directly comparable to methods which do not use 'dynamic evaluation'. We do not believe that it is fair to put SF and 'dynamic evaluation' methods in the same category, because we keep strict separation of train/validation and test sets.\n\nNote that in order to implement SF for the test phase, no changes are needed for the algorithm. The only change is within the SF-layer. In order to show this, we have supplied the source code. This proves that we don't get any more information than the original plain RNN. \n", "title": "Surprisal-feedback is not comparable to 'dynamic evaluation'. "}, "BySEMCTXg": {"type": "rebuttal", "replyto": "S1J0E-71l", "comment": "Source Code (C++/CUDA) for reproducing the results\n\nhttps://github.com/krocki/Surprisal-Feedback-LSTM", "title": "Source Code (C++/CUDA) for reproducing the results"}, "r1GuklzXx": {"type": "review", "replyto": "S1J0E-71l", "review": "Most of the papers that report test BPC scores on enwik8 also report the size of the models.\nThis is because, it is important to know how many parameters are required to obtain the performance that they are reporting.\n\"The algorithm was written in C++ and CUDA 8 and ran on GTX Titan GPU for up to 10 days\" is vague information without knowing the size of the model. Could you please report the size of the model that was used in this paper?\n\nAccording to the statement: \"This method does not belong to the \u2019dynamic evaluation\u2019 group: 1. It never actually sees test data during\ntraining. 2. It does not adapt weights during testing\"\nIn fact, it consumes the test error signal to achieve the test BPC score that is being reported. Most of the models in the same table do not use any test error signal at inference time. I think dynamic evaluation is more proper baseline in this case.This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. \n\n-This paper makes an  erroneous assumption: test label information is not given in most of the real world applications, except few applications. This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach. Also, comparing against the models that do not use test error signal at inference time is unfair. We cannot just say that the test label information is being observed, this only holds in online-prediction problems.\n\n-The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them. I understand that this paper came before the other papers, but the manuscript should be updated before the final decision.\n\n-The model size is still missing and without this information, it is hard to judge the contribution of the proposed trick.\n", "title": "Model size?", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1pQ4-zNe": {"type": "review", "replyto": "S1J0E-71l", "review": "Most of the papers that report test BPC scores on enwik8 also report the size of the models.\nThis is because, it is important to know how many parameters are required to obtain the performance that they are reporting.\n\"The algorithm was written in C++ and CUDA 8 and ran on GTX Titan GPU for up to 10 days\" is vague information without knowing the size of the model. Could you please report the size of the model that was used in this paper?\n\nAccording to the statement: \"This method does not belong to the \u2019dynamic evaluation\u2019 group: 1. It never actually sees test data during\ntraining. 2. It does not adapt weights during testing\"\nIn fact, it consumes the test error signal to achieve the test BPC score that is being reported. Most of the models in the same table do not use any test error signal at inference time. I think dynamic evaluation is more proper baseline in this case.This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. \n\n-This paper makes an  erroneous assumption: test label information is not given in most of the real world applications, except few applications. This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach. Also, comparing against the models that do not use test error signal at inference time is unfair. We cannot just say that the test label information is being observed, this only holds in online-prediction problems.\n\n-The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them. I understand that this paper came before the other papers, but the manuscript should be updated before the final decision.\n\n-The model size is still missing and without this information, it is hard to judge the contribution of the proposed trick.\n", "title": "Model size?", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkfVKMe7e": {"type": "review", "replyto": "S1J0E-71l", "review": "- It seems like for the recent language modelling experiments such as enwiki8, recurrent/variational dropout and batch/layer norm seems to be very effective. Have you tried those techniques with your model as well? \n- Do you have experiments on different datasets and tasks other than language modelling?\n- Do you have more theoretical justification or analysis for this?Summary:\nThis paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks.\n\nContributions:\nThe introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps.\n\nQuestions:\nA point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input.\n\nCriticisms:\nThe paper is really badly written, authors should rethink the organization of the paper.\nMost of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. \nThe justification is not convincing enough.\nExperimental results are lacking, only results on a single dataset are provided.\nAlthough the authors claim that they got SOTA on enwiki8, there are other papers such as the HyperNetworks that got better results (1.34) than the result they achieve. This claim is wrong.\nThe model requires the ground-truth labels for the test-set, however, this assumption really limits the application of this technique to a very limited set of applications(more or less rules out most conditional language modeling tasks).\n\nHigh-level Review:\n    Pros: \n        - A simple modification of the model that seems to improve the results and it is an interesting modification.\n\n    Cons:\n       - The authors need to use test-set labels.\n       - Writing of the paper is bad.\n       - The authors assume that they have access to the ground-truth labels during the test-set.\n       - Experimental results are lacking", "title": "Few Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyWZXPLEg": {"type": "review", "replyto": "S1J0E-71l", "review": "- It seems like for the recent language modelling experiments such as enwiki8, recurrent/variational dropout and batch/layer norm seems to be very effective. Have you tried those techniques with your model as well? \n- Do you have experiments on different datasets and tasks other than language modelling?\n- Do you have more theoretical justification or analysis for this?Summary:\nThis paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks.\n\nContributions:\nThe introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps.\n\nQuestions:\nA point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input.\n\nCriticisms:\nThe paper is really badly written, authors should rethink the organization of the paper.\nMost of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. \nThe justification is not convincing enough.\nExperimental results are lacking, only results on a single dataset are provided.\nAlthough the authors claim that they got SOTA on enwiki8, there are other papers such as the HyperNetworks that got better results (1.34) than the result they achieve. This claim is wrong.\nThe model requires the ground-truth labels for the test-set, however, this assumption really limits the application of this technique to a very limited set of applications(more or less rules out most conditional language modeling tasks).\n\nHigh-level Review:\n    Pros: \n        - A simple modification of the model that seems to improve the results and it is an interesting modification.\n\n    Cons:\n       - The authors need to use test-set labels.\n       - Writing of the paper is bad.\n       - The authors assume that they have access to the ground-truth labels during the test-set.\n       - Experimental results are lacking", "title": "Few Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJ9apDymx": {"type": "review", "replyto": "S1J0E-71l", "review": "1) Have you tried suprisal LSTM on some other datasets?\n2) Suprisal LSTM uses the same error signal than dynamic evaluation (http://arxiv.org/abs/1308.0850). Did you compare with them?\n3) In some of the backprop equations, why is there the term on the left-hand-side and right-hand-side of the equation, for instance \\partial E_t/\\partial y_t in equation 15?\nThis paper proposes to leverage \"surprisal\" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.\n\nThe general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.\n\n- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.\n\n- The experimental evaluation is limited. Only one dataset enwik8 is explored. I think it is necessary to try the idea on different datasets to see if feedback LSTM sees some consistent improvements.\nAlso, author claims state-of-art on enwik8, but hypernetwork, already cited in the paper, achieves better results (1.34 BPC, table 4 in the hypernetworks paper).\n\n- Author only compares to methods that do not use last prediction error as extra signal. I would argue that a comparison with dynamic evaluation would be more fair. \n Feedback LSTM uses prediction error as extra input in the forward prop, while dynamic evaluation  backprop it through the network and change the weight accordingly. Also they don't propagate the prediction error in the same way, they both leverage \"extra\" supervised information through the prediction errors.\n\n\nIn summary:\nPros: \n- Interesting idea\n- Seems to improve performances\n\nCons:\n- Paper writing\n- Weak evaluation (only one dataset)\n- Compare only with approaches that does not use the last-timestep error signal", "title": "Pre reviews question", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJvqO1GEl": {"type": "review", "replyto": "S1J0E-71l", "review": "1) Have you tried suprisal LSTM on some other datasets?\n2) Suprisal LSTM uses the same error signal than dynamic evaluation (http://arxiv.org/abs/1308.0850). Did you compare with them?\n3) In some of the backprop equations, why is there the term on the left-hand-side and right-hand-side of the equation, for instance \\partial E_t/\\partial y_t in equation 15?\nThis paper proposes to leverage \"surprisal\" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.\n\nThe general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.\n\n- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.\n\n- The experimental evaluation is limited. Only one dataset enwik8 is explored. I think it is necessary to try the idea on different datasets to see if feedback LSTM sees some consistent improvements.\nAlso, author claims state-of-art on enwik8, but hypernetwork, already cited in the paper, achieves better results (1.34 BPC, table 4 in the hypernetworks paper).\n\n- Author only compares to methods that do not use last prediction error as extra signal. I would argue that a comparison with dynamic evaluation would be more fair. \n Feedback LSTM uses prediction error as extra input in the forward prop, while dynamic evaluation  backprop it through the network and change the weight accordingly. Also they don't propagate the prediction error in the same way, they both leverage \"extra\" supervised information through the prediction errors.\n\n\nIn summary:\nPros: \n- Interesting idea\n- Seems to improve performances\n\nCons:\n- Paper writing\n- Weak evaluation (only one dataset)\n- Compare only with approaches that does not use the last-timestep error signal", "title": "Pre reviews question", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}