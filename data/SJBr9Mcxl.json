{"paper": {"title": "Understanding trained CNNs by indexing neuron selectivity", "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"], "summary": "", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "keywords": ["Computer vision", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "While this is interesting work, one major concern comes from Reviewer 2 regarding the attempt of characterizing the tuning properties, which has proven useless both in neuroscience and in machine learning. Currently, no attempt so far has lived up to the promises this line of research is aiming for. In summary, this work is explorative and incremental but worthwhile. We encourage the authors to further refine their research effort and resubmit."}, "review": {"HybnVh-ug": {"type": "rebuttal", "replyto": "SJBr9Mcxl", "comment": "Hi authors,\n\nCongrats for an interesting submission! I wonder what other selectivity indices you could use beside color and class.\n\nI just noticed that your definition of Neuron Feature (weighted average image) is very close to what we use in this paper (an average image):\n\nhttp://www.evolvingai.org/mfv\n\nAlso you might want to know about or discuss the connection between your work and our class visualization here:\nhttp://www.evolvingai.org/synthesizing\n\nSo we show that you can synthesize an image that activates two neurons at the same time (Fig. S11, S12). I think it would be interesting if you could compare them with your techniques. Do we get a real image that contains both objects or mostly just one?\n\nCheers,\n\nAnh", "title": "Some connections/cites"}, "HJfMrHIIg": {"type": "rebuttal", "replyto": "H1cc-_xSl", "comment": "\nWe will answer in three separate sections to the final 3 comments: \n\nAbout the 1st comment of the reviewer. \n                 Yes, you are right the reference is wrong:\n\n                 \u201cThe cite for \u201cLearning to generate chairs\u2026\u201d is wrong (first two authors combined resulting in a confusing cite)\u201d.\n\n                 The authors of the reference are incorrect. We are going to correct it in  an updated version of the paper and we will probably \n                 change it to the posterior PAMI paper on a similar work.\n\n                 Alexey Dosovitskiy, Jost Springenberg, Maxim Tatarchenko, Thomas Brox\n                 Learning to Generate Chairs, Tables and Cars with Convolutional Networks\n                 IEEE Trans. On PAMI, 2016\n\n\n\nAbout the 2nd comment of the reviewer (it is separated in 3 questions/comments)\n\n    ** Reviewer\u2019s first question: \u201c What exactly is the Color Selectivity Index computing?\u201d \n\n                  Color selectivity index is defined to represent with a single number, the level of activation of a neuron when a specific color is\n                  present in the input image.\n\n\n    ** Reviewer\u2019s second question: \u201cThe Opponent Color Space isn\u2019t well defined and it wasn\u2019t previously familiar to me.\u201d\n\n                 The color-opponent space is a 3D space that has one axis representing intensity information (grey-level or black-white axis) and two\n                 axes to represent chromaticity information, which is decomposed in the basis of red-green and blue-yellow axes. In color science,\n                 black and white is considered as the lack of color (achromatic). An easy way to understand the opponent space is relating the\n                 intensity axis with the diagonal line of the RGB cube that goes from black (0,0,0) to white (255,255,255), crossing all the greys\n                 (128,128,128) is the mean grey that can be seen as the (0,0,0) of the opponent space; the two chromaticity axes are on a plane that\n                 is orthogonal to this diagonal (intensity axis). In this sense, the opponent space transform can be seen as a translation plus two\n                 rotations from the usual RGB space. We did not put the definition since we assumed it was enough known in computer vision (In van de\n                 Sande, et-al PAMI-2010, they proved Opponent-SIFT as the best color descriptor for object recognition). However, we can add the\n                 definition in the text or in a footnote if you think it is necessary, or maybe we can add a citation to this PAMI about opponent\n                 sift where the opponent space is defined. \n\n    ** Reviewer\u2019s third comment: \u201cIntuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity\n                 NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge\n                 pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should \n                 probably be more clearly described.\u201d\n\n                 Considering the structure of the opponent color space, we have proposed to measure color selectivity index using the angle between \n                 the 1st principal component of the color distribution of the neuron feature and the intensity (black-white axis), the smaller is \n                 this angle the more achromatic is the neuron feature information, in this way we capture the bias of the neuron feature towards a \n                 chromatic component. A neuron feature computed from a set of images with a large diversity of color presents a grey average, similar \n                 to a neuron feature that is the average of grey-level images. \n                 Our color selectivity index is not specific to a single color, it measures the capacity of a neuron to give a high response on a \n                 specific color (or colors) and a low response when the colors of the images that maximally spike a neuron present just intensity \n                 variations; e.g, our color selective index determines if a neuron is achromatic or chromatic, and the degree of chromaticity if the\n                 second case.\n\n                 What we found in the analysed CNNs is that in lower layers, the neurons with high color selectivity index present a predominant 1st\n                 PCA component, e.g. the color distribution of the neuron feature is aligned with a single color axis, therefore if there are two\n                 colors, they are aligned. However, as we go deeper in the hierarchy, the alignment of the color distribution seems to disappear to \n                 give a more hue-based representation. As you can see in Fig. 5, there is a neuron in conv4 with a high color-selectivity index which \n                 is very selective to green and blue which are two non-aligned colors. This could be related with other findings in the human brain,\n                 but more work is needed to be able to do this statement. \n\n\n\n\nAbout the 3rd comment of the reviewer:\n\n\n                 \u201c... For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not \n                 for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or \n                 low? If it is high, it\u2019s somewhat misleading because the unit itself actually isn\u2019t color selective; the dataset just happens only \n                 to have red mushrooms in it. (It\u2019s a subtle point but worth considering and probably discussing in the paper) \u2026\u201d\n\n                 This is a very inspiring observation. In our methodology, the specific case that you mention will determine that the neuron is \n                 highly color-selective, since the neuron feature is going to be the average of red-mushrooms only. We assume that the neuron feature \n                 is computed on the dataset that the CNN has been trained (not tested) on. However, it seems quite unlikely that such a neuron that \n                 activates for an *any color*-mushroom image can exist in the trained CNN if the training dataset only had red mushrooms (we assume \n                 that a CNN only learns things that have been seen during the training and therefore, it seems difficult that this CNN, that gives a \n                 high representative power to color, can generalize to mushrooms of any color if it has only seen that this type of object is always \n                 red).\n\n                 To support the previous ideas, in figure 6 (of the paper), we can see the cropped images for the red-mushroom neuron. Among the \n                 crops we only find red mushrooms and one table under a red-curtain image. This make us to think that the activation of this neuron \n                 is highly dependent on the activation of a red horizontal edge which would not be highly activated  for *any color*-mushroom. If the \n                 training dataset only had red-mushrooms this color-dependency could even be reinforced. The discriminative power of color for \n                 specific classes seems to be used by the network, this is what also seems to be exploited by this CNN in classes like cardoon or \n                 digital-clock for which we find class-selective neurons in shallow layers. \n\n                 To prove this in more depth, we have studied the activation of the red-mushroom neuron in conv 4 (neuron 34) along the dataset. \n\n                 (Annexed document:  We have compiled in this document all the images we have extracted from some further experiments. It has been \n                 stored in the public link:\n                 \n                 ****  https://docs.google.com/document/d/1KMu09fO3Y4UuFJCUKAMPGC_5L2OMmBYs48eeyn_Xm9E/edit?usp=sharing ****)\n\n                 Therefore, we can see in Fig. A-1 that, although this neuron can activate for any-color-mushrooms, its red selectivity is highly \n                 preserved up to the first 500 activation (ranked by the activation value). In the figure we have sampled some cropped images that \n                 activates this neuron along the first 1000 highest activations, we qualitatively observe that up to a 0,6 activation value, red \n                 selectivity is highly preserved.   \n\n                 To explore further on the question suggested by the reviewer and considering the current dataset, which contains a variety of \n                 colored mushrooms, we can observe that the CNN has a kind of *any color*-mushroom neuron in the deepest layer.  We have analysed two                  \n                 similar final classes of mushrooms (here we are using the generic meaning of mushroom that determines anything resembling a                  \n                 mushroom) that exist in the Imagenet version that was used to train the CNN we are studying; these are: AGARIC and MUSHROOM. You can \n                 see two random subsets of these two different classes in Fig. A-2 of the annexed document. \n\n                 We can observe the composition of these two classes:\n                                     **AGARIC: is a class essentially presenting red mushrooms \n                                     **MUSHROOM:  is a class presenting a higher diversity of colored mushrooms, including red ones. \n                 So we have two classes with similar properties, mushrooms is generic, but one with just red-mushrooms and the second are any-color-\n                 mushrooms.\n\n                 To understand and compare how the CNN is encoding these two classes, we have selected a subset of essential neurons per each class \n                 (see Fig.A-4). We have used our proposed class-selectivity index. Therefore, we select the neurons presenting a not-null class-\n                 selectivity index for layers conv1, conv2, conv3, conv4 and conv5, for the mentioned classes, mushroom and/or agaric.  This will \n                 allow us to study the population of neurons that encode these two classes such that it is possible to observe how shared properties \n                 and non-shared properties are represented. \n\n                 **1st observation: \n                                  The more diversity the class has, the larger the number of neurons that seems to be required for encoding that \n                                  class.\n\n                                  \t                              Conv1     Conv2     Conv 3      Conv 4    Conv 5       Total\n                                   \t   Agaric               2             13          34             23           23       =     95\n                                  \tMushroom           1             19          48             41           24       =   133\n\n                                  The value of class-selectivity indexes of neurons usually increases with depth, however it is dependent on the \n                                  number of neurons that are used to encode each class. In this case, Agaric class is encoded with a smaller number \n                                  of neurons (95) than Mushroom class (133). They seem to present a different population code scheme. \n\n                 **2nd observation: \n                                  Neurons selective to both classes represent properties shared by both classes, e.g. red caps of different sizes and \n                                  shapes, white stems, amongst others basic shared structures and background contexts. Neurons selective to a single \n                                  class reinforce the specific features of the class. \n\n\n                 To prove these facts, we have separated the neuron population of the two classes in 3 groups: \n\n                                  Group 1: Neurons selective to BOTH classes (39 neurons)\n                                  Group 2: Neurons selective just to AGARIC (56 neurons) \n                                  Group 3: Neurons selective just to MUSHROOMS (94 neurons)\n\n                 The properties of these three groups are plotted in Fig. A-3 of the annexed document. We show the relevant neuron features for each \n                 group by visualizing their Neuron Features and the first cropped image of the classes activating that neuron. As we already \n                 mentioned above, we can observe that shared Neuron Features represent generic properties of mushrooms like red caps of different \n                 sizes, white stems, or common context properties. The other two groups present more specific features of the classes, like the color \n                 diversity of caps for mushroom class, or a blue-specific case for agaric class.\n\n\n                 In addition to visualizing the NF of these relevant neurons, in Fig. A-4 of the annexed document, we have studied the selectivity \n                 indexes for each group. We have used different colors to represent different indexes. First of all we observe that color selectivity \n                 indexes (in green) are equally distributed in all the groups. However, class-selectivity indexes present different distributions.  \n                 Agaric class presents higher indexes (in pink) than mushroom class (in blue). The same behaviour is observed by the neurons that are \n                 shared by these two classes. It means that the CNN has very specialized neurons for red mushrooms and they are shared by both \n                 classes. \n\n                 Finally, we also analyze which are the colors (hues) to which the relevant features are selective to. We build the distribution of \n                 color-selectivity covered by the Neuron Features for all the groups in Fig. A-5.  In all the groups color selectivity is mainly \n                 represented in the hue range that goes from red to orange/brown to yellow neurons, as expected, since the majority of the mushrooms \n                 caps in the nature have colors in this range. However, we can see some differences. In Group 1 color-selectivity is concentrated on \n                 the red-orange region representing the commonalities between the two classes. Group 2 is also concentrated on the red-orange region \n                 (Mean: 58.76), while Group 3 is shifted to the orange-yellow region (Mean: 67.18), representing the color differences between the\n                 two classes.  \n\n\n---------------------------------------------------------------------------------------------------------------------------------------------------\nIf the reviewer thinks it is worth to introduce this discussion in the paper, we suggest to add the following paragraph in an updated version of the \npaper. It could be inserted in Section 4.3 (Experiments on class-selectivity index) after the paragraph \u201cSecondly, we have visualized \u2026\u201d\n\n                 \u201cThirdly, we have explored how to use the class-selectivity index to visualize commonalities and differences between the neurons \n                 encoding two similar classes in the studied CNN. We focused on Agaric and Mushroom (Imagenet classes) both are composed by mushroom-\n                 shaped fungus, but agaric are specifically red, while mushrooms present a higher diversity of colors. Therefore, there exist some \n                 intersection and differences between the two classes (from their visual features point of view). Two main conclusion have arisen. \n                 Firstly we have analysed the number of relevant neurons per each class. We have defined as relevant neurons for X class as those \n                 presenting a non-null X-class-selectivity index. The number of relevant neurons for Agaric class (95) is lower than for Mushroom \n                 class (133), additionally agaric-class index values are higher than mushroom-class indexes,  that which makes us to conclude that \n                 they present different population code schemes. Secondly, we have separated the relevant neurons in three two groups: shared neurons                  \n                 and class-specific neurons. We have observed that neuron features represent shared properties by both classes, e.g. red caps of \n                 different sizes and shapes, white stems, amongst other basic shared structures and background contexts. Neurons selective to a \n                 single class reinforce the specific features of the class and their own diversity. Even color features differs between the groups.\u201d\n\n\nHowever we want to note that the length of our paper is currently 13 pages (including bibliography). We are not sure if the increase in the length can be a problem for the aim of this conference.  \n", "title": "Thanks for the inspiring review and sorry for our delay in the reply."}, "S1qbgsZrg": {"type": "rebuttal", "replyto": "HytLDIuNg", "comment": "Thanks a lot  for this very Interesting discussion.\n\n1) We are not neuroscientists, we are AI researchers, however, l think we can not agree on your comments about the progress in neuroscience research, you say:\n\n\u201c... In my view, much of what was done in neurophysiology of the type you seem to be inspired by did not advance the understanding of processing in the visual system, beyond the initial work of people like Hubel and Wiesel, and some others in higher ventral areas,...\u201d\n\nDo you really think that works like, Doris Tsao et-al. on face-selectivity, Bewil Conway et-al on specific-hue selectivity or  R.Q. Quiroga et-al on grand-mother cells (amongst others) did not advance in the understanding of processing in the visual system?\n\nWe can agree that studying individual neuron selectivity must be a hard work, and to make it really useful you need to formulate very good hypothesis, and this must be really hard. However, we\u2019ve been speaking with neuroscientists working in the color field and they have agreed that works like ours can help a lot in giving new intuitions about how to formulate good new hypothesis. Our results on color selectivity in shallow layers confirms what is already known in the brain, but color in deep layers of the brain is still far from being understood, and our results can be a source of inspiration.  \n\n2) About the works correlating indexed group of neurons with brain evidences, that you mentioned as further research lines, we think they should be published in other kind of journals apart from ICLR,which has a more engineering audience. We plan to publish our results on this topic in other vision journals like JOV, VR, PNAS or JN \u2026 \n\n3)   About this comment on our work or similar works published previously in ICLR, NIPS or CVPR \u2026 \n\n\u201cYou're right, my comments would apply to any other work along these lines as well.   Basically, in my view, the approach of visualizing the intrinsic features of CNN has been really disappointing, precisely because neither of the two main directions I mentioned in my review have been that useful.\u201d\n\nWe would like to mention that the computer vision community has spent the last decades hand-crafting image descriptors to achieve good classification rates in object recognition. Concepts like SIFT, bag-of-words, spatial-pyramids, part-based models, Fisher-vectors (amongst others), have been the focus attention. And the study of their properties, like scale or color invariance, how to combine them, and a lot of  different representation problems have been the aim of a lot of published papers.  These days, the use of CNNs has shifted the focus of attention, and now the design of good representations is not the aim anymore, but to understand the representations we get after training, it is still important. Trying to disentangle how color or space invariance is achieved, or how similarities between different objects are represented, or how the network hierarchy is entangling the visual representation is still important to tackle a lot of different problems open in the industry.  We are in academia, but we live surrounded by students dealing with their projects and companies posing new problems that require a better understanding of what the CNNs are learning. \n\nWe really believe that a catalogue of neuron selectivities across the layers of a trained network additionally with a visualization of the average feature of each neuron can be a useful tool for a better understanding of the visual properties encoded across the network hierarchy. This can help in the design of new architectures to solve multiple new problems that are constantly emerging these days.", "title": "response to reviewer2"}, "SJ0daSOEe": {"type": "rebuttal", "replyto": "rkn9pmD4e", "comment": "Thanks for this long review. We are really grateful to the reviewer for having read this paper considering he is \u201c... biased against the whole enterprise of this paper...\u201d.\n\nAbout specific comments of the reviewer we want to point out on some aspects:\n\n(1) Firstly, we want to admit that this paper is influenced by the ideas coming from neurophysiology literature where selectivity indexes of single neurons are used to understand internal representation; i.e., finding neurons which are selective to faces, to eyes or to specific color-hues can inform about visual codes in the brain. Therefore, adopting a multidisciplinary point of view, in this paper  we propose to translate this methodology to artificial intelligence at a moment where understanding CNN internal representations is a focus of attention in the area.\n\nWe would love to know why this methodology is considered as a \u201cplague\u201d by the reviewer in the following comment:\n\n\u201c... In a way, I feel like this paper suffers from the same problem that plagues a typical line of research in neurophysiology, in which a catalog of selectivity distributions of various neurons for various properties is produced -- full stop ... \u201d\n\nIf the reviewer specifies the concrete drawbacks of the selectivity-based methodology, perhaps it could help us to redirect our research,  or at least to fully understand all  the reviewer comments.\n\n(2) Recently, Prof. Bengio in a talk at NIPS pointed out that one of the big challenges in the area is: \u201cBridging the gap between deep learning and neuroscience is an opportunity to get inspiration and constraints\u201d. Our proposal is fully aligned with this challenge, since it is providing with a generic framework for inspiring about new architectures in AI or validating findings in brain research.\n\n(3) Understanding and visualizing the intrinsic features that activates a neuron in a CNN have been focus of interest in the computer vision community in the last years. Several papers have been published with this aim  (Zeiler&Fergus, Simonyan-et al, Yosinksi-et al, amongst others). Here we propose a new work, that is in the same line but trying to solve the main drawbacks of these previous works. This was already justified in a former comment of this review and was added to the paper in the updated version. Apart from giving a new approach to understand and visualize we also add the concept of selectivity index that gives a generic framework to go beyond the single neuron and group them considering their behaviours.\n\n(4) We think it is curious that the reviewer shows such a disapproval with this work at the same time he/she is proposing further research lines that can be derived from it and criticizing the fact we did not add the corresponding results.  We agree with the reviewer about the further lines that can be derived from this work. We thought it was more adequate first to publish this basic framework and subsequently use it to correlate neurophysiological evidences or to use it to constrain the loss function of an untrained network. Actually, we are already working on trying to correlate the derived indexed groups of neurons with already published results about color coding in the brain; and we are also working on an image dataset to train new architectures using these indexes to constraint the training stage in a classical computer vision problem, but results are still preliminary. We thought all this was out of the scope of this paper where we focused on a paper that deals with internal representations from a generic point of view, that we think matches the aim of this ICLR conference.   ", "title": "Review Comments "}, "rJeVOC-Nl": {"type": "rebuttal", "replyto": "SJBr9Mcxl", "comment": "We have updated the pdf of the paper after the reviewer comments, in this way:\n  - Giving more details about similarities and differences between the proposed approach and previous.\n  - Adding the clarifications required by the reviewers on Figure1\n  - Improving the global understanding by joining section 1 and 2 into a single one. ", "title": "Updated version"}, "r1v0j_b4l": {"type": "rebuttal", "replyto": "rkBv_8lVe", "comment": "Sorry for not having explained it adequately in the first version.\n \nThe y-axis represents the normalized activation value of a single neuron to an image of the dataset. Images are ranked on the x-axis according with their activation value, from highest to lowest activation (we just plot the first 400 images for each neuron).  \n\nBy normalized activation (y-axis) we mean the value of the maximum activation of a neuron for a specific input image, which is normalized by the maximum of these values achieved by the same neuron over all the images in the dataset. Therefore, the first relative activation value is always 1 for all neurons and then the normalized activation values decrease monotonically.  This normalization allows to compare different neuron behaviors, from neurons which are activated by most of the images (flatter behavior), to neurons that highly activates only for a subset of images and have very little activation for the rest (steeper behavior).\n\nIn figure 1 we show the behavior for a subset of  neurons at each layer. Neurons have been selected according to their area under the curve (AUC) value, which is represented by a percentage of area.  The percentage of area is computed over the area of the neuron that presents the maximum AUC in the entire architecture.  For each layer we have picked some neurons sampling different AUC values, equally distributed in the layer. We can observe different behaviors in all layers. In general, we can state that in deeper layers the behavior of the neurons is steeper (lower AUC), i.e. neurons highly spike for a small number of images. However, in shallower layers the behavior is flatter, i.e. neurons highly spike for a lot of images. This is an expected behavior, since the image features spiking neurons in first layers (e.g. oriented edges) are shared by almost all the images, while the features spiking shallow neurons are more selective features (e.g. faces) that only spike for specific images.\n\nWe are going to change this figure explanation in a paper update.", "title": "Clarification of Figure 1"}, "rkBv_8lVe": {"type": "review", "replyto": "SJBr9Mcxl", "review": "In Figure 1, what is the x-axis label? What is the y-axis label (i.e. what do the percents represent)? When computing the NF, Section 3 specifies \u201cThey [the activations] need to be accordingly ranked with the rest of the activations of the layer.\u201d Does this mean that activations are compared from one neuron against other neurons in that same layer?This paper makes three main methodological contributions:\n - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron\n - ranking of neurons based on color selectivity\n - ranking of neurons based on class selectivity\n\nThe main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.\n\nHowever, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:\n - \u201cIndexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.\u201d As far as I know, this had not been previously reported.\n - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)\n - \u201cour main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).\u201d Great observation!\n\nOverall, I\u2019d recommend the paper be accepted, because although it\u2019s difficult to predict at this time, there\u2019s a fair chance that one of the \u201csmaller conclusions\u201d would turn out to be important in hindsight a few years hence.\n\n\nOther small comments:\n - The cite for \u201cLearning to generate chairs\u2026\u201d is wrong (first two authors combined resulting in a confusing cite)\n\n - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn\u2019t well defined and it wasn\u2019t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.\n\n - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it\u2019s somewhat misleading because the unit itself actually isn\u2019t color selective; the dataset just happens only to have red mushrooms in it. (It\u2019s a subtle point but worth considering and probably discussing in the paper)\n", "title": "Details of NF", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1cc-_xSl": {"type": "review", "replyto": "SJBr9Mcxl", "review": "In Figure 1, what is the x-axis label? What is the y-axis label (i.e. what do the percents represent)? When computing the NF, Section 3 specifies \u201cThey [the activations] need to be accordingly ranked with the rest of the activations of the layer.\u201d Does this mean that activations are compared from one neuron against other neurons in that same layer?This paper makes three main methodological contributions:\n - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron\n - ranking of neurons based on color selectivity\n - ranking of neurons based on class selectivity\n\nThe main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.\n\nHowever, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:\n - \u201cIndexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.\u201d As far as I know, this had not been previously reported.\n - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)\n - \u201cour main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).\u201d Great observation!\n\nOverall, I\u2019d recommend the paper be accepted, because although it\u2019s difficult to predict at this time, there\u2019s a fair chance that one of the \u201csmaller conclusions\u201d would turn out to be important in hindsight a few years hence.\n\n\nOther small comments:\n - The cite for \u201cLearning to generate chairs\u2026\u201d is wrong (first two authors combined resulting in a confusing cite)\n\n - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn\u2019t well defined and it wasn\u2019t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.\n\n - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it\u2019s somewhat misleading because the unit itself actually isn\u2019t color selective; the dataset just happens only to have red mushrooms in it. (It\u2019s a subtle point but worth considering and probably discussing in the paper)\n", "title": "Details of NF", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkgU0bR7g": {"type": "rebuttal", "replyto": "Hyfgtls7e", "comment": "Likewise in Zeiler&Fergus, we pursuit visualizing the intrinsic feature that activates a neuron. As them, we perform it by analyzing the images that maximally activates a specific neuron. This aim is also shared in some other works, such as Simonyan-et al and Yosinksi-et al (amongst others), but in this case, they generate the image that maximizes the activation of the neuron.\n\n\nAlthough our approach is based on the images that maximally activates a neuron, like Zeiler&Fergus, our Neuron Feature is different from what they do in the sense that our estimation of the intrinsic property is much more generic. Zeiler&Fergus just visualize the projected feature of a single image activation, and they show up the first 9, but separately, i.e. without considering the commonalities in between them. However, our estimation is a weighted average that combines the N-first maximum activations (N=100 in the paper) and makes it not to be image-specific. We can see in Figure 3 of our paper that the variability in between the images that maximally activates a neuron, can present a high variability between them. This fact questions Zeiler&Fergus individual visualisation. In our Neuron Feature, strong spatial variability is visualized by the level of blurring of specific Neuron Feature regions. \n\n\nIn relation with the second group of works, Simonyan-et al and Yosinksi-et al, they also seeked to overcome the image-specific drawback by generating an image that captures a generic feature that highly activates that neuron. However, the gradient-based techniques, used to generate these non image-specific visualizations, makes to emerge a new drawback, which is their non realistic appearance. Their visualizations present important artifacts that complicate the understanding of the intrinsic property. Therefore, they explore different regularizations that  achieve more realistic intrinsic feature representations. Our Neuron Feature overcomes this problem by directly averaging on the image space, this has two main advantages: (a) keeping the properties of the natural images, and (b) providing a more straightforward approach to compute them. \n\n\nConsidering all the above considerations we end up with a new visualization that is not image-specific, is unique for each individual neuron, presents an understandable representation, and is obtained by a straightforward approach that can be applied to any neuron at any layer, convolutional or not. All these properties allow to achieve a higher level of abstraction in the understanding of a single neuron, making easier to define neuron descriptors that can derive selectivity indexes. These indexes allow to get comprehensible maps of the neuron functions at any level of any CNN.\n\n\nThe advantages of our approach with respect to Zeiler&Fergus and the rest of methods that visualize single neuron activity (Yosinksi-et al, Simonyan-et al) can be summed up as:\n   - More generic (non image-specific) and unique version of the intrinsic feature of a neuron\n   - More natural representation directly computed in the image space\n   - More understandable neuron function thanks to a higher level of abstraction in the neuron description\n  \nNow we see, we should have introduced this comparison in the paper. We will do it in an updated version of it. \n\n\nReferences:\n\n\n[ Simonyan-et al ] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In ICLR Workshop 2014.\n\n\n[ Yosinksi-et al ] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. In ICML Workshop 2015.\n\n\n[ Zeiler&Fergus ] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014\n", "title": "Clarification of Neuron Feature (NF)"}, "Hyfgtls7e": {"type": "review", "replyto": "SJBr9Mcxl", "review": "Could you clarify how your proposed \"Neural Feature\" is different from the Zeiler & Fergus method of finding images that maximally activate a neuron?\nThis paper attempts to understand and visualize what deep nets are representing as one ascends from low levels to high levels of the network.  As has been shown previously, lower levels are more local image feature based, whereas higher levels correspond to abstract properties such as object identity.  In semantic space, we find higher level nodes to be more semantically selective, whereas low level nodes are more diffuse.\n\nThis seems like a good attempt to tease apart deep net representations.  Perhaps the most important finding is that color figures prominently into all levels of the network, and that performance on gray scale images is significantly diminished.  The new NF measure proposed here is sensible, but still based on the images shown to the network.  What one really wants to know is what function these nodes are computing - i.e., out of the space of *all* possible images, which most activate a unit?  Of course this is a difficult problem, but it would be nice to see us getting closer to understanding the answer.  The color analysis here I think brings us a bit closer.  The semantic analysis is nice but I'm not sure what new insight we gain from this.  \n", "title": "Clarification of neural feature", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJLhbHQ4x": {"type": "review", "replyto": "SJBr9Mcxl", "review": "Could you clarify how your proposed \"Neural Feature\" is different from the Zeiler & Fergus method of finding images that maximally activate a neuron?\nThis paper attempts to understand and visualize what deep nets are representing as one ascends from low levels to high levels of the network.  As has been shown previously, lower levels are more local image feature based, whereas higher levels correspond to abstract properties such as object identity.  In semantic space, we find higher level nodes to be more semantically selective, whereas low level nodes are more diffuse.\n\nThis seems like a good attempt to tease apart deep net representations.  Perhaps the most important finding is that color figures prominently into all levels of the network, and that performance on gray scale images is significantly diminished.  The new NF measure proposed here is sensible, but still based on the images shown to the network.  What one really wants to know is what function these nodes are computing - i.e., out of the space of *all* possible images, which most activate a unit?  Of course this is a difficult problem, but it would be nice to see us getting closer to understanding the answer.  The color analysis here I think brings us a bit closer.  The semantic analysis is nice but I'm not sure what new insight we gain from this.  \n", "title": "Clarification of neural feature", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}