{"paper": {"title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"], "summary": "", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"]}, "meta": {"decision": "Reject", "comment": "This paper received borderline reviews. All reviewers as well as AC agree that the authors pursue a very interesting and less explored direction. The paper essentially addresses the problem of double grounding; visual information helping to group acoustic signal into words, and words helping to localize object-like regions in images. While somewhat hidden under the rug, this is what makes the paper different from the authors' previous work. The reviewers mentioned this to be a minor contribution. The AC agrees with the authors that this is an interesting and novel problem worth studying. However, the AC also agrees with the reviewers that this major novelty over the previous work is missing technical depth. The AC strongly encourages the authors to improve on this aspect of the paper. A simple intuition would be to look at https://arxiv.org/abs/1609.01704 in order to discover words, and use something along the lines of attention (eg https://arxiv.org/abs/1502.03044) to link with image regions."}, "review": {"HJ7ppfvLg": {"type": "rebuttal", "replyto": "Bkbc-Vqeg", "comment": "We'd like to first thank the reviewers for taking the time to read our submission and offer their thoughtful opinions and encouragement. Since the three reviewers raise some similar questions and concerns, I'll try to address them in one post instead of multiple replies.\n\nI'll first address the issue of novelty, which is the main criticism raised by all three reviewers. When thinking about research, I think it's important to consider the problem to be solved separately from the tool(s) used to solve it. While I agree that the deep neural network architecture used in this submission is not a significant departure from the one used in our NIPS 2016 paper, the way we use it is very different. I believe that the problem we address in this submission - that is, joint localization/isolation, clustering, and association of word-like speech patterns and object-like visual patterns - is significantly novel and wasn't studied in our NIPS paper. I don't believe that this submission qualifies as just an analysis paper, because it actually attempts to solve a specific and novel problem, and does so surprisingly well.\n\nThere exists an active sub-field of the speech/linguistics/cogsci communities which studies the problem of acquiring language from untranscribed speech audio alone (see relevant citations in the submission), and most of the techniques used in that problem space rely on segmentation and clustering of the speech signal into linguistically meaningful units (often called unsupervised term discovery or UTD when the desired granularity of the units is at the word or phrase level). The most widely-used and successful techniques for UTD are based on segmental dynamic time warping, which is inherently O(N^2) complexity. I believe that one of the most significant contributions of this submission is the demonstration that the addition of contextually relevant visual information is sufficient to reduce the computational complexity of UTD to O(N), allowing it to scale to much larger datasets.\n\nI'd also like to respond to a few specific points:\n\nFrom reviewer 2:\n\"As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\"\n\nI respectfully disagree that the move from text to speech audio constitutes an incremental step. The field of automatic speech recognition research has been grappling with the problem of mapping speech audio to symbolic strings for over 65 years, so it's not a trivial problem. By marrying language and vision at the raw signal level as we are here, we're not just creating models that learn the associations between words and images, but actually forcing the models to simultaneously learn how to perform speech recognition in their own, non-symbolic way.\n\nFrom reviewer 1:\n\"Clustering and grouping in section 4, is hacky. Instead of gridding the image, the authors could actually use an object detector (SSD, Yolo, FasterRCNN, etc.) to estimate accurate object proposals; rather than using k-means, a spectral clustering approach would alleviate the gaussian assumption of the distributions. In assigning visual hypotheses with acoustic segments, some form of bi-partite matching should be used.\"\n\nI think that these optimizations, while good suggestions if one's goal is to squeeze every last bit of performance out of the system, are not crucial to the central theme of the paper. We chose not to use an off-the-shelf object detection system for the same reason that we chose not to give the system oracle word boundaries derived from a speech recognizer. We wanted the model to learn to localize, identify, and associate spoken words and visual objects without being explicitly trained to do so. Spectral clustering is an O(N^3) complexity algorithm, and our problem deals with clustering on the order of a million points; a full similarity matrix at floating point precision would require on the order of 4 terabytes of memory, and that's just an O(N^2) subroutine in the spectral clustering algorithm. I realize that various approximate algorithms could possibly be made to work, but in our case I think that the fact that a classic algorithm like k-means works so well is actually a testament to the separability of the embeddings that are being learned by the multimodal CNN.", "title": "Rebuttal to reviews"}, "Bktnd8-mg": {"type": "rebuttal", "replyto": "S1Rrqtk7l", "comment": "The pre-print of the NIPS paper was recently made available here:\n\nhttp://papers.nips.cc/paper/6186-unsupervised-learning-of-spoken-language-with-visual-context.pdf\n\nIn the NIPS paper, we trained our models to associate entire audio captions with entire image frames. We did this by compressing the entire audio caption into its own embedding vector, and the image into its own embedding vector. The models were able to do a good job of associating the audio captions with semantically related images (and vice versa), but they weren't yet capable of automatically segmenting the audio caption into words, or picking out individual objects in the image.\n\nThe idea of introducing an automatic segmentation/clustering mechanism for learning individual words and objects was the key area of future work we talked about in the NIPS paper, and that's exactly what we did in this ICLR submission. In that way, this submission picks up where the NIPS paper left off.\n\nThis submission uses the same dataset as our NIPS paper, albeit with twice the amount of training data available (described in section 2), and uses a similar, but deeper, CNN architecture in the initial training phase (described in section 3). Everything from section 4 onward in this submission is completely novel.\n\nIn this submission, we introduce a grounding algorithm for associating sub-regions of an image to semantically related acoustic segments within its associated caption. This algorithm utilizes a multimodal CNN trained in the same way as our models from the NIPS paper, but modified to be applied locally to sub-regions of the image/caption. We demonstrate that in practice, this algorithm tends to pick out individual words or short phrases in the caption which reference specific objects in the images, e.g. in a spoken audio caption containing the words \"this is a lighthouse on a rocky shore during a sunny day\" the algorithm is able to localize the word \"lighthouse\" within the continuous speech signal and associate it with a bounding box around the lighthouse in the image.\n\nIn addition to producing a joint segmentation and alignment of an image/caption pair, the grounding algorithm outputs a local embedding vector for each discovered sub-region. We demonstrate that the different types of discovered acoustic patterns (\"words\") and visual patterns (\"objects\") are easily separated within this embedding space into highly pure clusters using simple clustering algorithms (k-means). We show that the semantic associations between the acoustic and visual clusters are preserved (e.g. the cluster of spoken instances of the word \"grass\" is highly associated with the cluster of image patches containing grass), and finally that the embedding space seems to be capturing semantic similarity between different acoustic pattern clusters (e.g. the \"river\" cluster is semantically similar to the \"lake\" cluster).", "title": "re:Difference from prior work"}, "S1Rrqtk7l": {"type": "review", "replyto": "Bkbc-Vqeg", "review": "Can you comment on the specific technical differences between this paper and Harwath et al, NIPS 2016?CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices\n", "title": "Difference from prior work", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkMNiGbNe": {"type": "review", "replyto": "Bkbc-Vqeg", "review": "Can you comment on the specific technical differences between this paper and Harwath et al, NIPS 2016?CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices\n", "title": "Difference from prior work", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}