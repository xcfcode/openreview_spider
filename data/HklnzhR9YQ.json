{"paper": {"title": "Approximation and non-parametric estimation of ResNet-type convolutional neural networks via block-sparse fully-connected neural networks", "authors": ["Kenta Oono", "Taiji Suzuki"], "authorids": ["k.oono.delta@gmail.com", "taiji@mist.i.u-tokyo.ac.jp"], "summary": "It is shown that ResNet-type CNNs are a universal approximator and its expression ability is not worse than fully connected neural networks (FNNs) with a \\textit{block-sparse} structure even if the size of each layer in the CNN is fixed.", "abstract": "We develop new approximation and statistical learning theories of convolutional neural networks (CNNs) via the ResNet-type structure where the channel size, filter size, and width are fixed. It is shown that a ResNet-type CNN is a universal approximator and its expression ability is no worse than fully-connected neural networks (FNNs) with a \\textit{block-sparse} structure even if the size of each layer in the CNN is fixed. Our result is general in the sense that we can automatically translate any approximation rate achieved by block-sparse FNNs into that by CNNs. Thanks to the general theory, it is shown that learning on CNNs satisfies optimality in approximation and estimation of several important function classes.\n\nAs applications, we consider two types of function classes to be estimated: the Barron class and H\\\"older class. We prove the clipped empirical risk minimization (ERM) estimator can achieve the same rate as FNNs even the channel size, filter size, and width of CNNs are constant with respect to the sample size. This is minimax optimal (up to logarithmic factors) for the H\\\"older class. Our proof is based on sophisticated evaluations of the covering number of CNNs and the non-trivial parameter rescaling technique to control the Lipschitz constant of CNNs to be constructed.", "keywords": ["CNN", "ResNet", "learning theory", "approximation theory", "non-parametric estimation", "block-sparse"]}, "meta": {"decision": "Reject", "comment": "The paper presents an interesting treatment of transforming a block-sparse fully connected neural networks to a ResNet-type Convolutional Network. Equipped with recent development on approximations of function classes (Barron, Holder) via block-sparse fully connected networks in the optimal rates, this enables us to show the equivalent power of ResNet Convolutional Nets.\u00a0\n\nThe major weakness in this treatment lies in that the ResNet architecture for realizing the block-sparse fully connected nets is unrealistic. It originates from the recent developments in approximation theory that transforming a fully connected net into a convolutional net via Toeplitz matrix (operator) factorizations. However the convolutional nets or ResNets obtained in this way is different to what have been used successfully in applications. Some special properties associated with convolutions, e.g. translation invariance and local deformation stability, are not natural in original fully connected nets and might be indirect after such a treatment.  \n\nThe presentation of the paper is better polished further. Based on ratings of reviewers, the current version of the paper is on borderline lean reject."}, "review": {"Skgxv9UK6m": {"type": "rebuttal", "replyto": "ryxrITy5nm", "comment": "Reply to specific comments:\n> Section 1, p.2: define M? define D? M seems to be used for different things in different paragraphs. The discussion on 'relative scale' could be made clearer.\n\nWe added the definition of D and M to the introduction section. We used the variable M mainly for three meanings: the number of blocks of a block-sparse FNN, the number of residual blocks in a ResNet-type CNN, and the number of parameters of an NN (either FNN or CNN). As we see from Theorem 1, the CNN which we constructed an FNN with M blocks has M residual blocks (plus the 0-th block). Therefore, we used the same character M. Since an FNN with M blocks has \\tilde{O}(M) parameters in common settings, we used M to indicate the number of parameters in the introduction. If it is confusing, we are thinking to use different characters for parameter counts and block counts.\n\n\n> Section 5.1: \"M = 1\" this is confusing, maybe use a different letter for the ridge expansion? \n\n\u201cM=1\u201d should have been D_1 = \\cdots = D_M = 1 and L_1^{(1)} = \\cdots = L_M^{(1)} = 1. We have fixed the description of Section 5.1 and Section E.\n\n> Section 2: Explain what is \"s\" in the Barron class, or at least point to the relevant definition in the paper\n\n\u201cs\u201d is a parameter in the definition of the Barron class that indicates the decay speed of signals in Fourier domain. We have added the reference to Definition 3.\n\n> Section 3.1:\n>   * 'estimation error' is usually called '(expected) risk' in the statistical literature (also in the introduction). estimation error would have to do with relating R and R^hat\n\nIndeed, in the statistics literature, the estimation error is frequently used for the (finite-dimensional) parameters. On the other hand, in nonparametric statistics, it is also common to use the terminology \"estimation error\" to indicate the expected risk, because parameters themselves are functions in the L2-space (while the estimation error is also sometimes referred as the variance term inside a model). Therefore, we used the terminology. \n\n>   * why is the estimator \"regularized\"?\n\nWe called this estimator \"regularized\" because we impose sparse constraints on the set of CNNs from which we pick the ERM estimator by restricting the maximum number of non-zero parameters. Now we do not impose such constraints, we have replaced it with the clipped ERM estimator.\n\n> Definition 2: shouldn't it be D_m^(0) = D instead of 1?\n\nYes. Thank you for pointing it out. We have fixed it.\n\n> Theorem 1: What is L? Also, it would be helpful to sketch the construction in the main paper given that this is the main result.\n\nWe intended that L is the total depth of the ResNet-type CNNs. We have changed the statement to specify the ResNet-type CNNs by the number of residual blocks and the depth of each block as we did in the definition of \\mathcal{F}^{\\mathrm{(CNN)}}.\n\n> Section 4.2: M_1 is the Lipschitz constant of what function?\n\nThe Lipschitz constant of a function realized by a CNN in \\mathcal{F}^{\\mathrm{(CNN)}}.\n\n> Section 5.2, 'if we carefully look at their proofs': more details on this should be provided.\n\nWe have added the detail of the proof as Lemma 7.", "title": "Reply from authors (2/2)"}, "rJxXsq8FTX": {"type": "rebuttal", "replyto": "HJeo4wAshX", "comment": "Thank you for your review. We appreciate your detailed feedback. We reply to your comments one by one.\n\n1.\n> It seems that when $M=1$, it reduces to the sparse NN considered in [Schmidt-Hieber 2017].\nBlocks in a block-sparse FNN is dense in general, as opposed to the sparse NN. Therefore, a block-sparse FNN with M=1 block is different from NNs used in [Schmidt-Hieber 2017]. Of course, we can apply our theorem with M=1 to derive the estimation error. The resulting CNN would be a ResNet with single residual block (since the number of blocks in FNN equals to the number of residual blocks in transformed ResNet-type CNN). However, the CNN has as many as O(M) channels. Since optimal FNN has M=O(N^\\alpha) (\\alpha > 0) blocks, we cannot keep the number of units per layer of the optimal CNN constant.\n\n> Thus, it seems unclear why we should consider such block-sparse family. Can any sparse NN be embedded in the family of CNNs?\nAlmost all FNNs used in the previous studies to approximate some specific function classes have block-sparse structures. For example, [Yarotsky 2017], [Yarotsky 2018], and [Zhou 2018]. Same is true of the case of the expansion of a Besov function by wavelet bases [B\u00f6lcskei et al. 2017]. From the viewpoint of functional analysis, block-sparse structure naturally corresponds to expansion of the target function with a set of basis functions approximated by dense FNNs.\nin view of sparsity, we would argue that our NNs are more practical than ones used in previous literature (e.g., [Yarotsky 2017], [Schmidt-Hieber 2017], and [Imaizumi & Fukumizu 2018]). They imposed somewhat artificial sparse constraints to FNNs by restricting the number of non-zero parameters. However, we need to train with L0 regularization to realize such NNs and hence actual NNs do not have such non-zero sparsity patterns. Contrary to that, our CNN is dense in general since the block-sparse FNNs from which we construct CNNs have dense blocks. We have fixed our paper to remove the sparsity constraints to made it clear that our CNN is dense in general.\n\n2. \n> In the Related Work, the authors only compare with 2 previous work on the approximation error of CNN. Actually, this work is more related to [Schmidt-Hieber 2017] due to borrowing the results.\nWe compared our work with Zhou (2018) and Petersen & Voigtlaender (2018) since their works are close to ours in analyzing approximation ability (and hence estimation ability) of CNNs.\n\n> It would be better to see what the novelties are compared with that work, especially in terms of the proof techniques.\nWe think the evaluation of the covering number of the set of CNNs is novel. It corresponds to the evaluation of M_1 in Theorem 2. If we naively trace the proof of [Schmidt-Hieber 2017] Lemma 12, the logarithm of covering number is prohibitively large to derive the desired estimation error.\nWe mainly used two techniques to deal with the problem: the architecture-aware evaluation of sup norms and the rescaling of parameters. First, we explicitly used the ResNet-type architecture and derived a tighter bound of the sup norms of the function realized by a CNN (Proposition 11 and Lemma 3). Secondly, if we apply our result on estimation error bounds to concrete classes using the approximation results of [Klusowski 2018] (for Barron class) and [Schimidt-Hieber 2017] (for H\\\u201dolder class), the assumption of Corollary 1 about M_1 is not satisfied because the Lipschitz constant of the CNN is too large. We devised the parameter rescaling technique to reduce the Lipschitz constant to meet the assumption. We discussed the problem in Section 5.1 (Barron class), Section 5.2 (H\\\u201dolder class), and Lemma 6.\n\n3. We have added the detail proof as Lemma 7.", "title": "Reply from authors"}, "H1l4r5IKpQ": {"type": "rebuttal", "replyto": "ryxrITy5nm", "comment": "Thank you for your detailed review. We would appreciate your insightful comments. We reply to your comments one by one.\n\n> However, the obtained CNN approximating architectures look quite unrealistic compared to most practical use-cases of CNNs,\n> since they specifically try to reproduce a fully-connected architecture, leading to residual blocks of depth ~= D/K,\n> which is very deep compared to usual CNNs/ResNets (considering, e.g. K=3 and D in the hundreds for images).\n\nIt is true that the residual blocks in the original ResNet have 2 layers, while those in ours have much more layers. However, identity connections skipping many layers are not rare. For example, one of the variants of DenseNet (Huang (2017)) used to train ImageNet consists of 201 layers, and its outermost connection skips 48 layers (>20% of the whole networks).\n\nAlthough It might be a different discussion point,  in view of sparsity, we would argue that our NNs are more practical than ones used in previous literature (e.g., Yarotsky (2017), Schmidt-Hieber (2017), and Imaizumi & Fukumizu (2018)). They imposed somewhat artificial sparse constraints to FNNs by restricting the number of non-zero parameters. However, we need to train with L0 regularization to realize such NNs and hence actual NNs do not have such non-zero sparsity patterns. Contrary to that, our CNN is dense in general since the block-sparse FNNs from which we construct CNNs have dense blocks. We have fixed our paper to remove the sparsity constraints to made it clear that our CNN is dense in general.\n\n\n> In particular, CNNs are typically used when there is some relevant inductive bias such as equivariance\n> to translations (and invariance with pooling operations) to take advantage of,\n> so removing this inductive bias by approximating fully-connected architectures seems a bit twisted.\n\nAs appeared in Zhou (2018) or Petersen & Voigtlaender (2018), it is one of the standard approaches in the function approximation theory for CNNs to approximate a target function with FNNs and to transform the FNNs into CNNs. Although this approach is somewhat indirect as you pointed out, we believe it is still useful from a viewpoint of inductive bias, too. If we can successfully reflect inductive biases as particular structures of FNNs, like block-sparseness as we did in this paper, CNNs can capture the biases via FNNs. Although this is just an idea, if the dataset has some invariance (such as translation invariance), we can expect blocks in an FNN might have some redundancy in some sense (e.g., blocks are similar to each other). Using the weight-sharing property of CNNs, we might need fewer parameters to realize a function using CNNs than using FNNs, as we pointed out in the conclusion section.\n\n\n> Separately, the presentation of the paper could be significantly improved, for instance by introducing relevant notions more clearly in the introduction and related work sections, and by providing more insight and discussion of the obtained results in the main paper.\n\nThank you for your suggestion. We are thinking to add an extended discussion in the next revision.", "title": "Reply from authors (1/2)"}, "r1evGqLtpQ": {"type": "rebuttal", "replyto": "rJxsSvvcnQ", "comment": "We appreciate your detailed and insightful comments. We reply to your comments one by one.\n\n> However, it is not very clear how the convolutional structure of CNNs help in the analysis of approximating FNNs. For example, in the analysis of C.1 and C.2, it will help better understand why CNNs may work from a high-level intuition when the authors construct the filters. \n\nConvolution with a size-1 filter, inspired by a 1x1 convolution used in image recognition models such as Inception (Szegedy et al. (2014)), is equivalent to dimension-wise affine transformation. Intuitively, it implies CNNs have as powerful learning ability as FNNs. There is room for discussion if our proofs can effectively utilize the convolutional structure of CNNs. However, we have shown that approximation and estimation error rates are no worse than that of FNNs. In particular, CNNs can already achieve the minimax optimal rate for the H\\\u201dolder class. That means even if we make full use of convolutional structure, we have no hope to improve the rate. Considering that the learning ability of CNNs had not been investigated deeply in the literature, we believe our analysis is a critical first step toward unveiling the learning ability of CNNs.\n\nWith that being said, we also want to leverage the inductive bias of data to yield advantageous learning ability of CNNs over FNNs. We believe the analysis of CNNs employing FNNs could be a promising strategy. If we can successfully reflect inductive biases as particular structures of FNNs, like block-sparseness as we did in this paper, CNNs can capture the biases via FNNs. Although this is just an idea, if the dataset has some invariance (such as translation invariance), we could expect blocks in an FNN has redundancy in some sense (e.g., blocks are similar to each other). Using the weight-sharing property of CNNs, we might need fewer parameters to realize a function using CNNs than using FNNs, as we pointed out in the conclusion section.\n\n\n> Moreover, it will also help better understand the expressive power of CNNs if the authors can provide some extended discussion on why approximating the block-sparse FNNs rather than arbitrary feed-forward networks. Is there any fundamental reason (or a counterexample) this cannot be realized, or is there to some extent a technical barrier in the analysis? \n\nAlmost all FNNs used in the previous studies to approximate some specific function classes have block-sparse structures. For example, Yarotsky (2017), Yarotsky (2018), and Zhou (2018). Same is true of the case of the expansion of a Besov function by wavelet bases (B\u00f6lcskei et al. (2017). From the viewpoint of functional analysis, block-sparse structure naturally corresponds to expansion of the target function with a set of basis functions approximated by dense FNNs.\nIt is not trivial how to (approximately) transform general FNNs without block-sparse structures into ResNet-type CNNs, because there is no principled way to decompose an FNN into residual blocks. Although block-sparse FNNs are somewhat theoretical tools, we can realize optimal dense ResNet CNNs, by bypassing them.", "title": "Reply from authors"}, "HJxRgdIKaX": {"type": "rebuttal", "replyto": "HklnzhR9YQ", "comment": "We have uploaded the revised version of our paper. The main differences from the previous one are as follow:\n\n- We removed the sparsity constraints (specified S by the previous version) from $\\mathcal{F}^{\\mathrm{(CNN)}}$ in order to emphasize that the CNNs we consider is dense in general. Accordingly, the statements of Theorem 2 and Corollary 1 (and Lemma 4) are changed so that they do not use S.\n- We added the lemma (Lemma 7) on how to approximate the \\beta-H\\\u201dolder function using block-sparse FNNs by modifying the proof of Schmidt-Hieber (2017).\n- Fixed typos and grammatical errors and changed several variables for readability.\n\nThank you for your interest.", "title": "Revised version uploaded"}, "HJeo4wAshX": {"type": "review", "replyto": "HklnzhR9YQ", "review": "This manuscript shows the statistical error of the ERM for nonparametric regression using the family of a Resnet-type of CNNs. Specifically, two results are showed. First, the authors show that any block-sparse fully connected neural network can be embedded in CNNs. Second, they show the covering number of the family of CNNs. Combining with the existing results of the approximation error of neural nets (Klusowski&Barron 2016, Yarotsky 2017, Schmidt-Hieber 2017), they show the L2 statistical risk. \n\nDetailed comments:\n\n1. The intuition of using block-sparse FNN seems unclear. It seems that when $M=1$, it reduces to the sparse NN considered in [Schmidt-Hieber 2017]. In the proof of Corollary 5, the authors directly use the error of approximating Holder smooth function by sparse FNN and show that the construction in [Schmidt-Hieber 2017] is actually block-sparse. Thus, it seems unclear why we should consider such block-sparse family. Can any sparse NN be embedded in the family of CNNs?\n\n2. In the Related Work, the authors only compare with 2 previous work on the approximation error of CNN. Actually, this work is more related to [Schmidt-Hieber 2017] due to borrowing the results. It would be better to see what the novelties are compared with that work, especially in terms of the proof techniques.\n\n3. The authors claim that the construction of approximator for Holder functions in [Schmidt-Hieber 2017] is block sparse. It would be nice to give more details of the construction since this is not claimed in [Schmidt-Hieber 2017].", "title": "The block sparse structure seems unnecessary given the results in [Schmidt-Hieber 2017].", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJxsSvvcnQ": {"type": "review", "replyto": "HklnzhR9YQ", "review": "The authors demonstrate the function expression properties for the Residual type convolutional neural networks to approximate the block sparse fully connected neural networks. Then it is shown that such Res-CNNs can approximate any function as long as it can be expressed by the block-sparse FNNs, including the Barron class and Holder class functions. The price to pay is that the number of parameters is larger than that of the FNNs by a constant factor. \n\nThe idea for connecting the expressive ability of CNNs with FNNs is interesting, which can fully take advantage of the power of FNNs to understand CNNs. However, it is not very clear how the convolutional structure of CNNs help in the analysis of approximating FNNs. For example, in the analysis of C.1 and C.2, it will help better understand why CNNs may work from a high-level intuition when the authors construct the filters. \n\nMoreover, it will also help better understand the expressive power of CNNs if the authors can provide some extended discussion on why approximating the block-sparse FNNs rather than arbitrary feed-forward networks. Is there any fundamental reason (or a counterexample) this cannot be realized, or is there to some extent a technical barrier in the analysis? \n\nMinor issue\n\nOn page 20, \u201cBounds residual blocks\u201d -> \u201cBounds for residual blocks\u201d\n", "title": "Approximate block sparse fully connected neural networks, the Barron class and the Holder class using the Residual CNNs", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ryxrITy5nm": {"type": "review", "replyto": "HklnzhR9YQ", "review": "The paper studies approximation and estimation properties of CNNs with residual blocks in the context\nof non-parametric regression, by constructing equivalent fully-connected architectures (with a block-sparse structure),\nand leveraging previous approximation results for such functions.\nExplicit risk bounds are obtained for regression functions in Barron and Holder classes.\n\nThe main contribution of the paper is Theorem 1, which shows that a class of ResNet-type CNNs\ncontains a class of \"block-sparse\" fully-connected networks, with appropriate constraints on various size quantities.\nThis result allows the authors to obtain a general risk bound for the ResNet CNN that minimizes empirical risk\n(Theorem 2, which mostly follows Schmidt-Hieber (2017)),\nas well as adaptations of the bound for the Barron and Holder classes, by relying on existing approximation results.\n\nThe construction of Theorem 1 is interesting, and shows that ResNet CNNs can be quite powerful function approximators,\neven with a filter size that is arbitrarily fixed.\nHowever, the obtained CNN approximating architectures look quite unrealistic compared to most practical use-cases of CNNs,\nsince they specifically try to reproduce a fully-connected architecture, leading to residual blocks of depth ~= D/K,\nwhich is very deep compared to usual CNNs/ResNets (considering, e.g. K=3 and D in the hundreds for images).\nIn particular, CNNs are typically used when there is some relevant inductive bias such as equivariance\nto translations (and invariance with pooling operations) to take advantage of,\nso removing this inductive bias by approximating fully-connected architectures seems a bit twisted.\nThe approach of reducing the function class to be approximated would seem more relevant here,\nas in the cited papers Petersen & Voigtlaender (2018) and Yarotsky (2018), and perhaps the results of\nthe present paper can be useful in such a scenario as well.\n\nSeparately, the presentation of the paper could be significantly improved,\nfor instance by introducing relevant notions more clearly in the introduction and related work sections,\nand by providing more insight and discussion of the obtained results in the main paper.\n\nMore specific comments:\n- Section 1, p.2: define M? define D? M seems to be used for different things in different paragraphs\n- Section 2: Explain what is \"s\" in the Barron class, or at least point to the relevant definition in the paper\n- Section 3.1:\n  * 'estimation error' is usually called '(expected) risk' in the statistical literature (also in the introduction). estimation error would have to do with relating R and R^hat\n  * why is the estimator \"regularized\"?\n- Definition 2: shouldn't it be D_m^(0) = D instead of 1?\n- Theorem 1: What is L? Also, it would be helpful to sketch the construction in the main paper given that this is the main result.\n- Section 4.2: M_1 is the Lipschitz constant of what function?\n- Section 5.1: \"M = 1\" this is confusing, maybe use a different letter for the ridge expansion? The discussion on 'relative scale' could be made clearer.\n- Section 5.2, 'if we carefully look at their proofs': more details on this should be provided.\n", "title": "Interesting approximation and estimation results, but considers somewhat unrealistic CNNs", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}