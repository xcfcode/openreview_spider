{"paper": {"title": "Stochastic Activation Pruning for Robust Adversarial Defense", "authors": ["Guneet S. Dhillon", "Kamyar Azizzadenesheli", "Zachary C. Lipton", "Jeremy D. Bernstein", "Jean Kossaifi", "Aran Khanna", "Animashree Anandkumar"], "authorids": ["guneetdhillon@utexas.edu", "kazizzad@uci.edu", "zlipton@cmu.edu", "bernstein@caltech.edu", "jean.kossaifi@gmail.com", "arankhan@amazon.com", "animakumar@gmail.com"], "summary": "", "abstract": "Neural networks are known to be vulnerable to adversarial examples. Carefully chosen perturbations to real images, while imperceptible to humans, induce misclassification and threaten the reliability of deep learning systems in the wild. To guard against adversarial examples, we take inspiration from game theory and cast the problem as a minimax zero-sum game between the adversary and the model. In general, for such games, the optimal strategy for both players requires a stochastic policy, also known as a mixed strategy. In this light, we propose Stochastic Activation Pruning (SAP), a mixed strategy for adversarial defense. SAP prunes a random subset of activations (preferentially pruning those with smaller magnitude) and scales up the survivors to compensate. We can apply SAP to pretrained networks, including adversarially trained models, without fine-tuning, providing robustness against adversarial examples. Experiments demonstrate that SAP confers robustness against attacks, increasing accuracy and preserving calibration.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "This is a borderline paper.  The reviewers are happy with the simplicity of the proposed method and the fact that it can be applied after training; but are concerned by the lack of theory explaining the results.  I will recommend accepting, but I would ask the authors add the additional experiments they have promised, and would also suggest experiments on imagenet."}, "review": {"ryrXQ4wyz": {"type": "review", "replyto": "H1uR4GZRZ", "review": "This paper investigates a new approach to prevent a given classifier from adversarial examples. The most important contribution is that the proposed algorithm can be applied post-hoc to already trained networks. Hence, the proposed algorithm (Stochastic Activation Pruning) can be combined with algorithms which prevent from adversarial examples during the training.\n\nThe proposed algorithm is clearly described. However there are issues in the presentation.\n\nIn section 2-3, the problem setting is not suitably introduced.\nIn particular one sentence that can be misleading:\n\u201cGiven a classifier, one common way to generate an adversarial example is to perturb the input in direction of the gradient\u2026\u201d\nYou should explain that given a classifier with stochastic output, the optimal way to generate an adversarial example is to perturb the input proportionally to the gradient. The practical way in which the adversarial examples are generated is not known to the player. An adversary could choose any policy. The only thing the player knows is the best adversarial policy.\n\nIn section 4, I do not understand why the adversary uses only the sign and not also the value of the estimated gradient. Does it come from a high variance? If it is the case, you should explain that the optimal policy of the adversary is approximated by \u201cfast gradient sign method\u201d. \n\nIn comparison to dropout algorithm, SAP shows improvements of accuracy against adversarial examples. SAP does not perform as well as adversarial training, but SAP could be used with a trained network. \n\nOverall, this paper presents a practical method to prevent a classifier from adversarial examples, which can be applied in addition to adversarial training. The presentation could be improved.\n", "title": "overall, this paper presents a practical method to prevent a classifier from adversarial examples, which can be applied in addition to adversarial training. The presentation could be improved.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJFnpOYxM": {"type": "review", "replyto": "H1uR4GZRZ", "review": "This paper propose a simple method for guarding trained models against adversarial attacks. The method is to prune the network\u2019s activations at each layer and renormalize the outputs. It\u2019s a simple method that can be applied post-training and seems to be effective.\n\nThe paper is well written and easily to follow. Method description is clear. The analyses are interesting and done well. I am not familiar with the recent work in this area so can not judge if they compare against SOTA methods but they do compare against various other methods.\n\nCould you elaborate more on the findings from Fig 1.c Seems that  the DENSE model perform best against randomly perturbed images. Would be good to know if the authors have any intuition why is that the case.\n\nThere are some interesting analysis in the appendix against some other methods, it would be good to briefly refer to them in the main text.\n\nI would be interested to know more about the intuition behind the proposed method. It will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method.\n\nAlso would like to see some notes about computation complexity of sampling multiple times from a larger multinomial.\n\nAgain I am not familiar about different kind of existing adversarial attacks, the paper seem to be mainly focus on those from Goodfellow et al 2014. Would be good to see the performance against other forms of adversarial attacks as well if they exist.", "title": "Simple and yet effective method against adversarial attack post traning.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ry5D1Z5xf": {"type": "review", "replyto": "H1uR4GZRZ", "review": "The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations. Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking.\n\nOn one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks. On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness. For example, how does this compare to random perturbation (say, zero-mean) of the weights? This adds stochasticity as well so why and why not this work? The authors do not give any insight in this regard.\n\nOverall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner. The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else).\n\n", "title": "Interesting heuristic but little theoretical justification", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJvA3yQQG": {"type": "rebuttal", "replyto": "ryrXQ4wyz", "comment": "We thank Reviewer2 for thorough comments. We are glad that the reviewer appreciated the value of an adversarial defense technique that can be applied post-hoc and our exposition. We reply to specific points below:\n\n1. You are correct, the defender does not know the what policy any actual adversary will use, only what the optimal adversary might do, thus the objective of minimizing the worst-case performance. We are improving the draft to be clearer in this regard.\n\n2. Regarding: \u201cIn section 4, I do not understand why the adversary uses only the sign and not also the value of the estimated gradient.\u201d: The reason why we are considering only the sign is because we cap the infinity norm of the adversarial perturbation. This leads to taking a step of equal size in each input dimension and thus the gradient magnitude does not come into play. This approach is standard in the recent academic study of adversarial examples and follows work by Goodfellow et al. (2014), which showed that imperceptible adversarial examples could be produced efficiently in this manner.. \n\nOne motivation for considering the infinity norm (vs L2 or L1) for constraining the size of an adversarial perturbation is that it accords more closely with perceptual similarity. For example, it\u2019s possible to devise a perturbation with small L2 norm that is perceptually obvious because it moves a small group of pixels a large amount. \n\nNaturally, a stronger adversary might pursue an iterative approach rather than making one large perturbation. To this end, we are currently running experiments with iterative attacks and the initial results are promising - SAP continues to significantly outperform the dense model. We will add these results to the paper when they are ready.\n\n3. We are grateful for the reviewer\u2019s suggestions for improving the exposition and are currently working to revise the draft in accordance with these recommendations. To start, we have improved some of the (previously) confusing language that might have failed to distinguish between the optimal adversary and some arbitrary adversary which may not apply the optimal perturbation.\n", "title": "Response AnonReviewer2"}, "rkk5517Qf": {"type": "rebuttal", "replyto": "SJFnpOYxM", "comment": "Thanks for your clear review of our paper. We are glad that you appreciated both the method and the clarity of exposition. \n\n1. Regarding Fig 1.c: While dense models are susceptible to adversarial attack, they are actually quite robust to random noise. The purpose of reporting the results of this experiment is to provide context for the other results. Because dense models are not especially vulnerable to random noise, we are not surprised that they perform well here. \n\n2. Thanks for the suggestion that the analysis in the appendix should be summarized within the body of the paper. Per your request, we have added an additional subsection (5.3) in the current draft that briefly describes the baselines and we have included a corresponding figure that shows the quantitative results for each.\n\n3. While we are reluctant to present an explanation for a phenomena that we do not fully understand, we are happy to share the intuitions that guided us in developing the algorithm: \n\nOriginally we were looking sparsifying the weights and/or activations of the network. We were encouraged by results, e.g. https://arxiv.org/abs/1510.00149, showing high accuracy with sparsified weights (as by pruning). We thought that by sparsifying a network, we might maintain high accuracy while lowering the Lipschitz constant and thus conferring some robustness against small perturbations. We later drew some inspiration from randomized algorithms that sparsify matrices by randomly dropping entries according to their weights and scaling up the survivors to produce a sparse matrix with similar spectral properties to the original.\n\n4. Sampling from the multinomial is fast. Without getting into detail about how many random bits are needed, given uniform samples, we can convert to a sample from a multinomial by performing a binary search. So it\u2019s roughly k log(n) where k is the number of samples and n is the number of activations. As a practical concern, sampling from the multinomial in our algorithms does not comprise a significant computational obstacle.\n\n5. As you correctly point out, In our experiments, we adopt approach from Goodfellow et al. of evaluating with adversarial perturbations produced by taking a single step with capped infinity norm. However, we generate these attacks differently for each model. Against our stochastic models, the adversary produces the attack by estimating the gradient with MC samples. \n\n6. Per your suggestions we have compared against a stronger modes of attack, namely an iterative update where we take multiple small updates, each of capped infinity norm. In these experiments, SAP continues to outperform the dense model significantly. We are currently compiling these results and will add them to the draft when ready. \n", "title": "Response to AnonReviewer1"}, "B1DQ5J77z": {"type": "rebuttal", "replyto": "ry5D1Z5xf", "comment": "Thanks for the thoughtful review of our paper. We are glad that you recognize the empirical strength of the result and the simplicity of the method. We are also share your desire for greater theoretical understanding.\n\nRegarding: \u201chow does this compare to random perturbation (say, zero-mean) of the weights?\u201d.\nWe ran this experiment, and found that it did not help. Additionally, for a more direct comparison, we compared against zero-mean Gaussian noise applied to the activations. We call this method Random Noisy Activations (RNA). It was previously described only in Appendix B, but we have now added a brief description to section 5 and reported the quantitative results in Figure 5.\n\nDespite extensive empirical study, precisely why our method works but random noise on the activations does not remains unclear. While we can imagine some ways of spinning a theoretical story post-hoc, the honest answer is that we do not yet possess a solid theoretical explanation. We share your desire for a greater understanding and plan to investigate this direction further in future work.\n\n***TL;DR: Per your suggestions, we have improved the draft by running additional experiments. Please find in Figure 5 results for 0-mean gaussian noise applied to weights with sigma values {.01, .02, \u2026, .05}, as well as results for several other sensible baselines and greater detail in Appendix B.***\n", "title": "Response to AnonReviewer3"}, "HJRkw1X7f": {"type": "rebuttal", "replyto": "H1uR4GZRZ", "comment": "We would like to thank the reviewers for their thoughtful responses to our paper. We are glad to see that there is a consensus among the reviewers to accept and are grateful to each of the reviewers for critical suggestions that will help us to improve the work. Please find individual replies to each of the reviews in the respective threads.", "title": "General reply to all reviewers"}}}