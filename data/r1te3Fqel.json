{"paper": {"title": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension", "authors": ["Yang Yu", "Wei Zhang", "Bowen Zhou", "Kazi Hasan", "Mo Yu", "Bing Xiang"], "authorids": ["yu@us.ibm.com", "zhangwei@us.ibm.com", "zhou@us.ibm.com", "kshasan@us.ibm.com", "yum@us.ibm.com", "bingxia@us.ibm.com"], "summary": "", "abstract": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset.", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"]}, "meta": {"decision": "Reject", "comment": "The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work."}, "review": {"SyDtUCxEg": {"type": "rebuttal", "replyto": "HkfluXx4x", "comment": "Thanks for reviewing and insightful questions. We updated the paper according your comments and the new version fixed the typo and makes the description more clear.\n\nThe below is a bit more clarification about the chunk representation that you had a question. After convolution layer, each passage word has 3 convolution outputs. Each one of three outputs is a convolution over concatenated forward and backward hidden states. So the first half of a convolution output encodes information in forward RNN states and the second half encodes information in backward RNN hidden states. We experimented with several pooling functions (e.g., max, average), and found out that, instead of pooling, the best function is to concatenate the first half of convolution output of the chunk's first word and the second half of convolution output of the chunk's last word. ", "title": "RE: no title"}, "r1bUa1IQl": {"type": "rebuttal", "replyto": "BkbXoK0ze", "comment": "Thanks for your questions and thoughts!\n\n1. For each word in passage, we convolute the contextual unigram, bigram and trigram to get multiple new states for that word in different perspectives.\n\nWith the convolution on attention layer output, we combine the convoluted hidden state of the first word in the chunk and the convoluted hidden state of the last word in the chunk to represent the chunk representation. Since each word has three convoluted hidden states, each chunk has three different representations from different perspectives.\n\n2. It is true that linguistic features are helping the model get the current best performance. We think these features are in similar effects as character embeddings used as input in some other approaches.\n", "title": "RE: Pre-review Questions"}, "BJ3ypJ8me": {"type": "rebuttal", "replyto": "ryZq9xG7l", "comment": "Thanks for your questions and recommendations!\n\nWe have some attention mechanism that are coincidentally similar to other approaches. However we also have some unique mechanisms, for example we have a convolution layer to consider n-gram tokens and we combine the convoluted hidden state of the first word in the chunk and the convoluted hidden state of the last word in the chunk to represent the chunk representation.\n\nWe did cross validation on training set to choose max-length N.\n\nWe just updated the paper using \\citep{x} and moving the description of the model to the main body.", "title": "RE: More clarifications"}, "ryZq9xG7l": {"type": "review", "replyto": "r1te3Fqel", "review": "I don't fully understand how your attention mechanism is different compared to other approaches. Could you please elaborate?\n\nWhen you enumerate chunks in the chunk representation layer, how is the max-length N chosen?\n\nBy the way, you may want to replace \\cite{x} with \\citep{x} for the more readable \"(X et. al, YYYY)\" citations, instead of the in-text \"X et al, YYYY\" style from the former.  I would also encourage the description of the model to be discussed in the main body of the paper, as opposed to in the Appendix.SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.\n\nTHOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of \"learning end-to-end \" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an \"end-to-end trained\" system.\n\nThe paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.\n\nFinally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/). Of course, it may be that further training and hyperparameter optimizations may improve these results.\n\nTherefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), I don't feel the paper is ready in its current form to be accepted to the conference.\n\nNote: The GRU citation should be (Cho et al., 2014), not (Bengio et al., 2015).", "title": "More clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJ5ROhbNx": {"type": "review", "replyto": "r1te3Fqel", "review": "I don't fully understand how your attention mechanism is different compared to other approaches. Could you please elaborate?\n\nWhen you enumerate chunks in the chunk representation layer, how is the max-length N chosen?\n\nBy the way, you may want to replace \\cite{x} with \\citep{x} for the more readable \"(X et. al, YYYY)\" citations, instead of the in-text \"X et al, YYYY\" style from the former.  I would also encourage the description of the model to be discussed in the main body of the paper, as opposed to in the Appendix.SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.\n\nTHOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of \"learning end-to-end \" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an \"end-to-end trained\" system.\n\nThe paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.\n\nFinally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/). Of course, it may be that further training and hyperparameter optimizations may improve these results.\n\nTherefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), I don't feel the paper is ready in its current form to be accepted to the conference.\n\nNote: The GRU citation should be (Cho et al., 2014), not (Bengio et al., 2015).", "title": "More clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkTXu3kmx": {"type": "rebuttal", "replyto": "r1st5Dnfx", "comment": "1) We tried using the context after the word, and that does not improve over model without using such a CNN layer. As bigram and trigram convolutions both used filters, we think that using filters on unigrams could help the convoluted results from each convolution be mapped to the same space.\n\n2) No, we don't have a further RNN for each l-gram. We create multiple representations for each chunk using l-grams. So each chunk has multiple comparisons to the question from different perspectives (3 comparisons to be exact, using unigram, bi-gram and tri-gram representation of the chunk, respectively) and we use a fully connected layer to consider all these perspectives together.\n\n3) It is more likely due to the data distribution, where questions with answer length 9 are relatively easier. We see similar pattern in other works, i.e.  from (Wang  & Jiang 2016).\n\n4) We have demonstrated that our model does not overfit to dev in previous test results. So we believe it will be similar in the other settings. In addition, our system is still under development so we won't want to submit the results too frequently in order to avoid the risk of peeking the test data.", "title": "Cnn questions"}, "BkbXoK0ze": {"type": "review", "replyto": "r1te3Fqel", "review": "1.\tI found the descriptions of DCR not very clear. Could the authors explain how does the convolution layer work? Also, it is also not very clear how DCR dynamically constructs the chunks. \n2.\tFrom Table 3, it seems that the additional linguistic features are very important for the performance of DCR. However, some of the recent models are able to obtain decent results without using these features. Could the authors provide some possible explanations here?\nThe paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.\n\nThere are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:\n\n1.\tThe use of convolution model, and\n2.\tDynamic chunking\n\nConvolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.\n\nThe dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.\n\nThe authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.\n\nIn short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.\n", "title": "Pre-review Questions", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkoowPWEe": {"type": "review", "replyto": "r1te3Fqel", "review": "1.\tI found the descriptions of DCR not very clear. Could the authors explain how does the convolution layer work? Also, it is also not very clear how DCR dynamically constructs the chunks. \n2.\tFrom Table 3, it seems that the additional linguistic features are very important for the performance of DCR. However, some of the recent models are able to obtain decent results without using these features. Could the authors provide some possible explanations here?\nThe paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.\n\nThere are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:\n\n1.\tThe use of convolution model, and\n2.\tDynamic chunking\n\nConvolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.\n\nThe dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.\n\nThe authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.\n\nIn short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.\n", "title": "Pre-review Questions", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1st5Dnfx": {"type": "review", "replyto": "r1te3Fqel", "review": "Hi, I have a few questions.\n1) In the CNN layer wy did you use the context before the word and not a context window around it?  I am also wondering what are the benefits of the unigram filter.\n2) Do you have a further RNN for each l-gram, after the CNN layer?\n3) Can you speculate on the peak of performance at lenght 9 in Figure 2(a)\n4) Why do not you have test set results of the last two models in Table 3?SUMMARY.\nThe paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.\nThe model first encodes the passage and the query using a recurrent neural network.\nWith an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.\nThe encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.\nThree convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.\nCandidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.\nEach candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated.\nThe scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.\n\nThe method is tested on the SQUAD dataset and outperforms the proposed baselines.\n\n----------\n\nOVERALL JUDGMENT\nThe method presented in this paper is interesting but not very motivated in some points.\nFor example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.\nThe contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.\nIn fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.\n\n----------\n\nDETAILED COMMENTS\n\nEquation (13) i should be s, not s^l.\n\nI still do not understand the sentence \" the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN\". The RNN is over what all the words in the chunk? in the passage? \nThe answer the authors gave in the response does not clarify this point.", "title": "Cnn questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkfluXx4x": {"type": "review", "replyto": "r1te3Fqel", "review": "Hi, I have a few questions.\n1) In the CNN layer wy did you use the context before the word and not a context window around it?  I am also wondering what are the benefits of the unigram filter.\n2) Do you have a further RNN for each l-gram, after the CNN layer?\n3) Can you speculate on the peak of performance at lenght 9 in Figure 2(a)\n4) Why do not you have test set results of the last two models in Table 3?SUMMARY.\nThe paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.\nThe model first encodes the passage and the query using a recurrent neural network.\nWith an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.\nThe encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.\nThree convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.\nCandidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.\nEach candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated.\nThe scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.\n\nThe method is tested on the SQUAD dataset and outperforms the proposed baselines.\n\n----------\n\nOVERALL JUDGMENT\nThe method presented in this paper is interesting but not very motivated in some points.\nFor example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.\nThe contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.\nIn fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.\n\n----------\n\nDETAILED COMMENTS\n\nEquation (13) i should be s, not s^l.\n\nI still do not understand the sentence \" the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN\". The RNN is over what all the words in the chunk? in the passage? \nThe answer the authors gave in the response does not clarify this point.", "title": "Cnn questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}