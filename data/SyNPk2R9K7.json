{"paper": {"title": "Learning to Describe Scenes with Programs", "authors": ["Yunchao Liu", "Zheng Wu", "Daniel Ritchie", "William T. Freeman", "Joshua B. Tenenbaum", "Jiajun Wu"], "authorids": ["georgeycliu@gmail.com", "14wuzheng@sjtu.edu.cn", "daniel_ritchie@brown.edu", "billf@mit.edu", "jbt@mit.edu", "jiajunwu@mit.edu"], "summary": "We present scene programs, a structured scene representation that captures both low-level object appearance and high-level regularity in the scene.", "abstract": "Human scene perception goes beyond recognizing a collection of objects and their pairwise relations. We understand higher-level, abstract regularities within the scene such as symmetry and repetition. Current vision recognition modules and scene representations fall short in this dimension. In this paper, we present scene programs, representing a scene via a symbolic program for its objects, attributes, and their relations. We also propose a model that infers such scene programs by exploiting a hierarchical, object-based scene representation. Experiments demonstrate that our model works well on synthetic data and transfers to real images with such compositional structure. The use of scene programs has enabled a number of applications, such as complex visual analogy-making and scene extrapolation.", "keywords": ["Structured scene representations", "program synthesis"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents a dataset and method for training a model to infer, from a visual scene, the program that would generate/describe it. In doing so, it produces abstract disentangled representations of the scene which could be used by agents, models, and other ML methods to reason about the scene.\n\nThis is yet another paper where the reviewers disappointingly did not interact. The first round of reviews were mediocre-to-acceptable. The authors, I think, did a good job of responding to the concerns raised by the reviewers and edited their paper accordingly. Unfortunately, not one of the reviewers took the time to consider author responses.\n\nIn light of my reading of the responses and the revisions in the paper, I am leaning towards treating this as a paper where the review process has failed the authors, and recommending acceptance. The paper presents a novel method and dataset, and the experiments are reasonably convincing. The paper has flaws and the authors are advised to carefully take into account the concerns flagged by reviewers\u2014many of which they have responded to\u2014in producing their final manuscript."}, "review": {"r1lOGdVV1E": {"type": "rebuttal", "replyto": "r1xvq3AZRQ", "comment": "Dear Reviewer 1,\n\nThanks again for your constructive review, which has helped us improved the quality and clarity of the paper. In addition to our response above, in the revision, we have included comparisons with additional baselines and increased the complexity of the scene.\n\nAs the discussion period is about to end, please don\u2019t hesitate to let us know if there are any additional clarifications that we can offer, as we would love to convince you of the merits of the paper. We appreciate your suggestions. Thanks!\n", "title": "Looking forward to your feedback"}, "SkgTlO4N1N": {"type": "rebuttal", "replyto": "Hkl3eaCWAm", "comment": "Dear Reviewer 2,\n\nWe would like to thank you again for your constructive review, which has helped us improved the quality of the paper. We have cited and discussed the related work as suggested. We have also included comparisons with additional, search-based baselines and increased the complexity of the scene.\n\nAs the discussion period is about to end, please don\u2019t hesitate to let us know if there are any additional clarifications that we can offer, as we would love to convince you of the merits of the paper. We appreciate your suggestions. Thanks!\n", "title": "Looking forward to your feedback"}, "H1g-kdE4yN": {"type": "rebuttal", "replyto": "BJeQhpCZC7", "comment": "Dear Reviewer 3,\n\nWe would like to thank you again for your constructive review. We have revised the paper accordingly. In particular, we have included comparisons with additional, search-based baselines and increased the complexity of the scene. We have also revised the text and included a supplementary material for better clarity and more implementation details.\n\nAs the discussion period is about to end, please don\u2019t hesitate to let us know if there are any additional clarifications that we can offer, as we would love to convince you of the merits of the paper. We appreciate your suggestions. Thanks!\n", "title": "Looking forward to your feedback"}, "HJlMBWP9R7": {"type": "rebuttal", "replyto": "BJeQhpCZC7", "comment": "Thanks again for your review. We have updated our manuscript accordingly and posted a summary of changes. Please don\u2019t hesitate to let us know if you have additional feedback.", "title": "Our Response to Reviewer 3"}, "HJePX-DcCm": {"type": "rebuttal", "replyto": "Hkl3eaCWAm", "comment": "Thanks again for your review. We have updated our manuscript accordingly and posted a summary of changes. Please don\u2019t hesitate to let us know if you have additional feedback.", "title": "Our Response to Reviewer 2"}, "Byxj--DqRX": {"type": "rebuttal", "replyto": "r1xvq3AZRQ", "comment": "Thanks again for your review. We have updated our manuscript accordingly and posted a summary of changes. Please don\u2019t hesitate to let us know if you have additional feedback.", "title": "Our Response to Reviewer 1"}, "SJgwjgP5R7": {"type": "rebuttal", "replyto": "rygrxnzECX", "comment": "Thanks for your comments! We have cited and discussed these related works in our revised manuscript.", "title": "Thank you for your interest in our work"}, "rkegVxPc07": {"type": "rebuttal", "replyto": "SyNPk2R9K7", "comment": "We thank all reviewers for their helpful comments. We have performed additional experiments and revised the manuscript according to the reviewers\u2019 suggestions, The specific changes include:\n\n1. We have cited and discussed additional related work in Sec. 2.\n\n2. We have added a search-based heuristic grouping method as an ablated version of our full model, with descriptions in Sec. \n4.2 (paragraph 1) and quantitative results in Table 2.\n\n3. We have added results on a new, more challenging dataset with random object arrangement. The updated quantitative results are in Table 2. We have also added relevant discussions in Sec. 4.1 (paragraph 2) and 4.2 (setup & results).\n\n4. We have included more qualitative results on both synthetic and real images in Fig. 3c and Fig. 7.\n\n5. We have included an appendix, summarizing implementation details on scene configuration (synthetic data generation), the heuristic grouping baseline, and data formats.", "title": "Our Revised Manuscript"}, "BJeQhpCZC7": {"type": "rebuttal", "replyto": "rJgijGzFnX", "comment": "Thank you for your thoughtful review.\n\n1. Data annotations \n\nOur model requires supervised training data in the pre-training stage for each of its components, but the program synthesizer needs no further training when generalizing to other data distributions since it works on object attribute space. We consider this as an advantage of our model, as it is easy to get synthetic data, but much harder to obtain annotations (in particular program annotations) on in-the-wild images. Disentangling vision recognition and program synthesis is a key to our model\u2019s success on real images (Fig. 7): our model accurately predicts programs for the test set with only 90 labeled real images for fine-tuning. \n\nThe group information is inherently included in the program and easy to obtain: when synthesizing data, we first sample several program blocks from our DSL, where each block corresponds to a set of objects that form a group. These program blocks are then combined into the overall program description of the scene. Our model explicitly learns the group information and shows advantages over baseline methods which learn directly from the overall program.\n\n2. Scene complexity \n\nIn our main experiment, we place objects on a grid and then jitter their positions and orientations. Results on these data suggest that scene programs describe these structured images well. We agree with reviewers on the importance of handling more complex data. In the revision by Nov. 26, we will add results on scenes where objects are placed at random.\n\n3. Specific questions\n\n1) Ganin et al: We will revise the description of this work in the updated draft.\n\n2) The stopping criterion is whether the scene has been successfully reconstructed, i.e., when the execution results of the program are the same as the object parsing results obtained from the vision module.\n\n3) Generalization: Yes, the main difference is the number of groups. This is only one of our experiments on generalization. The experiments on real image show our model\u2019s ability to generalize to new data distributions.\n\n4) Top proposals: Note that the group detector also outputs the classification result of the group category. Here \u201ctop\u201d refers to the softmax score. We don\u2019t have any information of the ground truth program at test time.\n\n5) Program representations: We will give detailed definitions in the Appendix. In short, a program is represented as a matrix, where each row contains a program command, which is a program token followed by its parameters.\n\nThanks! Please don\u2019t hesitate to let us know for any additional comments on the paper or on the planned changes.\n", "title": "Our Response to Reviewer 3"}, "Hkl3eaCWAm": {"type": "rebuttal", "replyto": "SylhmVThhQ", "comment": "Thank you for your thoughtful review. \n\n1. Scene program as a contribution \n\nThanks for suggesting the related work, which we\u2019ll cite and discuss. Our paper is new and different from all these papers. Most importantly, our scene programs focus on modeling the high-level structural relationships among multiple objects, building upon and extending CSGNet and De-rendering, which only explored sequential, primitive-level programs. Only with our scene programs and the loop structure, may we efficiently describe patterns that involve higher-order, program-like regularity among multiple objects (e.g., repetition) and edit images with minimal interactions (Fig. 6,7). \n\n2. High-level motivations \n\nAs mentioned above, many tasks become possible only with our program representations, not an attribute-based representation. For example, for the image in Fig. 6(a), it is natural for one to imagine how could we add another row of cylinders on the right side, or change cylinders to cubes. Scene programs allow us to perform such tasks in an efficient and intuitive way. However, if we represent the scene by its objects and attributes, we would have to individually edit each object, losing the benefit from their high-level correlations.\n\nWe agree that it\u2019s important to discuss the alternative approach, where the program synthesis is performed using search and grouping. There are two possible approaches for a structured search over the space of programs, both of which would be too slow for our task:\n- Constraint solving: we would have to use an SMT solver. Ellis et al [1] used SMT solvers to infer 2D graphics programs, and takes on the order of minutes per program. As 3D scenes (with attributes) have a much larger search space, such an approach would not be able to find a solution in reasonable time.\n- Stochastic search: Here the problem would be at least as tough as doing inverse graphics, so we can safely assume that this would work no better than MCMC for inverse graphics. In Picture (Kulkarni et al. [2]), their approach takes minutes for a 2D image with simple contours.\n\nWe have contacted the authors of these two papers, who confirmed our estimates of the efficiency of their methods. In comparison, on average our neural program synthesis model takes less than 0.4 second to generate scene programs for an image.\n\n[1] Ellis, Kevin, Armando Solar-Lezama, and Josh Tenenbaum. \"Unsupervised learning by program synthesis.\" NIPS 2015.\n[2] Kulkarni, Tejas D., et al. \"Picture: A probabilistic programming language for scene perception.\" CVPR 2015.\n\n3. Experimental evaluation \n\nIn our main experiment, we place objects on a grid and then jitter their positions and orientations. Results on these data suggest that scene programs describe these structured images well. We agree with reviewers on the importance of handling more complex data. In the revision by Nov. 26, we will add results on scenes where objects are placed at random.\n\nFollowing your suggestion, we have also experimented with a simple heuristic grouping algorithm: we start with a random object which forms a group. As long as there are objects of distance less than a threshold from this group, we add them into the group. It performs well in general, but cannot resolve difficult instances, such as when a group is surrounded by other objects. This justifies the need to use a deep model to learn from the data distribution. We\u2019ll include the results in the revision.\n\nTo summarize, our main contribution is to propose scene programs as a novel representation of scenes, modeling high-level relations beyond individual objects and pairwise relations. The scene program representation demonstrates strong advantages in tasks such as image editing and analogy making, compared with attribute-based representations. We also propose effective methods to generate accurate program descriptions from input images, which can be applied to real images without additional program supervision. While tackling in-the-wild scenes is an important future direction, we consider our efforts an important step towards high-level scene understanding via program synthesis.\n\nPlease don\u2019t hesitate to let us know for any additional comments on the paper or on the planned changes.\n", "title": "Our Response to Reviewer 2"}, "r1xvq3AZRQ": {"type": "rebuttal", "replyto": "Skg0MUM6nm", "comment": "Thank you for your thoughtful review. \n\n1. Comparing with baseline methods \n\nWe agree on the importance of a fair comparison. We\u2019d like to clarify that\n1) The derender-LSTM baseline requires the same supervision as ours.\n2) We acknowledge the fact that our method requires more supervision than the CNN-LSTM baseline method, and will revise the paper to clearly state that. Note the situation is different when we consider generalization to data where program supervision is hard to obtain (e.g. real images). Because our program synthesizer works on abstract object attribute space, it does not require any fine-tuning to work on real images, where the end-to-end CNN-LSTM approach would require (image, program) pairs. Disentangling vision recognition and program synthesis is a key to our model\u2019s success on real images (Fig. 7).\n\nOur method achieves higher test accuracy and better generalization performance than both baseline, and more importantly, it generalizes better to real images. As synthetic data can be easily obtained, we believe that the comparison in Table 2 is fairly established. These results and the experiments on real images demonstrate the significant advantage of our method.\n\n2. Scene graphs \n\nWe thank the reviewer for bringing up scene graphs, which is another important high-level representation of scenes. Here we would like to emphasize that the roles of scene graphs and the proposed scene programs are complementary: scene graphs focus on the pairwise relationships, e.g. an object can be on, in, or under another object; in contrast, scene programs focus on the higher-order, program-like regularity among multiple objects (e.g., repetition). \n\nCompared with scene graphs, scene programs (i) explicitly capture these regularities, modeling correlations among multiple objects; and (ii) are more efficient in terms of description length. For tasks such as editing structured images, scene programs are a more suitable representation, because editing can be performed on the program space for higher efficiency, as displayed in Fig. 6. With scene graphs, we would have to edit each object one by one.\n\n3. Scene complexity \n\nIn our main experiment, we place objects on a grid and then jitter their positions and orientations. Results on these data suggest that scene programs describe these structured images well. We agree with reviewers on the importance of handling more complex data. In the revision by Nov. 26, we will add results on scenes where objects are placed at random.\n\nPlease don\u2019t hesitate to let us know for any additional comments on the paper or on the planned changes.\n", "title": "Our Response to Reviewer 1"}, "Skg0MUM6nm": {"type": "review", "replyto": "SyNPk2R9K7", "review": "[Overview]\n\nIn this paper, the authors proposed a new format of representation called scene programs, to describe the visual scenes. To extract the scene programs from scenes, the authors exploited the off-the-shelf object detection and segmentations model, mask r-cnn to extract all objects and the corresponding attributes from the images, and then detect groups for those objects, which are then used to generate the programs which matches the input scenes. The experiments are performed on a synthetics datasets which consists of multiple shapes with different attributes. The experiments shows that the proposed model can infer more accurate programs from the scenes, and those generated programs can be used to recover the input scenes more accurately. Besides, the authors also showed that the generated scene programs can be used for image editing and making visual analogy.\n\n[Strengthes]\n\n1. The authors proposed a new representation, called scene programs, to describe the visual scenes with some textual program. This is a new scene representation, which could be potentially used in various scenarios, such as the image synthesis in graphics.\n\n2. The authors proposed a hierarchical method to model the structures in scenes. Specifically, the objects in a scene are first extracted and then grouped into multiple clusters, which will be used to guide the scene program synthesis. \n\n3. The experimental results demonstrate the effectiveness of the proposed method both qualitatively and quantitatively. The authors also showed the the programs generated  can be sued for image editing and cross-modality matching. \n\n[Weaknesses]\n\n1. It is a bit unfair to compare the proposed method with the two baseline methods listed in Table 2. The authors used a pre-trained mask-rcnn to detect all objects and predict the attributes for all objects. However, the counterpart methods have no access to this supervision. Even in this case, CNN-LSTM seems achieve comparable performance on the first three metrics. \n\n2. The advantage of scene program compared with scene graph (Johnson et al) are not clear to me. Scene graph is also a symbolic representation for images. Also, for all the tasks mentioned in this paper, such as image editing and visual analogy, scene graph can probably also complete well. The authors should comment about the specific advantages of scene program in comparison with scene graph.\n\n3. All the images shown in the paper seems arranged uniformly, which I think contains some bias to the proposed grouping strategy. I would like to see more diverse configurations of the foreground objects. It would be good to see if the proposed model can describe more complicated scenes.\n\n[Summary]\n\nThis paper proposed a novel scene representations, called scene program. To extract the scene program, the authors proposed a hieratchical inference method. The resulting scene programs based on the proposed model outperforms several baseline models quantitatively. The authors also showed the proposed scene program is suitable for image editing and visual analogy making. However, as pointed above, there are some unclear points to me, especially the advantages of scene program compared with scene graph, and the representation power of scene program for complicated scenes.", "title": "A novel scene representation proposed, but needs more clarification on its advantages and flexiblity.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SylhmVThhQ": {"type": "review", "replyto": "SyNPk2R9K7", "review": "This paper investigates a descriptive representation of scenes using programs. Given an input image and an initial set of detections obtained from bottom-up detectors a sequence to sequence network is used to generate programs in a domain specific language (DSL). The authors consider a dataset where simple primitives are arranged in layouts in 3D scenes with varying material and color properties. They argue that the scene representation lead to better generalization on novel scene types and improve over baselines on image analogy tasks. The paper is well written but the evaluation and technical novelty is weak. \n\nFirst, the use of scene programs is not a contribution of this paper. Going beyond the works cited in the related work section, several recent works have proposed and investigated the advantages of program synthesis for shape generation (e.g., CSGNet Sharma et al. CVPR 2018 and Scene derendering, Wu et al., CVPR 2017), visual reasoning (Modular networks, Andreas et al., 2015), among others. \n\nAt a high-level the motivation of the program level representation for the considered tasks is not highlighted. It seems that an attribute-based representation, i.e., the output of the mask R-CNN detector that describes the image as a collection of objects, material properties, and their positions and scales is a sufficient representation. The higher-order relationships can be relatively easily extracted from the detections since the images are clean and clutter free. A baseline approach where the program synthesis was performed using search and grouping should be compared with. \n\nThe considered tasks are relatively simple achieving 99.5% token-level accuracy. The evaluation beyond the synthetic datasets is fairly limited and it is unclear how well the method generalizes to novel images in clutter and occlusion. \n\nIn summary, the paper makes a number of observations that have been motivated in a number of prior works, but the contributions of this paper is not highlighted (e.g., over neural scene derendering). The main claim that higher-order relationships are being modeled is not apparent due to the simplicity of the scenes being considered. For example, the program blocks being considered are somewhat arbitrary and a comparison with a clustering based grouping approach should have been evaluated. The experimental evaluation is weak in several aspects. The generalization to real images is anecdotal with only two examples shown in the Figure 7. ", "title": "good problem; weak evaluation and motivation", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJgijGzFnX": {"type": "review", "replyto": "SyNPk2R9K7", "review": "This paper presents a system that infers programs describing 3D scenes composed of simple primitives. The system consists of three stages each of which is trained separately. First, the perceptual module extracts object masks and their attributes. The objects are then are split into several groups. Finally, each group is mapped to a corresponding DSL program using a sequence-to-sequence network similar to the ones typically employed in neural machine translation.\n\nPros:\n+ The paper is written clearly and easy to read.\n+ Visual program synthesis is very exciting and important direction both for image understanding and generation.\n+ The results on synthetic datasets are good. The authors also demonstrate the applicability of the approach to real-world data (albeit significantly constrained).\n+ I find it surprising that a seq2seq is good at producing an accurate program for a group of objects.\n+ Visual analogy making experiments are impressive.\n\nCons:\n- The proposed model requires rich annotation of training data since all the components of the systems are trained in a supervised fashion. It\u2019s not clear how to use the method on the in-the-wild data without such annotation.\n- Related to the previous point, even when it\u2019s possible to synthesize data, it is non-trivial to obtain the ground-truth grouping of objects. Judging by Table 2, it seems that the system breaks in absence of the grouping information.\n- The data used in the paper is quite simplistic (limited number of primitives located in a regular grid). I\u2019m wondering if there is a natural way to extend the approach to more complex settings. My guess is that the performance will drop significantly.\n\nNotes/questions:\n* Section 2, paragraph 1: The paper by [Ganin et al., 2018] presents both a system for reproducing an image as well as for sampling from a distribution; moreover, it presents experiments on 3D data (i.e., not limited to drawing).\n* Section 3.4, paragraph 2: I\u2019m not sure I understand the last sentence. How can we know that we successfully recovered the scene at test time? Could the authors elaborate on the stopping criterion for sampling?\n* Section 4.2, paragraph 2: Do I understand correctly that the main difference between the test set and the generalization set is the number of groups? (i.e., 2 vs 3). If so, it\u2019s a fairly limited demonstration of generalization capabilities of the system.\n* Section 4.2, paragraph 4: \u201cwe search top 3 proposals ...\u201d \u2013 How do we decide which one is better? Do we somehow have an access to the ground truth program at test time?\n* Could the authors explain the representation of a program more clearly? How are loops handled? How can one subtract/add programs in the analogy making experiment?\n\nOverall, I think it is a interesting paper and can be potentially accepted on the condition that the authors address my questions and concerns.", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}