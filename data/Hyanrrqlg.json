{"paper": {"title": "HFH: Homologically Functional Hashing for Compressing Deep Neural Networks", "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "authorids": ["shilei06@baidu.com", "fengshikun01@baidu.com", "zhuzhifan@baidu.com"], "summary": "", "abstract": "As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper presents some interesting and potentially useful ideas, but multiple reviewers point out that the main appeal of the paper's contributions would be in potential follow-up work and that the paper as-is does not present a compelling use case for the novel ideas. For that reason, the recommendation is to reject the paper. I would encourage the authors to reframe and improve the paper and extend it to cover the more interesting possible empirical investigations brought up in the discussion. Unfortunately, the paper is not sufficiently ground breaking to be a good fit for the workshop track."}, "review": {"r1ns_6X8g": {"type": "rebuttal", "replyto": "BJN9G3BBe", "comment": "Concern 1. The computation cost seems worse than HashedNets and is not discussed.\nReply: Thank you for the remind. We will add detailed computational time cost in the revised version. Particularly for example on CIFR-10 by a CNN, the inference time for a single image is about 37 ms, while it is about 45 ms for HFH, both by Torch on a single core Tesla K40m GPU. It should be noted that, there is still certain room for improvement on HFH implementation efficiency, for example by employing more light-weighted hashing functions.\n \n\nConcern 2. Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.\nReply: To achieve higher compression and faster inference, we can combine HFH and the pruning strategy seamlessly. We have found this is a good choice in practice.\n \n\nConcern 3. Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.\nReply: Having running multiple independent times in experiments, we found that the performance standard error is quite low especially on large datasets like ImageNet. We will report such results during revision.\n \n\nConcern 4. For completeness, please add U1 results to Table 1. And U4-G3 is listed twice with two different numbers.\nReply: The corresponding results will be provided. We are sorry that this is a typo, and the actual test error of U4-G3 is 1.32%. Thanks a lot for correction.\n \n\nConcern 5. Some sentences are not grammatically correct. Please improve the writing\nReply: The language expression will be improved under serious gramma checking and polishing. Thank you for the encouraging comments.", "title": "Answers to AnonReviewer3"}, "HkuvO6mIx": {"type": "rebuttal", "replyto": "rknpJiIBg", "comment": "Concern 1: Are you also counting the extra parameters for reconstruction network for the memory comparison?\nReply: Compared to the original network, there are significantly less parameters in the reconstruction network. For instance, the U4-G3 reconstruction network has merely 19 parameters, which are neglectable on memory consumption.\n \nConcern 2\uff1aPlease also show the impact on running time\u3001I am not convinced that this method will be lightweight.\nReply: We will specify this clearly in the revised version. Specifically for instance on CIFR-10 by a CNN, the inference time for a single image is about 37 ms, while it is about 45 ms for HFH, both by Torch on a single core Tesla K40m GPU. It should be clarified that, there is still certain room for improvement on HFH implementation efficiency, for example by employing more light-weighted hashing functions. Thanks for your remind.", "title": "Answers to AnonReviewer1"}, "BkiKsDRXl": {"type": "rebuttal", "replyto": "rkDrf01mx", "comment": "Your helpful comments are greatly appreciated.\n\nReply for Q1: On CIFR-10 by a CNN, the inference time for a single image is about 37 ms, while it is about 45 ms for HFH,  both by Torch on a single core Tesla K40m GPU. It should be noted that, there is still certain room for improvement on HFH implementation efficiency, for example by employing more light-weighted hashing functions. Even though, the additional time cost brought by HFH is acceptable compared to convolution operations. Last but not the least, this additional time cost is constant w.r.t. the compression ratio. We will make this clear in the revised version. Thank you.\n\nReply for Q2: The goal of HFH is to save memory under limited hardware budget, and the running time of current HFH implementation is just 20% lower than the original method. Furthermore, to achieve higher compression, we can combine HFH and the pruning work of Han et al seamlessly. HFH is also a good choice in practice. Taking DNNs with word embedding for example, 19G memory is required to save 5 million words with 1024 dimensions, which will further stand out in the personalization scenario, i.e., 190G memory is required to maintain user embeddings for 50 million users. In this case, a pure pruning method is hard to execute because it has to load the original memory to GPU, while HFH succeeds easily by just loading the compression space.", "title": "Answers to AnonReviewer2"}}}