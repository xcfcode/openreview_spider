{"paper": {"title": "Context Dependent Modulation of Activation Function", "authors": ["Long Sha", "Jonathan Schwarcz", "Pengyu Hong"], "authorids": ["longsha@brandeis.edu", "johnschwarcz@brandeis.edu", "hongpeng@brandeis.edu"], "summary": "We propose a modification to traditional Artificial Neural Networks motivated by the biology of neurons to enable the shape of the activation function to be context dependent.", "abstract": "We propose a modification to traditional Artificial Neural Networks (ANNs), which provides the ANNs with new aptitudes motivated by biological neurons.  Biological neurons work far beyond linearly summing up synaptic inputs and then transforming the integrated information.  A biological neuron change firing modes accordingly to peripheral factors (e.g., neuromodulators) as well as intrinsic ones.  Our modification connects a new type of ANN nodes, which mimic the function of biological neuromodulators and are termed modulators, to enable other traditional ANN nodes to adjust their activation sensitivities in run-time based on their input patterns.  In this manner, we enable the slope of the activation function to be context dependent.  This modification produces statistically significant improvements in comparison with traditional ANN nodes in the context of Convolutional Neural Networks and Long Short-Term Memory networks. ", "keywords": ["Artificial Neural Network", "Convolution Neural Network", "Long Short-Term Memory", "Activation Function", "Neuromodulation"]}, "meta": {"decision": "Reject", "comment": "The paper adds a new level of complexity to neural networks, by modulating activation functions of a layer as a function of the previous layer activations.  The method is evaluated on relatively simple vision and language tasks.\n\nThe idea is nice, but seems to be a special case of previously published work; and the results are not convincing.  Four of five reviewers agree that the work would benefit from: improving comparisons with existing approaches, but also improving its theoretical framework, in light of competing approaches."}, "review": {"r1xhUmtURX": {"type": "rebuttal", "replyto": "BJgfNNK92m", "comment": "I.1 We used a modulation to change the shape of the activation, it is still a single computation but the effect is multiplicative instead of additive and effects an entire layer rather than a single node. \nI.2 Our modulation was set on the convolution layer only before the activation layer. It adds a very small amount of parameters, we also did a \u2018lite\u2019 DenseNet version to compare our modification with fewer parameters.\nI.3 We used \u2018run-time\u2019 in the manuscript trying to express that our modulation method can take the input and use that information and change the shape of the activation function on the fly. \nII. we thank the reviewer for picking out minor errors, all suggestions taken.", "title": "Thank you very much for your helpful comments!"}, "rylOJXtUC7": {"type": "rebuttal", "replyto": "HJgd_ZWanX", "comment": "1. We did not compare performances with relevant works since we are generally not trying to compete against the existing tools (i.e. attention). Our modification focused on a different aspect of changing the slope of the activation and can be applied on top of the related works. We will explore other implementations of existing works in conjunction with modulator nodes in the future works. \n2. Regarding the CNN model performance, in the paper, we didn\u2019t report the best DenseNet as the baseline from the original work. After we tried more complexed model setup, we got the following new results:\n\nModel\t\t\t\t                               Top-1 Accuracy\n----------------------------------------------------------------------------\nDenseNet-161(k=48)\t\t                        93.79\nModulatedDenseNet-161(k=48)\t\t93.95\n\nWe also thank the reviewer for mentioning other recent works, we will explore the comparison in the future works.\n\f\n", "title": "Thank you very much for your helpful comments!"}, "ByemMfYIRX": {"type": "rebuttal", "replyto": "HJxlr2rg6Q", "comment": "1. The neuroscientific inspiration came simply from looking at what neurons were capable of, as opposed to a descriptive approach for why their capabilities may be useful for specific tasks. However, why would modulation benefit a supervised learning task is a completely valid and absolutely vital question. We did not find an easy location to address this question in the paper. However, for a brief explanation, supervised learning may benefit from the increased contrast in the amplitude of the signals propagating through the network. A sigmoidal modulator should theoretically learn to spatiotemporally inhibit signals, thereby increasing the relative gain of certain signals or pathways based. This context modulation is common in the visual system via reciprocal inhibition. Regarding \u2018Intrinsic Excitability\u2019, it is very true that a bias can capture it if node activation is envisioned as subthreshold voltage, however, if node activation is considered analogous to firing rate, once a neuron passes a threshold,  \u2018Intrinsic Excitability\u2019 has a multiplicative effect on the firing rate frequency.\n2. We thank the reviewer for suggesting the possible experimental setups that not included in this paper. We will strengthen the experiment section in the final paper and explore more in future works.\n", "title": "Thank you very much for your helpful comments!"}, "SJeiT-t8Cm": {"type": "rebuttal", "replyto": "H1l5sLRf6X", "comment": "I.1.  We thank the reviewer for pointing out the comparison of our modification with the attention mechanism. Since we stated in the paper that our modification focused on a different aspect which is the slope of the activation function, we didn\u2019t include this kind of comparison. In future works, we will explore the possibility of combining both methods.\nI.2.  From our experiments, the modulated vanilla network structure can outperform the counterpart by a small margin. After the epochs showed in the chart, the performance of the modulated network became almost flat. We tried to set the type of activation functions and optimization methods the same for comparing the performance of models in our work. Also, we can explore more combinations of setups in future works.\nII. we thank the reviewer for picking out minor errors, all suggestions taken.\n", "title": "Thank you very much for your helpful comments!"}, "Skg599BQRX": {"type": "rebuttal", "replyto": "SJgMQuLd6Q", "comment": "1. We thank the reviewer for pointing out the \u2018Network in Network\u2019 paper (Lin et. al. 2014), we will add the discussion with this work in our final version. In short, our approach applied the context modulation only before the activation layer, on the contrary, Lin et. al.\u2019s method was applied to every convolutional layer; also, the modulator weights were applied to all the feature maps providing a very easy to implement light weighted modification that was solely used to change the activation function slope.\n2. In vanilla LSTM, the input gate can control how much the input will affect the cell status, but our modification focuses on a different part which is adding a modulator to control the shape of the activation function.\n3. We focus on the context dynamic activation function which can have a side benefit of easing the gradient issue of other activation functions e.g. tanh in LSTM.\n3. And we will clear the discussion section to make our claim more clear. ", "title": "Thank you for your helpful comments!"}, "SJgMQuLd6Q": {"type": "review", "replyto": "rylWVnR5YQ", "review": "Paper summary:\n\nThis paper proposes a method to scale the activations of a layer of neurons in an ANN depending on the inputs to that layer. The scaling factor, called modulation, is computed using a separate weight matrix and activation function. It is multiplied with each neuron's activation before applying its non-linearity. The weight matrix of the modulator is learned alongside the other weights of the network by backpropagation. The authors evaluate this modulated neural unit in convolutional neural networks, densely connected CNNs and recurrent networks consisting of LSTM units. Reported improvements above the baselines are between 1% - 3%.\n\nPro:\n\n+ With some minor exceptions the paper is clearly written and comprehensible.\n+ Experiments seem to have been performed with due diligence.\n+ The proposed modulator is easy to implement and applicable to (almost) all network architectures.\n\nContra:\n\n- Lin et. al. (2014) proposed a network in network architecture. In this architecture the output of each neural unit is computed using a small neural network contained in it and thus arbitrary, input-dependent activation functions can be realized and learned by each neuron. The proposed neural modulation mechanism in the paper at hand is in fact a more restricted version of the network-in-network model and the authors should discuss the relationship of their proposal to this prior work.\n\n- When comparing the test accuracy of CNNs in Fig. 4 the result is questionable. If training of the vanilla CNN was stopped at its best validation loss (early stopping), the difference in accuracies would have been marginal. Also the choice of hyper-parameters may significantly affect the outcome of the comparison experiments. More experiments would be necessary to prove the advantage of this model over a wide range of hyper-parameters.\n\nMinor points:\n\n- It is unclear whether the modulator weights are shared along the depth of a CNN layer, i.e. between feature maps.\n\n- Page 9: \"Our modification enables a network to use previous activity to determine its current sensitivity to input [...]\" => A vanilla LSTM is already capable of doing that using its input gate.\n\n- Page 9: \"[...] the ability to adjust the slope of an Activation Function has an immediate benefit in making the back-propagation gradient dynamic.\" => In fact ReLUs do not suffer from the vanishing gradient problem. Furthermore DenseNets already provide a short-path for the gradient flow by introducing skip connections.\n\n- The discussion at the end adds little value and rather seems to be a motivation of the model than a discussion of the results.\n\nRating:\n\nMy main concern is that the proposed modulator is a version of the network in network model restricted to providing a scaling factor. Although the authors motivate this model biologically, I do not see sufficient empirical evidence to believe that it is advantageous over the full network in network model by Lin et. al. I would recommend to add a direct comparison to that model to a future version of this paper.\n", "title": "Restricted/simplified version of network in network by Lin et. al. without clear benefits", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1l5sLRf6X": {"type": "review", "replyto": "rylWVnR5YQ", "review": "Summary: \nThis paper introduces an architectural change for basic neurons in neural network. Assuming a \"neuron\" consists of a linear combination of the input, followed by a non-linear activation function, the idea is to multiply the output of the linear combination by a \"modulator\", prior to feeding it into the activation function. The modulator is itself a non-linear function of the input. Furthermore, in the paper's implementation, the modulators share weights across the same layer. The idea is demonstrated on basic vision and NLP tasks, showing improvements over the baselines. \n\nI - On the substance:\n1. Related concepts and biological inspirations\nThe idea is analogous to attention and gating mechanisms, as the authors point out, with the clear distinction that the modulation happens _before_ the activation function. It would have been interesting to experiment a combination of modulation and attention since they do not act on the same levels. \nAlso, the authors claim inspiration from the biological neurons, however, they do not elaborate in depth on the connections to the neuronal concepts mentioned in the introduction. \n\n2. The performance of the proposed approach\nIn the first experiment, the modulated CNN at 150 epochs seems to have comparable performance with the vanilla CNN at 60 (the latter CNN starts overfitting afterwards). Why not extending the learning curve to more epochs since the modulated CNN seems on a positive slope? \nThe other experiments show some improvements over the baselines, however more experiments are necessary for claiming generality. Especially, the baselines remain too simple and there are some well-known well-performing architectures, for both image and text processing, that the authors could compare to (cf winning architectures for imagenet for instance). They could also take these same architectures and augment them with the modulation proposed in the paper. \nFurthermore, an ablation study is clearly missing, what about different activation functions, combination with other optimization techniques etc.?\n\nII - On the form:\n1. the paper is sometimes unclear, even though the overall narrative is sound,\n2. wiggly red lines are still present in the caption of Figure 1 right.\n3. Figure 6 could be greatly simplified by putting its content in the form of a table, I don't find that the rectangles and forms bring much benefit here.\n4. Table 5 (should it not be Figure?): it is not fully clear what the lines represent and based on which input. \n5. some typos: \n - abstract: a biological neuron change[s]\n - abstract: accordingly to -> according to \n - introduction > paragraph 2 > line 11: Each target node multipl[i]es \n\nIII - Conclusion: \nThe idea is interesting and some of the experiments show nice results (eg. modulated densenet-lite outperforming densenet) but the overall paper needs further improvements. In particular, the writing needs to be reworked, the experiments to be consolidated, and the link to neuronal modulation to be further investigated. ", "title": "Interesting idea but the overall state of the paper needs improvements", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJxlr2rg6Q": {"type": "review", "replyto": "rylWVnR5YQ", "review": "Summary: this submission proposes a modification of neural network architectures that allows the modulation of activation functions of a given layer as a function of the activations in the previous layer. The author provide different version of their approach adapted to CNN, DenseNets and LSTM, and show it outperforms a vanilla version of these algorithms.\nEvaluation: In the classical context of supervised learning tasks investigated in this submission, it is unclear to me what could be the benefit of introducing such \u201cmodulators\u201d, as vanilla ANNs already have the capability of modulating the excitability of their neurons. Although the results show significant, but quite limited, improvements with respect to the chosen baseline, more extensive baseline comparisons are needed.\n\nDetails comments:\n1.\tUnderlying principles of the approach\nIt is unclear to me why the proposed approach should bring a significant improvement to the existing architectures. First, from a neuroscientific perspective, neuromodulators allow the brain to go through different states, including arousal, sleep, and different levels of stress. While it is relatively clear that state modulation has some benefits to a living system, it is less so for an ANN focused on a supervised learning task. Why should the state change instead of focusing on the optimal way to perform the task? If the authors want to use a neuroscientific argument, I would suggest to elaborate based on the precise context of the tasks they propose to solve. \nIn addition, as mentioned several times in the paper, neuromodulation is frequently associated to changes in cell excitability. While excitability is a concept that can be associated to multiple mechanisms, a simple way to model changes in excitability is to modify the threshold that must be reached by the membrane potential of a given neuron in order for the cell to fire. Such simple change in excitability can be easily implemented in ANNs architectures by affecting one afferent neuron in the previous layer to the modification of this firing threshold (simply adding a bias term). As a consequence, if there is any benefit to the proposed architecture, it is very likely to originate specifically from the multiplicative interactions used to implement modulation in this paper. However, approximation of such multiplicative interactions can also be implemented using multiple layers network equipped with non-linear activations. Overall, it would be good to discuss these aspects in great detail in the introduction and/or discussion of the paper, and possibly find a more convincing justification for the approach.\n\n2.\tWeak baseline comparison results\nIn the CNN experiments, modulated networks are only compared with a single vanilla counterpart equipped with ReLu. There are at least two obvious additional baseline comparison that would be useful: what if the Re-Lu activations are replaced with fixed sigmoids? And what if batch-normalization is switched on/off (I could not find whether it was used at all). Indeed it, we should exclude benefits that are simply due to the non-linearity of the sigmoid, and batch normalization also implements a form of modulation at training that may provide benefits equivalent to modulation (or on the contrary, batch norm could implement a modulation in the wrong way). It would be better to look at all possible combinations of these architecture choices.\nDue to lack of details in the paper and my personal lack of expertise in LSTMs, I will not comment on baselines for that part but I assume similar modifications can be done.\nOverall, given the weak improvements in performance, it is questionable whether this extra degree of complexity should be added to the architecture. Additionally, I could not find the precise description of the statistical tests performed. Ideally, the test, the number of samples, the exact p-value, and whether the method of correction for multiple comparison should be included each time a p-value is mentioned.\n\n\n\n\n\n \n", "title": "Idea with no very convincing benefits, baseline comparison to improve.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJgd_ZWanX": {"type": "review", "replyto": "rylWVnR5YQ", "review": "This paper proposes a scalar modulator adding to hidden nodes before an activation function. The authors claim that it controls the sensitivity of the hidden nodes by changing the slope of activation function. The modulator is combined with a simple CNN, DesneNet, and a LSTM model, and they provided the performance improvement over the classic models. \n\nThe paper is clear and easy to understand. The idea is interesting. However, the experimental results are not enough and convincing to justify it. \n\n1) The authors cited the relevant literature, but there is no comparison with any of these related works. \n\n2) Does this modulator actually help for CNN and LSTM architectures? and How? Recently, there are many advanced CNN and LSTM architectures. The experiments the authors showed were with only 2 layer CNNs and 1 layer LSTM. There should be at least some comparison with an architecture that contains more layers/units and performs well. There is a DenseNet comparison, but it seems to have an error. See 4) for more details.\n\n3) The authors mentioned that the modulator can be used as a complement to the attention and gate mechanisms. Indeed, they are very similar. However, the benefit is unclear. More experiments need to be demonstrated among the models with the proposed modulator, attention, and gates, especially learning behavior and performance differences. \n\n4) The comparison in Table 2 is not convincing. \n- The baseline is too simple. For instance on CIFAR10, a simple CNN architecture introduced much earlier (like LeNet5 or AlexNet) performs better than Vanilla CNNs or modulated CNNs.\n- DenseNet accuracy reported in Table 2 is different from to the original paper: DenseNet (Huang et al. 2017) CIFAR10 # parameters 1.0M, accuracy 93%, but in this paper 88.9%. Even the accuracy of modulated DenseNet is 90.2% which is still far from the original DenseNet.\nFurthermore, there are many variations of DenseNet recently e.g., SparsenNet: sparsified DenseNet with attention layer (Liu et al. 2018), # parameters 0.86M, accuracy 95.75%. Authors should check their experiments and related papers more carefully.\n\nSide note: page 4, Section 3.1 \"The vanilla DenseNet used the structure (40 in depth and 12 in growth-rate) reported in the original DenseNet paper (Iandola et al., 2014)\". This DenseNet structure is from Huang et al. 2017 not from Iandola et al. 2014.\n", "title": "Interesting, but no convincing results and analysis", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJgfNNK92m": {"type": "review", "replyto": "rylWVnR5YQ", "review": "The paper introduces a new twist to the activation of a particular neuron. They use a modulator which looks at the input and performs a matrix multiplication to produce a vector. That vector is then used to scale the original input before passing it through an activation function. Since this modulating scalar can look across neurons to apply a per-neuron scalar, it overcomes the problem that otherwise neurons cannot incorporate their relative activation within a layer. They apply this new addition to several different kinds of neural network architectures and several different applications and show that it can achieve better performance than some models with more parameters.\n\n\nStrengths:\n- This is a simple, easy-to-implement idea that could easily be incorporated into existing models and frameworks.\n- As the authors state, adding more width to a vanilla layer stops increasing performance at a certain point. Adding more complex connections to a given layer, like this, is a good way forward to increase capacity of layers.\n- They achieve better performance than existing baselines in a wide variety of applications.\n- The reasons this should perform better are intuitive and the introduction is well written.\n\nWeaknesses:\n- After identifying the problem with just summing inputs to a neuron, they evaluate the modulator value by just summing inputs in a layer. So while doing it twice computes a more complicated function, it is still a fundamentally simple computation.\n- It is not clear from reading this whether the modulator weights are tied to the normal layer weights or not. The modulator nets have more parameters than their counterparts, so they would have to be separate, I imagine.\n- The authors repeatedly emphasize that this is incorporating \"run-time\" information into the activation. This is true only in the sense that feedforward nets compute their output from their input, by definition at run-time. This information is no different from the tradition input to a network in any other regard, though.\n- The p-values in the experiment section add no value to the conclusions drawn there and are not convincing.\n\nSuggested Revisions:\n- In the abstract: \"A biological neuron change[s]\"\n- The conclusion is too long and adds little to the paper\n\n", "title": "modulating scalar applied per-neuron", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}