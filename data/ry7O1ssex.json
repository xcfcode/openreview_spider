{"paper": {"title": "Generative Adversarial Networks as Variational Training of Energy Based Models", "authors": ["Shuangfei Zhai", "Yu Cheng", "Rogerio Feris", "Zhongfei Zhang"], "authorids": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "rsferis@us.ibm.com", "zhongfei@cs.binghamton.edu"], "summary": "", "abstract": "In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\\mathbf{x})$ is approximated by a variational distribution $q(\\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\\mathbf{x})$, $q(\\mathbf{x})$ is updated to maximize the lower bound; $p(\\mathbf{x})$ is then updated one step with samples drawn from $q(\\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\\mathbf{x})$ corresponds to the discriminator and $q(\\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper is timely since it addresses the connections between energy-based models, GANS, and the general space of generative models. The two principal concerns about the paper are: lack of clarity and coherence in the paper; inability to effectively judge the effectiveness of the method. The responses of the authors address these concern to some extent, but these concerns remain. The paper will have much higher-impact after further introspection, but at present, the paper is not yet ready for acceptance at the conference."}, "review": {"BkbyUIb_l": {"type": "rebuttal", "replyto": "ByYqfXmwx", "comment": "Hi,\n\nThanks a lot for the comments.  The order of min-max of GAN is an easily overlooked problem, which we try to shed light on from the view of EBMs. This difference indeed causes VGAN and GAN to have different training dynamics in theory. In practice, we are not aware of any work that suggests increasing the number of updates for D improves the stability; on the other hand, there has been efforts in \"slowing down\" the updates of D that has been shown useful (e.g., label smoothing in Improved GAN). We have also verified in our experiments that increasing the updates of G benefits both GAN and VGAN, given an effective  entropy approximation (otherwise G will be encouraged to collapse even more). These empirical evidences lead us to the belief that in practice GAN behaves very similar to VGAN, both of which are consistent with the principle of variational training of an EBM. We believe it is thus easier to understand, diagnose and evaluate GAN from the perspective of an EBM, which may also lead to new directions in this area.\n\nThank,\nShuangfei", "title": "Re: review"}, "SkDBz_1Lg": {"type": "rebuttal", "replyto": "ry7O1ssex", "comment": "Thanks for the comments. \n\nAbout the distance metric: unsupervised learning does not necessarily need an accurate distance metric, it just happens that most of the commonly used methods do assume a simple distance metric (such Euclidean distance), such as K-means, Gaussian Mixture Models, PCA, Autoencoders, VAE and etc.. These models either directly use a predefined distance metric in the input space, or assume equivalent distributions (such as Gaussian distributions) in the input space, which directly limits their usefulness for high dimensional inputs where there exists no reasonable simple distance metric.\n\nWe do agree that we could do a better job at elaborating the relationship with existing works, we will provide an updated version to clarify the unclearness.\n", "title": "Re: AnonReviewer2"}, "S1CFgd1Ug": {"type": "rebuttal", "replyto": "r1ir0mX4e", "comment": "Thanks for the comments. We agree that including more related results would be useful for calibrating the contribution of the paper, we will provide an updated version later. However, we do not agree with the reviewer's point about \"Although the relationship between energy-based models and GANs is abundantly clear in the literature\".  We argue that our paper provide novel views to this connection, even considering all the parallel submissions to this conference. For example, when treating GAN as minimizing a variational lower bound of an NLL, it is critical to update G k steps per D update to tighten the lower bound, which is in contradiction with GAN's proposal. More recent findings, such as [Towards Principled Methods for Training\nGenerative Adversarial Networks, Arjovsky et al. 2016], also supports our argument that training D multiple steps according to the GAN theory leads to worse results. Although we do not think that our solution is perfect (simply updating G k steps), this remains an important issue to be solved to fully understand both GAN and the variational training of EBMs. Our other contributions that are non-existing in the previous literature also include the discussion of proper choices of energy functions, which is extremely important for designing and training new families of EBMs.", "title": "Re: AnonReviewer3"}, "HJQGveX4x": {"type": "rebuttal", "replyto": "S1xk8af4x", "comment": "Thanks for the comments.\n\nThe goal of this paper is to provide a principled view to guide the training of GAN-like models, which we interpret as minimizing a variational lower bound of the negative log likelihood of an EBM. From this view, we are able to identify several important issues of training GANs and EMBs that are not revealed in the previous literature. For example, we identified that minimizing a lower bound is a major source of instability, as when the bound is not tight, there is no guarantee that the true NLL is minimized. Therefore, it is important to switch the order of the G and D updates to update G K steps to tighten the bound then update D one step, instead of the other way around as proposed in the original GAN paper (see Table 1 and Figure 2). We also identified the importance of bounded energy parameterization in light of this training strategy, which is important but often ignored in related EBM literature. \n\nWe did not include the (failed) results trained without the entropy approximation, as the importance of diversity promoting regularizations is well recognized in recent GAN related papers (e.g., batch discrimination in improved gan). Moreover, the related work EBGAN that the reviewer pointed out actually does have a similar term as well, which they call the \"repelling regularizer\" (section 2.4 of Zhao et al.). We argue that observing the diversity of the generated samples is sufficient to verify the effectiveness of the proposed approximation, although we make no claim that our proposal is by any means an accurate approximation of the true entropy.\n\nThe motivation of VCD comes exactly from the difficulty of accurate approximation of the entropy term. VCD initializes p_g(x) as a distribution close to p_data(x), which essential bypasses the need of directly approximating and maximizing its entropy. The fact that this works well is not surprising either due to its similarity to contrastive divergence and the vast success CD achieves in training EBMs. Our way of evaluating the quality of generated samples with the proposed semi-supervised learning benchmark is also novel, and an arguably more objective metric those used previously such as visual quality, parzen window-based likelihood estimation and inception scores. We argue that the improvements gained by using only generated samples as data augmentation are nontrivial, and demonstrate the usefulness of the generative model in an objective way. We will update our paper with related semi-supervised learning methods in a later version for a more complete comparison.\n\nThanks,\nShuangfei", "title": "Re: AnonReviewer1"}, "SJqgPJQ4g": {"type": "rebuttal", "replyto": "HkKS8TGVg", "comment": "Thanks for the questions.\n\nWe believe Equation 9 is a reasonable approximation of the entropy in the sense that it encourages diversified samples from the generator. And no it is not a degenerate solution. To see this, when updating G, the objective subject to maximization (shown in algorithm 1) can be rewritten as: \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} KL(\\sigma_j^i \\| \\bar{\\sigma_j}), where \\sigma_j^i is the j-th sigmoid activation for the i-th generated sample, and \\bar{\\sigma_j} is the averaged activation over the generated mini-batch for the j-th sigmoid. Maximizing the this KL divergence encourages G to generate samples that cause each sigmoid unit to be either 0 or 1 with probability 0.5. As each W_j is initialized randomly, each generated sample tens to randomly fall into one of the 2^K optimal sigmoid activation patterns. Note that this step does not affect W_j, which is part of D. When updating D, the same KL divergence is maximized on a training data mini-batch (see algorithm 1), where the case when W_j are all zero yields a minimum KL divergence (least optimal solution) and will never occur with a correct optimization procedure.\n\nThe evidence that this does not happen is obvious. When all W_j is set to all zero vectors, the model crashes and no learning can proceed: the EBM simply encodes a uniform distribution over the input space. The fact that we are able to generate quality samples directly contradicts this hypothesis. ", "title": "Re: Questions"}, "HkKS8TGVg": {"type": "review", "replyto": "ry7O1ssex", "review": "Regarding the entropy approximation in equation 9, do you have any reason to think this is a reasonable approximation? You state that there is no theoretical guarantee \"recovers the true entropy\", but do you have any empirical evidence that this is a reasonable approximation in principle, or that the gradient is a reasonable approximation of the entropy gradient?\n\nYou state that maximizing the entropy approximation in equation 9 \"serves the same purpose of encouraging the generated samples to be diverse\", but it seems like a degenerate solution where all the W_j weights are zero would also maximize equation 9, but not at all encourage diversity. First, do you agree? Second, do you have evidence that this does not happen?\nThis paper presents a bridging of energy-based models and GANs, where -- starting from the energy-based formalism -- they derive an additional entropy term that is not present in the original GAN formulation and is also absent in the EBGAN model of Zhao et al (2016, cited work). The relation between GANs and energy-based models was, as far as I know, first described in Kim and Bengio (2016, cited work) who also introduced the entropy regularization. It is also discussed in another ICLR submission (Dai et al. \"Calibrating Energy-based Generative Adversarial Networks\"). There are two novel contribution of this paper: (1) VGAN: the introduction of a novel entropy approximation; (2) VCD: variational contrastive divergence is introduced as a  novel learning algorithm (however, in fact, a different model). The specific motivation for this second contribution is not particularly clear. \n\nThe two contributions offered by the authors are quite interesting and are well motivated in the sense that the address the important problem of avoiding dealing with the generally intractable entropy term. However, unfortunately the authors present no results directly supporting either contribution. For example, it is not at all clear what role, if any, the entropy approximation plays in the samples generated from the VGAN model. Especially in the light of the impressive samples from the EBGAN model that has no corresponding term. The results provided to support the VCD algorithm, come in the form of a comparison of samples and reconstructions. But the samples provided here strike me as slightly less impressive compared to either the VGAN results or the SOTA in the literature - this is, of course, difficult to evaluate. \n\nThe results the authors do present include qualitative results in the form of reasonably compelling samples from the model trained on CIFAR-10, MNIST and SVHN datasets. They also present quantitative results int he form of semi-supervised learning tasks on the MNIST and SVHN. However these\nquantitative results are not particularly compelling as they show limited improvement over baselines. Also, there is no reference to the many\nexisting semi-supervised results on these datasets. \n\nSummary: The authors identify an important problem and offer two novel and intriguing solutions. Unfortunately the results to not sufficiently support either\ncontribution.\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1xk8af4x": {"type": "review", "replyto": "ry7O1ssex", "review": "Regarding the entropy approximation in equation 9, do you have any reason to think this is a reasonable approximation? You state that there is no theoretical guarantee \"recovers the true entropy\", but do you have any empirical evidence that this is a reasonable approximation in principle, or that the gradient is a reasonable approximation of the entropy gradient?\n\nYou state that maximizing the entropy approximation in equation 9 \"serves the same purpose of encouraging the generated samples to be diverse\", but it seems like a degenerate solution where all the W_j weights are zero would also maximize equation 9, but not at all encourage diversity. First, do you agree? Second, do you have evidence that this does not happen?\nThis paper presents a bridging of energy-based models and GANs, where -- starting from the energy-based formalism -- they derive an additional entropy term that is not present in the original GAN formulation and is also absent in the EBGAN model of Zhao et al (2016, cited work). The relation between GANs and energy-based models was, as far as I know, first described in Kim and Bengio (2016, cited work) who also introduced the entropy regularization. It is also discussed in another ICLR submission (Dai et al. \"Calibrating Energy-based Generative Adversarial Networks\"). There are two novel contribution of this paper: (1) VGAN: the introduction of a novel entropy approximation; (2) VCD: variational contrastive divergence is introduced as a  novel learning algorithm (however, in fact, a different model). The specific motivation for this second contribution is not particularly clear. \n\nThe two contributions offered by the authors are quite interesting and are well motivated in the sense that the address the important problem of avoiding dealing with the generally intractable entropy term. However, unfortunately the authors present no results directly supporting either contribution. For example, it is not at all clear what role, if any, the entropy approximation plays in the samples generated from the VGAN model. Especially in the light of the impressive samples from the EBGAN model that has no corresponding term. The results provided to support the VCD algorithm, come in the form of a comparison of samples and reconstructions. But the samples provided here strike me as slightly less impressive compared to either the VGAN results or the SOTA in the literature - this is, of course, difficult to evaluate. \n\nThe results the authors do present include qualitative results in the form of reasonably compelling samples from the model trained on CIFAR-10, MNIST and SVHN datasets. They also present quantitative results int he form of semi-supervised learning tasks on the MNIST and SVHN. However these\nquantitative results are not particularly compelling as they show limited improvement over baselines. Also, there is no reference to the many\nexisting semi-supervised results on these datasets. \n\nSummary: The authors identify an important problem and offer two novel and intriguing solutions. Unfortunately the results to not sufficiently support either\ncontribution.\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1r8DQxVe": {"type": "rebuttal", "replyto": "ryB6CWyXg", "comment": "Hi, \n\nWe did not record multiple runs in our experiments, but we experienced very similar behaviors across different random initializations. You are also welcome to try out our code, which is available through the link given in the paper, and reproduce our experiments.\n\nThanks,\nShuangfei", "title": "Re: Learning Curve"}, "HkftXQxVx": {"type": "rebuttal", "replyto": "HJyIpR1Xl", "comment": "Hi, Thanks for the comments!\n\n1. The results in Figure 2 are trained without the transition operator, which demonstrates the importance of using a bounded multi-modal energy formulation. We did not try GAN with the transition operator, mainly because we are interested in a broader family of energy based models, which is not specific to GAN. In our framework, this amounts to directly using Equation (7) as the objective, which is actually slightly different from GAN (in the second term of the objective). \n\n2. H tilde is defined in Equation (9) and the following paragraph, which is the same as used in Algorithm 1. It is a differentiable function of either the discriminator or generator parameters, thus can be optimized by gradient descent.", "title": "Re: Questions"}, "HJyIpR1Xl": {"type": "review", "replyto": "ry7O1ssex", "review": "1. You use a transition operator to serve as a generator, and it appears that this choice contributes significantly to the results. To understand the role of the transition operator better, have you tried either training your model without transition operator, or training a regular GAN with transition operator?\n\n2. What is H tilde mentioned in algorithm 2? Is it a parametrized model? How do you optimize it?This paper presents an adversarial training formulation for energy models. Although the relationship between energy-based models and GANs is abundantly clear in the literature, the contributions of this paper seems to be a multimodal energy estimate, and training a transition operator instead of a sampler. The core technical contribution of the paper is not clear. The only two quantitative results in the paper demonstrate that the proposed method learns better features than a traditional GAN, and that the proposed method does a better job in SSL than the authors' chosen baseline. In either case, no comparison to existing literature is made, even though GANs have been used for SSL previously (https://arxiv.org/abs/1606.03498). The qualitative results (visual comparison of quality of samples) is inconclusive.\n\nThe lack of comparison to existing literature makes this paper a clear reject. To improve this paper, the authors need to make the core contribution of the paper much better, and situate the paper very clearly wrt existing literature. The motivation for the development of the model should be clearer. Further, experiments need to compare with existing literature.", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1ir0mX4e": {"type": "review", "replyto": "ry7O1ssex", "review": "1. You use a transition operator to serve as a generator, and it appears that this choice contributes significantly to the results. To understand the role of the transition operator better, have you tried either training your model without transition operator, or training a regular GAN with transition operator?\n\n2. What is H tilde mentioned in algorithm 2? Is it a parametrized model? How do you optimize it?This paper presents an adversarial training formulation for energy models. Although the relationship between energy-based models and GANs is abundantly clear in the literature, the contributions of this paper seems to be a multimodal energy estimate, and training a transition operator instead of a sampler. The core technical contribution of the paper is not clear. The only two quantitative results in the paper demonstrate that the proposed method learns better features than a traditional GAN, and that the proposed method does a better job in SSL than the authors' chosen baseline. In either case, no comparison to existing literature is made, even though GANs have been used for SSL previously (https://arxiv.org/abs/1606.03498). The qualitative results (visual comparison of quality of samples) is inconclusive.\n\nThe lack of comparison to existing literature makes this paper a clear reject. To improve this paper, the authors need to make the core contribution of the paper much better, and situate the paper very clearly wrt existing literature. The motivation for the development of the model should be clearer. Further, experiments need to compare with existing literature.", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryB6CWyXg": {"type": "review", "replyto": "ry7O1ssex", "review": "Do you have learning the curves from multiple runs? That could help comparing the stability of the models.The paper drives a variant of GAN's min max game optimizations from EBM's NLL minimization and propose a new generative model from this derivation.\n\nIn introduction, could you elaborate why accurate distance metric is needed for unsupervised learning.\n\nIt's a bit hard to read the paper as it jumps from points to points without clear connection and laying down the background.  The introduction, refers to many different concepts without any clear connection. And other sections as well is very incomprehensible even if one is familiar with the concepts.\n\nThere has been lots of interests in similar area recently, Authors do cite some of them in introduction and related work but the relationship and comparison are missing.\n\nIn the results, if it's comparison of the quality of samples it has to include other recent works and compare those as well. And if it's semi-supervised learning, again, the numbers should be compared to other works.\n\nIn summary, unfortunately the paper is very clear and cumbersome to read which makes it hard to judge it fairly. I strongly suggest re-write of the paper in more coherent matter to make it easier to read. And also extend the experiments with more comparisons with other works.", "title": "Learning curve", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S15LA5LVe": {"type": "review", "replyto": "ry7O1ssex", "review": "Do you have learning the curves from multiple runs? That could help comparing the stability of the models.The paper drives a variant of GAN's min max game optimizations from EBM's NLL minimization and propose a new generative model from this derivation.\n\nIn introduction, could you elaborate why accurate distance metric is needed for unsupervised learning.\n\nIt's a bit hard to read the paper as it jumps from points to points without clear connection and laying down the background.  The introduction, refers to many different concepts without any clear connection. And other sections as well is very incomprehensible even if one is familiar with the concepts.\n\nThere has been lots of interests in similar area recently, Authors do cite some of them in introduction and related work but the relationship and comparison are missing.\n\nIn the results, if it's comparison of the quality of samples it has to include other recent works and compare those as well. And if it's semi-supervised learning, again, the numbers should be compared to other works.\n\nIn summary, unfortunately the paper is very clear and cumbersome to read which makes it hard to judge it fairly. I strongly suggest re-write of the paper in more coherent matter to make it easier to read. And also extend the experiments with more comparisons with other works.", "title": "Learning curve", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkPoNs4zx": {"type": "rebuttal", "replyto": "BJQQl9Vfe", "comment": "Hi Matt, \n\nThanks for the comments! \n\nIt is true that for an RBM with binary visible units the free energy only takes on a finite number of values. However, each of the values can be infinitely small (or large) by varying the parameters of the RBM. For example, given an arbitrary input $\\mathbf{x}$, the energy $E(\\mathbf{x})$ can grow to -Inf by letting each \\mathbf{b}_{h,j} grow to +Inf. And you can do the same thing to an RBM with real-valued inputs to verify that it's unbounded as well. This is bad for the variational objective in Equation (5), as it minimizes the energy of the data distribution w.r.t. the parameters of the EBM, which is unbounded from below by design. Doing so will eventually lead the model to assign infinitely low energy to both samples from the data distribution and samples from the variational distribution, which we have experienced in our experiments. \n\nHope this clarifies your questions,\nShuangfei", "title": "Respond to Unboundedness of RBM free energy"}, "BJQQl9Vfe": {"type": "rebuttal", "replyto": "ry7O1ssex", "comment": "In section 4 when comparing VGANs to GANs, in the 'parameterization of energy' point you claim\n\n> An RBM has free energy $E(\\mathbf{x}) = -\\mathbf{b}_{\\mathbf{v}} - \\sum_{j=1}^K \\log\\left(1 + e^{\\mathbf{W}^T_j\\mathbf{x} + \\mathbf{b}_{h,j}\\right)$ which is unbounded.\n\nIt is a bit unclear to me what you mean here. The expression you give is for the free energy of RBM with binary hidden and visible units i.e. $x_i \\in \\lbrace 0,\\,1 \\rbrace$. As a function of a binary vector $\\mathbf{x}$ the energy $E$ is bounded (assuming finite parameters) as it can only take on a finite number of values. For a real-valued vector $\\mathbf{x}$ the energy quoted would be unbounded but as this is defined as the free energy for a binary state this is not a very interpretable statement. It would seem more natural to use the free energy of a RBM with Gaussian visible units, which is defined on a real valued vector $\\mathbf{x}$ and due to the quadratic term in $\\mathbf{x}$ is bounded from below. Or am I missing something here?", "title": "Unboundedness of RBM free energy"}}}