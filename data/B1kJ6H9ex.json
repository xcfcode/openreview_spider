{"paper": {"title": "Combining policy gradient and Q-learning", "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"], "summary": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "keywords": ["Deep learning", "Reinforcement Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "Reviewers agree that this a high-quality and interesting paper exploring important connections between widely used RL algorithms. It has the potential to be an impactful paper, with the most positive comment noting that it \"will likely influence a broad swath of RL\". \n \n Pros:\n - The main concepts of the paper came through clearly, and all reviewers felt the idea was interesting and novel.\n - The empirical part of the paper was convincing and \"empirical section is very well explained\" \n \n Cons:\n - There and some concerns about the writing and notation. None of these were too critical though, and the authors responded \n - Reviewers asked for some more comparisons with alternative formulations."}, "review": {"rk0INZ3Ug": {"type": "rebuttal", "replyto": "H1BxzUzVl", "comment": "Thankyou for your detailed and insightful comments.                             \n                                                                                \n- \\tilde Q is always to be interpreted as an estimate of the Q-values, eq. 5    \n  holds exactly because it is how we define \\tilde Q, as a function of the      \n  policy. We are basically saying we are going to use the policy as an estimate \n  of the Q-values.                                                              \n- We feel section 3.2 is important to bridge the gap between the tabular        \n  case, where we can show that \\tilde Q is equal to Q exactly at the fixed      \n  point, to the general case where we can't show that but at least we can say   \n  that the error between the two is minimized in an l2 sense, so that \\tilde Q  \n  is a valid estimate of the Q-values still.                                    \n- We have made the connections and differences between dueling and our method   \n  more explicit in the prior work section. Yes, the Q-learning variant we use in\n  the examples is very similar to dueling, we made that more explicit. We added \n  the interpretation of PGQ as a combination of expected SARSA and Q-learning to\n  sect 4.1.                                                                     \n- We have made an effort to make sect 3.3 clearer.                              \n                                                                                \n                                                                                \nMinor:                                                                          \n- Updated the expectation to depend on r.                                       \n- Clarified the definition of the Boltzmann policy.                               \n- Stated in sect 4.3 that it is only for the tabular case.                             \n- The grid world example has been updated (there was a bug where the learning   \n  rates were too small originally) and they now converge to the same policy,    \n  in the original draft they all converged to same policy performance given     \n  enough time (roughly double the iterations was sufficient).                   \n                                                                                \nTypos:                                                                          \nAll fixed! Thankyou!                                                            \n", "title": "Interesting links between policy-based and value-based methods"}, "SkY-VW2Ix": {"type": "rebuttal", "replyto": "ry3QEnzVx", "comment": "Thankyou for your comments and suggestions.                                                     \n                                                                                \n- We have made more explicit the differences and similarities between our method\n  and the dueling architecture in the prior work section. You are correct that  \n  policy gradient methods can be interpreted as a generalization of the dueling \n  architecture combined with a SARSA style update.                              \n                                                                                \n- In that equation we're saying to consider the euclidean projection problem of \n  the log-\\pi s onto a set of values under some arbitrary measure, the KKT      \n  conditions of that problem is the next equation (along with the \\pi s being   \n  probability measures). If we compare that equation to eq. 3 we see that they  \n  match up if the measure is the same and q = Q, so we can interpret eq. 3 as a \n  projection. This is not the same as solving the optimization problem in eq. 7 \n  with the measure \\pi and Q^\\pi, since they are both  \n  functions of \\pi and thus we would need to consider how changing \\pi affects  \n  them which would change the KKT conditions. However we are not   \n  claiming to solve that problem, which is more complicated than a simple       \n  projection.                                                           \n                                                                                \n- The Q-learning example we compare to in the results is using the same network \n  and parameterizing the architecture as in eq. 10, and so provides a point of\n  comparison between PGQ and the closest pure Q-learning method, that naturally\n  includes dueling.          \n                                                                                \n", "title": "Review"}, "B1WX7Z28g": {"type": "rebuttal", "replyto": "Hk2oimlSe", "comment": "Thankyou for your comments. You are totally correct that the use of the stationary (i.e. non-discounted) policy was glossed over originally. We have updated the paper to make the distinction more explicit.                                \n", "title": "An excellent paper pointing out connections between existing algorithms, which also leads to new algorithms with excellent results"}, "H18Az-28e": {"type": "rebuttal", "replyto": "B1kJ6H9ex", "comment": "Thankyou to the three reviewers, and apologies for the delay in responding. We have updated the paper in response to your comments. Further responses below.", "title": "Thankyou to the reviewers"}, "BkzL3Xxre": {"type": "review", "replyto": "B1kJ6H9ex", "review": "No questionsThis is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done.\nOne minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. \nOverall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.", "title": "No questions", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Hk2oimlSe": {"type": "review", "replyto": "B1kJ6H9ex", "review": "No questionsThis is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done.\nOne minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. \nOverall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.", "title": "No questions", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rkfWLUeEl": {"type": "rebuttal", "replyto": "HkAmxZp7l", "comment": "Very good point, I had not considered that. I will have to fix it in the paper, thanks for bringing it to our attention.", "title": "Discounted weighting of states?"}, "BJQm4i9Ql": {"type": "rebuttal", "replyto": "ByPmaZc7g", "comment": "Yes, that should be made clear in the paper, I will fix that. In practice the online algorithm just 'samples' from the distribution of states encountered under the policy, and applies SGD. Are you suggesting that some correction should be applied? The Q-learning update is from a replay buffer, so the measure is definitely 'wrong' in that case. The value of gamma is the same as the A3C paper, which is 0.99.", "title": "Discounted weighting of states?"}, "ByLiWic7e": {"type": "rebuttal", "replyto": "Sy3n-CK7g", "comment": "1. I think that reducing alpha could definitely improve performance in testing, we haven't tried that yet though since we wanted to be as close as possible to the A3C paper, which doesn't change alpha in the testing phase.\n\nI'm not sure why people don't train a using a weighting averaging of the target Q-values, possibly it's just simpler to do the easy thing and that works well enough in practice. There are counter-examples (which I'm sure you're aware of, but just posting this here in case others are interested) where training a greedy policy but testing with some noise can arbitrarily hurt performance, e.g. the cliff walking example here: https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html. Your suggestion would alleviate that, so it's something to think about in the future. \n\n\n4. Yes, that's basically my understanding too. The only slight difference is that in the original dueling paper the measure that normalizes the quasi-advantage function is fixed, where in this paper (and policy gradient in general) it is always equal to the policy and so changes at each iteration (i.e. closer to the real advantage function).", "title": "A few more questions"}, "HyXcV8t7l": {"type": "rebuttal", "replyto": "rkepRFu7g", "comment": "All we are saying there is that for a small (non-zero) choice of alpha there is a fixed point of the entropy regularized policy gradient update, and that fixed point policy induces Q-values with a 'small' Bellman error (small in the sense that it converges to zero as alpha converges to zero). If the entropy penalty is equal to zero, then the set of fixed points is the set of optimal policies (which are unbounded in log-space and so might not technically be considered fixed-points), for which we have the optimal Q-values and therefore zero Bellman error.", "title": "Page 6, last paragraph (a small doubt)"}, "Sy5WQ8K7g": {"type": "rebuttal", "replyto": "S15GTXPme", "comment": "Exactly, thanks!", "title": "From eq.3 to eq.4"}, "SyuCMUK7e": {"type": "rebuttal", "replyto": "BJZ-L1Pmg", "comment": "1. Yes, alpha is fixed throughout and used in testing as well. We did not try the greedy policy, but it is common to use a stochastic policy in testing to prevent the agent getting trapped in a cycle of actions (the same is done in the DQN atari paper).\n2. In the grid world the critic is the 1-step TD-error from a learned value function. In Atari it is the n-step return with a bootstrap, exactly as in A3C.\n3. We are freezing the target network for around 5000 steps, but not doing clipping, no particular reason other than that was the simplest to implement.", "title": "A few more questions"}, "rkepRFu7g": {"type": "rebuttal", "replyto": "B1kJ6H9ex", "comment": "The authors write at the page 6, last paragraph \"Under standard policy gradient the bellman residual will be small, then it follows that adding a term that reduces that error should not make much difference at the fixed point.\" \n\nThis point is not clear. Let's consider the standard policy gradient without the entropy-bonus. In this case, I was wondering if there is a way to determine the fixed-policy? I did the same maths inspired by the paper but I did not reach to any concluding result.", "title": "Page 6, last paragraph (a small doubt)"}, "Hyol_mPmx": {"type": "rebuttal", "replyto": "HkUZch8ml", "comment": "Thank you very much for the clarification.\n\nI have a one related question. Can we show that if we add equation (4) over actions, the result would be one?\n\nThank you.", "title": "From eq.3 to eq.4"}, "HkUZch8ml": {"type": "rebuttal", "replyto": "HJmFbgLml", "comment": "Keeping the same notation I used above, a fixed point is one where we can no longer move in the direction of \\nabla f without violating one of the constraints g_s(pi) = 1, this is equivalent to finding a point x where \\nabla f(x) is in the span of the vectors \\nabla g_s(x), i.e., finding a point x such that \\nabla f(x) = \\sum_s \\lambda_s \\nabla g_s(x) where the \\lambda_s are real numbers and Lagrange multipliers and the values must ensure that g_s(x) = 1 for every s. From this we can derive equation 3. \n\nThis is a standard result, see here for more information: https://en.wikipedia.org/wiki/Lagrange_multiplier#Multiple_constraints\n", "title": "From eq.3 to eq. 4"}, "HJmFbgLml": {"type": "rebuttal", "replyto": "ryR4Pl1Qx", "comment": "I am still not convinced with the above answer. I think the way that you reach to equation (3) is by finding the fixed-point of the regularized policy and then applying the baseline trick. \n\nWe use the Lagrange multiplier when we try to solve a constrained minimization problem. However, you have not shown that how finding the fixed point is equivalent to solving a constrained minimization problem for the tabular case. We cannot use the result of section 3.2 because then the argument will be circular.\n\nPlease clarify.\n\nThank you,", "title": "From eq.3 to eq. 4"}, "Byq7XNN7x": {"type": "rebuttal", "replyto": "ryJNg-X7l", "comment": "We're saying to consider the euclidean projection problem of the log-\\pi s onto a set of values under some arbitrary measure, the KKT conditions of that problem is the next equation (along with the \\pi s being probability measures). If we compare that equation to eq. 3 we see that they match up if the measure is the same and q = Q, so we can interpret eq. 3 as a projection. This is not the same as solving the optimization problem in eq. 7 with the measure \\mu_\\pi (let's call it that) and Q^\\pi, since they are both functions of \\pi and thus we would need to consider how changing \\pi affects them, as you say, which would change the KKT conditions. However we are not claiming to solve that problem, which is more complicated than a simple projection. If your objection is that we can't retroactively substitute \\mu_\\pi and Q^\\pi back in once we have the KKT conditions then that would be true if we were claiming to solve the more complicated problem, but we're only trying to interpret eq. 3 as a projection, which it is for whichever measure appears in eq. 3.\n", "title": "Independence assumption"}, "H11PK0fQx": {"type": "rebuttal", "replyto": "BJLW1aJ7e", "comment": "I just re-read it and I agree that section is unclear at the moment, we will rewrite it. Thanks for the feedback!", "title": "From eq. 3 to eq. 4"}, "ryR4Pl1Qx": {"type": "rebuttal", "replyto": "rJcswr0zg", "comment": "One way to think of c is as a Lagrange dual variable, there exists a c such that\nwhen you solve (3) you get a probability distribution in the end. So we can     \nconsider the pi to be unconstrained (so you get the delta function) but the     \nchoice of c will enforce that sum(pi) = 1.                                      \n                                                                                \nMore specifically, we have a gradient we want to follow upwards (namely the sum \nof the gradient of the cost function and the gradient of the entropy), let's    \ndenote this as \\nabla f, but we also want to ensure that sum(pi) = 1 for each   \nstate, let's call this  g_s(pi) = 1 for each s. Finding a critical point that   \nsatisfies the constraints is equivalent to finding a point pi such that \\nabla  \nf(pi) = \\sum_s \\lambda_s \\nabla g_s(pi). From this you can derive eq. 3 by      \nrolling all the per-state constants into a single constant called c, and        \nthere must be a choice of c that ensures the constraint holds.             \n", "title": "From eq. 3 to eq. 4"}, "rJcswr0zg": {"type": "review", "replyto": "B1kJ6H9ex", "review": "Hi,\n\nI'm having some trouble following the reasoning to reach eq. 4 from eq. 3. Equation 3 is valid \"for any c\", and yet it leads you to a specific value for c, which does not make sense to me. More precisely, the specific point causing this problem seems to be \"the gradient of the policy with respect to the parameters is the indicator function\": I believe this is incorrect since the parameters are not all free, they are linked together by the constraint sum_a pi(s, a) = 1. For instance if we have two actions then there is a single parameter, and the gradient of the policy with respect to this parameter is either 1 or -1, but not zero.\n\nPlease let me know if I'm missing something here, thanks!This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A", "title": "From eq. 3 to eq. 4", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1BxzUzVl": {"type": "review", "replyto": "B1kJ6H9ex", "review": "Hi,\n\nI'm having some trouble following the reasoning to reach eq. 4 from eq. 3. Equation 3 is valid \"for any c\", and yet it leads you to a specific value for c, which does not make sense to me. More precisely, the specific point causing this problem seems to be \"the gradient of the policy with respect to the parameters is the indicator function\": I believe this is incorrect since the parameters are not all free, they are linked together by the constraint sum_a pi(s, a) = 1. For instance if we have two actions then there is a single parameter, and the gradient of the policy with respect to this parameter is either 1 or -1, but not zero.\n\nPlease let me know if I'm missing something here, thanks!This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A", "title": "From eq. 3 to eq. 4", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byh_RNyZg": {"type": "rebuttal", "replyto": "r1T71wAxe", "comment": "Just re-uploaded, thanks.", "title": "ICLR Paper Format"}, "r1T71wAxe": {"type": "rebuttal", "replyto": "B1kJ6H9ex", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct margin spacing for your submission to be considered. Thank you!", "title": "ICLR Paper Format"}}}