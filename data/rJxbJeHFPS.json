{"paper": {"title": "What Can Neural Networks Reason About?", "authors": ["Keyulu Xu", "Jingling Li", "Mozhi Zhang", "Simon S. Du", "Ken-ichi Kawarabayashi", "Stefanie Jegelka"], "authorids": ["keyulu@mit.edu", "jingling@cs.umd.edu", "mozhi@cs.umd.edu", "ssdu@ias.edu", "k_keniti@nii.ac.jp", "stefje@mit.edu"], "summary": "We develop a theoretical framework to characterize what a neural network can learn to reason about.", "abstract": "Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.", "keywords": ["reasoning", "deep learning theory", "algorithmic alignment", "graph neural networks"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "This paper proposes a framework which qualifies how well given neural architectures can perform on reasoning tasks. From this, they show a number of interesting empirical results, including the ability of graph neural network architectures for learn dynamic programming.\n\nThis substantial theoretical and empirical study impressed the reviewers, who strongly lean towards acceptance. My view is that this is exactly the sort of work we should be show-casing at the conference, both in terms of focus, and of quality. I am happy to recommend this for acceptance."}, "review": {"HJe7pdvCYr": {"type": "review", "replyto": "rJxbJeHFPS", "review": "This paper presents a framework, dubbed algorithmic alignment, based on PAC learning and sample complexity, with the aim to explain generalization on reasoning tasks for different neural architectures. The framework roughly states that in order for the model to be able to learn and successfully generalize on a reasoning task, it needs to be able to easily learn (to approximate) steps of the reasoning tasks. The authors use this framework to propose an increasingly difficult set of tasks, designed to showcase the type of models that would be fit or unfit to solve them. The resulting experiments corroborate the theory, showing the limits of MLPs, Deep Sets, and consequently Graph Neural Networks. The final claim that an NP-hard task needs an enumerative architecture, and then experimental validation of that claim is nice and fits into the theory.\nThe added benefit of the paper is that the authors show as a side-effect that visual question answering and intuitive physics\n\nOverall, the paper presents a meaningful contribution to the theory of learning, formalizing the means of quantifying the capabilities of architectures to solve tasks of certain complexity. The paper, though dense, is well well written, and carries an interesting conclusion that better algorithmic alignment brings the sample complexity down, i.e. models with better algorithmic alignment to the task (function they want to approximate) should generalize better.\nThe formalization presented in the paper, though remarkably intuitive, might be difficult to practically use for more elaborate models and it is not clear whether it can be numerically computed. The paper (i.e. the reader) would certainly benefit from more examples of algorithmic alignment comparison of different models, such as one done in Corollary 3.7.\n\nQuestion:\n- difference to Kolmogorov complexity is that any algorithmic alignment that yields decent sample complexity is good enough - how do you define decent?\n- You state: \u201cin Section 4, we will show that we can usually derive a near-optimal alignment by avoiding as many \u201cfor loops\u201d in algorithm steps as possible.\u201d yet I did not see that there. Was that effectively shown in Corollary 3.7?\n\nSlightly related work: On the Turing Completeness of Modern Neural Network Architectures", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 1}, "rkgdaugtjS": {"type": "rebuttal", "replyto": "rJxbJeHFPS", "comment": "\nDear Reviewers and AC,\n\nWe have updated our draft to incorporate the nice suggestions of the reviewers. In particular, we have made the following changes:\n\n- We have added additional experiments to show test accuracy v.s. training set size on sub-sampled training sets to further support our theory. The results are shown on Figure 4 at page 7, which is also discussed in Sec 4.3. We thank Reviewer 3 for the good suggestion of probing the effect of the number of samples empirically. \n- Thanks to Reviewer 1 for a helpful comment that points out an imprecise statement. We changed it to make it more accurate, and added a discussion at the end of Sec 3 (page 4) regarding reasoning algorithms whose structure is obtuse, and regarding approximation algorithms. This should clarify the the range of problems we address in this paper, and how our results relate to various situations.\n- We have added the related work as suggested by Reviewer#2.\n- We will improve other minor points in the final version. \n\nIn addition, we have clarified all the concerns and confusion of the public comment regarding our theoretical parts. \n\nPlease let us know if you have additional questions. \n\nThank you,\nAuthors \n", "title": "Update"}, "rJePttBWor": {"type": "rebuttal", "replyto": "rJeOFDKjFr", "comment": "Thank you for your constructive feedback. Reviewer points out an imprecise statement/conclusion in our paper. We have adopted the reviewer's suggested version in the revised revision.\n\nReviewer asks whether neural networks can learn tasks where the algorithm is not known. Our answer is the algorithm we hope to learn does not need to be known, but knowing the structure of the algorithmic solution can help with designing architectures and theoretical guarantees. For example, our experiments (Sec 4.1, 4.3) show that different architectures that align to different algorithms can both learn the task well. \n\nReviewer asks to more carefully consider the situation where the algorithmic solution exists but is obtuse. In this paper, we focus on reasoning tasks whose underlying algorithm is exact and has clear structure, and leave the study of approximation algorithms (do not solve the task exactly) and unknown structures for future work. We discussed this at the end of Sec 3 at page 4. This should clarify the the range of problems we address in this paper, and how our results relate to various situations.\n\nIn the case where we face a problem where we do not have knowledge about the underlying algorithmic structure, in order to still generalize well, we think neural architecture search over the algorithmic structure space could be a promising future direction. We will discuss these in the final version.\n\n", "title": "Response"}, "HJeoqFHZsS": {"type": "rebuttal", "replyto": "Hylr6tgjKB", "comment": "Thank you for appreciating our work and giving the nice suggestion. It would indeed be very interesting to see sample sizes for different architectures and tasks in practice. However, the number of samples needed for models like MLP to learn the more complex tasks, e.g. DP, would be very high, so the experiments will be prohibitively expensive. We are considering experimenting models on smaller training set and plot  accuracy v.s. sample size to showcase the trend. We have included the experimental results in the revised version (Fig 4 and Sec 4.3). \n", "title": "Response"}, "rJgE4iCNjH": {"type": "rebuttal", "replyto": "rklnp2qziS", "comment": "Thanks for your interest again. We clarify your confusion below. \n\n1. The reader is confused because the reader\u2019s assumption -- \u201cthe sample complexity to approximate sum/mean pooling by MLP is not high compared to that to approximate a single step in DP.\u201d is indeed not correct. For-loops, including sum/max over functions of all objects, have high sample complexity for an MLP to learn by Thm 3.5, compared to a single step in DP, which is usually a function on a pair of objects (e.g., Bellman-Ford relaxation in Fig. 2). We also discuss this in Sec 3.1, 3.2, and show an example with sum-pooling in Corollary 3.7, where sample complexity increases polynomially with the number of objects to loop over. GNNs could avoid learning such for-loops in DP algorithms so they generalize well. Although Thm 3.5 also has simplifying assumptions, e.g. using gradient descent with infinitesimally small steps, it aligns well with our experimental results (Fig.3). \n\n2. This is a good question. Indeed, our bound suggests reasonably deep GNN should generalize well, even if its number of iterations is higher than the DP iteration. As the reader suggests, we have run additional experiments with GNN10 (each sub-module is a 4-layer MLP) on summary statistics task, where GNN1 already performs well. Our experiment shows GNN10 performs equally well as GNN1. Thus, the experiment aligns with our theory here. We also found this result interesting and will expand on it a bit more in the final version. For example, it contrasts with what has been observed in GNN node classification tasks on social networks etc [1], where without JK, 2-layer GNN often perform the best and deeper GNN perform worse. There are several differences between our settings and theirs, one being adaptivity (different algorithm steps and number of steps we shall act on each node) is often needed for different nodes depending on subgraph structures (expanders vs. trees) in node classification tasks [1], which is not the case in many reasoning tasks. Moreover, note that our GNN formula for reasoning (Eqn 2.2) is different from GCN and GIN, e.g. GCN uses one-layer perceptron but our reasoning GNN (Eqn 2.2) uses MLP. Our reasoning GNN also explicitly models pairwise functions but GCN and GIN do not. This makes a difference too, so for failures of deep GCN on node classification tasks do not necessarily hold here.\n\n[1] Representation learning on graphs with jumping knowledge networks. ICML 2018.\n\nHopefully this clarifies your confusion. Please let us know if you have other questions. Again, we appreciate your interests.\n", "title": "Confusion comes from reader\u2019s incorrect assumption "}, "SygoUFBWor": {"type": "rebuttal", "replyto": "HJe7pdvCYr", "comment": "Thank you for your helpful feedback. We answer your questions below.\n\n- \u201cdifference to Kolmogorov complexity is that any algorithmic alignment that yields decent sample complexity is good enough - how do you define decent?\u201d. Here, \u201cdecent\u201d is a loose term we use to refer to a tight enough algorithmic alignment for good generalization performance. We will explain more in the revised version.\n\n- \u201cYou state: \u2018in Section 4, we will show that we can usually derive a near-optimal alignment by avoiding as many \u2018for loops\u2019 in algorithm steps as possible.\u2019 yet I did not see that there\u201d. One example is Section 4.2: DeepSets does not algorithmically align well with the relational argmax task. It has to learn the for-loops (Claim 4.1), which requires many samples. On the other hand, GNN algorithmically aligns well with relational argmax --- the for-loops are hard-coded in the computation graph. Therefore, GNN achieves better sample efficiency by avoiding learning the for-loop. We will make the connection clearer in the revised version.\n\n- We will add the suggested reference and discuss the relation in the revised version. \n", "title": "Response"}, "ByehQFSWsH": {"type": "rebuttal", "replyto": "rJxbJeHFPS", "comment": "We sincerely appreciate all the reviews, they give positive and high-quality comments on our paper with a lot of constructive feedback. We answer each reviewer\u2019s questions individually. We will update the draft soon.\n", "title": "General response"}, "ryg30_SWiB": {"type": "rebuttal", "replyto": "HJe-F_CljB", "comment": "Thank you for your interest in our work. We address your concerns below. \n\nWhile we agree that assumptions of our theorems are strong, we do not over-claim: We have clearly stated our assumptions in the paper and discussed the relation to practice (Sec 3.2). We also write in the introduction that we provide \u201cinitial theoretical support\u201d to show that algorithmic alignment is desirable for generalization under \u201csimplifying assumptions\u201d. Several theoretical works on deep learning at times make simplifying assumptions. Still, these works have led to interesting insights and triggered many follow-up works. The main goal of our paper is to introduce the perspective of algorithmic alignment and take the first formal initiative towards understanding the interplay of reasoning tasks and NN architecture. Moreover, as we have discussed in Sec 3.2, in our experiments, all models are trained end-to-end. The experimental results agree with our theoretical results despite our sequential assumption, so we believe that future work can extend our theoretical results to end-to-end learning. \n\n\nHowever, we strongly disagree with the reader\u2019s other concerns. \n\n- \u201cIf such oracles are available for MLPs, the sample complexity bound of MLPs would be the same as or even lower than that of GNNs. The comparison between GNN and MLP's sample complexity is therefore unfair.\u201d This is not correct. Our comparison is fair. Although we assume an oracle for each sub-module in Thm 3.6, we do *not* assume oracles for individual layers in the MLP modules of GNN. If we add oracles to every layer of both the MLP and each MLP module in GNN as the reader suggests, we can still show that GNN has a better sample complexity. Intuitively, this is because the giant MLP still needs to learn the entire for-loop. On the other hand, GNNs do not need to learn the for-loop because it is encoded in the architecture (Fig. 2). In our theorem, we try our best to keep the number of oracles small so that it is close to practice, where models are trained end-to-end. Therefore, we do not assume oracles in MLP layers. Also, our theorem agrees with experimental results, so future work may further relax the assumption.\n\n- \u201c[Our theorem] induces that increasing the depth of GNNs will not have a huge or dramatic influence on the generalizability or sample efficiency, which is counter-intuitive. \u201d This is not correct. Increasing the depth of GNNs is crucial to achieving better algorithmic alignment for some tasks (Sec 4.3) and therefore improving sample efficiency. If the depth of the GNN is not sufficient, at least one of the sub-modules needs to learn for-loops. But GNNs with more iterations can align better and avoid such for-loops. One example is the shortest paths problem [Figure 3c]: The number of GNN iterations is the key to good performance. For other tasks, e.g. Fig 3ab, increasing GNN depth is not so necessary. Based on our theory and experiments, both GNN1 and GNN3 can perform well on simple relational argmax tasks. We hope this clarifies your concerns. \n\n- \u201cI believe in the intuition about relational inductive bias and that GNNs are truly more sample efficient than MLP on many relation-related and reasoning-related tasks.\u201d We would like to clarify that our intuition is more specific than what the reader describes. We not only formalize the relational inductive bias of some popular reasoning architectures, but we also characterize *which tasks* GNN does well, and provide examples where GNN fails.\n\nReply to minor concerns:\n\n- \u201cIt may be better to show some failure cases of GNNs.\u201d We have shown a failure case in the paper --  GNNs fail on the subset-sum task in Fig 3(d), while NES, an architecture that aligns better with the task, generalizes well.\n\n- \u201cIt's reasonable to see GNN7 behaves poorly on (a) and (b) tasks, i.e. summary statistics and relational argmax, in Figure 3.\u201d This is not correct. GNN7 performed well in our experiments (we do not show performances of all GNN depths in paper due to space limit).\n", "title": "Response"}, "Hylr6tgjKB": {"type": "review", "replyto": "rJxbJeHFPS", "review": "The paper proposes a measure of classes of algorithmic alignment that measure how \"close\" neural networks are to known algorithms, e.g. dynamic programming (DP). The measure is based on the number of samples needed such that the expected generalization error is less than epsilon with 1-delta probability, where epsilon and delta are free parameters.\n\nThe paper proves the link between several classes of known algorithms and neural network architectures by showing how their sample complexity varies. For instance the paper shows that Graph Neural Network (GNN), can approximate any DP algorithm in a sample efficient manner, whereas MLP and deep sets (permutation invariant NN) can't. The paper empirically verifies their claims on 4 toy datasets, each representing an increasingly complex algorithm needed to solve the problem. \n\nI recommend this paper be accepted, since I think it's an important direction of research, and it formalizes a lot of intuition about neural network architectures.\n\nIt would be very interesting if the authors could actually compute the number of samples, M, for different NN architectures on the toy datasets, and show how it matches empirical findings. This could be a powerful tool if it could be made easy to use for the common practitioner.", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 3}, "rJeOFDKjFr": {"type": "review", "replyto": "rJxbJeHFPS", "review": "This work seeks theoretical and empirical proof of the reasoning capacity of neural networks. The authors build on a body of research that demonstrates the usefulness of different neural network architectures for different reasoning problems. For example, Deep Sets have been proposed to answer questions about sets (e.g., a summary statistic), and GNNs about graph related problems, such as shortest path.\n\nI anticipate that readers would be very satisfied with the intuition behind the main result: neural networks that \u201calign\u201d with known algorithmic solutions are better able to learn the solutions. Many architectures have been proposed over the years, often with a high-level justification for the architecture\u2019s form. For example, Relation Networks noted the difficulty with learning n^2 relations using an MLP, which is an observation reflected in this work\u2019s explanation of the difficulty with learning a for loop. \n\nProvided here is a justification for these high-level design decisions. The authors provide some theory and experimental results to demonstrate their proposed notion of alignment, and show that NNs that align with known algorithmic solution do well, while those that do not align do not do well. In particular, I appreciate both the positive and negative evidence, since demonstrating lack of alignment (and poor performance) is a necessary condition to show alongside alignment (and good performance).\n\nI\u2019d like to caution the authors regarding their main conclusion, which is stated a few times in the paper:\n\n\u201cThis perspective suggests that whether a neural network can learn a reasoning task depends on whether there exists an algorithmic solution that the network aligns with\u201d.\n\nI think this logic is not precisely correct, and I would modify this to:\n\n\u201cIf the structure of a neural network aligns with a known algorithmic solution, then it can more easily learn a reasoning task than a neural network does not align\u201d. \n\nThis is a subtle but important difference. In particular, the original logic does not capture situations where an algorithmic solution is not known, but a neural network can otherwise still learn a solution (consider object classification). I think even the corrected logic as I\u2019ve spelled it out above might not be quite right either, since it does not consider situations where the algorithmic solution exists, but it obtuse. Would a neural network easily learn such a task? \n\nOverall I think the paper is clearly written, and the experiments are adequate. Unfortunately I am not well-versed in the theoretical literature on this topic, so my assessment of the proofs is limited, and I will need to defer to the other reviewers on these matters. My surface level assessment of them is that the logic seems generally sound, but I cannot make any strong statements placing them in the context of previous work, nor can I properly evaluate the nuances. Nonetheless, as a whole, I think this is a strong contribution and a nicely put together piece of work.\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}}}