{"paper": {"title": "Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering", "authors": ["Ramakrishna Vedantam", "Stefan Lee", "Marcus Rohrbach", "Dhruv Batra", "Devi Parikh"], "authorids": ["vrama@gatech.edu", "steflee@gatech.edu", "maroffm@gmail.com", "dbatra@gatech.edu", "parikh@gatech.edu"], "summary": "A probabilistic neural symbolic model with a latent program space, for more interpretable question answering", "abstract": "We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs, given a small annotated set of human programs. The key idea of our approach is to learn a rich latent space which effectively propagates program annotations from known questions to novel questions. We do this by formalizing prior work on VQA, called module networks (Andreas, 2016) as discrete, structured, latent variable models on the joint distribution over questions and answers given images, and devise a procedure to train the model effectively. Our results on a dataset of compositional questions about SHAPES (Andreas, 2016) show that our model generates more interpretable programs and obtains better accuracy on VQA in the low-data regime than prior work. ", "keywords": ["Neural-symbolic models", "visual question answering", "reasoning", "interpretability", "graphical models", "variational inference"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a latent variable approach to the neural module networks of Andreas et al, whereby the program determining the structure of a module network is a structured discrete latent variable. The authors explore inference mechanisms over such programs and evaluate them on SHAPES.\n\nThis paper may seem acceptable on the basis of its scores, but R1 (in particular) and R3 did a shambolic job of reviewing: their reviews are extremely short, and offer no substance to justify their scores. R2 has admirably engaged in discussion and upped their score to 6, but continue to find the paper fairly borderline, as do I. Weighing the reviews by the confidence I have in the reviewers based on their engagement, I would have to concur with R2 that this paper is very borderline. I like the core idea, but agree that the presentation of the inference techniques for V-NMN is complex and its presentation could stand to be significantly improved. I appreciate that the authors have made some updates on the basis of R2's feedback, but unfortunately due to the competitive nature of this year's ICLR and the number of acceptable paper, I cannot fully recommend acceptance at this time.\n\nAs a complete side note, it is surprising not to see the Kingma & Welling (2013) VAE paper cited here, given the topic."}, "review": {"ryl1ZpPly4": {"type": "rebuttal", "replyto": "HkgE_Wx1yE", "comment": "We are glad R2 found the updated version of the paper more clear, and would like to thank the reviewer for prompt responses. As suggested by R2, we will release the code for our paper, along with all the settings to recreate the experiments on our github, and will add additional technical details of the REINFORCE estimator. \n\nAdditionally, we would like to clarify for R2 what we think are the novel and original aspects of this work. While warm starting certain terms from previous stages is common practice, we still believe our key insight is not in terms of showing that warm-starts work, but in terms of realizing that one way to better capture the intent of a program specification is to model a stochastic latent space. This leads to better sharing of statistics across different questions and leads to a more meaningful latent program space. While this is well known for continuous valued latent variable (variational autoencoder style) models, this is relatively underexplored for the discrete, sequential program case.\n\nOverall, we believe our creativity and insight are not in terms of the mechanics or novelty of specific steps we undertook, but in terms of taking a concrete step towards probabilistic neural symbolic models which share statistics meaningfully in a latent space, and learn how to parse questions into programs as well as learn to execute programs using neural modules.\n\nA lot of important, open questions traditionally in AI have been in terms of representation learning, and modeling systematicity and compositional generalization [A], and we believe the line of work on neural-symbolic models (including ours as well as relevant works like [B, C]) are important steps towards solving these challenges.\n\nReferences\n[A]: Lake, Brenden M., and Marco Baroni. 2017. \u201cGeneralization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks.\u201d arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1711.00350.\n[B]: Yi, Kexin, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. 2018. \u201cNeural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding.\u201d arXiv [cs.AI]. arXiv. http://arxiv.org/abs/1810.02338.\n[C]: Evans, Richard, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. 2018. \u201cCan Neural Networks Understand Logical Entailment?\u201d arXiv [cs.NE]. arXiv. http://arxiv.org/abs/1802.08535.", "title": "Clarifying perspective on insight/novelty"}, "HJgdaqL5nQ": {"type": "review", "replyto": "ryxhB3CcK7", "review": "This paper proposes a variational neural module networks (V-NMN), which compared to neural module networks (NMNs), is formed in a probabilistic aspect. The authors compare the performance of V-NMN and NMN on SHAPES dataset.\n\nI find the technical part is hard to follow. To optimize the objective function, it involves many challenges. The authors described those challenges as well. It is not clear to me how those challenges are solved in section 2.1. I think that the presentation in section 2.1 needs to provide more details.\n\nIn the experiment, the authors only compare their work with NMNs without comparing it with other approaches for visual question answering. Besides accuracy, does V-NMN provide new applications that NMNs and other VQA models is not applicable because of the probabilistic formulation?", "title": "Need improvement on the presentation", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkeK5LLq07": {"type": "rebuttal", "replyto": "SkxggDsxam", "comment": "----------------------\n[R2] Why the objective (1) is hard to train but objective (4) is possible to train?\n\nObjective (4) is Objective (6) in the updated version, and we refer to that in the discussion below.\n\nAs mentioned in the submission (Page. 4, Sec. 2.1), objective (6) is possible to train because it uses \u2018warm-starts\u2019 from the other two stages of training (question coding, Eqn. 1, and module training, Eqn. 5 respectively). We provide further intuition for why objective (6) is inherently difficult to train: essentially the parameterization of p(a|i,z) is done by assembling neural module networks on the fly based on the predicted program (z), which is then trained using SGD. Thus, the optimization landscape is in some sense discontinuous in the parameters of the modules (since a different set of modules, with different parameters, could be chosen based on the program). Hence, optimizing Eqn. 6 from scratch is hard (without a good inference network q(z| x) and a good parameterization/ initialization of p(a|i,z)).\n\n----------------------\n[R2] Other related work:\n1] Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining (Zhang et al. 2018)\n-- Our work focuses on a different notion of interpretability for VQA compared to Zhang et.al. While we are interested in a notion of interpretability that preserves a notion/ syntactic specification of `how' to answer a question, this paper is interested in grounding the answer into appropriate regions in the image. This is an orthogonal notion of interpretability compared to what we are mainly interested in; in the sense that we are explicitly interested in question answering via human-interpretable programs, while Zhang et.al. is interested in grounding answers into relevant regions in an image.\n\n2] Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding (Yi et al. 2018).\n-- This is certainly very relevant work, thank you for the pointer. Conceptually, at a high level, the goals of this work and ours are similar: both the approaches want to do question answering with limited question program supervision. While Yi et al. seem to approach this problem by simplifying the p(a|i,z) mapping (in context of our model) by first converting the image into a symbolic table, we approach this goal by modeling a stochastic latent space. In some sense Yi et al., and our approaches are orthogonal, as one can use our probabilistic latent space in conjunction with this work., and thus we believe our approach is of independent interest.\n\nFurther, our approach is addressing the full complexity of learning such neural-symbolic models in an end to end manner, where program execution is being learned in conjunction with parsing questions into programs. This is arguably more general, as parsing images can into tables need not be a sufficient representation for visual recognition across different settings. We have added a discussion on the differences to Yi et.al in related work.", "title": "Addressing R2's concerns [Part 2/2]"}, "HkeMV8U907": {"type": "rebuttal", "replyto": "SkxggDsxam", "comment": "We reply to more specific concerns from R2 below, and hope to convince them that in light of the justifications (already in the paper and below) the proposed method is not heuristic and has sufficient clarity.\n\n----------------------\n[R2] Besides accuracy improvement, is there any other benefit by using V-NMN compared to NMN?\n\nAs mentioned in the paper, and pointed out by R1, V-NMN is not only more accurate but generates the right answers for the ``right\u2019\u2019 reasons by providing more correct program explanations for questions. This makes V-NMN more interpretable, which is one of our key stated goals. In general, interpretability is important for allowing humans to build trust to make actionable decisions from outputs generated by machines.\n\n----------------------\n[R2] How do the authors design the prior distribution p(z) and the variational distribution q_\\phi(z|x), and how do the authors optimize the KL(q_\\phi(z|x), p(z)) term? [..]\n\nThe appendix in the submitted version (Page. 14 in the revision) describes how we parameterize the variational distributions and the priors. Further, we have added text to the main paper (Page. 3, learning) to clarify how we parameterize prior and posterior distributions (LSTM recurrent neural network based sequence models), and highlight existing text in the paper (Page. 5, before Eqn. 4) and Algorithm 1, step 10 explaining how the KL divergence is optimized.\n\n----------------------\n[R2]  If the readers want to understand why it is good to relax \\beta to < 1, they need to check Alemi et al. (2018), which is the only information the authors provide.\n\nSince we directly use the results from Alemi et.al, the initial version did not have further justifications. However, in light of the reviewer\u2019s concern, we have added text to the paper (Page. 4 Sec. 2.1) explaining further why one needs to set \\beta < 1. \nEssentially, Alemi et.al. identify that the ELBO is comprised of the negative log-marginal likelihood term D, and the KL divergence term R, i.e. ELBO = -D -R. Further, they show the mutual information between the data \u201cx\u201d and the latent variable \u201cz\u201d, is bounded below by H-D and above by R (where H is a constant). Since the ELBO is equal to -D-R, the value of the ELBO on its own does not tell us about the mutual information between the observations and the latents, as different models (with say architectural differences) can achieve different D and R values for the same ELBO. Thus, Alemi et.al. prescribe setting $\\beta<1$ for architectures with an auto-decoding behavior, so that we can get higher R values (by emphasizing on minimizing it less), pushing up the upper bound on mutual information I(x, z) achieved by the model\n\n----------------------\n[R2] Could the authors provide technical details for this \"argmax decoded programs\"?\n\nWe perform beam search to get an approximate solution for the argmax program given a question, as is standard practice [A]. This detail is added to Sec. 2.1. We did beam search instead of sampling since sequence models are known to suffer from a distributional mismatch between training and sampling [B],\nmaking beam search a conservative (and standard choice) in the literature for inference in sequence models [B]. For the module training stage, we find this leads to a good warm-start for the full objective (Eqn. 6) (which is optimized via. sampling).\n\nWe also performed an experiment using sampling instead of beam search. In low-supervision settings, we find sampling leads to a drop in performance: with 10% supervision, accuracy on module training drops from 81.34 (+- 8.61) (with beam search) to 76.31 (+- 8.41) (with sampling) on validation. The performance with more supervision remains the same. We have added this finding to the paper (Page. 9, Results section).\n\n----------------------\n[R2]  \\gamma. The authors present the reason for that is because it is similar as Vedantam et al. (2018), which again totally points readers to check other references when the relaxation seems to have issue\n\nWe use values of $\\gamma > 1$ (see Appendix, and Page. 7) and thus for the case where $\\beta > 1$ this still corresponds to a valid lower bound on the ELBO (for discrete-valued probability distributions, which is the case for answers in VQA). Thus, as such, we believe the relaxation does not have any issues (in addition to the justification already presented in the paper based on Vedantam et.al.).\n\n----------------------\n[R2]  For objective (4), is \\beta >=1 or < 1? \n\n\\beta is set to 0.1 for all three stages, as clarified by the Algorithm box in the submitted version.\n\nReferences\n[A]: Vinyals, Oriol, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2014. \u201cShow and Tell: A Neural Image Caption Generator.\u201d arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1411.4555.\n[B]: Bengio, Samy, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. \u201cScheduled Sampling for Sequence Prediction with Recurrent Neural Networks.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1506.03099.", "title": "Addressing R2's concerns [Part 1/2]"}, "HygagHLqRX": {"type": "rebuttal", "replyto": "ryxhB3CcK7", "comment": "We thank the reviewers for the detailed comments and questions, and are encouraged reviewers found the paper well written [R3], our approach worth studying [R3] and that we address the problem well [R1]. In the revised version, we have highlighted the sections in magenta which address the concerns of R2 but have not changed since the initial submission, for ease of reference. Changes we made since the initial submission are marked in blue. \n\nPlease note that whenever we refer to text in the paper in this discussion, we are referring to the updated version of the paper (and not the submitted version, although we might point to text explicitly present in the submitted version based on the above color scheme).\n\nTo reiterate, the goal of this work is not just to get higher empirical performance on VQA. Instead, our aim is to augment an existing class of techniques -- that has been shown to have desirable properties like interpretability and compositionality (Johnson et.al., Hu et.al.) -- with a probabilistic treatment. Concretely and in the short term, this results in higher performance for capturing the intent of human program specifications better (via. better semi-supervised learning), but arguably equally importantly, in the longer term, this is a framework for building probabilistic, neural-symbolic models. Note that neural-symbolic models bring together the power of deep representation learning with the systematic generalization capabilities of symbolic reasoning, promising the best of both worlds. Moreover, probabilistic tools provide a systematic and flexible framework for modeling and inference in general. With these high-level goals, we generally restrict the comparison to previous VQA approaches which use explicit program representations and learn to execute them. \n\nComments addressing specific issues with Sec. 2.1 are in the replies to R2 and more specific comparisons to closely related work follow in replies to R2 and the Anonymous comment.\nWe thank R3 for pointing out typos other writing-related issues, which we address in the updated version.\n", "title": "Updates to paper addressing R2's concerns, specific replies to R2 and Anon."}, "Hkl7MurqR7": {"type": "rebuttal", "replyto": "ryxReMqxCX", "comment": "Thank you for pointing us to this highly relevant work! The approach is indeed quite relevant but has some key differences which we highlight now in the updated draft (in related work).\n\nIn Yin et.al., while the programs are modeled as a latent variable, the model does not capture ``how'' to execute the programs that it generates, and indeed the tasks considered only have a notion of ``parsing\u2019\u2019 into programs but not program execution. More specifically, the model presented in Yin et.al. is a unimodal model with a structured latent space, where the observed modality is the raw text/ question. However, in our model, we have a second modality which is the output of what gets executed when the program runs, and we capture both jointly in our model. Thus we argue that we address probabilistic neural-symbolic learning in a more general setting: where one has to parse a question into programs *as well as* learn to execute them by training neural modules.\nHowever, the key idea of a tree-structured syntactic latent space to represent programs from the work is very interesting and would be relevant to use in our model as well.", "title": "Relevant Work, Updates to paper to discuss differences."}, "HklnNrv53m": {"type": "review", "replyto": "ryxhB3CcK7", "review": "This paper proposes a discrete, structured latent variable model for visual question answering that involves compositional generalization and reasoning. In comparison to the existing approach, this paper well addressed the challenge of learning discrete latent variables in the presence of uncertainty. The results show a significant gain in performance as well as the capability of the model to generalize composition program to unseen data effectively. The qualitative analysis shows that the proposed model not only get the correct answer but also the correct behavior that leads to the answer.  ", "title": "Nice paper, well written and through evaluation", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1x1C3I7i7": {"type": "review", "replyto": "ryxhB3CcK7", "review": "The paper presents a new approach for performing visual query answering. System responses are programs that can explain the truth value of the answer.\nIn the paper, both the problems of learning and inference are taken into account.\nTo answer queries, this system takes as input an image and a question, which is a set of word from a given vocabulary. Then the question is modeled by a plan (a series of operation that must be performed to answer the query).  Finally, the found answer with the plan are returned. To learn the parameters of the model, the examples are tuples composed by an image, a question, the answer, and the program.\nExperiments performed on the SHAPES dataset show good performance compared to neural model networks by Johnson et al.\n\nThe paper is well written and clear. I have not found any specific problems in the paper, the quality is high and the approach seems to me to be new and worth studying.\nThe discussion on related work seems to be good, as well as the discussion on the results of the tests conducted.\n\nOn page 5, in equation (3) it seems to me that something is missing in J. Moreover, In Algorithm 1, in lines 4 and 9, the B after the arrow should be written in italic.\n\nOverall, there are several typos that must be corrected. I suggest a double check of the English. For example:\n- page 3, \"as modeling *uncertaintly* should...\"\n- page 6, \"Given this goal, we *consrtuct* a latent *varible* ...\"\n- page 8, in paragraph \"Effect of optimizing the true ELBO\", the word \"that\" is repeated twice in the 3rd row\n- page 13, \"for the\" repeated twice in \"Moving average baseline\" paragraph. Also, in the last line of this paragraph, the sentence seems incomplete.\n\n\n\nPros\n- The results are convincing\n- The approach is clearly explained\n\nCons\n- English must be checked", "title": "A good piece of work", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}