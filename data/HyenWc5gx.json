{"paper": {"title": "Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning", "authors": ["Matthew Riemer", "Elham Khabiri", "Richard Goodwin"], "authorids": ["mdriemer@us.ibm.com", "ekhabiri@us.ibm.com", "rgoodwin@us.ibm.com"], "summary": "We propose a novel general purpose regularizer to address catastrophic forgetting in neural network sequential transfer learning.", "abstract": "Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks.  We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.", "keywords": ["Deep learning", "Transfer Learning", "Natural language processing"]}, "meta": {"decision": "Reject", "comment": "The proposed approach is not consistently applied for the different experiments; this significantly harms the overall value of the research. The results are also quite domain-specific, and it is not clear if the findings would hold more generally. The paper is not clearly organised or written and does not give a specific enough introduction to the field of transfer learning."}, "review": {"B1s-Y10rl": {"type": "rebuttal", "replyto": "HyenWc5gx", "comment": "Thank you very much for your thoughtful reviews and suggestions. We have revamped our submission and title in order to address as many of the issues raised as we could. \n\nReviewer #1: \n\nThank you for your interest in experiments demonstrating that our model performs well on the original source task. We have added experiments to show that this is the case empirically in section 5.4 and Table 3 of the updated paper. Our forgetting cost methodology consistently experiences less source task performance deterioration than our baseline knowledge transfer algorithms.  \n\nWe have changed the wording of what we previously called the \u201cjoint training\u201d baseline in the paper to \u201cLearning without Forgetting (LwF)\u201d in order to credit the original inventors of this methodology, and to make it more clear that it is not simply two task multi-task learning. LwF is a more relevant comparison to the forgetting cost than multi-task learning because it allows us to directly isolate the effect of our proposal of freezing source task specific parameters during target task data integration. We have reformatted our discussion of the source task experiments in section 5.1 and baseline algorithms in section 5.2 in order to make our experiments, baseline algorithms, and motivation for picking them clearer. In section 5.2 we explain in the \u201cLearning without Forgetting\u201d paragraph that it can be considered equivalent to the forgetting cost for the newly explored scenario when the source task is in the same output space as the target task of Table 1 in the updated paper. \n\nWe have attempted to make our paper title and body more domain specific. As suggested, we have also put the ensemble methodology section along with the experiments. We moved Table 1 in the previous version to the appendix to address your comment. It was tough to fit the target task only result in the formatting of Table 2, so we have written section 5.1 to include this result in the text of the first paragraph.  Hopefully this highlights target task only performance to the reader and makes it clear that it applies to all of our reported results on the target task. Additionally, we have reworked our discussion of the logical rules in an attempt to make how we use them more transparent. \n\nReviewer #2:\n\nThank you for your questions and suggestions. We have re-worked our experiments description in section 5.1 of the updated draft in order to more clearly indicate our motivation for choosing each source task experiment to maximize performance on the target task based on prior art. We also attempt to demonstrate our clear use of uniform experimental practices for each technique by reformatting the algorithm descriptions in section 5.2 of the updated draft. Hopefully, we have achieved the desired more centralized disclosure of details about each model. \n\nWe can see how our discussion of the two possible equations for the forgetting cost in our original draft gave you the wrong impression. We have reworded this part a bit in the third paragraph of section 5.3 to make it more clear that the additional findings we present were the result of additional experiments that we performed in order to understand more about this result. We thought the fact that the model at one point has learned a good test set solution and later deteriorates to a less optimal solution for generalization would be a very informative observation for the reader in understanding this specific experimental result. \n\nWe have added a sentence at the beginning of the introduction describing sequential transfer learning and it is later in the paragraph related to multi-task learning. We have also added to that paragraph some context about the nature of the catastrophic forgetting problem and added citations to other recent work supporting the idea that increased representation stability can lead to improved generalization in comparison to fine-tuning. We have now integrated discussion of the logical rule engine alongside the other source tasks for clarity in section 5.1 and included a couple sentences to start the description with a more general view of how this idea can be applied before getting into the specifics of what we do in this experiment.\n\nReviewer #3:\n\nThank you for your suggestions. We have added additional empirical results directly highlighting the extent that source task knowledge is retained after target task integration to provide a more thorough comparison of techniques.  We have also changed the title to reflect the domain specific nature of our experiments based on your suggestion. \n\nThank you very much for your suggestion about the section ordering. We have in large part taken this advice while also balancing formatting suggestions made by the other reviewers. We now have 7 sections and an appendix that we hope makes the thought process easier to follow. \n\nComment: \n\nThank you very much for the reference. This is a great paper. We were not aware of this paper and we have now updated the submission throughout the introduction, related work, and experimental sections to highlight the ideas we utilize that are in this paper and elements unique to our work. In particular, we apply the Learning without Forgetting (LwF) paradigm to some new problem settings and make methodological choices such as freezing source task specific parameters during target task integration that work to our empirical benefit both when it comes to generalization and representation stability. LwF is actually very similar to the baseline we had called \u201cJoint Training\u201d in the prior versions of our paper. We have updated the discussion of this technique in section 5 to clearly highlight the empirical benefit of our proposed extensions in our problem setting in comparison to LwF as described in the original paper. \n", "title": "Response To Reviews and Comments"}, "S1vZ1lKBg": {"type": "rebuttal", "replyto": "Hkx_B09El", "comment": ":-(", "title": "no reply?"}, "Hkx_B09El": {"type": "rebuttal", "replyto": "HyenWc5gx", "comment": "Dear authors, \nare you aware of the work [Zhizhong Li , Derek Hoiem. \"Learning Without Forgetting\", European Conference on Computer Vision, 2016]? Li and Hoiem also aims at preventing catastrophic forgetting while learning (representations for) multiple tasks. They also rely on distillation as a form of regularization. \nCould you let us know how you see your own work in relation to theirs, in particular what would be main advantages of your technique?\n\nThanks you and happy holiday!\n", "title": "Relation to \"Learning without Forgetting\" (Li, Hoiem, ECCV 2016)"}, "Byi9TI0Qe": {"type": "rebuttal", "replyto": "B1DWiXoXe", "comment": "Thank you for your questions and very helpful suggestions. We have updated the paper to address each of them:\n\n1) This is a great suggestion. We have added a new subsection at the beginning of Section 4 to formalize the sequential knowledge transfer problem statement we explore in our experiments. \n\n2) We have reworked this section a bit to make the applicable setting more clear. One reason we expect a relationship between the tasks in Section 4.1 (now Section 4.2 after our update) is because the output representation of the source task data and target task data are aligned in the same representation space. This implies that the source task model is trying to solve a problem with output directly applicable to the target task. Although the best performance would likely be achieved when the source task model performs well on the target task, it is not mandatory for our method. If the source task model's performance on the target task is poor, you can still achieve strong target task performance by choosing a small value for the forgetting cost hyperparameter. By choosing a small value, we can minimize the penalty for forgetting the representation learned on the source task.  \n\n3) We have reworked the description in both of these sections to be more clear about what parameters are shared and how weights are initialized for learning the target task. \n\n4) Your assessment is correct. We have added a paragraph at the end of Section 2 to address where our approach fits in within the context of related work that incorporates rules into a neural network. We have also added details in the first paragraph of Section 7 to be more concrete about how we derive soft labels from the count based evidence model. \n\n5) In Section 8, we have reworked our wording to be clearer in our usage of the word task. We have also added some clarification in the first few sentences of Section 8 to make it more clear when each equation was used within our experiments. \n", "title": "Response to Comments and Questions"}, "Hyb_2GQXg": {"type": "rebuttal", "replyto": "BJL0ZXe7x", "comment": "Thank you very much for your questions and suggestions. Hopefully, we have now addressed each of them in the updated version of the paper:\n\nQ1: In our initial draft, we did use these words somewhat interchangeably. We have updated the word usage so that we do not use the word \u201cdomain\u201d when the word \u201ctask\u201d could be used without loss of meaning. \n\nQ2: We experienced a 3% performance penalty using equation 8 when equation 6 could be used instead. We have added a paragraph at the end of Section 8 describing what we found in these experiments. \n\nQ3: Other than the learning rate, our only hyperparameters are the mixing rate for joint training and the forgetting cost hyperparameters from equations 6 and 8. We have updated Section 6 so that this is more explicit.\n  \nQ4: We only considered one version of each of our rule distillation models because we did not experience high run to run variation. We do not use ensembles outside of Section 10. We have updated Section 6 and the second paragraph of Section 7 to make this clearer.  \n\nQ5: We have split the table into two for clarity (now tables 2 and 3) and followed your suggestion for the style of reporting the sequential knowledge transfer case.  \n\nQ6: Hopefully this is clearer now as we have expanded on these details in the second paragraph of Section 8. \n\nQ7: There actually was a results table (Table 4 in the original version and Table 5 in the updated version), but based on your note we realized we forgot to explicitly reference it in Section 10 and it was easy to miss because it was right before the references. This should be fixed now so it is easier to follow. \n \nRemark: Thank you for the note. This should be fixed now. \n", "title": "Response to questions"}, "BJL0ZXe7x": {"type": "review", "replyto": "HyenWc5gx", "review": "Q1: The intro mentions tasks and domains; is there a difference?\n\nQ2: Do you have any experiments for the simple model in equation (6)? Did you evaluate the model from equation (7) against the one in (6) in cases where (6) applies? Or did you never consider such simple cases? \n\nQ3: You mention hyperparameter optimization; which hyperparameters did you tune?\n\nQ4: Are the results in Table 1 the test results of the best run (w.r.t. validation performance) of 10? Running hyperparameter optimization for each of them? Does this also already use ensembles? \n\nQ5: Where do the baseline model results in Table 2 come from? I'm missing references. Table 2 is also poorly formatted; please have one column for each of the 4 methods being compared (and another one for the respective baseline).\n\nQ6: Can you please specify the details of the 4 methods being compared in Table 2? It would be useful to have a short paragraph in the paper describing what each of them entails.\n\nQ7: Could you include a results table that compares different variants of your approach to published numbers on any dataset? (The only such comparison I found is quite indirect and only given in the text in Section 10.)\n\nRemark: each of your citations gives rise to a grammatically wrong sentence. E.g., the following sentence from your introduction (first paragraph) is not a correct English sentence:\n\"[...] multi-task learning is not Caruana (1997)\".\n(Indeed, multi-task learning is not Caruana, it's a technique invented by him.)\nYou should only use inline citations as above only when you want to use the author name as a noun:\n\"[...] multi-task learning as invented by Caruana (1997)\".\nIn all other cases, please use standard parenthetical citations: \n\"[...] multi-task learning is not (Caruana, 1997)\".\nPlease fix this; it made it quite painful for me to read the paper.This paper introduces a new method for transfer learning that avoids the catastrophic forgetting problem. \nIt also describes an ensembling strategy for combining models that were learned using transfer learning from different sources.\nIt puts all of this together in the context of recurrent neural networks for text analytics problems, to achieve new state-of-the-art results for a subtask of the SemEval 2016 competition.\nAs the paper acknowledges, 1.5% improvement over the state-of-the-art is somewhat disappointing considering that it uses an ensemble of 5 quite different networks.\n\nThese are interesting contributions, but due to the many pieces, unfortunately, the paper does not seem to have a clear focus. From the title and abstract/conclusion I would've expected a focus on the transfer learning problem. However, the description of the authors' approach is merely a page, and its evaluation is only another page. In order to show that this idea is a new methodological advance, \nit would've been good to show that it also works in at least one other application (e.g., just some multi-task supervised learning problem). Rather, the paper takes a quite domain-specific approach and discusses the pieces the authors used to obtain state-of-the-art performance for one problem. That is OK, but I would've rather expected that from a paper called something like \"Improved knowledge transfer and distillation for text analytics\". If accepted, I encourage the authors to change the title to something along those lines.\n\nThe many pieces also made it hard for me to follow the authors' train of thought. I'm sure the authors had a good reason for their section ordering, but I didn't see the red thread in it. How about re-organizing the sections as follows to discuss one contribution at a time?\n1,2,4,3,8 including 6, put 9 into an appendix and point to it from here, 7, 5, 10. That would first discuss the transfer learning piece (4, and experiments potentially in a subsection with previous sections 3,8,6), then discuss the distillation of logical rules (7), and then discuss ensembling and experiments for it (5 and 10). One clue that the current structure is suboptimal is that there are 11 sections...\n\nI like the authors' idea for transfer learning without catastropic forgetting, and I must admit I would've rather liked to read a paper solely about that (studying where it works, and where it fails) than about the many other topics of the paper. I weakly vote for acceptance since I like the ideas, but if the paper does not make it in, I would suggest that the authors consider splitting it into two papers, each of which could hopefully be more focused.  \n", "title": "Clarifications", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ryO3U0GNe": {"type": "review", "replyto": "HyenWc5gx", "review": "Q1: The intro mentions tasks and domains; is there a difference?\n\nQ2: Do you have any experiments for the simple model in equation (6)? Did you evaluate the model from equation (7) against the one in (6) in cases where (6) applies? Or did you never consider such simple cases? \n\nQ3: You mention hyperparameter optimization; which hyperparameters did you tune?\n\nQ4: Are the results in Table 1 the test results of the best run (w.r.t. validation performance) of 10? Running hyperparameter optimization for each of them? Does this also already use ensembles? \n\nQ5: Where do the baseline model results in Table 2 come from? I'm missing references. Table 2 is also poorly formatted; please have one column for each of the 4 methods being compared (and another one for the respective baseline).\n\nQ6: Can you please specify the details of the 4 methods being compared in Table 2? It would be useful to have a short paragraph in the paper describing what each of them entails.\n\nQ7: Could you include a results table that compares different variants of your approach to published numbers on any dataset? (The only such comparison I found is quite indirect and only given in the text in Section 10.)\n\nRemark: each of your citations gives rise to a grammatically wrong sentence. E.g., the following sentence from your introduction (first paragraph) is not a correct English sentence:\n\"[...] multi-task learning is not Caruana (1997)\".\n(Indeed, multi-task learning is not Caruana, it's a technique invented by him.)\nYou should only use inline citations as above only when you want to use the author name as a noun:\n\"[...] multi-task learning as invented by Caruana (1997)\".\nIn all other cases, please use standard parenthetical citations: \n\"[...] multi-task learning is not (Caruana, 1997)\".\nPlease fix this; it made it quite painful for me to read the paper.This paper introduces a new method for transfer learning that avoids the catastrophic forgetting problem. \nIt also describes an ensembling strategy for combining models that were learned using transfer learning from different sources.\nIt puts all of this together in the context of recurrent neural networks for text analytics problems, to achieve new state-of-the-art results for a subtask of the SemEval 2016 competition.\nAs the paper acknowledges, 1.5% improvement over the state-of-the-art is somewhat disappointing considering that it uses an ensemble of 5 quite different networks.\n\nThese are interesting contributions, but due to the many pieces, unfortunately, the paper does not seem to have a clear focus. From the title and abstract/conclusion I would've expected a focus on the transfer learning problem. However, the description of the authors' approach is merely a page, and its evaluation is only another page. In order to show that this idea is a new methodological advance, \nit would've been good to show that it also works in at least one other application (e.g., just some multi-task supervised learning problem). Rather, the paper takes a quite domain-specific approach and discusses the pieces the authors used to obtain state-of-the-art performance for one problem. That is OK, but I would've rather expected that from a paper called something like \"Improved knowledge transfer and distillation for text analytics\". If accepted, I encourage the authors to change the title to something along those lines.\n\nThe many pieces also made it hard for me to follow the authors' train of thought. I'm sure the authors had a good reason for their section ordering, but I didn't see the red thread in it. How about re-organizing the sections as follows to discuss one contribution at a time?\n1,2,4,3,8 including 6, put 9 into an appendix and point to it from here, 7, 5, 10. That would first discuss the transfer learning piece (4, and experiments potentially in a subsection with previous sections 3,8,6), then discuss the distillation of logical rules (7), and then discuss ensembling and experiments for it (5 and 10). One clue that the current structure is suboptimal is that there are 11 sections...\n\nI like the authors' idea for transfer learning without catastropic forgetting, and I must admit I would've rather liked to read a paper solely about that (studying where it works, and where it fails) than about the many other topics of the paper. I weakly vote for acceptance since I like the ideas, but if the paper does not make it in, I would suggest that the authors consider splitting it into two papers, each of which could hopefully be more focused.  \n", "title": "Clarifications", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}