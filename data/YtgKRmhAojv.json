{"paper": {"title": "One Reflection Suffice", "authors": ["Alexander Mathiasen", "Frederik Hvilsh\u00f8j"], "authorids": ["~Alexander_Mathiasen2", "fhvilshoj@gmail.com"], "summary": "Instead of using many Householder reflections you can just use one \"auxillary\" reflection. ", "abstract": "Orthogonal weight matrices are used in many areas of deep learning. Much previous work attempt to alleviate the additional computational resources it requires to constrain weight matrices to be orthogonal. One popular approach utilizes *many* Householder reflections. The only practical drawback is that many reflections cause low GPU utilization. We mitigate this final drawback by proving that *one* reflection is sufficient, if the reflection is computed by an auxiliary neural network.", "keywords": ["Orthogonal Weights Householder Reflections Normalizing Flows"]}, "meta": {"decision": "Reject", "comment": "This paper proposes to use a single parametric Householder reflection to represent Orthogonal weight matrices.\nIt demonstrates that this is sufficient provided that we make the reflection direction a function of the input vector. It is also demonstrated under which conditions this modified transformation is invertible. The derivations are sound. \nThis insight allows for cheaper forwarding of the model but it also comes with extra costs: It has an increased computational cost for inversion (e.g. requires optimisation) and, importantly, it does not allow to cache the $O(d)$ matrix so  it is not clear there is an advantage of the method over exp maps when we have parameter sharing (e.g. as in RNNs), since the action of the matrix has to be recomputed every-time. The presented experiments are OK, but comparisons to other (potentially more efficient) methods are lacking as pointed out by the reviewers. As it stands it is not clear that this is an idea of broad interest, perhaps more suited to a specialised venue such as a workshop."}, "review": {"7uWF7BZooLp": {"type": "rebuttal", "replyto": "RuSDanYcf0r", "comment": "Something came up so we didn't mange to finish the experimeint, it took a much longer than anticipated. \n\nWe think the point with a (d, L) table is very good. We plan on writing proper expermiental code for this, instead of quickyl hacking something together. That said, before doing so we'll take a step back and rethink the entire project. Your comments will be valuable when doing so. \n", "title": "Response."}, "RKxR-Yo9iw": {"type": "rebuttal", "replyto": "JrKGB0KjQJm", "comment": "**Reviewer: On the other hand, something to take in acco..**\n\nTrue, that is a good point. Thanks. \n\n**Reviewer: .. f one only wants to use it in one layer (no weight sharing such as the RNN) and one doesn't want to compute the inverse or the transpose, one may implement the matrix exponential in a matrix-vector product form, recovering the $O(d^2bs).**\n\nWe were not aware of this, if possible this would definitely be an interesting comparison to include going forwards.  \n\n**Reviewer: Also, as pointed in the first paragraph, one has to be careful with the big-O notation, as that accounts for FLOPS, ... \"**\n\nAgreed. \n\n**Reviewer: And yes, I think that for the comparison it would be enough to check the time that training an epoch, as this is just to get a rough idea of how they compare in practice out of curiosity, it doesn't need to be bullet proof.**\n\nDid you see we added a point about $bs \\cdot L \\ll d \\rightarrow bs \\ll 16$ for $d=1024, L=64$? Also, was it $L=67$ a typo or did you really a little bit larger than power of two for caching reasons?\n", "title": "Response. "}, "dJGJhR9ROMV": {"type": "rebuttal", "replyto": "ev1ztTRZlzm", "comment": "Thanks for all the feedback, it is invaluable, and will save us a lot of time when continuing our work. \n\n**Comparison against Cayley and Exponential**\n\nThanks. While the above statement about determinant is technically true, it is misleading because of how big-O notation deals with constants. The quantity of interest is the total amount of time spent to deal with a linear layer when training a normalizing flow. \n\nAuxiliary Reflections: Forward pass $O(d^2bs)$ and determinant $O(d^2bs)$, the total time is then $O(2 d^2bs)=O(d^2bs)$ because big-O notation doesn't care about constants.  \n\nMatrix Exponential: Forward pass $O(d^3)$ and determinant $O(1)$, the total time is then $O(d^3 + 1)=O(d^3)$. \n\nSo it is true the the $O(1)$ part locally beats $O(d^2bs)$, but this only happens after spending $O(d^3)$ on the matrix exponential, which asymptotically is worse than spending $O(d^2bs)$ time twice. \n\n**Remark.** While we do not expect this clarification to necessarily change your opinion (we should've made all this clear in the paper). That said, it is important for us to understand if you acknowledge this point. If not, it would be very helpful if you elaborate. \n\n**Reviewer: Efficiency, could the authors provide a comparison of the wall-clock time needed to process a sequence of length 64 and an embedding of size 1024?**\n\nWould it make sense to measure training time each epoch instead of using profiler? Note that we wrote one needs $L\\cdot bs \\ll d$, so with $d=1024$ and $L=64$ we get $bs\\ll d/L=16$. Almost finished writing the code. ", "title": "Response."}, "2FdSfElOLuU": {"type": "rebuttal", "replyto": "gM8o6khS5nU", "comment": "**Reviewer: I fail to see where it could be useful.**\n\nThanks for raising this point. We apologize that this was not clear. We believe the main use of auxiliary reflections is scalability. \n\n**FCN:** Auxiliary reflections forward pass takes $O(d^2 bs)$ instead of $O(d^3)$. Example: batch size $bs=64$ and dimension $d=1024$, this gives a theoretical speed-up of $d^3/(d^2bs)=16$. We believe this is useful. \n\n**RNN:** Let $L$ be sequence length or number of recurrent steps. Auxiliary reflections forward pass takes $O(L\\cdot d^2\\cdot bs)$ instead of $O(d^3+L\\cdot d^2\\cdot bs)$. If $bs \\cdot L \\ll d$ this gives a speedup, which we also believe can be useful.\n\n**Question 0.** How convincing should the scaling of auxiliary reflections to be considered useful? ", "title": "(summary)"}, "gM8o6khS5nU": {"type": "review", "replyto": "YtgKRmhAojv", "review": "***Summary***\nThe authors present a way to learn the action of an arbitrary orthogonal matrix on a vector via a map from $\\mathbb{R}^{n\\times n}$ onto $\\operatorname{O}(n)$. They show that the map is surjective, and give conditions under which they can invert this action. They then compare against previous proposed schemes in one task and show the performance of their models in other two.\n\n***Comments***\n\nCorollary 1. It should be $d > 2$, as $S^1$ is not simply connected. Also, for the proof, when you are bringing results from a book, please cite the exact theorems that you are using, as citing a 700 pages book is not of much help.\n\nI do not see how proof of Theorem 4 is correct. You define $f(0) = 0$, but I do not see where you prove that $\\lim_{x\\to 0}f(x) = 0$, as in Lemma 3 you explicitly work on $\\mathbb{R}^n \\backslash \\{ 0 \\}$.\n\nThe paper puts all the proofs in the main paper. I believe that all these should be moved to the appendix, as they are just standard algebraic computations. A more in-depth study of the developed action could go (see next point). If anything, proof of Theorem 1 should be in the main text, as it is the result that drives the paper and its proof is one line.\n\nGiven the topology of $\\operatorname{O}(n)$, which has two disconnected components, any surjective action from $\\mathbb{R}^{n \\times n}$ is bound to be discontinuous and, in particular, will have exploding gradients at some points. This can be problematic in some situations, yielding instability in more difficult models. I think that it would be very beneficial for the paper to show this in an experiment.\n\n***Experiments***\n\nThe exponential map has been recently implemented in a very stable and fast way in PyTorch 1.7.0, getting some notable speed improvements over previous implementations. How does the method in this paper compare time-wise with this implementation?\n\nExperiment 3.2. Why don't you compare against Helfrich (Cayley), Casado (Riemannian exponential) and Lezcano-Casado & Mart\u00ednez-Rubio (matrix exponential)? Their results seem to converge faster and to a lower minimizer than those shown in this paper. Even if that is the case that is fine, but please add them to the paper for a fair comparison.\n\nRelated to the previous two concerns, the paper shows how this method is faster than the Cayley and Exponential map when just ONE product is computed. On the other hand, they do not show what happens in the setting of an RNN, where they method has to compute the action $768$ times, while the Cayley and Exponential are just computed once and used throughout the RNN.\n\nExperiment 3.3. How is the determinant computed? If it is computed every iteration using Lemma 1 this would make the parametrisation too expensive to use in general normalising flows.\n\n***Related work***\n\nUnder \"Different Approaches.\" you mention that:\n\"the Cayley map (Lezcano-Casado & Mart\u00ednez-Rubio, 2019) and the matrix exponential (Casado,2019).\"\nThis is not the case. In (Lezcano-Casado & Mart\u00ednez-Rubio, 2019), they use the matrix exponential, while in (Casado, 2019), they use the Riemannian exponential. The work that used the Cayley map to perform optimisation over $\\operatorname{SO}(n)$ was\nHelfrich, K., Willmott, D., and Ye, Q. Orthogonal recurrent neural networks with scaled Cayley transform. ICML 2018\n\n***Minor***\n\nPage 6. \"to conclude $f(x)$ is\" -> conclude that $f(x)$\n\nPage 11. \"Is an follows\" -> Follows\n\n\n***Conclusion***\n\nI like the idea of the paper as it is conceptually simple and fairly well implemented. On the other hand, I have three big concerns about the paper.\n\nFirst, I do not think that it is competitive with other approaches when it comes to efficiency. The paper does not benchmark against the previous approaches under common benchmarks (MNIST, PMNIST, TIMIT, treebank...) , even though it shows plots on these datasets (MNIST, PMNIST), which is suspicious. Even then, I do not think that every presented method should introduce themselves as improving the SoTA on a given task, as that is just not possible. On the other hand, I do think that the authors should find their niche, as I do not see how this approach would be preferable over simpler approaches like using the Cayley map or the exponential, as it has the drawback of not having explicit access to the inverse transformation, and it does need of a fairly expensive operation to compute the determinant of its Jacobian. I believe that the authors should present a strong case for why this method is practical and preferable in some context over others.\n\nSecond, I believe that the paper could do with some cleaning. The paper has too many computations in it, which does not add to the point it tries to make. This does not help elucidating where this method could be of use over other methods, as mentioned in the previous point.\n\nThird, as show in Lemma 1, and due to the need of using spectral normalisation, it seems like the authors have transformed the problem of optimisation with orthogonal constraints onto a problem of optimisation over symmetric matrices with some non-trivial eigenvalue constraints, which is arguably more difficult! This relates to the first problem I raised, as I fail to see how this method can be more useful than previous approaches in any context.\n\n", "title": "Nice idea, but I fail to see where it could be useful", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "bf9Aene1KBj": {"type": "rebuttal", "replyto": "x66N4F53v94", "comment": "### Comments\n\n**Reviewer: I do not see how proof of Theorem 4 is correct. You define $f(x):=0$, but I do not see where you prove that $\\lim_{x\\rightarrow 0}f(x)=0$, as in Lemma 3 you explicitly work on $\\mathbb{R}^d\\backslash\\{0 \\}$ .**\n\n**Question 1.** We suspect the issue is a very unfortunate formulation for which we apologize. The sentence \".. we see $f$ is continuously differentiable .. \" should instead have been \".. we see $f$ is continuously differentiable on $S^{d-1}$ .. \". In other words, we do not need $f$ to be continuously differentiable on $\\mathbb{R}^d$, just on $S^{d-1}$, which circumvents arguing about $\\lim_{x\\rightarrow 0} f(x)$. Does this address the concern? \n\n**Reviewer: The paper puts all the proofs in the main paper. I believe that all these should be moved to the appendix, as they are just standard algebraic computations.**\n\n**Question 2.** Thanks for the suggestion, we will move Section 2.3 to the appendix. Do you think it would be useful to keep the proof sketch in the main paper? \n\n**Reviewer: If anything, proof of Theorem 1 should be in the main text, as it is the result that drives the paper and its proof is one line.**\n\n**Question 3.** Thanks for the suggestion. We were scared proving Theorem 1 in \"Our Result\" would be a bit confusing, as we would have to introduce Lemma 5 which is hardly \"Our Result\". Do you think it would make sense with a subsection 1.2 called \"Proving Theorem 1\" just after \"Our Results\"?\n\n**Reviewer: ... bound to be discontinuous and, in particular, will have exploding gradients at some points. This can be problematic in some situations, yielding instability in more difficult models. I think that it would be very beneficial for the paper to show ```this``` in an experiment.**\n\n**Question 4.** It is not clear to us what ```this``` refers to. What is the objective of the experiment? To investigate whether gradient descent moves parameters of auxiliary reflections towards a place with exploding gradients? If so, would training a FCN with auxiliary reflections and noting its gradients doesn't explode suffice? If not, it would be helpful to hear an example of an experiment.\n\n**Reviewer: Corollary 1. It should be $d>2$.**\n\nThanks, this was fixed.\n\n**Reviewer: Also, for the proof, when you are bringing results from a book, please cite the exact theorems that you are using, as citing a 700 pages book is not of much help.**\n\nWe apologize, this will be fixed. \n\n### Related work\n\n**Reviewer: Under \"Different Approaches.\" you mention that: ...**\n\nWe apologize for this embarrassing mistake, we have fixed it. ", "title": "Response III: "}, "x66N4F53v94": {"type": "rebuttal", "replyto": "2pmy-M7cxJ0", "comment": "### Concerns \n\n**Reviewer: First, I do not think that it is competitive with other approaches when it comes to efficiency.**\n\nThanks for raising this concern. We think it is important to be clear about what 'efficiency' refers to here. For FCNs, auxiliary reflections take $O(d^2 \\cdot bs)$ time instead of $O(d^3)$. So if efficiency means time complexity auxiliary reflections are provably more efficient in this case. \n\nThat said, if efficiency means validation loss for RNNs, we do agree the $O(d^3)$ methods are more efficient when the number of recurrent steps are larger than the hidden dimension. While this is the case for the permuted MNIST task, it is often not true for NLP tasks (e.g. this paragraph has 67 words / recurrent steps, but could be processed by an RNN with hidden dimension 1000).  \n\n**Reviewer: On the other hand, I do think that the authors should find their niche, as I do not see how this approach would be preferable over simpler approaches like using the Cayley map or the exponential, as it has the drawback of not having explicit access to the inverse transformation, and it does need of a fairly expensive operation to compute the determinant of its Jacobian. I believe that the authors should present a strong case for why this method is practical and preferable in some context over others.**\n\nThanks for raising this point. With respect to inverse and Jacobian determinant. We did not stress this in the submitted manuscript, but if $W$ is triangular the time complexity of Jacobian determinant is $O(d^2bs)$ and the time complexity of inverse is $O(d^2 bs \\cdot r)$ where $r$ is iterations. For fully connected layers, this is asymptotically preferable to $O(d^3)$. \n\n**Reviewer: Second, I believe that the paper could do with some cleaning. The paper has too many computations in it, which does not add to the point it tries to make. This does not help elucidating where this method could be of use over other methods, as mentioned in the previous point.**\n\nThanks for raising this point. Our current plan is to move the proofs to appendix as suggested.\n\n\n**Reviewer: Third, as show in Lemma 1, and due to the need of using spectral normalisation, it seems like the authors have transformed the problem of optimisation with orthogonal constraints onto a problem of optimisation over symmetric matrices with some non-trivial eigenvalue constraints, which is arguably more difficult! This relates to the first problem I raised, as I fail to see how this method can be more useful than previous approaches in any context.**\n\nIf one wants to use orthogonality for Normalizing Flows, it is correct one needs to add symmetry and eigenvalue constraints for provably invertibility. That said, this does not apply when invertibility is not needed as in the RNN and FCN experiments. \n\n", "title": "Response II: "}, "2pmy-M7cxJ0": {"type": "rebuttal", "replyto": "gM8o6khS5nU", "comment": "We would like to thank the reviewer for taking the time to produce a detailed and very thoughtful review. \n\n### Experiments\n\n**Reviewer: Experiment 3.2. Why don't you compare against Helfrich (Cayley), Casado (Riemannian exponential) and Lezcano-Casado & Mart\u00ednez-Rubio (matrix exponential)? Their results seem to converge faster and to a lower minimizer than those shown in this paper. Even if that is the case that is fine, but please add them to the paper for a fair comparison.**\n\nThanks for raising this point. We felt the most interesting comparison was between auxiliary reflections and normal reflections since this is what our proofs concern. That said, we do acknowledge that the comparison would provide a clearer picture of related work, and are thus happy to add them as suggested.    \n\n**Reviewer: ... the paper shows how this method is faster than the Cayley and Exponential map when just ONE product is computed. On the other hand, they do not show what happens in the setting of an RNN, where they method has to compute the action 768 times, while the Cayley and Exponential are just computed once and used throughout the RNN.**\n\nThanks for raising this point. We see how this can be misleading for the reader, and apologize. While auxiliary reflections are faster than normal reflections for RNNs, they are indeed not faster than previous $O(d^3)$ methods when the number of recurrent steps is larger than the hidden dimension. We apologize for not clarifying this and will update the paper accordingly.  \n\n**Reviewer: Experiment 3.3. How is the determinant computed? If it is computed every iteration using Lemma 1 this would make the parametrisation too expensive to use in general normalizing flows.**\n\nThanks for raising this point. In the case of *free* auxiliary reflection one can choose $W$ to be triangular so the determinant and inverse in Lemma 1 takes only $O(d^2bs)$ time to compute using ```torch.triangular_solve```. We apologize for not clarifying this. \n\n**Reviewer: The exponential map has been recently implemented in a very stable and fast way in PyTorch 1.7.0, getting some notable speed improvements over previous implementations. How does the method in this paper compare time-wise with this implementation?**\n\nThanks for raising this point. We found PyTorch 1.7.0 ```torch.matrix_exp```to be roughly 2.5 times faster than the code used in our experiments in the setting of 3.1 (see below). This means auxiliary reflections in the setting Section 3.1 remain roughly 6/2.5=2.4 times faster than the matrix experimental.  We emphasize that auxiliary reflections remain competitive even though PyTorch 1.7.0 was released after the submission deadline (October 27 https://github.com/pytorch/pytorch/releases/tag/v1.7.0 ). \n\n```\nimport torch\nimport time \nimport sys \n\n!git clone https://github.com/Lezcano/expRNN.git\n\nsys.path.append(\"expRNN/\")\nfrom expRNN.expm32 import expm32\n\nfor _ in range(10): \n  A = torch.zeros((28*28, 28*28)).uniform_().cuda()\n\n  t0 = time.time()\n  torch.matrix_exp(A)\n  torch.cuda.synchronize()\n  time_new = time.time() - t0\n\n  t0 = time.time()\n  expm32(A)\n  torch.cuda.synchronize()\n  time_old = time.time()-t0\n\n  print(\"%-10f | %-10f | %-10f \"%(time_new, time_old, time_old/time_new))\n\nOUTPUT:\n0.007900   | 0.019571   | 2.477500   \n0.007514   | 0.019000   | 2.528701   \n0.007491   | 0.019777   | 2.640018   \n0.007537   | 0.019465   | 2.582463   \n0.007529   | 0.018793   | 2.496168   \n0.007472   | 0.018633   | 2.493635   \n0.007544   | 0.018443   | 2.444834   \n0.007159   | 0.015911   | 2.222466   \n0.005733   | 0.015127   | 2.638431   \n0.005357   | 0.014661   | 2.736948\n```\n\n", "title": "Response"}, "ouUNBHKNWTq": {"type": "rebuttal", "replyto": "TRbw7y0kEr", "comment": "**Reviewer: My first concern with this paper is that it does not seem well motivated (at least from the perspective of a non-expert). It does not explain why orthogonal matrices are important.**\n\nThanks for raising this point, we actually discussed the motivation a lot during writing. In particular, a previous draft had the following more explicit motivation as the first paragraph in the introduction. \n\nPrevious Paragraph: *Orthogonal matrices have shown several benefits in deep learning, i.e., for a neural network $n(x)=W\\sigma(Vx)$ there are benefits to choosing $W,V$ to be orthogonal so $W^T=W^{-1}$ and $V^T=V^{-1}$. Recurrent Neural Networks can circumvent exploding and vanishing gradients with orthogonal transition matrices (Arjovsky et al, 2016). Normalizing flows admit an easy inverse for orthogonal matrices, $W^T=W^{-1}$, and circumvents the need to compute expensive Jacobian determinants (Berg et al, 2018).  Convolutional Neural Networks attain better generalization with orthogonal kernels (Bansal et al, 2018).*\n\nWe felt the previous paragraph got a bit too technical for an introduction, and decided to simplify the paragraph into the following: \n\nActual Paragraph: *Orthogonal matrices have shown several benefits in deep learning, with successful applications in Recurrent Neural Networks, Convolutional Neural Networks and Normalizing Flows.*\n\n**Question 1.** Do you believe the previous paragraph provides an adequate motivation? \n\n**Question 2.** Do you think the previous paragraph gets a bit too technical? If so, do you think it would be better if we elaborated on the benefits of only RNNs and moved the benefits with NF and CNN to related work?\n\n**Reviewer: \"In terms of experimental results, the authors report an improvement in validation error for a classification task on MNIST data for a few different network architectures. It would be nice to see more comprehensive experiments. In particular to see some results on datasets other than MNIST.**\n\n**Question 3.** We are not sure why the CIFAR10 experiment in Figure 5 doesn't show results on datasets other than MNIST? The figure demonstrates that one \"free\" auxiliary reflection attain similar validation nll to many reflections when training a generative model on CIFAR10. \n\n**Question 4.** It would be helpful if you could clarify what the objective of further experiments is. Further evidence for the claim that one auxiliary reflection attain similar performance to many normal reflections? Evidence that this claim extends to larger neural networks or more difficult datasets? Maybe something else?  \n\n**Reviewer: There are typo/grammatical errors in the paper's title and Figure 3 caption.**\n\n**Question 5.** The title was inspired by the the classical mathematics paper \"Six Standard Deviations Suffice\" [1]. We are happy to change it to \"One Reflection Suffices\" if this is what you are referring to. \n\n\n---\n\n[1] https://www.ams.org/journals/tran/1985-289-02/S0002-9947-1985-0784009-0/home.html", "title": "Response"}, "IDYagJtsYzL": {"type": "rebuttal", "replyto": "fV6dRl3XiQC", "comment": "Thanks for reviewing our paper and taking the time to share minor comments, we updated all of them.  \n\n**Reviewer: \"Neural networks represented through Householder reflections are one layer neural networks with an identical activation function. Is it correct? \"**\n\nNo. Consider a neural network $n(x)=W \\sigma (V x)$.  Previous work suggest choosing $V,W$ to be orthogonal, i.e., $V^T=V^{-1}$ and $W^T=W^{-1}$. Householder reflections is just one method to make sure $V^T=V^{-1}$. It can be used in everything from ResNets to U-Nets and **is not** limited to one layer neural networks, you can have as many layers as you want. Furthermore, the activation function **does not** need to be identical, it can be whatever you want it to be.   \n\n**Reviewer: \"In the current version, the motivation for me is not clear.\"**\n\nThanks for raising this point, we actually discussed the motivation a lot during writing. In particular, a previous draft had the following more explicit motivation as the first paragraph in the introduction. \n\nPrevious Paragraph: *Orthogonal matrices have shown several benefits in deep learning, i.e., for a neural network $n(x)=V\\sigma(Wx)$ there are benefits to choosing $W,V$ to be orthogonal so $W^T=W^{-1}$ and $V^T=V^{-1}$. Recurrent Neural Networks can circumvent exploding and vanishing gradients with orthogonal transition matrices (Arjovsky et al, 2016). Normalizing flows admit an easy inverse for orthogonal matrices, $W^T=W^{-1}$, and circumvents the need to compute expensive Jacobian determinants (Berg et al, 2018).  Convolutional Neural Networks attain better generalization with orthogonal kernels (Bansal et al, 2018).*\n\nWe felt the previous paragraph got a bit too technical for an introduction, and decided to simplify the paragraph into the following: \n\nActual Paragraph: *Orthogonal matrices have shown several benefits in deep learning, with successful applications in Recurrent Neural Networks, Convolutional Neural Networks and Normalizing Flows.*\n\n**Question 1.** Do you believe the previous paragraph provides an adequate motivation? \n\n**Question 2.** Do you think the previous paragraph gets a bit too technical? If so, do you think it would be better if we elaborated on the benefits of only RNNs and moved the benefits with NF and CNN to related work? \n\n**Reviewer: \"There is a phrase in the abstract \u201cthe only practical drawback is that many reflections cause low GPU utilization\u201d. The word \u201conly\u201d does not really motivate, does it mean that it\u2019s not that good in terms of accuracy?\"**\n\nWe just meant that the previous method is great, it solves everything you would want it to solve, it only has one downside, it makes everything run very slow. \n\n**Reviewer: \"What is actually the accuracy of such representations? \"**\n\n**Question 3.** We are not sure what you mean by accuracy? Different types of neural networks benefit from orthogonal weight matrices in different ways. This can often improve validation accuracy of classifiers, as done in (Arjovsky et al 2016, Bansal et al 2018), but it can also improve likelihood of generative models as in (Berg et al, 2018). \n\n**Reviewer: \"A lot of missed articles \"**\n\n**Question 4.** It would be very helpful if you could elaborate on which articles we missed, and whether this is with respect to related work or comparisons in our experiments. \n\n---\n\n(Arjovsky et al, 2016) https://arxiv.org/abs/1511.06464\n\n(Berg et al, 2018) https://arxiv.org/abs/1803.05649\n\n(Bansal et al, 2018) https://arxiv.org/abs/1810.09102", "title": "Response"}, "sYetR2BSBPs": {"type": "review", "replyto": "YtgKRmhAojv", "review": "#### Summary:\nConstraining weight matrices to be orthogonal is a useful but resource-intensive task in DL. This paper presents a simple yet powerful approach that shows the sequential Householder reflection method can be replaced by a single learned reflection to achieve orthogonality more efficiently.\n\n#### Pros:\n- The paper provides both theoretical (for the linear case) and empirical evidence supporting their claims.\n- Not being sequential in nature, the proposed method is more efficient on GPU.\n- Relaxation of the invertibility constraint is interesting. Please see the ControlVAE work on PID based annealing, there might be some connection.\n\n#### Cons:\n- While I understand that the method can utilize GPU much better, empirical evidence will certainly help. \n- Since it is claimed that other methods trade-off expressiveness for compute efficiency, a quantitative analysis will help. Also, as a result of the compute efficiency, can the current approach tackle larger datasets?\n\n ", "title": "Simple yet powerful approach.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "fV6dRl3XiQC": {"type": "review", "replyto": "YtgKRmhAojv", "review": "**Summary**:\nOrthogonal matrices $d\\times d$ can be represented using $d$ Householder reflections. This paper shows that it is sufficient and, of course, computationally faster to use only one reflection. \n\nOverall, I think the paper is not ready to be published yet. In the current version, the motivation for me is not clear. \n\n\n**Comments**:\n- It\u2019s hard to get the motivation to consider the presented approach from the paper. I would like to see a better and more detailed introduction on orthogonal matrices, its connection to neural networks. There is a phrase in the abstract \u201cthe only practical drawback is that many reflections cause low GPU utilization\u201d. The word \u201conly\u201d does not really motivate, does it mean that it\u2019s not that good in terms of accuracy?\n- What is actually the accuracy of such representations? \n- Neural networks represented through Householder reflections are one layer neural networks with an identical activation function. Is it correct?  \n- Do you have ideas for future works? \n\n\n**Minor**: \n- A lot of missed articles \n- \u201cMuch previous work attempt to alleviate the additional computational resources it requires to constrain weight matrices to be orthogonal.\u201d - There is a problem here. \n- Abstract: no commas before \u201cif\u201d\n- \u201cbecause the d reflections needs\u201c\n- \u201cIt is the evaluation of these sequential Householder reflection that cause\u201d\n- \u201callows us simplify\u201d -> to simplify \n- \u201cto train invertible neural network as generative models\u201d -> networks \n- \u201cNewtons method\u201d -> Newton\u2019s \n- \u201cPrevious work demonstrate\u201d\n- \u201cSection 2.3.2 then present\u201d\n- \u201cinvertibiliy\u201d\n- \u201cis simple connected\u201d\n- \u201ccontinously\u201d\n- \u201cwe finally arive\u201d\n- \u201ca single auxiliary reflections\u201d\n- \u201cwhere the 6orthogonal matrices where attained\u201d\n- \u201cThe RNNs with 1 auxiliary reflection attains\u201d\n", "title": "The motivation is not clear", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "TRbw7y0kEr": {"type": "review", "replyto": "YtgKRmhAojv", "review": "This paper presents a method for representing orthogonal weight matrices of neural networks by simulating an arbitrary number of Householder reflections using an additional neural network to compute a single auxiliary reflection. \n\nMy first concern with this paper is that it does not seem well motivated (at least from the perspective of a non-expert). It does not explain why orthogonal matrices are important. I am not an expert in this field and it is possible that this is obvious, but I think it could be better explained to convince a reader of the importance of the work. \n\nIn terms of experimental results, the authors report an improvement in validation error for a classification task on MNIST data for a few different network architectures. It would be nice to see more comprehensive experiments. In particular to see some results on datasets other than MNIST. \n\nThere are typo/grammatical errors in the paper's title and Figure 3 caption. \n\nWithout a better motivation and explanation of it is important, or more comprehensive experiments, I would recommend this paper be rejected. ", "title": "Why is this work important? ", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}