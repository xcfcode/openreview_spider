{"paper": {"title": "Impact of Representation Learning in Linear Bandits", "authors": ["Jiaqi Yang", "Wei Hu", "Jason D. Lee", "Simon Shaolei Du"], "authorids": ["~Jiaqi_Yang2", "~Wei_Hu1", "~Jason_D._Lee1", "~Simon_Shaolei_Du1"], "summary": "We show representation learning provably improves multi-task linear bandits.", "abstract": "We study how representation learning can improve the efficiency of bandit problems. We study the setting where we play $T$ linear bandits with dimension $d$ concurrently, and these $T$ bandit tasks share a common $k (\\ll d)$ dimensional linear representation. For the finite-action setting, we present a new algorithm which achieves $\\widetilde{O}(T\\sqrt{kN} + \\sqrt{dkNT})$ regret, where $N$ is the number of rounds we play for each bandit. When $T$ is sufficiently large, our algorithm significantly outperforms the naive algorithm (playing $T$ bandits independently) that achieves $\\widetilde{O}(T\\sqrt{d N})$ regret. We also provide an $\\Omega(T\\sqrt{kN} + \\sqrt{dkNT})$ regret lower bound, showing that our algorithm is minimax-optimal up to poly-logarithmic factors.  Furthermore, we extend our algorithm to the infinite-action setting and obtain a corresponding regret bound which demonstrates the benefit of representation learning in certain regimes. We also present experiments on synthetic and real-world data to illustrate our theoretical findings and demonstrate the effectiveness of our proposed algorithms.", "keywords": ["linear bandits", "representation learning", "multi-task learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper studies the representation learning problem in the linear bandit setting, where each bandit \"task\" shares a common low-dimensional representation. The paper introduces a novel algorithm, it provides theoretical regret guarantees, and it illustrates the effectiveness of the proposed method in a number of experiments.\n\nThere is a general agreement among the reviewers about the relevance of the problem and the contribution of the paper. The authors properly addressed concerns about the novelty (e.g., comparison with linear bandit and low-rank structure) and about the underlying assumptions. Although some of them do seem relatively strong (and in some cases stronger than the state-of-the-art in bandit, such as the distribution on the contexts), it is indeed non trivial to understand whether such assumptions can be easily relaxed in the representation learning context. \n\nThe novelty of the algorithm is more on the specific problem and set of assumptions, but it mostly relies on known principles (e.g., using method-of-moment for estimating the underlying representation). In this sense, I see this paper more as a useful addition to the fast growing landscape of representation learning methods in online learning, rather than a breakthrough. Also, the structure of the algorithm seems very \"theoretical\" in nature, since the explore-than-commit approach is very rarely a good strategy in practice. \n\nAnother issue the authors clarified in their revised submission is the actual improvement obtained in the bounds depending on the parameters T, k, d, N. In this respect, I still would like to encourage the authors to further illustrate the regime where the bound is actually better than for the single-task approach. For instance, they could consider N fixed to a convenient value and produce a plot with x-axis T and y-axis the regret bound and report different curves for varying values of k and d. This would further clarify to the reader when representation learning can *provably* improve over plain single-task learning.\n\nOverall, given the general support from the reviewers and the revised version of the paper, I consider this contribution is significant enough to propose acceptance. As mentioned above, I believe it will serve as a reference for developing further the literature in this domain."}, "review": {"CIQ8gnM2ma": {"type": "review", "replyto": "edJ_HipawCa", "review": "This paper studies the benefits of learning a low-rank feature extractor in multi-task linear bandits. Specifically, the paper studies the setting where an unknown common linear feature extractor $B \\in R^{d \\times k}$ maps the original $d$-dimensional contexts $x$ to a $k$-dimensional representation. Essentially, for multi-task linear bandit problem $r_t = \\theta_t^T x$, this paper assumes the matrix of model parameters $\\Theta \\in R^{d\\times T}$ is low-rank and be factorized as $\\Theta = BW$ with rank k. The paper proposed algorithms to estimate both $B$ and $W$ in finite actions setting and infinite actions setting. In finite action setting, the proposed solution is a greedy algorithm while in infinite actions setting, the proposed solution is a explore-the-commit method. Theoretical result shows that the regret is $O(T\\sqrt{kN} + \\sqrt{dkTN})$ and matches the lower bound. Simulation result shows that the algorithm outperforms baselines running independent linear bandit algorithms. \n\nPros:\n1) The problem studied in this paper is interesting and important. It is a well-known concern that linear bandits has a $O(d)$ regret for infinite actions and $O(\\sqrt{d})$ regret for finite actions, which is too large for practical problem. Many recent studies aim to address this problem from varies perspectives, such as low-rank structure, sparsity, etc. The authors proposes to estimate a low-rank feature extractor for multi-task linear bandits to reduce the regret.\n\n2) The paper is generally well-written and easy to follow (only a few confusing descriptions, mentioned below). The required assumptions are presented and discussed clearly.\n\n3) The analysis of regret upper bound and lower bound are good contributions.\n\n\nCons:\n1) The paper introduces a very strong assumption (Assumption 2) that the context features are sampled from Gaussian  (a stochastic context setting), and is a serious limitation of this paper. Most linear bandits research focus on the adversarial contexts setting, where the solution can also solve bandits with stochastic context but not the other way around. The gaussian feature assumption suggests that arms are equally informative and thus the classical exploration-exploitation dilemma does not exist.  It is not clearly discussed in the paper that why this paper must be limited to the stochastic feature setting. The paper heavily follows Theoretical results from [1] and both adversarial and stochastic contexts settings are discussed in [1]. What is the challenge to propose a solution for adversarial contexts, for example a linear UCB based solution? \n\n2) It seems unclear on how to estimate both $B$ and $W$ and what is the estimation quality. According to the description the algorithm is doing an explicit matrix factorization. Since matrix factorization does not have a closed form solution, what is the guarantee of the estimation quality? In Lemma 2 the analysis requires the estimated $\\hat B$ and $\\hat W$ minimize the square loss so that the square loss with estimated $\\hat B$ and $\\hat W$ is smaller than the square loss with true $B$ and $W$(the first step of derivation)-- is this guaranteed to be achievable?\n\n3) The authors argued that the problem of learning a low-rank feature extractor has not been studied in the bandit setting before, which seems incorrect: [2] studies a very similar problem that tries to estimate a hidden projection matrix for linear bandits with low-rank structure. In [2] the project matrix is estimated by PCA.  I strongly suggest the authors to compare this paper with [2]. One difference is that in [2] the low-rank structure is about the features (thus can be directly estimated from the features), while in this paper the low-rank structure is about the parameters $\\Theta$. One potential advantages I can think of is this paper achieves regret in $O(\\sqrt{k})$ while [2] has regret in $O(k)$ -- is it because of the assumption on features are sampled from gaussian? \n\n\nOther questions:\n\n1) What is the baseline naive algorithm in Figure 1? It seems to be linear regression + greedy strategy and I would suggest the authors to clarify it.\n\n2) Is rank $k$ assumed to be an input to the algorithm? What if the algorithm does not know it or have a wrong knowledge of $k$?\n\n\n==========================================================================================\n\nWhile I still hold my concern on the i.i.d. assumption of the context as it is less interesting both practically and theoretically, the author response and the revised paper clearly resolve my other questions and concerns.  I am increasing my score to 6.\n\n\nReferences:\n[1] Han, Yanjun, Zhengqing Zhou, Zhengyuan Zhou, Jose Blanchet, Peter W. Glynn, and Yinyu Ye. \"Sequential Batch Learning in Finite-Action Linear Contextual Bandits.\" arXiv preprint arXiv:2004.06321 (2020). \n[2] Lale, Sahin, Kamyar Azizzadenesheli, Anima Anandkumar, and Babak Hassibi. \"Stochastic linear bandits with hidden low rank structure.\" arXiv preprint arXiv:1901.09490 (2019).", "title": "An interesting problem but has several limitations", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "THoN1ZjAQfD": {"type": "review", "replyto": "edJ_HipawCa", "review": "This article provide a theoretical analysis of the impact of learning a shared low-rank representation in multi-task linear bandits.\nTwo types of linear bandits are considered: the finite actions and the continuous actions settings.\nFor the finite-actions setting the authors propose an algorithm called MLinGreedy and provide a theoretical analysis with a tight minimax regret bound (Theorem 1 & 2). For the continuous-actions setting they propose an \"explore-then-explored-then-commit\" algorithm called E2TC. They provide an upper-bound for the regret (Theorem 3), and a lower bound (Theorem 4). A gap remains between upper and lower regret bounds for this setting.\nTheses bounds are roughly constituted of two parts: one part for the cost of learning the low-rank representation and one part which correspond to the cost of a linear bandit on a perfect low-rank representation.\nIn section 6 the authors provide two experiments for the finite actions setting. One experiment on a synthetic environment, another which is simulated-from \"real-life\" data. These experiments underline the advantage of learning a shared low-rank representation against a baseline where that tasks are considered independently. It does not expose situations where, according to the bounds, it should be at disadvantage.\n\nIf we omit the numerous typos that I listed at the end of this review, I found the paper clear and well written.\nI did not have time to delve deeply into all the demonstrations, but the proof of Theorem 1 seems solid.\nAs explained on page 5 it relies on three lemma. Lemma 2 give guarantees on the low-rank approximation. It  is based on a Hoeffding concentration inequality combined with an epsilon-net argument. Lemma 3, borrowed from (Han et al., 2020), and Lemma 4 show that this estimate works well with the doubling schedule of MLinGreedy.\n\n## Pro:\n- The paper is well written\n- The maths seem solid\n- The results give a theoretical insight on the impact/benefit of representation learning in the specific case of linear bandits.\n\n## Con:\n- The multi-task linear bandit models rely on several assumptions and simplifications which obfuscate its realism. These assumptions are however justified thoroughly on page 3 and 4.\n- I did not like the bias toward improvement of the introduction: for instance, according to the bound, if k is in Omega(d), the cost of learning the low-rank matrix is linear in d which results in a regret which is worse than the one of the \"naive\" approach for large dimensions. The authors should not be afraid to develop on the cases where the shared representation does not improve, it will not devaluate the significance of their work.\n\n## Minor remarks & typos:\np1: I am not a fan of the \"Provable benefits of\" phrase in the title. This work is more about the impact of shared low-rank representation and when it can improves from the naive, but much simpler, independent-tasks approach. As mentioned on page 5 it requires T to be in Omega(k) to improve. \"Impact of\" would be more appropriate.\np1: \"In this paper, we\" -> \"We\"\np2: The \\mathbold{x}_{n,t,a_{n,t}} notation is heavy and  not really needed. As mentioned on the bottom of page 3 one may interchangeably use \\mathbold{a}_{n,t} and \\mathbold{x}_{n,t,a_{n,t}} to refer the same action, so I would get rid of the heaviest notation.\np3: \"line of work analyzed\" -> \"line of work that analyzed\"\np3: \"as our algorithm\" -> \"with our algorithm\"\np3: \"In this paper, we\" -> \"We\"\np4: \"In this assumption\" -> \"With this assumption\"\np4: \"for at each task\" -> \"for each task\"\np4: \"In each round\" -> \"At each round\"\np4: \"task are sample from\" -> \"task are sampled from\"\np5: \"up to an constant error\" -> \"up to a constant error\"\np5: \"we use an method-of\" -> \"we use a method-of\"\np6:  \"value decomposition of \\hat{B}\" -> \"value decomposition of \\hat{M}\"\np7&8: the theoretical bounds should appear on Figure 2 and 4.\np8: Replacing the Time Horizon n with the number of tasks T as on Figure 2 would be more informative.\n", "title": "A theoretical study of the impact of a shared low-rank representation for multi-task linear bandits", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "4lh2vQfrG9C": {"type": "rebuttal", "replyto": "yrifL6eNBZG", "comment": "We thank AnonReviewer2 for the positive view and valuable suggestions.  Please find our response to the comments below.\n\n1. \u201cIt would have been better if the paper could throw some light on other variants of representation learning.\u201d:\n\nWe appreciate your advice and we have added a paragraph in the conclusion on representation learning with a general function class for the feature extractor.\n\n2. \u201cLinear bandit is quite popular news/ad recommendation systems\u2026...\u201d:\n\nThanks for the suggestion! We will try to implement our algorithms on datasets of recommendation.\n\n3. \u201cAssumption 2 is a quite strong assumption to make.\u201c\n\nWe believe it is quite challenging to relax Assumption 2 to the adversarial contexts setting. One central challenge for the upper bound is that existing analyses for multi-task representation learning requires i.i.d. inputs even in the supervised learning setting. Another challenge is how to develop a confidence interval for an unseen input in the multi-task linear bandits setting. This confidence interval should utilize the common feature extractor and is tighter than the standard confidence interval for linear bandits, e.g. LinUCB. We have added this discussion in the conclusion (**Adversarial Contexts** paragraph).\n\n4. Related work:\n\nWe have cited the suggested related work in the introduction. Thanks for mentioning this work.\n", "title": "Response to AnonReviewer2"}, "xiVmhNJE1pr": {"type": "rebuttal", "replyto": "edJ_HipawCa", "comment": "We thank all the reviewers for the comments and suggestions! We have revised our paper accordingly. The main changes are shown in red fonts in the revised paper and are summarized as follows.\n\n* Change the wording in the title.\n* Section 1: Add a discussion on the relative orders of $d, k, N, T$.\n* Section 2: Add extra related work on low-rank structure in linear bandits.\n* Section 6: Add the description of the baseline (aka. naive) algorithm.\n* Section 7: Add discussions on some future directions, including extending our results to the adversarial context setting and the generalized non-linear reward function setting. \n* Fix the typos.\n", "title": "Paper Revision"}, "vPOS0pWDc4k": {"type": "rebuttal", "replyto": "ib6L7Yvh8U7", "comment": "We thank AnonReviewer3 for the valuable comments and suggestions on our paper.  Please find our response to the comments below.\n\n\n1. Is it possible to allow for adversarially chosen context in the finite-action setting?\n\nThanks for asking. We\u2019ve added a paragraph in the conclusion on this setting (**Adversarial Contexts** paragraph). This is indeed a challenging problem. One central challenge for the upper bound is that existing analyses for multi-task representation learning requires i.i.d. inputs even in the supervised learning setting. Another challenge is how to develop a confidence interval for an unseen input in the multi-task linear bandits setting. This confidence interval should utilize the common feature extractor and is tighter than the standard confidence interval for linear bandits, e.g. LinUCB.\n\n\n2. Intuitively, what is the reason that we do not need explicit exploration in MLinGreedy? Is there any good motivating example showing that Assumption 2 is likely to be true in practice?\n\nThe intuition is that Assumption 2 guarantees that by choosing the greedy action, the algorithm could explore other directions while exploiting the near-optimal direction $\\hat{\\theta}$. This observation has been used in some previous work, such as Han et al. (2020). Assumptions similar to our Assumption 2 also appeared in\nBastani, Hamsa, Mohsen Bayati, and Khashayar Khosravi. \"Mostly exploration-free algorithms for contextual bandits.\" Management Science (2020).\nIn which the motivating example is in healthcare domains.\n\n\n3. \u201cIt would be better to give a concrete motivating example at the beginning to give the readers a better idea of what $N, T, d, k$ look like order-wise.\u201d\n\nThanks for the suggestion! We\u2019ve added a motivating example about the orders in the introduction.\n\n\n4. \u201cThere's a gap between upper and lower bound in the infinite-action setting, but this is acceptable as a conference submission.\u201d\n\nWe\u2019ve left it as an open problem for bridging this gap in Section 5.\n\n5. Typos:\n\nThanks for pointing out. We\u2019ve fixed them accordingly.\n", "title": "Response to AnonReviewer3"}, "TBM-tw2czHw": {"type": "rebuttal", "replyto": "hKgpj03cEco", "comment": " We thank AnonReviewer1 for the positive review and the valuable comments on our paper. Below we address the cons you raised. \n\n1. Some assumptions seem very strong compared to the general linear bandits. Is this fair to compare your result with naive algorithm?\n\nFirst, we would like to clarify that our Assumptions (3, 4) are only used when studying the infinite-action setting. They are not used for the finite-action setting. Second, for the infinite-action setting, the naive algorithm in our paper is to play $T$ linear bandits tasks independently. Since it plays independently, it cannot benefit from Assumption 4. \n\n2. It is strange that using $T$ as the number of bandit tasks:\n\nWe admit that $T$ is commonly used in bandits literature as the time horizon. However, $T$ is also commonly used in representation learning and multi-task learning literature as the number of tasks. So there's a conflict of notations between these two fields. To resolve it, we used $T$ as the number so as to be in line with the representation learning literature and we used $N$ to refer to the time horizon.\n", "title": "Response to AnonReviewer1"}, "ZMyumWO8OQe": {"type": "rebuttal", "replyto": "CIQ8gnM2ma", "comment": "We thank AnonReviewer4 for the valuable comments and suggestions. Please find our response to the comments below.\n\n\n1. \u201cThe paper introduces a very strong assumption (Assumption 2) \u2026\u201d\n\nOne central challenge for the upper bound is that existing analyses for multi-task representation learning requires i.i.d. inputs even in the supervised learning setting. Another challenge is how to develop a confidence interval for an unseen input in the multi-task linear bandits setting. This confidence interval should utilize the common feature extractor and is tighter than the standard confidence interval for linear bandits, e.g., LinUCB. We have added this discussion in the conclusion (**Adversarial Contexts** paragraph).\n\n\n2. \u201cWhat is the guarantee of the estimation quality of $\\hat{B}, \\hat{W}$?....\u201d\n\nNote $(\\hat{B}, \\hat{W})$ is the minimizer of the empirical loss. We directly analyzed the statistical property of $(\\hat{B}, \\hat{W})$, and we measure the estimation quality in terms of the reconstruction of the linear predictors ($\\Theta = BW$): $\\lVert \\hat{B}\\hat{W} - BW\\rVert_F^2$. See Lemma 4.\n\n3. Connection with [2]:\n\nWe appreciate you for pointing out the related paper. The main difference is that [2] assumed hidden feature extractor among actions, while we assumed the hidden feature extractor among tasks. Still, our Algorithm 2 has some similarities with [2], in that both used Davis-Kahan $\\sin \\Theta$ theorem to recover and exploit the low-rank structure.\n\nRegarding your question on the extra $O(\\sqrt k)$ factor in the regret bound in [2], we believe it is mainly due to that [2] used a confidence ball similar to that constructed by [3], resulting in an extra factor depending on the dimension. For finite-action linear bandit, the analysis in [4] is more commonly used to obtain a tighter confidence interval.\n\n\n4. What is the baseline naive algorithm in Figure 1?\n\nIt's linear regression + greedy strategy as you correctly pointed out. We have updated the paper as well.\n\n5. What if the algorithm does not know it or have a wrong knowledge of $k$?\n\nWe assumed $k$ to be an input to the algorithm. If $k$ would be unknown, the algorithm could use an upper bound $k\u2019$ of $k$, and the regret bound will degrade gracefully (replacing $k$ by $k\u2019$ in the regret bound).\n\nRefs: \n[3] Abbasi-Yadkori, Yasin, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. \"Improved algorithms for linear stochastic bandits.\" Advances in Neural Information Processing Systems. 2011. \n[4] Chu, Wei, et al. \"Contextual bandits with linear payoff functions.\" Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. 2011.\n", "title": "Response to AnonReviewer4"}, "HifzRMY0gzQ": {"type": "rebuttal", "replyto": "THoN1ZjAQfD", "comment": "We thank AnonReviewer5 for the detailed comments and suggestions. Please find our response to the comments below.\n\n1.  \u201cThe multi-task linear bandit models rely on several assumptions and simplifications \u2026.\u201d\n\nFor theoretical developments, some assumptions and simplifications are necessary. We would like to note that these assumptions have appeared in previous bandits / representation learning theory literature. We have also added a paragraph in the conclusion on designing a robust algorithm which achieves our regret bounds when these assumptions are satisfied and gracefully reduces to the standard regret bounds of $T$ linear bandits when the assumptions do not hold.\n\n2. \u201cI did not like the bias toward improvement of the introduction.....\u201d\n\nThanks for the suggestion! This paper focuses on the positive impact of representation learning. We have added a discussion in the **robust algorithm** paragraph in conclusion.\n\n3. Change of title:\n\nWe have changed the title according to your suggestion. Thanks. \n\n4. Typos:\n\nWe thank you for pointing out the typos in our paper. We have fixed them accordingly. Besides, we will add the theoretical bounds to Figures 2 and 4, and replace $T$ with $N$ in Figure 4 in our next revision.", "title": "Response to AnonReviewer5"}, "yrifL6eNBZG": {"type": "review", "replyto": "edJ_HipawCa", "review": "- Pros.\n   -  Learning the representation is crucial to model an efficient linear MAB. However, there is lot of room to include the characteristic of the learned representation into theoretical guarantee. This paper throws a light on this interesting problem.\n   - The paper is well-written and the bounds look convincing. I have not gone through the detailed derivations (in the appendix), but the overall idea looks good.\n\n- Cons.\n   -  It would have been better if the paper could throw some light on other variants of representation learning.\n   -  Linear bandit is quite popular news/ad recommendation systems. However, posing hand-writing recognition on MNIST data as linear bandit seems to be unnatural. There are DNN based approaches that solve the problem with a great accuracy. It will be interesting to see how does the algorithm perform on a real data set of news/ad recommendation.\n   - Assumption 2 is a quite strong assumption to make.\n   \nI believe the exiting work: A Contextual-Bandit Approach to Personalized News Article Recommendation by Li et al (2010) deserves a citation in this paper.\n", "title": "The paper presents small step as an advancement towards a much needed theoretical support for a popular practice of representation learning in linear bandits. The authors support their findings by experimental evaluations.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ib6L7Yvh8U7": {"type": "review", "replyto": "edJ_HipawCa", "review": "*Summary*\n\nThis paper theoretically studies the benefits of representation learning in linear bandit problems. The key assumption is the existence of a common linear feature extractor. Two different setting are studied. In the finite-action setting, the authors provide the MLinGreedy algorithm that achieves matching upper and lower bounds (up to polylog factors). In the infinite-action setting, the authors provide the $E^2TC$ algorithm that can achieve lower regret than the naive method when the number of tasks is large. Experiments on both synthetic and real-world data are conducted, which confirm the theoretical results.\n\n*Assessment*\n\nOverall I'm leaning towards acceptance. Representation learning in sequential decision making problems is an important problem that many readers of ICLR will care about, and it is valuable to offer some theory insights into this problem. Besides, the paper is overall clearly written and enjoyable to read. Still I have some questions regarding the assumptions that the theoretical results are based on (see questions).\n\n*Questions/Comments*\n- The context vectors $x_{t,a}$ can be arbitrarily chosen in LinUCB (Chu et al. 2011). Is it possible to allow for adversarially chosen context in the finite-action setting?\n- Intuitively, what is the reason that we do not need explicit exploration in MLinGreedy? I assume it has something to do with Lemma 3, which seems to rely on Assumption 2 ($\\lambda_{\\min}(\\Sigma_t) \\ge \\Omega(1/d)$). Following the previous question, Assumption 2 looks rather strong; is there any good motivating example showing that Assumption 2 is likely to be true in practice?\n- It would be better to give a concrete motivating example at the begining to give the readers a better idea of what $N, T, d, k$ look like order-wise.\n- There's a gap between upper and lower bound in the infinite-action setting, but this is acceptable as a conference submission.\n\n*Minor comments*\n- Page 4, first line: $[w_1, \\dots, w_T]$ should be in bold.", "title": "Official Blind Review #3", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "hKgpj03cEco": {"type": "review", "replyto": "edJ_HipawCa", "review": "Summary:\n\nThis paper introduced the representation learning techniques into the linear bandits and showed that representation learning could improve the regret bound when multiple tasks shared a common low dimensional linear representation. The authors also proved a lower bound and extended the algorithm to the infinite-action setting. They also present experiments to validate their theoretical findings.\n\nPros:\n(1) The idea of representation learning + linear bandits is very interesting.  Their results are also impressive which showed that combination will be better than naive algorithm. \n(2) The paper is well written and easy to read. Their proof part looks good.\n\nCons:\n The assumptions (1,3,4) are based on the representation learning setting. Some of them seems very strong compared to the general linear bandits, e.g.,  assumption 4. Is this fair to compare your result with naive algorithm ? The naive algorithm could have better regret bound under the same setting. e.g., if the source tasks are diverse enough, the linear bandits will become much easier.\n\nMinor Comments:\nThe author should use the better notations for bandit setting. It is strange that using T as the number of bandit tasks since T usually will be used as the time horizon of the experiments.\n\n", "title": "Good combination with representation learning and bandit", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}