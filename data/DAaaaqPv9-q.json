{"paper": {"title": "Self-supervised Graph-level Representation Learning with Local and Global Structure", "authors": ["Minghao Xu", "Hang Wang", "Bingbing Ni", "Hongyu Guo", "Jian Tang"], "authorids": ["~Minghao_Xu1", "~Hang_Wang1", "~Bingbing_Ni3", "~Hongyu_Guo1", "~Jian_Tang1"], "summary": "This work seeks to learn the local-instance and global-semantic structure of a set of unlabeled graphs.", "abstract": "This paper focuses on unsupervised/self-supervised whole-graph representation learning, which is critical in many tasks including drug and material discovery. Current methods can effectively model the local structure between different graph instances, but they fail to discover the global semantic structure of the entire dataset. In this work, we propose a unified framework called Local-instance and Global-semantic Learning (GraphLoG) for self-supervised whole-graph representation learning. Specifically, besides preserving the local instance-level structure, GraphLoG leverages a nonparametric strategy to learn hierarchical prototypes of the data. These prototypes capture the semantic clusters in the latent space, and the number of prototypes can automatically adapt to different feature distributions. We evaluate GraphLoG by pre-training it on massive unlabeled graphs followed by fine-tuning on downstream tasks. Extensive experiments on both chemical and biological benchmark datasets demonstrate the effectiveness of our approach. ", "keywords": ["Self-supervised Representation Learning", "Graph Representation Learning", "Hierarchical Semantic Learning"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a self-supervised learning method for learning representations for graph-structured data, with both local and global objectives. The local objective aims to maximize the mutual information between two correlated graphs generated with attribute masking [Hu et al. 19], with the InfoNCE loss [van den Oord et al. 18], and the global objective aims to cluster the graphs using the RPCL [Xu et al. 93] objective, which pulls the sample toward the closest cluster while pushing it away from the rival clusters. The proposed method is validated on standard graph classification benchmarks by training a linear classifier on top of the GNN pre-trained with it, and the results show that it largely outperforms existing graph pre-training methods. \n\nThis paper fell into a borderline case, receiving split reviews with two of the reviewers learning toward rejection, and two others proposing to accept. The reviewers in general agreed that the experimental validation is thorough (except for one reviewer), and some of the reviewers mentioned that the proposed idea of performing self-supervised learning at both local and global level makes sense. However, the negative reviewers were concerned with the limited novelty of the proposed method, since the proposed method seems like a simple combination of two objectives each of which are based on existing ideas (although the latter has not been explored for GNN pre-training). The reviewers had interactive discussions with the authors, and the authors provided detailed feedback. Yet, the reviewers were not convinced that the method has sufficient novelty to warrant publication even after the internal discussion period, and decided to keep their negative ratings.\n\nI believe that this is a simple yet effective pre-training method for GNNs on graph-structured data. The proposed method of combining the local and global objective seems like a promising solution to learn a metric space that well-captures the graph-level similarity and also is well-separated for discriminative classification, and it may have some practical impact given its good performance on benchmark datasets. However, as the two negative reviewers mentioned, the paper in its current form is presented as a simple combination of existing approaches. The local objective is a slight modification of attribute masking strategy of [Hu et al. 19], and the global objective of clustering has been explored in self-supervised learning of CNNs for image data [Asano et al. 20]. Thus, I lean toward rejecting the paper, considering its relative novelty and quality. \n\nHowever, I find the proposed work highly promising, and encourage the authors to further develop the method while also improving on the paper writing. I suggest the authors to focus more on the main idea of learning with both local and global objectives, without specifically tying each objective to any of the existing methods. The authors may consider various techniques for both local and global objectives (such as hinge loss-based contrastive loss with k-means clustering as shown in the response to R3), and suggest the proposed work as a more general framework.  \n\n[Asano et al. 20] Self-Labeling via Simultaneous Clustering and Representation Learning, ICLR 2020"}, "review": {"lVaICbG3nWr": {"type": "rebuttal", "replyto": "sV_pbElOkH", "comment": "Thanks for your insightful feedback!\n\nExactly as you said, the main contribution of this work lies in modeling the global-semantic structure of graph embeddings via clustering different graphs in a hierarchical fashion. To the best of our knowledge, it is the first attempt to explore the semantic structure of a set of graphs in an unsupervised/self-supervised way. In the proposed GraphLoG framework, this global clustering is established on the local instance-level structure which is an essential prior guarantee for the subsequent clustering. \n\nIn summary, this work embodies its novelty mainly on the global-semantic learning part, and also possesses the contribution of unifying both the local-instance and global-semantic learning into a single framework. \n", "title": "Official Response to AnonReviewer3"}, "FtDCBf4RY8y": {"type": "rebuttal", "replyto": "jR0UpSK7Mzg", "comment": "Thanks for your appreciation of our work!\n\nWe further respond to your question as follows:\n\nQ: Can randomly initialized prototypes also work in the proposed GraphLoG model?\n\nA: The results in **Section E.4** mainly illustrate that, when the hierarchical prototypes are initialized with different clustering initialization (i.e. different initial candidate cluster centers in the RPCL algorithm), the performance of the pre-trained model on downstream tasks is not affected so much. \n\nHowever, if we directly use the initial candidate cluster centers as the initial prototypes without conducting RPCL clustering, these prototypes cannot represent the local instance-level structure established by local-instance learning and thus derive random semantic structure that does not match with the graphs in the dataset, which will hurt the GraphLoG model\u2019s ability of producing graph embeddings with meaningful semantic structure. Because of the limitation of time in the discussion period, the experimental verification of this point has not been done, and we will finish it as soon as possible. \n", "title": "Official Response to AnonReviewer4 "}, "LqJCuONt-zR": {"type": "rebuttal", "replyto": "w3qHnI1IXyh", "comment": "Thanks for your insightful feedback and careful review of the revised paper!\n\nWe further respond to your questions as follows:\n\nQ1: In many datasets, the number of categories is limited, which makes it hard to construct hierarchical prototypes with effective structure.\n\nA1: As you said, some datasets in MoleculeNet (i.e. BBBP, HIV and BACE) contain only one binary classification task and thus two categories for constructing bottom layer prototypes. Therefore, for implementing the sup-GraphLoG model on various datasets, we use different depths of hierarchical prototypes, as listed in Table A. We would like to point out that, in the proposed model, the complexity of such semantic hierarchy is determined by the tasks studied in various datasets, *i.e.* the structure of hierarchical prototypes is task-specific. As a result, although the semantic hierarchy is quite simple for some datasets (containing only one hierarchy), the hierarchical prototypes are able to refine the structure of graph embeddings according to the tasks studied in each dataset.\n\nTable A: The depth of hierarchical prototypes for different datasets in MoleculeNet.\n \n|Dataset|BBBP|Tox21|ToxCast|SIDER|ClinTox|MUV|HIV|BACE|\n|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|Number of binary classification tasks|1|12|617|27|2|17|1|1|\n|Depth of hierarchical prototypes|1|2|4|3|2|2|1|1|\n\nQ2: How can the correlated graph pairs obtained by attribute masking effectively constrain the structure of graph embeddings?\n\nA2: We think the global-semantic loss (**Eq. (13)**) plays a critical role in constraining the global structure of graph embeddings. In specific, it can push the embeddings of the graphs with different structures but potentially similar semantics towards the same prototypes, such that the graph embeddings within a semantic cluster are embedded more compactly. Such statement is also demonstrated by the t-SNE visualization results on ZINC15 database (**Figure 3**), in which the embeddings of different molecules form more compact clusters after applying the global-semantic loss $\\mathcal{L}_{global}$.\n", "title": "Official Response to AnonReviewer2"}, "Y38wFBmjJrW": {"type": "review", "replyto": "DAaaaqPv9-q", "review": "This paper proposes an unsupervised framework to perform graph representation learning. The local-instance structure is learned by first gets patch-level and graph-level representations for each graph, then maximize the mutual information between both correlated patches and correlated graphs, which are decided by attribute masking strategy. The global-semantic structure is maintained by leveraging RPCL to derive hierarchical prototypes of the representation and maximizing the mutual information between correlated graph representation and the searching path in the prototypes.\n\nStrengths:\n+  This paper presents a framework to jointly consider the local instance structure and global-semantic structure of graphs. It is a meaningful direction and could be beneficial for explainability.\n+  The experimental results are quite thorough with comparisons to several baseline methods. Moreover, the ablation study of different mechanisms is provided in the experiments.\n\nWeaknesses:\n-  The proposed model seems like a simple combination of several existing techniques and thus lacks novelty.\n-  The performance of this model seems to heavily rely on the attribute masking strategy as all the operations are built upon the correlated graph pairing from the attribute masking strategy. But how reliable is this technique? It seems to be a bottleneck of the model, and I think there should be an explanation on this either theoretically or experimentally.\n\nOverall, the proposes a reasonable model for learning hierarchical graph representations. However, the novelty is limited since the proposed method seems like a simple combination of several existing techniques.\n\nQuestions:\n\n1.  As I mentioned earlier, I wonder how reliable the attribute masking strategy is. As graph matching is an extremely hard problem, can this strategy provide a reliable pairing between correlated graphs?\n2.  It is not clear how to leverage prototypes in classification tasks? I understand that the prototypes serve to ensure a better structure of the embeddings, but when classifying graphs, I wonder whether embeddings and prototypes are both used or not?\n3.  In the Constraint for the global-semantic structure part, the loss for a graph embedding includes both the representations for its correlated graph and its searching path consisting of several prototypes. When minimizing the loss, I wonder both representations of the correlated graph and prototypes are updated together? Or the prototypes are only updated in eq. (11) and (12).\n", "title": "Official Blind Review #2", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "y49jRnRy45N": {"type": "rebuttal", "replyto": "FaFBw3mrOOB", "comment": "Thanks for your feedback!\n\nWe would like to clarify that the core idea of this work (local-instance and global-semantic learning) can achieve superior performance without the help of existing effective techniques:\n\n1. First, compared with previous methods for self-supervised graph representation learning (e.g. Edge Prediction [a], InfoGraph [b], Context Prediction [c], etc.), the combination of the GraphLoG model with vanilla model components (i.e. hinge-loss-based contrastive loss, K-means clustering), denoted as **GraphLoG-vanilla**, can also achieve superior performance on the biological downstream task, as shown in Table A. This phenomenon illustrates that the GraphLoG model itself can obtain decent performance gain and surpass the state-of-the-art approaches.\n\nTable A: The performance comparison among different self-supervised methods on the biological function prediction benchmark.\n\n|Method|ROC-AUC (%)|\n|:----:|:----:|\n|EdgePred [a]|$70.5\\pm0.7$|\n|InfoGraph [b]|$70.7\\pm0.5$|\n|AttrMasking [c]|$70.5\\pm0.5$|\n|ContextPred [c]|$69.9\\pm0.3$|\n|GraphPartition [d]|$71.0\\pm0.2$|\n|GraphLoG-vanilla|$\\textbf{72.2}\\pm0.4$| \n\n2. Second, we apply the idea of local-instance and global-semantic learning to the supervised setting and use the vanilla model components (i.e. hinge-loss-based contrastive loss, K-means clustering), which is a plain version of the sup-GraphLoG model in **Section 4**, denoted as **sup-GraphLoG-vanilla**. This vanilla supervised model outperforms the Graph Isomorphism Network (GIN) [e] with a clear margin on the biological function prediction benchmark, as shown in Table B.\n\nTable B: The performance comparison between two supervised models on biological function prediction benchmark.\n\n|Method|ROC-AUC (%)|\n|:----:|:----:|\n|GIN [e]|$64.8\\pm1.0$|\n|sup-GraphLoG-vanilla|$\\textbf{66.9}\\pm0.7$|\n\n\n[a] Kipf, Thomas N., and Max Welling. \"Variational graph auto-encoders.\" arXiv:1611.07308 (2016).\n\n[b] Sun, Fan-Yun, et al. \"Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization.\" ICLR, 2020.\n\n[c] Hu, Weihua, et al. \"Strategies for Pre-training Graph Neural Networks.\" ICLR, 2020.\n\n\n[d] You, Yuning, et al. \"When Does Self-Supervision Help Graph Convolutional Networks?.\" ICML, 2020.\n\n[e] Xu, Keyulu, et al. \"How powerful are graph neural networks?.\" ICLR, 2019.\n", "title": "Official Response to AnonReviewer3"}, "FAm9-78C9-t": {"type": "rebuttal", "replyto": "jSgse61B9ao", "comment": "Thanks very much for your recognition in our work! \n \nWe address your two concerns as follows: \n\nQ1: The number of prototypes determined by RPCL cannot be adjusted during training.\n\nA1: In the **Section E.3** of appendix, we design an adaptive variant of RPCL clustering (Adaptive-RPCL) which is able to adjust the number of prototypes during training. Its performance on biological downstream task is comparable with that of vanilla RPCL clustering algorithm in our method, which shows that the proposed GraphLoG model is not too sensitive to the selection of clustering algorithm.\n\nQ2: Is the performance of GraphLoG robust to different clustering outputs?\n\nA2: In the **Section E.4** of appendix, we compare the performance of six pre-trained models derived by different clustering results. It is observed that the downstream task performance of these models is comparable with each other, which demonstrates that the GraphLoG model is fairly robust to different clustering outputs.\n \n\n**In the revised paper, you can refer to the bolded sections above for the detailed experimental results.**", "title": "Official Response to AnonReviewer4"}, "-w8buVLx4o1": {"type": "rebuttal", "replyto": "AIwlhBeS8Ij", "comment": "Thanks for your insightful comments and great suggestions, which definitely help us improve the quality of this work. \n \nWe respond to your concerns about the novelty of this work as follows:\n\nQ1: The combination of too many existing techniques shadows the novelty of this work.\n\nA1: Our core idea is to learn both the local-instance and global-semantic structure of a set of graphs in a self-supervised fashion. This problem, to the best of our knowledge, has not been explored by previous works, which makes the proposed GraphLoG model novel as a whole. Indeed, some existing techniques, i.e. attribute masking [a], InfoNCE loss [b] and RPCL clustering [c], have been adopted to construct the entire model. However, the additional ablation studies in the **Section E** of appendix show that these techniques (except attribute masking) can be replaced by their vanilla counterparts without too much hurt to model\u2019s performance, which demonstrates the effectiveness of local-instance and global-semantic learning, i.e. the key idea of this work.\n\nQ2: If we combine a plain GNN with hierarchical prototypes, can this model achieve superior performance?\n\nA2: Following your suggestion, we further design a supervised variant of GraphLoG, named as sup-GraphLoG, in **Section 4**. The sup-GraphLoG model establishes hierarchical prototypes on top of a plain GNN and performs graph classification by measuring the similarity between graph embeddings and prototypes. This model outperforms the plain GNN on chemical and biological benchmark datasets (shown in **Tabs. 1 and 2**), which illustrates the effectiveness of global-semantic learning under the supervised setting. \n\nIn addition, we would like to point out that the sup-GraphLoG model does not perform as well as the GraphLoG model, which demonstrates the necessity of self-supervised pre-training on massive unlabeled graphs.\n\n\n**In the revised paper, you can refer to the bolded sections above for the detailed contents related to your concerns.**\n \n \n[a] Hu, Weihua, et al. \"Strategies for Pre-training Graph Neural Networks.\" ICLR, 2020.\n\n[b] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv:1807.03748 (2018).\n\n[c] Xu, Lei, Adam Krzyzak, and Erkki Oja. \"Rival penalized competitive learning for clustering analysis, RBF net, and curve detection.\" IEEE Transactions on Neural networks, 1993.", "title": "Official Response to AnonReviewer3"}, "ZJzPCNpK2He": {"type": "rebuttal", "replyto": "Y38wFBmjJrW", "comment": "Thanks for your insightful reviews on this work!\n \nWe first would like to re-emphasize the novelty of this work. This paper is dedicated to self-supervised graph representation learning which preserves both the local-instance and global-semantic structure of a set of unlabeled graphs. The proposed GraphLoG model is novel as a whole, since this learning problem, to the best of our knowledge, has not been studied by previous works. During constructing the entire model, we adopt some existing techniques, i.e. attribute masking [a], InfoNCE loss [b] and RPCL clustering [c], to promote model\u2019s performance. However, these techniques can be substituted with the vanilla counterpart, e.g. InfoNCE loss -> hinge-loss-based contrastive loss, RPCL -> K-means, without too much hurt to model\u2019s effectiveness, which is analyzed by the additional ablation studies in the **Section E** of appendix. In summary, we utilize these techniques as the performance-boosting modules serving for the core idea, local-instance and global-semantic learning, instead of simply combining them together.\n \nWe respond to your questions as follows:\n\nQ1: The theoretical and experimental analysis about the reliability of attribute masking strategy should be supplemented.\n\nA1: We give a theoretical analysis about GNN\u2019s capability of repairing the information lost by attribute masking in the **Section A** of appendix, and also empirically show that the correlated graphs derived by attribute masking is more reliable in **Section E.1**.\n\nQ2: How to use hierarchical prototypes in graph classification tasks?\n\nA2: In the self-supervised model GraphLoG, since the hierarchical prototypes cannot directly correspond to the categories of downstream tasks, they are not employed during graph classification. To study this problem in-depth, we additionally design a supervised learning variant, sup-GraphLoG, in **Section 4** to verify the effectiveness of hierarchical prototypes on graph classification with explicit supervision. The sup-GraphLoG model outperforms the vanilla GNN on chemical and biological benchmark datasets (shown in **Tabs. 1 and 2**).\n\nQ3: In the global-semantic loss, are graph embedding and hierarchical prototypes optimized jointly?\n\nA3: Appreciate for this good question. In the current model, only the representation of correlated graph is updated by the global-semantic loss (Eq. 13), and hierarchical prototypes are only updated by Eqs. 11 and 12. The joint optimization of these two types of variables will be the direction of our future exploration.\n\n \n**In the revised paper, you can refer to the bolded sections above for the detailed contents related to your questions.**\n \n \n[a] Hu, Weihua, et al. \"Strategies for Pre-training Graph Neural Networks.\" ICLR, 2020.\n\n[b] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv:1807.03748 (2018).\n\n[c] Xu, Lei, Adam Krzyzak, and Erkki Oja. \"Rival penalized competitive learning for clustering analysis, RBF net, and curve detection.\" IEEE Transactions on Neural networks, 1993.", "title": "Official Response to AnonReviewer2"}, "ciENTA1Mf7x": {"type": "rebuttal", "replyto": "b4uaKv95lCI", "comment": "Thanks for your support to the motivation and methodology of this work!\n \nWe address your concern on the experimental verification as follows:\n\nQ1: More benchmark tasks should be added to evaluate the proposed method.\n\nA1: We additionally evaluate the proposed model on five graph classification benchmarks in the **Section D** of appendix. These five benchmark datasets involve the classification on the molecular graphs and social networks, and they are commonly used in previous self-supervised graph representation learning literature [a,b].\n\nQ2: More ablation studies should be conducted in order to better understand the proposed GraphLoG model.\n\nA2: We conduct more thorough ablation studies on the correlated graph construction, loss constraint and clustering algorithm in the **Section E** of appendix.\n\n\n**In the revised paper, you can refer to the bolded sections above for the detailed experimental results.**\n \n \n[a] Sun, Fan-Yun, et al. \"Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization.\"  ICLR, 2020.\n\n[b] Hassani, Kaveh, and Amir Hosein Khasahmadi. \"Contrastive Multi-View Representation Learning on Graphs.\" ICML, 2020.", "title": "Official Response to AnonReviewer1"}, "AIwlhBeS8Ij": {"type": "review", "replyto": "DAaaaqPv9-q", "review": "This paper proposed a method for self-supervised graph-level representation learning. The main idea is to enforce both the instance level smoothness embedding constraints, and a so-called global, semantic grouping structures across all instance graphs in the training data set.  To achieve this goal, the authors have adopted a global clustering framework to encourage the embedding of the graphs belonging to the same clusters to be close to each other, and by using a hierarchically organized set of prototypes. The proposed method is applied to pre-train GNN on massive unlabeled graphs, which is then fine-tuned to downstream learning tasks.\n\nEnforcing a global clustering structure can be useful in capturing the distribution of large number of graphs in the training data set and hopefully carry the learned representations over to other tasks. However, it appears to me that the paper has combined the carefully devised ideas from too many existing work, each of which alone has shown great success in improving the learning performance. Therefore it can be difficult to judge which part of the choices really leads to the final improvement, and in particular whether it is  the local and global structure preserving part, which seems to be the core theme of the paper (with the other theme being sel-supervised learning), that can fully explain the result.\n\nIn more detail, the authors have used (1) masking strategy by Hu et al., 2019 to generate correlated graph pairs, (2) mutual information estimation technique InfoNCE to enforce the correlation between paired graphs, and (3) Rival Penalized Competitive Learning (RPCL) as the main building block for hierarchical prototype-based learning. Therefore, a natural question to ask is, if one uses plain GNN architecture of each graph and plug it in a (hierarchical) clustering framework (without self-supervised learning and RPCL), whether similar improvements in the learning performance can still be obtained? If not, then the gains are merely due to the effectiveness of these specially designed components from the literatures and not by the general idea of local and global structures. \n\nWith regard to this concern, I would suggest the authors to clarify on what is the main theme of the work and demonstrate that the empirical performance gains are truly due to a novel, focused idea they propose rather than by combining some of the  existing algorithms which have shown great impact and performance gains in their respective context. In the current form, the novelrity of the work seems less significant by introducing so many components from other works.\n", "title": "A combination of too many existing ideas that shadows the novelty and makes it hard to judge who should be given the credit of the empirical performance gains", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "b4uaKv95lCI": {"type": "review", "replyto": "DAaaaqPv9-q", "review": "The motivation and novelty of the proposed method are good. However, the validation is kind of\nweak.\nI can understand that this papers follows the validation in Hu et al (2019), however, I feel that\ntwo tasks (one on chemical benchmark and one on biological benchmark) may not be sufficient\nto give a detailed idea of the improvement of the proposed GraphLoG over other baselines. I\nthink 3-5 tasks are much better.\nFor the ablation study in Section 5.4, these ablated items are good. However, I more would like\nto see fluctuated parts in the proposed GraphLoG. One example may be: is there any different\nchoice/option for hierarchical prototype? Which one is good/bad? What is the reason. Or other\npotential and similar examples exist in the proposed GraphLoG. I think this will help us to\nunderstand GraphLoG more.", "title": "This paper proposes GraphLoG for self-supervised graph-level representation learning. It can learn both local-instance and global-semantic information. Experiments are conducted on chemical and biological benchmark.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "jSgse61B9ao": {"type": "review", "replyto": "DAaaaqPv9-q", "review": "Pros:\n- The paper proposed a novel self-supervised learning method to embed graphs to vector space. Different from previous methods, the method proposed a global-semantic learning strategy to encourage the embeddings to form a hierarchical clustering structure.  Both the embedding network and the hierarchical structure can be jointly learned.\n\n- Authors have provided extensive and convincing comparison results and numerical analysis to show the effectiveness of the method.\n\n- The paper is well-organized and clearly written. To the best of my knowledge, the proposed method is technically feasible.\n\nCons:\n- The number of prototypes is determined by RPCL and can not be adjusted in training.\u3000\n- Clustering algorithms are usually not very robust. Since the prototypes of GraphLoG is initialized by RPCL, is the performance of GraphLoG robust? ", "title": "An interesting work for self-supervised graph representation learning", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}