{"paper": {"title": "Explicit homography estimation improves contrastive self-supervised learning", "authors": ["David Torpey", "Richard Klein"], "authorids": ["~David_Torpey1", "~Richard_Klein1"], "summary": "Explicit homography estimation improves contrastive self-supervised learning", "abstract": "The typical contrastive self-supervised algorithm uses a similarity measure in latent space as the supervision signal by contrasting positive and negative images directly or indirectly. Although the utility of self-supervised algorithms has improved recently, there are still bottlenecks hindering their widespread use, such as the compute needed. In this paper, we propose a module that serves as an additional objective in the self-supervised contrastive learning paradigm. We show how the inclusion of this module to regress the parameters of an affine transformation or homography, in addition to the original contrastive objective, improves both performance and rate of learning. Importantly, we ensure that this module does not enforce invariance to the various components of the affine transform, as this is not always ideal. We demonstrate the effectiveness of the additional objective on two recent, popular self-supervised algorithms. We perform an extensive experimental analysis of the proposed method and show an improvement in performance for all considered datasets. Further, we find that although both the general homography and affine transformation are sufficient to improve performance and convergence, the affine transformation performs better in all cases.", "keywords": []}, "meta": {"decision": "Reject", "comment": "All four reviewers raised concerns on the limited technical novelty and insufficient experiments. They unanimously recommended a rejection. I carefully read the authors' rebuttal but did not find strong reasons to go against the reviewers' recommendations. The reviewers made excellent points to further improve the paper. The authors are encouraged to incorporate those for a future submission."}, "review": {"pjGUiMqNmgw": {"type": "review", "replyto": "bWqodw-mFi1", "review": "**1. Summary:**\n\nThe authors propose a module that regresses the parameters of an affine transformation or homography as an additional objective in the contrastive self-supervised learning framework. The authors argue that the geometric information encoded in the proposed module can supplement the signal provided by a contrastive loss, improving both performance and convergence speed. The authors validate their claims with two recent contrastive self-supervised learning approaches (i.e., SimCLR, BYOL) on several benchmark datasets showing effective results.\n\n**2. Strengths:**\n\n+ The authors extend the typical contrastive objective with an additional homography estimation objective to enforce non-invariance to the affine transformation or homography. This extension is well motivated, and the authors provide a clear intuition for the proposed module.\n\n+ The authors evaluate the proposed module on several benchmarks showing nice results.\n\n**3. Weaknesses:**\n\nWhile the authors demonstrate the effectiveness of the proposed module on several considered datasets, some points make me concerned about the actual usefulness of this module:\n\n- The authors conduct the empirical study on three small-scale datasets (i.e., CIFAR10, CIFAR100, SVHN). This is fine in itself, however, without experiments on large-scale datasets like ImageNet, it is unknown whether the conclusion still holds since most of the previous effective contrastive learning methods (e.g., SimCLR, BYOL) are originally conducted on ImageNet. It would be better for the authors to provide such experiments to further validate the claims. In this case, there is no need to perform experiments with large batch size to achieve state-of-the-art performance, just a normal batch size (e.g., 256) can validate the effectiveness of the proposed module.\n\n- The authors show significant improvements across all datasets with the proposed module in Table 2 and Table 3. However, as shown in Table 4, when pretrained for longer epoches, the relative benefit diminishes and even degrades the performance a little. Although the authors give a brief explanation on this phenomenon, I am concerned whether the proposed module is actually useful when SimCLR and BYOL have sufficiently converged.\n\n- The authors use the linear evaluation to measure the quality of learned representations by pre-training and fine-tuning on the same dataset. Although it is one of the common evaluation protocols, pre-training and fine-tuning on the same dataset may induce some inevitable biases. Therefore, it would be better to provide more empirical evidence by transferring to other datasets to evaluate the effectiveness of the proposed module.\n\nTo sum up, although the paper is interesting, I think it requires more work (see points W1-W3 above) in order to become more complete and convincing. Therefore, I am leaning towards rejection.\n\n====================================================================================\n\n**Post-Rebuttal**\n\nAfter reading the rebuttal and the other reviewers' comments, my concerns persist:\n\n- The technical contribution is limited. Adding a pretext task of homography prediction itself brings little insights regarding how it improves upon contrastive representation learning from a different perspective. \n\n- The experimental results are not convincing compared to the recent advances. The authors are encouraged to include ImageNet results as well as transfer learning evaluation.\n\nTherefore, I would like to keep my initial rating as rejection.\n", "title": "Official Blind Review #1", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "lWjdMe-p2PY": {"type": "review", "replyto": "bWqodw-mFi1", "review": "====================================================\n\n**Update after rebuttal**\n\nI want to thank the authors for a long and highly detailed rebuttal. They clarified a lot of my questions and hopefully in the process they were able to improve the paper. However, my listed weaknesses still stand:\n* there is no transfer learning experiments\n* there not seem to be any consistent gains with the proposed approach over other methods, as seen in Table 4, after 500 epochs (when the models indeed have probably converged). Gains can be seen for 100 epochs, but from the absolute numbers  it is obvious that models havent really convered at that time. Furthermore, as the authors clarified,  they do require  30% higher training time, something that make the claim that they learn faster even weaker\n* There is no analysis It is unclear why/how forcing the feature vectors difference to be predictive (ie encode) the transformation and learning representations that are non-invariant to homography transforms actually helps for learning better representations. In fact, It turns out that simpler versions of the proposed module than homography estimation are the ones giving the best results. \n\nI will therefore retain my rejection rating.\n\n\n====================================================\n\nSummary:\n\nThe authors propose a module and objective that can be added to recent contrastive self-supervised learning (SSL) frameworks like SimCLR or BYOL. The module p`erforms an affine or homography transformation $\\phi$ on the input $x_1$. Then the difference of the features of $x_1$ and the geometrically transformed version are the input to an MLP that tries to regress to the parameters of  transformation $\\phi$, using an MSE loss. The authors show gains on CIFAR and SVHN by adding this module over SimCLR and BYOL. \n\nStrong points: The authors show that adding this module speeds up SSL pre-training during early epochs.\n\n\nWeak points: \n\nA) The authors do not measure transfer learning performance and evaluate *only* on a very superficial setup, where they train linear classifiers on top of encoders that are pre-trained with contrastive SSL learning on the exact same datasets.  Although this is indeed an experiment popular among SSL papers,  in all recent SSL works like SimCLR and BYOL the linear evaluation on the same dataset (usually imagenet) is only a small part of the evaluation suite that also involves testing performance on many different tasks and datasets. Transfer learning performance here is not measured, nor is performance on  semi-supervised learning tasks.\n\nB) The authors show some gains and faster learning for the first epochs of SSL pre-training. However, results should be taken with a pinch of salt as their gains are only visible in a realm where performance is still clearly suboptimal and learning is far from converged (eg for CIFAR100,  31.33 at epoch 100 vs 42.53 at epoch 500) To their credit, the authors themselves clearly say this in the text and show in Table 4. However they only present results at 100 and then 500 epochs, so it is hard to understand when (at which epoch) the two lines \"cross\" - at which epoch does the proposed approach stop giving gains? \n\nC) There is no analysis It is unclear why/how forcing the feature vectors difference to be predictive (ie encode) the transformation and learning representations that are non-invariant to homography transforms actually helps for learning better representations. In fact, It turns out that simpler versions of the proposed module than homography estimation are the ones giving the best results. In Table 5, the authors show that most of the gain can be achieved by only predicting the 2-dim sheer tranformation; this is something potentially interesting that the authors however do not investigate any further.\n\nQuestions and notes:\n* Although not used for SSL, the authors should cite and discuss the Spatial Transformer module of [Jaderberg et al 2015]\n* The notation could be clearer (eg although clearly explained in the text, it is a bit confusing that augmentation/transformation $a_2$ is sampled from set $T_1$ and $a_\\phi$ from $T_2$)\n* Design ablations: The authors use vector difference as the input to MLP h to regress to the transformation parameters, but other reasoning modules could also be used. How would performance be affected for other input functions beyond difference, eg concatenation or elementwise multiplication?\n* There area number of hyperparameter choices in Table 1 that do not match the corresponding papers (eg BYOL uses LARS not SGD). Why wasn't the original protocol followed, and were all the differences ablated?\n* How do you balance the two losses in Eq(5)? Is there no hyperparameter? what would happen if you favor one or the other loss?\n* How would performance change if this module was added after g() instead of f()?\n* Why only on $x_1$? Could there be extra gains if the MSE loss was also applied over $x_2$?\n* What is the added cost of the module in terms of training time? \n* It is unclear to me what the variance measures: Is it for results after multiple runs, and if so, how many runs? Were the multiple runs from scratch or only for the linear evaluation?\n* In Figure 2, how often was performance tested? markers would help understand where datapoints are.\n* The datasets used are relatively small and more importantly in very low resolution. Would the same hold for higher resolution datasets?\n* How dataset biased are the results presented? E.g. invariance to homographies might not be beneficial to digits (6 and 9 is a good example) but it would be useful in eg landmark datasets. Maybe a more suiting dataset/task would be something related to localization\n* [Tian et al 2020] is another related paper that could be discussed - there the authors explore different augmentations to get invariance to, and show highly increased performance even after longer training.\n\nReferences:\n[Jaderberg et al 2015] Spatial transformer networks.\" Advances in neural information processing systems. 2015.\n[Tian et al 2020] What makes for good views for contrastive learning, Arxiv May 2020, accepted at NeurIPS 2020 (not officially published yet)\n\n\n\n\n\n\n", "title": "Adding a secondary loss on contrastive SSL that explicitly predicts an affine augmentation", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "24BS139ICaU": {"type": "rebuttal", "replyto": "FzGlg6eOgxk", "comment": "Thank you for your response. Hopefully I can provide some clarification below:\n\nIndeed at 500 epochs on these dataset there are no consistent gains, however, we aim to show primarily that: using the propose homography estimation objectives allows for faster learning. It allows for convergence to a particular linear evaluation accuracy in a shorter amount of epochs than with only the contrastive objective. The homography estimation provides a signal for supervision that contains additional information that is not available from the contrastive objectives early in learning. We feel this is useful to know, as it speaks to the efficiency of current contrastive SSL methods.\n\nIndeed they did present results with additional ablation to the 100 epoch setting. However, we posit that ImageNet pretraining requires these high batch sizes and additional epochs. For the datasets we have covered, higher batch sizes did not yield performance gains in preliminary experiments thereto. The batch size used in our experiments seemed to be fairly optimal, and number of epochs had a more significant effect on downstream performance with these datasets.\n\nWe are currently running experiments on ImageNet to test our module on a large-scale, higher-resolution benchmark. However, due to computational resource limitations, this will take some time.\n\nThank you, and please let us know if you require any further clarification.\n\n", "title": "Clarification of research aims"}, "3u5d97sfoXh": {"type": "rebuttal", "replyto": "pjGUiMqNmgw", "comment": "We thank the reviewer for the time taken in reviewing our work, and useful suggestions for improvements of the paper.\n\nQ1: The authors conduct the empirical study on three small-scale datasets (i.e., CIFAR10, CIFAR100, SVHN). This is fine in itself, however, without experiments on large-scale datasets like ImageNet, it is unknown whether the conclusion still holds since most of the previous effective contrastive learning methods (e.g., SimCLR, BYOL) are originally conducted on ImageNet. It would be better for the authors to provide such experiments to further validate the claims. In this case, there is no need to perform experiments with large batch size to achieve state-of-the-art performance, just a normal batch size (e.g., 256) can validate the effectiveness of the proposed module.\n\nA1: We agree with the reviewer that ImageNet is an ideal experiment to include in the paper. However, we were unable to include it due to computational resource limitations. We will be running these experiments in future work.\n\n\nQ2: The authors show significant improvements across all datasets with the proposed module in Table 2 and Table 3. However, as shown in Table 4, when pretrained for longer epoches, the relative benefit diminishes and even degrades the performance a little. Although the authors give a brief explanation on this phenomenon, I am concerned whether the proposed module is actually useful when SimCLR and BYOL have sufficiently converged.\n\nA2: We understand the reviewer's concern. However, we argue that at 500 epochs (see Table 4), the methods have indeed converged for these small-scale datasets. This is evidenced by the fact that for a much more difficult dataset (ImageNet), SimCLR was only trained for 100 epochs.\n\nFurther, a primary aim of our work is to demonstrate the fact that being able to predict a homography/affine transformation as an additional objective provides the network with additional information during learning not available to it from the contrastive loss (particularly early on in learning). We opine that this is a useful finding, and merits further investigation into ways to improve or supplement contrastive SSL methods.\n\n\nQ3: The authors use the linear evaluation to measure the quality of learned representations by pre-training and fine-tuning on the same dataset. Although it is one of the common evaluation protocols, pre-training and fine-tuning on the same dataset may induce some inevitable biases. Therefore, it would be better to provide more empirical evidence by transferring to other datasets to evaluate the effectiveness of the proposed module.\n\nA3: We definitely agree with the reviewer. As per response A1, we will in future work be conducting experiments on ImageNet, including transfer learning experiments to be consistent with previous work. We were not able to perform these due to computational resource limitations.", "title": "Reviewer 1: Rebuttal"}, "CbKVRaT0EDe": {"type": "rebuttal", "replyto": "lWjdMe-p2PY", "comment": "\nQ11: The datasets used are relatively small and more importantly in very low resolution. Would the same hold for higher resolution datasets?\n\nA11: We agree with the reviewer in this regard. We planned to include ImageNet as part of our experiments (due to its larger scale in resolution and sample complexity) However, we were unable to include it due to computational resource limitations. We will be running these experiments in future work, including transfer learning experiments to be consistent with previous work.\n\n\nQ12: How dataset biased are the results presented? E.g. invariance to homographies might not be beneficial to digits (6 and 9 is a good example) but it would be useful in eg landmark datasets. Maybe a more suiting dataset/task would be something related to localization\n\nA12: We definitely agree with the reviewer in this regard. In future work we will be investigating the effect of invariance on a host of downstream tasks (object detection and semantic segmentation). However, we believe it is interesting to note that invariance to homographies/affine transformations in contrastive SSL does not yield any benefit in this low-resolution, small-scale regime of CIFAR10, CIFAR100, and SVHN.\n\n\nQ13: [Tian et al 2020] is another related paper that could be discussed - there the authors explore different augmentations to get invariance to, and show highly increased performance even after longer training.\n\nA13: This paper focuses on view generation, and the method proposed in the work focuses on learning effective views using ground truth label information from the downstream task. This label information is what informs the algorithm of effective views and implicitly learns how much invariance to employ. This is very different to our method, as our is fully unsupervised and we are not attempting to learn invariance. Thus it is difficult to draw any conclusion in comparisons with out work. We will, however, add this paper to the related work section.", "title": "Reviewer 2: Questions and notes rebuttal (part 2)"}, "I7PvLlneog2": {"type": "rebuttal", "replyto": "lWjdMe-p2PY", "comment": "We thank the reviewer for the time taken in reviewing our work, and useful suggestions for improvements of the paper.\n\nQ1: Although not used for SSL, the authors should cite and discuss the Spatial Transformer module of [Jaderberg et al 2015]\n\nA1: We see the link with spatial transformers as being the fact that we're also regressing the parameters of a transformation somewhere in the network. Beyond that, we don't see any clear link. Could the reviewer clarify the reason to include this work?\n\n\nQ2: The notation could be clearer (eg although clearly explained in the text, it is a bit confusing that augmentation/transformation $a_2$ is sampled from set $T_1$ and $a_{\\phi}$ from $T_2$)\n\nA2: We thank the reviewer for pointing this out. We will change the text to make the notation clearer.\n\n\nQ3: Design ablations: The authors use vector difference as the input to MLP h to regress to the transformation parameters, but other reasoning modules could also be used. How would performance be affected for other input functions beyond difference, eg concatenation or elementwise multiplication?\n\nA3: We definitely agree with the reviewer. We did indeed previously consider concatenation as an alternative way to represent the transformation instead of vector difference. However, from preliminary experiments the performance difference was negligible. We are currently scaling the concatenation experiments to get final numbers on this.\n\n\nQ4: There area number of hyperparameter choices in Table 1 that do not match the corresponding papers (eg BYOL uses LARS not SGD). Why wasn't the original protocol followed, and were all the differences ablated?\n\nA4: We thank the reviewer for pointing this out. The only difference between the original protocols are the optimiser-related hyperparameters. In preliminary experiments, we did indeed trial LARS with original hyperparameter values, however performance was significantly worse (borderline usable). Adam performed much better with far less hyperparameter tuning on these small-scale benchmarks. We posit that this is due to the hyperparameters in the paper being optimised for ImageNet, whereas datasets such as CIFAR10, CIFAR100, and SVHN require significantly different hyperparameters to train. In ImageNet experiments that we will be doing in future work, we will be using the original protocols.\n\n\nQ5: How do you balance the two losses in Eq(5)? Is there no hyperparameter? what would happen if you favor one or the other loss?\n\nA5: We agree with the reviewer that this is indeed a useful study to perform. We plan to do this in future work involving multiple kinds of downstream tasks (object detection and semantic segmentation). We posit that because the SSL algorithms with and without the module eventually appear to converge, balancing of the losses may need to be based on a schedule that favours one term of the loss more than the other over time.\n\n\nQ6: How would performance change if this module was added after g() instead of f()?\n\nA6: We definitely agree with the reviewer. We did indeed previously consider investigate using $g$ instead of $f$ for this. However, our experiments showed that the performance difference using $g$ is worse: 64.38 on CIFAR10; 29.99 on CIFAR100; and 82.49 on SVHN using SimCLR. We will add these findings to the appendix.\n\n\nQ7: Why only on $x_1$? Could there be extra gains if the MSE loss was also applied over $x_2$?\n\nA7: We thank the reviewer for pointing this out. We did instead trial adding our module to both $x_1$ and $x_2$. However, the performance different was negligible. We posit that this is because if the one module can solve homography estimation for $x_1$, then a module operating on $x_2$ will have to be able to solve homography estimation for it, because the same types of random homographies/affine transforms are being applied to both. \n\n\nQ8: What is the added cost of the module in terms of training time? \n\nA8: Total SSL training time is 30% larger on average with our module. We will systemically profile the training to identify where the bottleneck lies, and update you accordingly.\n\n\nQ9: It is unclear to me what the variance measures: Is it for results after multiple runs, and if so, how many runs? Were the multiple runs from scratch or only for the linear evaluation?\n\nA9: We thank the reviewer for pointing this out. This measure represents 10 random trials. Each run is a full pipeline trained from scratch: SSL pretraining + linear evaluation. We will clarify this in the paper.\n\n\nQ10: In Figure 2, how often was performance tested? markers would help understand where datapoints are.\n\nA10: Performance is tested every 10 epochs. We will add tick marks on to the plot, and clarify this in the text.", "title": "Reviewer 2: Questions and notes rebuttal (part 1)"}, "kpoQNoJB7cC": {"type": "rebuttal", "replyto": "lWjdMe-p2PY", "comment": "We thank the reviewer for the time taken in reviewing our work, and useful suggestions for improvements of the paper.\n\nQ1: The authors do not measure transfer learning performance and evaluate only on a very superficial setup, where they train linear classifiers on top of encoders that are pre-trained with contrastive SSL learning on the exact same datasets. Although this is indeed an experiment popular among SSL papers, in all recent SSL works like SimCLR and BYOL the linear evaluation on the same dataset (usually imagenet) is only a small part of the evaluation suite that also involves testing performance on many different tasks and datasets. Transfer learning performance here is not measured, nor is performance on semi-supervised learning tasks.\n\nA1: We agree with the reviewer that ImageNet is an ideal experiment to include in the paper. However, we were unable to include it due to computational resource limitations. We will be running these experiments in future work, including transfer learning experiments to be consistent with previous work.\n\n\nQ2: The authors show some gains and faster learning for the first epochs of SSL pre-training. However, results should be taken with a pinch of salt as their gains are only visible in a realm where performance is still clearly suboptimal and learning is far from converged (eg for CIFAR100, 31.33 at epoch 100 vs 42.53 at epoch 500) To their credit, the authors themselves clearly say this in the text and show in Table 4. However they only present results at 100 and then 500 epochs, so it is hard to understand when (at which epoch) the two lines \"cross\" - at which epoch does the proposed approach stop giving gains? \n\nA2: We understand the reviewer's concern. However, we argue that at 500 epochs, datasets such as CIFAR10, CIFAR100, and SVHN have indeed converged. Previous works (e.g. SimCLR) train for 100 epochs on the much more complicated ImageNet dataset. We will run experiments to train the methods for longer (1000 epochs) to verify this hypothesis. We will report results once they're in.\n\n\nQ3: There is no analysis It is unclear why/how forcing the feature vectors difference to be predictive (ie encode) the transformation and learning representations that are non-invariant to homography transforms actually helps for learning better representations. In fact, It turns out that simpler versions of the proposed module than homography estimation are the ones giving the best results. In Table 5, the authors show that most of the gain can be achieved by only predicting the 2-dim sheer tranformation; this is something potentially interesting that the authors however do not investigate any further.\n\nA3: We definitely agree with the reviewer. We did indeed previously consider concatenation as an alternative way to represent the transformation instead of vector difference. However, from preliminary experiments the performance difference was negligible. We are currently scaling the concatenation experiments to get final numbers on this.\n\nRegarding the reviewer's concern about the shear transformation outperforming SimCLR on SVHN, we would like to note that this marginal performance improvement of shear only occurred in 1 of 6 cases. Further, from visual inspection of samples from the SVHN dataset, we posit that shear appears to the most useful transformation to be able to encode for such house number images. We plan to investigate the transformation component analysis in future work.", "title": "Reviewer 2: Rebuttal"}, "XXIS0uhfp92": {"type": "rebuttal", "replyto": "U6Oj9TQQNkb", "comment": "We thank the reviewer for the time taken in reviewing our work, and useful suggestions for improvements of the paper.\n\nQ1: In my opinion, the paper needs to evaluate the methods in more large-scale benchmarks. Authors only use CIFAR and street view house numbers dataset, which are very specific benchmarks. However, the community is moving towards using larger and larger benchmarks and I think that the paper should follow this trend to be publicable. I think authors should at least apply their method to ImageNet training. \n\nA1: We agree with the reviewer that ImageNet is an ideal experiment to include in the paper. However, we were unable to include it due to computational resource limitations. We will be running these experiments in future work, including transfer learning experiments to be consistent with previous work.\n\n\nQ2: The performance improvement shown does not seem enough to merit publication, given the benchmark used. In particular, larger differences are shown only when one method does worse in absolute terms (Table 3). I think the combination of these two factors (the numerical improvement and the benchmarks used) make the paper weak.\n\nA2: We understand the reviewer's concerns. However, we would like to note that the experiments were statistically significant for 10 random trials in all experiments with a significance level of 1%. Further, in our work we primarily aim to show that being able to predict a homography/affine transformation provides the network with supplementary information during learning that is not available from the contrastive loss, particularly early in learning. We feel that this is useful information to know about the current state of contrastive SSL methods, and may assist in directing future work.\n\n\nQ3: I think it would be useful for the reader to see some examples of the homography to estimate in the main paper. I believe it helps the reader understand the task the model is attempting and how hard or difficult this is?\n\nA3: We agree with the reviewer. We will include example homographies and affine transformations of varying strengths in the paper.\n\n\nQ4: Have the authors considered using using a concatenation of the two vectors x1, x1' instead of its difference to predict the homography? I am missing some experiments motivating the choice of architecture. \n\nA4: We definitely agree with the reviewer. We did indeed previously consider concatenation as an alternative way to represent the transformation instead of vector difference. However, from preliminary experiments the performance difference was negligible. We are currently scaling the concatenation experiments to get final numbers on this.", "title": "Reviewer 3: Rebuttal"}, "epxKWdFZyzb": {"type": "rebuttal", "replyto": "i-22EnCSm3", "comment": "We thank the reviewer for the time taken in reviewing our work, and useful suggestions for improvements of the paper.\n\nQ1: a certain representation of A and H was chosen. Other decompositions are possible. Is the ad hoc chosen one suitable? Have other options been considered?\n\nA1: We previously considered concatenation as an alternative way to represent the transformation instead of vector difference. However, from preliminary experiments the performance difference was negligible. We are currently scaling the concatenation experiments to get final numbers on this.\n\n\nQ2: the representation of A and H influences the sampling. \n\nA2: We assume the reviewer means that we are predicting the actual individual parameters of an affine transform, instead of the transformation matrix elements. We plan to run experiments to analyse how the latter compares that what we've already implemented in the former.\n\n\nQ3: the three problems used for demonstrations are quite similar. It is unclear what type of vision problems would benefit from this technique.\n\nA3: In future work we plan to investigate the benefit of the proposed module on the following downstream tasks: objection detection, and semantic segmentation.\n\n\nQ4: I see very limited benefit to the reader.\n\nA4: The main benefit we aim to show with the proposed module is that being predictive of a homography/affine transformation provides the network with supplementary information not available from the contrastive loss, particularly early in learning. We feel that this is useful to know, and to investigate in future work.", "title": "Reviewer 4: Rebuttal"}, "U6Oj9TQQNkb": {"type": "review", "replyto": "bWqodw-mFi1", "review": "Summary: the paper introduces a novel technique for self-supervised learning where additionally to the self-supervised loss, the model is forced to predict a parameter of the homography between two images given the difference of its embedding vectors. Authors evaluate the proposed method in BYOL and SimCLR.\n\nStrengths:\n\n- Self-supervised learning has shown a lot of promise these last years. I believe that this work explores an interesting addition to traditional self-supervised methods. \n\n- The paper evaluates this addition in two of the state-of-the-art self-supervised models (BYOL and SimCLR).\n\n- Authors provide very detailed explanation of the experimental set-up, which is very good for reproducibility. \n\n\nWeaknesses:\n\n- In my opinion, the paper needs to evaluate the methods in more large-scale benchmarks. Authors only use CIFAR and street view house numbers dataset, which are very specific benchmarks. However, the community is moving towards using larger and larger benchmarks and I think that the paper should follow this trend to be publicable. I think authors should at least apply their method to ImageNet training. \n\n- The performance improvement shown does not seem enough to merit publication, given the benchmark used. In particular, larger differences are shown only when one method does worse in absolute terms (Table 3). I think the combination of these two factors (the numerical improvement and the benchmarks used) make the paper weak.\n\n- I think it would be useful for the reader to see some examples of the homography to estimate in the main paper. I believe it helps the reader understand the task the model is attempting and how hard or difficult this is?\n\n- Have the authors considered using using a concatenation of the two vectors x1, x1' instead of its difference to predict the homography? I am missing some experiments motivating the choice of architecture. \n\n\nConclusion: I think the research direction is interesting but the paper needs to evaluate in larger dataset to show results that merit publication. Furthermore, I think authors should motivate better their architectural choices. ", "title": "The paper needs stronger and more extensive experimentation results to be in a publicable state.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "i-22EnCSm3": {"type": "review", "replyto": "bWqodw-mFi1", "review": "The paper test a hypothesis that adding  affine transformation and homography estimation as an auxilliary loss in an image recognition task will improve performance. In experiments on CIFAR10, CIFAR100 and SVHN, improvement of a up to a few % points is observed.\n\nThe technique is close to augmentation and experimenting with A and H is not novel. \n\nThere are issues with the experiments:\n1. a certain representation of A and H was chosen. Other decompositions are possible. Is the ad hoc chosen one suitable? Have other options been considered?\n2. the representation of A and H influences the sampling. \n3. the three problems used for demonstrations are quite similar. It is unclear what type of vision problems would benefit from this technique.\n\nParts of the paper state well-known facts (description of H and A, there properties), some are irrelevant - for instance par 2. in the intro about transfer learning. More details should be provided about SimCLR and BYOL to make the submission more self-contained.\n\nI see very limited benefit to the reader.\n", "title": "The paper experiments with H and A estimation as an additional loss in the image recognition task. Marginal improvement is reported.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}