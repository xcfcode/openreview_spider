{"paper": {"title": "Generalizable Features From Unsupervised Learning", "authors": ["Mehdi Mirza", "Aaron Courville", "Yoshua Bengio"], "authorids": ["memirzamo@gmail.com", "aaron.courville@gmail.com", "yoshua.umontreal@gmail.com"], "summary": "Using generated data from a next frame predictor model to make a supervised model generalize better to unseen distributions.  ", "abstract": "Humans learn a predictive model of the world and use this model to reason about future events and the consequences of actions. In contrast to most machine predictors, we exhibit an impressive ability to generalize to unseen scenarios and reason intelligently in these settings.  One important aspect of this ability is physical intuition(Lake et al., 2016). In this work, we explore the potential of unsupervised learning to find features that promote better generalization to settings outside the supervised training distribution.  Our task is predicting the stability of towers of square blocks. We demonstrate that an unsupervised model, trained to predict future frames of a video sequence of stable and unstable block configurations, can yield features that support extrapolating stability prediction to blocks configurations outside the training set distribution", "keywords": ["Unsupervised Learning", "Deep learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "A summary of strengths and weaknesses brought up in the reviews:\n \n Strengths\n -Paper presents a novel way to evaluate representations on generalizability to out-of-domain data (R2)\n -Experimental results are encouraging (R2)\n -Writing is clear (R1, R2)\n \n Weaknesses\n -More careful controls are needed to ascertain generalization (R2)\n -Experimental analysis is preliminary and lack of detailed analysis (R1, R2, R3)\n -Novelty and discussion of past related work (R3)\n \n The reviewers are in consensus that the idea is exciting and at least of moderate novelty, however the paper is just too preliminary for acceptance as-is. The authors did not provide a response. This is surprising because specific feedback was given to improve the paper and it seems that the paper was just under the bar. Therefore I have decided to align with the 3 reviewers in consensus and encourage the authors to revise the paper to respond to the fairly consistent suggestions for improvement and re-submit. Mentime, I'd like to invite the authors to present this work at the workshop track."}, "review": {"B1Er683Xl": {"type": "rebuttal", "replyto": "H1cIAIW7l", "comment": "Thank you for your comment. Yes, the proposed models and the baseline models they all have harder time generalizing when the height of towers at train and test time are not the same as it was studied in \\cite{Zhang et al., 2016}.  \nThe contribution of this work is that, the generalization can significantly improve if we have future frame predictor model trained on all data in an unsupervised manner. \nThat is an interesting question. We did run some smaller scale experiments when trained on two heights and tested on the third height, there was a marginal improvement. But even in that case the future frame predicator model resulted in better generalization. The main goal of this work is to show that how unsupervised learning could help better generalization in unseen scenarios or for scenarios where labeled data is sparse. We are currently running more experiments in this setting you requested and will append to the paper as soon the results are in. \nThank you for noticing the cut off, we have updated the paper. ", "title": "Updated"}, "rJW5MBhXl": {"type": "rebuttal", "replyto": "rJWlOvJ7g", "comment": "Thank you for your comment. Yes, you are right, that is the case. During the training the model use the data (ground truth) at each time step and during test time the generated output feeds back at each time step. This is similar to the way language models based on recurrent neural networks are trained and evaluated.", "title": "Ground truth feeds in durning training "}, "H1cIAIW7l": {"type": "review", "replyto": "rJ6DhP5xe", "review": "Results indicate the proposed models have a harder time generalizing when the heights of the train and test towers are not the same. In all experiments only one tower height at a time is used for training. Does training with multiple heights lead to better generalization on unseen heights?\n\nSection 5 seems to be cut off at the end, could the authors upload a complete version?Summary\n===\nThis paper trains models to predict whether block towers will fall down\nor not. It shows that an additional model of how blocks fall down\n(predicting a sequence of frames via unsupervised learning) helps the original\nsupervised task to generalize better.\n\nThis work constructs a synthetic dataset of block towers containing\n3 to 5 blocks places in more or less precarious positions. It includes both\nlabels (the tower falls or not) and video frame sequences of the tower's\nevolution according to a physics engine.\n\nThree kinds of models are trained. The first (S) simply takes an image of a\ntower's starting state and predicts whether it will fall or not. The\nother two types (CD and CLD) take both the start state and the final state of the\ntower (after it has or has not fallen) and predict whether it has fallen or not,\nthey only differ in how the final state is provided. One model (ConvDeconv, CD)\npredicts the final frame using only the start frame and the other\n(ConvLSTMDeconv) predicts a series of intermediate frames before coming\nto the final frame. Both CD and CLD are unsupervised.\n\nEach model is trained on towers of a particular heigh and tested on\ntowers with an unseen height. When the height of the train towers\nis the same as the test tower height, all models perform roughly the same\n(with in a few percentage points). However, when the test height is\ngreater than the train height it is extremely helpful to explicitly\nmodel the final state of the block tower before deciding whether it has\nfallen or not (via CD and CLD models).\n\n\nPros\n===\n\n* There are very clear (large) gains in accuracy from adding an unsupervised\nfinal frame predictor. Because the generalization problem is also particularly\nclear (train and test with different numbers of blocks), this makes for\na very nice toy example where unsupervised learning provides a clear benefit.\n\n* The writing is clear.\n\n\nCons\n===\n\nMy one major concern is a lack of more detailed analysis. The paper\nestablishes a base result, but does not explore the idea to the extent\nto which I think an ICLR paper should. Two general directions for potential\nanalysis follow:\n\n* Is this a limitation of the particular way the block towers are rendered?\n\nThe LSTM model could be limited by the sub-sampling strategy. It looks\nlike the sampling may be too coarse from the provided examples. For the\ntwo towers in figure 2 that fall, they have fallen after only 1 or 2\ntime steps. How quickly do most towers fall? What happens if the LSTM\nis trained at a higher frame rate? What is the frame-by-frame video\nprediction accuracy of the LSTM? (Is that quantity meaningful?)\nHow much does performance improve if the LSTM is provided ground truth\nfor only the first k frames?\n\n* Why is generalization to different block heights limited?\n\nIs it limited by model capacity or architecture design?\nWhat would happen if the S-type models were made wider/deeper with the CD/CLD\nfall predictor capacity fixed?\n\nIs it limited by the precise task specification?\nWhat would happen if networks were trained with towers of multiple heights\n(apparently this experiment is in the works)?\nI appreciate that one experiment in this direction was provided.\n\nIs it limited by training procedure? What if the CD/CLD models were trained\nin an end-to-end manner? What if the double frame fall predictor were trained\nwith ground truth final frames instead of generated final frames?\n\n\nMinor concerns:\n\n* It may be asking too much to re-implement Zhang et. al. 2016 and PhysNet\nfor the newly proposed dataset, but it would help the paper to have baselines\nwhich are directly comparable to the proposed results. I do not think this\nis a major concern because the point of the paper is about the role of\nunsupervised learning rather than creating the best fall prediction network.\n\n* The auxiliary experiment provided is motivated as follows: \n\"One solution could be to train these models to predict how many blocks have\nfallen instead of a binary stability label.\"\nIs there a clear intuition for why this might make the task easier?\n\n* Will the dataset, or code to generate it, be released?\n\n\n\nOverall Evaluation\n===\nThe writing, presentation, and experiments are clear and of high enough\nquality for ICLR. However the experiments provide limited analysis past\nthe main result (see comments above). The idea is a clear extension of ideas behind unsupervised\nlearning (video prediction) and recent results in intuitive physics from\nLerer et. al. 2016 and Zhang et. al. 2016, so there is only moderate novelty.\nHowever, these results would provide a valuable addition to the literation,\nespecially if more analysis was provided.\n", "title": "Update and Experiment Request", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJLj58VNx": {"type": "review", "replyto": "rJ6DhP5xe", "review": "Results indicate the proposed models have a harder time generalizing when the heights of the train and test towers are not the same. In all experiments only one tower height at a time is used for training. Does training with multiple heights lead to better generalization on unseen heights?\n\nSection 5 seems to be cut off at the end, could the authors upload a complete version?Summary\n===\nThis paper trains models to predict whether block towers will fall down\nor not. It shows that an additional model of how blocks fall down\n(predicting a sequence of frames via unsupervised learning) helps the original\nsupervised task to generalize better.\n\nThis work constructs a synthetic dataset of block towers containing\n3 to 5 blocks places in more or less precarious positions. It includes both\nlabels (the tower falls or not) and video frame sequences of the tower's\nevolution according to a physics engine.\n\nThree kinds of models are trained. The first (S) simply takes an image of a\ntower's starting state and predicts whether it will fall or not. The\nother two types (CD and CLD) take both the start state and the final state of the\ntower (after it has or has not fallen) and predict whether it has fallen or not,\nthey only differ in how the final state is provided. One model (ConvDeconv, CD)\npredicts the final frame using only the start frame and the other\n(ConvLSTMDeconv) predicts a series of intermediate frames before coming\nto the final frame. Both CD and CLD are unsupervised.\n\nEach model is trained on towers of a particular heigh and tested on\ntowers with an unseen height. When the height of the train towers\nis the same as the test tower height, all models perform roughly the same\n(with in a few percentage points). However, when the test height is\ngreater than the train height it is extremely helpful to explicitly\nmodel the final state of the block tower before deciding whether it has\nfallen or not (via CD and CLD models).\n\n\nPros\n===\n\n* There are very clear (large) gains in accuracy from adding an unsupervised\nfinal frame predictor. Because the generalization problem is also particularly\nclear (train and test with different numbers of blocks), this makes for\na very nice toy example where unsupervised learning provides a clear benefit.\n\n* The writing is clear.\n\n\nCons\n===\n\nMy one major concern is a lack of more detailed analysis. The paper\nestablishes a base result, but does not explore the idea to the extent\nto which I think an ICLR paper should. Two general directions for potential\nanalysis follow:\n\n* Is this a limitation of the particular way the block towers are rendered?\n\nThe LSTM model could be limited by the sub-sampling strategy. It looks\nlike the sampling may be too coarse from the provided examples. For the\ntwo towers in figure 2 that fall, they have fallen after only 1 or 2\ntime steps. How quickly do most towers fall? What happens if the LSTM\nis trained at a higher frame rate? What is the frame-by-frame video\nprediction accuracy of the LSTM? (Is that quantity meaningful?)\nHow much does performance improve if the LSTM is provided ground truth\nfor only the first k frames?\n\n* Why is generalization to different block heights limited?\n\nIs it limited by model capacity or architecture design?\nWhat would happen if the S-type models were made wider/deeper with the CD/CLD\nfall predictor capacity fixed?\n\nIs it limited by the precise task specification?\nWhat would happen if networks were trained with towers of multiple heights\n(apparently this experiment is in the works)?\nI appreciate that one experiment in this direction was provided.\n\nIs it limited by training procedure? What if the CD/CLD models were trained\nin an end-to-end manner? What if the double frame fall predictor were trained\nwith ground truth final frames instead of generated final frames?\n\n\nMinor concerns:\n\n* It may be asking too much to re-implement Zhang et. al. 2016 and PhysNet\nfor the newly proposed dataset, but it would help the paper to have baselines\nwhich are directly comparable to the proposed results. I do not think this\nis a major concern because the point of the paper is about the role of\nunsupervised learning rather than creating the best fall prediction network.\n\n* The auxiliary experiment provided is motivated as follows: \n\"One solution could be to train these models to predict how many blocks have\nfallen instead of a binary stability label.\"\nIs there a clear intuition for why this might make the task easier?\n\n* Will the dataset, or code to generate it, be released?\n\n\n\nOverall Evaluation\n===\nThe writing, presentation, and experiments are clear and of high enough\nquality for ICLR. However the experiments provide limited analysis past\nthe main result (see comments above). The idea is a clear extension of ideas behind unsupervised\nlearning (video prediction) and recent results in intuitive physics from\nLerer et. al. 2016 and Zhang et. al. 2016, so there is only moderate novelty.\nHowever, these results would provide a valuable addition to the literation,\nespecially if more analysis was provided.\n", "title": "Update and Experiment Request", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJWlOvJ7g": {"type": "review", "replyto": "rJ6DhP5xe", "review": "In Figure 2, the model feeds back the generated output at each time step. But during training does the model use the ground truth as input at each time step ? This seems to be the case from Figure 3 : X_{t_n -1} is the input not \\tilde{X_{t_n-1}}. Just want to confirm if this is indeed the case.Paper Summary\nThis paper evaluates the ability of two unsupervised learning models to learn a\ngeneralizable physical intuition governing the stability of a tower of blocks.\nThe two models are (1) A model that predicts the final state of the tower given\nthe initial state, and (2) A model that predicts the sequence of states of this\ntower over time given the initial state. Generalizability is evaluated by\ntraining a model on towers made of a certain number of blocks but testing on\ntowers made of a different number of blocks.\n\nStrengths\n- This paper explores an interesting way to evaluate representations in terms of\n  their generalizability to out-of-domain data, as opposed to more standard\nmethods which use train and test data drawn from the same distribution.\n- Experiments show that the predictions of deep unsupervised learning models on\n  such out-of-domain data do seem to help, even though the models were not\ntrained explicitly to help in this way.\n\nWeaknesses\n\n- Based on Fig 4, it seems that the models trained on 3 blocks (3CD, 3CLD)\n  ``generalize\" to 4 and 5 blocks.  However, it is plausible that these models\nonly pay attention to the bottom 3 blocks of the 4 or 5 block towers in order to\ndetermine their stability. This would work correctly a significant fraction of\nthe time. Therefore, the models might actually be overfitting to 3 block towers\nand not really generalizing the physics of these blocks. Is this a possibility ?\nI think more careful controls are needed to make the claim that the features\nactually generalize. For example, test the 3 block model on a 5 block test set\nbut only make the 4th or 5th block unstable. If the model still works well, then\nwe could argue that it is actually generalizing.\n\n- The experimental analysis seems somewhat preliminary and can be improved. In\n  particular, it would help to see visualizations of what the final state looks\nlike for models trained on 3 blocks but test on 5 (and vice-versa). That would\nhelp understand if the generalization is really working. The discriminative\nobjective gives some indication of this, but might obfuscate some aspects of\nphysical realism that we would really want to test.  In Figure 1 and 2, it is\nnot mentioned whether these models are being tested on the same number of blocks\nthey were trained for.\n\n- It seems that the task of the predicting the final state is really a binary\n  task - whether or not to remove the blocks and replace them with gray\nbackground. The places where the blocks land in case of a fall is probably quite\nhard to predict, even for a human, because small perturbations can have a big\nimpact on the final state. It seems that in order to get a generalizable\nphysics model, it could help to have a high frame rate sequence prediction task.\nCurrently, the video is subsampled to only 5 time steps.\n\nQuality\nA more detailed analysis and careful choices of testing conditions can increase\nthe quality of this paper and strengthen the conclusions that can be drawn from\nthis work.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe particular setting explored in this paper is novel.\n\nSignificance\nThis paper provides a valuable addition to the growing work on\ntransferability/generalizability as an evaluation method for unsupervised\nlearning. However, more detailed experiments and analysis are needed to make\nthis paper significant enough for an ICLR paper.\n\nMinor comments and suggestions\n- The acronym IPE is used without mentioning its expansion anywhere in the text.\n\n- There seems to be a strong dependence on data augmentation. But given that\n  this is a synthetic dataset, it is not clear why more data was not generated\nin the first place.\n\n- Table 3 : It might be better to draw this as a 9 x 3 grid : 9 rows corresponding to the\nmodels and 3 columns corresponding to the test sets. Mentioning the train set is\nredundant since it is already captured in the model name. That might make it\neasier to read.\n\nOverall\nThis is an excellent direction to work and preliminary results look great.\nHowever, more controls and detailed analysis are needed to make strong\nconclusions from these experiments.", "title": "Feed-in ground truth or generated output during training ?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkG-C_NNl": {"type": "review", "replyto": "rJ6DhP5xe", "review": "In Figure 2, the model feeds back the generated output at each time step. But during training does the model use the ground truth as input at each time step ? This seems to be the case from Figure 3 : X_{t_n -1} is the input not \\tilde{X_{t_n-1}}. Just want to confirm if this is indeed the case.Paper Summary\nThis paper evaluates the ability of two unsupervised learning models to learn a\ngeneralizable physical intuition governing the stability of a tower of blocks.\nThe two models are (1) A model that predicts the final state of the tower given\nthe initial state, and (2) A model that predicts the sequence of states of this\ntower over time given the initial state. Generalizability is evaluated by\ntraining a model on towers made of a certain number of blocks but testing on\ntowers made of a different number of blocks.\n\nStrengths\n- This paper explores an interesting way to evaluate representations in terms of\n  their generalizability to out-of-domain data, as opposed to more standard\nmethods which use train and test data drawn from the same distribution.\n- Experiments show that the predictions of deep unsupervised learning models on\n  such out-of-domain data do seem to help, even though the models were not\ntrained explicitly to help in this way.\n\nWeaknesses\n\n- Based on Fig 4, it seems that the models trained on 3 blocks (3CD, 3CLD)\n  ``generalize\" to 4 and 5 blocks.  However, it is plausible that these models\nonly pay attention to the bottom 3 blocks of the 4 or 5 block towers in order to\ndetermine their stability. This would work correctly a significant fraction of\nthe time. Therefore, the models might actually be overfitting to 3 block towers\nand not really generalizing the physics of these blocks. Is this a possibility ?\nI think more careful controls are needed to make the claim that the features\nactually generalize. For example, test the 3 block model on a 5 block test set\nbut only make the 4th or 5th block unstable. If the model still works well, then\nwe could argue that it is actually generalizing.\n\n- The experimental analysis seems somewhat preliminary and can be improved. In\n  particular, it would help to see visualizations of what the final state looks\nlike for models trained on 3 blocks but test on 5 (and vice-versa). That would\nhelp understand if the generalization is really working. The discriminative\nobjective gives some indication of this, but might obfuscate some aspects of\nphysical realism that we would really want to test.  In Figure 1 and 2, it is\nnot mentioned whether these models are being tested on the same number of blocks\nthey were trained for.\n\n- It seems that the task of the predicting the final state is really a binary\n  task - whether or not to remove the blocks and replace them with gray\nbackground. The places where the blocks land in case of a fall is probably quite\nhard to predict, even for a human, because small perturbations can have a big\nimpact on the final state. It seems that in order to get a generalizable\nphysics model, it could help to have a high frame rate sequence prediction task.\nCurrently, the video is subsampled to only 5 time steps.\n\nQuality\nA more detailed analysis and careful choices of testing conditions can increase\nthe quality of this paper and strengthen the conclusions that can be drawn from\nthis work.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe particular setting explored in this paper is novel.\n\nSignificance\nThis paper provides a valuable addition to the growing work on\ntransferability/generalizability as an evaluation method for unsupervised\nlearning. However, more detailed experiments and analysis are needed to make\nthis paper significant enough for an ICLR paper.\n\nMinor comments and suggestions\n- The acronym IPE is used without mentioning its expansion anywhere in the text.\n\n- There seems to be a strong dependence on data augmentation. But given that\n  this is a synthetic dataset, it is not clear why more data was not generated\nin the first place.\n\n- Table 3 : It might be better to draw this as a 9 x 3 grid : 9 rows corresponding to the\nmodels and 3 columns corresponding to the test sets. Mentioning the train set is\nredundant since it is already captured in the model name. That might make it\neasier to read.\n\nOverall\nThis is an excellent direction to work and preliminary results look great.\nHowever, more controls and detailed analysis are needed to make strong\nconclusions from these experiments.", "title": "Feed-in ground truth or generated output during training ?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}