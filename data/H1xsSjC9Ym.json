{"paper": {"title": "Learning to Understand Goal Specifications by Modelling Reward", "authors": ["Dzmitry Bahdanau", "Felix Hill", "Jan Leike", "Edward Hughes", "Arian Hosseini", "Pushmeet Kohli", "Edward Grefenstette"], "authorids": ["dimabgv@gmail.com", "felixhill@google.com", "leike@google.com", "edwardhughes@google.com", "seyedarian.hosseini@umontreal.ca", "pushmeet@google.com", "etg@google.com"], "summary": "We propose AGILE, a framework for training agents to perform instructions from examples of respective goal-states.", "abstract": "Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples.  As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed.\nIn a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.", "keywords": ["instruction following", "reward modelling", "language understanding"]}, "meta": {"decision": "Accept (Poster)", "comment": "Pros:\n- The paper is well-written and clear and presented with helpful illustrations and videos.\n- The  training methodology seems sound (multiple random seeds etc.)\n- The results are encouraging.\n\nCons:\n- There was some concern generally about how this work is positioned relative to related work and the completeness of the related work.  However, the authors have made this clearer in their rebuttal.\n\nThere was a considerable amount of discussion between the authors and all reviewers to pin down some unclear aspects of the paper. I believe in the end there was good convergence and I thank both the authors and reviewers for their persistence and dilligence in working through this.  The final paper is much better I think and I recommend acceptance."}, "review": {"H1gboYgu2Q": {"type": "review", "replyto": "H1xsSjC9Ym", "review": "The previous version of the paper was not clear enough in the motivation and uniqueness of the work. After a long and devoted discussion with the authors, we agreed on certain ways of improving the paper presentation, including connection to some related work. \n\nThe current paper is much better, so I would like to raise my score to 6. My revised review is: \n\n[orginality and significance]\n\n+ The paper deals with a challenging navigation problem where natural language instructions can be underspecified and the environment is complex---thus a correct reward function being extremely hard to craft. \n+ The paper proposed to use a <instruction, state> discriminator D to compute a pseudo reward at each step, which is then used to reinforce an agent in natural-language-guided navigation task. The paper proposed to train the discriminator in an adversarial way---with expert supervised data. The idea is neat, and its effectiveness is empirically supported by extensive experimental results.  \n\n[clarity]\n\n+ The paper is well-written. The method is introduced with clear textual description, rigorous math formulations, and good illustration (Figure-1 and -2). The experiments are also well-documented, including training and testing details, results and analysis.   \n\n[quality]\n\n+ The paper was not clear at certain points but the authors had helpful discussions with me and the paper was revised accordingly. \n+ The experiments were done with multiple random seeds, so I believe the results are convincing. The authors did not only show the numerical results but also shared qualitative videos through anonymous URL.  Overall, it is a good paper.\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nBelow is my original review\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n[PROS]\n\n[originality and significance]\n\nThe paper proposed to use a <instruction, state> discriminator D to compute the reward at each step, which is then used to reinforce an agent in natural-language-guided navigation task. The paper proposed to train the discriminator in adversarial way. The idea is neat, and its effectiveness is empirically supported by extensive experimental results.  \n\n[clarity]\n\nThe paper is well-written. The method is introduced with clear textual description, rigorous math formulations, and good illustration (Figure-1). The experiments are also well-documented, including training and testing details, results and analysis. The experiments were done with multiple random seeds, so I believe the results are convincing. The authors did not only show the numerical results but also shared qualitative videos through anonymous URL.  Overall, it is a good paper.  \n\n[CONS]\n\n[quality]\n\nThe major issue of this paper is the lack of connection to existing related work in the field of dealing with reward sparsity problem. This is a long-standing problem in RL (very common in, but not only restricted to, navigation tasks) and people have proposed reward shaping techniques to handle it. But the paper did not discuss any work in this direction. For references, please first check this seminal work and then follow the line of research: \n\nNg, Andrew Y and Harada, Daishi and Russell, Stuart, ICML 1999, Policy invariance under reward transformations: Theory and application to reward shaping\n\nThe method proposed in this paper seems a way of automatically shaping the reward, but loses the optimal policy invariance (for how this invariance is ensured in reward shaping, please check out this tutorial: http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf). \n\nThe proposed method has two key components: 1) the discriminator D; and 2) the adversarial training. The method is shown effective in experiments and outperforms appropriate baselines with actual reward. But the design of D and how it is used as reward function seems somewhat ad-hoc. \n\nD is only trained on the final states of episodes (please correct me if I am wrong), but is used at all the steps as part of reward function to determine the stepwise reward, which seems odd. The authors should discuss what (implicit) assumptions they are relying upon to make this method work in this way. The transformation function from D to reward value seems ad-hoc---e.g. why 0.5, why indicator function instead of others (e.g. scaling of indicator function), how it is generalized to non-1/0 (but still sparse) reward cases, etc? Is the method only designed for 1/0-reward cases? The authors should clearly specify if it is the case. \n\nMoreover, the paper compared to RP (Jaderberg 2016), which still reinforces the agent with actual reward but only *shapes the features of the agent* by multi-tasking on predicting the reward of next step (please correct me if this is wrong). Interestingly, the RP method achieves better performance than the proposed method, although it does not address the reward sparsity problem. Could the authors provide any insight about why this happened? Is there any trade-off between these two methods? Is there any setting, in the authors\u2019 opinion, where the proposed method should outperform RP? \n\n[SUMMARY]\n\nI think this is good work---neat idea, nice results and clear writing. But there are indeed some issues that I hope the authors could address. So I gave a score of 5. \n", "title": "Well-motivated and innovative idea to resolve the reward-absence problem due to ambiguity, variability, and underspecification in natural language instructed RL.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkxYUMmnRX": {"type": "rebuttal", "replyto": "BkgNO00FCm", "comment": "We thank Reviewer 3 for taking the time to consider our response and giving us a detailed explanation of why our current presentation of differences between AGILE and GAIL may be confusing. In the camera-ready version we will make sure to move the explanation of these differences from Section 4 \u201cRelated Work\u201d to Section 2 \u201cAdversarial Goal-Induced Learning from Examples\u201d. The results of the experiments with log D(...) as the reward will also be included in the paper, either in the main text or in the appendix. \n", "title": "Final Remarks"}, "BygfD88v3Q": {"type": "review", "replyto": "H1xsSjC9Ym", "review": "\n==========\nUpdate\n==========\n\nUpon reviewing the paper revision and the author comments to my and the other reviewers' comments, I will revise my suggestion to that of acceptance. As I said in my summary, my primary concern was novelty with respect to prior work which the authors have clarified. They have also increased the rigor of their experimental results by providing variances in the plots.\nI think this work will be of interest to the community.\n\n\n==========\nStrengths:\n==========\n\n- The problem of learning to predict state rewards given language in interesting and useful. \n\n- The proposed AGILE framework is intuitively simple and works with any existing RL framework.\n\n- With the models and tasks explored in this paper, the approach does seem to learn to evaluate whether a state matches the instructions quite well. \n\n- The writing is very clear and direct. \n\n==========\nConcerns:\n==========\n\n[A] The discussion of differences to the closely related GAIL methodology is left until the related work after experiments. Given the similarities between GAIL and AGILE, this seems too late. The authors list three major differences between AGILE and GAIL:\n\t\n1) AGILE is conditioned on a goal specification, language in this case. GAIL is unconditioned and trained for one task.\n2) AGILE takes only the final/goal state rather than a trajectory like in GAIL.\n3) AGILE discretizes the discriminator probability when assigning reward, GAIL does not.\n\nSome concerns about each:\n\t\t\n1) This is an interesting and fair difference but also a necessary and somewhat obvious modification to GAIL in tasks with explicit goal-specification. \n\n2) This does not seem like an improvement, but rather a loss of generality. The authors justify this change saying \"in AGILE the reward model observes only states s_i (either goal states from an expert, or states from the agent acting on the environment) rather than traces (s1, a1),(s2, a2), . . ., learning to reward the agent based on \u201cwhat\u201d needs to be done rather than according to \u201chow\u201d it must be done.\" \n\nIn many real applications, the how is deeply important. For instance, navigation in the world is both a \"what\" (arrive at location X) and a \"how\" (in fastest time without hitting anything or in such a way that humans aren't frightened). Further, the trace includes the final state such that the \"what\" is recoverable in instances where the \"how\" is unimportant, as in the set of tasks presented in this paper. \n\n3) Letting the paper speak on this subject: \"We considered this change of objective necessary because the GAIL-style reward would take arbitrarily low values for intermediate states visited by the agent, as the reward model will be confident as those are not goal states. The binary reward in AGILE carries a clear message to the policy that all non-goal states are equally undesirable.\" Firstly, all non-goal states are not equally undesirable in that some lead more easily to goal states though it is fair to argue this should be learned by the policy through expected reward. My primary gripe is the footnote following these sentences which says: \"We tried values other than 0.5 for the binarization threshold, as well as not binarizing and using D\u03c6(c, st) directly as the reward. We got similar but slightly worse results.\" This seems to imply that this difference does not matter significantly, especially if different thresholds received significantly different hyperparameter tuning effort or were not conducted under multiple runs of random seeds.\n\nA pessimistic summary would place AGILE to be a conditional GAIL with reduced ability to represent intermediate or trajectory based rewards and a possibly slightly helpful reward discretization scheme. Don't get me wrong, I think an conditional extension to GAIL is interesting and worth sharing with the community. However, this discussion comes very late and includes a design decisions (2/3) that I find poorly justified in text and completely unjustified experimentally. \n\nI would like to hear from the authors if any of these criticisms are inaccurate. I would also welcome experiments evaluating the effect of these design decisions.\n\n[B] In 3.2 its reported that each experiment was repeated five times however the presented results are not described as means and no variances are shown. I would like to see the results plots with shaded variances from at least 5 runs with differing random seeds. \n\n[C] Unless I'm mistaken, the proposed architecture could also be trained with reward prediction. It would be interested in that case to see if improvement seen between A3C and A3C-AGILE extend to A3C-RP and A3C-RP-AGILE. As the authors note, the AGILE framework simply changes the source of the reward and is amicable to any RL approach. I would like to see this comparison.\n\n[D] The reward generalization experiments seemed surprising to me. The policy was fine-tuned on the test environments but only improved from 52% to 69.3%. Trying to think about this more, I'm having trouble disentangling whether this implies poor generalization of the reward function or increased difficulty in policy learning. Could the authors provide the A3C and A3C-RP baselines for this experiment to help clarify?\n\n[E] Just a Curiosity: What exactly is done in L2 weight clipping? (Training details in supplement)\n\n[F] Just a Thought: In the reward-prediction (RP) setting, both the RP model and the policy share parameters. It would be possible with such an architecture to still apply the AGILE loss and I would be curious to see if this leads to interesting changes in performance. I understand that one of the advantages to learning a separate reward model is to generalize to new policies, but it is unclear if this approach would generalize less well (and finding it out would be cool!)\n\n==========\nOverview:\n==========\n\nI think extending generative adversarial imitation learning to a task-conditional setting a cool step made even more interesting in this work by having the task-specification be in compositional language. Further, the results and analysis are generally interesting though I do note some weaknesses above. Aside from some questions about the experiments, I'm mostly concerned about the positioning of the paper -- specifically with respect to prior work.  I'm looking forward to hearing from the authors and other reviewers. \n\n\n", "title": "Interesting Work -- Confusing Contextualization with Prior Work", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkeqSjhY0X": {"type": "rebuttal", "replyto": "BygfD88v3Q", "comment": "We would like to report results of several additional experiments that we have run based on the valuable suggestions by Reviewer 3 (R3).\n\n- We have tried to use GAIL-style rewards, r_t = log D(c, s_t). We found that such a modification of AGILE would not perform better than 70% success rate, much as expected. The policy\u2019s return \\sum_t log D(c, s_t) would keep decreasing all the time, as the discriminator got confident in rejecting goal states and outputted very low values of log D(c, s_t), in line with our intuition that using GAIL rewards results in punishing the discriminator arbitrarily for entering intermediate states. We would be happy to incorporate these extra results in the paper should R3 find it necessary. \n- We have compared results of a fine-tuned AGILE policy on the immovable red square task with those of RL with ground truth rewards. We used the soon-to-be-open-sourced PPO-based reimplementation. In this reimplementation fine-tuning PPO-AGILE has improved the performance from 65% to 82%, which is pretty close to the 86% success rate of the PPO baseline. From this we conclude that imperfect 82% performance of fine-tuned PPO-AGILE is mostly due to the increased difficulty in policy learning, and not poor generalization of the reward model. \n\nWe hope that R3 finds these extra results informative. We believe our prior arguments about differences between AGILE and GAIL together with these extra results address most of R3\u2019s concerns, and we hope that they considering revisiting their evaluation of the paper.\n\nBesides, the extra results presented above, we have uploaded a revised PDF with the following changes:\n- standard deviations are now displayed as shades in all plots\n- weight norm clipping is explained better in Appendix and a reference to the paper where this method was proposed is added \n", "title": "final response to Reviewer 3"}, "ryxWDrhFCQ": {"type": "rebuttal", "replyto": "H1gboYgu2Q", "comment": "We thank the reviewer for revising their score, and more importantly, for actively participating in the discussion period. As you well know, with the growth of the field, it has been difficult for conferences to maintain a consistent standard of examination throughout the review process whereby reviewers are guaranteed to be thorough and fair. We appreciate your commitment to the clarification of this paper during this discussion, and without intent of flattery, can assure you that you have been thorough. We ask now, with utmost respect, that you be fair.\n\nYou have revised your review to indicate, through your score, that the paper is marginally acceptable. Yet the substance of your review indicated that, thanks in part to the clarifications you assisted us in producing, the paper is good. You kindly state, in support, that \u201cthe paper deals with a challenging navigation problem\u201d, that \u201cthe idea is neat, and its effectiveness is empirically supported\u201d, that \u201cthe paper is well-written\u201d, and \u201cthe results are convincing\u201d. On the basis of this review, you conclude \u201cOverall, it is a good paper\u201d.\n\nWe appreciate how this request may come across, but after the effort you yourself have committed to the improvement of this paper, we only ask that you either consider assigning it a score which reflects the strength you find in it, or alternatively give us some indication of where it still falls short so as to merit a borderline score. We appreciate, as always, your patience and diligence in this matter, and will respect your decision either way.", "title": "Some final comments"}, "SJlxrIwc6m": {"type": "rebuttal", "replyto": "SklRX9tw67", "comment": "Thank you for your recommendations. We are happy to make modifications to the paper to ensure both greater clarity and better links to existing research. We hope you will find them appropriate, and are open to further tweaks if you judge they are necessary.\n\n> The key motivation/challenge of this work is NOT obvious enough in either abstract or introduction.\n> Maybe you can try having one example in your abstract and introduction? [...] you only need to adjust the positions of examples and make them more connected to the relevant text.\n\nWe make the reason for this paper clearer by adding an example motivating, in our environment, the design of such reward models, in the new Figure 1 which is placed in the introduction.\n\n> I still recommend you to discuss connections to reward-shaping work---it can be light as a few sentences, but it is indeed connected---the simplest setting that you care may have an easy one-to-one instruction->goal state mapping so reward-shaping can be done. In other cases, your work is the solution. \n\nWe add, in the discussion, after \u201cOur analysis of the reward model\u2019s classifications gives a sense of how this is possible; the false positive decisions that it makes early in the training help the policy to start learning.\u201d, the following sentences: \u201cThe fact that AGILE\u2019s objective attenuates learning issues due to the sparsity of reward states within episodes in a manner similar to reward prediction suggests that the reward model within AGILE learns some form of shaped reward (Ng et al, 1999), and could serve not only in the cases where a reward function need to be learned in the absence of true reward, but also in cases where environment reward is defined but extremely sparse. As these cases are not the focal area of this study, we note this but leave such investigation for future work.\u201d\n\nPlease see the updated PDF for changes.\n", "title": "revision uploaded"}, "HkgsOyrm67": {"type": "rebuttal", "replyto": "rylCieSMp7", "comment": "We thank you again for continuing to engage in this discussion and for reading our lengthy response. We truly appreciate your patience and understanding.\n\n> In the GridLU-Arrangements task, there are many possible goal states for any given instruction, each of them should be associated with ground-truth reward 1 (that is used for success rate computation), and other states should give ground-truth reward 0.\n\nThis is correct, although this is true of many tasks and environments across the RL literature (although not all).\n \n> However, there may be a lot of ambiguity and uncertainty in the natural language instructions, and thus variabilities in the goal states.\n\nThe motivation for methods like AGILE is to deal with cases where we want to learn policies that act based on ambiguous or under-specified instructions, for which is may be difficult or impossible to write a reward function within the environment. Instructions in GridLU-Relations may be underspecified, but not to the point where we can\u2019t write a reward function (which admittedly required a surprising amount of code despite the relative simplicity of the instruction language); but this is not the important point, the important point is that we show that a framework which does not have access to the reward (even if the reward function is implemented) and can train a policy as well as one which does, and hence that this framework is useful in environments where it doesn\u2019t have access to the reward *because* the reward function isn\u2019t implemented.\n\n> Even though one can find human annotators for evaluation, it is not practical to use them for training the agents.\n\nYes, being able to deal with such scenarios is the motivation for the development of this framework. We emphasise again that this one of the key underlying motivations for all IRL approaches, not just ours.\n\n> Am I right? \n\nWe think we are on the same page now!\n\nThe question we would put to you, if this addressed your concerns, is (perhaps upon re-reading the relevant sections) is there anything we could clarify to avoid this misunderstanding our ambiguity in portraying our method and its motivation? We have a clear picture of why and how our framework is used, and thus are biased against perceiving unhelpful ambiguities in our presentation, but we are ready to accept that they are there if an informed reader such as yourself had trouble with this aspect of the paper. We are confident that, if there\u2019s anything we could tweak to improve the clarity of the paper, we can do so during the rebuttal period with your help as a byproduct of this discussion.\n", "title": "Further further clarifications :)"}, "r1emqB4Gp7": {"type": "rebuttal", "replyto": "BygfD88v3Q", "comment": "(see part 1 for the beginning of the response)\n\nBearing in mind our main argument that GAIL and AGILE have fundamental differences which can and will be presented in more detail in the revision we are drafting, with the help of your comments and questions, we hope that we have addressed the core concern that \u201cAGILE to be a conditional [IRL, rather than GAIL] ] with reduced ability to represent intermediate or trajectory based rewards and a possibly slightly helpful reward discretization scheme\u201d in that it presents a novel approach to learning inherently multi-task (language-conditional) policies from expert data, provides a set of experiments to prove the concept thereof, and while differentiating itself from related work, is reasonably clear about its self-imposed limitations and about where future research can make improvements.\n\nWe conclude our response to by replying to the rest of your comments:\n\n[B] Thank you for the suggestion to plot shaded variances, we will do so and update our submission as soon as possible.\n\n[C] In our early investigations the reward prediction objective did not help AGILE as much as it helped A3C. We find it unsurprising as reward prediction is especially necessary in cases when the reward is sparse, and AGILE\u2019s reward is more dense than the groundtruth one. This may also have something to do with the fact that the AGILE rewards are constantly changing, and hence predicting them may be not as valuable of an objective. \n\n[D] \u201cWe thank the reviewer for posing this very interesting question. We trained the A3C-RP baseline with immovable red square in our preliminary investigations and it also did not achieve a perfect performance. We believe that learning the policy that can deal with the remaining corner cases may be the bottleneck in this case, however, we will rerun this experiment and update you on the exact numbers.\u201c\n\n[E] The L2 weight clipping makes sure that incoming weights of a neuron have a total L2 norm of at most C, where C is a hyperparameter. This regularization is a hard alternative to weight decay proposed (to the best of our knowledge) in https://arxiv.org/pdf/1207.0580.pdf . We will make sure to add a description of this method in Appendix. \n\n[F] Thank for this interesting experiment suggestion. Sharing weights between the policy and the discriminator is indeed an option, which we have not tried mostly because of the stability concerns. We will definitely consider this in our future work. \n", "title": "on the differences between AGILE and GAIL and other comments (part 2) "}, "rkeb7SNMTm": {"type": "rebuttal", "replyto": "BygfD88v3Q", "comment": "We thank reviewer 3 for their very detailed comments, which while critical of the paper, give us much to respond to, and much to consider with regard to improving the paper. We will be honest: if indeed the crucial failing of the paper is its failure to successfully position itself with regard to GAIL, in particular due to the placement of the paragraph discussing comparison, we find that a score of 4 is a little strict. However, we appreciate the reviewer has been very rigorous in explaining their points regarding this weakness. We hope that through discussion we will both be able to present cogent counter-arguments to these objections where applicable, and otherwise satisfy the reviewer that the positioning of the paper can be improved, to their satisfaction, with minor revisions to the paper.\n\nAs the reviewer correctly summarized and if we put the conditioning on instructions aside, a key key difference (item 2) of AGILE from GAIL is the fact that AGILE uses images of goal-states as the training data instead of state-action trajectories required for GAIL. We believe that this difference does make AGILE applicable to situations where GAIL would not be. For example, consider the case when the expert that provides demonstrations has effectors different from that of the trained agent (image a human teaching their household robot to arrange objects on a table: the robot won\u2019t necessarily have 5 fingers). Besides, methods such as GAIL force the learner to imitate the training trajectories in their entirety, which can be a limitation when training trajectories are suboptimal (the above example with the human is applicable again, human may not use the shortest path to the goal when they perform their manipulations). Relying on goal-states only brings a certain extra flexibility that GAIL-like methods do not provide, admittedly, at the cost of restricting the applicability of AGILE to declarative instructions, i.e. those which can be verified by the final state only. We believe, however, that such instructions make a common and frequent case in instruction-following setups, and methods that are optimized for this case (such as AGILE ) are worth investigating.\n\nReviewer has also requested a clarification with regard to the difference between rewards provided by the discriminator in GAIL and AGILE. We admit that the paper may have been not clear enough on this point, as it stressed the discretization of the reward, whereas the real key difference is the switch from log D(...) as the reward in GAIL to (possibly discretized) D(...) in AGILE. The discriminator is trained to output arbitrary low values of log D(c, s) in AGILE for the non-goal-states, and using log D(c, s) as the reward would arbitrarily punish the policy for entering intermediate states that are clearly not goal-states but yet may be useful in the near future. We are currently performing an extra experiment to validate this claim, in which we use log D(c, s) as the reward, and so far we do not see the job getting above 70% success rate, confirming our intuition that GAIL-style log D(c, s) is not an appropriate reward for AGILE. We will update you on the progress of this additional investigation. \n\nNotwithstanding the above differences with GAIL, it\u2019s true that GAIL and AGILE both can be broadly characterised as inverse RL methods, and as such AGILE could be presented as \u201cjust\u201d extending IRL to the instruction-conditional case (and we believe we are reasonably up-front about this in the introduction). With all due respect, it is unfair to characterise this observation as a concern, or qualify it as an \u201cobvious extension\u201d: it is, to our knowledge, one of the first substantial studies of instruction-conditional IRL over pixel observations. Experiments show generalisation of the reward model to both new observed states and held-out instructions. The model and objective design underpinning these results are, while inspired by GAIL and related techniques, original insofar as new methods (e.g. discriminator rejection) had to be incorporated to make it work. We think that, on the strength of extending IRL techniques to multi-task contexts, where the task is specified by language, is new enough to warrant publication, and hope you will agree.\n\n(see part 2 for continuation)\n", "title": "on the differences between AGILE and GAIL and other comments (part 1)"}, "HkextZ7MaQ": {"type": "rebuttal", "replyto": "H1lXVonl6m", "comment": "With apologies for the length of our response, we hope this further clarifies things and shows the positioning not just of our work, but of IRL-related methods in general (against which the objections you raise here would also be levelled) with regard to the sparse reward problem and reward shaping. These are fundamentally different methods, that address different and orthogonal problems against the goal of teaching agents to act appropriately within the context of a task. If this discussion of the distinction satisfies you, we hope you will consider revising your score. If not, please let us know where our definitions continue to diverge.\n\nWe would like to conclude on the following conciliatory note: while we believe that your suggestions are based on a conflation of the spare reward problem with the case where reward functions are not available, your remarks do seem to highlight that the methods used to model reward in the latter case might be applicable to the former. This is not an unreasonable research direction, and we have had the opportunity to reflect on it, inspired by our discussion. Notwithstanding its merit, this research direction is separate to both the framing and motivation of this paper, and is not explored within the experiments. We could mention this within the further work section of the paper, but it would feel like taking credit for an idea that came out of this discussion, so our preference would be to not do so. Let us know how you feel about this proposed analysis and resolution.\n", "title": "Additional clarification (part 4)"}, "H1llub7G6m": {"type": "rebuttal", "replyto": "H1lXVonl6m", "comment": "> reward function is what we use to REINFORCE the RL agent---it sometimes is the ground-truth (esp. in cases where the ground-truth reward is rich and dense), but sometimes has to be specified (esp. in cases of sparsity.)\n\n*Reward* is what we use to optimise agents using RL algorithms. As stated previously/above, RL algorithms are intrinsically agnostic to where this reward comes from. When a reward function is implemented within the environment, the reward it provides can be used to train an agent. AGILE, GAIL, and other IRL methods are all about *learning* this reward function from auxiliary data, in the absence of a reward function implemented in the environment.\n\n> The ground truth reward is indeed 1-0 (depending on if goal state is reached) in all your cases because you eventually evaluate the RL agent on success rate---it is implicit, but it exists!\n\nWe refer you to our comments about the experimental set up above. The fact that a reward function exists in GridLU-Relations and is used for automated evaluation and the training of traditional RL baselines has not bearing on the nature of AGILE (or any other reward modelling approach). The reward signal it provides is not used, and for all intents and purposes does not exist. It\u2019s sole purpose is to compare methods which do not rely on the provision of an environmental reward function to those which do, with the clear caveat that the latter are not applicable in as wide a selection of settings the former.\n\n> (In more details, when you write ``the agent made the correct arrangement in 58% of the episodes\u2019\u2019, you indeed implicitly assign 1 to the correct arrangement but 0 to others, right?)\n\nAt test time, we explicitly and manually assign 1 to the correct arrangement and 0 to others. This is only for evaluation. The parameters of the model are not updated on the basis of these runs. To be clear: the policy networks require millions of episodes to learn a decent policy (as is the case across DeepRL) and it is not realistic to solicit human evaluation for each of them.\n\n> But in your settings, there is *no (good) reward functions (except the ground-truth one)* available, and you do not want to manually craft one\n\nWith the risk of repeating ourselves with regard to the experimental protocol, for AGILE there is no ground truth reward function. Only for the purpose of evaluating \u201ctraditional RL\u201d baselines, for comparison of the final policies (not the parameter inference procedure!), is there a \u201cground truth\u201d reward function, but it is not used by or exposed to the reward model or policy within AGILE. As an aside, the need for reward models is also motivated by the case where you *can\u2019t* write a reward function (e.g. imagine formally verifying the completion of ambiguous or underspecified instructions like \u201cclean the room\u201d or \u201cset up the table\u201d against a rich environment), not just the case where you don\u2019t want to.\n\n> you design a method to automatically learn a pseudo/surrogate/fake-reward function.\n\nThink of it this way: we want agents to maximise the reward a human would give them if they were observing the agent all the time during training (let\u2019s assume this is impossible, because the agent needs millions of episodes). Typically, reward functions are implemented by humans to \u201csimulate\u201d having this human present during training: they are designed to align with what human judgements would be. Now, if this reward function hasn\u2019t been implemented, what can we do? IRL methods such as GAIL, AGILE, etc are techniques which, based on some evidence from an expert, attempt to \u201creverse engineer\u201d what the reward function expressing alignment with human values would be. So it\u2019s not about learning a \u201cpseudo/surrogate/fake-reward function\u201d so much as it is about trying to learn what the \u201ctrue\u201d reward function could be.\n\n> TLDR: Sparse ground truth, no (other) reward function. Does it make sense to you?\n\nUnfortunately not, as our experiments for AGILE are about \u201c*No* ground truth, no (other) reward function\u201d (except the one which we are learning).\n", "title": "Additional clarification (part 3)"}, "Bkehr-QM67": {"type": "rebuttal", "replyto": "H1lXVonl6m", "comment": "Now, let us turn specifically to your comments.\n\n> every time a *reward* is mentioned in the paper, it means different things---sometimes *ground truth reward* but sometimes *reward functions*.\n\nReward functions are functions which take states (and, in our case, also instructions) and return a scalar reward for the associated timestep where the state was observed. In an environment like Atari games or Go, it is defined within the game logic or the rules of the board game. In \u201cno-reward\u201d scenarios, it has not been implemented, and must be learned by exploiting some auxiliary data (trajectories for GAIL, human preferences in papers like arXiv:1706.03741, or <instruction, goal state example> pairs in AGILE). In contrast, \u201creward\u201d means \u201cwhatever scalar value we are maximising the expected value thereof by changing the agent\u2019s parameters\u201d. It is a scalar value (possibly just 0) provided to the agent at each timestep. The RL algorithm we use to optimise the agent for both the RL baseline (which uses environment reward in experiments where it is available) and the agent in AGILE (which does not, and therefore is applicable in environments/tasks where no reward function exists) is agnostic to the source of the reward. We discuss this at the end of the 2nd paragraph of section 2 (\u201cWe note that Equation 1 differs from a traditional RL [...] the reward model to the environment.\u201d).\n\n> Ground truth reward (term borrowed from your paper and rebuttal) is what we eventually evaluate your RL agent on\n\nThis is perhaps where the source of confusion comes from. In order to compare policies trained using AGILE (a framework meant to be used when there is *no reward* provided by the environment during training) to policies trained under the \u201cidealised\u201d case where an exact reward function exists in the environment (and provided reward), we implemented, in one of our tasks (GridLU-Relations) a reward function, but only use it to train the A3C and A3C-RP baselines, and pretend that it does not exist when training with AGILE. You are correct that we used this \u201cground-truth\u201d reward function to automate evaluation in our experiments, but this is purely for convenience: this evaluation does not in anyway play a role in the updating of agent parameters, reward model parameters, or any other part of AGILE. For all intents and purposes, we could have done this evaluation manually, using a human, (as was done for AGILE-Arrangements) and the results would be exactly the same.\n\n> it always exists (I will shortly claim what ground truth reward I think is in your cases, including GridLU-Arrangements)\n\nThis is true of any environment: the objective reward \u201cexists\u201d in that, for a particular task, a human (or other expert) could observe the environments and provide judgements which, if sufficient in number, could be used to train an agent against an RL objective. As we discussed when contrasting \u201cno reward\u201d scenarios to \u201csparse reward\u201d scenarios, the possibility of occasionally soliciting reward signal from humans and directly optimising the expected value is broadly intractable for all but the most trivial tasks, environments, and agents. When we talk about reward \u201cnot existing\u201d, we simply mean that there is no reward function implemented as a program in the environment which would provide reward signal (sparse or otherwise) which would permit tractable optimisation of an agent.\n", "title": "Additional clarification (part 2)"}, "rkgpQWQMam": {"type": "rebuttal", "replyto": "H1lXVonl6m", "comment": "We thank you for your rapid response, and appreciate your being so willing to engage in the discussion. We say this with the deepest respect: there seems to be some fairly fundamental misunderstanding of AGILE (and reward modelling in general) at play here, but we are ready to accept this my in part be due to how we have explained things. We would like, here, to help clarify this misunderstanding, not only with the intent of convincing you that the comparison to related literature made in the paper is fair and (to the extent it can be with the space allowed) fairly complete, but also with the intent of tweaking the description of the method where necessary so that other readers may not arrive at the same conclusions as you.\n\nBefore responding to your latest comments, allow us to state what we understand to be the \u201csparse reward problem\u201d: in some settings/tasks/environments, the set of states within an episode where the agent receive a scalar reward signal is a very small proportion of the total states experienced (e.g. just the last state). This makes training an agent difficult, since a particularly complex credit assignment problem, with possibly long-range and structured dependencies between rewards and actions, needs to be solved. To this end, techniques such as reward shaping or reward prediction have proposed to give a denser \u201csignal\u201d based on which the agent can effectively learn to maximise the expected reward, in spite of the sparsity of the \u201ctrue\u201d reward signal.\n\nNow consider the cases where there is, quite literally, no reward function implemented in the simulator or environment. This means you *cannot* normally train an RL agent. Of course, a human *could* observe an agent operating in such environments, and assign reward but this fairly obviously would not scale to the requirements of any non-trivial agent and environment (even for classical tabular RL approaches). So in that sense, the IRL literature, GAIL, or the present method (AGILE) are not typically seen as addressing a sparse reward problem, but rather exploiting a small amount of expert data in order to extrapolate the reward function that an expert could conceptually have provided to the environment, but didn\u2019t because it was either impossible (because there is no formalisable verification function over the desired task due to ambiguity or underspecification), or intractable (because implementing such a function would be too onerous). Finally, and perhaps most crucially with regard to the orthogonality between no-reward scenarios and sparse-reward scenarios: the reward provided by a reward model in a no-reward scenario might itself be sparse (and therefore require reward shaping techniques to be applied for the agent to learn a policy).\n", "title": "Additional clarification (part 1)"}, "B1gzqfpeTX": {"type": "rebuttal", "replyto": "rklL8IgSn7", "comment": "Finally, to answer your questions at the end of the review:\n\n1) The primary stability issues that emerge when training have to do with the reward model decaying when the policy starts performing well, which lead to our false negative elimination method introduced in \u201cDealing with False Negatives\u201d, which lead to stable training. Despite the \u201cnon-stationary\u201d nature of the reward model, there was no need to manually change the learning rate during training (although note that we used RMSProp optimisers for both the policy and reward model, which adjusts the learning rate over time as a function of historical gradient norm).\n\n2) In AGILE, as is the case for GANs and GAN-inspired methods such as GAIL, drawing the negative examples from the \u201cgenerative model\u201d (the policy), is crucial for obtaining a discriminator which is tailored to evaluating the policy against the reference goal states. For any non-trivial problem with a reasonably large state space, getting negative examples from a random policy would cover very little of the state space (e.g. it\u2019s unlikely to provide examples of the agent holding an object), so coverage cannot be expected to be very good.\n\n3) We tuned rho via grid-search, as with other hyperparameters. Whereby each instruction corresponds to a specific task, we used a global rho for the entire task set (e.g. GridLU-Relations). We do not have theoretical guarantees to offer, but empirical study shows that training is more stable for low values of rho (trading off longer training for better final results).\n\n4) When holding out 10% of the instructions, these were randomly chosen from the space of possible instructions.\n\nThank you for these questions. We will try to make sure the answers to them are clearer just from reading the paper, in our revised draft.\n", "title": "Thank you for your comments (part 2)"}, "rJebwzal67": {"type": "rebuttal", "replyto": "rklL8IgSn7", "comment": "We thank Reviewer 1 for their review and statement of support for the paper\u2019s technical contributions. We hope, during this discussion period, to get a better understanding of your concerns and hopefully address them, making clarifications in the paper where needed.\n\nFirst, could you please clarify your first concern that \u201cthe proposed approach is a simple combination of A3C and the NMN architecture\u201d? While this is an accurate portrayal of some of our key results, we stress that:\n\n1) we also report results for LSTM + FiLM-ConvNet architectures for both policy and reward model networks, to showcase performance within AGILE when the syntax of the language is not given (see \u201cAGILE with Structure-Agnostic Models\u201d in Section 3).\n\n2) use of A3C for our policy network is not an essential part of AGILE. AGILE is a general framework for jointly training policies and reward models, both conditioned on language instructions. Any RL method and network architecture can be used, since the only difference between \u201ctraditional\u201d RL and AGILE is the source of the reward: in the former case, it comes from the environment, and in the latter, from the jointly learned reward model. As such, for both the baseline and the AGILE-based model, the fact that we used A3C is not important; all that is important is that we use the same RL algorithm and architecture for both the baseline and AGILE.\n\nWith this in mind, do you still believe this aspect of our evaluation is cause for concern?\n\nSecond, you suggest experiments which are more visually realistic with more complex relations. We agree that this is where this research should be going, and discuss such further work in section 5, along with the particular challenges we anticipate it will bring. We would be remiss not to point out two things, with regard to our current experiments:\n\n1) While simple, the environment and tasks are actually quite diverse and approached without the simplifying assumptions typically seen in grid worlds:\n    a) There are over 1,000 instructions in GridLU-Relations, each specifying a different task with millions of different initial environment states to solve.\n    b) The environment is observed, by both policy and reward model, at the pixel level, without predefined notions of what and where objects and their boundaries are, or \u201cprivileged features\u201d indicating which predicates apply to which objects (or even which predicates exist).\n\n2) As we see in the left plot of Figure 3, the A3C baseline results show that, due to the two points made above, even when the reward is specified by the environment (for our baselines), this is quite a difficult multi-task RL problem for fairly modern agent architectures and RL algorithms.  This is because it may take over a dozen steps to optimally obtain a goal state, the task is changing every episode, and reward is sparse. When you add to that the need to jointly infer the reward function from a limited number of expert examples, this constitutes a fairly significant machine learning problem.\n\nWhile we agree that there are no guarantees with regard to scalability to more realistic environments \u201cout of the box\u201d, we hope you will agree this makes a substantial contribution by showing that it is possible to obtain agents which align with expert notions of reward through the proxy of learned reward functions, from examples, and that it is reasonable to leave further developments which scale to more complex environments for further research.", "title": "Thank you for your comments (part 1)"}, "rklL8IgSn7": {"type": "review", "replyto": "H1xsSjC9Ym", "review": "The paper presents an approach for simultaneously learning policies and reward functions for reaching goals that are described by an instruction providing spatial relations among objects. The proposed platform, called Adversarial Goal-Induced Learning from Examples (AGILE), is composed of an off-the-shelf RL module like A3C and a separate module for learning a reward function, implemented using the NMN paradigm. The RL module is trained using the reward function learned by the reward module. The reward module is trained to map a given <instruction, state> into a score between 0 and 1 depending on how well the provided state satisfies the instructions provided in the instruction. The returned score is used as a reward function. The training of the reward function is performed by using a dataset of positive examples, and using the states visited by the agent while it's learning as negative examples. To account for the fact that the agent becomes better over time and its visited states can no longer be used as negative examples, the authors proposed a heuristic where the states visited by the agent are not all used as negative examples, but only those that have the lowest scores.\nThe paper also presents an empirical evaluation of the proposed approach on a synthetic task where the agent is tasked with move bocks of different shapes and colors to a desired final configuration. The AGILE approach was compared to the baseline A3C algorithm where a sparse binary reward signal was used only whenever the agent reaches the goal state. AGILE is also compared to A3C with an auxiliary task of reward prediction. \nThe paper is clearly written and technically strong. However, I have two issues with this paper: 1) the proposed approach is a simple combination of A3C and the NMN architecture, 2) the experiments are performed on simple synthetic tasks that make learning spatial relations fairly easy, I would love to see more real images as it has been demonstrated in prior works on learning  spatial relations. It is not clear from these experiments if the proposed approach will scale up to higher-dimensional inputs. Moreover, there are several stability issues that can be caused by the proposed approach. For instance, the reward function is changing over time, how does that affect the learning rate? Also, instead of using the learned policy itself to generate negative examples and run into non IID data, instabilities, and increasingly good negative examples, why not use a fixed dataset of negative examples generated with a random policy? It would be interesting to do perform an experiment where you compare to the classical reward learning setup where you simply provided labeled positive and negative examples and classify them offline, then use the learned reward function online for RL. \nHow did you tune the hyper-parameter \\rho (percentage of negative examples to discard) for specific tasks? Do you have any guarantees for this approach?\nIn the generalization experiments, it is mentioned that 10% of the instructions are held out. Are these 10% randomized?", "title": "This is a well-written paper. The idea of learning rewards from instructions is interesting although quite straightforward. The experiments show improvement over baseline reward, but they were performed on simple synthetic tasks.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rygJ2iPkpQ": {"type": "rebuttal", "replyto": "H1gboYgu2Q", "comment": "We thank Reviewer 2 for their kind words and detailed review, and for clearly stating what they believe is the limitation of the paper they would like to see addressed. With all due respect, we would be happy to discuss the relation of this work to the sparse reward problem, and to that of learning shaped rewards, but we believe this recommendation stems from a misunderstanding which we hope to clarify through this discussion period.\n\nSimply put, the primary use case of frameworks like AGILE specifically, and of reward modelling / inverse reinforcement learning in general, is where there is *no* reward function implemented (or even obtainable), rather than a sparse one. In such cases, we must learn a reward function from expert-provided information (trajectories, goal states). In contrast, from our understanding of reward shaping in the context of the reward sparsity problem, the ground truth reward is accessible in order to update the policy (or Q/V functions), and a shaped reward function is learned or defined to give a more dense \u201cfake\u201d reward (while preserving optimality guarantees) and make credit assignment easier.\n\nWe understand that the source of this confusion may come from the fact that in one of our experiments (GridLU-Relations), the ground truth reward *is* implemented and *is* sparse, but this is only for the purpose of automated evaluation and the training of baselines for comparison. When training agents with AGILE, or in the other experiments (GridLU-Arrangements) there is no reward provided from the environment.\n\nPerhaps we have misunderstood the point the reviewer is making, in which case we hope they can clarify how reward shaping and the sparse reward problem relate to the no-reward scenario in which our work and other work in inverse RL is situated. However, on the assumption that the reviewer has misunderstood the motivation and problem setting for our approach, we would be grateful if they could re-evaluate their assessment in this light, and/or perhaps let us know where would could have been clearer so as to not potentially confuse future readers along similar lines.\n", "title": "the paper is not about reward sparsity, but about learning the reward"}}}