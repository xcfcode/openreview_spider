{"paper": {"title": "Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization", "authors": ["Takayuki Osa", "Voot Tangkaratt", "Masashi Sugiyama"], "authorids": ["osa@mfg.t.u-tokyo.ac.jp", "voot.tangkaratt@riken.jp", "sugi@k.u-tokyo.ac.jp"], "summary": "This paper presents a hierarchical reinforcement learning framework based on deterministic option policies and mutual information maximization. ", "abstract": "Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.  \nIn our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.  Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.", "keywords": ["Hierarchical reinforcement learning", "Representation learning", "Continuous control"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a method for hierarchical reinforcement learning that aims to maximize mutual information between options and state-action pairs. The approach and empirical analysis is interesting. The initial submission had many issues with clarity. However, the new revisions of the paper have significantly improved the clarity, better describing the idea and improving the terminology. The main remaining weakness is the scope of the experimental results.\nHowever, the reviewers agree that the paper exceeds the bar for publication at ICLR with the existing experiments."}, "review": {"BylepC6n3m": {"type": "review", "replyto": "Hyl_vjC5KQ", "review": "The authors propose an HRL algorithm that attempts to learn options that maximize their mutual information with the state-action density under the optimal policy.\n\nSeveral key terms are used in ways that differ from the rest of the literature. The authors claim options are learned in an \"unsupervised\" manner, but it is unclear what this means. Previous work (none of which is cited) has dealt with unsupervised option discovery in the context of mutual information maximization (Variational intrinsic control, diversity is all you need, etc), but they do so in the absence of reward, unlike this paper. \"Optimal policy\" is similarly abused, with it appearing to mean optimal from the perspective of the current model parameters, rather than optimal in any global sense. Or at least I think that is what the authors intend. If they do mean the globally optimal policy, then its unclear how to interpret Equation 8, with its reference to a behavior policy and an advantage function, neither of which would be available if meant to represent the global optimum.\n\nEquation 10 comes out of nowhere. One must assume they meant \"maximize mutual information\" and not \"minimize\", but who knows. Why is white-noise being added to the states and actions? Is this some sort of noise-contrastive estimation approach to mutual information estimation? It doesn't appear to be, but it is unclear what else could motivate it. Even the appendices fail to shine light on this equation.\n\nThe algorithm block isn't terribly helpful. The \"t\" variable is used outside of its for loop, which draws into question the exact nesting structure of the underlying algorithm (which isn't obvious for HRL methods). There aren't any equations referenced, with the option policy network's update not even referencing the loss nor data over which the loss would be evaluated.\n\nSome of the experimental results show promise, but the PPO Ant result raises some questions. Clearly the OpenAI implementation of PPO used would have tuned for the OpenAI gym Ant implementation, and the appendix shows it getting decent results. But it never takes off in the harder RlLab version -- were the hyper-parameters adjusted for this new environment?\n\nIt is also odd that no other HRL approaches are evaluated against, given the number cited. Running these methods might be too costly, but surely a table comparing results reported in those papers should be included.\n\nA minor point: another good baseline would be TD3 with the action repeat adjusted to be inline with the gating policy.\n\nI apologise if this review came off as too harsh -- I believe a good paper can be made of this with extensive rewrites and additional experiments. But the complete lack of clarity makes it feel like it was rushed out prematurely.\n\nEDIT: Now this is a paper that makes sense! With the terminology cleared up and the algorithm fully unpacked, this approach seems quite interesting. The experimental results could always be stronger, but no longer have any holes in them. Score 3-->6", "title": "Potentially interesting idea, but a very poorly written paper", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1gOFYTK3m": {"type": "review", "replyto": "Hyl_vjC5KQ", "review": "Revision: The authors addressed most of my concerns and clearly put in effort to improve the paper. The paper explains the central idea better, is more precise in terminology in general, and the additional ablation gives more insight into the relative importance of the advantage weighting. I still think that the results are a bit limited in scope but the idea is interesting and seems to work for the tasks in the paper. I adjusted my score to reflect this.\n\nSummary:\nThe paper proposes an HRL system in which the mutual information of the latent (option) variable and the state-action pairs is approximately maximized. To approximate the mutual information term, samples are reweighted based on their estimated advantage. TD3 is used to optimize the modules of the system. The system is evaluated on continuous control task from OpenAI gym and rllab.\n\nFor the most part, the paper is well-written and it provides a good overview of related work and relevant terminology. The experiments seem sound even though the results are not that impressive. The extra analysis of the option space and temporal distribution is interesting. \n\nSome parts of the theoretical justification for the method are not entirely clear to me and would benefit from some clarification. Most importantly, it is not clear to me why the policy in Equation 7 is considered to be optimal. Given some value or advantage function, the optimal policy would be the one that picks the action that maximizes it. The authors refer to earlier work in which similar equations are used, but in those papers this is typically in the context of some entropy maximizing penalty or KL constraint. A temperature parameter would also influence the exploration-exploitation trade-off in this \u2018optimal\u2019 policy. I understand that the rough intuition is to take actions with higher advantage more often while still being stochastic and exploring but the motivation could be more precise given that most of the subsequent arguments are built on top of it. However, this is not the policy that is used to generate behavior. In short, the paper is clear enough about how the method is constructed but it is not very clear to me *why* the mutual information should be optimized with respect to this 'optimal' policy instead of the actual policy one is generating trajectories from.\n\nHRL is an interesting area of research with the potential to learn complicated behaviors. However, it is currently not clear how to evaluate the importance/usefulness of hierarchical RL systems directly and the tasks in the paper are still solvable by standard systems. That said, the occasional increase in sample efficiency over plain TD3 looks promising. It is somewhat disappointing that the number of beneficial option is generally so low. To get more insight in the methods it would have been nice to see a more systematic ablation of related methods with different mutual information pairings (action or state only) and without the advantage weighting. Could it be that the number of options has to remain limited because there is no parameter sharing between them? It would be interesting to see results on more challenging control problems where the hypothesized multi-modal advantage structure is more likely to be present.\n\nAll in all I think that this is an interesting paper but the foundations of the theoretical motivation need a bit more clarification. In addition, experiments on more challenging problems and a more systematic comparison with similar models would make this a much stronger paper.\n\nMinor issues/typos:\n- Contributions 2 and 3 have a lot of overlap.\n- The \u2018o\u2019 in Equation 2 should not be bold font. \n- Appendix A. Shouldn\u2019t there be summations over \u2018o\u2019 in the entropy definitions?\n\n\n", "title": "Interesting ideas and analysis but somewhat unclear motivation and limited empirical evaluation", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJxEm9nX0Q": {"type": "rebuttal", "replyto": "Hkgrq6jvaQ", "comment": "- The PPO baseline is updated to address the concern from the reviewer 3\n- The experimental results are updated to include the performance of a variant of the proposed method which does not use the advantage-weighted importance for computing mutual information.\n", "title": "Update log"}, "Syegx9hmAX": {"type": "rebuttal", "replyto": "rJx6n5iDa7", "comment": "To evaluate the benefit of the advantage-weighted importance, we evaluated a variant of adInfoHRL, which does not use the advantage-weighted importance for computing mutual information. The results show that the proposed method outperforms the version without the advantage-weighted importance on all the four tasks. We added the result of the results in the revised manuscript.\n", "title": "The performance without advantage-weighted importance"}, "BJlN6Qoy07": {"type": "rebuttal", "replyto": "ryex2YGdpm", "comment": "We performed a parameter sweep to tune the performance of PPO, and we updated the result graph. We observe that there is a trade-off of the performance across the tasks. For example,  when obtaining the better performance on Ant-rllab, the performance on the Walkder2d-v1 gets lower. We picked the hyperparameters of PPO that give the performance comparable to the one reported in [Haarnoja, ICML 2018],  although the hyperparameters of PPO used in [Haarnoja, ICML 2018] are not provided.", "title": "PPO baseline is updated"}, "Hkgrq6jvaQ": {"type": "rebuttal", "replyto": "Hyl_vjC5KQ", "comment": "Dear reviewers\n\nThank you for constructive comments. We made major revision, especially on the part where we motivate and explain our method. We believe that the manuscript is significantly improved thanks to the reviewers\u2019 comments.\n\nHere is the summary of our revision and answers to reviewers\u2019 questions\n1.\tRemoval of the term \u201coptimal policy\u201d\nIn the initial manuscript, a policy of the form \\frac{\\exp(A(s,a ))}{Z} is referred to as \u201coptimal policy\u201d, and we removed this expression. We consider a policy of this form in order to reduce the problem of finding the modes of the advantage function to that of finding modes of the probability density of state-action pairs. Any policy from which a sample is drawn and that results in a higher return with higher probability can be used for this purpose. In the revised manuscript, a policy of the form \\frac{f(A(s,a ))}{Z} is referred to as \u201ca policy based on the advantage function,\u201d  where f is a monotonically increasing function with respect to the input variable. We replaced \\exp with a monotonically increasing function f in the revised manuscript so that we can emphasize that the form of Equation 7 is not limited to the exponential function. Although we used f() = exp() in our implementation and a policy of the form \\frac{\\exp(A(s,a ))}{Z} is optimal in entropy-regularized RL, our method is not related to entropy-regularized RL. We revised the manuscript to avoid the confusion.\n\n2.\tClarification of the motivation of using the advantage-weighted importance\nWe can reduce the problem of finding the modes of the advantage function to that of the modes of the density of state-action pairs with the advantage-weighted importance. However, without the advantage-weighted importance, modes of the density of the state-action pairs induced by an arbitrary policy do no correspond to those of the advantage function in general. We revised the manuscript to clarify this point.\n\n3.\tBenefit of the deterministic option policies\nReviewer 1 questioned the benefit of the deterministic option policies. When learning stochastic option policies, the option-value function needs to be learned in addition to the action-value functions. As discussed in Section 4 in the revised manuscript, the option-value function does not need to be learned, since it can be estimated from the action-value function and the option policies when the option policies are deterministic. When option policies are stochastic, learning the option-value function needs to be updated if the option policies are updated. However, in the case of deterministic option policies, this additional learning cost is not necessary. Hence, the use of deterministic option policies can be more sample-efficient than that of stochastic option policies. \n\n4.\tComparison with other HRL methods\nWe put a table for comparison with recent HRL methods in Appendix. In terms of the achieved returns, our method outperforms IOPG (Smith et al., ICML 2018). Compared with SAC-LSP (Haanoja, ICML2018), our method outperforms SAC-LSP on Walker2d and Ant-rllab, and SAC-LSP shows its superiority on Hopper.\n\n5.\tRevision of premature descriptions\nReviewer 3 pointed out some issues of the description in Algorithm 1, and Reviewer 2 also pointed out some typos. We modified those points and revised several descriptions to improve the clarity. In addition, the term \u201cunsupervised\u201d was confusing in the initial manuscript, we removed the related descriptions. We also cited missing related work, such as variational intrinsic control and diversity is all you need.\n", "title": "summary of our revision and answers to reviewers\u2019 questions"}, "SJxeXTsD6Q": {"type": "rebuttal", "replyto": "BylepC6n3m", "comment": "Thank you for the comments. We revised our manuscript to clarify the motivation. Please refer to the above post for details. We also answer your question here.\n\n-\tClarification of the objective function for learning the latent variable\nReviewer 3 raised a concern on the objective function for learning the latent variable. The objective function is based on regularized information maximization (RIM) . Since the objective function is negative to the MI term, the latent variable is learned by minimizing the objective function. The KL term in the objective function is the regularization term based on virtual adversarial training (VAT). We revised our manuscript to make the story more easily followable.\n\n-\tHyperparameters of PPO\nWe used the default parameters in the baseline implementation, and we did not tune the parameter for Ant-rllab. We fixed the hyperparameters of adInfoHRL and TD3 as well, and we did not tune hyperparameters for specific tasks. The hyperparmeters are provided in Appendix.", "title": "Clarification of the objective function for learning the latent variable"}, "rJx6n5iDa7": {"type": "rebuttal", "replyto": "H1gOFYTK3m", "comment": "Thank you for the comments. We revised our manuscript to clarify the motivation. Please refer to the above post for details. We would also like to clarify some points here.\n\n- The reason why the advantage-weighted importance is necessary\nIf we do not use the advantage-weighted importance, we learn the latent variable with respect to the density of state-action pairs visited during the learning phase. However, modes of such a density correspond to not modes of the advantage function but the current location of the option policies. Therefore, the latent variable learned without the advantage-weighted importance do not improve the location of the option policies. By using the advantage-weighted importance, we can learn the discrete variable that corresponds to the modes of the advantage function. \n", "title": "The reason why the advantage-weighted importance is necessary"}, "BklAL5sPT7": {"type": "rebuttal", "replyto": "BJg4ANaphm", "comment": "Thank you for the comments. We answer some of your questions here. Please refer to the above post for other concerns and questions.\n\n- questions about information maximization\nOur approach is to maximize the mutual information between the latent variable of the hierarchical policy and the state-action pairs, which results in learning discrete representations of the state-action space. We revised the manuscript to clarify this point.\n\n- Please add more discussion on why the options are switched at every step\nThe options are not switched at every time step as shown in Figure 2. For example, the option indicated by yellow is activated for about 30 time-steps at most. \n\n- Question about whether our method is off-policy or not\nWe do not intend to list \u201coff-policy\u201d as one of the contributions, although it is one of the features of our approach. Our approach is off-policy in several points even though we employed an on-policy buffer for learning the options. In our method, samples are collected using a behavior policy instead of the \u201craw\u201d learned policy, and both the Q-function and the option policies are trained using the replay buffer in an off-policy manner. Therefore, we think that our method should be categorized as an off-policy method.\n\n-\tAvailability of the advantage function\nWe do not assume the availability of the advantage function. In practice, it is necessary to approximate the advantage function. Our approach finds the latent variable with respect to the current estimate of the advantage function. Since the Q-function converges to the optimum as learning progresses, our method can learn the latent variable with respect to the optimal advantage function at convergence. In actor critic, policy parameters are updated with respect to the current approximation of the Q-function or the advantage function. Likewise, one can interpret that the latent variable of our hierarchical policy is updated with respect to the current approximation of the advantage function in our method.", "title": "Our method learns discrete representations of the state-action space"}, "BJg4ANaphm": {"type": "review", "replyto": "Hyl_vjC5KQ", "review": "The paper considers the problem of hierarchical reinforcement learning, and proposes a criterion that aims to maximize the mutual information between options and state-action pairs.\n\nThe idea of having options partition the state-action space is appealing, because this allows options visit the same states, so long as they act differently, which is natural. The authors show empirically that the learned options do indeed decompose the state-action space, but not the state space.\n\nThere is a lot in the paper already, but the exposition could be much improved. Many of the design choices appear very ad hoc, and some are outright confusing. Some detailed comments:\n\n* I got really confused in Section 3 re: advantage-weighted importance sampling. Why do this? If the option policies are trying to optimize reward, won\u2019t they become optimal eventually (or so we usually hope in RL)? This section seems to assume that the advantage function is somehow given. It also doesn\u2019t look like this gets used in the actual algorithm, and in fact on page 5 it is stated that \u201cwe decided to use the on-policy buffer in our implementation\u201d. Then why introduce the off-policy bit at all, and list it as a contribution?\n* Please motivate the choices. The paper mentions that one of its contributions are options with deterministic policies. This isn\u2019t a contribution unless it addresses some problem that stochastic policies fail at. For example, DPG allows one to address continuous control problems.\nSame with using information maximization. The paper literally states that \u201can interpretable representation can be learned by maximizing mutual information\u201d. Representation of what? MI between what?\n* Although the qualitative results are nice (separation of the state-action space), empirical results are modest at best. This may be ok, because based on the partition of the state-action space it seems that the option policies learn diverse behaviors in the same states. Maybe videos visualizing different options from the same states would be informative.\n* Please add more discussion on why the options are switched at every step", "title": "Good idea, exposition can be much improved", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}