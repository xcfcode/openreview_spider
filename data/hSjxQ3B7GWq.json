{"paper": {"title": "Sample-Efficient Automated Deep Reinforcement Learning", "authors": ["J\u00f6rg K.H. Franke", "Gregor Koehler", "Andr\u00e9 Biedenkapp", "Frank Hutter"], "authorids": ["~J\u00f6rg_K.H._Franke1", "~Gregor_Koehler1", "~Andr\u00e9_Biedenkapp1", "~Frank_Hutter1"], "summary": "SEARL trains a population of off-policy RL agents while simultaneously optimizing the hyperparameters and the neural architecture sample-efficiently.", "abstract": "Despite significant progress in challenging problems across various domains, applying state-of-the-art deep reinforcement learning (RL) algorithms remains challenging due to their sensitivity to the choice of hyperparameters. This sensitivity can partly be attributed to the non-stationarity of the RL problem, potentially requiring different hyperparameter settings at various stages of the learning process. Additionally, in the RL setting, hyperparameter optimization (HPO) requires a large number of environment interactions, hindering the transfer of the successes in RL to real-world applications. In this work, we tackle the issues of sample-efficient and dynamic HPO in RL. We propose a population-based automated RL (AutoRL) framework to meta-optimize arbitrary off-policy RL algorithms. In this framework, we optimize the hyperparameters and also the neural architecture while simultaneously training the agent. By sharing the collected experience across the population, we substantially increase the sample efficiency of the meta-optimization. We demonstrate the capabilities of our sample-efficient AutoRL approach in a case study with the popular TD3 algorithm in the MuJoCo benchmark suite, where we reduce the number of environment interactions needed for meta-optimization by up to an order of magnitude compared to population-based training.", "keywords": ["AutoRL", "Deep Reinforcement Learning", "Hyperparameter Optimization", "Neuroevolution"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper tackles a very important topic in deep RL, namely automatic (non-differentiable) hyper-parameter tuning. It does so by combining ideas from genetic algorithms and neural architecture search with shared experience replay in order to obtain the key property of sample efficiency.  The proposed solution is communicated clearly, and the results are compelling (often 10x improvements), as well as qualitatively interesting.\n\nUnfortunately for the authors, their original submission contained only part of the intended results, hence the borderline scores by some reviewers. In the meanwhile, a second suite of experiments have been added, which I think are compelling enough evidence to validate the paper's approach."}, "review": {"U_wfxmPwhM": {"type": "rebuttal", "replyto": "hSjxQ3B7GWq", "comment": "Again, we thank all reviewers for their valuable time and feedback. As mentioned in previous responses, we performed an additional SEARL experiment for meta-optimizing Rainbow DQN on the Atari benchmark suite. We add the paragraph about \u201cGeneralization to Different Algorithms and Environments\u201d in Section 4.4 \u201cResults & Discussion\u201d and an additional Section J in the Appendix with a detailed experiment description. \n\nDue to the limited time and the computational budget available to us, we were only able to run experiments for SEARL and random search, with a limited frame count and random seeds. For the final / camera-ready version of this paper, we will increase the number of random seeds, the number of tested Atari environments, as well as environment frames to a similar extent as for the TD3/MuJoCo case study. We will also provide results from our modified PBT approach. However, we can already show similar benefits in terms of sample-efficiency and in performance as observed in the TD3/MuJoCo case study. \nWe expect this sample-efficiency and performance gain to be present irrespective of the off-policy algorithm being optimized or the environment in which we apply SEARL since the advantages of the proposed meta-optimization in SEARL are generally applicable to any off-policy RL algorithm.\nFor this reason, we do not expect the findings observed in the current state to change fundamentally with the additional number of seeds/environment steps.\nAdditionally, all code changes corresponding to additional experiments requested by the reviewers will be updated in the supplemental material.\n\nIf we cleared up some of your concerns, we would kindly ask you to update your assessment.\n", "title": "Additional Experiments"}, "vh4Uwmoo2-w": {"type": "rebuttal", "replyto": "ydvhaf6qvnu", "comment": "We appreciate your thoroughness regarding our evaluation. \nHowever, we do want to point out that there is, unfortunately, a lack of comparable AutoRL approaches that tune both the architecture and hyperparameters simultaneously during training the architecture. We provide evidence that SEARL is sample-efficient because of the shared replay memory. Any black-box optimization approach is by design less sample-efficient. What might be less obvious about SEARL is the final agent\u2019s performance. To test this, we compare SEARL to the best randomly sampled configuration of a strong baseline RL algorithm in each respective environment.\n\nFurther, we want to point out that PBT without modifications cannot serve as a baseline. We were only able to include it in the TD3 case study due to the modifications described. As even the modified PBT variant we used for this case study does not have a mechanism to efficiently use all experiences gathered by the individual agents, we\u2019re confident that SEARL\u2019s advantages w.r.t. sample efficiency will also hold in this case. However, in the final version of the paper, we could provide results from our modified PBT variant in the Arcade Learning environment.\n\nRegarding your concerns about the importance of the architecture search, we also want to point out that the experiments in the Arcade Learning Environment suggest that SEARL is capable of navigating even the arguably more complex search space of such vision-based RL tasks. In this context, SEARL adapts both linear and convolutional layers including the channel size, kernel size, and stride. \n", "title": "Response to AnonReviewer2 "}, "-Yu30p6sRL": {"type": "rebuttal", "replyto": "zNTfCLxfObh", "comment": "Please excuse the late update of our paper caused by the computational expense of our new experiment. We would like to point you to the recently added experiments in our paper, meta-optimizing Rainbow DQN on Atari environments. Please find details in the recent response to all reviewers. We understand your wish to see results for the latest and perhaps most challenging benchmark of ProcGen. However, due to the time constraints in this rebuttal period, we were unable to meet the computational needs required to perform ProcGen experiments. We hope the provided additional experiments can address your concerns at least to some extent by showing the general usefulness of SEARL to meta-optimize different algorithms in different environments.", "title": "Response to AnonReviewer2"}, "yTvFL2WNIT": {"type": "rebuttal", "replyto": "hWhOiAAwBpb", "comment": "Thank you for your kind response. As an update, we would like to point you to the recently added experiments in our paper, meta-optimizing Rainbow DQN on four Atari environments. Please find details in the recent response to all reviewers. While these experiments are still not ideal in terms of the number of seeds used and environment steps shown, we do hope that they provide you with the necessary insights to evaluate SEARL\u2019s general usefulness to meta-optimize different RL algorithms in different environments. \n", "title": "Response to AnonReviewer1"}, "l74y-Uqv4q9": {"type": "rebuttal", "replyto": "sGq2JUEqn7Y", "comment": "We thank the reviewer for mentioning the novelty of our approach and the comprehensive evaluation. \n\nWe fully understand the wish for more experiments on different off-policy algorithms and environments. However, ProcGen seems too expensive for a detailed evaluation in an AutoRL setting since we train at least 10 agents in parallel (especially for the short time window allocated for rebutting). As an alternative, also in line with requests by other reviewers, we implemented the popular Rainbow-DQN [1] on Atari environments, meta-optimized with the SEARL algorithm. These experiments are expensive as well. Thus, we do not expect to be able to finish the full 200million frames experiments commonly reported for Atari [1] during the short author response period. But we do believe this would be a valuable experiment and will include it in the final version of the paper.\n\nRegarding the impact of architecture adaption:\nArchitecture choices have been shown to be important in RL. For example, Henderson et al. [2] experimentally evaluated the impact of the network architecture choices and found that it does impact the performance of the trained agents. Further, our own ablation study indicates that architecture adaptation is sometimes very beneficial. Specifically, without architecture adaptation, there is a significant performance drop in HalfCheetah and Ant. We updated the paper and included our ablation study from Appendix H into the main paper. \nWe also added a paragraph in the main body, plus a new Appendix I, in which we focus on the found architectures. SEARL finds, e.g., for HalfCheetah larger actor-network sizes ( in average 2.8 layers/1000 nodes per network) compared to Walker2d ( in average of 1.7layers/680nodes per network). This could indicate that different environments require different network capacities. We also note that experiments with smaller, static network architectures in TD3 don't achieve the same performance as the setting reported in TD3. This suggests that growing networks like in SEARL constitutes achieving strong performance with small network sizes.\n\nAgain, thank you a lot for your review!\n\n[1] Hessel, Matteo, et al. \"Rainbow: Combining improvements in deep reinforcement learning.\" AAAI. (2018).\n\n[2] Henderson, Peter, et al. \"Deep Reinforcement Learning That Matters.\" AAAI. (2018).\n", "title": "Response to AnonReviewer2"}, "gGDpf4_KvMf": {"type": "rebuttal", "replyto": "kdhMor3nwqu", "comment": "Thank you very much for the positive rating and the kind words. We appreciate the pointer to recent related work and updated the paper with the following discussions on the mentioned works.\nThe work of Zoph et al. [1] on RL for neural architecture search (NAS) is an interesting counterpart to our work on the intersection of RL and NAS: they do RL for NAS (using RL to search better architectures), while we do NAS for RL (changing architectures to improve RL).\nIn Schmitt et al. [2], the on-policy experience is mixed with shared experiences across concurrent hyperparameter sweeps to take advantage of parallel exploration. However, this work does not tackle dynamic configuration schedules or architecture adaptation.\nIn [3], a subset of differentiable hyperparameters are meta-optimized in an outer loop. This, however, does not extend to non-differentiable hyperparameters and thus does not allow for online tuning of the network architecture. Such hyperparameters can therefore not be meta-optimized in the framework proposed by Zahavy et al.\n\nImpact of architecture adaptation:\nThe network architecture has in fact been shown to impact performance in RL before. For example, Henderson et al. [4] experimentally evaluate the impact of architecture choices on MuJoCo. Further, our own ablation study indicates that architecture adaptation is sometimes very beneficial. Specifically, without architecture adaptation, there is a significant performance drop in HalfCheetah and Ant. We updated the paper and included our ablation study from Appendix H in the main paper. We agree with the interest in a more complex domain and further experiments. Therefore we implemented the popular Rainbow-DQN [5] on Atari environments, meta-optimized with the SEARL algorithm. These experiments, however, are very expensive, and we thus do not expect to be able to finish the full 200million frames experiments commonly reported for Atari [5] during the short author response period. But we do believe this would be a valuable experiment and will include it in the final version of the paper.\n\nThank you again for your valuable remarks!\n\n\n\n[1] Zoph, Barret, and Quoc V. Le. \"Neural architecture search with reinforcement learning.\" ICLR. (2017).\n\n[2] Schmitt, Simon, et al. \"Off-policy actor-critic with shared experience replay.\" arXiv preprint arXiv:1909.11583 (2019).\n\n[3] Zahavy, Tom, et al. \"A self-tuning actor-critic algorithm.\" Advances in Neural Information Processing Systems 33 (2020).\n\n[4] Henderson, Peter, et al. \"Deep Reinforcement Learning That Matters.\" AAAI. (2018).\n\n[5] Hessel, Matteo, et al. \"Rainbow: Combining improvements in deep reinforcement learning.\" AAAI. (2018).\n", "title": "Response to AnonReviewer1"}, "hVtPEixto0m": {"type": "rebuttal", "replyto": "eZwoC2vi9JF", "comment": "We thank the reviewer for the appreciation of our work and the valuable comments.  \nWe focused on off-policy RL since it is by design generally more efficient due to the usage of a replay memory. In SEARL, off-policy RL can benefit from a diverse population in an evolutionary HPO approach. On-policy remains future work.\nOn the baselines: \nDue to the lack of applicable baselines to AutoRL approaches, we use random search (RS) since it is a commonly used HPO technique in RL. In our experiments, RS also acts as a performance baseline for HPO systems rather than a particularly sample-efficient method. The lack of sample-efficiency in PBT is one of the crucial motivations behind SEARL. Adding a shared replay buffer to a PBT-like approach is indeed a crucial part of our contributions, but our approach goes further to explicitly adopt the architecture of the neural networks in the deep RL agent. We added a discussion on recent additional related work as pointed out by reviewer 1, which includes a differentiable HPO approach.\n\nDetails of the search results:\nWe provided further insights on the learning rate adaptation in Appendix G, and we now use the additional page we have during the author response period to give these insights in the main paper. We now also added a paragraph in the main body, plus a new Appendix I, in which we focus on the found architectures. SEARL finds, e.g., for HalfCheetah larger actor-network sizes (in average 2.8 layers/1000 nodes per network) compared to Walker2d (on average of 1.7layers/680nodes per network). This could indicate that different environments require different network capacities. In such cases, SEARL is capable of automatically adapting the network size to the task complexity.\nAnother advantage of the dynamic configuration in SEARL lies in the possibility to achieve on-par performance reported in the TD3 paper with significantly smaller network architectures. Experiments with smaller, static network architectures in TD3 don't achieve comparable performance. This suggests that growing networks like in SEARL is essential for achieving strong performance with small network sizes.\n\n\nRegarding the comparison to simple heuristics:\nWe are not aware of any methods for learning rate scheduling in RL or AutoRL which include architecture adaptation (besides evolutionary algorithms we refer to in related work). It is notable that the hyperparameter schedules found by SEARL differ for the individual environments. This could indicate that simple heuristics would also perform well in some environments but not in all. Furthermore, the main goal of SEARL is efficient, automated hyperparameter optimization. The comparison against a handcrafted heuristic is not straight forward, as we cannot easily quantify the computational effort used in creating handcrafted heuristics (it would surely require hyperparameter optimization of its own, so in order to avoid optimizing on the environment of interest (and therefore already paying environment interactions for the tuning stage) one would have to perform leave-one-environment-out hyperparameter optimization. However, in an effort to investigate this valuable point, we ran experiments using simple heuristics to grow the network architecture within the TD3 algorithm over time. As a minimal change, we started using the \u201cadd nodes\u201d operator and the same initialization scheme as implemented in SEARL once after the first 200,000 steps in TD3. However, we consistently observe a huge performance drop at this point across all environments, from which TD3 seems unable to recover. This strongly suggests that TD3 itself, without frameworks such as SEARL, is not fit to include simple architecture change heuristics. We suspect that SEARL benefits from the diverse samples in the replay memory to allow more meaningful training updates.\n\nAgain, thank you for reviewing!", "title": "Response to AnonReviewer3"}, "2iDshpwdD0-": {"type": "rebuttal", "replyto": "V5xH1ySgVf", "comment": "Thank you for your helpful feedback and interest in our paper. We are glad you liked the motivation and clarity of the paper. We fully understand the wish for more experiments on different off-policy algorithms and environments. To this end, we implemented the popular Rainbow-DQN [1] on Atari environments, meta-optimized with the SEARL algorithm. These experiments, however, are very expensive, and we thus do not expect to be able to finish the full 200million frames experiments commonly reported for Atari [1] during the short author response period. But we do believe this would be a valuable experiment and will include it in the final version of the paper.\n\nConcerning the comparison to the related AutoRL work:\nAs can be seen in [2], which evaluates [3] on MuJoCo, they do not focus on sample-efficiency in AutoRL: their approach trains thousands of configurations from scratch using individual RL runs. To put SEARL\u2019s sample efficiency in perspective: applying the same fair evaluation protocol as proposed in our paper, it arrives at the same performance with up to 3 orders of magnitude fewer samples on the shown environments (see Figure 1 in evolving rewards paper, scaled by 1000 agents they use). This number is not directly comparable since they use SAC and PPO instead of TD3, but we strongly believe that any differences caused by the choice of algorithm will be far smaller and will not drastically change the sample efficiency achievable with SEARL. We fully agree that we should have made this difference more obvious in our paper and thus point to the fair evaluation protocol already in the related work section.\n\u2018Learning to design RNA\u2019 applies BOHB [4] by treating the RL training as black-box and does not focus on online optimization (like PBT or SEARL) nor sample-efficiency. In contrast to a black-box optimization, we jointly train the agent and dynamically optimize the hyperparameters (like PBT). We include random search as a fundamental baseline and to measure performance within the search space. \n\nAddressing the experimental evidence on benefits for the non-stationary nature of RL:\nOur intent by \u201cnon-stationary\u201d was focused on the training, e.g. at the beginning of the training some hyperparameter settings are potentially more suitable than others in comparison to later training stages. We see that our wording could be misread and therefore we update the paper accordingly.  Furthermore, in Appendix G we provide details on how the hyperparameters change during the training in different environments. Notably, different environments lead to different hyperparameter schedules (e.g., in HalfCheetah the learning rate decays while in the Hopper and Reacher environment the learning rate even increases). In terms of network architectures, we observe a benefit from a growing architecture in HalfCheetah, whereas the architecture stays small in the Ant environment. We updated the paper with a more detailed discussion on the ablation.\n\nOn the missing details:\nWe again thank you for your feedback in this regard and updated the paper to describe the technical details more thoroughly. \nWe initialize the hyperparameters of the architecture with a small (potentially too small) architecture and keep the learning rate of Adam\u2019s default value. The hyperparameters could also be initialized randomly without large performance differences, see Appendix E. This suggests that there is very little domain-specific knowledge required to effectively use the SEARL framework.\nIt is common practice in RL that the agent gets as many update steps as interactions with the environment (we refer to works introducing e.g. TD3, DQN, PPO). This is motivated by the fact that each new observed frame is a new sample that can be used in training. However, we reduce the number of update steps since it would be (A) very expensive since we train a whole population and not a single agent, and (B) since we benefit from the diverse samples gained from the diverse population of agents during our evaluation phase. The more diverse the samples are in the replay buffer, the larger the training gain we can expect from a mini-batch SDG update. This allows us to reduce the number of total update steps per agent.\n\nAgain, thank you for your time and effort. We appreciate your feedback.\n\n\n[1] Hessel, Matteo, et al. \"Rainbow: Combining improvements in deep reinforcement learning.\" AAAI. (2018).\n\n[2] Faust, Aleksandra, et al. \"Evolving rewards to automate reinforcement learning.\" AutoML Workshop ICML. (2019).\n\n[3] Chiang, Hao-Tien Lewis, et al. \"Learning navigation behaviors end-to-end with autorl.\" IEEE Robotics and Automation Letters 4.2 (2019).\n\n[4] Falkner, Stefan, et al. \"BOHB: Robust and efficient hyperparameter optimization at scale.\" ICML. (2018).\n", "title": "Response to AnonReviewer4"}, "INoIA_khp6C": {"type": "rebuttal", "replyto": "hSjxQ3B7GWq", "comment": "We appreciate the interest in our paper and thank the reviewers for their constructive and valuable feedback. To summarize, we updated our paper-based feedback by:\nincluding the ablation study w.r.t. different SEARL features in the main paper, showing their benefit in individual environments instead of the mean impact;\nadding more details about the training process to facilitate reproducibility and improve clarity;\nadding more detailed discussions of (additional) related work;\nincluding an additional Appendix section (I), in which we focus on the found architectures by SEARL.\nIn this updated version, we directly include the appendix as part of the paper for easier navigation. The appendix was previously only part of a zip archive in the supplemental material. In the following, we address each review individually.\n\nWe again want to thank all reviewers for their time and effort.\n", "title": "General Response"}, "kdhMor3nwqu": {"type": "review", "replyto": "hSjxQ3B7GWq", "review": "Motivated by the sensitivity of RL algorithms to the choice of hyperparameters and the data efficiency issue in training RL agents, the authors propose a population-based automated RL framework which can be applied to any off-policy RL algorithms. In the framework, they optimise hyperparameters together with neural architectures. The authors use TD3 on MuJoCo environments as a showcase to demonstrate the advantages of the proposed method. They reduced the number of environment interactions significantly compared to baselines like random search and a modified population-based training algorithm.\n\nPros:\n+ The authors propose an important novel component to PBT-like framework, which is sharing experience among agents in the population. This innovation itself leads to 10x improvement on sample efficiency;\n+ Very clear description of the motivations, related work, the details of the training framework, and the experiments. The paper reading has been super pleasant;\n+ Results of TD3 on MuJoCo environments, i.e., the ones in Figure 2, are very promising. In the five environments tested, SEARL algorithm shows an order of magnitude improvement on the sample efficiency.\n\nCons:\n- The proposed sharing experience among agents in the population is limited to off-policy RL algorithm, as also noted by the authors;\n- There are a few lines of AutoRL and AutoML research are missing in references or in discussions. For example, the architecture search lines of work from Zoph et al. , and for gradient-based meta parameters optimisation like \"A Self-Tuning Actor-Critic Algorithm\";\n- For the main contribution in the paper, shared experience replay within the population, there's a recent work by Schmitt et al. called \"Off-Policy Actor-Critic with Shared Experience Replay\" which also demonstrates that sharing experience replay in off-policy learning can improve sample efficiency dramatically. Though they are not in the setting of meta learning hyperparameters or architectures, I think it worths discussions;\n- From the ablations in Figure 5, it looks like the architecture adaption does not contribute much to the final performance. It might be because MuJoCo environments do not require complex neural architectures. To show that the neural architecture adaptation is a crucial part of the framework (which is a major difference between the proposed method and PBT), the authors might have to move to a complex domain of environments to demonstrate that.", "title": "Review", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "eZwoC2vi9JF": {"type": "review", "replyto": "hSjxQ3B7GWq", "review": "Summary: This paper propose a population-based AutoRL framework for hyperparameter optimization of off-policy RL algorithms. The framework optimizes both the hyperparameters and also the neural architecture in a one-shot manner, e.g., search and train at the same time. A shared experience replay buffer is used across the population, which as demonstrated in the experiments, substantially increase the sample efficiency compared to PBT and random search. \n\nStrengths:\n- The idea is simple and intuitively makes sense. By sharing the experiences across the population each experience sample gets re-used more often during the training hence the increase in sample efficiency.\n\nWeakness:\n- Only apply to off-policy RL. \n- Random search is not a very compelling baseline. PBT is not optimized for sample efficiency. It seems that the shared replay buffer can also be applied to PBT? What about differentiable HPO methods? Those are often shown to be much more sample efficient compared to evolution based approaches. \n- There is little discussions on what the search results look like, e.g., the learning rate schedule or the neural architecture found by the search method. How are they compared to the SOTA learning rate schedule or neural architecture? Or maybe not even SOTA just compare to some simple heuristics that gradually decrease the learning rate or increase the network size. It would be of limited value if the use of AutoRL can only achieve marginally better performance compared to those simple heuristics. ", "title": "Official Blind Review", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "sGq2JUEqn7Y": {"type": "review", "replyto": "hSjxQ3B7GWq", "review": "# Summary\n\nThe authors in this paper propose to optimize the hyperparameters and also the neural architecture while simultaneously training the agent. They evaluate the proposed method with TD3 in the MuJoCo benchmark suite. \n\nOverall, the proposed method is well-motivated and well-written, and they provide enough experiment/implementation details to reproduce the results. More importantly, they provide the source code. \n\n# Strength\n\n- Tuning architecture and hyperparameters with PTB for off-policy RL has not been studied before. Compared to baselines, the proposed method is much more sample-efficient.\n- The evaluation on MuJoCo is comprehensive, especially the ablation study in Section 4.\n\n# Weakness\n\n- They only test on a single benchmark with one method, TD3. MuJoCo is arguably simple. Actually, the visual world in MuJoCo is quite limited, so the encoder of the RL agent does not have to be huge. It could be more convincing if the authors can test on another benchmark, e.g. ProcGen. \n- I think compared to computer vision tasks with huge neural networks, the search space for the architecture of RL models is much smaller, which can be observed in the ablation study. Without architecture adaption, there is no observable difference. It seems that combining the tuning of architecture and hyperparameters is not that useful.", "title": "Experiments are not convincing", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "V5xH1ySgVf": {"type": "review", "replyto": "hSjxQ3B7GWq", "review": "Summary\uff1a\nIn this paper, the authors intend to propose an efficient automated reinforcement learning (RL) framework. To achieve this goal, they integrate three technologies, i.e., evolutionary RL for hyperparameter search, evolvable neural network for policy network design, and shared experience replay for improving data usage. The paper uses a case study on MuJoCo to demonstrate the claimed advantages over baselines.\n\nSome pros:\n1. The motivation is good. The automation of reinforcement learning is beneficial to the research community, especially for researchers who are not familiar with RL but in need of it.\n2. The framework is general and friendly to users. This framework is general and can be regarded as a plug-in module for a series of reinforcement learning methods. Besides, as we all know, both of the autoML and RL have a heavy computational burden, the adaption of evolvable neural network and shared experience replay greatly alleviate this dilemma. \n3. The organization of this paper is easy to follow. We can follow the authors from why they want to deal with the problem, to how they are inspired by existing work, and then to how the algorithms are design based on the questions to be answered and the existing technologies. \n\nSome cons:\n1. The experiment is far from enough. Actually, this paper only has a case study. First, the author claims that the framework can optimize arbitrary off-policy RL algorithms, why only try on TD3? From the perspective of robustness, the authors need to compare more off-policy algorithms with and without the proposed method. Second, the paper claims there is no directly comparable approach for efficient AutoRL, which I do not agree with. In its own related work, many AutoRL baselines are listed, e.g., H. L. Chiang et al.  'Learning navigation behaviors end-to-end with autorl', F. Runge et al. 'Learning to design RNA'. For these baselines, they can either take the same exploration steps and compare performance with the proposed method, or compare the performance/computational cost when reaching the same performance. Anyway, the readers expect to see more comparisons with more baselines from more perspectives. Third, the authors claim that to tackle the non-stationarity of the RL problem, existing studies can substantially increase the number of environment interactions, implying the proposed framework has advantages on non-stationarity RL environments, but still, no experimental results are given.\n2. Some technical details are missing. The logic is clear, the solution is reasonable, but the details are ignored. It is a good idea to keep the writing compact, but the lack of details may harm the readability of the paper. For example, since this is a general framework, how should we design hyperparameter settings of SEARL in initialization for different algorithms? In the training part, why should individual be trained for as many steps as frames have been generated in the evaluation phase, and why the training time could be reduced by using only a fraction j of the steps? ", "title": "A good paper with some flaws", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}