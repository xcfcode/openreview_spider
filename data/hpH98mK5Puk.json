{"paper": {"title": "InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective", "authors": ["Boxin Wang", "Shuohang Wang", "Yu Cheng", "Zhe Gan", "Ruoxi Jia", "Bo Li", "Jingjing Liu"], "authorids": ["~Boxin_Wang1", "~Shuohang_Wang1", "~Yu_Cheng1", "~Zhe_Gan1", "ruoxijia@vt.edu", "~Bo_Li19", "~Jingjing_Liu2"], "summary": "We propose a novel learning framework, InfoBERT, for robust fine-tuning of pre-trained language models from an information-theoretic perspective, and achieve state-of-the-art robust accuracy over several adversarial datasets on NLI and QA tasks.", "abstract": "Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust \ufb01ne-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) a Robust Feature regularizer, which increases the mutual information between local robust features and global features. We provide a principled way to theoretically analyze and improve the robustness of representation learning for language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks.\nOur code is available at https://github.com/AI-secure/InfoBERT.", "keywords": ["adversarial robustness", "information theory", "BERT", "adversarial training", "NLI", "QA"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper introduces two regularizers that are meant to improve out-of-domain robustness when used in the fine-tuning of pretrained transformers like BERT. Results with ANLI and Adversarial SQuAD are encouraging.\n\nPros:\n- New method with concrete improvements in several difficult task settings.\n- New framing of adversarial generalization.\n\nCons:\n- The ablations that are highlighted in the main paper body don't do a good job of isolating the specific new contributions. (Though the appendix provides enough detail that I'm satisfied that the main empirical contribution is sound.)\n- Reviewers found the theoretical motivation very difficult to follow in places."}, "review": {"nb6GgAh1z9G": {"type": "rebuttal", "replyto": "vhrC5Mujzf_", "comment": "Thank you very much for acknowledging our work as interesting and solid, as well as for the valuable careful reading. \n\nYour comments have substantially improved our manuscript. Thank you!\n", "title": "Thank you for the follow-up comments "}, "q6O1fiOCsFZ": {"type": "rebuttal", "replyto": "SlTviuAfBi", "comment": "Thank you for the valuable follow-up comments.\n\n**Q1**: \u201cwhy is equation 7 true in the general case? Suppose n=1, is this essentially saying that any sample from an empirical distribution would provide a lower bound for the true distribution\u201d\n\n**A1**: Thank you for the follow-up question. We are sorry for making the confusion. Note that $X_i$ does not refer to the training examples in the dataset to represent true distribution. As discussed in the paragraph above Equation (7), $n$ is sequence length and $X=[X_1, X_2, \u2026, \u2026, X_n]$ representing input sentences with $X_i$ being word token at $i$-th index. Here, theorem 3.1 proposes a feasible way to calculate $I(X;T)$: when $n$ is large, $X$ and $T$ are high-dimensional features that are difficult to compute $I(X;T)$, therefore we can instead optimize the lower bound of $I(X;T)$ by factoring $X$ and $T$ into their localized representation $X_i$ and $T_i$, and calculate $n\\beta \\sum_{i=1}^{n} I(X_i; T_i)$ that are easier to compute.\n\n**Q2**: \u201cthe proofs need to be elaborated upon\u201d\n\n**A2**: The intuition of theorem 3.1 is discussed in the previous answer. Theorem 3.2 wants to bound the adversarial performance drop $| I(Y;T) - I(Y; T') |$ by the mutual information between X and T. Specifically, we use equation (19), where the conditional entropy $|H(T \\mid Y) - H(T' \\mid Y)|$ can be bound by Eq. (36) in terms of MI between benign/adversarial input and representation $I(X_i;T_i)$ and $I(X'_i;T'_i)$, and the second summand $|H(T) - H(T')|$ is upper-bounded by a constant (Eq. (79)). The details of both proofs can be found in Appendix A.3. We have made the illustration of theorems more clear in the revision.\n\nWe have updated a new version to OpenReview. Please let us know if it solves the concern. Thank you!\n", "title": "Thank you for the follow-up comments"}, "wJg70v37u41": {"type": "rebuttal", "replyto": "hpH98mK5Puk", "comment": "We thank all the reviewers for their time and valuable suggestions. Based on the reviews, we have corrected several typos and made the illustration more clear. We also include more experimental results.\n\nSpecifically, we made the following revisions:\n1. We revised the equation typos and followed the common practice in terms of notations in Section 3.\n2. We added more illustrations on our definition and theorems in Section 3.\n3. We added more description of evaluation metrics in Section 4.\n4. We made the ablation studies that we studied more clear in Section 4.\n5. We fixed the typos in Appendix A.3.\n6. Previously we investigated the impact of the two regularizers via ablation studies from Appendix A.2.1 (Ablation Study on IB regularizer) and A.2.2 (Ablation Study on Anchored Feature Regularizer). We now include another section A.2.3 (Ablation Study on Disentangling Two Regularizers) to compare the results and make the conclusion more clear. We show that both regularizers can individually improve the robust accuracy on top of FreeLB with a similar margin. After combining both regularizers, InfoBERT achieves the best robust accuracy.\n\nAll of our revisions are updated in OpenReview now and highlighted in blue. Thank you!", "title": "General Response"}, "ZTn-RQhdcO3": {"type": "rebuttal", "replyto": "qOM3wROZy57", "comment": "Thank you for your valuable comments.\n\n**Q1:** \u201cdisentangle the performance contributions of the two proposed objectives\u201d\n\n**A1:** We investigate the independent impact of the two regularizers via ablation studies in Appendix A.2.1 (IB regularizer) and A.2.2 (Anchored Feature Regularizer). To make it more clear, we have added another section A.2.3 (Ablation Study on Disentangling Two Regularizers, updated now in OpenReview). In Table 8, we show that both regularizers can individually improve the robust accuracy on top of FreeLB with a similar margin. After combining both regularizers, InfoBERT achieves the best robust accuracy. We will make it more clear in the revision.\n\n\n**Q2:** \u201cisn't it common practice to put the $dy \\, dt$ at the very end of the integral?\u201d\n\n**A2:** We are following the practice of [1] for the lower bound of $I(Y;T)$. We have followed your suggestion and updated the equations in OpenReview. Thanks for pointing it out.\n\n\n**Q3:** \u201cleaving out punctuations from equations for equation 5\u201d\n\n**A3:** Thanks for the careful reading. Equation (5) is not the end of the sentence, but followed by a short sentence \u201cwith data samples  $\\\\{ {x}^{(i)}, {y}^{(i)} \\\\}_{i=1}^{N}$.\u201d \n\n\n**Q4:** \u201canother 1/N inside the square brackets\u201d\n\n**A4:** For the upper bound of $I(X;T)$, we follow the derivation from [2]. Specifically, Eq. (4) is estimated with data samples $\\\\{x^{(i)}, y^{(i)}\\\\}$ based on Eq. (15) in [2], where we calculate the conditional distribution via positive pair $\\\\{x^{(i)}, t^{(i)}\\\\}$ and negative pairs $\\\\{x^{(i)}, t^{(j)} \\\\}$ (for $j=1$ to $N$).\n\n\n**Q5:** \u201cEquation (7)\u201d\n\n**A5:** Yes, you are right. In particular, when $n=1, X = [X_1],  I(X; T) = \\sum_{i=1}^{n} I(X_i; T_i)$. Therefore, LHS=RHS, and it provides the lower bound for Equation (1).\n\n\n\n**Q6:** \u201cDefine stability and robustness\u201d\n\n**A6:** We define \u2018stability\u2019 as the stability of feature representation for local words before and after potential perturbation. By Algorithm 1, we ensure that local anchored features are perturbed under the threshold $c_h$. Here theorem 3.1 and 3.2 only focus on robustness.\n\nThe definition of robustness is the performance on the adversarial input, and we try to improve robustness by diminishing the performance gap between adversarial input and benign input. Since $I(Y;T)$ and $I(Y;T\u2019)$ can be regarded as model performance on benign and adversarial data ([1][2]), we bound the performance gap $||I(Y;T) - I(Y;T\u2019)||$ by $I(X;T)$ and $I(X\u2019; T\u2019)$, and formalize that the performance gap will become closer when $I(X;T)$ and $I(X\u2019; T\u2019)$ get closer, thus leading to better adversarial robustness.\n\n\n**Q7:** \u201cEq (9)\u201d\n\n**A7:** N is the size of the dataset. M is the number of local anchored features. Sorry for the typos in our previous version. We have updated it, and have added the multiplier $n$ back.\n\n\n**Q8:** \u201cIntermediate steps from Eq (6) to Eq (9)\u201d\n\n**A8:** Yes, you are right that Eq (6) is followed by Eq (9). In particular, we add the anchored feature regularizer (the $\\alpha$ term) to Eq (6). The anchored feature regularizer tries to align local stable features $T_{k_j}$ found by Algorithm 1 with sentence-level global representation $Z$, and increase MI $I(T_{k_j}; Z)$ between them. Therefore, we add this term from previous objective function (Eq (6)) to Eq (9) and add the coefficient \\alpha to control the trade-off. \n\n[1] Alemi, A.A., Fischer, I.S., Dillon, J.V., & Murphy, K. Deep Variational Information Bottleneck. (ICLR 2017)\n\n[2] Cheng et al. CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information. (ICML 2020)\n", "title": "Response to Reviewer #1"}, "6iS1FDKO5uY": {"type": "rebuttal", "replyto": "GWQ5A8zbRRp", "comment": "Thank you for your careful reading and comments.\n\n**Q1:** \u201cslightly worse performance on the more difficult tasks\u201d\n\n**A1:** We thank the reviewer for the insightful observation. To diversify the tasks, we conduct a comprehensive evaluation on common NLP tasks, including NLI and QA. We observe an overall improvement in our experiments. Specifically,  InfoBERT with adversarial training can achieve 5-10% robust accuracy improvement for ANLI, and around 6-point F1 score improvement for QA over the baseline. We do notice that the margin on TextFooler is relatively smaller (1-5%) and it could be potentially due to the fact that adversarial training has also improved the model against such specific attacks, therefore the final improvement can be marginal (1-5%). But if we compare with the vanilla baseline without adversarial training, the improvement is still significant (5-10%), which indicates that adversarial training is very effective in defending against adversarial attacks such as TextFooler, while it is not the case for other adversarial datasets. We will add this discussion in our revision.\n \n\n**Q2:** \u201cglobal representation Z\u201d\n\n**A2:** Thanks for pointing this out. BERT uses a final-layer hidden vector corresponding to the first input token ([CLS]) as sentence global representation (Devlin, J. et al. \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\u201d NAACL-HLT (2019)). We follow the practice of (Delvin, et al. 2019) and use the final-layer CLS embedding to represent global sentence-level features. We will make it more clear in the revision.\n\n**Q3:** \u201cdefinition 3.1\u201d\n\n**A3:** Our definition is based on (Jacobsen, J. et al. \u201cExcessive Invariance Causes Adversarial Vulnerability.\u201d(2019)). Due to the discrete nature of text input space, we adapt the definition to NLP by measuring adversarial distortion in the token embedding space. We will make it clear in our revision. \n\n\n**Q4:** \u201cit should be stated which argument is maximized\u201d\n\n**A4:** Thanks for pointing it out. $\\arg \\max$ selects the class with the highest logits/probability. We have added the description in the revision (updated now in OpenReview).\n\n\n**Q5:** \u201ccomments 4-7\u201d\n\n**A5:** We sincerely thank the reviewer for the careful reading, and we have corrected our typos in revision (updated now in OpenReview).\n", "title": "Response to Reviewer #3"}, "ZDMEHp8MoC0": {"type": "rebuttal", "replyto": "i_dm6rU--D_", "comment": "Thank you for your valuable comments. \n\n**Q1:** \u201ctargets the token-level (local) representation\u201d\n\n**A1:** Thank you for pointing this out, and you are right that the lack of robustness due to local superficial features is one of the main motivations. In addition, we also discuss different design choices of adopting token-level (local) representation v.s. sentence-level (global) features via ablation experiments in Appendix A.2.1 Table 6. While both features can boost model robustness, we observe that using local features yields higher robust accuracy than global ones. We will make it more clear in the revision.\n\n\n**Q2:** \u201cInterpretation of experimental results and choice of baselines\u201d\n\n**A2:** Sorry for the confusion. We will make the comparison clear in the revision. \nFirst, we emphasize that InfoBERT is not ensembled with FreeLB. Different from (Tram\u00e8r et.al 2018), InfoBERT does not augment adversarial data, but serves as regularizers added during training without additional computational cost.\nSecond, from theoretical perspective, Theorem 3.2 shows that reducing MI between *adversarial input* and representation $I(X\u2019;T\u2019)$ can lead to further improvement on robustness, thus applying InfoBERT to adversarial training is fair and standard in our evaluation setting. As a result, we indeed compare InfoBERT against other SOTA adversarial training baselines, such as SMART, which combines adversarial training and Bregman proximal point optimization as regularizers to improve robustness. We show that InfoBERT achieves the best robust accuracy among existing adversarial training methods. Besides, we show that even when applying InfoBERT individually, it achieves better or comparable performance than other baselines (Table 1 for RoBERTa and Table 4).\n\n\n**Q3:**  \u201cMissing evaluations and ablations\u201d\n\n**A3:** Detailed analysis of the individual impact of the two regularizers is provided in ablation studies in Appendix A.2.1 (IB regularizer) and A.2.2 (Anchored Feature Regularizer). To make it more clear, we have added another section A.2.3 (Ablation Study on Disentangling Two Regularizers, which is updated now in OpenReview). In Table 8, we show that both regularizers can individually improve robust accuracy on top of FreeLB with a similar margin. After combining both regularizers, InfoBERT achieves the best robust accuracy. We discuss the strategies for hyper-parameter tuning for both regularizers in Appendix A.1. We will make it more clear in the revision.\n\n\n**Q4:**  \u201cFormulation of the method\u201d\n\n**A4:** Our work mainly focuses on improving the robustness of language representations by formulating Information bottleneck as a regularizer, and our method aims to improve both local (word-level) representations and global (sentence-level) features; while Li & Eisner (2019) has a different formulation and setting, which focuses on designing a model (rather than a flexible regularizer) to extract information from local word representations. \nThanks for mentioning the interpretation. We design the anchored features by observing the magnitude of textual attack perturbation in the embedding space. We also provide visualization of the \u201cadversarial saliency map\u201d in ablation study in Appendix 2.2 Table 7. We will discuss related work and analysis in the revision.\n\n\n**Q5:** \u201cMore tasks and more attacks\u201d\n\n**A5:** Thanks for the suggestion. We indeed evaluated the proposed InfoBERT on existing common NLP tasks, comparing with SOTA baselines. Specifically, we choose two high-quality adversarial NLP datasets (ANLI and adversarial SQuAD) for different tasks (NLI and QA), plus one dataset created by effective textual adversarial attack TextFooler, and compare InfoBERT against SOTA adversarial training approaches including ALUM and SMART. \nWe will add more evaluation against adversarial attacks in the future. Please let us know if there are further suggestions on the evaluation.\n\nWe hope that you can raise your score if you find our answers address your questions. Thank you!\n", "title": "Response to Reviewer #4"}, "qOM3wROZy57": {"type": "review", "replyto": "hpH98mK5Puk", "review": "This work (InfoBERT) proposes additional objectives for transformer finetuning to obtain models more robust to adversarial inputs. The authors first propose a mutual information based information bottleneck objective, next the authors propose an adversarial loss inspired method for identifying robust features and a subsequent objective to emphasize the mutual information between global representations and these robust features. The experiments demonstrate that InfoBERT consistently outperforms other adversarial training approaches on a variety of adversarial evaluations.\n\nI largely follow the logic behind the derivation, however I find some of the details unclear. I would like to see proofs for the theorems as well as an explanation of the assumptions under which the theorems hold. The experimental results are convincing, however there are no ablation studies to disentangle the performance contributions of the two proposed objectives. For the first point, the questions I have are as follows:\n\nfor equations 1-3, I find the integral notation to be a bit odd - isn't it common practice to put the dydt at the very end of the integral? Also you should consider leaving out punctuations from equations\nfor equation 5, why is there another 1/N inside the square brackets?\nwhy is equation 7 true in the general case? Suppose n=1, is this essentially saying that any sample from an empirical distribution would provide a lower bound for the true distribution?\nI'd like to see a proof for Theorems 3.1 and 3.2\nhow do the authors define stability and robustness? The manuscript talk about them in vague terms and they do not seem to be precisely defined\nhow does equation 9 follow from 6? Can you put in the intermediate steps? Also in this case what is N and what is M? And what happened to the multiplier n from equation 6?", "title": "Weak accept", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "GWQ5A8zbRRp": {"type": "review", "replyto": "hpH98mK5Puk", "review": "\n##########################################################################\n\nSummary:\n\nThe paper proposes a novel learning framework for robust (against adversarial attacks) fine-tuning of pre-trained language models, that is based on information theoretic arguments. It introduces two regularization mechanisms and investigates their efficacy on various tasks.\n\n##########################################################################\n\nReasons for score: \n\nOverall I vote for accept. The approach is novel, interesting and well presented. The theoretical results seem to be sound. It also seems to outperform competitors in the field of adversarial language models. Concerning the experiments some questions remain, but I hope that the authors will address them in the rebuttal.\n\n##########################################################################\nPros: \n\n1. The idea is interesting and well formulated. The theoretical results seem to be correct to me.\n\n2. The approach is tested on several standard datasets used in adversarial language models. It seems to outperform previous approaches.\n\n3. The paper is well written and clearly structured\n\n\n##########################################################################\n\nCons: \n\n\n1. In my view the experiments seem to show a tendency towards a slightly worse performance on the more difficult tasks in comparison to the competitor methods. Thus, the better overall performance on the ANLI data could be driven by the easier tasks.  \n\n\n2. I couldn't find a clear description of the \"global representation\" Z. A more explicit description would be helpful.\n\n\n##########################################################################\n\nQuestions during rebuttal period: \n\n\nPlease address and clarify the cons above \n\n\n#########################################################################\n\nMinor comments: \n\n1. Is definition 3.1. a standard definition or is it introduced by the authors? \n\n2. Page 4, definition 3 contains an incomplete sentence (\"The q(x')....\").\n\n3. Page 6, \"Evaluation Metrics\": it should be stated witch argument is maximized.\n\n4. Page 16, Lemma A1: in the Proof of the lemma I think that all instances of Y_i should be replaced with T_i. In formula (13), in the rightmost term the token index n should be i-1.\n\n5. Page 17, formula (33): H(Y|Y) should probably be H(T|Y). Same goes for equ. (36).\n\n6. Squares might be missing in formulas (37) to (43).\n\n7. A reference to formula (44) would be nice.  \n", "title": "InfoBERT review", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "i_dm6rU--D_": {"type": "review", "replyto": "hpH98mK5Puk", "review": "## Summary:\nThe paper proposes two regularizers for finetuning pretrained mask LMs to improve the robustness of NLI and SQuAD models. When evaluated on adversarial NLI and SQuAD datasets, adding the regularizers to regular finetuning achieves a robust accuracy comparable to adversarial training baselines; adding the regularizers are added to adversarial training baselines archives extra robustness gains.\n\nThe first regularizer is an implementation of the Information Bottleneck principle specialized for contextual text representations. In the general IB objective, we seek to maximize the mutual information between the representation and the label, as well as minimizing the mutual information between the representation and the input. In this paper, the authors targets the token-level BERT representation. This design choice was not discussed in detail, but I assume it\u2019s motivated by the observation that our models\u2019 lack of robustness is often manifested in an overreliance on local, superficial features.\n\nThe second regularizer has a similar motivation. But instead of minimizing the mutual information between the input and the representation, the \u201canchoring feature\u201d regularizer minimizes the mutual information between the global representation and the token level ones, specifically those that are \u201cnonrobust and unuseful\u201d. To identify \u201cnonrobust and unuseful\u201d tokens in each input, the authors use a heuristic based on input gradient norm, similar to how interpretability people generate heatmaps for text classification.\n\n## Detailed comments\nInterpretation of experimental results and choice of baselines: The abstract claims that \u201cInfoBERT achieves state-of-the-art robust accuracy\u201d. This is not accurate. The best numbers reported in this paper are achieved by applying InfoBert regularizers to FreeLB adversarial training. This can be seen as an ensemble of two (or three, if you count the two regularizers separately since they can be applied independent of each other) adversarial training methods. Ensembling usually helps robustness (see for example Tram\u00e8r et.al ICLR 2018 Ensemble Adversarial Training: Attacks and Defenses). For a fair comparison, FreeLB should be ensemble with another adversarial training method, or with FreeLB applied to a second model. When applied individually, the gain from InfoBERT has a much smaller advantage compared to the baselines.\n\nMissing evaluations and ablations: An obvious ablation is missing: apply the two regularizers separately. I\u2019m especially curious if both lead to gains on top of FreeLB. The paper has a sort of disproportionate treatment of the two regularizers. Both theorem 3.1 and 3.2 are talking about the IB regularizer, while a lot of design choices for the anchoring feature regularizer are proposed without justification or verification, e.g. the portion of useful and robust tokens. The anchoring feature regularizer relies on various heuristics (definition of usefulness and robustness), and if it turns out to be the main contributing factor to InfoBERT, it would be good to know - if others would like to apply InfoBERT on other tasks, they might need to tune these hyperparameters.\nFormulation of the method: The authors cite the \u201clocalization\u201d of the IB principle in the IB regularizer as part of the novelty of the method. However, this kind of localization can be found in e.g. Li & Eisner 2019: Specializing Word Embeddings (for Parsing) by Information Bottleneck (EMNLP), which is one of the first applications of IB principle to NLP with pretrained, contextualized representations. In the anchoring feature regularizer, the use of input gradient norm for filtering \u201cnonrobust and unuseful\u201d tokens is reminiscent of how interpretation methods generate saliency maps for text classification. Both are missing from the references.\n\nImplications and verification of theorem 3.2: this is a minor point but in my opinion theorem 3.2 is a bit of an overkill just to solidify the intuition that \u201cthe performance gap becomes closer when I(X_i, T_i) and I(X\u2019_i, T\u2019_i) decreases\u201d. It would be nice to verify empirically through experiment that this theorem is correct.\n\nFinally, I encourage the authors to evaluate the method on more tasks and attacks, and especially focus on comparing against the naive adversarial training baseline. It would be good to have a better understanding of how much gain InforBERT brings and what are the most important factors.\n", "title": "not super convincing results. important ablations missing", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}