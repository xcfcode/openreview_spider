{"paper": {"title": "Towards Stable and comprehensive Domain Alignment: Max-Margin Domain-Adversarial Training", "authors": ["Jianfei Yang", "Han Zou", "Yuxun Zhou", "Lihua Xie"], "authorids": ["yang0478@e.ntu.edu.sg", "hanzou@berkeley.edu", "yxzhou@berkeley.edu", "elhxie@ntu.edu.sg"], "summary": "A stable domain-adversarial training approach for robust and comprehensive domain adaptation", "abstract": "  Domain adaptation tackles the problem of transferring knowledge from a label-rich source domain to an unlabeled or label-scarce target domain. Recently domain-adversarial training (DAT) has shown promising capacity to learn a domain-invariant feature space by reversing the gradient propagation of a domain classifier. However, DAT is still vulnerable in several aspects including (1) training instability due to the overwhelming discriminative ability of the domain classifier in adversarial training, (2) restrictive feature-level alignment, and (3) lack of interpretability or systematic explanation of the learned feature space. In this paper, we propose a novel Max-margin Domain-Adversarial Training (MDAT) by designing an Adversarial Reconstruction Network (ARN). The proposed MDAT stabilizes the gradient reversing in ARN by replacing the domain classifier with a reconstruction network, and in this manner ARN conducts both feature-level and pixel-level domain alignment without involving extra network structures. Furthermore, ARN demonstrates strong robustness to a wide range of hyper-parameters settings, greatly alleviating the task of model selection. Extensive empirical results validate that our approach outperforms other state-of-the-art domain alignment methods. Additionally, the reconstructed target samples are visualized to interpret the domain-invariant feature space which conforms with our intuition. ", "keywords": ["domain adaptation", "transfer learning", "adversarial training"]}, "meta": {"decision": "Reject", "comment": "This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.\n\nReviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.\n\nOverall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection."}, "review": {"HkxaMiKaFB": {"type": "review", "replyto": "BklEF3VFPB", "review": "This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.\n\nPros:\n- The writing is good\n- Satisfactory empirical results\n\nCons:\n- The proposed method is very similar to certain methods in the literature\n\nDetail comments:\n(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.\n\n(2) Experiments\n- How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.\n- What does the * in DRCN* mean in Table 1?\n- ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.\n\n(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.\n\nTypos:\n- In Eq.(1), there is a missing D in the first term. D_s should be \\mathcal{D}_s to match previous notation.\n- In Eq.(2), the \"0,\" is not meaningful given the definition of []^+.\n\nRefs\n- Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.\n- Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.\n- Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.\n\n# Update after rebuttal\n\nThank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are \"totally different\" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "SJxnv049ir": {"type": "rebuttal", "replyto": "HkxaMiKaFB", "comment": "Thanks for your constructive comments. We think some of the contents in the paper are misunderstanding, and we hope we can address them as below.\n\n(1) Is it similar to SiGAN?\nAnswer: No. The forms and the objectives of the loss in two papers are totally different. SiGAN applies a contrastive loss to the reconstruction of the two images, which aims to draw two similar images closer. In MDAT, we propose a margin loss for the reconstruction of the images from the source domain and the target domain. Our objective is to leverage the reconstruction network as a domain classifier that can transfer richer information (i.e. pixel-level domain alignment directly in adversarial training) and have better convergence (i.e. more effective gradients), compared with the classifier binary domain classifier, while SiGAN still leverage the classic binary domain discriminator in DCGAN. SiGAN only regards the contrastive loss as a regularizer during adversarial training, but ours is a new adversarial scheme based on reconstruction. Also, our loss function is more computational-efficient compared to SiGAN.\n\n(2) Experiments\n- How are the hyperparameters selected?\nAnswer: We obtain the hyperparameters by cross-validation on the small data SVHN$to$MNIST, and then apply the same hyperparameters for all the experiments. Note that one important novelty is that MDAT is not sensitive to the hyperparameter. As shown in Table 3, as long as the margin m>=1.0 & alpha<=0.1, the results are very robust. Moreover, compared to DANN, the MDAT can always converge with the effective gradients by the reconstruction network.\n\n- What does the * in DRCN* mean in Table 1?\nAnswer: Sorry, we actually miss some notes here. This means that we reproduce DRCN to fill some results as they do not provide the result for SYN->SVHN.\n\n- ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.\nAnswer: Yes. We conduct ARN w.o. MDAT to verify the efficacy of MDAT in ARN, as an ablation study. It is a good suggestion to just ignore the margin. However, in Table 3, we have shown that if the margin is small (m<0.5), the results are very similar to the standard DAT (DANN),which means that when a small margin or no margin is applied, MDAT has the similar mechanism as DANN. We further added the experiments when m=0 as the reviewer suggested, and supplement the results to Table 3, which further prove the effectiveness of the margin.\n\nm       0       0.1   0.3   0.5   0.7   1.0   2.0   5.0   10.0\nacc    64.3  64.5 75.2 90.0 92.6 96.0 97.4 97.7 96.7\n\n-  In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.\nAnswer: Yes. We agree with it. There is a little difference that we should point out here. Since we use the hinge loss for domain adaptation, we define the domain label (-1 and +1) in advance that directly confroms with the illustration of the main part (Eq.8). The intuition is the same as the common hinge loss that tell two categories apart from a margin. We will add the explanation after Eq.(2).\n\n\n-Typos.\nWe have revised the typos according to the comments.\n\nRefs\n- Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.\n- Ghifary, Muhammad, et al. \"Deep reconstruction-classification networks for unsupervised domain adaptation.\" European Conference on Computer Vision. Springer, Cham, 2016.", "title": "SiGAN uses recontruction term in their paper but it is totally different from ours."}, "r1xjXBH9jr": {"type": "rebuttal", "replyto": "HJx-7YCVtr", "comment": "Thanks for your valuable comments. We agree with your comments and add an experiment as well as more comparisons accordingly.\n\n- Compare their method with recent state-of-the-art methods (such as MCD, DIRT-T, etc).\nAnswer: We agree that we should compare our method with some recent state-of-the-art adversarial methods. We compare our method with GTA that employ a generative model to generate source-like features, CDAN that preserves more discriminability in DAT, and MCD that employs the classifier discrepancy. As shown below, we list the comparisons on 3 digit adaptation tasks, where our method outperforms them and MDAT is more computational-efficient. \n\nMethod      M-U      U-M       S-M\nGTA             97.8      98.2       91.7\nCDAN          93.9      96.9       88.5\nMCD            96.5      94.1       96.2\nMDAT          98.6      98.4       97.4 \n\nAs to DIRT-T, it consists of the adversarial training (VADA) and the boundary adjustment module. As the boundary adjustment is an extra technique, we compare MDAT with VADA for fair. The MDAT could be further improved by adding the techniques such as DIRT-T. Note that they employ a 9 Conv layer network while we only use 3 Conv layer as same as DANN and MCD. We compare MDAT with VADA based on these two models.\n\nMethod       M-U      U-M       S-M\nVADA-3        96.4      95.5       83.5\nVADA            98.5      97.3       97.9 \nMDAT           98.6      98.4       97.4\n\n- Is the method effective for object recognition datasets, such as Office or OfficeHome? \nAnswer: We totally agree with it to enrich the availability of our approach on more challenging task and applications. As Office-31 is very small and imbalanced, we add the experiment on the more challenging benchmark, i.e. Office-Home. We compare it with the novel domain alignment approach including DAN, DANN, JAN and CDAN. The proposed ARN with MDAT achieves the best average accuracy of 65.1%.\n\n------------------------------------------------------------------------------------------------------------------------------------------------\nMethod\tAr->Cl\tAr->Pr\tAr->Rw\tCl->Ar\tCl->Pr\tCl->Rw\tPr->Ar\tPr->Cl\tPr->Rw\tRw->Ar\tRw->Cl\tRw->Pr Avg\nResNet-50\t34.9\t50.0\t58.0\t37.4\t41.9\t46.2\t38.5\t31.2\t60.4\t53.9\t41.2\t59.9\t46.1\nDAN\t43.6\t57.0\t67.9\t45.8\t56.5\t60.4\t44.0\t43.6\t67.7\t63.1\t51.5\t74.3\t56.3\nDANN\t45.6\t59.3\t70.1\t47.0\t58.5\t60.9\t46.1\t43.7\t68.5\t63.2\t51.8\t76.8\t57.6\nJAN\t45.9\t61.2\t68.9\t50.4\t59.7\t61.0\t45.8\t43.4\t70.3\t63.9\t52.3\t76.8\t58.3\nCDAN\t49.0\t69.3\t74.5\t54.4\t66.0\t68.4\t55.6\t48.3\t75.9\t68.4\t55.4\t80.5\t63.8\nMDAT\t51.3\t69.7\t76.2\t59.5\t68.3\t70.0\t57.2\t48.9\t75.8\t69.1\t54.3\t80.6\t65.1\n------------------------------------------------------------------------------------------------------------------------------------------------\n\n- In addition, this method seems to have clear connection with \"Zhao, Junbo, Michael Mathieu, and Yann LeCun. \"Energy-based generative adversarial network.\"\nAnswer: We will supplement the references. Thanks.\n\nReference\n- Long, Mingsheng, et al. \"Conditional adversarial domain adaptation.\" Advances in Neural Information Processing Systems. 2018.\n- Saito, Kuniaki, et al. \"Maximum classifier discrepancy for unsupervised domain adaptation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n- Sankaranarayanan, Swami, et al. \"Generate to adapt: Aligning domains using generative adversarial networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n- Zhao, Junbo, Michael Mathieu, and Yann LeCun. \"Energy-based generative adversarial network.\" arXiv preprint arXiv:1609.03126 (2016).\n- Shu, Rui, et al. \"A dirt-t approach to unsupervised domain adaptation.\" arXiv preprint arXiv:1802.08735 (2018).", "title": "We added the experiments on large-scale dataset and compare our method to the representative methods."}, "HkxmDDVqjH": {"type": "rebuttal", "replyto": "HJxSQjc9cr", "comment": "Thanks for the comments. We realize that it is necessary to conduct the experiments on large-scale image datasets. As the reconstruction part aims to tackle the pixel-level domain shift, such shift usually exists in image or image-like dataset such as our WiFi recognition dataset.\n\n- The experimental part of this paper is weak.\nAnswer: We have added the experiment on Office-Home. The experimental results are shown below. In comparison, the proposed approach achieves 65.1% mean accuracy, outperforming other domain alignment approaches. MDAT shows significant improvement against the standard DAT (DANN) by 7.5%.\n------------------------------------------------------------------------------------------------------------------------------------------------\nMethod\tAr->Cl\tAr->Pr\tAr->Rw\tCl->Ar\tCl->Pr\tCl->Rw\tPr->Ar\tPr->Cl\tPr->Rw\tRw->Ar\tRw->Cl\tRw->Pr Avg\nResNet-50\t34.9\t50.0\t58.0\t37.4\t41.9\t46.2\t38.5\t31.2\t60.4\t53.9\t41.2\t59.9\t46.1\nDAN\t43.6\t57.0\t67.9\t45.8\t56.5\t60.4\t44.0\t43.6\t67.7\t63.1\t51.5\t74.3\t56.3\nDANN\t45.6\t59.3\t70.1\t47.0\t58.5\t60.9\t46.1\t43.7\t68.5\t63.2\t51.8\t76.8\t57.6\nJAN\t45.9\t61.2\t68.9\t50.4\t59.7\t61.0\t45.8\t43.4\t70.3\t63.9\t52.3\t76.8\t58.3\nCDAN\t49.0\t69.3\t74.5\t54.4\t66.0\t68.4\t55.6\t48.3\t75.9\t68.4\t55.4\t80.5\t63.8\nMDAT\t51.3\t69.7\t76.2\t59.5\t68.3\t70.0\t57.2\t48.9\t75.8\t69.1\t54.3\t80.6\t65.1\n------------------------------------------------------------------------------------------------------------------------------------------------\n\n- The organization and presentation of this paper should be polished.\nAnswer: We will polish the presentations carefully.\n\nReference\n- Venkateswara, Hemanth, et al. \"Deep hashing network for unsupervised domain adaptation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n- Long, Mingsheng, et al. \"Conditional adversarial domain adaptation.\" Advances in Neural Information Processing Systems. 2018.", "title": "We supplement the experiment on Office-Home."}, "HJx-7YCVtr": {"type": "review", "replyto": "BklEF3VFPB", "review": "This paper proposed a new method for unsupervised domain adaptation. Different from a conventional domain classifier based adaptation, they propose to utilize the loss of autoencoder to extract domain-invariant features. They trained reconstruction network to reconstruct source examples well whereas making reconstruction loss of the target examples large with some margin. Their goal is to stabilize the training of adversarial training for domain adaptation, incorporate pixel-level information, and give interpretable learned feature space. They performed experiments on digits datasets and WiFi Gesture Recognition datasets. Through experiments, they have shown that their method shows better performance than baseline methods and their method is not parameter-sensitive, is stable and provides interpretable adaptation results. \n\nI think their method is interesting and motivation is important. However, their experimental results are not convincing enough. \nFirst, they did not compare their method with recent state-of-the-art methods.  For example, there are classifier's discrepancy based adversarial learning method, Saito, Kuniaki, et al. \"Maximum classifier discrepancy for unsupervised domain adaptation.\". In addition, they did not compare with \"A dirt-t approach to unsupervised domain\nadaptation\", which they cited in the paper. I think their method is for stable and interpretable adversarial learning. So, it does not have to outperform other methods in accuracy. However, they need to show some superiority over these representative adversarial methods. \nSecond, their experiment is only on digits and WIFI datasets. Is the method effective for object recognition datasets, such as Office or OfficeHome? This is an important question to be addressed because the two datasets are benchmark domain adaptation dataset and the behavior on this dataset will show how this method is applicable to various datasets. I would say that the method does not have to outperform state-of-the art methods for these datasets, but they need to show how the method works on this dataset with respect to stability and interpretability. \n\nIn addition, this method seems to have clear connection with \"Zhao, Junbo, Michael Mathieu, and Yann LeCun. \"Energy-based generative adversarial network.\" arXiv preprint arXiv:1609.03126 (2016)\". They need to add this paper to a reference  and explain some connections. \n\nTo sum up, due to the two questions listed above, I think this paper is marginally below the acceptance threshold. Please respond to the questions.\n ", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 4}, "HJxSQjc9cr": {"type": "review", "replyto": "BklEF3VFPB", "review": "\n###Summary###\nThis paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. \n\nThe motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. \n\nIn the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. \n\nThe paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. \n\nThe paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. \n\n### Novelty ###\n\nThe model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  \n\n###Clarity###\n\nOverall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. \n\n###Pros###\n\n1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. \n2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. \n\n3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. \n\n###Cons###\n\n\n1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. \n\nIt will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:\nDomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/\nOffice-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/\n2) The organization and presentation of this paper should be polished.\n\nBased on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.\n\n", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 3}}}