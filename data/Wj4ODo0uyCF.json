{"paper": {"title": "Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation", "authors": ["Biao Zhang", "Ankur Bapna", "Rico Sennrich", "Orhan Firat"], "authorids": ["~Biao_Zhang2", "~Ankur_Bapna1", "~Rico_Sennrich1", "~Orhan_Firat1"], "summary": "We investigate and improve parameter-sharing strategies in multilingual Transformers by utilizing conditional computation.", "abstract": "Using a mix of shared and language-specific (LS) parameters has shown promise in multilingual neural machine  translation (MNMT), but the question of when and where LS capacity matters most is still under-studied. We offer such a study by proposing conditional language-specific routing (CLSR). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. By manipulating these gates, it can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. Moreover, CLSR can easily scale up to massively multilingual settings. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many-to-one translation, particularly with unbalanced training data. Our study further verifies the trade-off between the shared capacity and LS capacity for multilingual translation. We corroborate our analysis by confirming the soundness of our findings as foundation of our improved multilingual Transformers. Source code and models are available at https://github.com/googleinterns/cct-m4.", "keywords": ["language-specific modeling", "conditional computation", "multilingual translation", "multilingual transformer"]}, "meta": {"decision": "Accept (Oral)", "comment": "This paper proposes a conditional language-specific routing (CLSR)  mechanism for multilingual NMT, which also considers the trade-off between language specificity and generality.\n\nAll of the reviewers think this paper is interesting for both idea and empirical findings. Therefore, it is a clear acceptance."}, "review": {"XLn-GRLCRat": {"type": "rebuttal", "replyto": "UtH739qczBl", "comment": "Thanks for your constructive comments!\n\n(1) We updated our paper with a discussion of your mentioned studies in the related work.\n\n(2) Thanks for your suggestion! We toned down the performance improvement claim in the updated version.\n\n(3) Your understanding of the gate parameters is correct: these parameters are shared across language pairs. We agree that sharing these parameters might impose some inductive bias discouraging the language-specific behavior of CLSR. We added some additional discussion in the updated version to illustrate this point.\n", "title": "Response to Reviewer #2"}, "evoGdfTLIUX": {"type": "rebuttal", "replyto": "lYGH7w5rKp_", "comment": "Thanks for your insightful feedback. \n\nThe goal of our study is to improve the understanding of the trade-off between LS and shared capacity for multilingual translation, and specifically answer the question of when and where LS capacity matters for multilingual NMT. We hope our study could shed light on developing novel multilingual architectures for future research. \n\nPlease notice that the sparsely gated MOE model (Lepikhin et al., 2020) aims at scaling up Transformer, which has no language-specific components. The adapter (Bapna et al., 2019) is mainly used in a fine-tuning process on top of a pre-trained shared NMT model to largely benefit from the shared model parameters. It is separately fine-tuned for each language pair, and would significantly increase the training complexity in a massively multilingual setting.\n\nRegarding each of your questions:\n* Thanks for pointing out this missing related work (Wang et al., EMNLP 2018). We will extend our related work section to mention it soon. Using a fixed mix of language-specific and shared parameters (Wang et al 2018) is a valid strategy, but orthogonal to our research question to explore where language-specific computation is especially helpful.\n* Leaving the shared projection empty is also a potential choice, but that would introduce network structure differences between models using and not using the LS projections. Concretely, models routing through more LS paths will contain more linear sublayers than those skipped variants, and this difference might affect the training of the gating models and result in uncontrollable biases towards favoring or avoiding LS paths.\n", "title": "Response to Reviewer #1"}, "FTBVywGCvVv": {"type": "rebuttal", "replyto": "UUMF0tx7frV", "comment": "Thanks for your review and insights. Our response to each weakness you mentioned is below.\n\n(1) Thanks for pointing out these related studies in a broader context [1,2,3]. We didn\u2019t find the mentioned work (Ruder et al., AAAI 2018) and (Fan et al., ICLR 2019). Instead, we found (Ruder et al, AAAI 2019, [1]) and (Fan et al., ICLR 2020, [2]). Please correct us if we misunderstood.\nWe will include a discussion of these studies in our updated version soon based on your feedback.\n\n(2) The increase of model parameters doesn\u2019t explain CLSR\u2019s performance improvements. Please notice that, apart from LS$^\\diamond$, we also offer CLSR-L for a more fair comparison. CLSR-L extends LS$^\\diamond$ by applying the language-specific projection to each sub-layer of the Transformer. It utilizes roughly the same amount of parameters as CLSR* does. Our results show that CLSR-L yields inferior translation performance in almost all settings compared to CLSR*.\n\n(3) Making the hyperparameter p language-specific seems an interesting direction, although we don\u2019t have much prior knowledge on how to adequately distinguish it among different language pairs. It might be possible that using a smaller p_l for low-resource languages could improve knowledge transfer towards Low languages for M2O translation. \nBut based on our experiments, we would argue that the conclusion about linguistic correlation has a large chance to hold even if we adopt p_l rather than p. Under the budget constraint enforced by p as in Eq. (6), CLSR has enough freedom to schedule different amounts of LS capacity across different language pairs and different sublayers. However, our results show that the arrangement of LS capacity ends up being quite similar among different languages and sublayers, even though we vary p.\n\n[1] Ruder et al. Latent Multi-task Architecture Learning. In AAAI 2019\n\n[2] Fan et al. Reducing Transformer Depth on Demand with Structured Dropout. In ICLR 2020\n\n[3] Sukhbaatar et al. Adaptive Attention Span in Transformers. In ACL 2019\n", "title": "Response to Reviewer #2"}, "LJClxqO4GXl": {"type": "rebuttal", "replyto": "l_Gg-p4Mt2r", "comment": "Thanks for your insightful feedback.\n\n* About the clarification on \u201cthe amount of LS computation\u201d\n\n  Thanks for pointing this out. The term \u201cthe amount of LS computation\u201d refers to the proportion of open gates where CLSR selects to route information through the LS path instead of its shared counterpart, which is directly regularized and guided by the budget constraint $p$. We will make this much clearer in our updated version soon.\n\n* About the human judgements\n\n  We agree that a human evaluation would be interesting, but want to point out that we evaluate each system on 188 (OPUS-100) and 26 (WMT-14) translation directions, respectively. Repeating this evaluation with human judgments would be a daunting task. \n\n* About the source code and models\n\n  Yes, we will release our source code and pretrained models to ease further study.\n", "title": "Response to Reviewer #4"}, "Igws6zyqoNZ": {"type": "rebuttal", "replyto": "zvfRbxLUcGq", "comment": "Thanks for your insightful feedback and constructive suggestions.\n\nFor each of your concerns:\n* The consideration that the number of training languages might affect the scheduling behavior of CLSR totally makes sense. In fact, to make our claim convincing, we have already included one result with fewer training languages in our initial submission. Please notice that, apart from Figure 6 (Appendix B) on OPUS-100, we also took the same experiment on WMT-14 (14 languages involved) and showed the result in Figure 9 (Appendix C). The heatmaps in Figure 6 and Figure 9 reveal very similar patterns, both supporting that the LS capacity schedule in CLSR has little to do with linguistic characteristics. \n* Your observations on M2O translation are insightful. Our experiments show that LS capacity doesn\u2019t work well in some M2O low-resource settings. Since different language pairs share the same target language in the M2O setting, we argue that sharing parameters could largely encourage positive knowledge transfer and deliver better translation quality towards low-resource languages whose training data might be too scarce to well-train their LS components. \n1) Regarding M2O Low results in Table 1\n\n  Following your suggestion, we inspect the individual BLEU scores for M2O Low test languages. Below shows the result on the oversampled OPUS-100, where we list the results for the 8 languages to save space and report relative improvements against the baseline.\n|                | li   | my   | ig    | gd    | yi   | kn   | or   | tk   | WR on Low |\n|----------------|------|------|-------|-------|------|------|------|------|-----------|\n| LS$^\\diamond$  | -6.2 | -4.  | -8.   | -15.6 | -9.4 | -5.3 | -2.3 | -4.5 | 4.76      |\n| CLSR-L         | 0.7  | -4.  | -10.6 | -22.8 | -3.1 | -2.7 | -2.7 | -9.  | 14.29     |\n| Top-Bottom     | -1.1 | -1.3 | -2.2  | 2.4   | -7.7 | -4.7 | -0.7 | -2.1 | 14.29     |\n| Dedicated      | -0.5 | -0.1 | -4.4  | 2.8   | -0.1 | -1.4 | -0.6 | -3.5 | 23.81     |\n| CLSR*          | 4.9  | -1.7 | -1.6  | 2.2   | 1.2  | -2.3 | -0.9 | -0.8 | 23.81     |\n\n  In line with your hypothesis, we observe that LS models suffer from large performance drop on some languages like gd, ig and tk, particularly with LS$^\\diamond$ and CLSR-L. But we also notice that the win ratio on Low is small (<23.81%),  revealing the importance of sharing parameters for M2O Low.\n\n2) Regarding M2O WR results in Table 2\n\n  Compared to OPUS-100, changes in BLEU tend to be more similar across languages on WMT-14, meaning that the same average improvement has a higher effect on the WR. Even in absolute terms, there is a clear gap between Top-Bottom and Dedicated/CLSR*. We argue that this is caused by the suboptimal schedule of LS capacity in Top-Bottom, which is partially supported by the evidence in Figure 10(c,d) and Table 5 (Appendix C). This is consistent with our finding that both the amount and the position of LS capacity matters for multilingual translation.\n", "title": "Response to Reviewer #3"}, "lYGH7w5rKp_": {"type": "review", "replyto": "Wj4ODo0uyCF", "review": "In this paper, the authors present a study of different aspects of language-specific model capacity for massively multilingual machine translation. To this end, language-specific behaviour is achieved via a combination of conditional computation to decide whether to use language-specific parameters or not and statically assigning experts for each languages. The language specific sub-layers are incorporated throughout the network. The training objective allow budgetary constraints on the amount of language-specific parameters. The paper does a systematic analysis on the role of language specific parameters using the proposed architecture. Based on the analysis, recommendations on design of multilingual NMT architectures are proposed and their efficacy validated experimentally. The study sheds light on the amount of language specific parameter sharing, their distribution in the network, impact of language, etc. \n\nI find the analysis presented in the paper very interesting and insightful - and distinguishes it from previous work in this area. The hypothesis are clearly stated and the experiments are well designed. The findings from the analysis are an important addition to the understanding of the role of language specific parameters in multilingual NMT. \n\nIn terms of modelling, the work follows in the line of recent work on language-specific parameters for multilingual NMT. The deviation from existing work is mixing elements of conditional computation with language specific computation. I see this work more as an analysis on language-specific parameters for a particular LS-model rather than a novel architecture. It is not clear how this model would compare to other models using language specific parameters (sparsely gated mixture of experts (Lepikhin et al 2020), light-weight adapters (Bapna et al 2019)  ).\n\n\nQuestions: \n\n- Previous work has tried to combine both language-specific and shared parameters (Wang et al 2018), rather than making a binary choice between these. Did the authors compare with such an approach? \n- Since a major part of the model contains shared parameters, was there a need for new set of shared parameters along with the language-specific parameters. The gating decision could have been to bypass the language-specific sublayer or not.\n\nReferences\nYining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu, and Chengqing Zong. Three strategies to improve one-to-many multilingual translation. EMNLP. 2018.", "title": "A systematic analysis of language specific parameters in multilingual translation", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "l_Gg-p4Mt2r": {"type": "review", "replyto": "Wj4ODo0uyCF", "review": "The work proposes a hybrid architecture that has: (1) language-specific (LS) components; (2) as well as the components that are shared across all the languages -- a trade-off between specificity and generality.  A key conclusion of the work is that the best architectures typically are. the ones that have ~10-30% language-specific capacity. \n\nIn terms of experimental work, the work uses WMT-14 and OPUS-100 datasets to show the proposed trade-off. \n\nIn terms of exposition of the ideas, it's a well-written paper for the most part. \n\nOne issue that the authors could improve on is clarifying how \"the amount of LS computation\" is measured. You have mentioned it several times in the abstract/intro and it's neither clear nor referenced (it could be the number of parameters, it could be the number of basic computations, etc). For a new reader, it takes quite a while to find that $p$ is defined in eq. 6 and defined as a budget contains. \n\nOne other quibble is that all the trade-off figures are shown based BLEU/automatic metrics, which are known to be inaccurate. It would be nice to repeat one of the included evaluation with human judgments. \n\nOverall, I view this as a good contribution to pave the way towards stronger, but reasonably-sized multilingual models. This is partially assuming that the authors will stay true to their promise that \"Source code and models will be released.\"\n\n", "title": "Cross-language parameter-sharing for multi-lingual translation ", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "UUMF0tx7frV": {"type": "review", "replyto": "Wj4ODo0uyCF", "review": "Manual parameter sharing schemes are generally costly to come up with and when they are obtained for certain language pairs they do not necessarily generalize well to arbitrary language pairs in multilingual NMT. The idea of learning which parameters to share across languages in multilingual transformer models is original and potentially useful for designing and analyzing multilingual models in the context of NMT.  \n\n**Strengths**:\n\nThe paper is well-written and easy to follow. The idea was (reasonably) well-positioned with respect to prior work and clearly presented. \n\nThe technical merit is essentially in coming up with the budget constraint term in the loss function that forces the multilingual encoder-decoder \"super-network\" to use the desired percentage of language-specific computation using gating. \n\nA significant part of the contribution was in the analysis of the results, obtained by this learning-based parameter sharing approach, which was quite informative and revealed some interesting insights about where and when a language-specific computation is required. The takeaways should be of interest to researchers and practitioners interested in designing and analyzing multilingual NMT systems. \n\n**Weaknesses**: \n\n(1) Even though it is the first time such a method is applied in the context of NMT, the idea is not as much novel in the broader context of deep learning. Prior work has explored \"learning-to-share\"  strategies for parameter sharing in multi-task learning (see Ruder et al., AAAI 2018), and using gating/masking to control computational paths in a differentiable way (see Fan et al., ICLR 2019, Sukhbaatar et al., ACL 2019); it is clear that the focus is NMT but it should be worth mentioning/discussing such studies to better situate the work and to help the reader assess the actual contributions. \n\n(2) Another weakness is that the comparison with the vanilla and LS baselines does not seem to be properly controlled in terms of parameters. I appreciate that the authors do not read too much into it and focus more on the analysis of the results, but one thing that remains unanswered in this paper is how the proposed method fairs against multilingual baselines that utilize (roughly) the same number of parameters; currently, the best models outperform the LS baseline by ~28M and ~10M parameters on OPUS-100 and WMT-14 respectively. How important is this difference?\n\n(3) In the experiment about linguistic similarity, it appears that the capacity schedule is the same across languages and the authors conclude from this that the schedule has little to do with linguistic characteristics. However, the main driving force in the choice of the language-specific computation is currently a single hyper-parameter p which is the same across languages; so, this will lead to choices that are good on average for all language pairs involved for a given *universal* budget. Do you think the conclusion would be still the same if a language-specific hyper-parameter p_l was used instead? \n", "title": "Interesting contribution in the context of multilingual NMT", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "zvfRbxLUcGq": {"type": "review", "replyto": "Wj4ODo0uyCF", "review": "In this work, the authors present a conditional language-specific routing (CLSR) scheme for transformer-based multilingual NMT systems. They introduce a CLSR layer after every transformer encoder and decoder layer; each such layer is made up of hard gating functions conditioned on token representations that will either select a language-specific projection layer or a shared projection layer. Further, a budget is imposed on the language-specific capacity measured by aggregating the number of gates that allow for language-specific computations; this budget constraint forces the network to identify the sub-layers that will benefit most from being language-specific.\n\nThis is nice work. The proposed technique has been described clearly, the idea is intuitive and the experiments are pretty compelling. I have a couple of minor comments/suggestions for the authors.\n\n* The authors show heat-maps of LSScore distribution in Figure 6 (Appendix B) which suggest that the LS capacity schedule might have little to do with linguistic characteristics. However, this might have to do with the multilingual model being trained on as many as 94 different languages. It seems plausible that linguistic similarities might govern LS capacity scheduling when there are fewer training languages to learn from. To check for this, it might be interesting to redo this experiment with the medium resource and low resource buckets containing 26-28 languages each.\n\n* There are two (among many other) interesting things that stand out from the results in Tables 1 and 2. (1) From Table 1, the only setting where CLSR* (as well as \"Top-Bottom\" and \"Dedicated\") underperforms compared to the baseline is M2O for low-resource languages. It seems like the use of language-specific layers here has a strong adverse effect on performance (-4.56 with CLSR-L) which is largely offset by CLSR*. Some more insights based on the individual BLEU scores for each test language in the \"Low\" bin and whether there were certain languages that were largely responsible for the drop in performance would be interesting to the reader. (2) From M2O in Table 2, the win ratios of Top-Bottom are much lower when compared with Dedicated and CLSR* (61.54 vs. 84.62 vs. 84.62; 30.77 vs. 84.62 vs. 100).  Could the authors share their thoughts on why this drop might be appearing?", "title": "Nice work showing how to add language-specific modeling capacity to large multilingual NMT models in a principled manner", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}