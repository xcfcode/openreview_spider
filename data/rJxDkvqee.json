{"paper": {"title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "authors": ["Wanjia He", "Weiran Wang", "Karen Livescu"], "authorids": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"], "summary": "", "abstract": "Recent work has begun exploring neural acoustic word embeddings\u2013fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "The paper explores a model that performs joint embedding of acoustic sequences and character sequences. The reviewers agree the paper is well-written, the proposed loss function is interesting, and the experimental evaluation is sufficient. Having said that, there are also concerns that the proxy tasks used in the experiments are somewhat artificial."}, "review": {"BydLG08Il": {"type": "rebuttal", "replyto": "rJxDkvqee", "comment": "Thank you for your helpful comments and suggestions.  We have uploaded a revised version of the paper addressing some of the comments as well as fixing some typos etc.  Below are our replies to specific comments.\n\n*Regarding the data set and task (Reviewers 1, 4):  \nThe data set is indeed on the small size, and the assumption of known word boundaries is a strong one.  This paper focuses on improving the current state of research on learning acoustic embeddings, so we are comparing to the most relevant prior work, which largely uses this data set and the word discrimination task.  Now that we have achieved very high average precisions on this task, we believe that future work should indeed focus on (and standardize) larger data sets and tasks.\n\nWe would like to point out, however, some prior work suggesting that improvements on this data set/task can transfer to other data/tasks.  Specifically, Levin et al. took embeddings optimized on this data set/task (Levin et al., ASRU 2013) to improve a query-by-example system without known word boundaries (Levin et al., ICASSP 2015), by simply applying their embedding approach to non-word segments as well.  This is encouraging.  On the other hand, it would also be straightforward, and more principled, to extend our approach to directly train on both word and non-word segments.  We mention this in the revised paper.\n\n*Regarding experimenting with phone sequences rather than character sequences (Reviewer 1):  \nAlthough working with phone sequences requires a bit more supervision, we agree that this is an interesting and straightforward experiment and we are currently working on it (though it is not complete).  In the meantime, in our revised paper we have included the rank correlation between our embedding distances (trained with character supervision) and phonetic edit distances, which are not too different from the correlations with orthographic distance (Table 3).   This is nice since it suggests we might not be losing too much by using orthographic supervision vs. phonetic supervision.\n\n*Regarding homophones (Reviewer 1):  \nWe expect our approach to be unable to distinguish homophones, since they can only be distinguished in context.  Our data set does not include a sufficient number of homophones to confirm this, but future work should look at this problem in the context of more data and longer contexts.\n\n*Regarding ASR baselines (Reviewer 4):  \nThe revised paper includes an ASR-based baseline from prior work in Table 2, using DTW on phone posteriors, which is worse than ours despite training on vastly more data.  While it is an older result, it shows that it is not trivial to get our numbers.  We believe that this is because there is a benefit to embedding the entire sequence and training with a loss that explicitly optimizes a measure related to the discrimination task at hand.\n\n*Regarding additional analysis (Reviewer 4):  \nWe have added (in the appendix) a precision-recall curve and scatterplot of embedding vs. orthographic distances.\n\n*Regarding open-sourcing the code (Reviewer 3):\nWe agree.   We are in the process of updating our code and releasing it online.\n\nThank you also for pointing out the issue with Fig. 1 (it has been fixed in the revised paper).\n\n", "title": "Responses"}, "BJXA6j27l": {"type": "rebuttal", "replyto": "HysYkhymg", "comment": "How big are the training/dev/test sets in seconds?\n\n------Each of them is roughly 10K seconds (10K words, each 0.5-2 seconds long).\n\nHaving 60M test word pair samples, how many samples have the answer yes, how many no? (Prior probabilities of the 2 classes)\n\n------# of yes: 96929, # of no: 60661847.  As is typical in detection/retrieval problems, the vast majority of answers are \u201cno\u201d and the hard part is in finding the \u201cyes\u201ds. \n\nSaying always \"no\" (different words) seems to be a good strategy.\n\n------Saying always \u201cno\u201d corresponds to a single point on the precision-recall curve, where recall equals 0 and precision is undefined (when calculating AP, we ignore this point).  As is typical in detection/retrieval problems, we do not consider a single classifier but rather the entire precision-recall curve, and report the average precision as a measure of performance.\n\nWhy have the authors not considered proper ASR techniques as baselines:\n a, Cross-view word discrimination: Measuring score difference between a background HMM model and a word model, a la keyword spotting.\n\n b, Acoustic word discrimination: Training phone/grapheme based HMM neural network acoustic model, phone posteriors could be extracted.\n    Using DTW in posterior feature space with KL-divergence would definitelly result in a better DTW baseline, aka. posterior based features in template matching.\n\n------For technique (a), one would need to know in advance the set of query words and have a reasonable amount of training data for each.  This is different from the setting we are considering, which is an open-vocabulary task with no prior knowledge of the query words.  We have >3000 unique query words, much larger than in any typical keyword spotting task (but we are open to suggestions -- if there is a reference you can recommend that does address this setting with a more typical keyword spotting-type model, please let us know).\n\n------For technique (b), this has been done by others and the best reported AP on this test set (from Carlin et al. 2011) is 0.497.  This is much worse than our results and also uses a large amount of external training data.  We will add this point to the paper.", "title": "Reply"}, "Hkp5Tshme": {"type": "rebuttal", "replyto": "rkgOxB0Mx", "comment": "have you tried using all combination of losses and doing ablation for each one of them? that would be informative.\n\n------We did not try all possible combinations and ablations, only the most promising ones based on the single-loss results.  Yes, we can consider additional combinations, although we expect that it will not add a very significant value to the paper beyond the combinations we have already included.\n\nhave you tried pretraining characterLSTM on a large text corpus to initialize your model? I think your dataset is too small to learn a good character model.\n\n------That is a good idea.  For the time being we wanted to stick to using the closed data set that others have used, for a fair comparison, but for future work we can expand to larger data sets, for pretraining or otherwise.", "title": "Reply"}, "HysYkhymg": {"type": "review", "replyto": "rJxDkvqee", "review": "How big are the training/dev/test sets in seconds?\nHaving 60M test word pair samples, how many samples have the answer yes, how many no? (Prior probabilities of the 2 classes)\nSaying always \"no\" (different words) seems to be a good strategy.\n\nWhy have the authors not considered proper ASR techniques as baselines:\n  a, Cross-view word discrimination: Measuring score difference between a background HMM model and a word model, a la keyword spotting.\n  b, Acoustic word discrimination: Training phone/grapheme based HMM neural network acoustic model, phone posteriors could be extracted.\n     Using DTW in posterior feature space with KL-divergence would definitelly result in a better DTW baseline, aka. posterior based features in template matching.\nPros:\n  Interesting training criterion.\nCons:\n  Missing proper ASR technique based baselines.\n\nComments:\n  The dataset is quite small.\n  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.\n  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection\n  performance of out-of-vocabulary words.\n  It would be interesting to show scatter plots for embedding vs. orthographic distances.\n", "title": "questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJmOdpVEl": {"type": "review", "replyto": "rJxDkvqee", "review": "How big are the training/dev/test sets in seconds?\nHaving 60M test word pair samples, how many samples have the answer yes, how many no? (Prior probabilities of the 2 classes)\nSaying always \"no\" (different words) seems to be a good strategy.\n\nWhy have the authors not considered proper ASR techniques as baselines:\n  a, Cross-view word discrimination: Measuring score difference between a background HMM model and a word model, a la keyword spotting.\n  b, Acoustic word discrimination: Training phone/grapheme based HMM neural network acoustic model, phone posteriors could be extracted.\n     Using DTW in posterior feature space with KL-divergence would definitelly result in a better DTW baseline, aka. posterior based features in template matching.\nPros:\n  Interesting training criterion.\nCons:\n  Missing proper ASR technique based baselines.\n\nComments:\n  The dataset is quite small.\n  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.\n  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection\n  performance of out-of-vocabulary words.\n  It would be interesting to show scatter plots for embedding vs. orthographic distances.\n", "title": "questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkgOxB0Mx": {"type": "review", "replyto": "rJxDkvqee", "review": "- have you tried using all combination of losses and doing ablation for each one of them? that would be informative.\n- have you tried pretraining characterLSTM on a large text corpus to initialize your model? I think your dataset is too small to learn a good character model.\n\nMINOR issues:\n- in figure-1 half of $f_2(x)$ is missing.this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.\n\nalthough I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:\n- investigating the use of fairly known architecture on a new domain.\n- providing novel objectives specific to the domain\n- setting up new benchmarks designed for evaluating multi-view models\n\nI hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.", "title": "combination of losses & pretraining", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HydYVVQVx": {"type": "review", "replyto": "rJxDkvqee", "review": "- have you tried using all combination of losses and doing ablation for each one of them? that would be informative.\n- have you tried pretraining characterLSTM on a large text corpus to initialize your model? I think your dataset is too small to learn a good character model.\n\nMINOR issues:\n- in figure-1 half of $f_2(x)$ is missing.this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.\n\nalthough I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:\n- investigating the use of fairly known architecture on a new domain.\n- providing novel objectives specific to the domain\n- setting up new benchmarks designed for evaluating multi-view models\n\nI hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.", "title": "combination of losses & pretraining", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}