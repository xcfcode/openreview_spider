{"paper": {"title": "Generative Paragraph Vector", "authors": ["Ruqing Zhang", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng"], "authorids": ["zhangruqing@software.ict.ac.cn", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn"], "summary": "With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts.", "abstract": "The recently introduced Paragraph Vector is an efficient method for learning high-quality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.\n", "keywords": ["Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning"]}, "meta": {"decision": "Reject", "comment": "The contribution of this paper generally boils down to adding a prior to the latent representations of the paragraph in the Paragraph Vector model. An especially problematic point about this paper is the claim that the original paper considered only the transductive setting (i.e. it could not induce representations of new documents). It is not accurate, they also used gradient descent at test time. Though I agree that regularizing the original model is a reasonable thing to do, I share the reviewers' feeling that the contribution is minimal. There are also some serious issues with presentation (as noted by the reviewers), I am surprised that the authors have not addressed them during the review period."}, "review": {"Sy7EP_AMx": {"type": "rebuttal", "replyto": "SylrvBAGg", "comment": "Dear Reviewer,\n    Thanks for reviewing our paper.\n    1. Paragraph vector is incomplete in that it provides no model at the text level, just the same as PLSA. So when you want to obtain the PV for a new text for some prediction task, the only way you can do is to use fold-in method to learn the vector representation for the text. That is, you take that text together with original training dataset to learn the paragraph vector. However, if there is a generative process for the vector at the text level, we can use learned word vectors to directly infer the PV for the new test text. This is much more efficient in practice and this is the exactly the motivation of our work.\n    2. Same with the learning framework in PV, we apply the negative sampling idea to accelerate the learning. And, we use MAP estimates to approximate our learning problem. Because the MAP estimation problems for different documents are independent, we can solve them on multiple machines in parallel. The number of \u201cnegative\u201d samples, the vocabulary size of your dataset, and the parallel threads decide the training time. The training of GPV is almost of the same efficiency as PV model.\n    Hope this clarifies your questions.\n    ", "title": "Thank you for your comment"}, "SylrvBAGg": {"type": "review", "replyto": "ryT9R3Yxe", "review": "However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set.\n--> This makes no sense to me. That model clearly still works and computes a presentation for test texts.\n\nHow fast is your model?While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following:\n\n1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data\n2) Numerous basic formatting and Bibtex citation issues.\n\nLack of novelty of yet another standard directed LDA-like bag of words/bigram model.\n", "title": "intro", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r19g6_sEl": {"type": "review", "replyto": "ryT9R3Yxe", "review": "However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set.\n--> This makes no sense to me. That model clearly still works and computes a presentation for test texts.\n\nHow fast is your model?While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following:\n\n1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data\n2) Numerous basic formatting and Bibtex citation issues.\n\nLack of novelty of yet another standard directed LDA-like bag of words/bigram model.\n", "title": "intro", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}