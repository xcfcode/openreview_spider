{"paper": {"title": "Semi-supervised semantic segmentation needs strong, high-dimensional perturbations", "authors": ["Geoff French", "Timo Aila", "Samuli Laine", "Michal Mackiewicz", "Graham Finlayson"], "authorids": ["g.french@uea.ac.uk", "taila@nvidia.com", "slaine@nvidia.com", "m.mackiewicz@uea.ac.uk", "g.finlayson@uea.ac.uk"], "summary": "Why semi-supervised semantic segmentation is a challenging problem (no cluster assumption) and how to get consistency regularisation to work", "abstract": "Consistency regularization describes a class of approaches that have yielded ground breaking results in semi-supervised classification problems. Prior work has established the cluster assumption\\,---\\,under which the data distribution consists of uniform class clusters of samples separated by low density regions\\,---\\,as key to its success. We analyze the problem of semantic segmentation and find that the data distribution does not exhibit low density regions separating classes and offer this as an explanation for why semi-supervised segmentation is a challenging problem. \nWe then identify the conditions that allow consistency regularization to work even without such low-density regions. \nThis allows us to generalize the recently proposed CutMix augmentation technique to a powerful masked variant, CowMix, \nleading to a successful application of consistency regularization in the semi-supervised semantic segmentation setting and\nreaching state-of-the-art results in several standard datasets.", "keywords": ["computer vision", "semantic segmentation", "semi-supervised", "consistency regularisation"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a method for semi-supervised semantic segmentation through consistency (with respect to various perturbations) regularization. While the reviewers believe that this paper contains interesting ideas and that it has been substantially improved from its original form, it is not yet ready for acceptance to ICLR-2020. With a little bit of polish, this paper is likely to be accepted at another venue."}, "review": {"HyxVtjH3sB": {"type": "rebuttal", "replyto": "H1lvGISPoB", "comment": "We have added a little more to Section 3.2 paragraph 4, in that we have stated that the perturbations should be high dimensional in order to adequately constrain a decision boundary in the high-dimensional space of natural images.", "title": "Response to Official Blind Review #2 (2)"}, "HyxCccH3iB": {"type": "rebuttal", "replyto": "HkxEhHrDoS", "comment": "We have added results for ICT, CutOut, CutMix and CowOut using DeepLab2 for Cityscapes with 372 supervises samples. We will run experiments to produce results for other values for number of supervised samples, U-Net architecture and the Pascal dataset in due course.\n\nThe new results show that CutOut harms performance, CutMix contributes an improvement (but only just) while CowOut makes a fair improvement, but trailing that of CowMix, strengthening the position of CowOut and CowMix relative to the others.\n", "title": "Response to Official Blind Review #3 (2)"}, "rJg6t8rPsr": {"type": "rebuttal", "replyto": "SJlKzMliKB", "comment": "Thank you for your review. We appreciate your effort and we thank you for highlight areas in need of clarification.\nIf there are any others that come to mind, please feel free to let us know.\n\nWe now state explicitly that we do use supervised loss in section 4.1 paragraph 2. In order to stay within paper\nlength limitations, we removed some details from this paragraph as they are also present in Appendix D.3. We\nwill look further at Figure 3 before the end of the response/discussion period.\n\nWe used different split ratios to that of Hung et al. as the practical benefit of semi-supevised learning is maximised by\nreducing the required number of labelled samples -- and therefore the effort required to label them -- by\nas much as possible. We therefore tested our approach using a significantly smaller number of labelled samples\nin order to illustrate that our approach gives strong performance in these challenging but practically useful\nconditions. Labelling 1,323 images (12.5% of augmented Pascal) or 5,291 images (50%) requires a considerable\namount of manual labour.\n\nWe have attempted to answer your query concerning applying geometric transformations in reverse in Appendix D.1.1.\nIn short, classification is translation invariant while semantic segmentation is translation *variant*. As a consequence\nin semantic segmentation scenarios any translation must in effect be reversed elsewhere in the pipeline to prevent\nthe network from learning from erroneous training data. That said, using translation can cause a part of the image\nto take a slightly different path through the convolutional layers of the network, so it can provide a small\nimprovement. Our standard augmentation based unsupervised regularizer can and does utilise this, although it does not\nachieve gains in semi-supervised segmentation.\n\nFigure 2: we have replaced with word gap with the term 'low density region' that is consistent with the rest of the\npaper. We constructed two similar artificial scenarios for Fig 2 (a) and (b); one with a low density region\nseparating the two regions of unsupervised samples and one without.\n\nWe have expanded the text in Appendix A to explain how Figure 2(c) was made. We hope this clears things up.\nIf not, please feel free to let us know.\n", "title": "Response to Official Blind Review #1 "}, "H1lvGISPoB": {"type": "rebuttal", "replyto": "H1eLF2upFH", "comment": "Thank you for your review and thank you for identifying areas that are of concern.\n\nI will attempt to answer your query concerning how our analysis of a 2D example carries over to a high dimensional\nproblem.\n\nTo recap, Figure 2(d) shows that consistency regularization can succeed without requiring low density regions in the input\ndistribution by constraining the perturbations that drive consistency regularisation to be parallel to the\nintended decision boundary. In such a simple 2D example, perturbing along a line parallel to the decision boundary\n(or rather parallel to a line tangent to the decision boundary at the closest point on said boundary)\nis sufficient. In higher dimensions, perturbing a sample in one direction (making it trace out a line) is insufficient as \nthe decision boundary is free to orient itself almost arbitrarily while still being perpendicular to the line of perturbation.\nIn order to properly constrain the orientation of the decision boundary, the perturbations must operate in as many\ndimensions as possible. The cutting and mixing regularizers (CutOut, CutMix, CowOut and CowMix) discussed in this paper\nare high dimensional as an axis of perturbation is supplied by each pixel in the cutting/mixing mask.\n\nFurthermore, masking out part of an object (as in CutOut or CowOut) does not change the ground truth class of the pixels\nof the object that remain, hence these perturbations do not cross the class boundary. Mixing the images does not change\nthe class for a similar reason.\n\nIt is our intention look carefully at the wording of this part of the paper in the next few days.\n", "title": "Response to Official Blind Review #2"}, "HkxEhHrDoS": {"type": "rebuttal", "replyto": "BklLvzGCFS", "comment": "Thank you for your very helpful and detailed review. You have identified several areas in which we can improve and clarify our work.\n\nWe have updated the caption in Figure 1 and the text in Section 3.1 to clarify that we are comparing the contents\nof overlapping patches that are centred on neighbouring pixels; a patch A is compared with a neighbouring patch B\nthat is shifted e.g. one pixel to the right of patch A. Figure 1 (b) and (c) are generated by computing the distance between patches that are centered on each pixel in the image.\n\nWe have expanded Section 3.1 to confirm that we are compare the raw pixel content of image patches. While the use\nof raw pixel space -- as opposed to e.g. a feature space from a pre-trained network -- may seem unusual given the\nhighly non-linear nature of neural networks, prior consistency regularisation based semi-supervised learning approaches\n(e.g. Laine et al. etc.) apply sample perturbation in the input space. We specifically reference the virtual\nadversarial approach of Miyato et al. as they use adversarial techniques to generate an adversarial example\n$\\hat{x}$ from $x$ that maxmimizes the distance between predictions $d(\\hat{y}, y)$ (where y = f_\\theta(x)).\nOnce again this perturbation is performed in raw pixel / input space, illustrating the significance of the input space\ndata distribution.\n\nAs far as we know the data distribution of semantic segmentation problems has not been studied in prior work.\nTo recap, we observed that computing the distance between overlapping neighbouring patches is equivalent to applying\na uniform filter to the squared gradient image, thus suppressing the fine details that would be required for\nlow density regions to manifest along object or texture boundaries. We believe that this should apply to natural\nimages in general. We have however analysed the patch distribution in the Cityscapes dataset to confirm this.\nDue to the space limitations we have added this as Appendix B.\n\nWe are currently in the process of conducting experiments on the Cityscapes dataset using CutOut, CutMix\nand CowOut using the DeepLab2 network. We hope to get results for ICT as well. We will revise the paper\nlater during the rebuttal period to include them when the experiments have finished running. We will only be able\nto do this for 372 supervised samples prior to the end of the rebuttal period due to lack of available\ncompute at this time and the short amount of time available. We will endeavour to produce a complete\nset of results should our work be accepted.\n\nApplying CowOut and CowMix to classification problems is a topic that we intend to explore in further work.\nWe believe that the challenging nature of semi-supervised semantic segmentation that we have described\nin this work has resulted in a regularizer that could have interesting properties when used in classification\nand are eager to evaluate it.\n", "title": "Response to Official Blind Review #3 "}, "SJlKzMliKB": {"type": "review", "replyto": "B1eBoJStwr", "review": "This work analyzes the consistency regularization in semi-supervised semantic segmentation. Based on the results on a toy dataset, this work proposes a novel regularization for semi-supervised semantic segmentation, which is named CowMix.\n\nPros:\n-- The proposed CoxMix is easy to understand and implement.\n-- The experimental results seem to benefit from this proposed CoxMix at a first glance.\n\nCons:\nThe writing is not clear. Sometimes I have to make a ``guess\" about the technical details. For example:\n-- Other than L_{cons}, is there any other Loss term utilized in this work? Based on Figure 3, it seems only L_{cons} is utilized. If so, is it a waste not to use the label training data (although very few) to calculate a cross-entropy loss?\n\n-- It seems the experimental setting in this submission follows the settings in Hung, 2018. However, for the experiment on VOC 2012 validation set, Hung tested their method on 1/8 1/4 1/2 of labeled data (Table 1). While in this submission, Table 3 shows the results on label data of 100, 200, 400, 800, 2646(25%). The split ratios seem different from Hung's work, which confuses me.\n\n-- \"Note that in context of semantic segmentation, all geometric transformations need to be applied in reverse for the result image before computing the loss (Ji et al., 2018). As such, translation turns into a no-op, unlike in classification tasks where it remains a useful perturbation.\" \n    Is there any experimental result to support this claim?\n\n-- It is a little hard for me to fully understand Figure 2. For example, how to get 2. (c)? What is the meaning of the word \"gap\" here?\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 2}, "H1eLF2upFH": {"type": "review", "replyto": "B1eBoJStwr", "review": "This paper provided first provided analysis for the problem of semantic segmentation. Through a few simple example, the authors suggested that the cluster assumption doesn\u2019t hold for semantic segmentation. The paper also illustrated how to perturb the training examples so that consistency regularization still works for semantic segmentation. \nThe paper also introduce a perturbation method that can achieve high dimensional perturbation, which achieve solid experimental results.\n\nThe analysis part seems interesting and innovative to me. But it is very qualitative and I'm not fully convinced that the analysis on 2d example can actually carry over to high dimensional spaces for images. I also don't quite see the connection between the toy example and the proposed perturbation method. For example, why the proposed perturbation method has the property of \"the probability of a perturbation crossing the true class boundary must be very small compared to the amount of exploration in other dimensions\"?\n\nThe proposed algorithm is an extension of the existing cutout and cut mix. The way to generate new mask is a very smart design to me. This should be the most important contribution of the paper.\n\nThe writing of the paper is very clear and easy to follow. The experimental results look very convincing overall and proposed algorithm does show very promising results. \n\nTo sum up, the paper is an ok paper from the practical perspective, but the analysis in the paper wasn't strong enough to me.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 1}, "BklLvzGCFS": {"type": "review", "replyto": "B1eBoJStwr", "review": "\n\n# Summary\n\nThis paper proposes a method for semi-supervised semantic segmentation. \nThe authors tackle this problem through consistency regularization, a successful technique in image classification, that encourages the network to give consistent predictions on unlabeled samples which are perturbed in multiple ways. The authors argue that the cluster assumption (to which effectiveness of consistency regularization has been partially attributed) does not hold in semantic segmentation. Thus, in order to enable class boundaries to become low-density regions and then better guide contrastive regularization, the authors argue that a stronger perturbation must be inserted. To this effect they first propose looking at CutOut and CutMix types of methods. They improved upon them by putting forward a variant of CutMix, coined CowMix, with more degrees of freedom and using flexible masks instead of rectangular ones. CowMix is evaluated on the Cityscapes and PascalVOC 2012 datasets in the semi-supervised regime and showing encouraging results.\n\n\n# Rating\nI find the paper and the advanced ideas of interest for the community and I consider they are novel. I'm currently on the fence between Weak Accept and Weak Reject, mostly due to incomplete evaluations and support for claims made in the introduction regarding the infeasibility of contrastive regularization methods for semantic segmentation. I would be happy to upgrade my rating if authors addressed these concerns.\n\n\n# Strong points\n- The paper is well written and mostly clear with a good coverage and positioning w.r.t. related work. The authors illustrate well the reasoning and the choices they have made. The author provide plenty of ablation studies (e.g., per class statistics) and implementation details, improving significantly the reproducibility of the contribution.\n- The flexible masking technique that is advanced here is novel and experimentally seems effective.\n- This work is among the few that address semi-supervised semantic segmentation in a non-adversarial manner, so I would give it some novelty credit.\n- I appreciate the evaluation protocol of averaging across multiple runs.\n\n# Weak points\n\n## Unclear aspects\n- The authors argue that consistency regularization has had little success so far in semantic segmentation problems since low density regions in input data do not align well with class boundaries. It would be useful to provide a reference to this claim or at least validate it experimentally on a large dataset.\n\n- In Figure 1, it is not clear on which features where the distances between patches computed? Is it on raw pixels or intermediate feature maps from a CNN? \nIf the distances are made over raw pixels, I find it difficult to make the connection between distances in the pixel space and distances in the class space.\nAre the neighbor patches overlapping with the central/query patch?\n\n## Experiments\n- The authors compare against other methods on the CamVid dataset. CamVid is a small and relatively limited dataset (~367 images for training from the streets of Cambridge). I'm worried that this dataset might not be enough to conclude and emphasize the benefits of this method over other semi-supervised techniques. For instance CowOut does not seem do be above CutOut, while CutMix has convergence problems and low scores. \nThe other experiments on Cityscapes and Pascal VOC are certainly interesting, but the method is compared only against Hung et al. which a different family of methods and the subset baseline (which is useful but not enough). I think this work would benefit from an additional baseline in the style of contrastive regularization methods, e.g. ICT, and eventually CutOut, to support the initial arguments regarding the limitations of these methods in semantic segmentation and respectively the effectiveness of the flexible masks over the rectangular ones in this setup.\n\n\n# Suggestions for improving the paper:\n1) It would be useful to include other semi-supervised baselines, e.g. ICT, and the baseline perturbation CutMix on larger experiments, in order to better emphasize the contributions of this work.\n\n2) Did the authors try the flexible masking on image classification? How is it expected to perform over ICT, MixUp or MixMatch?\n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}}}