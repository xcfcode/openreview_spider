{"paper": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "summary": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "keywords": ["Natural Language Processing", "BERT", "Representation Learning"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "This paper proposes three modifications of BERT type models two of which is concerned with parameter sharing and one with a new auxiliary loss. New SOTA on downstream tasks are demonstrated. \n\nAll reviewers liked the paper and so did a lot of comments. \n\nAcceptance is recommended."}, "review": {"HkxZGWJisB": {"type": "rebuttal", "replyto": "H1eA7AEtvS", "comment": "We want to thank the reviewers again for their suggestions! We have updated the paper with the following changes: \n  - Addressing the typo pointed out by Reviewer 2.\n  - Addressing Reviewer 3\u2019s concern on the overgeneralization problem of this sentence \u201cdropout can hurt performance in large Transformer-based models\u201d\n- Addressing Reviewer 3\u2019s suggestion of comparing embedding factorization with other related methods in the literature.  \n- Addressing Reviewer 3\u2019s suggestion clarifying the additional data usage.  \n- Addressing all other public comments. \n\nWe also move the section of comparing BERT-xxlarge to BERT-large with the amount of training time to appendix so that we can fit the 10 page limit.  \n", "title": "Paper updates "}, "B1x9O5W-jr": {"type": "rebuttal", "replyto": "rJlMUMIoFH", "comment": "Dear Reviewer #3, \n\nThank you so much for going through the paper carefully and providing positive and useful feedback to our work! \nPlease see our responses below:\n\nWe do mention that \u201cThe parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.\u201d However, we did not want to interpret along this dimension too much, as we did not have rigorous proof that regularization can solve the model degradation problem. One of the major difficulties in doing experiments with the BERT-xlarge setup is that it needs to train on 1024 TPUs v3 units, which is an expensive resource.\n\nFor the overgeneralization problem in the sentence \u201cdropout can hurt performance in large Transformer-based models\u201d, we will make it clear by adding this sentence: \u201cHowever, the underlying network structure of ALBERT is a special case of the Transformer, and further experimentation is needed to see if this phenomenon appears with other Transformer-based variants.\u201d \n\nWe agree that a better discussion of related work would help people to better understand our work.  Currently, we have a public comment from Sachin Mehta asking to compare related works such as \u2018Adaptive input representations for neural language modeling.\u2019, \u2018transformer-xl\u2019, and \u2018Efficient softmax approximation for GPUs\u2019. Please let us know if you have any specific additional work that you want us to include in the comparison of our embedding factorization methods, and we will incorporate it in the final version of the paper. \n \nThe reason we did not report the inference time (latency) is because it is a platform-specific metric. We would need different strategies to optimize for TPUs and CPUs. However, we do have some TPU-based metrics for BERT-base and ALBERT-base. By looking at these numbers, we see that ALBERT-base is about 3x faster than BERT-base at inference time. Other people have also converted a Chinese version of ALBERT-tiny (we would like to thank brightmart for implementing this project and he got an amazing number (1.4k) of stars in such a short amount of time!) into tf-lite format and measured the inference time on mobile devices. Here is the quote from his website (https://github.com/brightmart/albert_zh): \u201cOn an Android phone w/ Qualcomm's SD845 SoC, via the above benchmark tool, as of 2019/11/01, the inference latency is ~120ms w/ this converted TFLite model using 4 threads on CPU, and the memory usage is ~60MB for the model during inference. Note the performance will improve further with future TFLite implementation optimizations.\u201d\n\nThat being said, we would say ALBERT could have a huge impact on inference speed because inference speed is usually memory-bandwidth bound and memory bandwidth is limited by memory capacity. For example, if you can keep the model weights used in matmuls in a smaller high-bandwidth memory, you can go 10x - 100x faster than if you need to read it out of a larger lower-bandwidth memory.\n\nIn terms of model size in MB, they are roughly 4x as large as the parameter size as the weights are mostly in float format. \n\nWe use all the XLNet data (126G) as well as the stories data (31G) of raw data. Our 1M step training went though the same number of iterations over the data as Roberta 500K, as they use 2x as large a batch size as ours.\n\nThanks for bringing up these points, they are certainly valid and relevant. We will incorporate more info along the discussion above in the next version of our paper. \n", "title": "Thank you so much for going through the paper carefully and providing positive and useful feedback to our work"}, "B1gKTRWbjH": {"type": "rebuttal", "replyto": "BkxJzVhyoB", "comment": "Hi Hi Renjie! \n\nThanks for commenting on our paper! I enjoy reading your paper and hoped that I could come up with such an elegant solution!  \n\nWould address the name error problem and acknowledge that DEQ (you) also found that simple weight-sharing would result in oscillation. \n", "title": "Thanks for helping us to explain the oscillating phenomenon"}, "ByxuKYW-jB": {"type": "rebuttal", "replyto": "SygC0QIhFB", "comment": "Dear Reviewer # 1\n\nThank you so much for going through the paper carefully and providing such a positive feedback about our work! Please see our answers below:\n\nFor hyper-parameter tuning, we only explore those hyper-parameters that are related to model size. This is done for the following two reasons: 1) To keep the comparison as meaningful as possible (so we fixed all other parameters as in BERT, and always used LAMB optimizer) 2) To keep under control the number of experiments we need to run; we already have a lot of experiments to report on; if we were to tune other hyper-parameters like learning rate and optimizer, the number of experiments needed can easily run out of control. \nFor optimizer, we choose LAMB because it allows us to use large batch sizes. We haven\u2019t tested other optimizers yet. \nBecause large models that can cause degenerate solutions are extremely expensive to run, we did not explore that area very much. For example, in order to run with a batch size of 4096, BERT-xlarge requires 1024 TPUs v3 units, which is an expensive resource. However, we are working on this and hopefully can give a reasonable explanation/solution to this problem soon.  ", "title": "Thank you so much for going through the paper carefully and providing such a positive feedback about our work"}, "Bkx3GYb-iB": {"type": "rebuttal", "replyto": "Hkl2NgFecr", "comment": "Dear Reviewer #2, \n\nThank you so much for going through the paper carefully and providing such a positive feedback. Please see below our response to your comments:\n\nAbout all-sharing vs non-shared: yes, non-shared gives better results, but the number of parameters is increased dramatically. We tried other strategies of sharing the parameters across layers. For example, we divided the L layers into N groups of size M (L=N*M), and each size-M group shares parameters. Overall, our experimental results show that the smaller the group size M is, the better the performance we get. However, decreasing group size M also dramatically increase the number of overall parameters. We chose the all-shared strategy to maximize our parameter reduction. \nWe are glad that you like our SOP objective. We did try other changes to the objectives, such as multiword masking, but papers proposing these ideas were posted before our work, so for simplicity we adopted and cited the previous works. \nThank you so much for helping us to correct the typo, we will fix them in our next version. \n", "title": "Thank you so much for going through the paper carefully and providing such a positive feedback"}, "rJlMUMIoFH": {"type": "review", "replyto": "H1eA7AEtvS", "review": "The authors present ALBERT, a modification of the BERT architecture with substantially fewer parameters. They show that despite being much smaller, the performance is very strong and achieves state of the art on a variety of different tasks. There are several ideas proposed here: embedding factorization, sharing layers, and sentence ordering as a training objective. \n\n1. The point that naively increasing the size of the BERT architecture does not work is a good one, but the authors don't acknowledge that this is tied up in the effect of regularization. Cross layer parameter sharing has a regularization effect that simply scaling up BERT large to x-large or such sizes does not have. This is also an issue with the authors making the statement that they are the first to show that dropout is harmful for Transformers. This is a large generalization that seems to be a special case of not only the regularized architecture they propose but also the large quantity of data that the model still underfits to.\n\n2. The authors propose embedding factorization to reduce the number of parameters in the embedding dimension. This is very intuitive, but the authors do not cite or compare to related approaches. I understand these models are computationally intensive and thus do not expect large quantities of detailed ablations. However, this kind of dimensionality reduction has been explored with other techniques, for example for knowledge distillation, quantization, or even adaptive input/softmax (and with subword as well, not just whole word modeling). These techniques have also been applied to machine translation models, which do not use them to learn rare words. I believe a better discussion of these methods should be added to the paper, as this is not a novel proposition.\n\n3. A large takeaway I have from this paper is that parameter size is not a good metric. While ALBERT is substantially smaller, the authors do not make it clear that this model is very slow at inference time due to the large size. This raises several questions: is it better to have models that are deeper or more wide? Can the authors actually report the latency in a comparative table next to BERT? Can the authors provide a sense of how large this model is in MB - e.g. presumably a goal of less parameters would be to have a model with less memory, but then the decision between memory and latency that different models make should be made more clear.\n\n4. Section 4.8 is not clear. Exactly how much data, in terms of GB of uncompressed text, is used here? Is it the data of XLNet and RoBERTa, so larger than both of those settings individually? Further, the authors train for 1 million steps. This is larger than both XLNet and RoBERTa, is that correct? Or there is some detail about the size of the batch that actually makes it comparable? The many small tables where the changes are not clearly delineated makes it difficult to compare results. \n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 4}, "SygC0QIhFB": {"type": "review", "replyto": "H1eA7AEtvS", "review": "Summary: This paper investigates improving upon BERT by reducing complexity in terms of free parameters and memory footprint as well as computation steps. They propose 2 strategies for doing this: 1) Splitting the embedding matrix into two smaller matrices (going from V x A to V x B + B x A where B <<<< A); 2) layer-wise parameter sharing. They also utilize sentence order prediction to help with training. These coupled with a bunch of other choices such as using the lamb optimizer, certain hyperparameters etc help show dramatic empirical gains across the board on a wide variety of NLP/NLU tasks.\n\nPositives: This paper has a dramatic, seemingly statistically significant reduction in error across a wide-variety of tasks. It provides a thorough experimental plan and approaches the few addendums to training (splitting the embedding matrix, the layer-wise parameter sharing, and the sentence order prediction).\n\nConcerns & Questions: There's a lot of experimentation here and a lot of seemingly deliberate choices after seeing empirical results during the research phase. How crucial are the choices of optimizer and other specific hyperparameters? Were there ones you observed that were more brittle than others? Any specific 'reasonable' configurations/settings that caused degenerate solutions?", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 3}, "Hkl2NgFecr": {"type": "review", "replyto": "H1eA7AEtvS", "review": "This paper proposes a new pre-trained BERT-like model called ALBERT. The contributions are mainly 3-fold: factorized embedding parameterization, cross-layer parameter sharing, and intern-sentence coherence loss. The first two address the issue of model size and memory consumption in BERT; the third corresponds to a new auxiliary task in pre-train, sentence-order prediction (SOP), replacing the next sentence prediction (NSP) task in BERT. These modifications lead to a much leaner model and improved performance. As a result, ALBERT pushes the state of the art on GLUE, RACE, and SQuAD while having fewer parameters than BERT-large. \n\nThis is a well-written paper which is easy to follow even for readers without deep background knowledge. The proposed method is meaningful and effective. Its empirical results are impressive. \n\nOther comments:\n\n- Section 4.9. Why use the all-share condition for state-of-the-art ALBERT results (as indicated in Table 2)? Judging from Table 4 and 5, shouldn't the non-shared condition give better results? The number of parameters would be larger, of course. \n\n- I like the justification/motivation given for replacing NSP with SOP. I wonder if the authors have tried other objectives (but didn't work out). Such negative results are valuable to practitioners.\n\n- Typo in Sec. 4.1: x1,1, x1,2 should be x2,1, x2,2. \n", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 3}, "SklNxFQ6KB": {"type": "rebuttal", "replyto": "SkgS1BMsKB", "comment": "Hi Erhan, \n\nThank you for your comment. Here are the answers to your questions. \n\n1. Is the large batch size/LAMB combo essential for the performance of Albert? Bert uses a smaller batch size/Adam. At least in my implementation Albert converges slower when pretraining on Wiki.\n\nNot sure how large is the batch size you mentioned, but I tested on the 512/1M step setting from this paper (https://arxiv.org/pdf/1904.00962.pdf) as well and they converge to a roughly similar MLM accuracy.  I also tried to use Adam and the results there are also similar. \n\nGiven that none of our models really converge, I am not sure how you define slower convergence. However, albert-base does perform worse than bert-base, so you will see lower MLM accuracy given the same number of iterations. \n\n2. Are the initial parameters shared between input and output projections? \nYes. There are shared as BERT does. ", "title": "Thanks for your comment"}, "rJe1YLXptS": {"type": "rebuttal", "replyto": "HkelX735YH", "comment": "Hi Tomotaka, \n\nThe code and models are out in the public. But I cannot put the URL here due to the anonymous policy. However, if you put your email here, I can send you the link directly. Another way is to search the code online. ", "title": "the url to the code and models"}, "HkeS39ottH": {"type": "rebuttal", "replyto": "H1eA7AEtvS", "comment": "When we mention \"full network pre-training (Radford et al., 2018; Devlin et al., 2019) has led to a series of breakthroughs in language representation learning\", we miss the following two seminal work: \n1) Dai, Andrew M., and Quoc V. Le. \"Semi-supervised sequence learning.\" Advances in neural information processing systems. 2015. \n2) Howard, Jeremy, and Sebastian Ruder. \"Universal language model fine-tuning for text classification.\" arXiv preprint arXiv:1801.06146 (2018).  \n\nWe apologize to the authors of these two papers and will add them to our citation list in our revised version. Thanks for people who bring this to our attention. ", "title": "Missing two important citations "}, "SyxfFRUKYH": {"type": "rebuttal", "replyto": "ryeI2ojwKS", "comment": "Hi Qingqing, \n\nThanks for your comment and I am glad that you like our work. We haven't throughly tested the inference speed of ALBERT models yet but theoretically speaking, ALBERT won't improve the inference speed.  It is more of an improvement on memory consumption. As stated in the discussion section, we are currently working on improving the inference speed by applying sparse/block attention.", "title": "Thank you for your comment"}, "Hygq1uOBtS": {"type": "rebuttal", "replyto": "rJgHaTrStr", "comment": "Hi Tomotaka, \n\nThanks for your comment. I am glad that you like our work. We are in the process of releasing the code and models. They are expected to be out late this week or early next week. Please stay tuned!", "title": "Thanks for your comment"}, "BJej4IRGtB": {"type": "rebuttal", "replyto": "rylDeWCGKH", "comment": "Hi Shuailiang, \n\nThanks for your comment. Will correct this typo in next version. ", "title": "Thanks for your comment "}, "rke69I1GYr": {"type": "rebuttal", "replyto": "Bke6BA0WFB", "comment": "Hi Chen, \n\nThanks for your comment. I am glad that you find our work interesting. \n\nWe are working on releasing the code as well as the models. As the same time, please let us know if you have any question about where the improvements come from. We will try out best to make it clear to you. ", "title": "Thank you for your comments"}, "rkedPXbJYB": {"type": "rebuttal", "replyto": "Syx5NANRuH", "comment": "Hi Brandon, \n\nThank you so much for helping us to check the parameter counts! your numbers are correct. The ALBERT-xxlarge discrepancy is due to a typo. We meant to put in 223M as in Table 14  (appendix) instead of 233M. We will update the ALBERT-xxlarge count to be 225M and ALBERT-xlarge count to be 60M. ", "title": "Thanks for the comment"}, "rJl4bouVOr": {"type": "rebuttal", "replyto": "HkxHtfRyOB", "comment": "Hi Sachin, \nThank you for your comments, I am glad that you enjoy our paper. \nAlthough Baevski and Auli [r1], Dai et al. [r2], and Grave et al. [r3] also try to address the large vocabulary problem, they have different settings from the one in our paper for the following reasons:\n1) Different problem settings:\n[r1][r2][r3] use whole words embeddings as they main setting while we use subword embeddings. Because of this difference, their vocabulary sizes are much larger than the one we have. They have a problem of how to learn good embeddings for those words that rarely occur while this problem is less of an issue for us. Our main problem is the large memory consumption because of the large number of parameters. \n2) Different main ideas of solutions:\nBecause of the different problems we face, we use different solutions. [r1] and [r2] have adaptive embedding size to make it easier for infrequent words to learn embeddings. We use same but smaller embedding size for all our subwords. One evidence that illustrates the differences between these two solutions is that In [r1], they have a comparison with subword embeddings, where they still tie the embedding size and hidden size. \n3) Different experimental settings: \n[r1][r2][r3] focused on standard language modeling tasks rather than the pretraining/finetuning setting we have. \nNonetheless, It is helpful to illustrate our idea by comparing embedding factorization methods with adaptive softmax [r2] [r3] and embeddings [r1][r2]. And we agree that these relevant papers are worth citing and including in our paper. Thanks for your useful suggestions!\n", "title": "Thank you for your comment"}, "S1lYooO4dS": {"type": "rebuttal", "replyto": "B1gxrqAsPr", "comment": "Hi Emma, \n\nThanks for your comment. I am glad that you like our work. \n\nFor the cross-layer parameters sharing technique in the final results, we use all-shared technique as stated in section 3.1: \"The default decision for ALBERT is to share all parameters across layers.\" We will make it more explicit by adding the following sentence: \"all our experiments use this default decision unless otherwise specified\".\n\nRegarding speedup comparisons: fair enough. The reason we treated BERT-xlarge as a Speedup of 1X in Table 3 is because it's the slowest. To compare to BERT-large instead, one can divide all Speedup numbers by 3.8. We also have direct speed comparison between ALBERT-xxlarge and BERT-large in Section 4.7 and Section 5. However, we take your point that the BERT-large Speedup numbers are more useful for the community and will modify this in the next version.  \n\nFor the overgeneralization problem in the sentence \u201cdropout can hurt performance in large Transformer-based models\u201d, we will make it clear by adding this sentence: \u201cHowever, the underlying network structure of ALBERT is a special case of the transformer and further experimentation is needed to see if this phenomenon appears with other transformer-based architectures or not.\u201d\n", "title": "Thank you for your comment"}}}