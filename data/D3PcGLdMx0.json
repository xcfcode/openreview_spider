{"paper": {"title": "MELR: Meta-Learning via Modeling Episode-Level Relationships for Few-Shot Learning", "authors": ["Nanyi Fei", "Zhiwu Lu", "Tao Xiang", "Songfang Huang"], "authorids": ["~Nanyi_Fei1", "~Zhiwu_Lu1", "~Tao_Xiang1", "~Songfang_Huang1"], "summary": "This is the first work on explicitly modeling episode-level relationships for few-shot learning.", "abstract": "Most recent few-shot learning (FSL) approaches are based on episodic training whereby each episode samples few training instances (shots) per class to imitate the test condition. However, this strict adhering to test condition has a negative side effect, that is, the trained model is susceptible to the poor sampling of few shots. In this work, for the first time, this problem is addressed by exploiting inter-episode relationships. Specifically, a novel meta-learning via modeling episode-level relationships (MELR) framework is proposed. By sampling two episodes containing the same set of classes for meta-training, MELR is designed to ensure that the meta-learned model is robust against the presence of poorly-sampled shots in the meta-test stage. This is achieved through two key components: (1) a Cross-Episode Attention Module (CEAM) to improve the ability of alleviating the effects of poorly-sampled shots, and (2) a Cross-Episode Consistency Regularization (CECR) to enforce that the two classifiers learned from the two episodes are consistent even when there are unrepresentative instances. Extensive experiments for non-transductive standard FSL on two benchmarks show that our MELR achieves 1.0%-5.0% improvements over the baseline (i.e., ProtoNet) used for FSL in our model and outperforms the latest competitors under the same settings.", "keywords": ["few-shot learning", "episodic training", "cross-episode attention"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper explores the effect of poorly sampled episodes in few-shot learning, and its effect on trained models. The improvements from the additional attention module (CEAM) and regularizer (CECR) are strong, and the ablations are thorough. The reviewers are not fully convinced that poor sampling is indeed the main issue. That is, it could be that CEAM and CECR improve performance for other reasons, but the hypothesis is sensible, and the reviewers believe a more thorough investigation is beyond the scope of this work.\n\nDuring discussions, one note that came up is whether CEAM works because of cross-episode attention, or if the idea of an instance-level FEAT is itself a good one. One ablation to sort this out would be to apply FEAT and an instance-level FEAT on episodes that are twice as large as those seen by CEAM so that the effective episode size is the same. This would help answer: is it the reduced noise due to effectively larger episodes, a stronger attention mechanism using instance-level information, or is the idea of crossover episodes indeed the important factor? The reviewers agree that this baseline, or an analogous baseline, should be included in the final version.\n"}, "review": {"nnkihBavup": {"type": "review", "replyto": "D3PcGLdMx0", "review": "Summary of Paper:\nThis paper proposes to improve Prototypical Networks by a method MELR which aims to fix the problem caused by poorly represented classes in sampled episodes and to reap benefits from enforcing cross-episode consistency. The proposed method achieves State of Art results on commonly used benchmarks miniImageNet and tieredImageNet. \n\nReasons for score:\nOverall, I tend towards rejecting this paper. I think cross-episode relationship is an important topic of study in the context of few-shot learning, and the experimental results in this paper are strong. However, I am unconvinced that this \u2018hammer\u2019 is the right tool for the \u2018nail\u2019 that the authors claim to solve. A more thorough study of the motivating problem and how & why MELR works would greatly improve this paper.\n\nPros:\n1.The proposed method performs well in standard benchmark datasets miniImageNet, tieredImageNet, and CUB200. Assuming correctness of experimental protocol, the improvement over previous methods is significant.\n2. Visualization of embedding space by t-SNE lends further credibility to the strong performance. Samples from each class are well clustered yet still disperse.\n3. Ablation of hyperparameters and algorithmic alternatives is mostly complete and honestly presented.\n\nCons:\n1. The paper is motivated by the supposed \u201cpoorly sampled episodes\u201d problem. While it intuitively makes sense that some data points are more representative than others, whether this has a disparate impact on episode few-shot learning is unclear. In my opinion, the small sample problem in few-shot episodes is no worse than that encountered in standard batch training. In standard supervised learning tasks, batch size as small as 1 has been used successfully to train deep networks given sufficient training epochs. Without empirical or theoretical illustration, I am not convinced that the problem the authors seek to address is a real problem.\n2. The reasoning behind equation 2 is unclear. It is not clear why the attention module output in CEAM is added to the embedding F if the goal is to ignore bad examples and emphasize good examples. The choice to ignore class labels when doing attention is also surprising as it doesn\u2019t use the fact that both episodes have the same classes.\n3. The proposed method aims to use inter-episode information to stabilize representation learning, hence samples two episodes at a time and apply CEAM and CECR to the joint episode. I don\u2019t see why the authors restrict the method to just two episodes. Classifier consistency should hold transductively across any number of episodes with the same base classes. Thus, you could make the number of episodes an hyperparameter and experimentally verify what is the best choice.\n4. Grammar mistakes are common and writing generally lacks polish. \n\nQuestions:\n\tPlease address the points in cons above.\n\nMinor points and additional feedback:\nIntroduction\n\u201ceven may be impossible\u201d -> may even be impossible\n\u201creliance of deep neural networks on sufficient annotated training data\u201d: you always need \u201csufficient\u201d data, FSL aims to make fewer data be sufficient.\n\u201cMeta-training\u201d, \u201cepisode\u201d and \u201cbase class\u201d used before definition\n\u201cConcretely, MELR consists of two key components: a Cross-Episode Attention Module (CEAM) and a Cross-Episode Consistency Regularization (CECR)\u201d -> remove \u2018a\u2019\n\u201ctwo cross-episode components (i.e., CEAM and CECR) to explicitly enforcing\u201d -> to explicitly enforce\nRelated Work\nVery few model-based methods are mentioned but I guess that\u2019s beyond the point here.\n\u201cAlmost all existing meta-learning based FSL methods ignore the relationships across episodes\u201d -> \u201cIgnore\u201d is probably not true since many works (incl. MAML) frame the problem of Meta-learning as an inter-task learning process (as presented in this review https://arxiv.org/pdf/2004.05439.pdf). There\u2019s also this work (https://arxiv.org/abs/1909.11722) that looks at the role of shots when building episodes during and after meta-training.\nMethodology\nToo many inline equations. Even with a dedicated definition section there is still a new definition almost every paragraph. Important equations should be made standalone, definitions placed into its own section, and fluff math be removed.\nWhy does CEAM take S as argument twice? Should they be S_k and S_v instead?\nEqn 2: Is the softmax taken over rows or columns (or both) of F_qS_k^T?\n\n[Post rebuttal] I have increased the score of my review to 7. Below is a copy-pasta of my comments post discussion:\n\nWhile my original concern about how much sampling affects FSL is still not fully addressed, I think it is not a trivial question to answer and a full exploration of the topic could constitute its own paper. So although I'm not fully convinced about the motivation of this paper, I think the thorough experimental evaluation along with the strong empirical results together warrants publication. From my perspective, a particular important strength of this paper is its ablations. I'm fairly convinced that the presented implementation is likely the best way to implement the idea of cross-episode attention + distillation. I think the baseline proposed by the AC makes sense. It would be great if that could be incorporated into the final version of the paper. A possible explanation for the drop in performance when using 3 or more episodes is due to the relative decrease in episode diversity. Results on wider datasets could corroborate this hypothesis.", "title": "Review of MELR: META-LEARNING VIA MODELING EPISODELEVEL RELATIONSHIPS FOR FEW-SHOT LEARNING", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "R7iSCVPAks": {"type": "review", "replyto": "D3PcGLdMx0", "review": "### Summary\nThis paper proposes a way to exploit relationships across tasks in episodic training with the goal of improving the trained models who might be susceptible to poor sampling in for few-shot learning scenarios. The proposed model consists of two components: a cross-attention transformer (CEAM) which is used to observe details across two episodes, and a regularization term (CECR) which imposes that two different instances of the same task (which have the exact same classes) are consistent in terms of prediction. Cross-attention is computed via a scaled-attention transformer using both support and query set. The consistency loss is a knowledge distillation that imposes an agreement on the two episodes. The soft target is chosen among the two predictions selecting the classifier with the highest accuracy.\n### Considerations: \n- I like the idea of exploiting the information across tasks to improve the performance of episodic meta training. This is an interesting direction that should might definitely help disambiguate in the case of poor sampling.\n- The ablation study is accurately performed giving the impression of a careful examination of the components of the model proposed.\n- I'm not sure the authors can claim sota results: here some of the latest models that perform best on mini-imagenet https://paperswithcode.com/sota/few-shot-image-classification-on-mini-1 I would prefer to restate the contribution as an improvement of x% over the baseline. It is obvious that sota performance requires higher capacity models such as dense-net. I think that other experiments are needed in order to make the claim of achieving sota, otherwise, if the claim is changed, I'm satisfied with the experiments.\n- I suggest the authors moving algorithm 1 in the main paper, maybe replacing the verbose description of each step with actual formulas and pseudocode.\n- I think there is still room for improvement on the manuscript.  The paper might be a good contribution to the scientific community, but I'll wait for the authors' response on my doubts before my final decision. \n\n### Questions:\n- Q1: Why only considering tasks with the same classes for consistency? Why not considering also partial overlapping of classes across tasks? I guess it is only for simplicity, but it might be beneficial to consider other types of relationships.\n- Q2: It is not exactly clear how the meta-test evaluation is performed. I understand that during training you always consider a pair of episodes that are used to transform the features, but how does this translate at meta-test time? Do you always need a pair of episodes? My guess is no, but not using the transformer should change the distribution of the features at test-time and I don't find it trivial to see how this is taken into account. Maybe I'm just misreading the paper. I suggest the authors clarifying this point in the paper.\n", "title": "Good paper, proper experimental evaluation.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rlobgeneqx": {"type": "rebuttal", "replyto": "ZoXPXNyrque", "comment": "**1. It would be more interesting to have the same ablation of figure 2a for tieredImageNet 5-shots with resnet12 and conv4-512 since there is much more margin to see the actual contribution of each component.** \\\nA: We are sorry for being unable to present the required results since *tiered*ImageNet is too large and there is no enough time for us to run the experiments. Instead, we conduct the suggested ablation study on the fine-grained CUB dataset. The obtained ablation results are given in the table below (which have also been added in Table 3 of Appendix A.4).\n\n| Method | Backbone | 5-way 1-shot | 5-way 5-shot |\n| - | :-: | :-: | :-: |\n| ProtoNet | Conv4-64 | 64.42$\\pm$0.48 | 81.82$\\pm$0.35 |\n| ProtoNet+CEAM | Conv4-64 | 68.92$\\pm$0.50 | 84.54$\\pm$0.32 |\n| MELR | Conv4-64 | 70.26$\\pm$0.50 | 85.01$\\pm$0.32 |\n\nWe can see that: (1) Adding our CEAM to the baseline ProtoNet improves the performance by a large margin (2.7% - 4.5%) and our MELR further achieves noticeable improvements (0.5% - 1.3%) by adding CECR on top of ProtoNet+CEAM. This indicates that both CEAM and CECR are crucial for fine-grained FSL. (2) CEAM does make greater contribution to the final FSL results than CECR. This is expected since CECR is essentially a consistency constraint which does not have any learnable parameters. \n\nMoreover, we can also make similar observations with the ablation results under transductive FSL in Table 7 of Appendix A.10, where CECR achieves noticeable improvements (0.6% - 1.4%) on top of CEAM but again contributes relatively less to the final FSL performance than CEAM does (2.5% - 4.8% improvements over the baseline Semi-ProtoNet). Please also refer to Appendix A.10 for more details.", "title": "Response to Further Question on Ablation Study of AnonReviewer1"}, "lT933fmeaG": {"type": "rebuttal", "replyto": "baj_nIn73RU", "comment": "Thanks for the reviewer\u2019s constructive comments and suggestions. \n\n**1. If the goal is to reduce outlier effect by selecting the best supports, wouldn't it make more sense to attend on $\\mathbf{S}^{(1)}$ with $\\mathbf{S}^{(2)}$? I think adding a discussion on ... would help me understand this. Maybe even a toy example to illustrate the concept explicitly?** \\\nA: To demonstrate how our proposed CEAM can alleviate the negative effect of the poorly sampled shots, we present a schematic illustration of CEAM with a toy visual example (for easy understanding but without loss of generality) in Figure 7 of Appendix A.8. Please refer to the revision for more details.\n\nAdditionally, we provide the results obtained by an extra CEAM alternative (as the reviewer suggested) in Table 6 of Appendix A.8. We can see from Table 6 and also Figure 2(b) of the main paper that our choice achieves the best results among all CEAM alternatives. One possible explanation for why we resort to updating all samples is that transforming support and query samples into the same embedding space is beneficial to the model learning.\n\n**2. Regarding Q3, for using CEAM with multiple episodes, wouldn't it be more intuitive to concatenate all supports in the \u201cother\u201d episodes and perform attention on that?** \\\nA: Thanks for the suggestion. We name our former implementation as \u2018Implement. (1)\u2019 and name the suggested one as \u2018Implement. (2)\u2019. The results on *mini*ImageNet using Conv4-64 with \u2018Implement. (2)\u2019 are shown below.\n\n| Method | #episodes | 5-way 1-shot | 5-way 5-shot |\n| - | :-: | :-: | :-: |\n| MELR | 2 | 55.35$\\pm$0.43 | 72.27$\\pm$0.35 |\n| MELR | 3 | 55.19$\\pm$0.43 | 71.90$\\pm$0.35 |\n| MELR | 4 | 55.03$\\pm$0.44 | 71.76$\\pm$0.35 |\n\nWe have also added these results in Table 4 of Appendix A.6. Please refer to the revision for more details.\n\n**3. Computationally, how expensive is using CECR? Does the performance benefits justify the added complexity. Also, are there usecases where CECR do have a more significant impact on performance (e.g. are there episode settings where MELR significantly outperforms PN+CEAM)?** \\\nA: Thanks for the comment. Since CECR (in our MELR) requires no extra learnable parameters and only computes a loss for consistency constraint, it brings very limited computational cost. Empirically, the meta-training time of MELR is almost the same as that of ProtoNet+CEAM, indicating that the improvement over ProtoNet+CEAM is obtained by CECR at an extremely low cost.\n\nTo study when CECR has a significant impact on the final FSL performance, we select 10 meta-test episodes from the 2,000 ones used in the evaluation stage and visualize them in Figure 8 (please refer to Appendix A.9 for more details). We can see that: (1) When the few-shot classification task is hard (i.e., ProtoNet obtains relatively low accuracy), CEAM leads to significant improvements (about 3% - 9%). (2) In the same hard situation, CECR further achieves significant improvements (about 5% - 8%) on top of CEAM and shows its great effect on the final FSL performance. This indicates that CECR and CEAM are complementary to each other in hard situations, and thus both are crucial for solving the poor sampling problem in meta-learning based FSL.\n\n**4. If I understand correctly, your reported experimental results are under the non-transductive setting? If so, I think this could be made more clear in the introduction of the paper. I am also curious about whether MELR can improve performance in the transductive setting too.** \\\nA: Good suggestion! We did report the experimental results strictly under the non-transductive setting. And we have made it clearer in the abstraction, introduction, and conclusion of the main paper. Moreover, we present comparative results under the transductive FSL setting on *mini*ImageNet in the table below. We can see that our MELR achieves the best results among all the transductive FSL methods. Please also refer to Appendix A.10 for more details.\n\n| Method | Backbone | 5-way 1-shot | 5-way 5-shot |\n| - | :-: | :-: | :-: |\n| Semi-ProtoNet (Ren et al., 2018) | Conv4-64 | 55.50$\\pm$0.10 | 71.76$\\pm$0.08 |\n| TPN (Liu et al., 2019) | Conv4-64 | 55.51$\\pm$0.84 | 69.86$\\pm$0.67 |\n| TEAM (Qiao et al., 2019) | Conv4-64 | 56.57 | 72.04 |\n| FEAT (Ye et al., 2020) | Conv4-64 | 57.04$\\pm$0.16 | 72.89$\\pm$0.20 |\n| ProtoNet+CEAM (ours) | Conv4-64 | 60.30$\\pm$0.49 | 74.28$\\pm$0.36 |\n| MELR (ours) | Conv4-64 | 61.67$\\pm$0.51 | 74.87$\\pm$0.35 |", "title": "Response to Further Comments of AnonReviewer2"}, "EZW63AVOGCk": {"type": "rebuttal", "replyto": "jI2RSfUOSA3", "comment": "Regarding Q1 ('It is not clear why CEAM can alleviate the negative effect of badly-sampled support instances'), we have added a discussion in Appendix A.8. Concretely, we present a schematic illustration of CEAM with a toy visual example (for easy understanding but without loss of generality) in Figure 7 of Appendix A.8. Please refer to the revision for more details.\n\nThank you again for your constructive review!", "title": "Additional Response to Q1 of AnonReviewer3 "}, "rY3sY_G3ZLS": {"type": "rebuttal", "replyto": "H6O-YPOXUi", "comment": "Thanks. Please refer to our response to further comments of AnonReviewer2.", "title": "RE: Agree with R2 "}, "txKNbF1gXm": {"type": "rebuttal", "replyto": "ptXS46Gec14", "comment": "**Q1: Question regarding the number of parameters.** \\\nA1: We select several representative/latest FSL models from Table 1 of the main paper and list their numbers of parameters in the table below. The accuracies in this table are obtained on *mini*ImageNet. We can observe that: (1) With an extra CEAM in addition to the backbone (Conv4-64 or ResNet-12), our MELR has about 13-15% relatively more parameters than the baseline ProtoNet. Note that CECR (in our MELR) leads to no extra parameters. Considering the (statistically) significant improvements achieved by our MELR over ProtoNet, we think that our MELR is cost-effective because it requires not much additional parameters. (2) The number of MELR's parameters is almost the same as that of FEAT's and is much less than that of PARN's, but our MELR achieves better results than FEAT and PARN, indicating that our MELR is the most cost-effective among these three methods. Note that FEAT also takes ProtoNet as the baseline. We have added this discussion in Appendix A.7. \n\n| Method | Backbone | # Parameters | 5-way 1-shot | 5-way 5-shot |\n| - | :-: | :-: | :-: | :-: |\n| ProtoNet (Snell et al., 2017) | Conv4-64 | 113.09K | $52.78\\pm0.45$ | $71.26\\pm0.36$ |\n| PARN (Wu et al., 2019) | Conv4-64 | 405.49K | $55.22\\pm0.84$ | $71.55\\pm0.66$ |\n| FEAT (Ye et al., 2020) | Conv4-64 | 129.66K | $55.15 \\pm 0.20$ | $71.61 \\pm 0.16$ |\n| MELR (ours) | Conv4-64 | 129.66K | $\\bf55.35\\pm0.43$ | $\\bf72.27\\pm0.35$ |\n| ProtoNet (Snell et al., 2017) | ResNet-12 | 12.42M | $62.41\\pm0.44$ | $80.49\\pm0.29$ |\n| FEAT (Ye et al., 2020) | ResNet-12 | 14.06M | $66.78\\pm0.20$ | $82.05\\pm0.14$ |\n| MELR (ours) | ResNet-12 | 14.06M | $\\bf67.40\\pm0.43$ | $\\bf83.40\\pm0.28$ |", "title": "Response to AnonReviewer1"}, "ayxLS08Ks2m": {"type": "rebuttal", "replyto": "nnkihBavup", "comment": "Thanks for the reviewer\u2019s constructive comments and suggestions. \n\n**Q1: I am not convinced that the problem the authors seek to address (\u2018poorly sampled episodes\u2019 problem) is a real problem \u2026 In my opinion, the small sample problem in few-shot episodes is no worse than that encountered in standard batch training.** \\\nA1: Sorry for the confusion. Indeed, in standard supervised learning, the model updating in each mini-batch iteration suffers little from the poorly sampled mini-batches problem. However, it is very different from the \u2018poorly sampled episodes\u2019 problem studied in this paper under the few-shot learning setting. More specifically, in mini-batch based supervised learning, the training instances of the same classes will be sampled in different mini-batches. That is, the overall classification task remains unchanged across different mini-batches. So any mini-batch with poor samples will have a small impact on the final trained model \u2013 there are much more iterations/mini-batches that have no outliers to recover the negative effect. In FSL, however, each episode contains a new meta-training task sampled from a pool of seen classes, and in the next episode the task will be completely different. So when there are outliers in each episode, their negative impact on the task must be dealt with immediately. This is because the same model will be deployed for meta-test where *an unseen task is presented only once*, potentially corrupted by outliers. The purpose of our MELR is to meta-learn a mechanism (i.e., meta-learn the \u2018poor sampling problem\u2019-solver) so that the negative impact of outliers can be dealt with for any unseen new tasks during meta-test. \n\n**Q2: It is not clear why the attention module output in CEAM is added to the embedding F if the goal is to ignore bad examples and emphasize good examples. The choice to ignore class labels when doing attention is also surprising as it doesn\u2019t use the fact that both episodes have the same classes.** \\\nA2: The attention module used in CEAM is essentially a transformer as those used in NLP (e.g., BERT and GPT). So it is designed to update the embeddings of a set of instances through inter-instance attention, such that outliers can be identified and their effect minimized. Adding attention on top of the original embedding of each instance (i.e., adopting a residual structure) is thus a common practice. Moreover, we present the results in the following table when the residual structure is removed from our CEAM (i.e., we remove the original embedding F in Eqs. (2) and (6)). It can be clearly seen that the residual structure is important for solving the original FSL problem (not only the poor sampling problem needs to be solved) since the original embedding contains descriptive information.\n\n|Method&nbsp;|&nbsp;Backbone&nbsp;|&nbsp;5-way 1-shot&nbsp;|&nbsp;5-way 5-shot|\n|-|:-:|:-:|:-:|\n|ProtoNet+CEAM (w/o residual struct.)&nbsp;|Conv4-64|49.45$\\pm$0.42|64.81$\\pm$0.39|\n|ProtoNet+CEAM (w/ residual struct.)&nbsp;|Conv4-64|55.01$\\pm$0.43|72.01$\\pm$0.35|\n\nThe comment on the class label is an interesting one. We could add a class embedding and combine it to the feature embedding F as input to the attention module. However, this is not needed in our model: the output of our CEAM (i.e., updated embeddings) will be subject to the few-shot classification loss in Eq. (9), which clearly needs to use the class labels of each support set instance. We find that this loss can help to guide the attention module to maintain class separation in the updated embedding space. Moreover, we choose to devise the CECR module that aligns the distributions of predicted scores w.r.t. the two episodes for each query sample. That is, the fact that the two episodes have the same classes is implicitly explored in our CECR.\n\n**Q3: I don\u2019t see why the authors restrict the method to just two episodes.** \\\nA3: Thanks for the suggestion. We have added the experiments in Appendix A.6 by varying the number of episodes $N_e$. Concretely, for each episode $e^{(i)}$ ($i = 1, \\cdots, N_e$), the output of CEAM is defined as:\n$$\\mathbf{\\hat{F}}^{(i)} = \\frac{1}{N_e-1} \\sum_{j = 1, \\cdots, N_e, j \\ne i} \\text{CEAM} (\\mathbf{F}^{(i)}, \\mathbf{S}^{(j)}, \\mathbf{S}^{(j)}).$$\nAs for CECR, we determine the episode with the best accuracy and distill knowledge to the rest $N_e-1$ episodes. The results on *mini*ImageNet using Conv4-64 in the following table show that the performance drops slightly as the number of episodes increases. One possible explanation is that too much training data make the model fit better on the training set but fail to improve its generalization ability on novel classes. \n\n|Method&nbsp;|&nbsp;#episodes&nbsp;|&nbsp;5-way 1-shot&nbsp;|&nbsp;5-way 5-shot&nbsp;|\n|-|:-:|:-:|:-:|\n|MELR|2|55.35$\\pm$0.43|72.27$\\pm$0.35|\n|MELR|3|55.26$\\pm$0.44|71.88$\\pm$0.35|\n|MELR|4|55.15$\\pm$0.43|71.63$\\pm$0.35|", "title": "Response to AnonReviewer2 \u2013 Part 1/2"}, "Hv9EAL-fsgh": {"type": "rebuttal", "replyto": "ayxLS08Ks2m", "comment": "**Q4: Grammar mistakes are common and writing generally lacks polish.** \\\nA4: Thanks for the suggestion. We have carefully polished our paper.\n\n**Q5: \u2018Ignore\u2019 is probably not true since many works frame the problem of meta-learning as an inter-task learning process.** \\\nA5: Sorry for the confusion. We have changed the claim to \u2018In the FSL area, relatively less effort has been made to explicitly model the relationships across episodes.\u2019\n\n**Q6: Why does CEAM take $\\mathbf{S}$ as argument twice? Should they be $\\mathbf{S}_K$ and $\\mathbf{S}_V$ instead? Eqn. (2): Is the softmax taken over rows or columns (or both) of $\\mathbf{F}_Q \\mathbf{S}_K^T$?** \\\nA6: (a) CEAM indeed takes $\\mathbf{S}$ as argument twice since one is multiplied with $\\mathbf{W}_K$ (resulting in $\\mathbf{S}_K$) and the other is multiplied with $\\mathbf{W}_V$ (resulting in $\\mathbf{S}_V$), which means that the linear projections are done inside CEAM.\n\n(b) The softmax is taken over columns in Eqn. (2) since each row is an independent weight vector from other rows.", "title": "Response to AnonReviewer2 \u2013 Part 2/2"}, "jI2RSfUOSA3": {"type": "rebuttal", "replyto": "gw7f_NOqe6a", "comment": "Thanks for the reviewer\u2019s constructive comments and suggestions. Our responses are as follows.\n\n**Q1: It is not clear why CEAM can alleviate the negative effect of badly-sampled support instances. Can the authors qualitatively explain the reason for this? In the last row of fig 4 (*now Figure 5 in the revision*), it seems that CEAM creates obvious outlier instances. Why does this happen?** \\\nA1: Under the few-shot setting, the biggest negative effect of having an outlying support instance is that the class mean/prototype will be heavily biased by the outlier. This would lead to prototypes of different classes to be close to each other, causing problems for classifying query samples using these prototypes.\n\nOur CEAM is essentially a self-attention based transformer. It transforms the latent embedding of each support set sample by allowing it to examine (attend to) other samples, both from the same class and different classes, in the support set, and update its embedding as a weighted sum of all samples. The weight is determined mostly by similarity or proximity in the original embedding space. Based on this understanding, it is now easy to understand the usefulness as well as the limitation of CEAM for countering the negative effect of bad samples. In particular, the learned transformer is subject to the cross-entropy loss computed on the query using the updated prototypes. This encourages the transformer in CEAM to update the embeddings so that different classes become well separable, therefore largely neutralizing the main negative effect of the outlying samples, as supported by Figure 3(b) vs. 3(a) and more visualizations in Figure 5 in the revision.\n\nHowever, the effect of CEAM on the relationship of outliers and inliers of the same class is limited, which is determined by the nature of the transformer: the inliers are far away from the outliers so would have limited effect in pulling them toward the inlier majority. This is shown clearly in Figure 5 bottom row \u2013 when the outliers are extremely different from the inliers, not being able to pull them close to the inliers would hinder CEAM\u2019s main objective of preventing class overlapping. That is why in this extreme case we need CECR: the predictions based on two sets of prototypes need to be consistent using CECR, which means that the learned embeddings must pull the outliers closer to the inliers. Therefore, both CEAM and CECR are necessary for our full model.\n\n**Q2: Hyper-parameter candidates used for MELR are not described. How much does the proposed method depend on hyper-parameters such as $T$ and $\\lambda$?** \\\nA2: Thanks. We have added the hyper-parameter candidates in Section 4.1 of the main paper. Concretely, we select $T$ from {16, 32, 64, 128} and $\\lambda$ from {0.02, 0.05, 0.1, 0.2} based on the validation performances. Moreover, we have also added a hyper-parameter analysis in Appendix A.5. Our method is shown to be insensitive to the hyper-parameters.\n\n**Q3: In eqs. (1) and (9), although argmax is taken in the loss function, it is not correct when using the cross-entropy loss.** \\\nA3: Thanks. We have made the correction in the revision.", "title": "Response to AnonReviewer3"}, "jqhJ58iTKkX": {"type": "rebuttal", "replyto": "R7iSCVPAks", "comment": "We\u2019d like to thank the reviewer for the constructive comments and suggestions. We have accordingly made changes in the revision. \n\n**Q1: I'm not sure the authors can claim SOTA results.** \\\nA1: Thanks for pointing this out. To avoid misunderstanding on the SOTA claim, we have now modified the claim in the revision as suggested by the reviewer. Indeed, higher results have been reported elsewhere but achieved with larger backbones, external data, and/or the transductive setting. Under the most standard FSL setting (with three commonly used backbones, no external data, non-transductive), our MELR indeed achieves the best performance on the two benchmarks.\n\n**Q2: I suggest the authors moving algorithm 1 in the main paper.** \\\nA2: Thanks for the suggestion. We have moved Algorithm 1 to the main paper and modified the descriptions.\n\n**Q3: Why only considering tasks with the same classes for consistency? Why not considering also partial overlapping of classes across tasks?** \\\nA3: Great suggestion! After the ICLR paper submission, we have actually considered episodes with partially overlapped sets of classes. Our main idea is to modify the CECR part but with CEAM unchanged. Concretely, given a pair of randomly sampled episodes, for support samples from the disjointed classes (if any) that come from only one episode, we apply data augmentation (e.g., random crops/horizontal flip) to them so that each class from the two episodes now has two sets of $K$ shots for us to implement CECR. The obtained results under the 5-way 1-shot and 5-shot settings on *mini*ImageNet (with Conv4-64 as the backbone) are 54.72%$\\pm$0.43% and 71.51%$\\pm$0.35%, respectively. These are good results. However, it seems that exploiting partial overlapping of classes across tasks does not lead to performance improvements over our main results in Table 1 (55.35%$\\pm$0.43% for 1-shot and 72.27%$\\pm$0.35% for 5-shot). Perhaps new algorithms need to be designed to model other types of cross-episode relationships more effectively \u2013 we will leave it to the future work.\n\n**Q4: It is not exactly clear how the meta-test evaluation is performed.** \\\nA4: Sorry for the confusion about evaluation protocols. Indeed, only one episode is needed during meta-test, which is the same as previous works. We have stated in Section 4.1 that our MELR is evaluated over meta-test episodes independently (i.e., one episode at a time). Concretely, we apply the trained CEAM within each episode $e^{(test)}$ by inputting the triplet $(\\mathbf{F}^{(test)}, \\mathbf{S}^{(test)}, \\mathbf{S}^{(test)})$, where $\\mathbf{S}^{(test)} \\in \\mathbb{R}^{NK \\times d}$ and $\\mathbf{F}^{(test)} \\in \\mathbb{R}^{N(K+Q) \\times d}$ are the feature matrices of support samples and all samples in $e^{(test)}$, respectively. That is, for each meta-test episode, we take its own support samples (instead of those from another episode) as keys and values for the trained CEAM. Note that we strictly follow the *non-transductive* evaluation setting since the embedding of each query sample in $e^{(test)}$ is transformed by the trained CEAM independently according to the support sets.", "title": "Response to AnonReviewer1"}, "IpdlIqVPx0P": {"type": "rebuttal", "replyto": "ZbQoHiLYmoG", "comment": "We\u2019d like to greatly thank the reviewer for the positive comments!", "title": "Response to AnonReviewer4"}, "gw7f_NOqe6a": {"type": "review", "replyto": "D3PcGLdMx0", "review": "Summary:\n- This paper proposes a meta-learning method (MELR) to alleviate the negative impact of poor sampling of support sets.\nMLER consists of two main modules. The first module (CEAM) applies the attention mechanism to two sampled episodes and it alleviates the negative impact of badly-sampled instances.\nThe second module (CECR) enhances the consistency of classifiers obtained by using two episodes to deal with the sensitivity for the badly-sampled instances.\nSpecifically, to realize this, CECR utilizes a knowledge distillation framework. Experiments with two real-world datasets demonstrate that MELR works well.\n\nPros:\n- This paper proposes a new meta-learning method to alleviate the negative effect of poor sampling of support sets.\n- Experimental results show that MELR can improve the baseline method (Propnet).  These results show some evidence of the effectiveness of two proposed modules (CEAM and CECR).\n\nCons:\n- It is not clear why CEAM can alleviate the negative effect of badly-sampled support instances.\n- Hyper-parameter candidates used for MELR are not described.\n\nDetailed comments and questions:\n- Although empirical results seem to support the effectiveness of CEAM, I do not understand why CEAM alleviates the negative effect of badly-sampled few shots for a given query instance. Can the authors qualitatively explain the reason for this?\n- In the last row of fig 4, it seems that CEAM creates obvious outlier instances. Why does this happen?\n- How much does the proposed method depend on hyperparameters such as $T$ and $\\lambda$?\n\nMinor comments:\n- In eqs. (1) and (9), although argmax is taken in the loss function $L$, it is not correct when using the cross-entropy loss.\n", "title": "Meta-learning method to alleviate the negative impact of poorly-sampled support sets is proposed. Explanation of why the observed improvements could be achieved will improve the paper's quality.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ZbQoHiLYmoG": {"type": "review", "replyto": "D3PcGLdMx0", "review": "Summary\n\nOne problem of few-shot episodic learning is a poor sampling resulting in negative impacts on the learned model. \nThe paper proposes a new episodic training by exploiting inter-episode relationships to deal with poor sampling problem and improve the learned model by enforcing consistency regularization. Cross Episode Attention Module (CEAM) is proposed to alleviate the effect of poorly-sampled shots and Cross-Episode Consistency Regularization (CECR) is proposed to enforce robustness of the classifiers.\n\nStrength\n\n- The paper proposes a novel idea of how to improve few-shot learning by exploiting inter-episode relationships. Using multiple episodes and exploiting inter-episode is a new attempt.\n- There have been attempts to improve few-shot training by batch construction, but the proposed method outperforms the previous approaches with a sizable margin.\n- The extensive ablative studies provide comprehensive comparisons among possible design choices. (including supplementary materials) \n\nWeakness\n\n- I could not find a significant weakness of the paper.\n\n\nRating\n\nI like the overall idea of using inter-episode relationships for few-shot training. The proposed approach shows strong performance and technically straightforward and easy to understand. Another strength of the method is that no additional hyper-parameter is used to tune the performance. The paper is clear and extensive experiments support the effectiveness of the paper including supplementary materials.\n", "title": "The contribution of the paper is clear and novel. My initial rating is accept.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}