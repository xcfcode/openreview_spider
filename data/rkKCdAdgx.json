{"paper": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"], "summary": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "keywords": ["Applications", "Deep learning", "Unsupervised Learning"]}, "meta": {"decision": "Reject", "comment": "The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline."}, "review": {"r1BMxGB8l": {"type": "rebuttal", "replyto": "rkKCdAdgx", "comment": "Updated PDF with suggested references.", "title": "Update"}, "SyrcxReUg": {"type": "rebuttal", "replyto": "Bk1fdnjHe", "comment": "Correct. That's the reference pointed out below by us and by Reviewer 3.", "title": "Answer to \"previous work\""}, "Bk1fdnjHe": {"type": "rebuttal", "replyto": "rkKCdAdgx", "comment": "Bloom Filters have been previously used to reduce training and prediction complexity when facing large output spaces:\nhttps://papers.nips.cc/paper/5083-robust-bloom-filters-for-large-multilabel-classification-tasks.pdf\n", "title": "previous work"}, "By9A88YHx": {"type": "rebuttal", "replyto": "HkSUrgFBx", "comment": "Thanks for the (somewhat delayed) review. The main critic seems to be summarized in the phrase: \"It is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here.\" That is a surprising way to criticize the work, since many advances in deep learning come about through the application of a relatively simple method to a network or model. For instance, batch-normalization is the application of z-score normalization to each layer in the network, dropout is the application of sampling to the units of each layer in the network, embedding is adding a dense layer between the network and the input, residual learning is adding the input to the layer directly to the next layer, and much more. In that respect, we are somewhat surprised to see that the simplicity of our approach is seen as its main weakness.\n\nOur approach is simple but yields very good results and we are not aware of any \"off-the-shelf existing embedding\" that provides all the benefits of Bloom embeddings (such as they can be applied to both inputs outputs, minimal computation overhead, increase in performance compared to alternatives or uncompressed networks in many cases, etc.). We would be grateful if the reviewer could point out any such \"off-the-shelf existing embedding\" with similar properties.\n\nRegarding studying more embedding dimensionalities for competing approaches, we do not do so because the majority of such approaches do not scale in terms of time and space. Consider, for instance, the time required to perform an SVD decomposition from a 70K-by-70K sparse matrix (MSD data set) into 30K dimensions and the space to store the resultant 30K-by-30K dense projection matrix. Moreover, considering different input and output projections would result in an unattainable number of experiments (InputDim \\times OutputDim \\times #Datasets (\\times #Approaches)), which is somewhat prohibitive given our GPU resources. Note though that, even without optimizing for the 'right' input/output embedding dimensionality, our method still performs very well.\n", "title": "Answer to Reviewer 1"}, "B1VuI8YSl": {"type": "rebuttal", "replyto": "HkwLhQrEg", "comment": "Thank you for your review. In the next iteration of the paper we will include and comment the Shi et al. (JMLR 2009) and Cisse et al. (NIPS 2013) references. In addition, we will add the suggested \"Tensorflow\" and \"integer/binary weights\" references to our related work section. Regarding the \"model compression approach\" reference, we'd like to note that we already cite it in Section 2, together with https://arxiv.org/abs/1511.06530.", "title": "Answer to Reviewer 3"}, "S1cVUUtSl": {"type": "rebuttal", "replyto": "rkYspezNl", "comment": "Thank you for your review. We'll try to simplify the description of the method in Section 3.", "title": "Answer to Reviewer 1"}, "BJbgx8m7e": {"type": "rebuttal", "replyto": "rJ4xT_1Xx", "comment": "Thanks for your questions. \n\n1) Yes, the produced vector has k times more ones than the original one. The crucial point here is that the embedding dimension m is smaller than the original d, thus considerably lowering the space requirements of the model.\n\n2) The computational complexity of decoding the output is O(dk), k<<d, typically k={3,4,5} as used in the paper. Notice however that the output of the models after the softmax is not sparse, and one has to iterate over all the outputs, even in the uncompressed case (for instance, to sort or to find the maximum, thus leading to an O(d) process). Moreover, in the case of Bloom Embeddings, such decoding is only needed at prediction time, but not at training time. Finally, the driving motivation of our paper is space efficiency, with affordable timings as a byproduct.\n\n3) Yes. As embeddings can be viewed as an intermediate layer between the encoding of the items and the actual model, our approach yields to embeddings that are distributed across k vectors. Moreover, the total space of the embedding is compressed by a factor of d/m. \n", "title": "Answer to \"Can you discuss the implications for representation learning?\""}, "B1yFyIXml": {"type": "rebuttal", "replyto": "B1xZ8sl7g", "comment": "Thank you for the questions and the interest in the paper. And thank you for pointing out the multilabel classification case. Certainly we missed to include it in the related work section. After some search, we found the work by Cisse et al (NIPS 2013). Was that the work being referred to?\n\n1) Input compression is applied to all data sets without exception. Output compression is applied to all data sets except CADE, which is a 12-class data set. The compression ratios (m/d) reported in the paper are both applied to inputs and outputs (except the CADE case, where it is only applied to the input).\n\n2) The only considered data sets that originally had count data were the CADE and the MSD data sets, which we reduced to a binary indicator of present/not present and like/not like, respectively. As it is reported, we obtain state-of-the-art (or slightly better) results using this trick. In general, many recommendation/collaborative filtering (CF) methods discard count data. Of course, using the actual count is a signal that can improve performance, as it imposes a type of more fine-grained relevance. Count data can be incorporated in CF models in several ways, e.g., using the counts for ranking in the loss function, using them as weights in a weighted least squares model (Wu et al, 2016), or modeling the data with poisson models (Gopalan et al, 2014). In our work, we focused on the most common setting of binary relevance (with no grades), as mostly all the data we used was of binary relevance (or ratings that were binarized), but it is definitely possible to extend the Bloom compression to count data. One option would be to use some kind of counting Bloom filters, which would most likely help in improving the recommendation accuracy in these scenarios.\n\n3) In the experiments we used Keras and Theano. We did not use any sparse layers, and the type of implementation was the same for both compressed and uncompressed networks. We believe that the timing experiments will be quite similar even if sparse layers are used at the input. Basically, in the case of sparse layers, we would have O(c) complexity for the input which, using BE, becomes O(ck), with k typically equal to 3 or 4 and, in many cases, k<c. In that case, the timings would be still dominated by the output layer (O(d) or O(m), d>m>c>=k). Moreover, our main motivation in the paper is fundamentally to reduce the space of the models. Good timings are a byproduct of our approach, but not the driving idea of the paper.\n\n4) We did not. Although we plan to do so in an extension of this work, we expect though to get quite similar results in terms of relative performance (between compressed and uncompressed models). One can expect a 1-3% performance improvement overall by using a ranking loss function (see Hidasi et al, 2016). One could also use a similar subsampling trick on a softmax-type loss as shown in the paper by Ji et al (ICLR 2016). Note though that using an uncompressed model with the subsampling trick, one would still have to store the full output matrix. Interestingly, after the reconstruction of the original space through the proposed Bloom Embedding, we can still use a ranking-based subsampling loss.\n", "title": "Answer to \"Questions on experiments and baselines\""}, "B1xZ8sl7g": {"type": "review", "replyto": "rkKCdAdgx", "review": "Thank you for this very interesting paper. While Bloom filters have already been used to encode outputs in multilabel classification, the paper makes a more general contribution by using it for inputs and outputs, and in particular gives a method to apply bloom filters for ranking tasks.\n\nSome questions regarding the experiments:\n\n1) As far as I understood the \"dimensionality ratio\" is the compression applied to both inputs and outputs, as far as this is applicable (e.g., I suppose input compression is not performed for PTB). In case compression was applied to both (e.g., recommendation), did you try different compression ratios in the input ?\n\n2) regarding the use of bloom filters for input encoding: bloom filters do not encode counts of objets, but some extensions do. In particular for recommendation tasks with count data, to what extent the suppression of the counts affects performance ? \n\n3) Some Deep Learning libraries have layers specialized for sparse inputs (e.g., SparseLinear in torch, which behaves exactly like a Linear layer). When timings are reported, do they use such implementations ? In particular, the Bloom filter approach for the inputs leads to inputs that are *less* sparse (but in smaller dimension), so I think it is important on the exact implementation when reporting timings.\n\n4) When using bloom filters on the output, I suppose the baseline uses a sofmax activation layer. However, since all your evaluations are in terms of ranking and not in terms of perplexity/log-likelihood, a pairwise ranking loss could be used instead of softmax+cross-entropy, making it possible to severely subsample the negative classes during training (each epoch would essentially cost about the number of positive labels, even though this doesn't say anything about the overall training time nor on the final accuracy). Did you experiment with such losses ?The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) \"Hash Kernels for Structured Data\" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) \"Robust Bloom Filters for Large MultiLabel Classification Tasks\". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.\n\nThe main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization). More recent methods are e.g.,\n\n1) the model compression approach of https://arxiv.org/abs/1510.00149\n2) training with integer/binary weights https://arxiv.org/abs/1511.00363\n\nOverall, the advantage of the Bloom filter approach is its simplicity and its wide applicability -- it could, as well, be used in conjunction with weight quantization and other compression methods. The main merits of the paper is to present extensive experiments on how well the vanilla Bloom filter approach can perform, but overall the novelty is fairly limited.", "title": "questions on experiments and baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkwLhQrEg": {"type": "review", "replyto": "rkKCdAdgx", "review": "Thank you for this very interesting paper. While Bloom filters have already been used to encode outputs in multilabel classification, the paper makes a more general contribution by using it for inputs and outputs, and in particular gives a method to apply bloom filters for ranking tasks.\n\nSome questions regarding the experiments:\n\n1) As far as I understood the \"dimensionality ratio\" is the compression applied to both inputs and outputs, as far as this is applicable (e.g., I suppose input compression is not performed for PTB). In case compression was applied to both (e.g., recommendation), did you try different compression ratios in the input ?\n\n2) regarding the use of bloom filters for input encoding: bloom filters do not encode counts of objets, but some extensions do. In particular for recommendation tasks with count data, to what extent the suppression of the counts affects performance ? \n\n3) Some Deep Learning libraries have layers specialized for sparse inputs (e.g., SparseLinear in torch, which behaves exactly like a Linear layer). When timings are reported, do they use such implementations ? In particular, the Bloom filter approach for the inputs leads to inputs that are *less* sparse (but in smaller dimension), so I think it is important on the exact implementation when reporting timings.\n\n4) When using bloom filters on the output, I suppose the baseline uses a sofmax activation layer. However, since all your evaluations are in terms of ranking and not in terms of perplexity/log-likelihood, a pairwise ranking loss could be used instead of softmax+cross-entropy, making it possible to severely subsample the negative classes during training (each epoch would essentially cost about the number of positive labels, even though this doesn't say anything about the overall training time nor on the final accuracy). Did you experiment with such losses ?The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) \"Hash Kernels for Structured Data\" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) \"Robust Bloom Filters for Large MultiLabel Classification Tasks\". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.\n\nThe main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization). More recent methods are e.g.,\n\n1) the model compression approach of https://arxiv.org/abs/1510.00149\n2) training with integer/binary weights https://arxiv.org/abs/1511.00363\n\nOverall, the advantage of the Bloom filter approach is its simplicity and its wide applicability -- it could, as well, be used in conjunction with weight quantization and other compression methods. The main merits of the paper is to present extensive experiments on how well the vanilla Bloom filter approach can perform, but overall the novelty is fairly limited.", "title": "questions on experiments and baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJ4xT_1Xx": {"type": "review", "replyto": "rkKCdAdgx", "review": "As far as I can tell, this paper is proposing to apply multiple hashes and take the union, as opposed to taking a single hash of input or output values. While the intuition about bloom filters is nice, at the end of the day, this technique is just producing a smaller, but more dense, binary vector, is that correct? \n\nAlso, what is the computational complexity of decoding the output? The bloom filter can tell you quickly if a given element is in the set. But for producing a sparse output it looks like the suggested procedure is just to iterate over all possible outputs...which defeats the purpose of a fast sparse coding of the output.\n\nCan you discuss in more detail also how this will affect neural models that learn embeddings of the input? A straightforward application of your method would suggest that you represent each input as a sum of embeddings indexed by random hashes. Is that correct?  Is there anything further going on here?The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.  This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox.\n\nPros:\n\n- Can be applied to practically any model, either at the input or hte output.\n- Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. \n\nCons:\n\n- The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.\n- The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.\n\nThis seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental.", "title": "Can you discuss the implications for representation learning?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkYspezNl": {"type": "review", "replyto": "rkKCdAdgx", "review": "As far as I can tell, this paper is proposing to apply multiple hashes and take the union, as opposed to taking a single hash of input or output values. While the intuition about bloom filters is nice, at the end of the day, this technique is just producing a smaller, but more dense, binary vector, is that correct? \n\nAlso, what is the computational complexity of decoding the output? The bloom filter can tell you quickly if a given element is in the set. But for producing a sparse output it looks like the suggested procedure is just to iterate over all possible outputs...which defeats the purpose of a fast sparse coding of the output.\n\nCan you discuss in more detail also how this will affect neural models that learn embeddings of the input? A straightforward application of your method would suggest that you represent each input as a sum of embeddings indexed by random hashes. Is that correct?  Is there anything further going on here?The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.  This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox.\n\nPros:\n\n- Can be applied to practically any model, either at the input or hte output.\n- Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. \n\nCons:\n\n- The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.\n- The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.\n\nThis seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental.", "title": "Can you discuss the implications for representation learning?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}