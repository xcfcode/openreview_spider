{"paper": {"title": "On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach", "authors": ["Yuanhao Wang*", "Guodong Zhang*", "Jimmy Ba"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "gdzhang@cs.toronto.edu", "jba@cs.toronto.edu"], "summary": "", "abstract": "Many tasks in modern machine learning can be formulated as finding equilibria in sequential games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we propose Follow-the-Ridge (FR), a novel algorithm that provably converges to and only converges to local minimax. We show theoretically that the algorithm addresses the notorious rotational behaviour of gradient dynamics, and is compatible with preconditioning and positive momentum. Empirically, FR solves toy minimax problems and improves the convergence of GAN training compared to the recent minimax optimization algorithms. ", "keywords": ["minimax optimization", "smooth differentiable games", "local convergence", "generative adversarial networks", "optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "The submission proposes a novel solution for minimax optimization which has strong theoretical and empirical results as well as broad relevance for the community. The approach, Follow-the-Ridge, has theoretical guarantees and is compatible with preconditioning and momentum optimization strategies.\n\nThe paper is well-written and the authors engaged in a lengthy discussion with the reviewers, leading to a clearer understanding of the paper for all. The reviews all recommend acceptance. "}, "review": {"r1xED6s2YB": {"type": "review", "replyto": "Hkx7_1rKwS", "review": "Summary: This paper designs a set of dynamics for learning in games called follow-the-ridge with the goal of finding local stackelberg equilibria. The main theoretical results show that the only stable attractors of the dynamics are stackelberg equilibria. Moreover, the authors give a deterministic convergence rate for the vanilla algorithm and a convergence rate using momentum. Empirical results show the learning dynamics cancel out rotational components and drive the vector field to zero rapidly, while reaching good performance on simple GAN examples.\n\nReview: This paper focus on sequential games, which is the common formulation of GANs and a number of games in machine learning applications. From this perspective, it is natural to look at Stackelberg equilibria. In my opinion, the objective of the paper is important and relevant. The theoretical and empirical results are reasonably convincing. However, I do have some rather serious concerns about the general-sum game results and several questions regarding the relation to related work and the experiment details that need to be addressed.\n\n1. The FR dynamics in algorithm 1 are closely related to the dynamics in [1]. In particular, the Jacobian of the FR dynamics is a similarity transform of the Jacobian of the dynamics in [1]. As a result, each algorithm has the same set of stable attractors. This should probably be mentioned in the paper. Given this relation, it is not clear what the advantage of the FR dynamics are over the dynamics in [1]. Could you please discuss this?\n\n2. The gradient penalty regularization connection does not make sense in section 4.1. The optimization problem presented has an issue because the dimensions do not align in the constraint. The quantity \\nabla_x f(x, y)^T H_yy^{-1}\\nabla_x f(x, y) would not be defined if the dimensions of the players are not equal. \n\n3. In the related work it is claimed that two time-scale GDA converges only to local minimax and [2] is cited. I would avoid using this claim with respect to that paper since the statement following the main result in the paper is not right (see proposition 11 of [3] for proof). It is not clear what is meant when it is claimed that [1] can converge to non-local Stackelberg points. The dynamics in [1] only converge to local minimax points in the special case of zero-sum games.\n\n4. Since the dynamics in the paper are the closest to those in [1], it seems that the paper would be stronger by comparing with that set of dynamics. \n\n5. I found it to be quite impressive that the vector field is driven to zero in the GAN examples. Just to clear, for each algorithm when the \u2018gradient norm\u2019 is shown, does this mean the norm of the update for each norm or does it mean the individual derivative for each player. For example in FR, would it be the norm of the derivative with respect to the follower variable of the function or the norm of the update including the second order information?\n\n6. The path angle plot was interesting to see for the GAN example. The authors claim that the eigenvalues of the second order equilibria condition are non-negative. It would be nice if the authors could show the eigenvalues in the appendix and discuss how they were computed since it may be non-trivial to compute depending on the network size.\n\n7. The damping method to stabilize training is not quite clear. Could you provide more details about how this was done?\n\n\nMy primary concerns have to do with the portion of the paper considering general-sum games. I do not understand where proposition 7 and 8 come from. I am not convinced the definitions provided are necessary and sufficient conditions for Stackelberg equilibria. In [1], a differential Stackelberg equilibrium is defined. The definition in this paper does not appear to agree with the definition in [1]. The final positive definite condition in proposition 7 and 8 does not appear to be taking the total derivative 2 times when I evaluate the derivatives, so I am not sure what the quantity is. If this is not a proper set of conditions for the equilibria, then it would also mean that the dynamics do not only converge to equilibria in general-sum games. It is important that the authors clear up this concern since I do not believe Theorem 3 holds as a consequence of problems with propositions 7 and 8.\n\n[1] Fiez et al.,  \"Convergence of Learning Dynamics in Stackelberg Games\",  2019.\n[2] Heusel et al., \"GANs trained by a two time-scale update rule converge to a local Nash equilibrium\", 2017.\n[3] Mazumdar et al.,  \"On Finding Local Nash Equilibria (and only Local Nash Equilibria) in Zero-Sum Games\", 2019.\n\nPost Author Response: Thanks to the authors for the effort in discussing the paper with me. The authors made several changes to the paper in response to my comments including removing section 4.1, fixing comments about related work, including details on the damping procedure, showing experimental comparisons to [1] along with an explanation of why the dynamics in this paper may be preferred for training GANs, providing details on propositions 7 and 8 and including reference to [1], adding further assumptions on the functions, and attempting to make theorem 3 more clear. Overall, I think this paper proposes an interesting set of dynamics, several meaningful theoretical guarantees, and impressive empirical results. I would be curious to see how it performs on even more large-scale GAN problems in the future. As a result, I have changed my original score from a weak reject to a weak accept. My primary concerns with the paper regarded the general-sum convergence results and I appreciated the explanations from the authors. I am still of the opinion that theorem 3 could be stated more rigorously in the sense that the neighborhood on which the local convergence holds should be more explicit. It seems to me that the convergence result may only hold in a ball around an equilibrium in which the implicit function is well-defined and the FR dynamics will be attracted to r(x) and that this space could be arbitrarily small for some problems. Nonetheless, this result is only in the appendix, and the paper includes enough contributions beyond this to warrant acceptance. \n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "BJxZKhUnoB": {"type": "rebuttal", "replyto": "r1xED6s2YB", "comment": "On the one hand, we thank you for all detailed feedback and comments. We really appreciate your consistent dialogue with us. Your comments definitely helped make our paper stronger. On the other hand, we are a bit frustrated and upset. We believe we have addressed all your concerns so far, especially your concern about the validity of Theorem 3. It's a bit unfair to us since we cannot further participate in the reviewer discussion period. However, we won't give up and would like to give it our final best try. Hope we can reach consensus before rebuttal deadline.\n\nIn your last response, you raised concern with that fact that we used $y = r(x)$ in our proof of Theorem 3. We would clarify that in the proof of Theorem 3, we are indeed just calculating the Jacobian at a fixed point (we can therefore safely use $y = r(x)$, see the statement before the calculation of Jacobian in page 16). Theorem 3 itself only says about stability of fixed points, which is a proxy for convergence. For local convergence, we have to combine it with Proposition 4 (see the newly added Remark 2). We apologize that we didn't refer to Proposition 4 in previous version, we've updated the paper to make this connection explicit.\n\nWe stress again that the key is that we are allowed to do Taylor expansion around the fixed point (see Proposition 4). Therefore, we only need to calculate the Jacobian matrix for fixed points in Theorem 3. The combination of Theorem 3 and Proposition 4 implies local convergence (see Remark 2).\n\nWe believe our paper deserves a score of 6 or even higher given our theoretical and empirical contributions. We've resolved all your concerns so far, and we hope you can reconsider your rating.", "title": "Final BEST attempt. We updated the statement around Theorem 3."}, "BylKxgP3oH": {"type": "rebuttal", "replyto": "rkls3yVosH", "comment": "Thank you for all the comments and suggestions. \n\nIn terms of our debate around the right notion of game underlying GANs, we agree that this issue is controversial, so we will modify our paper as it's not the main focus of our paper. We have already toned down our claim in the related work section in the current revision. We were spending most of our time discussing with reviewer #1 and didn't get enough time to go through the paper to modify all those statements, and are really really sorry about that. We promise that we will update the paper according to your suggestions in the camera ready version should our paper be accepted.", "title": "Further Response"}, "S1lLhU1hsB": {"type": "rebuttal", "replyto": "SJlGh5Aoor", "comment": "Thank you for your response and valuable input over the discussion period. We believe the discussion has benefited both of us. However, we respectfully disagree with your assessment of our general-sum results. In particular, your main concern of our result is already answered above.\n\n- Regarding why Theorem 3 uses $y=r(x)$, but we can reason about dynamics (8)\n\nWe believe we have answered this question already in the paragraph \"Connecting the result of Theorem 3 to our local convergence claim\". Essentially, we can do this is because we can use the first-order Taylor series at the fixed point to characterize the behavior of the algorithm in the neighborhood of the fixed point.  Thus, we only need to compute the Jacobian at fixed points in Theorem 3. We would like to know exactly which part of this argument you find not convincing.\n\n- Regarding the statement of Theorem 3:\n\nWe believe our statement of Theorem 3 is clear. The stability of fixed points is defined in Definition 2, and our requirement of the learning rate is given explicitly in the proof. Should you find the statement of Theorem 3 vague, we would like to ask you to point out exactly what requires further clarification.\n\nWe would like to ask you to point out what exactly is wrong *in our reasoning*. We also think that stating your doubts clearly and exactly would greatly benefit the following reviewer discussion period.", "title": "The main concern has been addressed."}, "rke54KqoiH": {"type": "rebuttal", "replyto": "rylHGhdjjH", "comment": "Thank you for your presenting your arguments. Nevertheless, we are still not convinced of the need to use timescale separation. We also believe that there is no discrepancy between our statement of Theorem 3 and our claim in the main text.\n\n- An minor unstated assumption:\n\nWe have noticed that we indeed made a mistake in our results in that, to apply Proposition 4, one needs the update rule to be differentiable. This in turn requires the objective function(s) to be thrice differentiable (in Assumption 1 we stated twice differentiable). We will modify Assumption 1 in our next revision.\n\n- About the example you mentioned in the second paragraph:\n\nThe example you bring up is very interesting. However, we believe that as long as the update rule of FR is differentiable so that its Jacobian changes smoothly, we will be able to apply Proposition 4, i.e. use Taylor expansion at a equilibrium $(x^*,y^*)$. In that case, how Hessian changes becomes immaterial, as that does not affect the Jacobian of the update rule. In other words, our results will hold in your example if the cost functions are thrice differentiable.\n\n- Connecting the result of Theorem 3 to our local convergence claim:\n\nOur claim for local convergence is basically: there exists a neighborhood $U$ with $(x^*,y^*)\\in U$ such that when initialized in $U$, FR converges to $(x^*,y^*)$. We believe such convergence follows from Theorem 3 just as the non-zero-sum convergence claim follows from Theorem 1. In particular, from Theorem 3, we know that at Stackelberg equilibria, the Jacobian eigenvalues fall in $[-1,1]$. Suppose that we consider a \"strict Stackelberg equilibrium\" $(x^*,y^*)$, i.e. $\\tilde{H}_{xx}$ and $G_{yy}$ are both positive definite, then the Jacobian eigenvalues would fall in $(-1,1)$. Then, from Proposition 4 (well known result), there exists a neighborhood $U$ of $(x^*,y^*)$ such that when initialized in $U$, the following iterates will be contractions in $\\Vert\\cdot\\Vert_2$. \n\nWe kindly welcome the reviewer to point out the discrepancy between Theorem 3 and our claim of local convergence in the line of reasoning above.\n\nNote: \"Strict Stackelberg equilibrium\" is essentially the definition for differentiable Stackelberg equilibrium in [1].\n\n- About the arguments you presented about IFT:\n\nThe reasoning you give concerns best-response gradient dynamics. However, FR is fundamentally different from best-response gradient dynamics, so currently we do not see how the analogy works for FR.\n\nFirst, what FR does in general-sum games is applying a correction to best-response gradient dynamics. Recall that GDA (with best-response gradient) is problematic, but FR's correction of GDA leads to satisfactory convergence. Therefore, problems with best-response gradient dynamics (the example you mentioned) does not apply to FR at all. We hope the reviewer to read our previous response about the relationship between FR and best-response gradient dynamics. They're fundamentally different.\n\nSecond, the problem you raised is exactly one of the motivations underlying FR (see Section 4, although it is stated for zero-sum games). The correction term of FR allows the follower to move toward $r(x)$ at every iteration (finally it hits $r(x^*)$, see Figure 1), even in non-zero-sum games. As we argued in our previous responses, the correction term of FR can be thought of as a replacement of timescale separation. Though the exact dynamics of FR is different from $x_{t+1} = x_t - \\gamma_1 h_1(x_t, r(x_t))$ (as we don't require y to be on the ridge $r(x)$), but it doesn't affect the fact that our FR converges to the right solution. And that's exactly our main contribution.\n\nTo be clear, our FR dynamics is given by:\n$x_{t+1} = x_t - \\gamma_1 h_1(x_t, y_t)$\n$y_{t+1} = y_t - \\gamma_2 \\nabla_y g(x_t, y_t) + \\gamma_1 (\\nabla_{yy} g)^{-1}\\nabla_{yx}g  h_1(x_t, y_t)$\n\nTo sum up, we are still not convinced that there is any need to use two timescale update in FR for our results to hold. We kindly ask the reviewer for providing other evidences to support his/her arguments. Otherwise, our theorem 3 should be taken as correct.", "title": "Your reasoning about best-response gradient dynamics does NOT apply to FR dynamics. Please pinpoint which step of our proof is wrong or give a specific example where FR fails."}, "r1g7wOsooH": {"type": "rebuttal", "replyto": "rylHGhdjjH", "comment": "As we notice in your last response, it seems that you are still reasoning about the issues of GDA + best-response gradient dynamics. Actually, that's exactly our motivation to propose FR. FR and timescale separation are two solutions to overcome the undesirable convergence of GDA + best-response gradient dynamics (in main paragraph, it was stated in the setting of zero-sum games). That also explains why we don't need two timescale separation. We stress again that FR is sort of a better replacement of timescale separation.\n\nLet us know if you have any other questions/comments. Hope we can reach consensus soon. And if you agree that our explanation makes sense and therefore theorem 3 holds. Please update your rating as you acknowledged that our method is novel and our contributions are significant.", "title": "More clarifications. FR is sort of a replacement of timescale separation."}, "BJx9afqiiB": {"type": "rebuttal", "replyto": "SJxTI6tior", "comment": "We're working on our response now, will get back to you very soon.", "title": "Yes"}, "SJeHw7FsiH": {"type": "rebuttal", "replyto": "rylHGhdjjH", "comment": "Thanks you for the explanation. Nevertheless, we feel rather confused about your notations in IFT part. \n\n- What do $x$ and $y$ mean? The leader and the follower?\n\n- What's the definition of $f$ and $g$? Is $f$ the objective of the leader while $g$ the objective of the follower?\n\n- Why you sometimes use $h$, sometimes $h_1$ and $h_2$? What's the difference between them?\n\n- What's the definition of $r$? Is it the same as ours? Why it sometimes take two arguments and sometimes just one?\n\n- What does $D_2$ mean?\n\n- Why would you expect the update of $y_t$ (the second last equation) coincides with $x_t$ (the last equation) if they are different players?", "title": "Elaborate a bit about the notations."}, "SJlg77SioH": {"type": "rebuttal", "replyto": "B1eWY0Voir", "comment": "First, we really appreciate your further comments and clarifications. Sorry if our tone was a bit aggressive.\n\nRegarding the figure with the eigenvalues, we will update the paper once you post the response for Theorem 3. In the next version, we will include all eigenvalues. I meant to say positive semi-definite and sorry for the confusion.\n\nWe're looking forward to discussing more with you on Theorem 3.", "title": "Thank you for quick reply!"}, "SklQELDcjB": {"type": "rebuttal", "replyto": "BJlOStb5ir", "comment": "- About the contribution of this paper in zero-sum-games: On a second read we now agree that the some stochastic results in [1], namely remark 4, hold under less restrictive assumptions. However, we do not believe this undermines our contribution significantly, even in the zero-sum setting.\n\nAmong other things, on the theory side, we: 1. proposed a novel algorithm and shown local convergence result for it (Theorem 1); 2. incorporated preconditioning and momentum into the algorithm with the same convergence guarantees (Proposition 6 and Theorem 2); 3. gave explicit local convergence rates (Theorem 2). None of these are corollaries of results in [1], and they hold under arbitrary constant learning rate ratios, which is much more favorable in practice.\n\nOn the practical side, we demonstrated effectiveness of our algorithms in both simple examples and GANs. In the GAN setting in particular, we show that the gradient norm quickly diminishes to zero. In contrast, the algorithm in [1] does not achieve this in GAN training (see details below).\n\nFor these reasons, we believe the contribution of our work is significant. This is not to mention our result for non-zero-sum games (Theorem 3).\n\n- About empirical comparison of FR and algorithm in [1]: We provide empirical results in GANs comparing FR and the dynamics in [1]. Here is the link for the results (https://docs.google.com/document/d/1HQbbd3nphY9bmhAu8Mir1M5gqpo5ZnlmewrVNVNdbsM/edit?usp=sharing). Currently, we find that the algorithm in [1] doesn't perform as well as FR (cf. Fig. 4 fourth column). In particular, the discriminator's output is not constant, and gradient norm does not decrease to zero. To be noted, we cannot exclude the factor of hyperparameter tuning for now, but we have tried our best to tune their method before the rebuttal deadline.\n\n- About the timescale separation, the majority of modern GAN papers use roughly same scale learning rates for both the generator and discriminator (e.g., BigGAN [2]). In [1], the learning rate of the generator has to be infinitely smaller than that of the discriminator. In practice, it potentially leads to mode collapse issue when a much smaller learning rate is used for the generator since the discriminator becomes too good and the gradient w.r.t the generator vanishes (see WGAN [3] paper for more discussions).\n\n- About plotting the gradient norm: First, we would like to point out that points where individual gradients are 0 have to be fixed points of FR. Note that when gradients are 0, all updates of FR are zero since the preconditioners are positive definite. In fact, the set of fixed points of FR is exactly the points where individual gradients are 0 (which is argued in our proof of Theorem 1). We are confused about the reviewer's remark that \"The norm of the individual derivative does not indicate if it is at a critical point or not\".\n\nSecond, we mean to use the gradient norm of different algorithms as a measure for convergence (see e.g. Fig. 5). These algorithms use different update rules, but are all based on individual gradients. Therefore, we believe that it makes more sense to plot the norm of individual gradients for a comparison.\n\n- Regarding the description of the results in [1]: We would clarify this in our next revision.\n\n- Regarding the eigenvalues, we plot the top-20 eigenvalues (sort according to the magnitude) for the Hessian of the discriminator and the Schur compliment in Appendix E.3.\n\n\nLastly, we thank reviewer #1 for all the comments again. We hope our responses address your concerns, especially your primary concern on Theorem 3. \n\n\n\n[1] Fiez et al.,  \"Convergence of Learning Dynamics in Stackelberg Games\", https://arxiv.org/pdf/1906.01217v2.pdf\n[2] Brock et al., \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\", https://arxiv.org/pdf/1809.11096.pdf\n[3] Arjovsky et al., \"Wasserstein GAN\", https://arxiv.org/pdf/1701.07875.pdf", "title": "Further Response [2/2]"}, "SJePYIDciH": {"type": "rebuttal", "replyto": "BJlOStb5ir", "comment": "Thank you for the detailed feedback! We will try to answer your questions more clearly this time.\n\n- About Proposition 7 and 8: We believe these results are straightforward results of the definition of local Stackelberg equilibrium, which we already acknowledged to be proposed in [1] (see page 5). We've added another reference to [1] before Definition 4 for further clarity.\n\n- About the correctness of Theorem 3: The dynamics we consider is always (8), where all derivatives are evaluated at $(x_t,y_t)$, so we do not require the follower to play the best response in the algorithm. We would like to emphasize that $r(x)$ is the global solution to a non-convex problem that we never explicitly use. We would like to breakdown the proof of Theorem 3 so that the reviewer may point out which step does not hold if under the dynamics of (8).\n\nStep 1: We care about the fixed points of (8), and since the preconditioner is invertible, a fixed point of (8) necessarily satisfies $\\nabla_{y}g(x,y)=0$ and $D_x f(x,y)=0$.\n\nStep 2: At a point such that $\\nabla_{y}g(x,y)=0$ and $D_x f(x,y)=0$, the Jacobian of (8) can be calculated to be the last equation on page 16.\n\nStep 3: The eigenvalues of the Jacobian of (8) at such a point can be shown to be those of $I-\\eta_{x}\\tilde{H}_{xx}$ and $I-\\eta_{y}G_{yy}$.\n\nWe cannot see which step won't hold under our dynamics, namely Eqn (8). We kindly welcome the reviewer to point out which step is incorrect under the current dynamics. Otherwise, we should conclude that Theorem 3 is mathematically correct. If you are still concerned about Theorem 3, we are happy to discuss more before the rebuttal deadline.\n\n\n[1] Fiez et al.,  \"Convergence of Learning Dynamics in Stackelberg Games\", https://arxiv.org/pdf/1906.01217v2.pdf", "title": "Further Response [1/2]"}, "HyxYimzssH": {"type": "rebuttal", "replyto": "BJlOStb5ir", "comment": "As the reviewer is concerned with our FR algorithm in general-sum games, we'd like to further clarify the relationship between our FR, timescale separation and best-response gradient dynamics (i.e. the dynamics of [1]). Hopefully, we can reach consensus before rebuttal deadline. Again, we appreciate all you detailed comments and suggestions.\n\nFirst, we present a view that FR in general-sum games is applying an additional preconditioner to the best-response gradient dynamics so as to fix the second-order condition of local Stackelberg equilibrium.\n\nFrom the sufficient/necessary conditions of local Stackelberg equilibrium, it can be seen that using best-response gradient to update is a way to match the first-order conditions. Therefore, it can be argued that best-response gradient dynamics is in fact the counterpart of GDA in general-sum games. Comparing FR in zero-sum games and non-zero-sum games ((5) and (8)), one can see that FR in non-zero-sum games essentially applies the same preconditioning matrix to best-response gradient dynamics. When applied to GDA in zero-sum games, the preconditioner fixes the second-order condition for local minimax and eliminates the need for timescale separation (Theorem 1). What we show in Theorem 3 is that, when applied to best-response gradient dynamics, it fixes the second-order condition for local Stackelberg equilibrium. From this point of view, it is reasonable why our algorithm does not require a timescale separation.\n\nIn short, our FR algorithm adds a correction term to the follower's update (or equivalently applys an asymmetric preconditioner for the whole dynamics), which can be thought as a replacement of timescale separation rather than the best-response gradient term (which is to fix the first-order condition). We believe that our FR is better than timescale separation in the sense that 1. timescale separation can lead to slow convergence due to the use of small learning rate. By contrast, we are allowed to use large learning rates while still converge to the right solution. 2. FR in general-sum games converges exactly to local Stackelberg equilibria, whereas best-response graident dynamics with timescale separation [1] is not yet shown to have this desirable property (see Remark 3 in [1]).\n\nNext, we would like to discuss a potential reason why the best-response dynamics in [1] works for zero-sum-games (in the sense that stable fixed points are exactly local Stackelberg equilibria) without timescale separation, but not necessarily in general-sum-games. Our understanding is that, in zero-sum games, the best-response term $\\nabla_{xy}^2 g (\\nabla_{yy} g)^{-1} \\nabla_y f$ becomes $\\nabla_{xy}^2 f(\\nabla_{yy} f)^{-1} \\nabla_y f$. This provides the best-response gradient dynamics with a nice alternative explanation, namely predicting the gradient of $\\nabla_{x} f(x,r(x))$ by estimating $y-r(x)$ to be $(\\nabla_{yy}f)^{-1}\\nabla_yf$ (a Newton step). It can be seen that this alternative explanation is no longer valid for general-sum games, since $y-r(x)\\approx (\\nabla_{yy}g)^{-1}\\nabla_y g$, whereas the best-response term uses $ (\\nabla_{yy} g)^{-1} \\nabla_y f$. This coincident is also reflected in the Jacobian calculation (see (4) in [1]): the upper-right block of the Jacobian exactly cancels to zero.\n\nTo sum up, we believe the key novelty of our algorithm is orthogonal to best-response gradient dynamics. Instead, we view it as a way to match the second-order conditions for local minimax/Stackelberg equilibria by combining it with GDA or best-response gradient dynamics. In the zero-sum setting, using best-response gradient dynamics indeed works (as shown in [1]). However, if our conjectured explanation is true, this could be somewhat coincidental.", "title": "Further Clarifications and Comparisons with [1]. The reason why we don't need timescale separation to work."}, "ByxHav0yoB": {"type": "rebuttal", "replyto": "r1xED6s2YB", "comment": "Thank you for your detailed comments and feedback. We hope that our responses below adequately resolve your concerns. Although we believe the current revision does a much better job of presenting these arguments, we warmly encourage you to provide any criticisms that may help us further express these points more clearly.\n\n- Regarding the comparison with [1]:\n\nWe are aware of and has cited the work in [1], and agree that more discussion is due. Compared to the dynamics in [1] (that is, Eqn (1) without noise), we believe that FR has two advantages. First, FR has a more intuitive interpretation, as it is trying to follow the follower's best response function, i.e., the ridge. In comparison, when positioned on the ridge, the update of the dynamics in [1] coincides with gradient descent-ascent and will drift away from the ridge. Second, the guarantees of FR carries over to non-zero-sum games. We believe this is important since it is a much more general setting and has applications such as hyperparameter tuning. In comparison, the authors of [1] acknowledged that in non-zero-sum games, their algorithm can converge to a point that is not differential/local Stackelberg equilibrium (see Remark 3 [1]).\n\nWe also note that the main focus of [1] is different, which is proving convergence of gradient dynamics in the presence of noise. However, to do so, they assume that there is only one local maximum for the follower (Assumption 2), and that there is a timescale separation between the leader and the follower. Because of these two assumptions, their main results are not directly applicable to many problems including GAN training. Roughly speaking, they showed stronger results under stronger assumptions.\n\n\n- Regarding the definition of local Stackelberg equilibrium:\n\nOur definition for local Stackelberg equilibrium agrees with the definition in [1] up to edge cases. In particular, the Hessian of $\\phi(x) := f(x, r(x))$ is exactly $D^2f_1$ in Definition 4, [1]. Thus, requiring $D^2f_1$ to be positive semi-definite is almost the same as requiring $x$ to be a local minimum of $\\phi(x)$.\n\nWe apologize for stating Proposition 7 and 8 without further explanation. We have updated our paper with a detailed explanation for their derivation.\n\nIn particular, we can see that $\\nabla\\phi(x)=\\nabla_{x}f+\\nabla r(x)^T\\nabla_{y}f$, which is the same as the transpose of $Df(x,r(x))$ using the notation in [1]. It then follows that $\\nabla^2\\phi(x)=\\nabla_{x}(\\nabla_{x}f+\\nabla r(x)^T\\nabla_{y}f)+\\nabla_{y}(\\nabla_{x}f+\\nabla_{x}r(x)^T\\nabla_{y}f)\\nabla r(x)=DD f$. Substituting $\\nabla r(x)$ with $-G_{yy}^{-1}G_{yx}$ would prove Proposition 7 and 8.\n\n- Regarding the gradient penalty regularization, we removed this section as it has little connection with our main contributions.\n\n- Regarding the gradient norm, we meant the individual derivative for each player.\n\n- Regarding the detailed damping scheme, we added the details in Appendix D.4 of current revision.\n\n- Regarding the eigenvalues of the second order equilibria condition, we are able to compute the Hessian and its inverse exactly for networks we used for mixture of Gaussian experiments. The networks we used were 2-hidden-layer MLP with 64 hidden units for each layer. To be specific, we compute each row of the Hessian by multiplying the Hessian with a vector (with all entries $1$). To be noted, the Hessian-vector product can be done efficiently by doing reverse-mode autodiff (backpropgation) twice. For exact values, we notice that the eigenvalues of the generator's Hessian are all zero (which is not surprising since the discriminator outputs a flat line) while the eigenvalues of the discriminator's Hessian are all positive (as we added $L_2$ regularization). Therefore, it is easy to see that the Schur compliment $H_{xx} - H_{xy}H_{yy}^{-1}H_{yx} = - H_{xy}H_{yy}^{-1}H_{yx}$ is positive definite.\n\n- Regarding the local minimax convergence claims in other works: It is indeed our mistake to cite [2] for our claim; we have removed this citation in our revision. However, we do not believe Proposition 11 in [3] contradicts our claim. In their example, the min player moves faster than the max player, so the dynamics converge to a local maximin. By \"[1] can converge to non-local Stackelberg points\", we meant that stable limit points of [1] can be points that are not local Stackelberg, which is acknowledged by the authors of [1] in Remark 3.\n\n\n\n[1] Fiez et al.,  \"Convergence of Learning Dynamics in Stackelberg Games\",  2019.\n[2] Heusel et al., \"GANs trained by a two time-scale update rule converge to a local Nash equilibrium\", 2017.\n[3] Mazumdar et al.,  \"On Finding Local Nash Equilibria (and only Local Nash Equilibria) in Zero-Sum Games\", 2019.", "title": "Response to Reviewer #1"}, "S1gN8fC1jS": {"type": "rebuttal", "replyto": "Hkx7_1rKwS", "comment": "We thank each of the reviewers for their time and comments. \n\nWe have uploaded our first revised version of our paper which addresses main concerns of reviewer #1. Particularly, we added details of our damping scheme in CG (Appendix D.4) and detailed derivation of proposition 7 and 8. Besides, we also removed section 4.1 due to the mistake reviewer #1 spotted.", "title": "First Revision"}, "H1gvHQuxir": {"type": "rebuttal", "replyto": "r1g-eWsqKH", "comment": "Thank you for your detailed comments and kind words about our work.\n\nRegarding the claim that GANs training is a sequential game, we note that the majority of GAN papers consider GAN training as a divergence minimization problem. For example, f-GAN is minimizing f-divergence, the original GAN is minimizing JS-divergence and W-GAN is minimizing Wasserstein distance. By taking this perspective, we are implicitly modeling GAN training as a sequential game since the divergence interpretation involves solving the maximization in the inner loop. Hence minimax should be an appropriate solution concept. We will clarify this point in our next revision. \n\nA main observation of [1] is that when GANs are trained to generate good samples, the generator seems to be closer to a saddle point than a local minimum of the loss (see Figure 5 and 6). Thus the GANs are not at local Nash equilibria, but achieve good empirical performance. Since (local) Nash equilibrium is the typical solution concept for simultaneous games, we consider this an evidence against viewing GANs as simultaneous games.\n\nThe reason why we used batch size 2,000 for MNIST is that our analysis was done for noiseless setting (full-batch). To exclude the factor of subsampling noise, we used large batch training. We leave the stochastic version of our algorithm to future work as it is highly non-trivial.\n\nRegarding the comparison with consensus optimization, we measured the training steps instead of wall-clock time. In terms of wall-clock time, consensus optimization does take fewer computation at each step. But we note that is not the main focus of our work.\n\nRegarding how we invert the Hessian, we discussed the details in section 6.2.1 in our submission and we've added more details in the Appendix D.4. Specifically, we solve the linear system $\\mathbf{H}_{yy}^2 z = \\mathbf{H}_{yy} \\mathbf{H}_{yx} \\nabla_{x} f$.\n\n\n\n[1] Berard et al., \"A closer look at the optimization landscapes of generative adversarial networks\", 2019.", "title": "Response to Reviewer #2"}, "ryxdoBDeoS": {"type": "rebuttal", "replyto": "r1x_rSVkqr", "comment": "Thank you very much for your kind words about our work. It's really encouraging that you think the problem we study is important.\n\nRegarding the use of Hessian (and its inverse) in our method, we agree that it seems hard to generalize to large-scale machine learning tasks for exact Hessian computation. However, we note that conjugate gradient method (or Hessian-free method) only involves Hessian-vector product which has roughly the same computation cost as one backpropagation. Particularly, conjugate gradient has been successfully applied to a wide range of tasks such as reinforcement learning[2], image classification[3] and meta learning[4]. In the paper, we used conjugate gradient for GAN training and we've added more details in the Appendix. Lastly, we would like to note that Hessian is not necessary for standard supervised learning tasks since first-order methods like gradient descent converges to the right solution (local minima)[1]. However, it might not be the case for sequential games, we believe that the use of Hessian is necessary for problem we study, otherwise we might find a wrong solution. In terms of the approximation error of conjugate gradient and how it affects our convergence guarantees, we leave it for future work.\n\n[1] Lee et al., \"Gradient descent only converges to minimizers\", 2017\n[2] Schulman et al., \"Trust Region Policy Optimization\", 2015\n[3] Martens, \"Deep learning via Hessian-free optimization\", 2010\n[4] Rajeswaran et al., \"Meta-Learning with Implicit Gradients\", 2019", "title": "Response to Reviewer #3"}, "r1g-eWsqKH": {"type": "review", "replyto": "Hkx7_1rKwS", "review": "Summary\nThe present work proposes a new algorithm, \"Follow the Ridge\" (FR) that uses second order gradient information to iteratively find local minimax points, or Stackelberg equilibria in two player continuous games. The authors show rigorously that the only stable fixed points of their algorithm are local minimax points and that their algorithm therefore converges locally exactly to those points. They show that the resulting optimizer is compatible with heuristics like RMSProp and Momentum. They further evaluate their algorithm on polynomial toy problems and simple GANs.\n\nDecision\nI think that this is a solid paper that addresses the well-defined goal of finding an optimizer that only converges to local minimax points. This is established based on both theoretical results and numerical experiments. Since there has been a recent interest in minimax points as a possible solution concept for GANs, I believe the paper should be accepted.\n\nThe paper occasionally makes claims that the solutions of GANs should consist of local minimax points (\"We emphasize that GAN training is better viewed as a sequential game rather than the simultaneous game, since the primary goal is to learn a good generator.\"), which are not backed up by empirical results or reference to existing literature. If anything, the empirical results in this paper do not show improvement of the resulting generator (with the exception of the 1-dimensional example that has a particular rigidity since low discriminator output can easily restrict the movement of generator mass based on first order information). The right solution concept for GANs is not what the paper is about, but before publication the authors should remove these claims, identify them as speculative, or substantiate them .\n\nSuggestions for revision\n(1) In the last displayed formula on page 4 it should be the gradient w.r.t x.\n(2) Remove, substantiate, or mark as speculative the claims regarding the right notion of solution concept for GANs.\n\nQuestions to the authors\n(1) You write \" There is also empirical evidence against viewing GANs as simultaneous games (Berard et al., 2019). \". Could you please elaborate, why Berard et al. provides empirical evidence against viewing GANs as simultaneous games?\n(2) The Batch size for MNIST of 2000 is much larger than the values I have seen in other works. What is the effect of using more realistic batch sizes in training?\n(3) When measuring the speed with which consensus optimization and FR converge, shouldn't you allow consensus optimization five times as many iterations, since you are using five iterations of CG to invert the Hessians in each step?\n(4) You mention that you use CG to invert the Hessian, but the Hessian is not positive definite? Do you apply CG to the adjoint equations?", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "r1x_rSVkqr": {"type": "review", "replyto": "Hkx7_1rKwS", "review": "In this paper, the authors introduce a new optimization algorithm for minimax problems, or finding equilibria in sequential \ntwo-player zero-sum games. Such problems are common in machine learning, including generative adversarial networks or primal-dual reinforcement learning. The commonly used gradient descent-ascent algorithm, corresponding to taking a gradient step for both players (or for both variables being minimized and maximized over), does not converge, in general, to local minimax points. Moreover, it can converges to fixed points which are not local minimax. To address these issues, the authors introduce the \"follow the ridge\" algorithm for minimax optimization problems. Given a minimax problem min_x max_y f(x, y), this algorithm consists in adding a correction term to the gradient corresponding to the y variable (corresponding to the max). This term is derived from the observation that minimax optimization should follow ridges (i.e. local maximum w.r.t. to y) of the function. Ridges can be defined as the implicit functions such that the gradient w.r.t. y is equal to zero, allowing to design an update that would stay \"close\" to the ridge. The correction term corresponding to the update thus involve the inverse of the Hessian w.r.t. y. The authors prove that all the fixed points of this algorithm are minimax, and that all local minimax are fixed points of the algorithm. The proof use first and second order conditions for local minimax points, which were recently derived in a paper by Jin et al. The proposed algorithm can also be used with momentum and preconditioning, and be generalized to Stackelberg games. Finally, the authors evaluate the follow the ridge algorithm on toy low dimensional GAN problems, as well as experiments on the MNIST dataset, showing better convergence that other methods used for minimax optimization problem.\n\nThe problem studied in this paper is an important one, as it arises in multiple area of machine learning such as adversarial \ntraining or reinforcement learning. It has also received significant attention from the community in the recent years. This paper propose a simple solution, which is well motivated, to the problem as well as a proof of convergence. A limitation of the proposed method is that it uses the Hessian of the problem, probably making it hard to apply on large  scale problems that are common in deep learning. I believe that it would make the paper stronger to discuss potential ways to mitigate this issue (e.g. inspired by L-BFGS), and their impact on theoretical guarantees. (Note that the authors briefly mention using the conjugate gradient algorithm in the experimental section).\n\nOverall, the paper is well written, and easy to follow (even for non-expert like me). I believe that it does a good job at introducing the problem and existing work on which it builds, and to motivate the proposed solution. I have not checked the proofs carefully, but they seem sensible. A small weakness of the paper is the experimental section: for example, I am not sure the MNIST experiments bring much to the paper, and would have preferred more convincing experiments. However, this is mostly a theoretical paper, and I do not think this is a big concern.\n\nTo summarize, I think the paper study an important problem, proposes a sound solution and is clearly written. For these reasons, I believe that the paper should be accepted to the ICLR conference. However, as I am not an expert on this area, my recommendation is a low confidence one.\n\n\nMinor comment: I believe that at the beginning of second paragraph of section 4, \"Suppose that y_t is a local minimum of f(x_t, .)\" should be \"maximum\".", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 1}, "r1lnJfUHtr": {"type": "rebuttal", "replyto": "Hkx7_1rKwS", "comment": "We noticed a small visualization problem in Fig. 4. The KDE plots (first row) were generated by the seaborn package. The function kdeplot (https://seaborn.pydata.org/generated/seaborn.kdeplot.html) chooses a Gaussian kernel with improper bandwidth by default, so the modes in our figures look wider than they actually are. We emphasize that this does not affect our claim that FR learns the true distribution and our comparison of FR with other algorithms.", "title": "Small visualization problem in Fig. 4"}}}