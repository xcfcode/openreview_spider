{"paper": {"title": "Building effective deep neural networks one feature at a time", "authors": ["Martin Mundt", "Tobias Weis", "Kishore Konda", "Visvanathan Ramesh"], "authorids": ["mundt@fias.uni-frankfurt.de", "weis@ccc.cs.uni-frankfurt.de", "kishore.konda@insofe.edu.in", "ramesh@fias.uni-frankfurt.de"], "summary": "A bottom-up algorithm that expands CNNs starting with one feature per layer to architectures with sufficient representational capacity.", "abstract": "Successful training of convolutional neural networks is often associated with suffi-\nciently deep architectures composed of high amounts of features. These networks\ntypically rely on a variety of regularization and pruning techniques to converge\nto less redundant states. We introduce a novel bottom-up approach to expand\nrepresentations in fixed-depth architectures. These architectures start from just a\nsingle feature per layer and greedily increase width of individual layers to attain\neffective representational capacities needed for a specific task. While network\ngrowth can rely on a family of metrics, we propose a computationally efficient\nversion based on feature time evolution and demonstrate its potency in determin-\ning feature importance and a networks\u2019 effective capacity. We demonstrate how\nautomatically expanded architectures converge to similar topologies that benefit\nfrom lesser amount of parameters or improved accuracy and exhibit systematic\ncorrespondence in representational complexity with the specified task. In contrast\nto conventional design patterns with a typical monotonic increase in the amount of\nfeatures with increased depth, we observe that CNNs perform better when there is\nmore learnable parameters in intermediate, with falloffs to earlier and later layers.", "keywords": ["convolution neural networks", "architecture search", "meta-learning", "representational capacity"]}, "meta": {"decision": "Reject", "comment": "Regarding clarity, while the paper definitely needs work if it is to be resubmitted to an ML venue, different revisions would be appropriate for a physics audience. And given the above comment, any suggested changes are likely to be superfluous."}, "review": {"SkvTjWqxG": {"type": "review", "replyto": "SkffVjUaW", "review": "The authors propose an approach to dynamically adjust the feature map depth of a fully convolutional neural network. The work formulates a measure of self-resemblance, to determine when to stop increasing the feature dimensionality at each convolutional layer. The experimental section evaluates this method on MNIST, CIFAR-10/100 and a limited evaluation of ImageNet. Generally, I am a very big proponent of structure learning in neural networks. In particular, we have seen a tremendous boost in performance in going from feature engineering to feature learning, and thus can expect similar effects while learning architectures rather than manually designing them. One important work in this area is \"Self-informed neural network structure learning\" by Farley et al. that is missing from the citations. \nHowever, this work falls short of its promises.\n\n1. The title is misleading. There really isn't much discussion about the architecture of networks, but rather the dimensionality of the feature maps. These are very different concepts.\n2. Novelty of this work is also limited, as the authors acknowledge, that much of the motivation is borrowed from Hao et al., while only the expansion mechanism is now normalized to avoid rescaling issues and threshold tuning.\n3. The general approach lacks global context. All decisions about individual feature depths are made locally both temporally and spatially. In particular, expanding the feature depth at layer f at time t, may have non trivial effect on layer f-1 at time t + 1. In other words, there must be some global state-space manifold to help make decisions globally. This resembles classical dynamic programming paradigms. Local decisions aren't always globally optimal.\n4. Rather than making decision on per layer basis at each iteration, one should wait for the model to converge, and then determine what is useful and what is not.\n5. Finally, the results are NOT promising. In table 1, although the final error has reduced in most cases, it comes at the expense of increases capacity, in extreme cases as much as ~5x, and always at the increased training time, in the extreme case ~14x, An omitted citation of \"Going deeper with Convolution\" is an example, where a much smaller footprint leads to a higher performance, further underlying the importance of a smaller footprint network as stated in the abstract.\n\n", "title": "Greedy network feature depth optimization", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1gFMVoeM": {"type": "review", "replyto": "SkffVjUaW", "review": "This paper introduces a simple correlation-based metric to measure whether filters in neural networks are being used effectively, as a proxy for effective capacity. The authors then introduce a greedy algorithm that expands the different layers in a neural network until the metric indicates that additional features will end up not being used effectively.\n\nThe application of this algorithm is shown to lead to architectures that differ substantially from hand-designed models with the same number of layers: most of the parameters end up in intermediate layers, with fewer parameters in earlier and later layers. This indicates that common heuristics to divide capacity over the layers of a network are suboptimal, as they tend to put most parameters in later layers. It's also nice that simpler tasks yield smaller models (e.g. MNIST vs. CIFAR in figure 3).\n\nThe experimental section is comprehensive and the results are convincing. I especially appreciate the detailed analysis of the results (figure 3 is great). Although most experiments were conducted on the classic benchmark datasets of MNIST, CIFAR-10 and CIFAR-100, the paper also includes some promising preliminary results on ImageNet, which nicely demonstrates that the technique scales to more practical problems as well. That said, it would be nice to demonstrate that the algorithm also works for other tasks than image classification.\n\nI also like the alternative perspective compared to pruning approaches, which most research seems to have been focused on in the past. The observation that the cross-correlation of a weight vector with its initial values is a good measure for effective filter use seems obvious in retrospect, but hindsight is 20/20 and the fact is that apparently this hasn't been tried before. It is definitely surprising that a simple method like this ends up working this well.\n\nThe fact that all parameters are reinitialised whenever any layer width changes seems odd at first, but I think it is sufficiently justified. It would be nice to see some comparison experiments as well though, as the intuitive thing to do would be to just keep the existing weights as they are.\n\nOther remarks:\n\nFormula (2) seems needlessly complicated because of all the additional indices. Maybe removing some of those would make things easier to parse. It would also help to mention that it is basically just a normalised cross-correlation. This is mentioned two paragraphs down, but should probably be mentioned right before the formula is given instead.\n\npage 6, section 3.1: \"it requires convergent training of a huge architecture with lots of regularization before complexity can be introduced\", I guess this should be \"reduced\" instead of \"introduced\".", "title": "simple idea that is shown to work well in practice, preliminary ImageNet results demonstrate scalability", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJWfJzTez": {"type": "review", "replyto": "SkffVjUaW", "review": "This paper aims to address the deep learning architecture search problem via incremental addition and removal of channels in intermediate layers of the network. Experiments are carried out on small-scale datasets such as MNIST and CIFAR, as well as an exploratory run on ImageNet (AlexNet).\n\nOverall, I find the approach proposed in the paper interesting but a little bit thin in content. Essentially, one increases or decreases the number of features based on equation 2. It would be much valuable to see ablation studies to show the effectiveness of such criterion: for example, simple cases one can think of is to model (1) a data distribution of known rank, (2) simple MLP/CNN models to show the cross-layer relationships (e.g. sudden increase and decrease of the number of channels across layers will be penalized by c^l_{f^{l+1}, t}), etc.\n\nThe experimentation section uses small scale datasets and as a result, it is relatively unclear how the proposed approach will perform on real-world applications. One apparent shortcoming of such approach is that training takes much longer time, and the algorithm is not easily made parallel (the sgd steps limit the level of parallelization that can be carried out). As a result, I am not sure about the applicability of the proposed approach.", "title": "Not sure about the novelty / contribution of the paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJhvF46Xf": {"type": "rebuttal", "replyto": "SkffVjUaW", "comment": "We would like to briefly remark that there seems to have been some difficulty in posting the rebuttal as an \"official comment\" at the time. To clarify, the \"anonymous\" comments marked with \"rebuttal\" and the \"Comments for AnonReviewer3\" have been posted by this paper's authors and should be regarded as official comments.   \n\nWe again thank the reviewers for their efforts and have uploaded a revised document improving upon suggested aspects wherever possible. To give a short summary we have:\n\n* included 2 suggested valuable references into related work\n* made a minor modification to the title by omitting the word \"architectures\" and instead simply writing \"neural networks\" as reviewer 1 has kindly noted that the word and concept of architectures seems to have different interpretations in the community and thus could be misleading in the title of our work. \n* added an additional appendix section discussing increase of networks' capacities beyond the reference (addressing reviewer 1). We provide an example with loss, training and validation curves to show the non-triviality of effective capacity when regularizers are present.\n* made minor modifications to the main body to further underline the novelty to the reader and avoid miss-conceptions about concepts being borrowed by \"Hao et. al.\" or other pruning papers that are not in the scope of the expansion framework presented in this work. (addressing reviewer 1)\n* simplified equation 2 with respect to the explicit indices of the norm and the spatial dimensions. We have furthermore made corresponding changes to the description of the equation to portray the cross-correlation concept earlier. This should improve readability and understanding of the equation. (addressing reviewer 3)          \n* added a section in the appendix addressing the possibility and open questions of applying our proposed framework without the need for re-initialization. The section should further clarify why we have decided to not include a demo of such an experiment as we believe it would lead to potentially misleading results and interpretation. (addressing reviewer 3) \n* made minor modifications to wording and corrected some few typos. \n* rephrased a short part about the computational perspective of our approach to emphasize the approach's modularity and potential for parallelization with no limitations known to us beyond regular SGD optimization (addressing reviewer 2)\n\nUnfortunately we have not been able to include the request made by reviewer 2: \"data distribution of known rank and simple models to show cross-layer relationships\". We have thought long and hard about this statement and could not come to a conclusion of how to conduct such an experiment in a convincing manner. We believe that such experiments about cross-layer relationships are absolutely desirable, but still an open-challenge for deep learning in general and thus not immediate to our contribution. We have requested some clarification about the nature of such experiments and did not yet receive further explanation. Independent of the decision of acceptance of our work we would be extremely grateful if the reviewer could extend and clarify the review so that we can draw more value from it and include it in future work and improvements.  \n\nAs a last remark we would like to again point out our concern with the very harsh lack of novelty statement made by reviewer 1. The reviewer seems to believe our mechanism is \"borrowed\" from Hao et al's paper, which is concerned _only_ with pruning of already _trained_ networks, and voices correspondingly harsh feedback about the value (or lack there-of) of our bottom-up expansion approach. We are particularly concerned with the one-sided nature of statements such as \"one should wait for the model to converge, and then determine what is useful and what is not.\". Independent of whether such a statement turns out to be true, we strongly believe that exploration of alternatives (one presented here) to always training networks to full convergence before making modifications is crucial and provides necessary insights beyond \"pushing benchmark numbers\".  \n", "title": "Revision including reviewer feedback"}, "rknxzzRAb": {"type": "rebuttal", "replyto": "BJ4OYp8Tb", "comment": "Thank you for the pointer to the ICLR 2017 paper. We were presently unaware of this paper, but after taking a brief look identified it as a relevant reference. \n\nWe will go through it more thoroughly and then add it where appropriate in the related work section. \n\nBest,", "title": "Related work"}}}