{"paper": {"title": "Extensions and Limitations of the Neural GPU", "authors": ["Eric Price", "Wojciech Zaremba", "Ilya Sutskever"], "authorids": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"], "summary": "", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length.  We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size.  The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive.  We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the  precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy.\n\nIn addition, we gain insight into the Neural GPU by investigating its failure modes.  We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for  example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$.  These failure modes are reminiscent of adversarial examples.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper is clearly written, and contains original observations on the properties of the neural GPU model. These observations are an important part of research, and sharing them (and code) will help the field move forward. However, these observations do not quite add up to a coherent story, nor is a particular set of hypotheses explored in depth. So the main problem with this paper is that it doesn't fit the 'one main idea' standard format of papers, making it hard to build on this work.\n \n The other big problem with this paper is the lack of comparison to similar architectures. There is lots of intuition given about why the NGPU should work better than other architectures in some situations, but not much empirical validation of this idea."}, "review": {"SkYY9_CUg": {"type": "rebuttal", "replyto": "SyQmiKWVl", "comment": "* Misleading title, there is no extension to the Neural GPU model, just to its training strategies.\n\nWojciech: mostly agree. we have tried in the paper several extensions but they seem to work in narrow cases. These regard how to align data.\n\n* No comparisons to similar architectures (e.g. Grid LSTM, NTM, Adaptive Computation Time).\n\nWojciech: We should add such comparison.\n\n* More experiments on other tasks would be nice, it is only tested on some toy tasks.\n* No positive results, only negative results. To really understand the negative results, it would be good to know what is missing to make it work. This has not been studied further.\n\nWojciech: Our results outperform everything that was previously presented in NeuralGPU paper.\n\n* Some details remain unclear or missing, e.g. if gradient noise was used in all experiments, or the length of sequences e.g. in Figure 3.\n\nEric: Except in Figure 2, we do not use gradient noise.  [It generally did not seem to have much effect.]\n\n* Misleading number of NTM computation steps. You write O(n) but it is actually variable.\n\nWojciech: The point is that it scales linearly with amount of data (n is the length of data). E.g. NTM doesn't perform quadratic number of computation steps.\n\nIn general, thanks for comments. \n", "title": "reply"}, "HknBFORLx": {"type": "rebuttal", "replyto": "S1PY_TAze", "comment": "Eric:\n * Except in Figure 2, we do not use gradient noise.  [It generally\n   did not seem to have much effect.]\n\n * In Figure 3, we trained on up to length-20 numbers and tested on\n   length-100 numbers.\n\nWojciech:\nNone of the models like LSTM or RLNTM is able to generalize to x2 longer sequences. Same most likely holds for NTM, however it's extremely hard to replicate NTM model. This model sometimes generalizes to sequences x10 longer. ", "title": "reply"}, "HyTWRz3Ng": {"type": "review", "replyto": "ryjp1c9xg", "review": "-Overall the paper has the feel of a status update by some of the best researchers in the field. The paper is very clear, the observations are interesting, but the remarks are scattered and don't add up to a quantum of progress in the study of what can be done with the Neural GPU model.\n\nMinor remark on the use of the term RNN in Table 1: I found Table 1 confusing because several of the columns are for models that are technically RNNs, and use of RNNs for e.g. translation and word2vec highlight that RNNs can be characterized in terms of the length of their input sequence, the length of their input, and the sizes (per step) of their input, output, and working memories.\n\nBasic model question: How are inputs presented (each character 1-hot?) and outputs retrieved when there are e.g. 512 \u201cfilters\u201d in the model ?  If inputs and outputs are 1-hot encoded, and treated with the same filters as intermediate layers, then the intermediate activation functions should be interpretable as digits, and we should be able to interpret the filters as implementing a reliable e.g. multiplication-with-carry algorithm. Looking at the intermediate values may shed some light on why the usually-working models fail on e.g. the pathological cases identified in Table 3.\n\nThe preliminary experiment on input alignment is interesting in two ways: the seeds for effective use of an attentional mechanism are there, but also, it suggests that the model is not presently dealing with general expression evaluation the way a correct algorithm should.\n\nThe remarks in the abstract about improving the memory efficiency of Neural GPU seem overblown -- the paragraph at the top of page 6 describes the improvements as using tf.while_loop instead of unrolling the graph, and using swap_memory to use host memory when GPU memory runs short. These both seem like good practice, but not a remarkable improvement to the efficiency of the model, in fact it would likely slow down training and inference when memory does in fact fit in the GPU.\n\nThe point about trying many of random seeds to get convergence makes me wonder if the Neural GPU is worth its computational cost at all, when evaluated as means of learning algorithms that are already well understood (e.g. parsing and evaluating S-exprs). Consider spending all of the computational cycles that go into training one of these models (with the multiple seeds) on a traditional search through program space (e.g. sampling lisp programs or something).\n\nThe notes on the curriculum strategies employed to get the presented results were interesting to read, as an indication of the lengths to which someone might have to go to train this sort of model, but it does leave this reviewer with the impression that despite the stated extensions of the Neural GPU model it remains unclear how useful it might be to practical problems.", "title": "-", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hk0gRzhNg": {"type": "review", "replyto": "ryjp1c9xg", "review": "-Overall the paper has the feel of a status update by some of the best researchers in the field. The paper is very clear, the observations are interesting, but the remarks are scattered and don't add up to a quantum of progress in the study of what can be done with the Neural GPU model.\n\nMinor remark on the use of the term RNN in Table 1: I found Table 1 confusing because several of the columns are for models that are technically RNNs, and use of RNNs for e.g. translation and word2vec highlight that RNNs can be characterized in terms of the length of their input sequence, the length of their input, and the sizes (per step) of their input, output, and working memories.\n\nBasic model question: How are inputs presented (each character 1-hot?) and outputs retrieved when there are e.g. 512 \u201cfilters\u201d in the model ?  If inputs and outputs are 1-hot encoded, and treated with the same filters as intermediate layers, then the intermediate activation functions should be interpretable as digits, and we should be able to interpret the filters as implementing a reliable e.g. multiplication-with-carry algorithm. Looking at the intermediate values may shed some light on why the usually-working models fail on e.g. the pathological cases identified in Table 3.\n\nThe preliminary experiment on input alignment is interesting in two ways: the seeds for effective use of an attentional mechanism are there, but also, it suggests that the model is not presently dealing with general expression evaluation the way a correct algorithm should.\n\nThe remarks in the abstract about improving the memory efficiency of Neural GPU seem overblown -- the paragraph at the top of page 6 describes the improvements as using tf.while_loop instead of unrolling the graph, and using swap_memory to use host memory when GPU memory runs short. These both seem like good practice, but not a remarkable improvement to the efficiency of the model, in fact it would likely slow down training and inference when memory does in fact fit in the GPU.\n\nThe point about trying many of random seeds to get convergence makes me wonder if the Neural GPU is worth its computational cost at all, when evaluated as means of learning algorithms that are already well understood (e.g. parsing and evaluating S-exprs). Consider spending all of the computational cycles that go into training one of these models (with the multiple seeds) on a traditional search through program space (e.g. sampling lisp programs or something).\n\nThe notes on the curriculum strategies employed to get the presented results were interesting to read, as an indication of the lengths to which someone might have to go to train this sort of model, but it does leave this reviewer with the impression that despite the stated extensions of the Neural GPU model it remains unclear how useful it might be to practical problems.", "title": "-", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1PY_TAze": {"type": "review", "replyto": "ryjp1c9xg", "review": "From the title (Extensions ... of the Neural GPU) and chapter 4 (Improvements to the Neural GPU), it sounds like you extend or improve the Neural GPU model. From the text, I understand that you don't modify the Neural GPU model but you use improved training techniques and an improved implementation for memory efficiency. I think you should make that more clear in the title. So, there is no modification to the Neural GPU model?\n\nFrom Figure 2, it seems that gradient noise could slightly help. Have you used that in your remaining experiments? What exactly have you used in your remaining experiments?\n\nFor some cases, you describe the length of train data sequences and test data sequences but I haven't found that information for example for the experiments in Figure 3 and maybe others. How long was it? How much longer was test compared to train?\n\nYou state that the NTM number of steps is O(n). But I think that is actually variable, right?\n\nYou don't have any comparisons to other related models, like Grid LSTM, NTM, Adaptive Computation Time, etc?\nThe paper investigates on better training strategies for the Neural GPU models as well as studies the limitations of the model.\n\nPros:\n* Well written.\n* Many investigations.\n* Available source code.\n\nCons:\n* Misleading title, there is no extension to the Neural GPU model, just to its training strategies.\n* No comparisons to similar architectures (e.g. Grid LSTM, NTM, Adaptive Computation Time).\n* More experiments on other tasks would be nice, it is only tested on some toy tasks.\n* No positive results, only negative results. To really understand the negative results, it would be good to know what is missing to make it work. This has not been studied further.\n* Some details remain unclear or missing, e.g. if gradient noise was used in all experiments, or the length of sequences e.g. in Figure 3.\n* Misleading number of NTM computation steps. You write O(n) but it is actually variable.\n\nAfter the results from the paper, the limitations still remain unclear because it is not clear exactly why the model fails. Despite showing some examples which make it fail, it was not studied in more detail why it failed for those examples, and how you could fix the problem.\n", "title": "extensions / improvements to NeuralGPU model, test data length, NTM steps", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyQmiKWVl": {"type": "review", "replyto": "ryjp1c9xg", "review": "From the title (Extensions ... of the Neural GPU) and chapter 4 (Improvements to the Neural GPU), it sounds like you extend or improve the Neural GPU model. From the text, I understand that you don't modify the Neural GPU model but you use improved training techniques and an improved implementation for memory efficiency. I think you should make that more clear in the title. So, there is no modification to the Neural GPU model?\n\nFrom Figure 2, it seems that gradient noise could slightly help. Have you used that in your remaining experiments? What exactly have you used in your remaining experiments?\n\nFor some cases, you describe the length of train data sequences and test data sequences but I haven't found that information for example for the experiments in Figure 3 and maybe others. How long was it? How much longer was test compared to train?\n\nYou state that the NTM number of steps is O(n). But I think that is actually variable, right?\n\nYou don't have any comparisons to other related models, like Grid LSTM, NTM, Adaptive Computation Time, etc?\nThe paper investigates on better training strategies for the Neural GPU models as well as studies the limitations of the model.\n\nPros:\n* Well written.\n* Many investigations.\n* Available source code.\n\nCons:\n* Misleading title, there is no extension to the Neural GPU model, just to its training strategies.\n* No comparisons to similar architectures (e.g. Grid LSTM, NTM, Adaptive Computation Time).\n* More experiments on other tasks would be nice, it is only tested on some toy tasks.\n* No positive results, only negative results. To really understand the negative results, it would be good to know what is missing to make it work. This has not been studied further.\n* Some details remain unclear or missing, e.g. if gradient noise was used in all experiments, or the length of sequences e.g. in Figure 3.\n* Misleading number of NTM computation steps. You write O(n) but it is actually variable.\n\nAfter the results from the paper, the limitations still remain unclear because it is not clear exactly why the model fails. Despite showing some examples which make it fail, it was not studied in more detail why it failed for those examples, and how you could fix the problem.\n", "title": "extensions / improvements to NeuralGPU model, test data length, NTM steps", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}