{"paper": {"title": "Experimental Design for Overparameterized Learning with Application to Single Shot Deep Active Learning", "authors": ["Neta Shoham", "Haim Avron"], "authorids": ["~Neta_Shoham1", "~Haim_Avron4"], "summary": "", "abstract": "Abstract The impressive performance exhibited by modern machine learning models hinges on the ability to train such models on a very large amounts of labeled data. However, since access to large volumes of labeled data is often limited or expensive, it is desirable to alleviate this bottleneck by carefully curating the training set. Optimal experimental design is a well-established paradigm for selecting data point to be labeled so to maximally inform the learning process. Unfortunately, classical theory on optimal experimental design focuses on selecting examples in order to learn underparameterized (and thus, non-interpolative) models, while modern machine learning models such as deep neural networks are overparameterized, and oftentimes are trained to be interpolative. As such, classical experimental design methods are not applicable in many modern learning setups. Indeed, the predictive performance of underparameterized models tends to be variance dominated, so classical experimental design focuses on variance reduction, while the predictive performance of overparameterized models can also be, as is shown in this paper, bias dominated or of mixed nature. In this paper we propose a design strategy that is well suited for overparameterized regression and interpolation, and we demonstrate the applicability of our method in the context of deep learning by proposing a new algorithm for single shot deep active learning.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper gives a method of performing experimental design (one-round active learning) in overparameterized regression. Although the comparison with the coreset method baseline is a nice addition, the reviewers still have concerns in the following aspects:\n- The novelty compared to the classical v-optimality design is limited\n- The hyperparameter t is hard to choose in practice. This is important because each different hyperparameter setting would induce a different set of examples for label queries. \n- Computational complexity\n- It is unclear exactly how the proposed method (or other experimental design method) can mitigate double descent \n\nWe encourage the authors to take these into account in the revision. \n"}, "review": {"CQU172-Lw2K": {"type": "rebuttal", "replyto": "kyit56oPyU6", "comment": "We thank the reviewer for his constructive comments,\n\n\n(1) \u201cI couldn't find any particular technical procedures or discussions specific to the double decent phenomenon in the main text. Although the authors claimed that the proposed method can mitigate the double descent phenomena, this claim is not so particularly attractive currently because the classical OED also mitigates it in the illustration of Figure 1\u201d\n\nThe technical procedure appears after Prop 3.  We slightly revised the next in order the clarify the connection. What we try to express in Figure 1 is that while classical methods mitigate the double decent issue, they do it on the expense of not dealing well with the bias, as can be seen in Figure 1: the bias oriented regime is not handled well by classical methods. Yes, they flatten the variance dominated area, but do not gain anything in the bias dominated area.\n\n(2) It\u2019s true that 'Francis R. Bach, Active learning for misspecified generalized linear models, NeurIPS2007' also relates to the bias term. However, the bias term has a different source in their work (misspecified model) and the solution (re-waiting the loss) is fundamentally different than ours. We were aware of this work, but failed to cite it do to oversight. We now added a citation.\n\n(3) Regarding the work of 'Gu, et al., Selective Labeling via Error Bound Minimization, NeurIPS2012': we were actually not aware of it and we thank the reviewer for pointing us to it. Indeed, Gu, et al. showed that the criterion of Yu et al 2006 is a bound on both the bias and variance, however they did not show what we show in Prop 4., that  the criterion of Yu et al 2006 is actually equal to ours when lambda=t. We think that this fact is crucial and has practical meanings. We were already related to this practical meaning in Appendix D.  However, in the revised version we clarified it further, with direct reference to the work of  'Gu, et al., Selective Labeling via Error Bound Minimization, NeurIPS2012' and a new experiment that show why the  'Gu, et al./Yu et al.  it is important to understand that the bound implies t=lambda and how to use it.\n\n(4) Obviously a good setting of the parameter t requires some good model of the noise (that what we see in figure 1. actually). In sequential experimental design there is plenty of room to set both t and lambda. However, we stress that even in single shot experimental design there is setting then t=lambda as Yu et al/Gu, et al., suggest may. We made it more clear now with a new experiment on UCI data that shows that many times setting t=0 is better then setting t=lambda, and anyway is not particularly worse.\n\n (5)\nThe experiment section is now improved. There is a direct comparison with what we see as our direct competitors Yu et al. and Core Set (Sener & Savarese, 2018), We show new state of the art results on UCI classification tasks and add the comparison to Core Set to Fig 2 comparing to the single shot deep active learning. \n", "title": "Response to the issues raised by the reviewer "}, "tX9ZDwKNCZt": {"type": "rebuttal", "replyto": "JAgj6Jl0_Q9", "comment": "We thank the reviewer for his constructive comments,\n\nAs for the technical novelty - Our work reveals an important connection between our criterion and the two of what we see as the main methods in this field: Transductive Experimental Design (Yu et al. 2006) and Core Set  (Sener & Savarese, 2018) (See Prop 3,4, Appendix B).\n\n\nAs for the experiments -  We compared ourselves to Core Set and to Transductive Experimental Design. This comparison is now made clearer with a new experiment on UCI classification datasets in which we show state of the art results. Furthermore, the results of the comparison to coresets was moved from the appendix to  Figure 2.\n\n\nAs for the title \u201csingle shot active learning\u201d - In many articles, Optimal Experimental Design and Active Learning are synonyms terms, where the first one is used by the ML community and second by the statistical community. Both of them do not have to be sequential, per our understanding, and classical experimental design is definitely \u201csingle shot\u201d. We use the term \u201csingle shot active learning\u201d and not \u201csingle shot experimental design\u201d because it is more suitable for label prediction tasks (and not for parameter estimation for example). \n\nWe fixed the typo, and thank the reviewer for notifying us about it.\n\n", "title": "Response to the issues raised by the reviewer"}, "9ZDxVv2MhJS": {"type": "rebuttal", "replyto": "sZ0kXGkTwWo", "comment": "We thank the reviewer for his constructive comments. We focus on the reasons he gave for rejecting the paper.\n\n1) We first try to examine active learning in an overparameterized linear model and then see the relation of it with deep active learning. As two first results in this direction we have:\n(a) A theoretical analysis which shows that the popular core set approach for deep active learning approach might be suboptimal (see Prop 3 and Appendix B);\n(b) A novel successful single shot active learning /experimental design for deep learning (see Fig 2.)\nWe also experimentally support our conjecture that this approach can further lead to a novel sequential algorithm but we leave it to further research.\n2) We actually explored the relation of t with lambda (see Prop 4.)\n3) We have experimental results with core set. Originally they were  reported in the appendix, but we now moved them to Fig. 2.\n\nRegarding the term \"single shot active learning\":  the meaning is that there is no opportunity to alternate between improving the model and acquiring new labels, they all have to be acquired in one batch before starting to train. We will try to make it clearer in the paper. \n\nWe thank the reviewer for putting our attention to the typo in sentence (1).  It is fixed now.", "title": "Response to the concerns raised by the reviewer"}, "4hGlRRLp6mY": {"type": "rebuttal", "replyto": "7fh2Th1CgaM", "comment": "We thank the reviewer for  his constructive comments.\n\n1) \u201cAs such, the theory reported in the literature is often not applicable in the interpolative regime\u201d. The fact that classical experimental design works in the under-parameterized regime can be verified by reading classical work which we cite in the related work section. The fact that the most of the classical work is not applicable for the overparameterized regime is explained in section 2.\n\n2) \u201csometimes able to find better design than random selection\u201d - We changed it to \u201cour algorithm is able to find designs that are better than state of the art\u201d. That statement is based on comparison with our two direct competitors - Transductive Experimental Design (Yu et al.)  and Core Set (Sener & Savarese, 2018). The comparison with Core Set was originally reported in the appendix, and we did not put it in Fig 2. However, we now added it to the main text. We also added a new important experiment on UCI classification  datasets in which we compare our method with t=0 vs Yu et al. 2006 , which is a special case of our criterion (t=lambda) as we show in Prop 4.   \n\n3) We stress that reducing the number of samples from ten of thousands to hundreds is relevant for real life deep learning tasks.  For example consider sequential batch active learning. Our work is the first work to suggest solution for how to choose the first batch. \n\n", "title": "Response to the concerns raised by the reviewer"}, "D6fLEkiodQF": {"type": "rebuttal", "replyto": "rqzZDh8jqGj", "comment": "We thank the reviewers for their very enlightening comments.\n\nOur main reversions for the final version of the paper are as follows:\n\n1) We added a new experiment that shows state-of-the-art results on UCI classification datasets*.\n\nState-of-the-art in the sense that  we show better result than what we see with our direct competitor in this field: transductive experimental design (Yu et al. 2016).\n\n2) We added a comparison to core set in Figure 2.\n3) We added an important explanation for the relation of our work to Gu, et al. 2012 (end of Sec 3)\n4) We revised a little bit the contribution section, clarifying things and highlighting our contributions in light of recent experiments. \n\n", "title": "Main revisions in the final version"}, "kyit56oPyU6": {"type": "review", "replyto": "rqzZDh8jqGj", "review": "The paper proposes an active learning (AL) strategy that is based on both of variance and bias of the linear model unlike the classical variance-based criteria. The paper claims that, for over-parameterized setting, incorporating the bias information is particularly important, and further, shows a kernelized extension and a greedy calculation strategy.\n\nThe paper focuses on an important problem of active learning, and is written clearly. The approach is reasonable, and the method would be easy to implement. However, technical significance would be somewhat weak. The derived bound of the expected error is quite simple, though it is ok but an important unknown constant is kept and no detailed discussion is provided for that constant. Further, the experimental result is not convincing to show the superiority of the proposed criterion compared with other existing approaches. Detailed comments are as follows.\n\nIn the first section, the authors mentioned about the recent 'double decent' phenomenon in over-parameterized model, and related the proposed approach with it. However, I couldn't find any particular technical procedures or discussions specific to the double decent phenomenon in the main text. Although the authors claimed that the proposed method can mitigate the double descent phenomena, this claim is not so particularly attractive currently because the classical OED also mitigates it in the illustration of Figure 1. If the proposed method can mitigate double decent that classical OED cannot, it would be attractive. \n\nThe derivation of the proposed AL criterion is performed through the classical expected risk. The paper repeatedly claims importance of incorporating bias-dominated or mixed nature, unlike classical variance based approach. However, this idea (considering the bias effect in AL) itself is not novel though it is not clearly described. For example, 'Francis R. Bach, Active learning for misspecified generalized linear models, NeurIPS2007'.\n\nA similar bound analysis to the paper is also shown in 'Gu, et al., Selective Labeling via Error Bound Minimization, NeurIPS2012'. Although this paper is for a manifold based semi-supervised learning, it includes the usual linear model estimation as a special case. They derived a bound for the estimation error of the parameter that reflects both of the bias and variance terms (from which expected error can be derived). Their approach is similar to (Yu et al 2006) that the authors cited, but an important point is that they showed the criterion of (Yu et al 2006) can be seen as a bound of 'both of' bias and variance terms though it is originally shown as a variance-based. This would be important past analysis closely related to the proposed method, and should have been mentioned.\n\nObviously, hyper-parameter t would be a key factor in the proposed criterion. However, no practical discussion on how to set t is provided. In my opinion, as far as this constant remains unknown, the proposed bound is not so particularly innovative. Further the regularization parameter lambda can be problematic because unlike the usual supervised learning scenario, it sometimes should be determined before observing y.\n\nThe experiment in Section 7 is not convincing because the authors only compared with random design, and it only shows results of the bias-dominated setting of the proposed method, though the mixed nature of the expected error was repeatedly emphasized in the paper. Comparison with other state-of-the-art AL or classical OED would be indispensable.", "title": "Reasonable analysis, but technical significance is somewhat weak", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "JAgj6Jl0_Q9": {"type": "review", "replyto": "rqzZDh8jqGj", "review": "This paper studies a straightforward generalization of v-optimality from linear regression to (kernel) ridge regression. A standard greedy algorithm is used to optimize the v-optimality criterion.  A simple experiment is conducted comparing the proposed method with random sampling on MNIST. \n\nI vote for rejection. This is a simple derivation of the v-optimality of ridge regression (or Bayesian linear regression). The novelty is somewhat limited. For a paper with such limited technical novelty, the empirical studies are too thin; only one experiment is presented, comparing with only a naive random baseline. \n\nThe title \u201csingle shot active learning\u201d seems a little inappropriate. As far as I understand, the word \u201cactive\u201d in the context of active learning means the model \u201cactively\u201d query labels from some oracle in an __iterative__ fashion, so it already means \u201csequential\u201d I think. Also, it's easy for people to confuse this with \"one-shot learning\".\n\ntypo: K=VV^t \\in R^{n,n} should be {m,m}? \n", "title": "limited technical novelity, weak experiments ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "sZ0kXGkTwWo": {"type": "review", "replyto": "rqzZDh8jqGj", "review": "This authors analyzed the V-optimality criterion of the experimental design for the ridge regression setting, proposed a greedy algorithm to optimize the criterion, and connected with the infinite neural tangent kernel. \n\nThe paper contains a number of interesting results, particularly in connecting the V-optimality criterion with the bias-variance trade-off in predictive modeling. However, the problem setting has some overly strong assumptions, such as prefixed design and iid errors, which can make the solutions trivial and not very useful in practice.\n\nI don't recommend the paper to be accepted in its current form for the following reasons:\n\n1. I think the main results of the paper in Section 3-5 concerns a conventional problem: Experimental design for the ridge regression. They do not provide a solution aligned with the main motivation, bridging experimental design and deep learning.\n2. The theoretical analysis did not provide sufficient new insights. It actually confuses me that it introduces an additional parameter t on page 5, without exploring its relationship with the original regularization parameter lambda. \n3. The connection with deep active learning is not sufficiently explained. Also the evaluation task looks overly simple and uses a trivial baseline; I would expect at least a comparison with another active learning algorithm, for example, using the coreset approach.\n\nOther specific concerns:\n\n-  I find the expression \"single shot active learning\" confusing. Does it mean active learning with a prefixed (hence unsupervised) design?\n- The last sentence of Sec 2, para 1 is incorrect. Actually, it contradicts Eq 1. \n\n\nIn general, I think that the paper can be improved with a more focused contribution statement, and better connecting the results with the claimed contributions. The paper's relevance of deep active learning is unconvincing to me; probably the results are more suitable for a different conference.\n\n", "title": "Interesting but incomplete work", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "7fh2Th1CgaM": {"type": "review", "replyto": "rqzZDh8jqGj", "review": "In this paper, the authors develop a data selection scheme aimed to minimize a notion of Bayes excess risk for overparametrized linear models. The excess Bayes risk is the expected squared error between the prediction and the target. The authors note that solutions such as V-optimality exist for the underparametrized cases (linear regression), and offer extensions to ridge regression. After the development of a greedy schemes and a tentative extension to deep learning models, the authors show that their selection scheme can outperform random selection on MNIST with a specific model.\n\nActive learning, both online and batch, is a well-studied problem with real-world applications. The paper does a fair job of developing the new technique but the addition seems incremental, and the algebra even though involved, is largely straightforward. The results are also unconvincing, and the one figure in the main body (Fig 2) only shows comparison with a weak (random selection) baseline. Furthermore, in the deep learning regime, 500-800 data points are somewhat unrealistic and do not cement the value of the method in real-world scenarios. Some other concerns include:\n\n1. Unsubstantiated claims like \u201cAs such, the theory reported in the literature is often not applicable in the interpolative regime\u201d. \n\n2. Unconvincing contributions description, \u201csometimes able to mitigate the double descent phenomena\u201d, \u201csometimes able to find better design than random selection\u201d.\n\n3. The computational complexity of the method is prohibitive \u2014 O(m^2 n^2), where m is the dataset size.\n\n4. Minor formatting issues: \u201crevealing a three possible regimes\u201d, \u201can interesting connections\u201d, \u201capproach that suggest differs\u201d, \u201clarger then\u201d, \u201cexpected expected risk\u201d, etc.\n\nOverall, even though active learning is a highly relevant problem domain, in my opinion, the paper falls short in establishing itself as a strong competitor in the domain for reasons described previously. I would encourage the authors to resubmit with stronger justifications of the contribution and more convincing results.\n\n", "title": "Good development. Unconvincing results. Limited applications.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}