{"paper": {"title": "AlgebraNets", "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"]}, "meta": {"decision": "Reject", "comment": "The paper proposes to deep neural network models with elements of the weight from algebras, and considers a wide range of algebras and large scale promising experiments. The paper raised a heated discussion.\n\nPros: \n\n- Using algebras, one can hope for more efficient architectures \n\n- Numerical experiments on a wide range of problems\n\nCons:\n\n - The theoretical grounding provided in the current version of the paper is not sufficient. The study is empirical (nothing wrong about it), but there is no clear understanding/explanation of why particular choice is better than another, and also why it works in the particular setup. \n\n- The title does not reflect the content of the paper. It is too broad, and also in some sense \u201cprovocative\u201d. The reader expects something much more significant from it.\n\n- Experiment setup: the resulting flops/accuracy figure (main result, Figure 1) does not contain error bars.  I.e., the accuracies should be averaged over several random seeds in order to guarantee the resulting metrics. Also, this figure does not show a clear advantage over the ResNet-50 baseline."}, "review": {"b13emmFhpZs": {"type": "review", "replyto": "guEuB3FPcd", "review": "The paper proposes an interesting kind of networks, AlgebraNets, which is a general paradigm of replacing the commonly used real-valued algebra with other associative algebras. This paper considers C, H, M2(R) (the set of 2 \u00d7 2 real-valued matrices), M2(C), M3(R), M4(R), dual numbers, and the R3 cross product, and investigates the sparsity within AlgebraNets. \n\nThe work in the paper is interesting and this paper is generally written well. However, there are a few issues/comments with the work:\n\n1.The citation of the references in the main body of this paper is not easy to read. It will be better to replace the format \u201cauthor(s) (year)\u201d with the format \u201c(author(s), year)\u201d ;\n\n2.Some figures and tables do not appear near the discussion, for example, Figure 1 is shown on Page but it is discussed until page 5, which makes it difficult to read;\n\n3.In Figure 1, the subfigure in the second row and first column, it seems that the performance of model with H and whitening the best stable performance.  The subfigure in the second row and second column, it can be seen that the model with H  is not better than the baseline model;\n\n4.There are many inconsistencies in the format of the reference, for example,\n\n1)In some places the author's name is abbreviated, while in others it is not. References \u201cC. J. Gaudet and A. S. Maida. Deep quaternion networks. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138, 2018. \u201d and \u201cGeoffrey E. Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In ICLR, 2018. \u201d;\n\n2)In some places the conference\u2019s name is abbreviated with the link, while in others it is not. References \u201cSiddhant M. Jayakumar, Wojciech M. Czarnecki, Jacob Menick, Jonathan Schwarz, Jack Rae, Simon Osindero, Yee Whye Teh, Tim Harley, and Razvan Pascanu. Multiplicative interactions and where to find them. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rylnK6VtDH.\u201d and \u201cGeoffrey E. Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In ICLR, 2018. \u201d.\n\nPlease check carefully and correct the inconsistencies.\n\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nThe paper replaces the traditional real-valued algebra with other associative algebras and shows its parameter and FLOP efficiency.  In the beginning, \"I think it is an interesting piece of work, and it may be helpful to develop the basic structural design of neural networks. \". However, after getting the response from the author(s), I more doubt the significance of the work in this paper: although many types of models have been proposed in this paper, the improvement over the baseline models is limited. I did not lower the grade on this paper since I thought it would be interesting and important (if effective) to extend the traditional real number field to more complex algebraic structures.\n\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++", "title": "Interesting study of replacing the traditional real-valued algebra with other associative algebras", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "x_NtIWstj3y": {"type": "rebuttal", "replyto": "kYdrqZBz_b", "comment": "Because the reviewer has such strong concerns about the title, we wonder if changing the title would allow the reviewer to reconsider their opinion on the rest of the paper?", "title": "Alternative Title?"}, "kYdrqZBz_b": {"type": "review", "replyto": "guEuB3FPcd", "review": "In this paper, the authors propose the usage of complex numbers in deep neural networks. Would be good to know that complex numbers, n x n matrices, quaternions, diagonal matrices, etc. all can be used in neural networks. The authors also claims benchmark performance in large-scale image classification and language modeling.\n\nHowever, this work cannot be appreciated due to the following aspects:\n1. A first question is \"Why it is necessary?\"  Interestingly, the authors already included Section 2.1 Why Algebras?   However,  I am not convinced at all.  A good answer may take either of the two forms: A). simply a math step showing great potential behind;  2) large-scale neural networks that have engineering advantages.  It seems that the authors took the second approach, however, ImageNet is not that challenging and there may be no clear need to switch to complex numbers.  Would the authors be able to justify this?\n\n2. Then, the authors directly go to evaluations. The figures seem to show good advantages.  However, could you please justify your x,y-axis?  The reported results look high biased. As a reviewer, I have to doubt that the authors may have selectively present their results. \n\n    A good research paper on such a big topic, should give clear methodology first, right?  If the methodology is questionable, such good results may become noise to the community.\n    I would hope the authors clarify their methodology, and then present that advantages obtained in the experiments.\n\n3. As a top AI conference, I believe that we are looking for intellectual contributions.\n    This paper is working on a huge title, which is attractive. However, when I try to identify the intellectual contributions (can be theory, algorithm, engineering, applications), I am not convinced at all.  I know such a topic is not easy to handle. I would simple ask the authors to respond to a direct question: how would like the community to appreciate your work?\n\nNOTE: a lot of disputes are around \"the huge title 'AlgebraNets'\". However, I did not receive justification response from the authors. A possible reason may be the authors are not aware of how big the topic it is, and were so attractive/confident in the current experimental improvements (which is also very appreciated).\n\n", "title": "Huge title without convincing contribution", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "beqnp3pzIr": {"type": "rebuttal", "replyto": "b13emmFhpZs", "comment": "As the discussion period ends soon, we were wondering if there are any more concerns we could address to help increase your score of our work?", "title": "Anything more we can address? "}, "PEdsqqhMCw": {"type": "rebuttal", "replyto": "guEuB3FPcd", "comment": "We would like to thank the reviewers for the comments. We have updated the manuscript. We have moved the figures such that they appear closer to where they are referenced. We updated the citation style, as asked for. We also standardised the presentation of citations. In the supplement, we added a discussion of the change in activation memory. ", "title": "Updated Version"}, "9PElhsrS11z": {"type": "rebuttal", "replyto": "KHqXw_Vu8sh", "comment": "The reviewer seems to believe that using the algebras in this paper would be a large engineering challenge.   We note that it is fairly simple to implement these networks in Tensorflow or Pytorch, just as we\u2019ve shown in the appendix for JAX.  This does leave some performance on the table, but this is a natural progression for almost all new ideas. First it is demonstrated that they work, then later maximum performance implementations are created.  The initial implementations of real-valued convolutions were far from optimal as one example.\n\nIt is also odd to claim that \u201csome improvements on well-studied datasets are not enough\u201d when probably a majority of all papers accepted will do exactly this \u2014 show improvements on well-studied datasets.  Indeed, one should be skeptical of claims on poorly studied datasets, it is far harder to show gains on well studied datasets.  We strongly disagree with the implication that because \"we already have very effective neural networks\", research on _more_ effective techniques is not necessary. \n\nThe reviewer repeatedly claims that \u201cImageNet \u2026 is not convincing enough\u201d and \u201cexperiments and claims are from two tasks (on two datasets) which are not enough\u201d, but then when we ask directly for which additional tasks would be useful to include, they instead say \u201cThe concern is not \u2018What tasks would you like to see?\u2019 but why engineers need such a switch.\u201d  If the reviewer could clarify their position, the authors would find it most helpful.  And for what it\u2019s worth, we would like to clarify that we believe we have three tasks and four datasets.  Image classification, character level and word level language modeling are the tasks, and ImageNet, CIFAR-10 (appendix), enwik8 and WikiText-103 are the datasets", "title": "Response"}, "FxUQ8wGlRce": {"type": "rebuttal", "replyto": "_sCd8EyDKe", "comment": "Deep Complex Networks is an interesting paper that highlighted some of the potential of investigating these alternate algebras. However, they only investigate a single algebra (complex numbers) and do not recognize the increased compute density of algebra nor explore pruning or sparsity inducing methods that would greatly benefit from this increased compute density on modern hardware. Additionally, while their proposal is parameter efficient, it is not FLOP efficient due to the computationally expensive whitening procedure. \n\nWe test a large number of algebras and find an algebra (2x2 matrix rings) that actually work better than anything that has previously been looked at, in terms of performance per FLOP. Additionally, we show that we do not need some of the complexities discussed in earlier work exploring these algebras: specific initialisation schemes, for example, do not seem to matter as much.\n\nLastly, we find some crucial differences in terms of the efficacy of these algebras in testing at scale. Using ImageNet instead of CIFAR-10 one does not recover the same performance per-parameter. To further test this regime, we also use the more computationally efficient MobileNet. Finally, we test the most promising algebras on a variety of different domains as well.\n\nDeep Complex Networks was an exciting work but we think we have made a series of new contributions that are important to anyone interested in complex networks or other algebras.\n", "title": "We feel there are many new insights and contributions."}, "AHl56e811mE": {"type": "rebuttal", "replyto": "fNwPsjdj4Kr", "comment": "Sorry for not responding to the third point-- in Figure 1, we are showing performance per parameter (left) and performance per FLOP (right). This is important because while an algebra may be able to reduce the parameter counts, there may be an increased FLOP cost, especially due to a procedure like whitening. We note that in earlier work (Deep Complex Networks and Deep Quaternion Networks) the cost of whitening was largely ignored, since results focused on parameter efficiency. In terms of performance-per-parameter, many algebras are actually more performant than the baseline. However, in terms of FLOPs, only M_2(R) is able to match the baseline performance. It is important to note, however, that these algebras have added benefits: specifically, the higher compute density. Aside from being important for sparsity, with proper kernels (or even hardware!) the performance gap may be negligible.", "title": "Response to Item 3"}, "jwaCE4SlSJG": {"type": "rebuttal", "replyto": "b13emmFhpZs", "comment": "Thank you for your careful read of our manuscript. We appreciate the comments very much, and will update the paper with the changes to the references and try to better stagger the introduction and reference to the different figures in the manuscript. \n\nAre there technical issues we can address/clarify/improve that would help improve the perception of our work? \n", "title": "Thank you for the review"}, "EQScenKpK1t": {"type": "rebuttal", "replyto": "k3b56qlIUhz", "comment": "Thanks for the review. We have tried to address your section of cons below, and will update the text in the next few days to reflect these changes. We, of course, thank you for listing the pros of our work, we agree that the exploration of alternate algebras is both useful and impactful! \n\n> The authors motivate this work with computational efficiency; however, I did not find any discussion or comments on the total memory footprint. Do any of the algebras require us to keep track of partial computations/intermediate steps - subsequently increasing the total memory footprint? In the case of vision examples, which are dominated by the activations, what are the implications? If the memory footprint is indeed not consistent with a real-valued algebra, then are we trading model/input size for fewer parameters/efficient computation?\n\nThere are two issues that could increase the memory footprint.  The first is that regardless of implementation, the number of activations will be larger by a factor of about 1.3 (empirically) for the M2R networks when matching the performance of the real network.\n\nThe second issue is that with our current implementation, there are indeed intermediate feature maps that could increase the memory usage.  For M2R there are 8 convolutions of size C/4, which means the memory usage would approximately be doubled.  However, we note that if the appropriate kernels were written to perform the algebra calculation at the lowest level, then this doubling overhead would not exist. \n\nWe will update the text saying that this is a possible concern and point out mitigating strategies.\n\n\n> Are certain algebras more amenable to specific hardware architectures? If so, a brief discussion would enhance the paper overall.\n\n\nThe matrix algebras would map nicely to the currently popular systolic arrays common on accelerators such as GPUs and TPUs.  Although the arrays on current GPUs and TPUs are bigger than sizes considered here, it is possible that future hardware could move to smaller arrays.  Having a larger number of smaller systolic arrays would map nicely to sparse algebra networks.  It would also be possible to build specific algebra multipliers at the hardware level for any algebra.\n\nThese algebras would also accelerate inference cases that would otherwise have a batch size 1 and be completely bandwidth limited, by increasing the compute density even in this case.\n\nWe agree this is an interesting direction and will add a section to the appendix that emphasises this further.\n", "title": "Thank you for your review"}, "P9MLsJMrRQW": {"type": "rebuttal", "replyto": "kYdrqZBz_b", "comment": "Thank you for your review. Below, we\u2019ve tried to answer your questions, but firstly here is our motivation for this work, which we hope will help frame both the manuscript and our response.\na) We wanted to search for more efficient alternatives to real numbers to use in neural networks.  This was the goal from the beginning.  We were especially interested if we could combine the higher compute density of algebras with sparsity.\nb) There had been some prior work showing complex numbers and quaternions were more parameter efficient but nothing about FLOPs. FLOPs are correlated with runtime and often at least as important as parameter efficiency, especially in vision models.\nc) We noticed the prior work on complex and quaternions used very FLOP expensive whitening and special initialization.\nd) We chose to investigate those algebras and many more on both a parameter and FLOP efficiency basis. Furthermore, we made preliminary steps towards testing sparsity inducing techniques and these algebras.\n\nIn response to your specific queries: \n* Do you mean why are more efficient neural networks necessary?  Or why are different algebras necessary for more efficient networks?  They are one approach to finding more efficient architectures, but certainly not the only one.\nWe are surprised that you do not think ImageNet is not a suitable task for demonstrating this method: it is a challenging task and is widely used as a way to benchmark state-of-the-art methods. We also have results on enwik8 and wikitext-103 language modeling which, while certainly not large by GPT-3 standards, has been considered a standard language modeling benchmark in the literature.  What tasks would you like to see?\n\n* Thanks for commenting that the results look promising. We choose our axes based upon the standards in other work on efficiency in neural networks.  For example: EfficientNet (M. Tan, et al 2019) show results as FLOPs/parameters vs top-1 accuracy. Similarly, MobileNet (A.G. Howard et al 2017) and MobileNet v2 (M. Sandler et al 2018) also present the same axes. Many pruning papers also use these same axes, for example, \u201cWhat is the State of Neural Network Pruning?\u201d from D. Blalock et al 2020 and \u201cThe State of Sparsity in Deep Neural Networks\u201d from T. Gale et al 2019.  We thank the reviewer for commenting that they found some of the methodology unclear -- are there certain aspects that you found to be particularly confusing?  \n\n* We feel there are a series of important contributions in the work:\n a.) We find some complexities from prior works are not needed.  For example, we do not need special initializations for good performance. \nb.) Clear demonstration of which algebras are more efficient both in terms of parameters and FLOPs in a modern regime across multiple domains. \nc.) We discover that M_2R is better than all algebras that have been previously considered in terms of performance per FLOP while still offering a substantial parameter reduction. \nd.) Showing that M_2R networks can be made sparse and will be better than normal sparsity due to the higher compute density of the algebra.\n\nWe hope that this helps address your concerns.  Please do let us know if there is anything more we can clarify. \n", "title": "Thank you for your review"}, "k3b56qlIUhz": {"type": "review", "replyto": "guEuB3FPcd", "review": "## Summary\nThe authors propose AlgebraNets - a previously explored approach to replace real-valued algebra in deep learning models with other associative algebras that include 2x2 matrices over real and complex numbers. They provide a comprehensive overview of prior methods in this direction and motivate their work with potential for both parameter and computational efficiency, and suggest that the latter is typically overlooked in prior literature. The paper is very well-written and follows a nice narrative, and the claims are mostly backed empirically with experimental results. \n## Pros \n* Empirically justified with experiments on state-of-the-art benchmarks in both computer vision and NLP. \n* Establishes that exploring other algebras is not just an exercise for mathematical curiosity but also practical, and encourages deep learning practitioners to extend the results. \n* Perhaps the most useful aspect is that the experiments fit well into a standard deep learning framework \u2013 with conventional operations, initialization, etc. That is, the algebras do not require significant custom ops/modifications to achieve state-of-the-art results. \n* Shows multiplicative efficiency (parameter count and FLOPs) in many cases \n## Cons \n* The authors motivate this work with computational efficiency; however, I did not find any discussion or comments on the total memory footprint. Do any of the algebras require us to keep track of partial computations/intermediate steps - subsequently increasing the total memory footprint? In the case of vision examples, which are dominated by the activations, what are the implications? If the memory footprint is indeed not consistent with a real-valued algebra, then are we trading model/input size for fewer parameters/efficient computation?\n* An intuitive justification of the algebras used in these experiments, along with insight for future algebras might be a nice addition, although I wouldn't consider it a con.\n* Are certain algebras more amenable to specific hardware architectures? If so, a brief discussion would enhance the paper overall.", "title": "Impactful paper with strong empirical results", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}