{"paper": {"title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "authors": ["Xue Bin Peng", "Aviral Kumar", "Grace Zhang", "Sergey Levine"], "authorids": ["xbpeng@berkeley.edu", "aviralkumar2907@gmail.com", "grace.zhang@berkeley.edu", "svlevine@eecs.berkeley.edu"], "summary": "We represent a simple off-policy reinforcement learning algorithm that uses standard supervised learning methods as subroutines.", "abstract": "In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.", "keywords": ["reinforcement learning", "policy search", "control"]}, "meta": {"decision": "Reject", "comment": "This paper caused a lot of discussions before and after the rebuttal. The concerns are related to the novelty of this paper, which seems to be relatively limited. Since we do not have a champion among positive reviewers, and the overall score is not high enough, I cannot recommend its acceptance at this stage.\n"}, "review": {"Byl2SGsuur": {"type": "review", "replyto": "H1gdF34FvS", "review": "[Note: I wrote this review after John Schulman's first comment, before any reply, and before Gehrard Neumann's comment]\n\nThe authors propose an actor-critic algorithm based mostly on regression. Being off-policy, the algorithm can learn from multiple policies. It can also be applied to continuous as well as to discrete actions, and it can be trained in a batch RL setting. They compare it to a set of state-of-the-art algorithms in standard openAI gym continuous action benchmarks and show competitive performance despite a much simpler implementation of the algorithm.\n\nI basically subscribe to John Schulman's comment below, both about empirical results and about citing Self-Imitation Learning, but I have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature.\n\nThe paper does not cite or discuss the one below, though it looks VERY close:\n\n@inproceedings{neumann2009fitted,\n  title={Fitted Q-iteration by advantage weighted regression},\n  author={Neumann, Gerhard and Peters, Jan R},\n  booktitle={Advances in neural information processing systems},\n  pages={1177--1184},\n  year={2009}\n}\n\nThis paper also starts from RWR and performs weighted regression based on the advantage rather than on the return. So to me it is exactly the same idea, and it is mandatory that the authors clearly establish what is the novelty of their work with respect to this previous paper.\n\nLess importantly, the authors may also want to have a look at :\n\n@article{zimmer2019exploiting,\n  title={Exploiting the sign of the advantage function to learn deterministic policies in continuous domains},\n  author={Zimmer, Matthieu and Weng, Paul},\n  journal={arXiv preprint arXiv:1906.04556},\n  year={2019}\n}\n\nwhich also uses ideas along the same line.\n\nTo me, a good way to improve the novelty of this work would be to perform a detailed empirical study of the inner mechanisms of the algorithm on very simple benchmarks where the value function and policy could be visualized. In particular, how stable is the estimation of the value function? This is known to be an issue, as most algorithms avoid approximating it and prefer estimating the Q-function.\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "Bkx4jhqnjH": {"type": "rebuttal", "replyto": "H1gdF34FvS", "comment": "We thank the reviewers for their constructive feedback. We have revised the paper to include additional discussion of prior work and quantitative results. We summarize the main updates below:\n\n1) comparisons with Neumann & Peters (https://imgur.com/a/YjfMQS9)\n\n2) additional discussion of prior work in Section 4, including FQI by AWR, SIL, and MARWIL\n\n3) a more detailed review RWR and EM methods in Section 2\n\n4) additional ablation experiments in Appendix E\n\n5) additional PPO experiments based on John Schulman\u2019s updated PPO implementation (https://imgur.com/a/ny2rqFd) \n\n6) discussion of differences between policy gradient and AWR update in Appendix D \n\n7) experiments with 10 random seeds instead of 5 (https://imgur.com/a/yLntOzf)\n", "title": "Author Response: Update Summary"}, "SJlNOIGiir": {"type": "rebuttal", "replyto": "Hkxkf1xssS", "comment": "We used the public implementation of TD3 available in RLKit: https://github.com/vitchyr/rlkit. There is a large variation in the performance of TD3 across different runs. As we have discussed in the other posts, methods that directly use a Q-function to update the policy are more susceptible to bias in the learned Q-function, which could be a factor in the performance of TD3 that we observe. Please note that the performance of TD3 does not collapse on Walker2D. In fact, it achieves a respectable score of 4212.\n\nSome of the TD3 runs were terminated earlier than others due to the slow wall-clock time of the TD3 code. We will update TD3 learning curves with longer runs once we have the time to train the policies for more time.\n\nWe would like to emphasize again, that the goal of our experiments is not to show whether one algorithm is necessarily better than another. It is to show that a simple off-policy RL algorithm that uses supervised learning as subroutines can be competitive with current state-of-the-art techniques.", "title": "TD3"}, "H1l6B0MKiB": {"type": "rebuttal", "replyto": "H1gdF34FvS", "comment": "In regard to novelty, one thing we would like to note is that the combination of principled theoretical backing and solid empirical results that show a generally known recipe (in this case RWR), with a few careful design choices not present in prior work, can result in an effective deep RL algorithm is very much in line with widely recognized previously published papers. The same criticism in regard to novelty could have been applied to TRPO \u2014 which extended the theoretical backing of natural policy gradient and showed that it worked in the deep RL setting. Also PPO, which implementation wise is a small and simple modification on the very well known importance sampling policy gradient. The same for DDPG, which is a small but critical modification on NFQCA. By the novelty standard in the reviews, none of these prior papers \u2014 now viewed as landmark papers in deep RL \u2014 would have passed the bar. We believe that demonstrating, for the first time, that an RWR-like recipe produces a competitive deep RL algorithm with simple subroutines for value and policy fitting, together with theoretical motivation, is a contribution of similar scope in the context of prior work", "title": "Regarding novelty"}, "BklEPf_dsH": {"type": "rebuttal", "replyto": "ByeI-RU_sS", "comment": "Ok, seems like we are talking about the same equation. The equation in 3.1 should be correct. We adjusted the notation to be consistent with the notation we use in this paper, but it is saying the same thing as the equation in Neumann & Peters. For your convenience, we have included a side-by-side comparison of the three equations here:\nhttps://imgur.com/a/oEAqBhP\nNote that the new equation is different from our Equation 8. It doesn't not include the likelihood of an action under \\mu. It is however semantically the same as equation 11 from Neumann & Peters, with only some notation differences. We replaced the normalization constant in the denominator with Z(s), the temperature was changed from \\tau to \\beta, and we also expanded the definition of the advantage A(s, a) = R_{s, a} - V(s).", "title": "The equation should be correct"}, "ByeI-RU_sS": {"type": "rebuttal", "replyto": "S1xbcZkOsr", "comment": "The equation in 3.1 for the definition of the policy from Neumann & Peters should be correct. Notice that the likelihood of an action depends only on its advantage and the likelihood under the sampling distribution is not present. Are you referring to some other mix-up?", "title": "Neumann & Peters equation"}, "HygmpBpPoB": {"type": "rebuttal", "replyto": "Byl2SGsuur", "comment": "Thank you again for the detailed feedback. Here is a summary of the changes we have made to the latest draft of the submission:\n\n1) We have performed additional experiments comparing our methods to FQI-by-AWR and results are available here https://openreview.net/forum?id=H1gdF34FvS&noteId=H1xClC-Lsr. These experiments will be added to the paper once we have more time to run additional random seeds.\n\n2) A more thorough discussion of additional prior work has been added to Section 4. \n\n3) We have expanded the preliminary section (Section 2) to include a more detailed review of RWR and EM algorithms, as well as a summary of their differences as compared to policy gradient algorithms.\n\n4) We have added a discussion of the differences between our definition of the policy and the policy in FQI-by-AWR to Section 3.1.\n\n5) We have included a more in-depth discussion of the differences between the AWR update and the standard policy gradient update to the supplementary material (Section D).\n\n6) The additional ablation experiments for weight clipping that you requested for have been added to Section E.1 in the supplementary material.\n\nWe hope these changes and experiments have helped to address your concerns. When you get a chance to, please update your review to reflect these changes, and let us know if you have any additional feedback.", "title": "Updates to the submission"}, "H1xClC-Lsr": {"type": "rebuttal", "replyto": "H1gdF34FvS", "comment": "We have implemented the FQI-by-AWR algorithm as described in Algorithm 1 from Neumann & Peters (https://papers.nips.cc/paper/3501-fitted-q-iteration-by-advantage-weighted-regression.pdf), but instead of kernel-based function approximators, we used neural networks, like in our method, to ensure a fair comparison -- we believe that this likely makes the method stronger. A link to the code is available here:\nhttps://drive.google.com/file/d/1vZdYSm84tdqMy1etJ7rtPGdgho3Zn1Jq/view?usp=sharing\n\nLearning curves comparing the two methods are available here:\nhttps://imgur.com/a/YjfMQS9\nSo far, our AWR algorithm performs substantially better than FQI-by-AWR on the tasks considered, despite also been much simpler. To the best of our knowledge, we are not aware of any prior work that shows the algorithm from Neumann & Peters to be effective for deep RL on this suite of tasks. \n\nHere is a highlight of the differences between their methods and ours:\n1) FQI-by-AWR performs fitted Q-iteration by learning both a Q-function and a value function. Our method requires only learning a value function.\n\n2) FQI-by-AWR updates the policy by computing advantages using the learned Q-function and value function, whereas AWR simply uses monte-carlo returns (+ TD lambda) and a learned value function to update the policy. FQI-by-AWR\u2019s use of the Q-function for policy updates could explain some of the instability observed in our experiments, since it is more susceptible to bias in the learned Q-function. This instability is commonly observed in other Q-learning based algorithms and often requires a myriad of stabilization techniques (https://arxiv.org/pdf/1710.02298.pdf, https://arxiv.org/pdf/1802.09477.pdf), none of which are needed for AWR.\n\n3) In addition to fitting a Q-function and a value function, FQI-by-AWR also requires fitting estimators for the mean and standard deviation of the advantage at each state, which is not needed for our method.\n\n4) FQI-by-AWR fits the value function using a td update with 1-step bootstrapping, while AWR uses either simple Monte Carlo returns or an off-the-shelf TD-lambda estimator.\n\nWe will continue tuning the parameters of our implementation of FQI-by-AWR and run the algorithm with more random seeds. The paper will be updated with the additional comparisons. We emphasize that we made a best-faith effort to implement this method, and were able to get it to achieve non-trivial performance. We hope the insights in our work for producing a simple but effective off-policy RL algorithm will be of interest to the community.\n", "title": "Neumann & Peters  Comparison"}, "ByxfG-fUoS": {"type": "rebuttal", "replyto": "HyxTjQo6FH", "comment": "We have conducted additional experiments comparing AWR to the method from Neumann & Peters. Please refer to the general post for details:\nhttps://openreview.net/forum?id=H1gdF34FvS&noteId=H1xClC-Lsr", "title": "Neumann & Peters comparison"}, "B1ely-G8sH": {"type": "rebuttal", "replyto": "rylVAPJCFH", "comment": "We have conducted additional experiments comparing AWR to the method from Neumann & Peters. Please refer to the general post for details:\nhttps://openreview.net/forum?id=H1gdF34FvS&noteId=H1xClC-Lsr", "title": "Additional experiments"}, "rJxlseGLsr": {"type": "rebuttal", "replyto": "Byl2SGsuur", "comment": "We have conducted additional experiments comparing AWR to the method from Neumann & Peters. Please refer to the general post for details:\nhttps://openreview.net/forum?id=H1gdF34FvS&noteId=H1xClC-Lsr", "title": "Neumann & Peters comparison"}, "Ske7cbjNsH": {"type": "rebuttal", "replyto": "rkxEYyY7sS", "comment": "We really appreciate the detailed feedback. We will adjust the writing to incorporate your suggestions. We will do our best to improve the clarity of our presentation and let you know once the new revision is ready.", "title": "Thank you for the suggestion"}, "rkgHb-o4jB": {"type": "rebuttal", "replyto": "SJgoLpzbsS", "comment": "We are added additional ablation experiments comparing AWR with and without weight clipping. Results are available in the supplementary section D.1. To summarize, here are learning curves comparing the different policies:\nhttps://imgur.com/a/FOsC0Cg\nPolicies trained without weight clipping are substantially more unstable, exhibiting drastic fluctuations in performance as a result of exploding gradients from excessively\nlarge weights. Some training runs without clipping are terminated early due to exploding\ngradients causing the networks to output NaNs. These experiments suggest that weight clipping is vital for ensuring stable training with AWR.", "title": "Weight Clipping"}, "rkxEYyY7sS": {"type": "rebuttal", "replyto": "SJgoLpzbsS", "comment": "Re: Prior work\nWe appreciate the pointers to MARWIL and FQI by AWR, and we have included a discussion of this work in the latest revision of the paper (Section 4). We will also perform additional experiments that compare AWR directly with FQI by AWR. We would like to emphasize that FQI by AWR as a method uses a kernel-based Q-function estimator to perform fitted Q-iteration. MARWIL is arguably more related to our method in terms of the form of the update, but addresses a different problem setting -- imitation learning rather than reinforcement learning from scratch.\n\nRe: Improving exposition\nThank you for the suggestions, we would be happy to revise the presentation to be more didactic. For clarification, could you elaborate on what precisely you have in mind for improving the presentation of the method? We believe that our exposition is already more formal and rigorous than prior work in terms of justifying why we should expect off-policy AWR to perform well. We describe how the method implements a type of trust region (Section 3.1) and how the baseline that give rise to advantages emerges as a natural consequence of the conservative policy improvement objective in Equation 8. We believe that this derivation, which is not presented in Neumann & Peters, contributes substantially in terms of elucidating the reason such a method should work well as an RL algorithm.\n\nRe: difference to PG\nWhile the AWR update appears similar to a PG update, there are subtle but important differences between the two. In terms of derivation, AWR is not a policy gradient algorithm, it is an EM algorithm (http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/ICML2007-Peters_4493[0].pdf). PG methods update a policy by directly estimating the gradient of the expected return with respect to the policy parameters. But with EM algorithms, they first construct an estimate of the optimal action distribution at each state, and then project that action distribution onto the space of parameterized policies. Algorithmically, the two vital differences are: \n1) in AWR the log probability of an action is weighted by \u201cexp(adv)\u201c, while in PG the log probability is weighted by just \u201cadv\u201d without the exponential. Since exp(adv) is non-negative, the objective used in the AWR update is a maximum likelihood objective that tries to maximize the likelihood of all actions, but to varying amounts depending on exp(adv). In the case of PG, the advantage can be both positive and negative, therefore PG updates will try to decrease the likelihood of actions with a negative advantage and thus it is not a conventional maximum likelihood objective. In practice, negative TD updates are often the source of instability when applying PG to off-policy data.\n2) policy gradient methods differentiate through the sampling distribution, while AWR can in principle use any off-policy data without the need to differentiate through the sampling distribution (as shown in equation 10).\n\nre: weight clipping\nYes, we will perform additional ablation experiments to show the impact of weight clipping. We will update you with the results once those are ready.", "title": "Response to R3"}, "r1gTvqFziS": {"type": "rebuttal", "replyto": "Byl2SGsuur", "comment": "Re: Fitted Q-iteration by Advantage Weighted Regression\nThank you for the pointer, We have revised the paper to include a discussion of \u201cFitted Q-iteration by Advantage Weighted Regression\u201d [Neumann & Peters] at the end of Section 4 (highlighted in red). Neumann & Peters proposed a kernel-based fitted Q-iteration (FQI) algorithm. Though their method also uses exponentiated advantages as weights, their definition of the policy is different from our formulation:\nhttps://imgur.com/a/0DuRH7o\nThe key difference is that in our method, the likelihood of an action is determined by both the likelihood of the sampling policy \\mu and the exponentiated advantage, while the policy in Neumann & Peters depends only on the advantage. Therefore, the policy update from Neumann & Peters does not enforce a trust region penalty that ensures the new policy is similar to the sampling policy, which is crucial for obtaining a good estimate of the objective using off-policy data collected from the sampling policy. Our method is simpler, does not perform fitted Q-iteration, and incorporates experience for off-policy learning. Furthermore, we provide a principled derivation of the advantage weights from a conservative policy improvement perspective. We further extend this analysis to AWR with experience replay and demonstrate its effectiveness for batch RL, both of which were not presented in Neumann & Peters.\n\nRe: Self-Imitation\nSIL as described in https://arxiv.org/abs/1806.05635, augments policy gradient algorithms with an auxiliary behaviour cloning loss to reuse samples from past experiences. In contrast to SIL, AWR is a standalone algorithm, and does not need to be combined with an auxiliary RL algorithm. Also note that the weights in the self-imitation loss is given by max(adv, 0) rather than exp(adv).\n\nRe: PPO results\nWe will update the PPO results with the data from John\u2019s modified PPO implementation. We would like to point out that the original PPO results in our paper uses the standard implementation from OpenAI baselines, and the performance matches those reported in the original PPO paper and subsequent work that compares to PPO. As shown in our experiments, the difference between the deterministic and stochastic policies is fairly minor:\nhttps://imgur.com/a/mTOOZyc\nand most of the performance improvements are due to the other modifications (action squashing, reward scaling, etc\u2026), which are not included in the standard PPO implementation:\nhttps://imgur.com/a/ny2rqFd", "title": "Initial response to R3"}, "SylWvtKMsB": {"type": "rebuttal", "replyto": "rylVAPJCFH", "comment": "Thank you for the feedback, we will improve the writing to more clearly articulate the contribution of this work and include a more thorough discussion to contrast our method with related techniques. We have revised the paper to include a discussion of these prior works at the end of Section 4 (highlighted in red). We will also perform experiments that directly compare AWR with REPS and FQI by AWR [Neumann & Peters].\n\nWhile exponentiated-advantage weights have been used in a number of prior work, we present a principled derivation of AWR from a conservative policy improvement perspective, and also provide an analysis of AWR when combined with experience replay. We show that a number of simple design decisions, such as the use of TD-lambda and experience replay, enables AWR to achieve competitive performance with state-of-the-art algorithms both for RL and batch RL settings.", "title": "Initial response to R2"}, "SkePE_FzjS": {"type": "rebuttal", "replyto": "HyxTjQo6FH", "comment": "Initial response to R1\nThank you for your feedback, we will aim to run the following additional experiments to address your questions:\n1) We will perform experiments that directly compare AWR with REPS and FQI by AWR [Neumann & Peters].\n2) We will include additional tasks with discrete action spaces such as Cartpole-v1, which is more commonly used.\nPlease let us know if there are any additional experiments that you would find helpful.\n\nRe: Fitted Q-iteration by Advantage Weighted Regression\nThank you for the pointer, We have revised the paper to include a discussion of \u201cFitted Q-iteration by Advantage Weighted Regression\u201d [Neumann & Peters] in Section 4 (highlighted in red). Neumann & Peters proposed a kernel-based fitted Q-iteration algorithm. Though their method also uses exponentiated advantages as weights, their definition of the policy is different from ours:\nhttps://imgur.com/a/0DuRH7o\nThe key difference is that in our method, the likelihood of an action is determined by both the likelihood of the sampling policy \\mu and the exponentiated advantage, while the policy in Neumann & Peters depends only on the advantage. Therefore, the policy update from Neumann & Peters does not enforce a trust region penalty that ensures the new policy is similar to the sampling policy, which is crucial for obtaining a good estimate of the objective using off-policy data collected from the sampling policy. Our method is simpler, does not perform fitted Q-iteration, and incorporates experience for off-policy learning. Furthermore, we provide a principled derivation of the advantage weights from a conservative policy improvement perspective. We further extend this analysis to AWR with experience replay and demonstrate its effectiveness for batch RL, both of which were not presented in these prior works. \"Model-Free Preference-Based Reinforcement Learning\" uses REPS as its learning algorithm, and a discussion of REPS is available in Section 4. We will perform additional experiments to directly compare AWR with REPS and FQI by AWR.\n\nRe: Not outperforming previous methods\nWhile AWR does not outperform previous methods on all tasks, which we acknowledge in the paper, we would like to emphasize that our goal is to show that a simple off-policy method that uses supervised regression as subroutines can in fact be competitive with a number of current state-of-the-art algorithms. We believe that simple and effective RL algorithms are of interest to the ICLR community, and beating all state-of-the-art methods is not a prerequisite. We also demonstrate AWR\u2019s effectiveness in the fully off-policy batch RL setting, where previous methods such DDPG and SAC perform poorly. Furthermore, AWR does not require the additional complexity of batch RL methods such as BCQ and BEAR.\n\nRe: Benchmarks\nWe will also include additional tasks with discrete action spaces such as Cartpole-v1, which is a more commonly used benchmark. Note, our focus is primarily on continuous control tasks, and the addition of the discrete tasks is mainly to show that AWR can also be easily applied to discrete actions. The motivation for the tasks in section 5.3 is to show that AWR is also effective for controlling complex agents with larger numbers of degrees-of-freedom. Since the agents in standard benchmark tasks are relatively simple, we opt to use new environments to better demonstrate this capability.\n\nRe: additional random seeds\nWe have ran additional experiments for AWR with 10 different seeds. Here are learning curves comparing the results with 5 seeds and 10 seeds. The results are similar. We will update the paper to include results with 10 seeds.\nhttps://imgur.com/a/yLntOzf\n\nRe: Q1 Importance sampling in Equation 5\nImportance sampling (IS) is not required in Equation 5. The objective itself does not require IS. For policy gradient methods like TRPO and PPO, IS is used to estimate the policy gradient from samples. In the AWR formulation, the solution of the Lagrangian in Equation 8 yields an update that does not require importance sampling, but it does require that \\mu and \\pi are similar, which is enforced by the trust region constraint (Equation 6).\n\nRe: Q2 Optimize w and uniform d(s)\nThe algorithm does not optimize with respect to w. w_i represents the probability of selecting samples from a particular policy \\pi_i from the replay buffer, which is a constant. d(s) is also not a uniform distribution over states, it represents the state-marginal of the sampling distribution. In our implementation, we sample uniformly from the replay buffer, which does not result in a uniform d(s), since some states might be visited more frequently than others.\n\nRe: code\nThis is the same code used in the experiments. We do not manually assign random seeds to each run, instead we use python\u2019s default random seed initialization, which assigns a different random seed to each execution of the problem. \u201caction_std\u201d specifies the standard deviation of the gaussian action distribution.", "title": "Initial response to R1"}, "S1x-voXMiS": {"type": "rebuttal", "replyto": "B1eG4Bzlir", "comment": "re: tuning temperature\nThe temperature does require some tuning, but more automated methods can be used, such as dual gradient descent. However, this strategy would require manually specifying an upperbound on the trust region constraint, which becomes another hyperparameter that requires tuning, which can also vary across different tasks. Therefore, we opt for a simpler strategy and used a fixed temperature.\n\nre: MARWIL\nThank you for the pointer to MARWIL, indeed the method does appear similar to AWR, but their method was demonstrated only for imitation learning, while we include both RL and batch RL tasks, and compare our method to a number of state-of-the-art RL and batch RL algorithms. We also provide an analysis of AWR when combined with experience replay, where the sampling distribution is modeled by a trajectory-level mixture of rollouts collected from different policies, which was not presented in MARWIL. Though our design decisions such as TD-lambda and experience replay are fairly simple, we show that they are important components for an effective RL algorithm.  These consideration may not be as important when dealing only with imitation learning.", "title": "MARWIL"}, "SJgoLpzbsS": {"type": "rebuttal", "replyto": "Syl_SLxxsr", "comment": "Thank you for the clarification. Yes, off-policy methods that use only a Q-function to update the policy (e.g. DDPG and SAC) can be more susceptible to model bias in the Q-function, which can lead to instability (https://papers.nips.cc/paper/3964-double-q-learning, https://arxiv.org/abs/1802.09477). For AWR, the policy is  updated using a combination of monte-carlo returns and a value function, where the value function is used primarily as a baseline. This enables AWR to be less susceptible to inaccurate value estimates, compared to methods that use only a value/Q function for policy updates. This can be seen in the experiments from Figure 4. The \"No Baseline\" policies can be interpreted as using a highly inaccurate value function that just returns zeros for all states. This does lead to some deterioration in performance, but AWR is still able to learn reasonably effective policies. If such an inaccurate value function is used for methods such as DDPG or SAC, the policy will fail to learn anything. Therefore, these results do seem to suggest that AWR can be less susceptible to inaccuracies in the value function.", "title": "value function"}, "B1lG-o6yiB": {"type": "rebuttal", "replyto": "Byl2SGsuur", "comment": "Thank you for the feedback, we will aim to conduct additional experiments to address your questions. But first, could you clarify what you mean by the stability of the value function? Value functions are very commonly used for actor-critic algorithms, such as policy gradient methods, while Q-functions are more commonly used for off-policy methods. To the best of our knowledge, neither one is necessarily easier to learn than the other.", "title": "clarification"}, "HyxTjQo6FH": {"type": "review", "replyto": "H1gdF34FvS", "review": "This paper proposes an off-policy reinforcement learning method in which the model parameters have been updated using the regression style loss function. Specifically, this method uses two regression update steps: one update value function and another one update policy using weighted regression. To compare the proposed method with others [main comprasion], 6 MuJoCo tasks are used for continuous control and LunarLander-v2 for discrete space. \n\n-- Even though this paper has done a good job in terms of running different experiments, the selection of some of the benchmarks seems arbitrary. For example, for discrete action space, this paper uses LunarLander which is rarely used in any papers so it makes very difficult to draw a conclusion based on these results. Common 49 Atari-2600 games should have been used for comparison. The same thing about experiments in section 5.3 is true too as those tasks are not that well-known. \n\n-- The proposed method doesn't outperform previous off-policy methods on Mujoco task (Table 1). Since the main claim of this paper is a new off-policy method, outperforming the previous off-policy methods is a fair game. The current results are not convincing enough.\n\n-- There are significant overlaps between this paper, \"Fitted Q-iteration by Advantage Weighted Regression\", \"Model-Free Preference-Based Reinforcement Learning \", and \"Reinforcement learning by reward-weighted regression for operational space control\" which makes the contribution of this paper very incremental.\n\n-- The authors used only 5 seeds to run Mujoco experiments. Given the sensitivity of Mujoco for different starting points, the experiments should have been run at least with 10 different seeds.\n\nQuestions:\n1) Shouldn't be an importance sampling ratio between \\pi and \\mu in the equations? starting from eq.5. \n2) Does the algorithm optimize the respect to $w$ as well? (eq. 15) if yes, why it is not mentioned in algorithm 1? Plus, since $d(s)$ is uniform dist. (at least this is assumed for implementation), eq. 14,15,and 16 (wherever there is d(s)), those can be simplified, e.g. \\hat{V} = \\sum(w_i V_i), wouldn't be better just introduced simplified version rather than current ones? (referring to the only equation above section 3.3)\n3) Is this the same code used to report results in this paper? if yes, I didn't see any seed assignment in the code?! and what is \"action_std\" in the code?\n\nThere are a couple of recent works in merging on-policy with off-policy updates which you might want to cite them. \n\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "rylVAPJCFH": {"type": "review", "replyto": "H1gdF34FvS", "review": "\n# Summary\nThe paper shows that good old reward weighted regression (RWR) with value-function baseline is still state-of-the-art algorithm.\n\n# Decision\nThe paper is well-written and provides many evaluations. The contribution should be articulated more carefully, though, taking into account that most algorithmic ideas are present in prior work (https://openreview.net/forum?id=H1gdF34FvS&noteId=Bkxi11nsdr). Perhaps, the experience replay part is somewhat novel. It seems emphasizing more that the aim is to show that simple methods are competitive rather than focusing on novelty could be a good idea.\n\nProvided that the authors incorporate the feedback of the other reviewers and update the paper accordingly, it will make a good contribution.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "H1lFeZYDKB": {"type": "rebuttal", "replyto": "SJx0Wp89uH", "comment": "Thanks again for the suggestion. We have tried applying observation normalization to TD3 for the humanoid. Here's a performance comparison when training with and without normalization:\nhttps://imgur.com/a/ss7baug\nFor this task, it doesn't seem to lead to a noticeable improvement in performance.", "title": "observation normalization"}, "rkluHDAXYS": {"type": "rebuttal", "replyto": "rkxEQg_9uB", "comment": "After scaling the returns, they seem to be similar to the plots you posted. We have conducted some ablation experiments on the modifications that were made to PPO. The overall conclusion appears to be that the difference between deterministic vs stochastic evaluation is fairly small for these tasks. Most of the performance improvements are a result of the other modifications. Here're some comparisons of the different modifications:\nhttps://imgur.com/a/ny2rqFd\n\n\"Baselines\": the original PPO code from OpenAI Baselines that we included in the paper (ppo2), which matches the performance in the original PPO paper.\n\"Modified\": uses the modified hyperparameters from your script (e.g. larger batch size, as well as using ppo1 instead of ppo2).\n\"R Scale\": applying reward scaling.\n\"A Squash\": applying action squashing.\n\nReward scaling and action squashing seems to be responsible for most of the improvements. AWR still shows comparable performance on most of the tasks. We will update the paper to include these new results with the modified PPO implementation.", "title": "new experiments"}, "BkeyCI73_H": {"type": "rebuttal", "replyto": "Bkxi11nsdr", "comment": "Thank you for the references! We indeed missed this paper when we were preparing our submission, and we agree that this is a very relevant reference. We've added this reference to the paper, along with a discussion. We cannot update the openreview submission at this time, but will include an updated related work section in the final.\n\nWe definitely agree that this prior paper also uses an advantage weighting formulation, though in contrast with this work, our paper does not perform fitted Q-iteration, instead opting for the simplest possible approach for policy search with off-policy data.", "title": "references"}, "rJgY9sVcuS": {"type": "rebuttal", "replyto": "S1xI3dbuuH", "comment": "Hi John,\n\nThank you for the feedback, and the code. We ran the code that you linked, but our results do not match the ones you posted. In fact, our original PPO comparison (which was included in the paper) performed substantially better. We provide learning curves for both versions here:\nhttps://imgur.com/a/UWamd6N\n\"Original\" refers to using the original baselines PPO code, and \"Modified\" refers to the script you provided. We have also asked other people run your script and they observe similar results to ours. We have ran your script without any modifications, but were unable to  reproduce the results you posted, and those results do not appear to match the figures reported in the original PPO paper. We are not aware of any other previously published papers or publicly available code that generate results similar to the ones that you posted.\n\nThe PPO performance figures in our comparisons match those reported in the original PPO paper. We also find that  the performance of the stochastic and deterministic PPO policies from the baselines implementation are fairly similar:\nhttps://imgur.com/a/mTOOZyc\nHowever, we will update the performance statistics with those from the deterministic policies. The performance of the stochastic policy in your plots also appears to be substantially better than those from the baselines implementation. Perhaps there are other important differences besides using a deterministic vs stochastic policy?\n\nIn the paper, we were using the ppo2 implementation from the baselines, since it seems that ppo1 might be deprecated:\nhttps://github.com/openai/baselines/issues/485\nThe PPO results from the original paper more closely align with the ones reported in our comparisons.\n\nThat said, we would like to emphasize that the purpose of our comparisons is to illustrate that AWR attains results that are comparable to current state-of-the-art methods. Our goal is not to show that a particular algorithm is necessarily better than another. It is well known that the particular details of the implementation can make a significant difference, especially on widely studied tasks like the gym benchmarks. So in the interest of reproducible, we prefer to use more standard and publicly available implementations of these algorithms.\n\nWe would also like to point out that most hyperparameters are the same across the various environments. There are some parameters that varies a bit from environment to environment, such as the actor stepsizes for the humanoid and lunarlander, and the standard deviation of the action distribution.", "title": "PPO Results"}, "HkeJshE9OS": {"type": "rebuttal", "replyto": "rJgU1TfYur", "comment": "Hi Zhaoming,\n\nThank you for the suggestion and insight. We will look to try out input normalization for TD3. We would like to point out that it is generally standard practice to use publicly available code when comparing to prior algorithms. Since implementation details can have a significant effect on the performance of RL algorithms, in the interest of reproducibility, we prefer to use more standard and publicly available implementations of these algorithms in  our experiments. We would like to emphasize that the goal of our comparisons is to show that AWR can achieve comparable performance to current state-of-the-art methods, it is not to show whether one algorithm is necessarily better than another.", "title": "TD3"}}}