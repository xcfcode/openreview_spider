{"paper": {"title": "Gradients of Counterfactuals", "authors": ["Mukund Sundararajan", "Ankur Taly", "Qiqi Yan"], "authorids": ["mukunds@google.com", "ataly@google.com", "qiqiyan@google.com"], "summary": "A method for identifying feature importance in deep networks using gradients of counterfactual inputs", "abstract": "Gradients have been used to quantify feature importance in machine learning models. Unfortunately, in nonlinear deep networks, not only individual neurons but also the whole network can saturate, and as a result an important input feature can have a tiny gradient. We study various networks, and observe that this phenomena is indeed widespread, across many inputs.\n\nWe propose to examine interior gradients, which are gradients of counterfactual inputs constructed by scaling down the original input. We apply our method to the GoogleNet architecture for object recognition in images, as well as a ligand-based virtual screening network with categorical features and an LSTM based language model for the Penn Treebank dataset. We visualize how interior gradients better capture feature importance. Furthermore, interior gradients are applicable to a wide variety of deep networks, and have the attribution property that the feature importance scores sum to the the prediction score. \n\nBest of all, interior gradients can be computed just as easily as gradients. In contrast, previous methods are complex to implement, which hinders practical adoption.", "keywords": ["Deep learning", "Computer vision", "Theory"]}, "meta": {"decision": "Reject", "comment": "This paper was reviewed by 3 experts. All 3 seem unconvinced of the contributions, point to several shortcomings, and recommend rejection. I see no basis for overturning their recommendation. To be clear, the problem of achieving insight into the inner workings of deep networks is of significant importance and I encourage the authors to use the feedback to improve the manuscript."}, "review": {"Sk4dUN4Ix": {"type": "rebuttal", "replyto": "rJzaDdYxx", "comment": "We added two new sections (2.5 and 2.6) to the paper. Section 2.5\nproposes two very desirable axioms for attribution methods,\nand uses them to rule out other attribution methods from the literature.\nSection 2.6 proposes a full axiomatization under which our method is\nunique.\n\nIt is possible that sections 2.3-2.7 may constitute a shorter 6-page\nself-contained paper with the title --- \"attributions using interior\ngradients\".\n", "title": "New material added to the paper"}, "HJFn-CqNx": {"type": "rebuttal", "replyto": "HkiWSoKVe", "comment": "The Inception model we analyzed did not use batch normalization, but the drug discovery network did. We do not think batch normalization affects saturation in the network---at least we don't have any theory or empirical evidence for it yet.", "title": "Regarding batch normalization"}, "HkiWSoKVe": {"type": "rebuttal", "replyto": "B1qHBhx4x", "comment": "We thank the reviewer for the review.\n \nRegarding \u201cauthors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant\u201d. \n\nWe do think we discuss this. Notice that Section 2.1 (\u201cGradients do not reflect feature importance\u201d) gives examples of gradients not reflecting feature importance, and Section 2.2 (\u201cSaturation\u201d) discusses why to an extent. \n\nThe thesis proposed by the reviewer \u201cIs it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients\u201d seems plausible empirically. For instance, see this GIF for the label \u201cdrilling platform\u201d obtained by combining the visualizations of interior gradients at various scaling intensities https://github.com/ankurtaly/Attributions/blob/master/Visualizations/Gifs/6717aba6a10b230f.gif). Gradients at lower intensities seem to emphasize prominent features while those at higher intensities emphasize less important ones (peripheral?). Notice also that ablating the gradient at the image often does not change the prediction, another data point toward the thesis that they are unimportant (see Section 2.1 for an example).  \n\nOne exercise is we plan to carry out is to find the set of neurons that contribute to the gradient (and via Proposition 1 to the final prediction), at lower values of the scaling parameter (alpha)---these gradients look to us like they capture meaningful features and therefore the neurons through which they flow seem critical. We could check whether these critical neurons remain saturated at higher alpha, including for alpha=1. This exercise serves to connect the apparent variation in the interior gradients as alpha increases to the operation of important neurons within the network. ", "title": "Response to AnonReviewer1"}, "H1_4ViKEl": {"type": "rebuttal", "replyto": "rJzaDdYxx", "comment": "We thank the reviewers for a detailed review.  The rebuttal below addresses some of the mentioned concerns.\n\nRegarding \u201cfar too long\u201d and \u201cunnecessarily grandiose name for literally, a scaled image\u201d: \n\nWe\u2019d agree that the paper is long for the ideas in it. The length stems from the difficulty of not having a crisp evaluation technique for feature importance. So we try to resort to qualitative discussions together with images. But we  can definitely try to tighten the writing. We are open to changing the title of the paper to \u201cInterior Gradients\u201d or something like it, though it is worth noting that while scaling intensities seems natural for images, analogous scaling for Text or Drug Discovery models results in inputs that are more obviously fake, i.e., counterfactual.\n\nRegarding \u201chow the proposed scheme for feature importance ranking is useful\u201d: \n\nWhile debugging deep networks is hard in general, examining feature importance scores offers a limited but useful insight into the operation of the network on a particular input. For us, the experience with the Drug Discovery network where we found, via our attributions, that the bond features were severely underused (see Section 3.1) was a concrete instance of how feature importance analysis could help debug and improve networks. As we discussed in section 2.7, we do mention the limitations of our technique in understanding what the network does. The same pros and cons would seem to apply to other feature importance techniques (see Section 2.8). The key difference is that ours is much easier to implement--- as simple as computing a gradient.\n\nRegarding \u201cThe quantitative evidence is quite limited and most of the paper is spent on qualitative results\u201d: \n\nWe address with the following multipart response; apologies for the lengthy response.\n\nFirst, we do plan to produce a comparison with side by sides for LRP and our method for the MNIST data set over the next few weeks as a sanity check.\n\nHowever, we don\u2019t think that that there is a strong metric to compare different feature importance techniques. This is acknowledged by Samek et al in their 2015 ICML Visualization Workshop work. We elaborate on this further at the end of this rebuttal.  \n\nMethods like DeepLift and Layer-wise Relevance Propagation (LRP) break a fundamental axiom in our mind: the attributions depend on the implementation, i.e. two networks that implement identical input-output  relationships can have different attributions. This seems odd---see Section 2.4 and Figure 14.  Perhaps, we did not emphasize this enough in the paper. \n\nThe main focus for us in the evaluation conducted so far has been to ensure that our output was sensible. In Section 2.5, we discuss a combination of approaches that we used to assess the attributions, including eyeballing, localization, and ablations. We welcome you to visualize more attributions at: https://github.com/ankurtaly/Attributions.\n\nOn the lack of a good evaluation metric:\n\nFundamentally,  it is hard to tell apart inaccuracies in the attribution from inaccuracies in the network\u2019s predictions. The two metrics we know of --- ablations and localization---both come with their own downsides (though we did implement both of them for our attribution technique).\n\nAblation:  The idea here is to ablate features that receive a high attribution and measure the corresponding drop (or increase) in the network\u2019s prediction score. There are two main issues with such an ablation based metric.\n* First, ablating features may lead to unnatural inputs which the network has never seen before and therefore the prediction score drop may be reflective of the absurdness of the ablated input rather than the importance of the feature.\n* Second, when features interact then ablating individual features may not be enough to measure their importance. For instance, if two feature interact disjunctively then ablating either one would not lead to a drop in the prediction score.\n\nLocalization: In object recognition networks (such as Inception), where the prediction can be localized to a small region of the image, the accuracy of an attribution map can be measured based on the amount of attribution falling on the corresponding region---larger the attribution the better. Such an analysis can be carried out using human-drawn bounding-box labels, which are available for the ImageNet dataset. Unfortunately, this evaluation also has its own set of flaws.\n* First, simply measuring the total attribution falling on the object does not tell how effectively the attribution distinguishes various features of the object. \n* Second, while the object may be localized to a region of the image, the context around the object would certainly contribute to its prediction. \nFor instance, in this image https://github.com/ankurtaly/Attributions/blob/master/Visualizations/IntegratedGradients/1bd6987fa9219dec.jpg, the attribution from the gradients at the image is more localized to the object (\u201ccabbage butterfly\u201d) than the attribution from the interior gradients. Yet to a human, the  attribution from the interior gradients appears qualitatively better as it focuses sharply on features of the butterfly and also shows parts of the context that seem relevant to the prediction.", "title": "Response to AnonReviewer2 and AnonReviewer3"}, "r1x1ZNUQe": {"type": "review", "replyto": "rJzaDdYxx", "review": "Could you provide quantitative or qualitative comparison to one or more of the prior methods mentioned in the discussion on score back-propagation based methods (eg guided back-propagation)?  As there is significant prior work here, it would be appropriate to include it in the experimental comparison rather than simply arguing against it in the text.  The simplest option (gradients at image) is dismissed more easily than these alternatives, yet serves as the only experimental reference.This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.\n\nWhile motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-review question articulated this same concern and has not been answered.\n", "title": "comparison to prior work", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJ_JRc4Nl": {"type": "review", "replyto": "rJzaDdYxx", "review": "Could you provide quantitative or qualitative comparison to one or more of the prior methods mentioned in the discussion on score back-propagation based methods (eg guided back-propagation)?  As there is significant prior work here, it would be appropriate to include it in the experimental comparison rather than simply arguing against it in the text.  The simplest option (gradients at image) is dismissed more easily than these alternatives, yet serves as the only experimental reference.This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.\n\nWhile motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-review question articulated this same concern and has not been answered.\n", "title": "comparison to prior work", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}