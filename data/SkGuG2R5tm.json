{"paper": {"title": "Spreading vectors for similarity search", "authors": ["Alexandre Sablayrolles", "Matthijs Douze", "Cordelia Schmid", "Herv\u00e9 J\u00e9gou"], "authorids": ["asablayrolles@fb.com", "matthijs@fb.com", "cordelia.schmid@inria.fr", "rvj@fb.com"], "summary": "We learn a neural network that uniformizes the input distribution, which leads to competitive indexing performance in high-dimensional space", "abstract": "Discretizing floating-point vectors is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of the quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net whose last layers form a fixed parameter-free quantizer, such as pre-defined points of a sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping.  For this purpose, we propose a new regularizer derived from the Kozachenko-Leonenko differential entropy estimator and combine it with a locality-aware triplet loss. \nExperiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Further more, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyser that can be applied with any subsequent quantization technique.\n", "keywords": ["dimensionality reduction", "similarity search", "indexing", "differential entropy"]}, "meta": {"decision": "Accept (Poster)", "comment": ". Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\n- The proposed method is novel and effective\n- The paper is clear and the experiments and literature review are sufficient (especially after revision).\n \n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\nThe original weaknesses (mainly clarity and missing details) were adequately addressed in the revisions.\n\n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it\u2019s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nNo major points of contention.\n\n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be accepted."}, "review": {"BkgZlICzyN": {"type": "rebuttal", "replyto": "rygYNIkTRm", "comment": "Thank you for the feedback. The method \"Catalyst + Lattice + end2end\" refers to using a quantization layer during training with the straight-through estimator described in Section 4.2. In contrast, the version \"Catalyst + Lattice\" also optimizes Eqn (4) but without including the quantization layer during training. We will clarify this point in the manuscript, and add the precisions regarding the architecture as suggested above. ", "title": "End2end uses a quantization layer at training time"}, "HkeDt3GOsQ": {"type": "review", "replyto": "SkGuG2R5tm", "review": "The idea, transforming the input data to an output space in which the data is distributed uniformly and thus indexing is easier, is interesting. \n\nMy main concerns come from experimental results.\n\n(1) Table 1: where are the results of OPQ and LSQ from? run the codes by the authors of this paper? or from the original paper?\n\nIt is not consistent to the LSQ paper (https://www.cs.ubc.ca/~julm/papers/eccv16.pdf). For BigANN1M, from the LSQ paper, the result is >29 recall at 1 for 64 bits. \n\n(2) Figure 5: similarly, how did you get the results of PQ and OPQ?\n\n(3) There are some other advanced algorithms: e.g.,  additive quantization (Babenko & Lempitsky, 2014) and composite quantization (https://arxiv.org/abs/1712.00955)\n\nThe above points make it hard to judge this paper.", "title": "Problematic  experimental results", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Ske30l-iAm": {"type": "rebuttal", "replyto": "S1g23fYDRX", "comment": "Thank you for your feedback. \n\n1) Regarding the architecture, we discarded many choices that are tailored to specific assumptions (like convolutional layers that assume spatial or temporal shift-invariance), since we do not make these assumptions about our data. Then, we started from a simple multi-layer perceptron, with the standard ReLU non-linearity and pre-activation batch norm. We chose to keep the same width at each layer, and varied depth and width to find optimal parameters (see details below).\nWe also tried several variants of the architecture and the training regime that did not improve performance or made it worse:\n\n* other types of architectures  (we tested expansion: 128-256-512, 512-1024; reduction: 64-48-32, 1024-512; and both: 128-256-512-256-128-64)\n* sampling new training points every nth epoch (tried every epoch, every 3, 10, 30 epochs) \n* varying learning rate decay (factors 1, 1.01, 1.05 or 1.1)\n* variants of hard negative mining: only using negatives for which there is no positive closer to the query point\n\nNone of these variants improved above the architecture/schedule, which had the advantage of being conceptually simpler.\nThe parameters reported in the paper were the best we found with the amount of training data available at hand and with our standard optimization scheme. To conclude on this point, on one side it is difficult to answer the question \"if/how we can improve it\" for most works in deep learning. What we suggested for the architecture is the best choice we found to date. On the other side, from our experiments, we generally observed that the choice of the discretization layer has more impact on the overall performance of the system than the choice of a particular structure for the catalyzer (compare binary and lattice for instance), therefore there is likely some margin on improvement in this direction. \n\n2) \"The performance improves when augmenting the width of the network, but with diminishing returns.', how was the width of the network(s) used for the empirical evaluation selected. Were they fixed to some value (if so, why) or were they cross-validated over?\"\n\nWe conducted preliminary experiments on the validation set, varying the depth of the network from 0 to 6 latent layers, and the size of the latent layers from 128 to 2048 by doubling it every time. The best results with respect to depth were obtained with 2 latent layers; doubling the width provided increments of 2 - 5 points until 512, then 512 \u2192 1024 resulted in a 0.5 improvement and 1024\u2192 2048 resulted in a negligible improvement while having a longer runtime.\n", "title": "The architecture was cross-validated "}, "BJeQGL2HCQ": {"type": "rebuttal", "replyto": "SkGuG2R5tm", "comment": "We updated the paper to correct typos, add some precisions and include the references suggested by the reviewers.", "title": "Updated paper"}, "S1epFsSOpm": {"type": "rebuttal", "replyto": "HkeDt3GOsQ", "comment": "We thank the reviewer for their review and comments. We provide detailed answers below.\n\n\"My main concerns come from experimental results.\"\nUpon publication of the paper, we will release the code that replicates the experiments. \n\n\"(1) Table 1: where are the results of OPQ and LSQ from?  [...] It is not consist to the LSQ paper\"\nIn our experiments, we used the reference public implementation from the authors of LSQ [1]. The discrepancy in the reported 64-bit recall1@1 comes from the fact that the datasets are different: we use Bigann1m (28.4 recall) whereas the LSQ paper reports results on Sift1m. We conducted experiments on Bigann1m because the training set associated with Sift1m is too small (100k vectors) for learning the catalyzer. As a sanity check, we re-ran the code of [1] on Sift1m and obtained 28.99, which is consistent with the results reported by [A] (Table 1, LSQ, 29.37) and [Martinez et al, 2018] (Figure 3, LSQ SR-C and SR-D, ~28; Table 4 corresponds to another experimental setting).\n\n\u201c(2) Figure 5: similarly, how did you get the results of PQ and OPQ?\u201d\nWe used the open-source Faiss library [2] to obtain the results of PQ and OPQ. This library is used as a reference implementation in recent papers like [D, E]. There is a comparison point with [F] on Deep1M at 64 bit: the Faiss implementation of OPQ obtains recall@1 = 15.6 vs 16.1 in [F] (table 1). \n\n\u201c(3) There are some other advanced algorithms: e.g., additive quantization [B] and composite quantization [C]\u201d\nWe did not compare directly to AQ and CQ as it was shown that they underperform LSQ by some margin (Table 1 in [A]). Besides, in general, we insist in the paper that the encoding time for additive quantization methods is at least an order of magnitude slower than product quantization and our catalyzer + lattice (122s for LSQ vs < 10s for PQ/Lattice, cf. Table 1).\n\nReferences\n[1] https://github.com/una-dinosauria/local-search-quantization\n[2] https://github.com/facebookresearch/faiss\n\n[A] Revisiting additive quantization, Martinez et al., ECCV'2016\n[B] Additive Quantization for Extreme Vector Compression, Babenko & Lempitsky, CVPR'2014\n[C] Composite Quantization, J Wang et al. ICML'14\n[D] Link and code: Fast indexing with graphs and compact regression codes, Douze et al, CVPR'18\n[E] Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors, Baranchuk et al, ECCV'18\n[F] AnnArbor: Approximate Nearest Neighbors Using Arborescence Coding, Babenko & Lempitsky, ICCV'17\n", "title": "Response to Review #1"}, "SJlQccS_TQ": {"type": "rebuttal", "replyto": "B1x0Ka5M2X", "comment": "We thank the reviewer for their comments. \n\n\u201cThe training times for the catalyzer is never discussed in this manuscript. [...] Moreover, it is not clear if the inference time of the catalyzer is included in the results such as Table 1.\u201d \nTraining takes between 2 and 3 hours on a CPU machine using 20 cores, and the reported query timings take into account inference. \n\n\u201cOne important point not discussed in this manuscript is the choice of the structure (architecture) of the catalyzer. Is the catalyzer architecture dependent on the data?\u201d\nGenerally, we observe that beyond 3 layers there is no improvement in accuracy. The performance improves when augmenting the width of the network, but with diminishing returns. We use the same architecture across datasets. We successfully used the same architecture on other datasets, but we report results here on the standard datasets of the field.\n\n\u201cWhat is it about the proposed architecture that makes it sufficient for all data sets?\u201d\nWe have observed that the dimensions of the hidden layers in our architecture provide enough representation power for the model to be performant across all the datasets we have tested (those of the paper plus some internal datasets).\n\n\u201cIs the parameter r in the rank loss same as the norm r in the lattice quantizer?\u201d\nThe parameter r is not the same as the norm r of the lattice quantizer, we thank the reviewer for spotting this, we will update the paper to lift this ambiguity.\n", "title": "Response to Review #3"}, "BygLXqBd6m": {"type": "rebuttal", "replyto": "Bkxs4_VP2m", "comment": "We thank the reviewer for their review. Upon publication of the paper, we will open-source the code that replicates the experiments. In the meantime, we provide more details below:\n\n1) \u201cin the related work overview it would be good to also check possible connections with optimal transport methods using entropy regularization.\u201c \nIn the related work, we mention Bojanowski & Joulin (2017), who use optimal transport (without entropy regularization) to match images with random points on the sphere. The entropy regularization in optimal transport is a bit different than our entropy regularization as it is used mainly for speed purposes, whereas our entropy regularization provides a trade-off between the quality of nearest neighbors and how spread-out the output of the neural network is.\n\n2) \u201cat some points in the paper, e.g. section 3.3, the authors mention Voronoi cells. However, in the related work in section 2 vector quantization and self-organizing maps have not been mentioned.\u201d\nWe will update the related work with these references. In the context of section 3.3, Voronoi cells correspond to the quantization cells of the lattice.\n\n3) \u201cmore details on the optimization or learning algorithms for eq (3)(4) should be given. The loss function is non-smooth and rather complicated. What are the implications on the learning algorithm when training neural networks? Is it important to have a good initialization or not?\u201d\nWe found that standard practice for training neural networks worked quite well in our setting (even though we have no guarantee of getting to the global minimum of the objective function). More specifically, we train our networks with Stochastic Gradient descent with an initial learning rate of 0.5, momentum of 0.9, and decay the learning rate when the validation accuracy does not go up for an epoch. We did not need specific initialization to make the networks converge.\n\n4) \u201cHow reproducible are the results? In Table 1 only one number in each column is shown while eqs (3)(4) are non-convex problems. Is it the best result of several runs or an average that is reported in the Table? \u201c\nOur preliminary experiments have shown that the difference in performance between different trainings is very small (despite the problems being non-convex).  Therefore we train only once per set of hyper-parameters (d_out and lambda), and report the corresponding result. Our open-source code will reproduce these results up to the (very small) variations due to random initialization and mini-batch sampling.\n\n", "title": "Response to Review #2"}, "Bkxs4_VP2m": {"type": "review", "replyto": "SkGuG2R5tm", "review": "The authors propose a method to adapt the data to the quantizer, instead of having to work with a difficult to optimize discretization function. The contribution is interesting.\n\nAdditional comments and suggestions:\n\n- in the related work overview it would be good to also check possible connections with optimal transport methods using entropy regularization.\n\n- at some points in the paper, e.g. section 3.3, the authors mention Voronoi cells. However, in the related work in section 2 vector quantization and self-organizing maps have not been mentioned.\n\n- more details on the optimization or learning algorithms for eq (3)(4) should be given. The loss function is non-smooth and rather complicated. What are the implications on the learning algorithm when training neural networks? Is it important to have a good initialization or not?\n\n- How reproducible are the results? In Table 1 only one number in each column is shown while eqs (3)(4) are non-convex problems. Is it the best result of several runs or an average that is reported in the Table? \n\n", "title": "Spreading vectors for similarity search", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1x0Ka5M2X": {"type": "review", "replyto": "SkGuG2R5tm", "review": "Pros\n----\n\n[Originality]\nThe authors propose a novel idea of learning representations that improves the performance of the subsequent fixed discretization method.\n\n[Clarity]\nThe authors clearly motivate their solution and explain the different ideas and enhancements introduced. The manuscript is fairly easy to follow. The different terms in the optimization problem are clearly explained and their individual behaviour are presented for the better understanding.\n\n[Significance]\nThe empirical results for the proposed scheme are compared against various baselines under various scenarios and the results demonstrate the significant utility of the proposed scheme.\n\nLimitations\n-----------\n\n[Clarity]\nThe training times for the catalyzer is never discussed in this manuscript (even relative to the training times of the considered baselines). Moreover, it is not clear if the inference time of the catalyzer is included in the results such as Table 1. Even if, PQ and the catalyzer+lattice might have comparable search recalls, it would be good to understand the relative search times to get similar accuracy especially since the inference time for the catalyzer (which is part of the search time) can be fairly significant.\n\n[Clarity/Significance]\nOne important point not discussed in this manuscript is the choice of the structure (architechture) of the catalyzer. Is the catalyzer architecture dependent on the data?\n  - If yes, how to find an appropriate architecture?\n  - If no, what is it about the proposed architecture that makes it sufficient for all data sets?\nIn my opinion, this is extremely important since this drives the applicability of the proposed scheme beyond the presented examples.\n\n[Minor question]\n- Is the parameter r in the rank loss same as the norm r in the lattice quantizer? This is a bit confusing.", "title": "Well motivated novel idea; excellent results", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}