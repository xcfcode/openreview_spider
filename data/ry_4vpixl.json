{"paper": {"title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "authors": ["Zoe McCarthy", "Andrew Bai", "Xi Chen", "Pieter Abbeel"], "authorids": ["zmccarthy@berkeley.edu", "xiaoyang.bai@berkeley.edu", "c.xi@berkeley.edu", "pabbeel@berkeley.edu"], "summary": "Recurrent equation for RNNs that uses the composition of two orthogonal transitions, one time invariant and one modulated by input, that doesn't suffer from vanishing or exploding gradients.", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem.  The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients.  We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.", "keywords": ["Deep learning", "Theory"]}, "meta": {"decision": "Reject", "comment": "Paper has an interesting idea, but isn't quite justified, as pointed out by R2. Very minimal experiments are presented in the paper.\n \n pros:\n - interesting idea\n \n cons:\n - insufficient experiments with no real world problems.\n - no rebuttal either :(."}, "review": {"Sy8acg4Qx": {"type": "review", "replyto": "ry_4vpixl", "review": "Hi, \n\nsorry for the delay in reviewing the paper. Interesting paper, here are a few questions that I'd love to hear back on: \n (1) Saxe et al work was on deep linear feedforward models. I'm wondering why would expect his findings to be mirrored exactly in your setup (given, for e.g. that you have an input at every time step, issue that he doesn't have in this model, shared weights, etc.). I'm mostly referring to sentences like we would expect that learning time is constant regardless of sequence length.\n\n (2) More importantly maybe, most of the work I've seen on orthogonal matrices for RNNs focus only on the learning dynamics of this model. What about the representational power? The main advantage of not having an orthogonal rotation matrix is that you can *forget*, which seems a fundamental property to dealing with information. Otherwise dealing with noise becomes really hard. Do you have any intuition on what restrictions on the representational power to you have due to your parametrization choices? Isn't it true that while the gradients might not vanish, because of noise, learning could actually still be extremely hard? In particular, the only choice the model has is to designated a particular dimension for noise and make sure that puts all the noise in the input in this dimension (or dimensions). Otherwise accumulated noise, would make solving the task impossible. To me this seems a harder learning problem. IMHO this all boils down to: have you experimented with this model in more realistic task, maybe ones where you have large amounts of noise (or irrelevant features) ? One simple step forward that you might have time to do is to check the task of Pascanu et al 2013 ( https://arxiv.org/abs/1211.5063) which is identical to the one you proposed except that you have noise between the two symbols (I think is called the temporal order task). I'd love to see how the model performs on that, though I think even that task is very simplistic at the end of the day, and the ability of the model to deal with noise will still not be fully answered. Btw, that paper also seems relevant, though not cited (this however is maybe just a minor detail). My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. \n\nMy own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can't have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can't learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. \n\nI would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.", "title": "the pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJp48JVNx": {"type": "review", "replyto": "ry_4vpixl", "review": "Hi, \n\nsorry for the delay in reviewing the paper. Interesting paper, here are a few questions that I'd love to hear back on: \n (1) Saxe et al work was on deep linear feedforward models. I'm wondering why would expect his findings to be mirrored exactly in your setup (given, for e.g. that you have an input at every time step, issue that he doesn't have in this model, shared weights, etc.). I'm mostly referring to sentences like we would expect that learning time is constant regardless of sequence length.\n\n (2) More importantly maybe, most of the work I've seen on orthogonal matrices for RNNs focus only on the learning dynamics of this model. What about the representational power? The main advantage of not having an orthogonal rotation matrix is that you can *forget*, which seems a fundamental property to dealing with information. Otherwise dealing with noise becomes really hard. Do you have any intuition on what restrictions on the representational power to you have due to your parametrization choices? Isn't it true that while the gradients might not vanish, because of noise, learning could actually still be extremely hard? In particular, the only choice the model has is to designated a particular dimension for noise and make sure that puts all the noise in the input in this dimension (or dimensions). Otherwise accumulated noise, would make solving the task impossible. To me this seems a harder learning problem. IMHO this all boils down to: have you experimented with this model in more realistic task, maybe ones where you have large amounts of noise (or irrelevant features) ? One simple step forward that you might have time to do is to check the task of Pascanu et al 2013 ( https://arxiv.org/abs/1211.5063) which is identical to the one you proposed except that you have noise between the two symbols (I think is called the temporal order task). I'd love to see how the model performs on that, though I think even that task is very simplistic at the end of the day, and the ability of the model to deal with noise will still not be fully answered. Btw, that paper also seems relevant, though not cited (this however is maybe just a minor detail). My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. \n\nMy own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can't have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can't learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. \n\nI would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.", "title": "the pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyrfzFhfe": {"type": "review", "replyto": "ry_4vpixl", "review": "- You said the model trained after 5000 time steps was unstable but still able to succeed. How do we measure this when the curves go up and down like this. What does \"success\" mean precisely?\n- Have you compared this to other approaches (the document says more results will appear soon)?\n- Have you also done experiments with the more standard setting of the task, so that it's easier to compare with previous results?\nThis is a nice proposal, and could lead to more efficient training of\nrecurrent nets. I would really love to see a bit more experimental evidence.\nI asked a few questions already but didn't get any answer so far.\nHere are a few other questions/concerns I have:\n\n- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)\n- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?\n- The experiments are a bit disappointing as the number of distinct input/output\nsequences were in fact very small and as noted by the authr, training\nbecomes unstable (I didn't understand what \"success\" meant in this case).\nThe authors point that the experiment section need to be expanded, but as\nfar as I can tell they still haven't unfortunately.\n", "title": "Some questions", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rk5c3reEe": {"type": "review", "replyto": "ry_4vpixl", "review": "- You said the model trained after 5000 time steps was unstable but still able to succeed. How do we measure this when the curves go up and down like this. What does \"success\" mean precisely?\n- Have you compared this to other approaches (the document says more results will appear soon)?\n- Have you also done experiments with the more standard setting of the task, so that it's easier to compare with previous results?\nThis is a nice proposal, and could lead to more efficient training of\nrecurrent nets. I would really love to see a bit more experimental evidence.\nI asked a few questions already but didn't get any answer so far.\nHere are a few other questions/concerns I have:\n\n- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)\n- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?\n- The experiments are a bit disappointing as the number of distinct input/output\nsequences were in fact very small and as noted by the authr, training\nbecomes unstable (I didn't understand what \"success\" meant in this case).\nThe authors point that the experiment section need to be expanded, but as\nfar as I can tell they still haven't unfortunately.\n", "title": "Some questions", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}