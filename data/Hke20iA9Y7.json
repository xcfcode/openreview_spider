{"paper": {"title": "Efficient Training on Very Large Corpora via Gramian Estimation", "authors": ["Walid Krichene", "Nicolas Mayoraz", "Steffen Rendle", "Li Zhang", "Xinyang Yi", "Lichan Hong", "Ed Chi", "John Anderson"], "authorids": ["walidk@google.com", "nmayoraz@google.com", "srendle@google.com", "liqzhang@google.com", "xinyang@google.com", "lichan@google.com", "edchi@google.com", "janders@google.com"], "summary": "We develop efficient methods to train neural embedding models with a dot-product structure, by reformulating the objective function in terms of generalized Gram matrices, and maintaining estimates of those matrices.", "abstract": "We study the problem of learning similarity functions over very large corpora using neural network embedding models. These models are typically trained using SGD with random sampling of unobserved pairs, with a sample size that grows quadratically with the corpus size, making it expensive to scale.\nWe propose new efficient methods to train these models without having to sample unobserved pairs. Inspired by matrix factorization, our approach relies on adding a global quadratic penalty and expressing this term as the inner-product of two generalized Gramians. We show that the gradient of this term can be efficiently computed by maintaining estimates of the Gramians, and develop variance reduction schemes to improve the quality of the estimates. We conduct large-scale experiments that show a significant improvement both in training time and generalization performance compared to sampling methods.", "keywords": ["similarity learning", "pairwise learning", "matrix factorization", "Gramian estimation", "variance reduction", "neural embedding models", "recommender systems"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents methods to scale learning of embedding models estimated using neural networks. The main idea is to work with Gram matrices whose sizes depend on the length of the embedding. Building upon existing works like SAG algorithm, the paper proposes two new stochastic methods for learning using stochastic estimates of Gram matrices. \n\nReviewers find the paper interesting and useful, although have given many suggestions to improve the presentation and experiments. For this reason, I recommend to accept this paper.\n\nA small note: SAG algorithm was originally proposed in 2013. The paper only cites the 2017 version. Please include the 2013 version as well.\n"}, "review": {"rJgsqcLt3m": {"type": "review", "replyto": "Hke20iA9Y7", "review": "This paper proposes an efficient algorithm to learn  neural embedding models with a dot-product structure over very large corpora. The main method is to reformulate the objective function in terms of generalized Gramiam matrices, and maintain estimates of those matrices in the training process. The algorithm uses less time and achieves significantly better quality than sampling based methods. \n\n1. About the experiments, it seems the sample size for sampling based experiments is not discussed. The number of noise samples have a large influence on the performance of the models. In figure 2, different sampling strategies are discussed. It would be cool if we can also see how the sampling size affects the estimation error. \n\n2. If we just look at the sampling based methods, in figure 2a, uniform sampling\u2019s Gramian estimates is the worst. But the MAP of uniform sampling on validation set for all three datasets are not the worst. Do you have any comments?\n\n3. wheter an edge -> whether an edge.\n", "title": "Nice work", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJxICZhjam": {"type": "rebuttal", "replyto": "Hke20iA9Y7", "comment": "We would like to thank all reviewers for their careful reading and helpful suggestions. We have uploaded a revision of the paper with the following changes:\n- We added a new section to the appendix (Appendix C) discussing how to adapt the methods to a non-uniform weight matrix.\n- We added Appendix E.1 to relate the gradient estimation error to the Gramian estimation error, with a numerical experiment (Figure 6) showing the effect of our methods on gradient estimates.\n- We added a comment to the conclusion to emphasize that our experiments were focused on problems with very large vocabulary size.\n- We rearranged the introduction, and improved transitions between sections.\n- We added comments to the numerical experiments (Section 4 and Appendix E) highlighting the effect of the batch size and of the sampling distribution.\n\nWe thank the reviewers again for their time and helpful comments.\n", "title": "Thank you for the comments; revision uploaded"}, "rkxc_enipX": {"type": "rebuttal", "replyto": "r1gf2j2w27", "comment": "Thank you for your review and your helpful suggestions.\n\nWe updated the organization following the reviewer's suggestions, by reorganizing the introduction and improving the transitions between sections. We also added a comment about our choice of hyper-parameters: in the main experiments of Section 4, the hyper-parameters were cross-validated using the baseline. The effect of some of the hyper-parameters is further studied in the appendix: the effect of the batch size and learning rate is studied in Appendix D.2 (now Appendix E.4 in the revision), and the effect of the penalty coefficient \u03bb is illustrated in Appendix C (now Appendix D in the revision). We did not include these results in the main body of the paper for space constraints, and to keep the message focused, but we added a note to Section 4 pointing to the appendix for further details on the effect of the various hyper-parameters.\n", "title": "Thank you for your review"}, "ryltke3iT7": {"type": "rebuttal", "replyto": "rJgsqcLt3m", "comment": "Thank you for your review and your helpful suggestions.\n\n1) On the effect of sample size: we agree that the sample size directly affects the performance of these methods. We investigated this effect in Appendix D.2 (which is now Appendix E.4 in the revision), where we ran the same experiment on Wikipedia English with batch sizes 128, 512 (Tables 3 and 4), and compared the results to batch size 1024 (Table 2). We simultaneously varied the learning rate to understand its effect as well, but focusing on the effect of batch size only, we can observe that\n(i) the performance of all methods increases with the batch size (at least in the 128-1024 range). \n(ii) the relative improvement of our methods (compared to the baseline) is larger for smaller batch sizes: the relative improvement is 19.5% for 1024, 26.7% for 512, and 29.1% for 128.\nOf course, one cannot increase the batch size indefinitely as there are hard limits on memory size, and the key advantage of our methods is in problems where sampling-based methods give poor estimates even with the largest feasible batch size.\nThe effect of the batch size can also be seen to some extent in Figure 2.a, where we show the quality of the Gramian estimates for batch size 128 and 1024. The figure suggests that the quality improves, for all methods, with larger batch sizes, and that SOGram with batch size 128 has a comparable estimation quality to the baseline with batch size 1024.\n\n2) The reviewer raises an interesting point. We have observed in our experiments that for a fixed sampling distribution, improving the Gramian estimates generally leads to better MAP, but we cannot draw conclusions when the sampling distribution changes. One possible explanation is that the sampling distribution affects both the quality of the Gramian estimates, and the frequency at which the item embeddings are updated. In particular, tail items are sampled more often under uniform sampling than under the other distributions, and updating their embeddings more frequently may contribute to improving the MAP. We added a comment (Appendix E.2 in the revision) to highlight this observation.", "title": "Thank you for your review"}, "BJgu7y3o67": {"type": "rebuttal", "replyto": "ryx3glNinm", "comment": "Thank you for your assessment and your helpful suggestions.\n\nRegarding evaluation: since the focus of the paper is on the design of an efficient optimization method, we wanted to choose an experiment where (i) the evaluation metric is aligned with the optimization objective, and (ii) the vocabulary size is very large (on the order of 10^6 or more), making traditional sampling-based methods inefficient, because they would require too many samples to achieve high model quality. This is why we chose the Wikipedia dataset, which is, to our knowledge, one of the few publicly available datasets of this scale. It also offers different subsets of varying scale, which allowed us to illustrate the effect of the problem size, suggesting that the benefit of the Gramian-based methods increases with vocabulary size. We added a note to the revision to comment on our choice.\nWe also agree that it will be beneficial to evaluate these method on other applications such as more traditional natural language tasks, and this is something we intend to pursue in future work.", "title": "Thank you for your review"}, "ryx3glNinm": {"type": "review", "replyto": "Hke20iA9Y7", "review": "Summary of the paper:\n\nThis work presents a novel method for similarity function learning using non-linear model. The main problem with the similarity function learning models is the pairwise component of the loss function which grows quadratically with the training set. The existing stochastic approximations which are agnostic to training set size have high variance and this in-turn results in poor convergence and generalisation. This paper presents a new stochastic approximation of the pairwise loss with reduced variance. This is achieved by exploiting the dot-product structure of the least-squares loss and is computationally efficient provided the embedding dimensions are small. The core idea is to rewrite the least-squares as the matrix dot product of two PSD matrices (Grammian). The Grammian matrix is the sum of the outer-product of embeddings along the training samples. The authors present two algorithms for training the model, 1)SAGram: By maintaining a cache of all embedding vectors of training points (O(nk) space)$, whenever a point is encountered it's cache is replaced with it's embedding vector. 2) SOGram: This algorithm keeps a moving average of the Grammian estimate to reduce the variance. Experimental results shows that this approach reduces the variance in the Grammian estimates, results in faster convergence and better generalisation.\n\nReview:\n\nThe paper is well written with clear contribution to the problem of similarity  learning.  My only complain is that, I think the evaluation is a bit weak and does not support the claim that is applicable all kinds of problems e.g. nlp and recommender systems. This task in Wikipedia does not seem to be standard (kind of arbitrary) \u2014 there are some recommendation results in the appendix but I think it should have been in the main paper.\n\nOverall interesting but I would recommend evaluating in standard similarity learning for nlp and other tasks (perhaps more than one)\n\nThere are specific similarity evaluation sets for word embeddings. It can be found in following papers: https://arxiv.org/pdf/1301.3781.pdf  \nhttp://www.aclweb.org/anthology/D15-1036", "title": "Good paper with clear contribution, could be made stronger with better evaluation", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1gf2j2w27": {"type": "review", "replyto": "Hke20iA9Y7", "review": "This paper proposes a method for estimating non-linear similarities between items using Gramian estimation. This is achieved by having two separate neural networks defined for each item to be compared, which are then combined via a dot product. The proposed innovation in this paper is to use Gramian estimation for the penalty parameter of the optimization which allows for the non-linear case. Two algorithms are proposed which allow for estimation in the stochastic / online setting. Experiments are presented which appear to show good performance on some standard benchmark tasks. \n\nOverall, I think this is an interesting set of ideas for an important problem. I have two reservations. First, the organization of the paper needs to be addressed in order to aid user readability. The paper often jumps across sections without giving motivation or connecting language. This will limit the audience of the paper and the work. Second (and more importantly), I found the experiments to be slightly underwhelming. The hyperparameters (batch size, learning rate) and architecture don\u2019t have any rationale attached to them. It is also not entirely clear whether the chosen comparison methods fully constitute the current state of the art. Nonetheless, I think this is an interesting idea and strong work with compelling results. \n\nEditorial comments:\n\nThe organization of this paper leaves something to be desired. The introductions ends very abruptly, and then appears to begin again after the related work section. From what I can tell the first three sections all constitute the introduction and should be merged with appropriate edits to make the narrative clear.\n\n\u201cwhere x and y are nodes in a graph and the similarity is wheter an edge\u201d \u2192 typo and sentence ends prematurely. \n", "title": "Good work overall", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "r1gepzTgnm": {"type": "rebuttal", "replyto": "BygKKCEuiX", "comment": "1) For observed pairs, one can use arbitrary weights  \ud835\udc64_\ud835\udc56\ud835\udc57 . For the unobserved data, in our problem setting, the set of all possible pairs (i, j) is too large to specify an arbitrary weight matrix (say if the vocabulary size is 10^7 or more, the full weight matrix would have more than 10^14 entries). In such situations one needs to provide a concise description of this weight matrix. One such representation is the sum of a sparse + low-rank component, and our methods handle this case: the sparse component can be optimized directly, and the low-rank component can be optimized using our Gramian estimation methods. The previous answer describes the rank-1 case where  \ud835\udc64_\ud835\udc56\ud835\udc57 = \ud835\udc4e_\ud835\udc56 \ud835\udc4f_\ud835\udc57 , and the same argument generalizes to the low-rank case (for a rank-r matrix weight matrix, one needs to maintain 2*r Gramians).\n\n2) In retrieval setting with a very large corpus, the dot product structure can be the only viable option, as scoring all candidates in linear time is prohibitively expensive, while maximum inner-product search is approximated in sublinear time. As mentioned above, even in models that don't have the dot product structure, our method applies to the global orthogonal regularizer in any embedding layer.\nWe believe our methods are applicable to industrial settings. Our experiments suggest that the relative improvement (w.r.t. existing sampling based methods) grows with the corpus size (see Table 2), so we expect to see large improvements in applications with very large corpora. As for comparing different model classes (neural embedding models Vs. factorization machines) this is outside the scope of the paper, our focus is instead on developing efficient optimization methods for the neural embedding model class.", "title": "On using more general weight matrices"}, "r1xBJUGgo7": {"type": "rebuttal", "replyto": "rJx8ypbkjQ", "comment": "Thank you for your comments. We will discuss each point below.Thank you for your comments. We will discuss each point below.\n\n1) We agree that it is often a good idea to use non-uniform weights, (as well as non-uniform sampling distributions), and the proposed methods support these variants. We did not discuss non-uniform weights to avoid overloading the presentation, but we can certainly add a section to the appendix. As discussed in our previous comment, if we define the penalty as 1/\ud835\udc5b^2 \u2211_\ud835\udc56 \u2211_\ud835\udc57 \ud835\udc4e_\ud835\udc56 \ud835\udc4f_\ud835\udc57 \u27e8\ud835\udc62_\ud835\udc56, \ud835\udc63_\ud835\udc57\u27e9^2 , (where in a recommendation setting, \ud835\udc4e_\ud835\udc56 is a user-specific weight and \ud835\udc4f_\ud835\udc57 is an item-specific weight), then this expression is equivalent to \u27e8\ud835\udc3a^\ud835\udc62, \ud835\udc3a^\ud835\udc63\u27e9 where \ud835\udc3a^\ud835\udc62, \ud835\udc3a^\ud835\udc63 are weighted Gram matrices, defined by \ud835\udc3a^\ud835\udc62 = 1/\ud835\udc5b \u2211_\ud835\udc56 \ud835\udc4e_\ud835\udc56 \ud835\udc62_\ud835\udc56\u2297\ud835\udc62_\ud835\udc56 and similarly for \ud835\udc3a^\ud835\udc63. The same methods (SAGram, SOGram) can be applied to the weighted Gramians.\n\n2) The dot product structure remains important in recent literature, e.g. [1, 2, 3], especially in retrieval settings where one needs to score a large corpus, as finding the top-k items in a dot product model is efficient (see literature on Maximum Inner Product Search, e.g. [4, 5] and references therein). In addition to such models, our methods can also apply to arbitrary models using the Global Orthogonal regularizer described in [6]. The effect of the regularizer is to spread-out the distribution of embeddings, which can improve generalization. We show in Appendix C that this regularizer can be written using Gramians, thus one can apply SOGram or SAGram to such models.\n\n3) On the choice of loss function: the loss on observed pairs (the function \u2113 in our notation) is not limited to square loss, and could be logistic loss for example. The penalty function on all pairs, (\ud835\udc54 in our notation) is a quadratic function. It can be extended to a larger family (the spherical family discussed in [7]), but this is beyond the scope of this paper.\n\n4) On the derivation of the Gramian formulation: we gave a concise derivation in Section 2.2 due to space limitations, but we can expand here and give some intuition. The penalty term \ud835\udc54 is a double-sum 1/\ud835\udc5b^2 \u2211_\ud835\udc56 \u2211_\ud835\udc57 \u27e8\ud835\udc62_\ud835\udc56, \ud835\udc63_\ud835\udc57\u27e9^2 . If we focus on the contribution of a single left embedding \ud835\udc62_\ud835\udc56 , we can observe that this is a quadratic function \ud835\udc62 \u21a6 \u2211_\ud835\udc57 \u27e8\ud835\udc62, \ud835\udc63_\ud835\udc57\u27e9^2 . Importantly, this is the same quadratic function that applies to all the \ud835\udc62_\ud835\udc56 (independent of \ud835\udc56 ). A quadratic function on \u211d^\ud835\udc51 can be represented compactly using a \ud835\udc58\u00d7\ud835\udc58 matrix, and this is exactly the role of the Gramian \ud835\udc3a^\ud835\udc63, and because the same function applies to all \ud835\udc62_\ud835\udc56, we can maintain a single estimate and reuse it across batches (unlike sampling-based methods that recompute the estimate at each step). There is additional discussion in Appendix C on the interpretation of this term.\n\n5) On the choice of the weight \ud835\udf06: as mentioned in the experiments, this is a hyper-parameter that we tuned using cross-validation. Intuitively, a larger \ud835\udf06 puts more emphasis on penalizing deviations from the prior, while a lower \ud835\udf06 emphasizes fitting the observations. We have experiments in Appendix C that explore this effect, e.g. the impact on the embedding distribution in Figure 4, and the impact on precision in Figure 5.\n\n6) In eq (1), \ud835\udc5b denotes the number of observed pairs (size of the training set). To simplify, we also define the Gramians as a sum over training examples, although in a recommendation setting, this can be rewritten as a sum over distinct users and distinct items. More precisely, if we let S be the set of users, and \ud835\udc53_s the fraction of training examples which involve user s, then \ud835\udc3a^\ud835\udc62=1/\ud835\udc5b \u2211_\ud835\udc56 \ud835\udc62_\ud835\udc56\u2297\ud835\udc62_\ud835\udc56 = \u2211_s\u2208S \ud835\udc53_s \ud835\udc62_s\u2297\ud835\udc62_s.\n\n7) We plan to open-source our TensorFlow implementation in the near future.\n\n[1] P. Neculoiu, M. Versteegh and M. Rotaru. Learning Text Similarity with Siamese Recurrent Networks. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016.\n[2] M. Volkovs, G. Yu, T. Poutanen. DropoutNet: Addressing Cold Start in Recommender Systems. NIPS 2017.\n[3] P. Covington, J. Adams, E. Sargin. Deep Neural Networks for YouTube Recommendations. Proceedings of the 10th ACM Conference on Recommender Systems (RecSys 2016).\n[4] B. Neyshabur and N. Srebro. On symmetric and asymmetric lshs for inner product search. ICML 2015.\n[5] A. Shrivastava and P. Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). NIPS 2014.\n[6] X. Zhang, F. X. Yu, S. Kumar, and S. Chang. Learning spread-out local feature descriptors. In IEEE International Conference on Computer Vision (ICCV 2017).\n[7] P. Vincent, A. de Brebisson, and X. Bouthillier. Efficient exact gradient update for training deep networks with very large sparse targets. In NIPS 2015.", "title": "Thank you for your comments"}, "H1ejQiWgsX": {"type": "rebuttal", "replyto": "rkeCV3lK5m", "comment": "Thank you for your comments, we will discuss each point below.\n\n1) The dot-product structure is important in many applications, especially in retrieval with very large corpora (since it allows efficient scoring using maximum-inner product search techniques [1, 2]). In addition to dot-product models, our methods can also be useful in more general architectures when used jointly with the Global Orthogonal regularizer proposed in [3], which \"spreads-out\" the embeddings by pushing the embedding distribution towards the uniform distribution. This was shown to improve generalization performance. In the last paragraph of Appendix C, we show that the Global Orthogonal regularizer can be written in terms of Gramians, thus our methods can be used in such models.\n\n2) Using non-uniform weights can be important, and it is supported by the methods we propose. They also support the use of a non-uniform sampling distribution, and non-uniform prior (as discussed in Appendix B). For non-uniform weights, if we define the weight of a left item i to be  \ud835\udc4e_\ud835\udc56  and the weight of a right item  \ud835\udc57  to be  \ud835\udc4f_\ud835\udc57 , and define the penalty term as  1/\ud835\udc5b^2 \u2211_\ud835\udc56 \u2211_\ud835\udc57 \ud835\udc4e_\ud835\udc56 \ud835\udc4f_\ud835\udc57 \u27e8\ud835\udc62_\ud835\udc56, \ud835\udc63_\ud835\udc57\u27e9^2, then one can show, using the same argument as in Section 2.2, that this is equal to the matrix inner-product \u27e8\ud835\udc3a^\ud835\udc62, \ud835\udc3a^\ud835\udc63\u27e9  where  \ud835\udc3a^\ud835\udc62, \ud835\udc3a^\ud835\udc63  are now weighted Gram matrices given by  \ud835\udc3a^\ud835\udc62 = 1/\ud835\udc5b \u2211_\ud835\udc56 \ud835\udc4e_\ud835\udc56 \ud835\udc62_\ud835\udc56\u2297\ud835\udc62_\ud835\udc56  and similarly for  \ud835\udc3a^\ud835\udc63 . One can then apply SAGram/SOGram to the weighted Gramians.\n\n3) It is our intention to open-source our TensorFlow implementation in the near future.\n\n[1] Behnam Neyshabur and Nathan Srebro. On symmetric and asymmetric lshs for inner product search. In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015).\n[2] Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). In Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS 2014).\n[3] Xu Zhang, Felix X. Yu, Sanjiv Kumar, and Shih-Fu Chang. Learning spread-out local feature descriptors. In IEEE International Conference on Computer Vision (ICCV 2017).", "title": "Non-uniform weights"}, "SJl1I_Zgi7": {"type": "rebuttal", "replyto": "rke9t6tEcm", "comment": "Thank you for your comments and for the suggestion.\nFirst, one can make a formal connection between the quality of Gramian estimates and the quality of gradient estimates.\nThe prior term can be written as  1/\ud835\udc5b \u2211_i \u27e8\ud835\udc62_\ud835\udc56, \ud835\udc3a^\ud835\udc63 \ud835\udc62_\ud835\udc56\u27e9 , thus the partial derivative w.r.t.  \ud835\udc62_\ud835\udc56  is  \u2207_\ud835\udc62\ud835\udc56 \ud835\udc54 = 2/\ud835\udc5b \ud835\udc3a^\ud835\udc63 \ud835\udc62_\ud835\udc56 . If the Gramian  \ud835\udc3a^\ud835\udc63  is approximated by \u011c^\ud835\udc63 , then the gradient estimation error is  2/\ud835\udc5b \u2211_i \u2016(\ud835\udc3a^\ud835\udc63\u2212 \u011c^\ud835\udc63) \ud835\udc62_\ud835\udc56\u2016^2 = 2/\ud835\udc5b \u2211_i \u27e8(\ud835\udc3a^\ud835\udc63 \u2212 \u011c^\ud835\udc63)\ud835\udc62_\ud835\udc56,(\ud835\udc3a^\ud835\udc63 \u2212 \u011c^\ud835\udc63)\ud835\udc62_\ud835\udc56\u27e9  which is equal to 2\u27e8(\ud835\udc3a^\ud835\udc63 \u2212 \u011c^\ud835\udc63),(\ud835\udc3a^\ud835\udc63 \u2212 \u011c^\ud835\udc63)\ud835\udc3a^\ud835\udc62\u27e9 , in other words, the estimation error of the right gradient is the \"\ud835\udc3a^\ud835\udc62 -weighted\" Frobenius norm of the left Gramian error.\nWe generated these plots as suggested, on Wikipedia simple, and we observe the same trend for the gradient estimation errors as the Gramian estimation error in Figure 2.a. We will include this experiment in the updated version of the paper during rebuttal. Thanks again for the suggestion.", "title": "Quality of gradient estimates"}}}