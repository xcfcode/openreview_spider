{"paper": {"title": "Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks", "authors": ["Haoran You", "Chaojian Li", "Pengfei Xu", "Yonggan Fu", "Yue Wang", "Xiaohan Chen", "Richard G. Baraniuk", "Zhangyang Wang", "Yingyan Lin"], "authorids": ["hy34@rice.edu", "cl114@rice.edu", "px5@rice.edu", "yf22@rice.edu", "yw68@rice.edu", "chernxh@tamu.edu", "richb@rice.edu", "atlaswang@tamu.edu", "yingyan.lin@rice.edu"], "summary": "", "abstract": "(Frankle & Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as Early-Bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 5.8x ~ 10.7x energy savings while maintaining comparable or even better accuracy as compared to the most competitive state-of-the-art training methods, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training.", "keywords": []}, "meta": {"decision": "Accept (Spotlight)", "comment": "This work studies small but critical subnetworks, called winning tickets, that have very similar performance to an entire network, even with much less training. They show how to identify these early in the training of the entire network, saving computation and time in identifying them and then overall for the prediction task as a whole. \n\nThe reviewers agree this paper is well-presented and of general interest to the community. Therefore, we recommend that the paper be accepted."}, "review": {"BJxIRropKH": {"type": "review", "replyto": "BJxsrgStvr", "review": "The paper empirically analyzed the wide existence of \"early-bird tickets\", e.g., the \"lottery tickets\" emerging and stabilizing in very early training stage. The potential connection to (Achille et al., 2019; Li et al.,2019) reads interesting. \n\nThe authors made several contributions in addition to the observation: (1) the EB tickets stay robust under large learning rates (while early stopping) and low-precision training; (2) the EB tickets can be detected using epoch-wise consecutive comparison (mask distance), rather than comparing with some oracle ticket; (3) the application of EB ticket towards energy efficient training, which is interesting as this is perhaps the first practical application demonstrated of lottery ticket. \n\nWhile I like how the paper connects theory hypothesis to real applications, the experiments need to be solidified in a few aspects:\n\n1) Figures 1 and 2, why a few plunges of curves (say Fig 2.a, p = 70%)? Does it imply the training might not be stable?\n\n2) Table 1, the authors test two lr schedules to show \"large learning rates favor the emergence of EB Tickets\". Yet the choice of lr matters a lot and can be tricky. Why the authors pick the two specific learning rate schedules? Why are they \"comparable\"? What if being more aggressive in choosing larger lr, say starting from lr= 0.5?\n\n3) The low precision EB ticket is not actually applied or evaluated in Section 4. It would have been interesting to see.\n\n4) I fail to find the 4.7 times energy saving as claimed in abstract from Table 2?\n", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 2}, "HkxzLrTjoB": {"type": "rebuttal", "replyto": "HkghlnSjjB", "comment": "Thanks for your careful review and feedback.\n\nQ1: \nSorry for the confusion. The FLOPs of all the pruned models (including EB Train) in the table consist of both winning ticket search and retraining costs, hence leading to higher FLOPs of the pruned NS and SFP models over the unpruned model.\n\nQ2: \nWe appreciate your suggestion and share your curiosity. Due to the limited time frame in rebuttal, we actually did not have enough time and resources to finish the experiments under 50% and 70% pruning. \n\nWe will continue our experiments and make sure to obtain those results to be updated in the final version, to verify your \"imagination\". We would also try the low-precision schemes (e.g., EB Train LL) and see if better accuracy-training FLOPs trade-offs could be achieved. ", "title": "Response to Reviewer#2"}, "B1xzRFWqjH": {"type": "rebuttal", "replyto": "rk5HxSNKr", "comment": "We thank the reviewer for the insightful comments. We have revised the manuscript and added the following response to address your concerns and questions:\n\nC1: ResNet and VGG on CIFAR-10/100 are popular benchmarks widely used in both latest \u201clottery ticket\u201d and efficient CNN training papers. Furthermore as requested, we add a group of experiments on ImageNet and ResNet18 show that our method translates to a harder dataset. As shown in the following table, we compare the retrain accuracy and training FLOPs of EB Train with those of the unpruned ResNet18, network slimming, and SFP on ResNet18 + ImageNet. \n\n---------------------------------------------------------------------------------------------------------------------------------------------\nMethods    Pruning ratio   Top-1 Acc.   Acc. Improv. (%)   Top-5 Acc.   Acc. Improv. (%)   Training FLOPs (P)\n---------------------------------------------------------------------------------------------------------------------------------------------\nUnpruned      \t -\t          69.57            \t-\t                89.24\t             -\t                         1259.13\nNS\t                       10%\t          69.65                +0.08\t                89.20\t         -0.04\t                 2424.86\nNS\t                       30%\t          67.85\t            -1.72\t                88.07\t         -1.17        \t         2168.89\nSFP\t                       30%\t          67.10\t            -2.47\t                87.78\t         -1.46\t\t         1991.94\nEB-Train\t               10%\t          69.84\t           +0.27\t                89.39               +0.15\t\t         1177.15\nEB-Train\t               30%\t          68.28\t            -1.29\t                88.28\t         -0.96\t\t         952.46\n---------------------------------------------------------------------------------------------------------------------------------------------\n\nThe above table shows that 1) when the pruning ratio is 10%, EB Train achieves a better accuracy (+0.27 vs. +0.08 over the unpruned one) while requiring 51% less total training FLOPs, as compared to the network slimming (NS) pruning method; 2) when the pruning ratio is 30%, EB Train results in a higher accuracy (-1.29 vs. -1.72(NS)/-2.47(SFP) over the unpruned one) while reducing the training FLOPs by 56% and 52%, as compared to the NS and SFP baselines. Note that \u201cAcc. Improv.\u201d in the above table is referred to that of the unpruned models. In the above table, the unpruned results are based on the official implementation (see https://github.com/facebookarchive/fb.resnet.torch; The SFP results are obtained from their original paper (see https://arxiv.org/pdf/1808.06866.pdf), which did not provide results at 10% pruning ratio; and the NS results are obtained by conducting the experiments ourselves.\n\nC2: Thank you for pointing out this and we agree with your comment. Also, we have conducted experiments to show the retrain accuracy of sub-networks drawn from different learning rate schedules. Please kindly see our response to Q2 of Reviewer 1.\n\nQ1: It is pruned after training for 1 epoch because Figure 1 plots the retrain accuracy of the sub-networks obtained by pruning after training for M epochs (M starts from 1). Sorry for not having made it clear enough. \nWhile sometimes the extracted networks after training for 1- 2 epochs might sometimes achieve good accuracy, they can be unstable. As an illustration, we have plotted error bars for all data points in Figures 1 and 2 based on three independent runs.\n\nQ2: Figure 5 shows the total training FLOPs of EB Train vs. different early-stop epochs as an \u201cablation\u201d example to show the effectiveness of EB Train. We apologize for the confusion. Meanwhile, we use Table 2 to illustrate the proposed mask distance-based method which automatically identifies the early stopping point, by benchmarking over the state-of-the-art designs.\n\nQ3: Please kindly find our response in the reply to Q1 of Reviewer 1. \n\nThank you for providing your kind suggestions on our writing! We have: 1) toned down our language in the revised manuscript; and 2) corrected the typos. We will carefully proofread our paper. ", "title": "Response to Reviewer#2"}, "r1lESd-qsr": {"type": "rebuttal", "replyto": "Bkx7VRIhFH", "comment": "Thanks for your careful review and positive feedback! We have addressed your comments in our revised manuscript as summarized below:\n\nwe have followed your kind suggestions to 1) remove the while condition (Max(Q) > eps) in algorithm 1 for making it more concise; and 2) plot d and invert the color bar in Figure 3 to better visualize the effectiveness of the proposed mask distance. \n\nWe have toned down the strong statement in the abstract. Furthermore, we have added experiments on the ImageNet dataset in Table 3 to more thoroughly validate our claim. ", "title": "Response to Reviewer#3"}, "HJedJdZ9jr": {"type": "rebuttal", "replyto": "BJxIRropKH", "comment": "Thanks for your careful review and comments and for appreciating our contributions! We have conducted more experiments and revised our paper to 1) address your comments and 2) improve the paper. Please find our itemized responses below. \n\nQ1: After the submission, we carefully revisited all experiments and found that the plunges were caused by the resulting over-pruning of the networks\u2019 certain layers when the target pruning ratio is high (e.g., 70%), due to the adopted global pruning scheme, i.e., enforcing to reach the target compression ratio in a network-wise instead of layer-wise manner. For example, pruning VGG-16 on CIFAR100 by 70% will lead to several layers of the resulting network to have less than 10 channels.\n\nFurthermore, we found that this \u201cplunge\u201d issue can be fixed by applying a simple \u201cprotective pruning\u201d heuristic on top of our current algorithm. Specifically, we stop pruning a layer, when its remaining channels after pruning become less than 10% of the original, for avoiding overly slimming this layer; meanwhile we (uniformly) prune other layers more to meet the overall pruning ratio. Experiments show that such a simple protective strategy can effectively eliminate the plunges as shown in the updated Figures 1-2, without incurring overhead. Note that this \u201cprotective pruning\u201d is only activated when there are over-pruned layers and thus affects only a few data points in the figures.\n\nQ2: we followed the common learning rate setting for drawing winning tickets (see Section 6 of https://arxiv.org/pdf/1810.05270.pdf). As you kindly suggested, here we show the retrain accuracy of sub-networks drawn from different learning rate schedules.\n\nInterestingly, a large initial lr of 0.5 and degraded schedule [80_{LR\\rightarrow0.05}, 120_{LR\\rightarrow0.005}] indeed works better for drawing EB tickets in VGG16 performed on CIFAR-10/100, leading to an earlier emergence of EB tickets and a higher retrain accuracy. This seems promising and is consistent with our hypothesis \u201clarge learning rates favor the emergence of EB Tickets.\u201d \n\n(VGG16@CIFAR10)\n---------------------------------------------\nInitial LR   Epoch EB drawn from\n\t\t  10       20       40\t final\n---------------------------------------------\n     0.1     93.26  93.34  93.20  92.96\n     0.5     93.49  93.45  93.44  93.29\n---------------------------------------------\n(VGG16@CIFAR100)\n---------------------------------------------\nInitial LR   Epoch EB drawn from\n\t\t  10       20       40\t final\n---------------------------------------------\n     0.1     71.11  71.07  69.14  69.74\n     0.5     70.69  71.65  71.94  71.58\n---------------------------------------------\n\nA similar observation is found when the initial lr is 0.2, in experiments on ResNet + CIFAR-10/100, although the schedule of initial lr 0.5 does not seem to further help EB tickets:\n\n(PreResNet101@CIFAR10)\n---------------------------------------------\nInitial LR   Epoch EB drawn from\n\t\t  10       20       40\t final\n---------------------------------------------\n     0.1    93.60  93.46  93.56  92.42\n     0.2    93.40  93.46  93.87  93.69\n---------------------------------------------\n\n(PreResNet101@CIFAR100)\n---------------------------------------------\nInitial LR   Epoch EB drawn from\n\t\t  10       20       40\t final\n---------------------------------------------\n     0.1    71.58  72.67  72.67  71.52\n     0.2    72.58  72.90  72.86  72.71\n---------------------------------------------\n\nGiven the aforementioned observation in experiments with various models and datasets (thanks to the Q2 comment from review#2), we revise the claim in the paper to a more accurate one as \u201cappropriate large learning rates can enable an earlier emergence of EB tickets\u201d. We will conduct more experiments with even larger lrs to find more insights on \u201clr vs. emergence time of EB tickets\u201d and update in the camera ready version.\n\nQ3: Indeed, applying low-precision EB tickets on top of low/full precision retraining is more interesting as it can achieve more energy savings while maintaining a comparable retrain accuracy. As you kindly suggested, we add two sets of corresponding experiment results into Table 2: 1) EB Train with low precision search and full precision retrain (EB-Train LF) and 2) EB Train with both low precision search and retrain. Specifically, the resulting energy savings and training FLOPs are 5.8-24.6x and 1.1-5.0x over the baseline competitors.\n\nQ4: the 4.7x energy savings can be found in Table 2\u2019s experiment on PreResNet-101@CIFAR100: when the pruning ratio is 70%, the energy savings of EB-Train compared to the lottery ticket (one-shot) baseline is 6095/1294 (\\approx 4.7x). Note that additional experiments as responding to your Q3 comment show that EB Train (with both low precision search and retrain) can lead up to 24.6x energy savings. \n", "title": "Response to Reviewer#1"}, "rk5HxSNKr": {"type": "review", "replyto": "BJxsrgStvr", "review": "This paper presents a method to speed up training of deep neural networks. The main contribution is a method to quickly identify winning lottery tickets (denoted early-bird, or EB by the authors), without running the model to convergence. The authors present interesting preliminary experiments that motivate their method, and show that it works on two image recognition datasets using two models.\n\nThis paper addresses an under-explored, but very important problem in AI: the increasing cost of training models. The authors present interesting evidence about the potential to detect EBs early on. The experiments presented in Figures 1 and 3 are convincing and will be of interest to the community. The proposed method seems to work, at least on the setups explored by the authors. I am leaning towards acceptance, but am concerned with the following: \n\n1. The authors experiment with a limited set of datasets (CIFAR-10 is a relatively easy task), and with a set of non-competitive baselines (SOTA for CIFAR-10/100 is 99%/91.3%, see https://benchmarks.ai/cifar-10{,0}). I would have liked to see whether the proposed method translates to harder datasets and stronger models.\n\n2. I might be missing something here, but to the best of my understanding the large learning rate part (page 4) does not demonstrate the benefits of increasing the learning rate, but the problems with *decreasing* it. The two might seem like the same thing, but in fact they're not: the authors claim the [80,120] policy is standard, and use it when training the subnetwork, so showing that [0,100] is inferior does not present a way to improve over the current approach, but evidence that the other approaches are inferior.\n\nOther questions: \n1. In Figure 1, it seems that the extracted subnetworks are doing very well even after 0 epochs. Does this mean that a trained version of a random subnetwork could reach within 1-2 points of the unpruned model? or is it pruned after training for 1 epoch?\n\n2. If I understand correctly, Figure 5 should be illustrating the proposed method, which automatically identifies the early stopping point. In that case, I am not sure why the plot is a function of the epoch.\n\n3. Do the authors have any intuition as to the sharp decrease in the 70% graph in Figures 1 and 2 around epoch 50?\n\nWriting: \n\n1. The language used by the authors is sometimes exaggerated. Expressions such as \"bold guess\" (section 3.2), \"innovative ... scheme\" (section 4) and comparisons to Winston Churchill would be better left out of the manuscript. \n\n2. Typos and such: \n-- several across the paper. For instance: \n- Intro: After *bring* identified (should be \"being\")\n- Related work: when training *it* isolation (in)\n\n-- Missing venue for Frankle and Corbin (2019)\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "Bkx7VRIhFH": {"type": "review", "replyto": "BJxsrgStvr", "review": "The authors further study the lottery ticket hypothesis formulated by Frankle and Carbin. They demonstrate that the sparsity pattern corresponding to a lottery ticket for a given initialization can be uncovered via low-cost training. By doing so, they propose a method to: 1) first identify the lottery ticket efficiently and 2) exploit the sparsity of the resulting network to train it at a lower cost.\n\nThe contribution is however a bit incremental in my opinion. The original LT paper was not focused on efficiency, and it is not a far stretch to try to find the tickets sooner during the training. On the other hand, the experiments are well conducted (especially 4.3) and even if the original idea is simple, it is of interest to see it tested as clearly. All in all, I found this paper convincing and worth reading, and I think it should be accepted.\n\nPositive points:\n- The literature review is sufficient and present with great clarity the latest results.\n- The problem tackled is of great interest and has potentially impactful applications.\n- The authors focus on hardware friendly types of pruning.\n- The paper is well written and enjoyable.\n- The algorithm used to compute the EB tickets seems a bit ad hoc, but in my opinion sufficient as a first approach.\n\n\nNitpicking:\n- Not sure why Max(Q) > eps is in the while condition (return if Max(Q) < eps should be sufficient)\n- The treatment of the mask distance in figure 3 is confusing. It is not obvious why the authors are plotting 1-distance, and the legend of the figure suggest that a mask has a distance of 1 with itself. Recommend plotting d instead of 1-d and invert the color bar instead if they feel so inclined (yellow=0).\n- Abstract: \u201cconsistently exist across models and datasets\u201d a bit of a strong claim as only cifar is used.\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}}}