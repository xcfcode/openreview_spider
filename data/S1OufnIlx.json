{"paper": {"title": "Adversarial examples in the physical world", "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"], "summary": "", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "keywords": ["Supervised Learning", "Computer vision"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "This paper studies an interesting aspect of adversarial training with important practical applications: its robustness against transformations that correspond to physical world constraints. The paper demonstrates how to construct adversarial examples such that they can be used to attack a machine-learning device through a physical interface such as a camera device. \n \n The reviewers agreed that this is a well-written paper which clearly describes its contributions and offers a detailed experimental section. The authors took into account the reviewer's concerns and the rebuttal phase offered interesting discussions. \n \n In light of the reviews, the main critique of this work is its lack of significance, relative to existing works in adversarial examples. The authors, however, did a good job during the rebuttal phase to highlight the empirical nature of the work and the potential practical significance of their findings in the design of ML models. The AC concludes that the potential practical applications, while not significant enough to be part of the conference proceedings, are worthy to be disseminated in the workshop. I therefore recommend submitting this work to the workshop track."}, "review": {"B1fLrJnUx": {"type": "rebuttal", "replyto": "rJ4Lg0bNe", "comment": "Thanks for the review.\n\nIt's worth noting that the two papers mutually \"scooped\" each other.\nWhile Sharif et al submitted to a conference earlier, our work has been publicly available longer and has more citations.\nAccording to Google scholar on Jan 17, 2017, our article has been cited 9 times ( https://scholar.google.com/scholar?cites=263531058904899909 ), while that of Sharif et al has been cited 4 times ( https://scholar.google.com/scholar?cites=10006479087737690195 ). One of these 4 citations to Sharif et al is the citation we added in November.", "title": "Reply to the review"}, "SyzXr128x": {"type": "rebuttal", "replyto": "rk2M7kzNe", "comment": "Thanks for the review.\nWe would like to address some of the cons you mentions.\n\n\nWe acknowledge that there are existing papers which describe various adversarial attacks (including white-box and black box attacks) on various machine learning systems (including neural networks).\nHowever we would like to emphasize that the main difference of our work and prior work is the fact that the attacker in prior work has direct access to inputs of the classifier thus can fine-tune all input values, while in our work we have shown that adversarial attacks are possible even when data is perceived through the sensors (like a camera). Overall this observation is important for practical machine learning systems operating in real world.\nAs we mentioned in the paper, the closest work to ours is Sharif et al. However their work has somewhat different scope and also became publicly available after our paper.\n\n\n\nRegarding whether the size of perturbation is small or not.\nWe agree that there is no good quantitative threshold assessing whether a perturbation is small or not and this is a very subjective measure. In this sense our primary criterion was whether the perturbation was large enough to interfere with human recognition of the image.\n\nAlso we have run additional experiments with eps=2 and 4 (results are added to Tables 1, 2 and 3 in the paper). These experiments are showing the same trend as experiments with higher epsilon - adversarial images may remain misclassified after \u201cphoto transformation\u201d. In particular, for eps=2 and 4 about half of \u201cFast\u201d adversarial examples remain misclassified after \u201cphoto transformation\u201d (see destruction rate in Table 3).\n\nArguably adversarial examples with eps=4 and 2 are small enough to be hard to notice (examples of adversarial image with eps=4 can be found in Figure 5). So hopefully the fact that physical adversarial examples exist even for such small epsilon should address your concern.\n\n\n\nRegarding \u201cSome hypotheses proposed in the paper based on one-shot experiments seems too rushy,\u201d we\u2019d be happy to provide further support for any particular hypothesis you think is invalid.\n\n\n\nRegarding \u201cthe results of this paper seems not really improving the understanding of the adversarial example phenomenon\u201d: this paper is an empirical paper and not a theoretical paper, but we\u2019ve found important empirical observations that theory should aim to explain, such as the reduced transferability of iterative adversarial examples.\nIn addition the paper helps to advance research on adversarial examples by showing that they also exist in the physical world.\n", "title": "Reply to the review"}, "ryf0NkhIe": {"type": "rebuttal", "replyto": "SyapBiWNe", "comment": "Thanks for the review.\n\nWe would like to address the significance of the paper.\nWe agree that we did not propose radically novel methods or explanations of the adversarial example problem.\nAt the same time the observations made in this paper are significant for practical ML applications because they show how real world machine learning systems are susceptible to adversarial attacks similar to \u201cdigital-only\u201d machine learning systems.\n\nThus the novelty and significance of this paper is in uncovering a new risk for practical real-world machine learning systems. Or in other words it could be considered as a security paper with a new vulnerability disclosed.", "title": "Reply to the review"}, "ByAd0I0me": {"type": "rebuttal", "replyto": "ryMXRcJ7e", "comment": "Yes, you are correct that this method is not guaranteed to cause a\nmisclassification. We do not claim that it is guaranteed to cause a\nmisclassification. Indeed, we report the accuracy of the model on such\nadversarial examples.\n\nMain motivations of Iter L.L. method were following.\nFirst of all, to find strong adversarial method which would produce non-trivial misclassification, as opposed to fast method which tend to produce less interesting misclassification (like one breed of dog confused with another breed of dog).\nSecond, to keep computational cost manageable.\n\nIter L.L. method achieved both of those goals.\nIn addition we observed that for large enough \\epsilon this method actually was able to force class label to become least likely class. In particular for \\epsilon = 16 more than 90% of all adversarial examples were classified as least likely class y_{LL}.\nFor \\epsilon=8 about 80% of all adversarial images were classified as y_{LL}\n\nAdv_Alpha and Adv_Loss from [1] are shown to produce strong adversarial examples as could be seen from Figure 1 from that paper.\nHowever these two methods are not suitable for our needs for the following reasons:\n1. Both of them are using L_2 norm to construct adversarial examples. Which means that adversarial examples would have fractional pixel values when converted to range [0, 255]. Given that we\u2019re saving and printing adversarial images, all fractional values will be rounded up, and this would lead to decrease of quality of adversarial examples.\nSection 2 of [1] mentions how to use Adv_Alpha method with L_{\\infty} loss, however accuracy on such adversarial examples was not reported in [1]\n2. Adv_Alpha method required to compute Jacobian matrix (i.e. compute gradient for each class) and then solve linear equation for each of class. When dataset has a lot of classes(ImageNet has 1000 distinct classes) this become prohibitively expensive from the computational point of view. On the other iterative least likely class from our paper need to compute gradient only up to 20 times.\n\n\n> Also, what is the actual perturbation of the basic iterative method in the results of Figure 2? When \\epsilon is large, the actual perturbation may be much smaller than \\epsilon.\n\nFor epsilon from 2 to 16 (which we have used in our experiments) L_{\\infty} norm of perturbation was equal or close to actual value of the epsilon. \nFor examples, for \\epsilon=8, L_{\\infty} norm of adversarial perturbation was 8 for all images from validation set. For \\eps=16 more than 98% of generated adversarial perturbations had L_{\\infty} norm equal to 16.\n\nIf you visually look at adversarial images produced by Fast and Iter L.L methods with same epsilon you might have an impression that perturbation of Iter L.L. adversarial images is smaller compared to Fast adversarial images.\nThis is explained by the fact that Fast method modifies all pixels of the image by the same absolute value (but with different sign). At the same time iterative methods are modifying pixels by different value. And only one pixel has to be modified by epsilon for L_{\\infy} norm to be equal epsilon.", "title": "Reply to reviewer's question"}, "SJB2a8Cml": {"type": "rebuttal", "replyto": "SkhI1qP7x", "comment": "> Can you provide more details about this timeline?\n\nOur work appeared on arXiv on 8 July 2016, see timestamp of the first version at https://arxiv.org/abs/1607.02533 \n\nSharif's work was submitted to the CCS2016 conference, which had submission deadline 23 May 2016 and acceptance notification on 22 July 2016 (see https://www.sigsac.org/ccs/CCS2016/call-for-papers/ ).\n\nIt seems that they made their work publicly available only in the beginning of November ( https://www.reddit.com/r/hackernews/comments/5auig2/real_and_stealthy_attacks_on_stateoftheart_face/ ). At that time we have learned about it and decided that we should add a citation.\n\n> Other than the three-point description of differences, how does this work differ from the earlier conference submission?\n\n\tThe two works are somewhat different in spirit.\n\tThe goal of our work was to test whether existing forms of adversarial examples have an effect when displayed in the physical world. The goal of the work by Sharif et al was to develop a practical attack against a face recognition system.\n\tSharif et al developed an approach to printing out paper glasses frames that could be attached to real glasses. The design on the paper glasses is chosen to fool a face recognition system. In this case, the goal was primarily to demonstrate that this specific exploit was possible. The number of classes in the machine learning system is small (it can recognize the co-authors of the paper and a few celebrities) and some new development of adversarial example construction techniques was necessary to make the attack viable using only glasses frames.\n\tIn our work, the goal is to measure how well existing adversarial example techniques transfer to the real world. This meant that our focus was on evaluating large numbers of adversarial examples from large numbers of categories (the 1,000 imagenet categories) rather than on developing new adversarial example construction techniques.\n", "title": "Reply to reviewer's question"}, "SkhI1qP7x": {"type": "review", "replyto": "S1OufnIlx", "review": "The submission says, \"The most similar work to this paper is Sharif et al. (2016), which appeared publicly after our work but had been submitted to a conference earlier.\" Can you provide more details about this timeline? Other than the three-point description of differences, how does this work differ from the earlier conference submission?In some sense, the Sharif et al. work \"scooped\" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities). The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study.", "title": "expanding on the Sharif et al. connection", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJ4Lg0bNe": {"type": "review", "replyto": "S1OufnIlx", "review": "The submission says, \"The most similar work to this paper is Sharif et al. (2016), which appeared publicly after our work but had been submitted to a conference earlier.\" Can you provide more details about this timeline? Other than the three-point description of differences, how does this work differ from the earlier conference submission?In some sense, the Sharif et al. work \"scooped\" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities). The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study.", "title": "expanding on the Sharif et al. connection", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ryMXRcJ7e": {"type": "review", "replyto": "S1OufnIlx", "review": "Based on my understanding, no matter how large \\epsilon is, the method proposed in section 2.3 can not guarantee that the adversarial sample will be misclassified as y_{LL}. Actually, it can't even guarantee that the image will be misclassified, no matter how large \\epsilon is. See the discussion in Proposition 2 of the paper of Huang et al. [1] I am wondering if the target label actually appears in your experiments. Also, is this effect taken into account in the experiment settings? On the other hand, if the motivation of the LL method is to find a strong adversarial method (as shown in Figure 2), the alpha method in [1] may be a better one.\n\nAlso, what is the actual perturbation of the basic iterative method in the results of Figure 2? When \\epsilon is large, the actual perturbation may be much smaller than \\epsilon. \n\n[1] Huang, R., Xu, B., Schuurmans, D., & Szepesv\u00e1ri, C. (2015). Learning with a strong adversary. CoRR, abs/1511.03034.The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. \n\nPros:\n1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.\n2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. \n\nCons:\n1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration (the real pictures), it is arguable that the perturbations in the experiments are still small.\n2. Some hypotheses proposed in the paper based on one-shot experiments seems too rushy.\n3. As mentioned above, the results of this paper seems not really improving the understanding of the adversarial example phenomenon.", "title": "About the least-likely class method and the basic iterative method", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rk2M7kzNe": {"type": "review", "replyto": "S1OufnIlx", "review": "Based on my understanding, no matter how large \\epsilon is, the method proposed in section 2.3 can not guarantee that the adversarial sample will be misclassified as y_{LL}. Actually, it can't even guarantee that the image will be misclassified, no matter how large \\epsilon is. See the discussion in Proposition 2 of the paper of Huang et al. [1] I am wondering if the target label actually appears in your experiments. Also, is this effect taken into account in the experiment settings? On the other hand, if the motivation of the LL method is to find a strong adversarial method (as shown in Figure 2), the alpha method in [1] may be a better one.\n\nAlso, what is the actual perturbation of the basic iterative method in the results of Figure 2? When \\epsilon is large, the actual perturbation may be much smaller than \\epsilon. \n\n[1] Huang, R., Xu, B., Schuurmans, D., & Szepesv\u00e1ri, C. (2015). Learning with a strong adversary. CoRR, abs/1511.03034.The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. \n\nPros:\n1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.\n2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. \n\nCons:\n1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration (the real pictures), it is arguable that the perturbations in the experiments are still small.\n2. Some hypotheses proposed in the paper based on one-shot experiments seems too rushy.\n3. As mentioned above, the results of this paper seems not really improving the understanding of the adversarial example phenomenon.", "title": "About the least-likely class method and the basic iterative method", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}