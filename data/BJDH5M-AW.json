{"paper": {"title": "Synthesizing Robust Adversarial Examples", "authors": ["Anish Athalye", "Logan Engstrom", "Andrew Ilyas", "Kevin Kwok"], "authorids": ["aathalye@mit.edu", "engstrom@mit.edu", "ailyas@mit.edu", "kevink16@gmail.com"], "summary": "We introduce a new method for synthesizing adversarial examples robust in the physical world and use it to fabricate the first 3D adversarial objects.", "abstract": "Neural network-based classifiers parallel or exceed human-level accuracy on many common tasks and are used in practical systems. Yet, neural networks are susceptible to adversarial examples, carefully perturbed inputs that cause networks to misbehave in arbitrarily chosen ways. When generated with standard methods, these examples do not consistently fool a classifier in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations. Adversarial examples generated using standard techniques require complete control over direct input to the classifier, which is impossible in many real-world systems.\n\nWe introduce the first method for constructing real-world 3D objects that consistently fool a neural network across a wide distribution of angles and viewpoints. We present a general-purpose algorithm for generating adversarial examples that are robust across any chosen distribution of transformations. We demonstrate its application in two dimensions, producing adversarial images that are robust to noise, distortion, and affine transformation. Finally, we apply the algorithm to produce arbitrary physical 3D-printed adversarial objects, demonstrating that our approach works end-to-end in the real world. Our results show that adversarial examples are a practical concern for real-world systems.\n", "keywords": ["adversarial examples"]}, "meta": {"decision": "Reject", "comment": "This paper studies the problem of synthesizing adversarial examples that will succeed at fooling a classification system under unknown viewpoint, lighting, etc conditions. For that purpose, the authors propose a data-augmentation technique (called \"EOT\") that makes adversarial examples robust against a predetermined family of transformations.\n\nReviewers were mixed in their assessment of this work, on the one hand highlighting the potential practical applications, but on the other hand warning about weak comparisons with existing literature, as well as lack of discussion about how to improve the robustness of the deep neural net against that form of attacks.\nThe AC thus believes this paper will greatly benefit from a further round of iteration/review, and therefore recommends rejection at this time. "}, "review": {"H1FaG8FgM": {"type": "review", "replyto": "BJDH5M-AW", "review": "Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network.\nRather than optimizing the log-likelihood of a single example, the optimization if performed over a the expectation of a set of transformations of sample images. Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%. They also create two 3D printed objects which most pictures taken from random viewpoints are fooling the network in its class prediction.\n \n\nMain comments:\n- The idea of building 3D adversarial objects is novel so the study is interesting. However, the paper is incomplete, with a very low number of references, only 2 conference papers if we assume the list is up to date. \nSee for instance Cisse et al. Houdini: fooling Deep Structured Prediction Models, NIPS 2017 for a recent list of related work in this research area.\n- The presentation of the results is not very clear. See specific comments below.\n- It would be nice to include insights to improve neural nets to become less sensitive to these attacks.\n\n\nMinor comments:\nFig1 : a bug with color seems to have been fixed\nModel section: be consistent with the notations. Bold everywhere or nowhere\nResults: The tables are difficult to read and should be clarified:\nWhat does the l2 metric stands for ? \nHow about min, max ?\nAccuracy -> classification accuracy\nModels -> 3D models\nDescribe each metric (Adversarial, Miss-classified, Correct)\n", "title": "Interesting idea but needs a major revision", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyRDNVqxz": {"type": "review", "replyto": "BJDH5M-AW", "review": "The authors present a method to enable robust generation of adversarial visual\ninputs for image classification.\n\nThey develop on the theme that 'real-world' transformations typically provide a\ncountermeasure against adversarial attacks in the visual domain, to show that\ncontextualising the adversarial exemplar generation by those very\ntransformations can still enable effective adversarial example generation.\n\nThey adapt an existing method for deriving adversarial examples to act under a\nprojection space (effectively a latent-variable model) which is defined through\na transformations distribution.\n\nThey demonstrate the effectiveness of their approach in the 2D and 3D\n(simulated and real) domains.\n\nThe paper is clear to follow and the objective employed appears to be sound. I\nlike the idea of using 3D generation, and particularly, 3D printing, as a means\nof generating adversarial examples -- there is definite novelty in that\nparticular exploration for adversarial examples.\n\nI did however have some concerns:\n\n1. What precisely is the distribution of transformations used for each\n   experiment? Is it a PCFG? Are the different components quantised such that\n   they are discrete rvs, or are there still continuous rvs? (For example, is\n   lighting discretised to particular locations or taken to be (say) a 3D\n   Gaussian?) And on a related note, how were the number of sampled\n   transformations chosen?\n\n   Knowing the distribution (and the extent of it's support) can help situate\n   the effectiveness of the number of samples taken to derive the adversarial\n   input.\n\n2. While choosing the distance metric in transformed space, LAB is used, but\n   for the experimental results, l_2 is measured in RGB space -- showing the\n   RGB distance is perhaps not all that useful given it's not actually being\n   used in the objective. I would perhaps suggest showing LAB, maybe in\n   addition to RGB if required.\n\n3. Quantitative analysis: I would suggest reporting confidence intervals;\n   perhaps just the 1st standard deviation over the accuracies for the true and\n   'adversarial' labels -- the min and max don't help too much in understanding\n   what effect the monte-carlo approximation of the objective has on things.\n\n   Moreover, the min and max are only reported for the 2D and rendered 3D\n   experiments -- it's missing for the 3D printing experiment.\n\n4. Experiment power: While the experimental setup seems well thought out and\n   structured, the sample size (i.e, the number of entities considered) seems a\n   bit too small to draw any real conclusions from. There are 5 exemplar\n   objects for the 3D rendering experiment and only 2 for the 3D printing one.\n\n   While I understand that 3D printing is perhaps not all that scalable to be\n   able to rattle off many models, the 3D rendering experiment surely can be\n   extended to include more models? Were the turtle and baseball models chosen\n   randomly, or chosen for some particular reason? Similar questions for the 5\n   models in the 3D rendering experiment.\n\n5. 3D printing experiment transformations: While the 2D and 3D rendering\n   experiments explicitly state that the sampled transformations were random,\n   the 3D printing one says \"over a variety of viewpoints\". Were these\n   viewpoints chosen randomly?\n\nMost of these concerns are potentially quirks in the exposition rather than any\nissues with the experiments conducted themselves. For now, I think the\nsubmission is good for a weak accept \u2013- if the authors address my concerns, and/or\ncorrect my potential misunderstanding of the issues, I'd be happy to upgrade my\nreview to an accept.", "title": "Review - Accept pending clarifications", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1nifI9eG": {"type": "review", "replyto": "BJDH5M-AW", "review": "The paper proposes a method to synthesize adversarial examples that remain robust to different 2D and 3D perturbations. The paper shows this is effective by transferring the examples to 3D objects that are color 3D-printed and show some nice results.\n\nThe experimental results and video showing that the perturbation is effective for different camera angles, lighting conditions and background is quite impressive. This work convincingly shows that adversarial examples are a real-world problem for production deep-learning systems rather than something that is only academically interesting.\n\nHowever, the authors claim that standard techniques require complete control and careful setups (e.g. in the camera case) is quite misleading, especially with regards to the work by Kurakin et. al. This paper also seems to have some problems of its own (for example the turtle is at relatively the same distance from the camera in all the examples, I expect the perturbation wouldn't work well if it was far enough away that the camera could not resolve the HD texture of the turtle).\n\nOne interesting point this work raises is whether the algorithm is essentially learning universal perturbations (Moosavi-Dezfooli et. al). If that's the case then complicated transformation sampling and 3D mapping setup would be unnecessary. This may already be the case since the training set already consists of multiple lighting, rotation and camera type transformations so I would expect universal perturbations to already produce similar results in the real-world.\n\nMinor comments:\nSection 1.1: \"a affine\" -> \"an affine\"\nTypo in section 3.4: \"of a of a\"\nIt's interesting in figure 9 that the crossword puzzle appears in the image of the lighthouse.\n\nMoosavi-Dezfooli, S. M., Fawzi, A., Fawzi, O., & Frossard, P. Universal adversarial perturbations. CVPR 2017.", "title": "Persuasive real-world results, would benefit from a comparison to universal examples", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJnDHwnQf": {"type": "rebuttal", "replyto": "BJDH5M-AW", "comment": "We thank the anonymous reviewers for helping us improve the paper. In response to their feedback, we have made the following revisions to our paper (in addition to the fixing of a few spelling mistakes/typos):\n\n* Related work: We have updated our description of Kurakin et al. and added a comparison with Universal Adversarial Perturbations (Moosavi-Dezfooli et al.) and adversarial eyeglasses (Sharif et al.)\n\n* Evaluation: We have included an additional 5 models in our Robust 3D Adversarial Examples evaluation. We have additionally further explained our evaluation metrics, improved and defined previously confusing terminology, and added standard deviations to further elucidate the distributions of adversariality and classification accuracy.\n", "title": "Update"}, "SkwQmwnXM": {"type": "rebuttal", "replyto": "H1FaG8FgM", "comment": "Thank you for your review. In the latest revision of our paper we greatly expand on the related work section, both discussing in more detail our current list, and introducing other related works from the field which we explain and differentiate from our own. We hope that this gives the reader a more complete view of the field, and further indicates the novelty of our work.\n\nThe focus of this work was to demonstrate that it is possible to construct transformation-tolerant adversarial examples, even in the physical world; defenses against adversarial examples are beyond the scope of this paper. We hesitate to present intuitions for defenses without rigorous experimentation, because as researchers like Carlini have shown, developing defenses is challenging, and many proposed ideas for defenses are easily defeated [1].\n\nWe have addressed all of the minor comments including:\n* Fixing the color bug\n* Removing the selected bolding from the model section\n* Elaborated and defined the l2 metric, and removed min/max in favor of mean/stdev\n* Models -> 3D models and accuracy -> Classification accuracy\n* Added a paragraph defining the terms \u201cadversarial,\u201d \u201cmisclassified,\u201d and \u201ccorrect\u201d as we use them\n\n[1]: https://arxiv.org/abs/1705.07263\n", "title": "Thank you"}, "Skul7w3mz": {"type": "rebuttal", "replyto": "SyRDNVqxz", "comment": "Thank you for your review. We have made several clarifications in the exposition that we believe address your concerns and improve the paper. In particular:\n\n1. The parameters of the distribution used in generating examples was given in the Appendix, but the method by which they are sampled was not made clear; we now explicitly state in the evaluation section that the parameters are sampled as independent uniformly distributed continuous random variables (except for Gaussian noise, which is sampled as a Gaussian continuous RV). There was no fixed number of transformations chosen during the synthesis of the adversarial example: the transformations are independently sampled at each gradient descent step. We have updated the text in the approach section to clarify this.\n\n2. Yes, we agree: we minimized LAB, not RGB, and Euclidean distances make more sense in a perceptually uniform color space like LAB. We have switched to reporting LAB distances.\n\n3. While we gave the distribution of adversariality across examples in a graph the appendix, we did not explicitly state the standard deviation/confidence intervals. This has been resolved in the latest version. We have also removed the min and max metrics from the evaluation section, and have added the standard deviation over the accuracies for the true and \u2018adversarial\u2019 labels as suggested. We report mean/standard deviation for 2D and rendered 3D experiments and not the 3D printing experiment because we report the statistics for each 3D objects separately.\n\n4. In the case of the 3D printing experiment, we were limited by printer capability and shipping feasibility for this revision, but would be happy to include a few more in the camera ready version. We also included 5 more models in the 3D rendering experiment, making a total of 200 adversarial examples (10 models, 20 randomly chosen targets for each model). The turtle and baseball models were chosen because they could be easily adapted for the 3D printing process. The adversarial targets for the turtle and baseball (as well as all our other experiments) were randomly chosen across all the eligible ImageNet classes. Models for the 3D simulation experiment were chosen based on the first 10 realistic, textured 3D models we could find in OBJ format.\n\n5. We have added a footnote to address this concern; although the viewpoints were not selected or cherry-picked in any capacity, we opt to not call them \u201crandom\u201d because in contrast to the 2D and 3D virtual examples, the viewpoints were not (and realistically could not have been) uniformly sampled from some concrete distribution of viewpoints; instead the objects were repeatedly moved and rotated on a table with humans walking around them and taking pictures from \u201ca variety of viewpoints.\u201d\n", "title": "Thank you"}, "H1B9fD3Qf": {"type": "rebuttal", "replyto": "H1nifI9eG", "comment": "Thank you for the review and detailed comments. We are glad you enjoyed the paper. We have made revisions to the related work section, including a clearer description of Kurakin et al. and a more thorough discussion of other works (including the suggested \u201cUniversal Perturbations\u201d paper by Moosavi-Dezfooli et al). We have additionally fixed all the minor issues you pointed out.", "title": "Thank you"}, "BJ2bfP3QM": {"type": "rebuttal", "replyto": "rk4nKr_0b", "comment": "Our updated paper includes a revised abstract and related work that takes into account your feedback. We hope this clarifies our explanation of the work of Kurakin et al. Please let us know if you have any other feedback on the related work or the ideas presented in the rest of the paper, and we will take it into account before the next deadline.", "title": "Paper updated"}, "HymvZv37G": {"type": "rebuttal", "replyto": "r1bnlcZMf", "comment": "Thank you for taking the time to reproduce the results in our paper! We\u2019re glad that you were able to replicate our results.\n\nWe assume that you did not reproduce our 3D results due to a lack of an openly available differentiable renderer (it is somewhat of a pain to implement). We\u2019ll do our best to have ours open-sourced by the time of the conference.\n\nIt\u2019s interesting to see that there\u2019s some degree of transferability between these EOT adversarial examples as well; we hadn\u2019t explored this much in our work. If you do explore this further, or try new things like optimizing over an ensemble, please let us know how it goes!\n", "title": "Re: Reproducibility Report"}, "Byp5w7OAW": {"type": "rebuttal", "replyto": "B1e1yJLCb", "comment": "Thank you for the feedback. We\u2019ve made the following changes for the revised version:\n\n1. To resolve the misunderstanding caused by the term \u201ccontrolled,\u201d we\u2019ve removed the word and, instead, directly stated the experimental conditions of Kurakin et al., as described in their Section 3.2 Experimental Setup. In their setup, a photo is taken of the image, then warped so that \u201ceach example has known coordinates\u201d using printed QR codes, and cropped \u201cso that they would become squares of the same size as source images.\u201d We also describe the attached video directly as \u201capproximately axis-aligned.\u201d We write our analysis of Kurakin et al. based on the distribution of transformations described in the latest version of the peer-reviewed ICLR 2017 Workshop paper. Please advise if a better description of the setup is available. Note that our method covers any differentiable transformation, and our 2D experiments cover noise and lightening/darkening, as well as rotation, skew, translation, and zoom, the three of which are not covered in Kurakin et al. (our 3D experiments cover even more). We are sorry for the misunderstanding.\u2028\n\n2. We will remove the world \u201conly\u201d and mention exactly the setup described in Kurakin et al as described above. We did experiments and they indicated that the Kurakin et al. method fails under a combination of rescaling, rotation, and translation (e.g. as used in the distribution used in our 2D case, see Table 4 in the Appendix); However, because robustness to such transformations was never claimed in Kurakin et al., we decided to not include these findings in our paper. \u2028\n\n3. Our work states that in general, adversarial examples fail to transfer because of the combination of \u201cviewpoint shifts, camera noise, and other natural transformations.\u201d The degrees to which each of these transformations contribute was not studied, nor were any related claims made. We will make this more explicit in our revised version.\u2028\n\nWe hope that these edits, in addition to the other differentiations already stated in the paper (untargeted vs targeted adversarial attacks, and 2D vs 3D examples) now appropriately represent the difference between Kurakin et al. and this work. We welcome additional feedback, and we thank you again for helping us improve our writeup.", "title": "Clarification"}, "HJb4TgKAb": {"type": "rebuttal", "replyto": "B1p3Q4_Rb", "comment": "EOT produces examples that are robust under the chosen distribution; it does not promise anything about out-of-distribution samples. We demonstrate that our adversarial examples work over varying levels of zoom (in addition to other transformation) in both the 2D and 3D cases: see our Appendix for the exact parameters we chose.\n\nOur research focuses on classifiers. We did not try attacking YOLO or Fast-RCNN in this work. However, given that detectors use a pretty similar architecture (and basically re-use the classifier, like VGG-16), we expect that it wouldn't be very different to attack a detector.\n", "title": "Re: Robustness of Perturbation"}, "BkwU6xF0b": {"type": "rebuttal", "replyto": "rk4nKr_0b", "comment": "Yes, we've disambiguated it to mean the combination of the natural transformations as you suggest. Thanks again for your feedback, and please let us know if you see anything else we can improve.", "title": "Disambiguated"}, "rJkwuN9RZ": {"type": "rebuttal", "replyto": "Bk-AZjY0W", "comment": "Thanks! We think it would be neat to work on extending this to black-box systems and systems deployed in the real world.", "title": "Re: Future work"}}}