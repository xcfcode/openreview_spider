{"paper": {"title": "PA-GAN: Improving GAN Training by Progressive Augmentation", "authors": ["Dan Zhang", "Anna Khoreva"], "authorids": ["dan.zhang2@de.bosch.com", "anna.khoreva@de.bosch.com"], "summary": "We introduce a new technique - progressive augmentation of GANs (PA-GAN) - that helps to improve the overall stability of GAN training.", "abstract": "Despite recent progress, Generative Adversarial Networks (GANs) still suffer from training instability, requiring careful consideration of architecture design choices and hyper-parameter tuning. The reason for this fragile training behaviour is partially due to the discriminator performing well very quickly; its loss converges to zero, providing no reliable backpropagation signal to the generator. In this work we introduce a new technique - progressive augmentation of GANs (PA-GAN) - that helps to overcome this fundamental limitation and improve the overall stability of GAN training. The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input space, thus enabling continuous learning of the generator. We show that the proposed progressive augmentation preserves the original GAN objective, does not bias the optimality of the discriminator and encourages the healthy competition between the generator and discriminator, leading to a better-performing generator. We experimentally demonstrate the effectiveness of the proposed approach on multiple benchmarks (MNIST, Fashion-MNIST, CIFAR10, CELEBA) for the image generation task.", "keywords": ["Deep Learning", "GANs", "Augmentation", "Generative Modelling"]}, "meta": {"decision": "Reject", "comment": "The submission hypothesizes that in typical GAN training the discriminator is too strong, too fast, and thus suggests a modification by which they gradually increases the task difficulty of the discriminator. This is done by introducing (effectively) a new random variable -- which has an effect on the label -- and which prevents the discriminator from solving its task too quickly. \n\nThere was a healthy amount of back-and-forth between the authors and the reviewers which allowed for a number of important clarifications to be made (esp. with regards to proofs, comparison with baselines, etc). My judgment of this paper is that it provides a neat way to overcome a particular difficulty of training GANs, but that there is a lot of confusion about the similarities (of lack thereof) with various potentially simpler alternatives such as input dropout, adding noise to the input etc. I was sometimes confused by the author response as well (they at once suggest that the proposed method reduces overfitting of the discriminator but also state that \"We believe our method does not even try to \u201cregularize\u201d the discriminator\"). Because of all this, the significance of this work is unclear and thus I do not recommend acceptance."}, "review": {"Ske1nzYI-N": {"type": "rebuttal", "replyto": "rkxYdRUz-N", "comment": "Thanks for your interest in this paper. \n\n1) Regarding your first question on adaptive learning rate: Since the two baseline works that we compared with used fixed learning rates for both the discriminator and the generator, we stick to such setup in our experiments as well for the sake of fair comparison. Certainly, our approach can work with adaptive learning rate. However, we did not claim that it is a \"must\" to adjust the learning rate during the course of progressive augmentation. It is advised, as it can be beneficial to speed up the adaptation to the new augmentation level, being analogous to the linear model (11) and as illustrated in Fig. A10.\n\n2) You stated that \"we used the difference between the current KID and the average of the previous ones instead of the current one\". From your description, we believe that your understanding is correct. Maybe it is good to further note that the current KID and the previous two KID measures shall be attained at the same augmentation level.\n\n3) For the SN-DCGAN experiment, the learning rate is 2e-4 for both the discriminator and generator. It is the best learning rate selected by Kurach et al., 2018.\n\n", "title": "Reply to Experiment Reproduction"}, "Bkxsudlv14": {"type": "rebuttal", "replyto": "HJgC9I9H14", "comment": "First of all, we thank R1 for acknowledging the revision of the paper and correctness of the proof. In the following, we answer further questions and concerns of R1.\n\n1. \n\n- The theory presented in the paper is itself agnostic to the network architecture and means to show that with the progressive augmentation of the input space the original objective of the discriminator is preserved. We would like to highlight that in our approach we proposed to augment the input space only. As illustrated in Fig.1, we can clearly observe from the toy example that the discrimination task becomes more challenging in this case as decision boundary becomes more complicated. In the appendix, we further show experimentally that the task of the discriminator becomes gradually harder with each augmentation level. We also explored in the appendix alternative ways of providing the augmentation bits into the network, confirming the effectiveness of our proposed approach. We agree with R1 that it is worth further investigating PA across different network designs, in particular optimizing it towards a specific application. However, we consider this a future work.\n\n-  The intuition of R1 is correct in terms of how the first layer tries to process the augmentation bits. However, if the task of the discriminator is to only classify x (the classification problem for the discriminator is kept the same), it can easily learn to set the weights for the augmentation channels to zeros, as they are irrelevant redundant information for making a classification decision. In other words, after few initial iterations the training boils down to the classic GAN case. We ran experiments in the CIFAR10 INFOGAN case. As expected, the achieved FID score 57.1 \\pm 2 is on par with the original NS-GAN FID score 58.5\\pm 1.9, while the proposed PA improves it to 44.6 \\pm 1.9 as reported in Table 2.\n\n- Adding or multiplying some feature maps with some random variables are non-invertible distortions to the learning process. With the target of classifying x only, the discriminator is trained to be invariant to them, thereby avoiding over-fitting. However, the augmentation bits do not affect the learning process of the discriminator as non-invertible distortions. Given the way they are fed into the network, the discriminator can easily ignore them by setting the associated weights to zeros. The key is: They carry information that influence the classification decision. This makes the proposed method different to stochastic regularization techniques, which introduce random variables to the network and remain the task of classifying x only. \n\n- We believe our method does not even try to \u201cregularize\u201d the discriminator, as the augmentation bits do not aim to smoothen the decision boundary specified by the discriminator. Instead, they make the decision boundary lie in a higher dimensional space. Both GP and spectral normalization are known to be effective regularization techniques for GANs and we experimentally show the complementary of the proposed PA to them. It is worth noting that on the higher resolution dataset CelebaHQ (128x128) the gain of employing PA is much larger than for CIFAR10. This shows a good potential of this approach for more challenging applications.\n\n2.\nWe agree that it would be beneficial to illustrate this point experimentally as well. Intuitively, for each realization of the augmentation bit the generator sees a different loss function to minimize, thereby providing it diverse paths to approach the data distribution. Since the augmentation bits affect the classification process of the discriminator, this plays a role of ensuring the diversity in the paths. This is different to the cases where the discriminator is trained to be invariant to the random variables involved in the network. Upon revision, we will remove this claim to avoid any misunderstandings.  \n", "title": "Respond to R1's further comments on the revision"}, "B1lgHRnr2Q": {"type": "review", "replyto": "ByeNFoRcK7", "review": "This paper proposes a new trick to improve the stability of GANs. In particular the authors try to tackle the vanishing gradient problem in GANs, when the discriminator becomes to strong and is able to perfectly separate the distribution early in training, resulting in almost zero gradient for the generator. The authors propose to increase the difficulty of the task during training to avoid the discriminator to become too strong.\n\nThe paper is quite well written and clear. However there is several unsupported claims (see below).\n\nA lot of work has been proposed to regularize the discriminator, it's not clear how different this approach is to adding noise to the input or adding dropout to the discriminator.\n\nPros:\n- The experimental section is quite thorough and the results seem overall good.\n- The paper is quite clear.\n\nCons:\n- There is a major mistake in the derivation of the proposed method. In eq. (6) & (7), (c) is not an equivalence, minimizing the KL divergence is not the same as minimizing the Jensen-Shannon divergence. The only thing we have is that: KL(p||q) = 0 <=> JSD(p||q) = 0 <=> p=q . The same kind of mistake is made for (d). Note that the KL-divergence can also be approximated with a GAN see [1]. Since the equivalence between (6) and (7) doesn't hold, the equation (11) doesn't hold either.\n\n- The authors say that the discriminator can detect the class of a sample by using checksum, the checksum is quite easy for a neural networks to learn so I don't really see how the method proposed actually increase the difficulty of the task for the discriminator. Especially if the last layer of the discriminator learns to perform a checksum, and the discriminator architecture has residual connections, then it should be straight-forward for the discriminator to solve the new task given it can already solve the previous task. So I'm not sure the method would still works if we use ResNet architecture for the discriminator.\n\n- I believe the approach is really similar to adding noise to the input. I think the method should be compared to this kind of baseline. Indeed the method seems almost equivalent to resetting some of the weights of the first layer of the discriminator when the discriminator becomes too strong, so I think it should also be compared to other regularization such as dropout noise on the discriminator.\n\n- The authors claim that their method doesn't \"just memorize the true data distribution\". It's not clear to me why this should be the case and this is neither shown theoretically or empirically. I encourage the author to think about some way to support this claim. \n\n- The authors states that \"adding high-dimensional noise introduces significant variance in the parameter estimation, which slows down training\", can the author give some references to support that statement ?\n\n- According to the author: \"Regularizing the discriminator with the gradient penalty depends on the model distribution, which changes during training and thus results in increased runtime\". While I agree that computing the gradient penalty slightly increase the runtime because we need to compute some second order derivatives, I don't see how these increase of runtime is due to change in the model distribution. The authors should clarify what they mean.\n\nOthers:\n- It would be very interesting to study when does the level number increase and what happens when it increase ? Also what is the final number of level at the end of training ?\n\nConclusion:\nThe idea has some major flaws that need to be fixed. I believe the idea has similar effect to adding dropout on the first layer of the discriminator. I don't think the paper should be accepted unless those major concerns are resolved.\n\nReferences:\n[1] Nowozin, S., Cseke, B., & Tomioka, R. (2016). f-gan: Training generative neural samplers using variational divergence minimization. NIPS", "title": "Some major flaws in the approach", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJejbYHd0m": {"type": "rebuttal", "replyto": "B1eexKHdCQ", "comment": "\n5) Ablation studies\n\n- R1: It would be very interesting to study when does the level number increase and what happens when it increase ?what is the final number of level at the end of training\n\nIn this work, we use the KID score as a metric to decide when to increase the augmentation level. As such, we can let the augmentation take place whenever there is a need.  In principle, other metrics for evaluating the performance of GAN are applicable as well, but they are not the focus of this work. As mentioned in the paper for the reported experiments in Table 1 and Table 2 the maximal augmentation level reached was 7, and for the new experiments on CELEBA-HQ it was 10.\n\n- R3: the paper would be much improved by a thorough investigation of the method's effect on training stability\n\nWe try to address these questions in the new section A.7 in the Appendix. In Figure A11, we analyze the loss of the discriminator and the generator over iterations with and without PA. Without PA, we can clearly observe that the loss of the discriminator quickly reduces after initial several thousand iterations. Accordingly, the generator loss increases. Such observation takes place even if we have adopted spectral normalization in the discriminator network.  On the other hand, our proposed PA can effectively prevent the discriminator from over-fitting. Whenever a new augmentation level is reached, the discriminator loss increases again while the generator loss reduces. Overall, through progression, we can on-the-fly control the two-player game between the generator and the discriminator. \n\n6) R2: (1) How does the proposed approach perform when no progressive scheduling is used? (2) How does it perform without the linear model for increasing p?  (3) How does the learning rate of G impact the quality? Arguably, if one includes the FID/KID to modify the learning rates in the competing approaches, one could find a good setup which yields improved results.\n\nWe address the first two questions in ablation studies in Appendix A.6:\nIn Fig. A8, we empirically illustrate the importance of progressively increasing the augmentation levels to maintain the balance between the discriminator and the generator. In this example, the initial augmentation only helps to improve the performance at early iterations, however, the performance starts degrading once the discriminator learns the task but no progression is scheduled.\nIn Fig. A10, we depict the loss of the discriminator over training iterations. Without the linear model for increasing p, we observe the discriminator can require a considerable number of iterations to pick up the new augmentation bit. Whenever this takes place, it can no longer guide the generator to learn the data distribution. The linear model helps to speed up the learning process, taking precautions against potential ill adaptation to new augmentation levels.\n\nRegarding the third question,  in [2], the authors adopted KID score to adapt the learning rate and reported KID=0.015 for CIFAR10 plus SNDCGAN. With our proposal, we are able to achieve KID=0.013. Since KID is not the primary evaluation metric for the quality of synthetic data, this number is only reported in the Appendix A.3 (see Table A5  and Figure A3). \n[2] M. Binkowski et. al. \u201cDemystifying MMD GANs\u201d, ICLR\u20192018.\n\n7) R3: Figure 3 and Table 2 both suggest that the augmentation does nothing to reduce variance between runs.\n\nHere we would like to note that the numbers reported in Table 2 except our results with PA are the outcome of removing outliers among 50 independent runs (the numbers are taken from Lucic et al. 2018).  In Table A11, we did a similar analysis while removing the outliers.  Table A11 shows that in the same setting as Lucic et al. 2018 PA not only improves the FID values, but also reduces their standard deviations across multiple runs, highlighting that PA is helpful to improve the training stability. \n\nWe additionally address individual concerns of reviewers by directly responding to the reviewers comment (see below).\n", "title": "High-level Comments (Part C)"}, "B1eexKHdCQ": {"type": "rebuttal", "replyto": "S1xNidrdAX", "comment": "3) R1: \u201cif the discriminator architecture has residual connections, then it should be straight-forward for the discriminator to solve the new task given it can already solve the previous task\u201d\n\nIn Appendix A.8 we provide experiments using the discriminator networks with and without residual connections while performing the proposed progressive augmentation.  We show that the task remains difficult and it is not straightforward for the discriminator with residual connections to learn the task (see Fig. A12 and A13). Even though the checksum operation might become easier with residual connections, the separation of the input into x and s still remains challenging for the discriminator. \n\nAdditionally, we also experimented with adding skip connections to the discriminator. However, we did not observe any degradation in the performance of PA-GAN (see Fig. A14-A15 and Table A12). This showcases that the proposed PA generalizes well across different architectures, supporting our claims in the paper. We refer R1 to Appendix A.8 for more details.\n\n4) R1, R2, R3: Difference to other methods of weakening the discriminator:\n\n-  R1: \u201chow different this approach is to adding noise to the input or adding dropout to the discriminator\u201d, R3: \u201cno direct comparison to other methods of weakening the discriminator\u201d\n\nBoth adding noise and dropout are regularization techniques. In the non-overlapped space of Pd and Pg, they both attempt to let the discriminator make a soft decision. As such, the support of Pd will not be surrounded by a zero-gradient area, preventing Pg from approaching it. Our augmentation scheme is orthogonal to them. We propose to increase the problem dimensionality. Initially, the discriminator needs to separate two inputs living in an N-dimensional space. Now, the separation is conducted in an (N+L)-dimensional space, where L is the augmentation level. In other words, the intrinsic dimension of the discriminator function is increased, see Figure 1. Since we do not regularize the discriminator, the decision boundary can still be very sharp. Therefore, PA can be applied along with the regularization techniques. \n\nIn Sec. 5, we have experimented on using PA with gradient penalty (GP), which is shown to be an effective regularization technique for the discriminator [Kurach et al. 2018]. The achieved gain is more pronounced than using either PA or GP alone. This empirically confirms that PA and regularization techniques are orthogonal and more importantly mutually beneficial. \n\nAs discussed by Roth et al. NIPS2017, adding high-dimensional noise can introduce significant variance in the parameter estimation process. By analytically convolving the Gaussian noise distribution with the data and generative model distributions, they alternatively proposed a regularizer, adding it to the discriminator loss function together with an annealing process to adjust the weighting factor. The regularizer resembles GP that has been successfully combined with PA in Sec. 5, e.g., see Figure 3. One difference to GP is that it is zero-centered. \n\n- R2: \u201cdifference to [Sajjadi et al. ICML\u201918]\u201d\nSajjadi et al. proposed to blur the input and gradually remove the blurring effect during the course of training. We believe that this technique has some common ground with employing instance noise, as both techniques perform direct modifications on the data samples. In contrast, PA does not directly modify the data samples, but rather structurally appended to them increasing the input dimensionality of the discriminator. As we have mentioned previously, our approach is orthogonal to the techniques that directly modify the data samples and in principle can be combined with them.\n\nIn this revision, we additionally provide new results of combining PA with dropout in Appendix A.9. Figure 16 shows that dropout alone does not improve the performance of NS-GAN, in fact, causing a small performance degradation. Whereas PA shows to be more effective and preserves the performance gain even in combination with the dropout. Analogous to our early observations with GP, this shows that PA can work in combination with different regularization techniques.\nThe combination of the zero-centered GP of Roth et al. NIPS2017  with PA requires further investigation. Unlike the GAN case, the two distributions to be matched are not simply the data and generative model distributions. In order to properly take into account the augmentation bits, we need to revise the derivation of the zero-centered GP. \n ", "title": "High-level Comments (Part B)"}, "HkgT_jrOC7": {"type": "rebuttal", "replyto": "BJg7dMla27", "comment": "We thank R3 for the reviewing effort and comments that allowed us to improve the quality of our work. We have addressed R3's questions in our general response (A-C).", "title": "Reponse to R3"}, "r1l4-oHuCm": {"type": "rebuttal", "replyto": "B1eXZ7a43Q", "comment": "Apart from the questions that have been addressed in our general response (A-C), here are answers to R2's remaining questions:\n\n1) Several pages of theory can really be summarized into \u201clearning the joint distribution implies that the marginals are also correctly learned\u2019 (similar to ALI/BIGAN). \n\nFrom the perspective of joint distribution matching, ALI/BiGAN constructs two joint distributions which marginals are with respect to the data and model distribution. Therefore, when two joint distributions are mutually matched, the model distribution approaches the data distribution. In our case, as mathematically shown in Appendix A.1, the two joint distributions have identical marginals by construction. Upon a completely different line, we prove its equivalence to the original problem of GAN.\nFurthermore, ALI/BIGAN aim at generative latent modeling, whereas we aim at stabilizing the training process of GAN (generative modeling without the latent code). The augmentation bit sequence s in our case is not a latent code of the input image. The generator does not take s as its input to generate the synthetic data. A successful training of ALI/BIGAN requires x and z being mutually dependent, e.g., its variant ALICE enforcing the dependence through conditional entropy. We are in the opposite situation, namely, x and s being mutually independent implies a perfect generator.\n\n2) \u201cCan you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing?  In some approaches such as Wasserstein GAN, we should train the discriminator to optimality in each round.\u201d\n\nWe first would like refer the reviewer to works of [Arjovsky & Bottou ICLR\u201917 and S\u00f8nderby et al. ICLR\u201917]. In these works, the authors provide the explanation of the fundamental problem of instability of GANs and explain why additional techniques are required to weaken the discriminator. In particular, the problem is that the support of the real data distribution and the generative model distribution are often non-overlapping (in image modelling, distribution of natural images is often assumed to be concentrated on or around a lower-dimensional manifold). In such situations, the divergences which GANs are minimizing become meaningless (the Jensen-Shannon divergence is saturated so its maximum value), and the discriminator is extremely prone to overfitting, which can lead to instabilities. One of the ways to avoid this behavior is to weaken the discriminator by making its job harder, which we successfully achieve by performing PA.  Recent work address the problem of weakening the discriminator in two ways, which are orthogonal to our approach, either by regularizing the discriminator via different variations of the gradient penalty [Gulrajani et al. 2017, Roth et al. 2017, Fedus et al. 2018] or altering directly the data samples [Arjovsky & Bottou 2017, S\u00f8nderby et al. 2017, Sajjadi et al. 2018].\n\nThe Wasserstein distance belongs to the family of integral probability metrics, which are well defined in contrast to f-divergences [Arjovsky & Bottou 2017]. Thus, [Arjovsky et al. 2017] advises to train the discriminator to optimality. However, in WGAN the class of discriminators is restricted to Lipschitz continuous functions, which yields a hard constraint on the function class that is empirically hard to satisfy. Moreover, [Mescheder et al. 2018] show that WGANs (and WGAN-GPs) do not converge as in practice the discriminator is trained with a fixed number of discriminator updates per generator update and thus the discriminator optimality is not guaranteed. Therefore, preserving a healthy competition between the generator and discriminator remains challenging (the escalation of signal magnitudes in the generator and discriminator is still observed in practice [Karras et al. 2018]). PA can serve as a regularization technique in this case. Empirically we show that employing PA with WGAN and WGAN-GP leads to better performance.\n\n3) Why is the proposed approach more practical then approaches such as https://arxiv.org/abs/1706.08500  \n\nDifferent learning rates of the generator and discriminator (or different number of updates) introduce an additional hyper-parameter which requires a careful tuning to achieve a performance improvement. In contrast, our progressive augmentation is done automatically by a simple threshold test of the KID score. We would like to point out, that it is possible to combine both approaches, which potentially might be benefiting to each other. We consider the comparison/combination with the two time-scale update rule as part of the future work.\n\nWe thank R2 for the comments and suggestions that allowed us to greatly improve the quality of the work.\n", "title": "Response to R2"}, "SkevCtHOC7": {"type": "rebuttal", "replyto": "B1lgHRnr2Q", "comment": "Apart from the questions that have been addressed in our general response (A-C), here are answers to remaining questions of R1:\n\n1) \"adding high-dimensional noise introduces significant variance in the parameter estimation, which slows down training\", can the author give some references to support that statement? \n\nWe would like to refer R1 to [Roth et al. NIPS\u201917]. This work argues that \u201chigh-dimensional noise introduces significant variance in the parameter estimation process\u201c, \u201ccounteracting this requires a lot of samples and therefore ultimately leads to a costly or impractical solution\u201c  and that \u201cexplicitly adding noise in high-dimensional ambient spaces introduces additional sampling variance\u201d, which leads to the increase of the overall training time.\n\n2) According to the author: \"Regularizing the discriminator with the gradient penalty depends on the model distribution, which changes during training and thus results in increased runtime\". I don't see how these increase of runtime is due to change in the model distribution. The authors should clarify what they mean. \n\nHere we meant that there are two drawbacks of employing the gradient penalty [Kurach et al. 2018]. First, it can depend on the model distribution, which changes during training. Second, computing the gradient norms results in increased runtime.\n\n3) The authors claim that their method doesn't \"just memorize the true data distribution\". It's not clear to me why this should be the case and this is neither shown theoretically or empirically. I encourage the author to think about some way to support this claim. \n\nOur aim was to provide the intuition to the reader why we think employing PA result in better FID and inception scores, which are known to correspond to the variation of generated images. We believe that structurally augmenting the input sample space and mapping it to higher dimensions encourages the generator to explore various paths towards the true data distribution, leading to the improved variation of generated images, which we observe in the improved FID and IS metrics. \n\nWe thank R1 for pointing out the confusing statements mentioned above, we adjusted those statements in the in the revision to improve the overall clarity.\n", "title": "Response to R1"}, "S1xNidrdAX": {"type": "rebuttal", "replyto": "HJxEdurdC7", "comment": "Next, we address some of the high-level concerns raised and requested clarifications.\n\n1)\tR1: \u201ca major mistake in the derivation\u201d:\n\nR1 is concerned with the validity of the proof in Sec. 4.1 and overall derivation of the method, questioning the equivalence in eq. (6). We argue that the provided equivalence is valid and believe that the confusion comes from the shortening of the proof due to the page limit. To improve the clarity, we altered the explanation in sec. 4.1 and provided a detailed derivation of eq. (6) in Appendix A.1. We invite R1 to re-asses its quality and hope the detailed explanation will clarify the original confusion.\n\nBelow we briefly summarize the derivation presented in the paper: \nIntroducing a random bit s, we have two ways to associate s=0 and s=1 with the sample x~Pd and x~Pg, respectively. These two ways simply define two joint distributions P(x, s) and Q(x, s). The JS divergence between Pd and Pg is equal to the JS divergence between P(x,s) and Q(x,s), for any given pair of (Pd and Pg). Therefore, it is equivalent to minimize either of the two JS divergences for optimizing G.\nTo show that the two JS divergences are identical, we based on 1) the connection between JS divergence and mutual information, 2) mutual information is a KL divergence, 3) 0.5 P(x, s) + 0.5 Q(x, s) = Pm(x) P(s), where Pm(x) is the marginal of P(x, s) and Q(x, s). It is important to note that the marginal of P(x,s) and Q(x,s) with respect to x are always identical by construction and matching P(x,s) and Q(x,s) does not fall into the framework of ALI/BiGAN.\n\n2) R1, R3:\u201dsurprising why D doesn\u2019t immediately learn the new task\u201d, \u201chow the method proposed actually increase the difficulty of the task for the discriminator?\u201d\n\nAlthough the naive checksum (XOR) operation between two bits is an easy task for the neural network, in case of the PA-GAN setting additional challenges arise.  First, the task of the discriminator D becomes two folded: it not only needs to decide if the image is real or fake, but on top of it to figure out a new task \u2013 perform a checksum operation between the image input x and random bit s.  Second, to perform the checksum D also needs to learn how to separate the input into x and s, making the task more difficult (even for networks with residual and skip connections, see Appendix A.8).\n\nThe way we provide the random bit s to the discriminator plays the role how fast it is able to learn the task. In our proposed implementation s is concatenated with the data point x and provided to the discriminator D as an input, making harder for D to make separation between x and s. We also experimented with providing s only to the intermediate layers or just to the last layer of D. Our observations are aligned with the intuition of R1: providing s directly to the last layer makes the task easy for D, as it needs now only to solve the checksum between two bits (the task for x has already been solved and separation between x and s is trivial) and does not result in any improvement.\nIn addition, to figure out the task D needs to see at least 4 times more data points (all input combinations for XOR operation).  We can leverage this to control the task difficulty by regulating the balance between classes in the batch (by biasing uniform sampling of s).\n\nAt last, we also would like to point out that we do not claim that performing checksum operation is a difficult task for the discriminator. We only claim that with each level of the augmentation the task becomes gradually harder and it takes some time for the discriminator to reach the decision confidence. When the discriminator learns the task the next augmentation step is performed. This strategy allows to maintain the healthy balance between the generator and discriminator. We show experimentally that it is beneficial to use PA-GAN.\n\nWe provide the detailed analysis of the task difficulty of the discriminator with PA in Appendix A.8, where we experimentally show that that the task of the discriminator becomes harder with each level of PA.\n\n", "title": "High-level Comments (Part A)"}, "HJxEdurdC7": {"type": "rebuttal", "replyto": "ByeNFoRcK7", "comment": "We thank reviewers for their feedback indicating that the paper is well written (R1) and easy to follow (R2), with thorough experiments (R1) in the standardized setup (R2); that the proposed approach is interesting (R3) and simple (R2), but yet makes the task of discriminator progressively more difficult (R2) and hence has a beneficial effect on GAN training across different architectures in different data scenarios (R3), leading to overall good results (R1).\n\nDespite the positive comments, we believe some important points of the paper have been missed or misunderstood by the reviewers.  Thus, we would like to re-state our contributions and invite the area chair and the reviewers to re-assess our work. Our contributions are:\n-\tWe propose a novel method - progressive augmentation of GANs (PA-GAN), which addresses the important problem of maintaining a healthy competition between the generator and discriminator, leading to a better quality generator. \n-\tWe experimentally show the effectiveness of the PA-GAN and report the pronounced performance improvement across different datasets (~20 % on average, see Table 2 ) using the standardized setup (performing multiple independent runs while avoiding additional hyper-parameter tuning). \n-\tWe theoretically prove that the proposed approach preserves the original GAN objective (see Sec. 4.1 and Appendix A.1) and in contrast to other techniques  does not bias the optimality of the discriminator by modifying the real data samples or introducing noise to the input (such as [Sajjadi et al. ICML\u201918] or [Arjovsky & Bottou ICLR\u201917 and S\u00f8nderby et al. ICLR\u201917]).\n-\tThe proposed technique can be easily integrated into different GAN frameworks (including networks with residual or skip connections) with minimal changes (see Sec. 5 and Appendix A.8).\n-\tOur technique is orthogonal to existing work, it can be successfully employed with other regularizations strategies, e.g., gradient penalty and spectral normalization, which we experimentally show in Sec. 5.  ", "title": "General Response"}, "BJg7dMla27": {"type": "review", "replyto": "ByeNFoRcK7", "review": "This paper modifies the GAN objective by defining the TRUE and FAKE labels in terms of both the training sample, and a newly introduced random variable s. The intuition is that by progressively changing the definition of s, and its effect on the label, we can prevent the discriminator network from immediately learning to separate the two classes. \n\nThe paper doesn't give any strong theoretical support for this intuition. And it I found it a bit surprising that the discriminator doesn't immediately learn the one extra bit of information introduced by every new level of augmentation. However, the results do seem to show that this augmentation has a beneficial effect on two different architectures in different data scenarios, although the increase is not uniform over all settings.\n\nThe approach presented in this paper is motivated primarily as a method of increasing stability of training but this is not directly investigated. Figure 3 and Table 2 both suggest that the augmentation does nothing to reduce variance between runs. There is also no direct comparison to other methods of weakening the discriminator, although these are mentioned in the related work. I think the paper would be much improved by a thorough investigation of the method's effect on training stability, to go along with the current set of evaluations.", "title": "Interesting idea, but lacking theoretical support or sufficient empirical analysis.", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "B1eXZ7a43Q": {"type": "review", "replyto": "ByeNFoRcK7", "review": "Authors argue that the main issue with stability in GANs is due to the discriminator becoming too powerful too quickly. To address this issue they propose to make the task progressively more difficult: Instead of providing only the samples to the discriminator, an additional (processed) bitstring is provided. The idea is that the bitstring in combination with the sample determines whether the sample should be considered true or fake. This in turn requires the decision boundary of the discriminator to become more complicated for increasing lengths of the bitstring. In a limited set of experiments the authors show that the proposed approach can improve the FID scores.\n\nPro:\n- A simple idea to make the problem progressively more difficult.\n- The writing is relatively easy to follow.\n- Standardized experimental setup.\n\nCon:\n- Ablation study of the training tricks is missing: (1) How does the proposed approach perform when no progressive scheduling is used? (2) How does it perform without the linear model for increasing p? (3) How does the learning rate of G impact the quality? Does one need all of these tricks? Arguably, if one includes the FID/KID to modify the learning rates in the competing approaches, one could find a good setup which yields improved results. This is my major issue with this approach.\n- Clarity can be improved: several pages of theory can really be summarized into \u201clearning the joint distribution implies that the marginals are also correctly learned\u2019 (similar to ALI/BIGAN). This would leave much more space to perform necessary ablation studies. \n- Comparison to [1] is missing: In that model, it seems that the same effect can be achieved and strongly improves the FID. Namely, they introduce a model in which observed samples pass through a \"lens\" before being revealed to the discriminator thus balancing the generator and discriminator by gradually revealing more detailed features.\n- Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing? In some approaches such as Wasserstein GAN, we should train the discriminator to optimality in each round. Why is the proposed approach more practical then approaches such as [2]?\n\n[1] http://proceedings.mlr.press/v80/sajjadi18a.html\n[2] https://arxiv.org/abs/1706.08500", "title": "Provide an additional bitstring to the discriminator which can swap the label for observed and generated samples.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}