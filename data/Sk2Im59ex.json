{"paper": {"title": "Unsupervised Cross-Domain Image Generation", "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"], "summary": "", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper."}, "review": {"SyyI8_GCx": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data. ", "title": "Training set for SVHN domain?"}, "HyNYXHXvg": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "This paper presents an unsupervised domain transfer from the image of domain S to the image of domain T. \nIt was really refreshing that this conversion was possible without any mapping data. \nFor example, in the paper, the model can transfer the SVHN image '3' to the MNIST image '3' without the mapping data.\nThe model can be roughly divided into GAN and Content Extractor (f in the paper).\n\n1. GAN\nDuring training, the discriminator sees the mnist image and learns to determine it as a real image. \nAnd with GAN loss, the generator learns to get the mnist image as output when it receives an svhn image as input to deceive the discriminator.\n\n2. Content Extractor\nIf the model use only GAN loss, the content in the image may not be retained even if the domain is changed.\nFor example, the generator may convert the svhn image '3' to the mnist image '2' to deceive the discriminator.\nIn this paper, authors introduce a new function called 'f' to maintain the content.\nThe generator includes f and generates a fake mnist image when it receives an svhn image as input.\nThe original svhn image and the generated fake mnist image are put back into f.\nThen additional loss function is set so that the resulting values \u200b\u200bare the same.\nHere, f is learning to extract content regardless of domain.\n\n\nI felt very fresh in this paper so i implemented this paper myself.\nHere is the code I implemented.\n\nhttps://github.com/yunjey/dtn-tensorflow", "title": "Interesting Work"}, "B1H0yKyDl": {"type": "rebuttal", "replyto": "rJFsDvIVx", "comment": "Dear reviewer,\n\nThank you for updating the review!\n\nThe authors.", "title": "Re: Updated Review"}, "Hk_hDPcLx": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "We thank the reviewers for their time and insights. All 3 reviewers seem to agree that the work is interesting, well-written and presents extensive experiments. R1 & R2 both note the f-constancy as a novelty of this work. Also, the fact that the method does not require training pairs for the two domains is noted by R3 as a major contribution, which \u201ccould be impactful in broad problem context\u201d. R2 & R3 both agree that the output generations are visually appealing. We have no factual dispute with the reviewers and replied to each individually below.\n\nThe open review discussion has been extremely beneficial to us and the paper has been revised in order to address all actionable items raised throughout the review period. We thank the reviewers for their thoughtful and constructive reviews and all other community members who shared their comments. During the review period, the work has already been cited several times, reimplemented on github, and drawn considerable attention.\n", "title": "Authors' summary of discussion"}, "BJGDuzYHx": {"type": "rebuttal", "replyto": "rJFsDvIVx", "comment": "Dear reviewer, \n\nIn addition to information provided below (\"More explanations\") we would like to note that we just performed the expression transfer experiment you mentioned in your review. As it turns out, the same f can be used for both identity and expression!\n\nWe believe that the additional information we provide at openreview.net as well as the manuscript revision could justify a second view and would be very much interested in an open discussion in order to find out if there are any remaining unfavorable factors.\n\nThank you,\n\nThe authors\n", "title": "Re: Review"}, "B1jCDMKBl": {"type": "rebuttal", "replyto": "B1JrbRHVe", "comment": "Thank you for your positive and supporting review. Regarding the few observed disadvantages:\n\n[AnonReviewer2:] 1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution) \n\n[Authors:] There are a few very recent and concurrent publications that do images analogies. All require paired training samples. Our work is unique in its unsupervised nature, which we think is noteworthy. f-constancy is the means of obtaining this.\n\n[AnonReviewer2:] 2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain 2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f. \n\n[Authors:]  In our experiments, f is demonstratively more powerful in S than in T (Tab. 1 last row). Still we observe what we call \u201cthe magic of analogies\u201d \u2014 an f that is rather weak on T as a classifier is good enough in order to create analogies using nothing more than unlabeled examples from both domains. Based on these analogies we can transfer labels from S and create a good classifier in T (Tab. 2). \n\nAs the experiment in the second revision show, the f trained on photos can maintain not only identity information in T but also expression information. This further supports that even when f is trained separately on S, f-constancy assures that x and g(f(x)) are tightly related along many dimensions, including dimensions that f is supposedly invariant to.\n\n[AnonReviewer2:] 3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches. \n\n[Authors:] We agree. The purpose of the DA experiment is to show that the reduction of the DA problem to the domain transfer problem (Sec. 2) is viable. While the experiment is very successful, it is indeed only a single experiment and is not meant to shift the focus on the paper from the new cross domain transfer problem to the well explored problem of DA.\n\n", "title": "Re: Review"}, "B1-uGX_Sg": {"type": "rebuttal", "replyto": "HyxO2R7Be", "comment": "We thank the reviewer for the comments. Indeed, the main invention is learning to map without the supervision of matching pairs. \n\nThe reviewer pointed to two shortcomings of the experiments.\nReviewer: -It will be more interesting to show results in other domains such as texts and images. \n\nAuthors: Indeed, the options are endless. Analogies are a very powerful tool, and making these in an unsupervised manner has a lot of applications that are not explored in this manuscript. However, we are afraid that this is beyond the scope of the current submission.\n\nReviewer: -In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.\n \nAuthors: Following the request of both AnonReviewer1 and AnonReviewer3, we added new experiments and revised our manuscript. As it turns out the face identification network does contain enough expression information to allow cross-domain transfer of expression. All that was needed is to provide unlabeled smiling emoji so that the discriminator would not identify smiling emoji as fake. Please see our subsequent comment regarding the revised version.\n", "title": "An interesting work"}, "B1NvWQuBx": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "Following the request of the reviewing team, we have just uploaded a new version of our manuscript, which includes expression preserving experiments. \nThe new experiments are in Appendix B. \n\nIn order to provide a quick way to track the changes from the original submission, we color all modifications in red. \n\nThank you for the extremely useful feedback.\n", "title": "Revision #2"}, "H1KGNfvNx": {"type": "rebuttal", "replyto": "rJFsDvIVx", "comment": "Thank you for your review. \n\n**1.  AnonReviewer1: f-constancy is the main novelty of the work. **\n\nAuthors: Thank you for noting the novelty. A few prominent recent contributions present image to image mapping using supervised (input image,output image) pairs. We are unique in that we do not require such pairs. This is quite remarkable and is achieved using f-constancy among other contributions. \n\n**2.  AnonReviewer1: It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. **\n\nAuthors:  Since L_CONST requires the preservation of identity, the information in f is exactly the relevant information. Experimentally, we show a very strong advantage over the baseline method \"DTN G does not contain f\" in Table 1.\n\n**3.  AnonReviewer1: As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). **\n\nAuthors: We respectfully disagree. The fact that the generated caricatures are remarkably identifiable (median rank of 16 out of 100,000 for retrieving among real images) clearly shows that identity information is extremely well preserved.\n\n**4.  AnonReviewer1: Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset. As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. **\n\nAuthors: Yes, but this is desirable. For example, in the face experiments we wish to maintain identity and not other factors such as expression, illumination, head pose, etc.\n\n**5.  AnonReviewer1: Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly? **\n\nAuthors: As stated above (2), incorporating f in G allows the network to focus on the most relevant aspects of this mapping. In the unsupervised setting this seems crucial.\n\n**6.  AnonReviewer1: Figure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. **\n\nAuthors: Figure 5 aims to clarify the major difference between the tasks of style-transfer and domain transfer:  \"the output image [in 5(c)] is perhaps visually appealing; However, it does not belong to the space t of emoji\". Therefore, style transfer does not perform cross-domain generation and solves a different problem. \n\n**7.  AnonReviewer1: Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?**\n\nAuthors: Style transfer is not relevant to Table 4 since, as stated above, it does not create an emoji image. It is therefore not comparable to the other methods in the table.\n\n***\n\nSince AnonReviewer1 asked for additional explanations, we respectfully ask that the reviewer consider updating the review based on our clarifications or let us know if these are not satisfactory.\n\n", "title": "More explanations"}, "Bkp4_qHVl": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "Following the ongoing discussion with the reviewing team as well as with other readers, we have just uploaded a new version of our manuscript, which is aimed at improving clarity. \nIn order to provide a quick way to track the changes, we color all modifications in red. \nIn addition, as mentioned below, we will soon share our open implementation in Torch. \nThank you all for the extremely useful feedback.", "title": "Revision"}, "ByLSb8BEg": {"type": "rebuttal", "replyto": "HywugTNVe", "comment": "Thank you for your question. Reconstructing images in the target domain (loss L_{TID} modulated by \\beta) helps to obtain better performance, but seem the least crucial amongst the generator's L_{G} losses, as we further discuss in Section 5.1 & Table 1 (where we also omit this loss).", "title": "Accuracy without reconstructing images in target domain"}, "HywugTNVe": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "Given the goal of transferring samples from source domain to target domain, I am wondering whether it is necessary to simultaneously train the model to reconstruct images in the target domain. Any ablation studies on this?", "title": "Accuracy without reconstructing images in target domain"}, "r1mJO5qXl": {"type": "rebuttal", "replyto": "HkWTACumg", "comment": "Thank you for your questions.\n\n1.  Equation 1 is the GAN risk for a binary GAN that compares \u201cfake\u201d (i.e, generated) samples crated by mapping samples in S to \u201creal\u201d samples from T. It evolves to both Equation 3 and Equation 4. The differences between 1 and 3+4 are that we moved from the language of risk, i.e., using expectations, to the language of loss, using sums (the writing as expectations and not as sums is a typo). Second, we moved from a binary D to a ternary D and distinguish between three populations, labeled respectively as classes 1,2,3: (1) G mapping of samples from s, (2) G mapping of samples from t, (3) the actual samples from t.\n\nEquations 3 and 4 can no longer be specified as one equation, since we want to be specific that G maps the first two classes to class 3. If G were to simply maximize 3, there would be other options, e.g., confusing classes 1 and 2.\n\n2.  Equations 3 and 4 make sure that the generated analogy, i.e., the output of G,  is in the target space T. However, they do not enforce any similarity between the source sample x and the generated G(x). This is done by Equations 5 and 6. Equation 5 enforces f-constancy for x \\in S. Equation 6 enforces that for samples x\\in T, which are already in the target space, G is the identity mapping. This is the desirable behavior, e.g., given an emoji, one would like it to remain the same emoji under the mapping of G. It can also be seen as an autoencoder type of  loss, applied only to samples from T. This example and motivation will be added to the revised manuscript.\n\nWhile Equation 5 is sufficient (see Tab. 1, DTN w/o L_{TID}), Equation 6 helps obtain better performance. Interestingly, Equation 6, even without Equation 5, is a strong enough constraint to ensure a meaningful mapping (see Tab 1, DTN w/o L_{CONST}). Removing both Equation 5 and Equation 6, (Tab1, DTN w/o L_{CONST} & L_{TID}) leads to a mapping that does not preserve identity, as expected. These observations appear in Sec. 5.1, and we will make sure to clarify the role of Equation 6 further.\n", "title": "Re: Explanations"}, "HkWTACumg": {"type": "review", "replyto": "Sk2Im59ex", "review": "1. Could you please explain equation (3)? I assume R_GAN in equation (1) translate to L_GANG in equation (4), what is the motivation behind (3)? \n2. Why do you want to enforce identify matrix on G as in equation (6)?Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. \n\n\nThe work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. \n\nThe paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. \n\nIt seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. \n\nDo the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?\n\nFigure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?", "title": "Explanations", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJFsDvIVx": {"type": "review", "replyto": "Sk2Im59ex", "review": "1. Could you please explain equation (3)? I assume R_GAN in equation (1) translate to L_GANG in equation (4), what is the motivation behind (3)? \n2. Why do you want to enforce identify matrix on G as in equation (6)?Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. \n\n\nThe work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. \n\nThe paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. \n\nIt seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. \n\nDo the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?\n\nFigure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?", "title": "Explanations", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJpyTHW7x": {"type": "rebuttal", "replyto": "r1h8YLkmg", "comment": "Thank you for your questions!\n\n1. f-constancy is the mechanism that ensures that the analogy is valid, i.e., that the mapping of a sample from S to T maintains a similarity as defined by f. For example, the DeepFace representation ensures that the created emoji maintains the identity information of the original image. We will make sure to clarify this point in the abstract and introduction.\n\n2.  (i) As mentioned in the manuscript, our GAN follows DCGAN (Radford et al. 2015). No special tricks were applied. \n(ii) Stopping was performed after 3 epochs. \n(iii) As mentioned in Sec. 5.2, the upscaling to 152x152 was done in order to employ a pretrained $f$. We failed to mention the exact upscaling method \u2014 it was done using a simple bilinear upscaling. The superresolution method was not applied during training and is only employed to create print quality images. The appendix shows the results without it. \n", "title": "Re: Questions"}, "r1h8YLkmg": {"type": "review", "replyto": "Sk2Im59ex", "review": "1. From the introduction and abstract it's not clear what do the authors mean by f. Why would anyone care about f and f-constancy? It would be nice if that could be explained in the very beginning.\n\n2. In the emoji experiment, did you apply any tricks to make GAN training stable? What was the stopping criterion for the training procedure? Is superresolution applied during training too (to upscale model outputs to 152x152 and match the resolution of the real data)?Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model\u2019s outputs can produce a false impression of the visual quality of the transferred samples. I\u2019d suggest moving original outputs from the appendix into the main part.", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1JrbRHVe": {"type": "review", "replyto": "Sk2Im59ex", "review": "1. From the introduction and abstract it's not clear what do the authors mean by f. Why would anyone care about f and f-constancy? It would be nice if that could be explained in the very beginning.\n\n2. In the emoji experiment, did you apply any tricks to make GAN training stable? What was the stopping criterion for the training procedure? Is superresolution applied during training too (to upscale model outputs to 152x152 and match the resolution of the real data)?Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model\u2019s outputs can produce a false impression of the visual quality of the transferred samples. I\u2019d suggest moving original outputs from the appendix into the main part.", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ry42-WPWg": {"type": "rebuttal", "replyto": "HkqzxaS-x", "comment": "Very glad to hear that. In my re-implementation, the architecture of D is 1x32x32 -> 128x16x16 -> 256x8x8 -> 512x4x4 -> 3(reshape and fully connected), all the conv kernels are 5x5. The architecture of G is 128x1 -> 512x4x4 -> 256x8x8 -> 128x16x16 -> 1x32x32 as you suggested, all the deconv kernels are 5x5. The solver uses AdamOptimizer, beta1 = 0.5, learning rate is set to 0.0002, alpha=beta=15. Before sending the target MNIST images and generated source images into f, they are replicated three times. The D_loss and G_loss are computed using softmax_with_cross_entropy_loss with different labels. According to this setting, I can generate MNIST images very well but fail to adapt the SVHN images into MNIST-like images. Very disappointed, I can not find reasons. Could you tell me the reasons? Sorry to trouble you.", "title": "About the configuration of the generator"}, "HkqzxaS-x": {"type": "rebuttal", "replyto": "SyOcVLHZx", "comment": "There is no discrepancy.  $g$ employs four blocks, and map vector representations to 32x32 grayscale images. The first layer can be seen as either a convolution or a fully connected layer (please see Radford et al.). The two are equivalent and in fact all blocks employ the Torch layer nn.SpatialFullConvolution. For the last layer, the paper clearly states \"In order to apply $f$ on MNIST images, we replicate the grayscale image three times.\" (see also reply to the thread \u201cOne puzzle\u201d below).  You might be happy to learn  that we plan to post our code online.", "title": "About the configuration of the generator"}, "SyOcVLHZx": {"type": "rebuttal", "replyto": "By89tlHZl", "comment": "Thank you, but this is not consistent with what you stated in the paper. In the paper, for the digits, 'g employs four blocks of deconvolution' and 'map the 128D representations to 32*32 grayscale images',  but actually you use three blocks and generate three channel images, so I am confused which one is true. Finally, I am wondering if the architecture of the discriminator is the inverse of the generator. Thanks for your kind help.", "title": "About the configuration of the generator"}, "By89tlHZl": {"type": "rebuttal", "replyto": "rkqFFrN-l", "comment": "Thank you for your question. Generator $g$'s first layer, which takes 128d vectors in the case of digits (256 with faces) could be called fully-connected as it is just a matrix multiplication, but the result is reshaped into a 3-dimensional tensor. As discussed in Section 5.1, we then apply 'blocks' of: convolution, followed by batch-normalization, followed by ReLU. \n\nFor faces, as we mention in section 5.2, \"adding 1x1  convolution to each block resulted in lower $L_{\\text{CONST}}$ training errors, and made $g$ 9-layers deep\", hence we added such to each block. So overall, for digits we used: 128x1 -> 512x4x4 -> 256x8x8 -> 128x16x16 -> 3x32x32 , for faces: 256x1 -> 512x4x4 -> 512x4x4 -> 256x8x8 -> 256x8x8 -> 128x16x16 -> 128x16x16 -> 64x32x32 -> 16x32x32 -> 3x64x64. Both were terminated with tanh. These architectures were based on Radford et al., and it's possible that better ones exist as we did not search exhaustively for the best one.", "title": "About the configuration of the generator"}, "rkqFFrN-l": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "Hi, it is really interesting and I want to re-implement this work. But the paper doesn't provide much information about the configuration of the generator. For example, do you use a fully connected layer as the first layer? If true, what is the dimension?  How many filters of each deconvolution layer do you use ?  Thank you very much.", "title": "About the configuration of the generator "}, "HkpRm8QZg": {"type": "rebuttal", "replyto": "SJcGTBm-l", "comment": "Yes.", "title": "One puzzle"}, "SJcGTBm-l": {"type": "rebuttal", "replyto": "BkG8kSmZg", "comment": "Thank you, so before feeding to f to compute L_const, the generated source images, i.e., g(f(x)) for x from source domain, are replicated three times ?", "title": "comment"}, "BkG8kSmZg": {"type": "rebuttal", "replyto": "HJbhsNQWg", "comment": "Thank you for your comment. Your second suggestion is exactly what we do. Quoting from the paper: \"In order to apply $f$ on MNIST images, we replicate the grayscale image three times.\"", "title": "One puzzle"}, "HJbhsNQWg": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "In section 5.1, you map SVHN-trained representation to 32*32 grayscale images, but the encoder f's input is three-channel, how do you solve this ? Why don't you transform RGB SVHN images into grayscale images or replicate the grayscale MNIST images three times ? I mean why don't you use the same image format for training f and the adversarial network? ", "title": "One puzzle"}, "B1lrlX1Wg": {"type": "rebuttal", "replyto": "H1tNe0Cge", "comment": "Thank you for your question. The links between our work and style transfer are discussed in Sec. 2. In addition, we directly compare our work to style transfer in Sec. 5.3.\n", "title": "How is this work different from style transfer?"}, "H1tNe0Cge": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "Can the authors comment on what they are doing in terms of end-use/goal in addition to what we know from style transfer already?", "title": "How is this work different from style transfer?"}, "S1aXlMalx": {"type": "rebuttal", "replyto": "BJjU2w2ge", "comment": "Thank you very much for the very quick and important feedback.\n\nThe work by Dosoviskiy and Brox \"Generating Images with Perceptual Similarity Metrics based on Deep Networks\", arXiv preprint arXiv:1602.02644, 2016  is indeed relevant, and we are sorry to have missed it. The components used in their work have 1:1 mapping to most components of our loss, as you correctly point out.\n\nHowever, there are important differences:  First, Dosoviskiy and Brox (2016) deal with learning a supervised mapping from a set of input-target pairs, while we solve the unsupervised domain transfer network.  Second, while the terms are similar, they are applied differently: L_{CONST} is applied in the source domain S and L_{TID} is applied at T. There are indeed two pipelines in our system \u2014 one for samples that arise from S and one for samples that arise from T. The pipeline for samples from S is unsupervised and uses L_{CONST}. The pipeline for samples that arise from T is supervised and the part of it that begins after the extraction of the representation by f bares similarity to the pipeline of Dosoviskiy and Brox (2016). Third, in our work, the discriminator D is ternary and not binary.\n\nExperimentally, Dosoviskiy and Brox (2016) is particularly interested in reconstructing input image I, hence it makes use of the activations of lower layers (e.g. Conv5, see Fig. 5 there) of its encoder (AlexNet) in order to have a better reconstruction. In contrast, our work focuses on the most semantic layers, since domain transfer is ultimately a semantic task. We have noticed that using activations from lower layers has a clear detrimental effect on the  transfer results, due to the differences between S & T.\n\nTo summarize: we are sorry for overlooking Dosoviskiy and Brox (2016) despite the mentioned similarities in constraining the output of the generator (namely, the loss in feature and pixel spaces). However, both the task that we solve and the method are crucially different than the ones presented in that paper. We will revise Sec. 2 to reflect the above discussion and provide proper credit. We are also grateful to the reviewer for pointing out the missing \\log in equation #3.\n", "title": "About the reference and one minor error"}, "BJjU2w2ge": {"type": "rebuttal", "replyto": "Sk2Im59ex", "comment": "This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work \"Generating Images with Perceptual Similarity Metrics based on Deep Networks\" by Alexey Dosoviskiy and Thomas Brox.  \n\nThe loss function in this work vs that of the latter: \nL_const is equal to L_feat;\nL_tid is equall to L_img;\nL_GANG is equal to L_adv.\n\nHowever, I do not see the later's name in your reference paper list.  Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss  on page 4?", "title": "About the reference and one minor error"}}}