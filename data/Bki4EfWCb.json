{"paper": {"title": "Inference Suboptimality in Variational Autoencoders", "authors": ["Chris Cremer", "Xuechen Li", "David Duvenaud"], "authorids": ["ccremer@cs.toronto.edu", "lxuechen@cs.toronto.edu", "duvenaud@cs.toronto.edu"], "summary": "We decompose the gap between the marginal log-likelihood and the evidence lower bound and study the effect of the approximate posterior on the true posterior distribution in VAEs.", "abstract": "Amortized inference has led to efficient approximate inference for large datasets. The quality of posterior inference is largely determined by two factors: a) the ability of the variational distribution to model the true posterior and b) the capacity of the recognition network to generalize inference over all datapoints. We analyze approximate inference in variational autoencoders in terms of these factors. We find that suboptimal inference is often due to amortizing inference rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.", "keywords": ["Approximate Inference", "Amortization", "Posterior Approximations", "Variational Autoencoder"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "Thank you for submitting you paper to ICLR. This paper provides an informative analysis of the approximation contributions from the various assumptions made in variational auto-encoders. The revision has demonstrated the robustness of the paper\u2019s conclusions, however these conclusions are arguably unsurprising. Although the work provides a thorough and interesting piece of detective work, the significance of the findings is not quite great enough to warrant publication.\n\nReviewer 1 was searching for a reference for work in similar vein to section 5.4: The second problem identified in the reference below shows examples where using an approximating distribution of a particular form biases the model parameter estimates to settings that mean the true posterior is closer to that form.\n\nR. E. Turner and M. Sahani. (2011) Two problems with variational Expectation Maximisation for time-series models. Inference and Learning in Dynamic Models. Eds. D. Barber, T. Cemgil and S. Chiappa, Cambridge University Press, 104\u2013123, 2011."}, "review": {"HyXty1qlM": {"type": "review", "replyto": "Bki4EfWCb", "review": "* EDIT: Increased score from 5 to 6 to reflect improvements made in the revision.\n\nThe authors break down the \"inference gap\" in VAEs (the slack in the variational lower bound) into two components: 1. the \"amortization gap\", measuring what part of the slack is due to amortizing inference using a neural net encoder, as compared to separate optimization per example. 2. the \"approximation gap\": the part of the slack due to using a restricted parametric form for the posterior approximation. They perform various experiments to analyze how these quantities depend on modeling decisions and data sets.\n\nBreaking down the inference gap into its components is an interesting idea and could potentially provide insights when analyzing VAE performance and for further improving VAEs. I enjoyed reading the paper, but I think its contribution is on the small side for a conference paper. It would be a good workshop paper. The main limitation of the proposed method of analysis I think is that the two parts of the inference gap are not really separable: Because the VAE encoder is trained jointly with the decoder, the different limitations of the encoder and decoder all interact. E.g. one could imagine cases where jointly training the VAE encoder and decoder finds a local optimum where inference is perfect, but which is still much worse than the optimum that could be achieved if the encoder would have been more flexible. The authors do seem to realize this and they provide experiments examining this interaction. I think these experiments should be elaborated on. For example: What happens when the decoder is trained separately using more flexible inference (e.g. Hamiltonian MC) and the encoder is trained later? What happens when the encoder is optimized separately for each data point during training as well as testing?", "title": "new way of analyzing slack in ELBO for VAEs, contribution is limited", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJ5VMfcxG": {"type": "review", "replyto": "Bki4EfWCb", "review": "=======\nUpdate:\n\nThe new version addresses some of my concerns. I think this paper is still pretty borderline, but I increased my rating to a 6.\n=======\n\nThis article examines the two sources of loose bounds in variational autoencoders, which the authors term \u201capproximation error\u201d (slack due to using a limited variational family) and \u201camortization error\u201d (slack due to the inference network not finding the optimal member of that family).\n\nThe existence of amortization error is often ignored in the literature, but (as the authors point out) it is not negligible. It has been pointed out before in various ways, however:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class).\n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation. That this approach works well implies that amortization error cannot be ignored.\n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network.\n* The very recent paper by Krishnan et al. (posted to arXiv days before the ICLR deadline, although a workshop version was presented at the NIPS AABI workshop last year; http://approximateinference.org/2016/accepted/KrishnanHoffman2016.pdf) examines amortization error as a core cause of training failures in VAEs. They also observe that the gap persists at test time, although it does not examine how it relates to approximation error.\n\nSince these earlier results existed, and approximation-amortization decomposition is fairly simple (although important!), the main contributions of this paper are the empirical studies. I will try to summarize the main novel (i.e., not present elsewhere in the literature) results of these:\n\nSection 5.1:\nInference networks with FFG approximations can produce qualitatively embarrassing approximations.\n\nSection 5.2:\nWhen trained on a small dataset, training amortization error becomes negligible. I found this surprising, since it\u2019s not at all clear why dataset size should lead to \u201cstrong inference\u201d. It seems like a more likely explanation is that the decoder doesn\u2019t have to work as hard to memorize the training set, so it has some extra freedom to make the true posterior look more like a FFG.\n\nAlso, I think it\u2019s a bit of an exaggeration to call a gap of 2.71 nats \u201cmuch tighter\u201d than a gap of 3.01 nats.\n\nSection 5.3:\nAmortization error is an important contributor to the slack in the ELBO on MNIST, and the dominant contributor on the more complicated Fashion MNIST dataset. (This is totally consistent with Krishnan et al.\u2019s finding that eliminating amortization error gave a bigger improvement for more complex datasets than for MNIST.)\n\nSection 5.4:\nUsing a restricted variational family causes the decoder to learn to induce posteriors that are easier to approximate with that variational family. This idea has been around for a long time (although I\u2019m having a hard time coming up with a reference).\n\nThese results are interesting, but given the empirical nature of this paper I would have liked to see results on more interesting datasets (Celeb-A, CIFAR-10, really anything but MNIST). Also, it seems as though none of the full-dataset MNIST models have been trained to convergence, which makes it a bit difficult to interpret some results.\n\n\nA few more specific comments:\n\n2.2.1: The \\cdot seems extraneous to me.\n\n5.1: What dataset/model was this experiment done on?\n\nFigure 3: This can be inferred from the text (I think), but I had to remind myself that \u201cIW train\u201d and \u201cIW test\u201d refer only to the evaluation procedure, not the training procedure. It might be good to emphasize that you don\u2019t train on the IWAE bound in any experiments.\n\nTable 2: It would be good to see standard errors on these numbers; they may be quite high given that they\u2019re only evaluated on 100 examples.\n\n\u201cWe can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.\u201d: Why not just compare the optimal with the AIS evaluation? If you trust the AIS estimate, then the result will be the actual KL divergence between the FFG and the true posterior.", "title": "An interesting topic, and some interesting results, but probably a bit below the bar.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1_Ulf9gz": {"type": "review", "replyto": "Bki4EfWCb", "review": "This paper studies the amortization gap in VAEs. Inference networks, in general, have two sources of approximation errors. One due to the function family of variational posterior distributions used in inference and the other due to choosing to amortize inference rather than doing per-data-point inference as in SVI.\n\nThey consider learning VAEs using two different choices of inference networks with (1) fully factorized Gaussian and (2) normalizing flows. The former is the de-facto choice of variational approximation used in VAEs and the latter is capable of expressing complex multi-modal distributions.\n\nThe inference gap is log p(x) - L[q], the approximation gap is log p(x) - L[q^* ] and the amortization gap is L[q^* ] - L[q]. The amortization gap is easily evaluated. To evaluate the first two, the authors use estimates (lower bounds of log p(x)) given from annealed importance sampling and the importance sampling based IWAE bound (the tighter of the two is used).\n\nThere are several different observations made via experiments in this work but one of the more interesting ones is quantifying that a deep generative model, when trained with a fully factorized gaussian posterior, realizes a true posterior distribution that is (more) approximately Gaussian. While this might be (known) intuition that people rely on when learning deep generative models, it is important to be able to test it, as this paper does. The authors study several discrete questions about the aforementioned inference gaps and how they vary on MNIST and FashionMNIST. The concerns I have about this work revolve around their choice of two small datasets and how much their results are affected by variance in the estimators.\n\nQuestions:\n* How did you optimize the variational parameters for q^* and the flow parameters in terms of learning rate, stopping criteria etc.\n* In Section 5.2, what is \"strong inference\"? This is not defined previously.\n* Have you evaluated on a larger dataset such as CIFAR? FashionMNIST and MNIST are similar in many ways.\n* Which kind of error would using a convolution architecture for the encoder decrease? Do you have insights on the role played by the architecture of the inference network and generative model?\n\nI have two specific concerns:\n* Did you perform any checks to verify whether the variance in the estimators use to bound log p(x) is controlled (for the specific # samples you use)? I'm concerned since the evaluation is only done on 100 points.\n* In Section 5.2.1, L_iw is used to characterize encoder overfitting where the argument is that L_ais is not a function of the encoder, but L_iw is, and so the difference between the two summarizes how much the inference network has overfit. How is L_iw affected by the number of samples used in the estimator? Presumably this statement needs to be made while also keeping mind the number of importance samples. For example, if I increase the number of importance samples, even if I'm overfitting in Fig 3(b), wouldn't the green line move towards the red simply because my estimator depends less on a poor q?\n\nOverall, I think this paper is interesting and presents a quantitative analysis of where the errors accrue due to learning with inference networks. The work can be made stronger by addressing some of the questions above such as what role is played by the neural architecture and whether the results hold up under evaluation on a larger dataset.", "title": "This paper studies and attempts to break down the sources of errors in doing inference in VAEs on MNIST.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJ201f3QG": {"type": "rebuttal", "replyto": "HyXty1qlM", "comment": "\nWe would like to thank Reviewer 3 for providing a detailed review and interesting suggestions for further experimentation. \n\nOverall, we acknowledge that the different limitations of the encoder and decoder all interact, e.g. in Section 5.3 we have quantitatively demonstrated that a VAE trained with a factorized Gaussian, typically have a true posterior that is more like a factorized Gaussian. Without doubt, we also agree that there could be cases where the generative model fits embarrassingly to the data, and yet inference is perfect. However, this does not hinder our analysis of the gaps according to our definitions (Section 3.1). We would also like to note that, although our calculations of the gaps are only estimates, such a amortization-approximation decomposition may be valuable to guiding improvements to approximate inference. \n\n\"What happens when the decoder is trained separately using more flexible inference (e.g. Hamiltonian MC) and the encoder is trained later? What happens when the encoder is optimized separately for each data point during training as well as testing?\"\n\nThese are interesting ideas. We performed local optimization of the variational parameters only for evaluation purposes. The quality of inference is an important factor for the optimization of the generator. Consequently, training with HMC would likely result in a better trained generator compared to training via amortized inference, especially early during training. We refer to [1] for experiments that explore optimizing the variational parameters in the inner loop of amortized inference during training.\n\n[1] R. G. Krishnan, D. Liang, and M. Hoffman.  On the challenges of learning with inference networks on sparse, high-dimensional data.ArXiv e-prints, October 2017\n", "title": "Response to Reviewer 3"}, "By5gZbnmf": {"type": "rebuttal", "replyto": "r1_Ulf9gz", "comment": "\nWe would like to thank Reviewer 2 for their thorough analysis of our work. We acknowledge their concerns and address their comments below:\n\n\u201cHow did you optimize the variational parameters for q^* and the flow parameters in terms of learning rate, stopping criteria etc.\u201d\n\nThank you for asking. We have added the description in section 6.4 of the Appendix.\n\n\"What is \"strong inference\"? This is not defined previously.\"\n\nBy strong inference, we mean there is a small inference gap. We understand that this could be confusing given no prior explanation, thus we\u2019ve changed the wording accordingly. \n\n\u201cHave you evaluated on a larger dataset such as CIFAR? FashionMNIST and MNIST are similar in many ways.\u201d\n\nWe acknowledge that both MNIST and Fashion-MNIST are similar datasets. To enhance our analysis, we performed some new experiments on CIFAR-10 whose result is added to Table 2 and analyzed in section 5.2. \n\n\u201cWhich kind of error would using a convolution architecture for the encoder decrease?\" \n\nAlthough we have not experimented extensively on the influence of the encoder architectures, more powerful encoders usually lead to lower amortization error. Our experiments with larger encoders demonstrates this (Section 5.2). \n\n\"Do you have insights on the role played by the architecture of the inference network and generative model?\u201d\n\nThank you for the interesting question. Our most recent draft contains new results exploring the effect of increasing the capacity of the generative model. We observe that increasing the capacity leads to true posteriors that fit better to the choice of approximation. (Section 5.3)\n\n\u201cDid you perform any checks to verify whether the variance in the estimators use to bound log p(x) is controlled (for the specific # samples you use)? I'm concerned since the evaluation is only done on 100 points.\u201d\n\nWe acknowledge that the variance of the bounds can be quite large, and the numbers we obtained for evaluating 100 datapoints might suffer from this. Thus, we re-performed all experiments on MNIST and Fashion-MNIST with 1k datapoints to reduce the variance. The new results are consistent with our previous results.\n\n\"Presumably this statement needs to be made while also keeping mind the number of importance samples.\"\n\nThank you for pointing this out. Yes, this statement needs to be made while also keeping in mind the number of importance samples, since measuring the overfitting is dependent on the number of samples.\n\n", "title": "Response to Reviewer 2"}, "Byip9x2XG": {"type": "rebuttal", "replyto": "rJ5VMfcxG", "comment": "\nWe would like to thank Reviewer 1 for their detailed comments regarding our contributions and providing citations of relevant work.\n\nWe address their comments below:\n\n\u201cWhen trained on a small dataset, training amortization error becomes negligible. I found this surprising, since it\u2019s not at all clear why dataset size should lead to 'strong inference' \"\n\nWe believe the explanation for better inference on a smaller dataset is mostly due to the encoder having fewer datapoints to memorize, reducing the amortization error. Our analysis with larger encoders in Section 5.2 is relevant to supporting this claim. \n\n\"It seems like a more likely explanation is that the decoder doesn\u2019t have to work as hard to memorize the training set, so it has some extra freedom to make the true posterior look more like a FFG.\"\n\nThis idea is interesting. We believe that the decoder having to work less hard is related to reducing the approximation error. Our results of Section 5.3 explore this idea.\n\n\"Also, I think it\u2019s a bit of an exaggeration to call a gap of 2.71 nats \u201cmuch tighter\u201d than a gap of 3.01 nats.\"\n\nYes, we agree. We have re-worded that statement.\n\n\"Using a restricted variational family causes the decoder to learn to induce posteriors that are easier to approximate with that variational family.\"\n\nYes, this idea has been around for a while. One example is demonstrated with visualizations in Appendix C of the IWAE paper, which we\u2019ve noted in the Related Works section. Our results provide quantitative measurements of this intuition. \n\n\u201cThese results are interesting, but given the empirical nature of this paper I would have liked to see results on more interesting datasets (Celeb-A, CIFAR-10, really anything but MNIST). \u201d\n\nWe agree that more extensive empirical results are important. To this end, we performed new experiments on CIFAR-10 whose results are added to Table 2 and section 5.2. \n\n\u201cThe \\cdot seems extraneous to me.\u201d\n\nThank you for pointing it out, we have fixed this. \n\n\"What dataset/model was this experiment done on?\"\n\nWe trained our VAE models on MNIST for the visualization. \n\n\u201cIt would be good to see standard errors on these numbers; they may be quite high given that they\u2019re only evaluated on 100 examples.\u201d\n\nWe acknowledge that the variance of the bounds can be quite large, and the numbers we obtained for evaluating 100 datapoints might suffer from this. Thus, we re-performed all experiments on MNIST and Fashion-MNIST with 1k datapoints to reduce the variance. The new results are consistent with our previous results.\n\n\u201cWhy not just compare the optimal with the AIS evaluation? If you trust the AIS estimate, then the result will be the actual KL divergence between the FFG and the true posterior.\u201d\n\nThank you for pointing this out. We have corrected the analysis accordingly. \n\n", "title": "Response to Reviewer 1"}, "S14LJgnmG": {"type": "rebuttal", "replyto": "Bki4EfWCb", "comment": "\nWe\u2019d like to thank the reviewers for the thoughtful and thorough reviews. \n\nThe consensus of the reviews is that the contribution of the original paper was limited, of which we completely agree. We\u2019ve thus taken steps to extend our results with relevant experiments. We\u2019ve used the same methodology as before in settings that we think highlight and strengthen important points about the paper.  \n\nMain additions:\n\nCIFAR-10: we\u2019ve run the same experiments on the CIFAR-10 dataset in order to gain a more comprehensive view of inference suboptimality. (see Section 5.2 and Table 2)\n\nMore datapoints: previously we evaluated the various gaps on a subset of 100 datapoints. We\u2019ve increased the subset to 1000 datapoints for most experiments in order to make our results more reliable. The new results are consistent with our previous results.\n\nInfluence of flows on amortization: we demonstrate that the parameters used in increasing the expressiveness of the approximate distribution also contribute to reducing the amortization error. (see section 5.2.1 and Table 4)\n\nInfluence of decoder capacity on approximation gap: we demonstrate that increasing the number of hidden layers of the decoder leads to smaller approximation gaps. (see Section 5.3 and Table 5)\n\n\nOther modifications:\n\nTitle: We changed the title of the paper from \u2018Inference Dissection in Variational Autoencoders\u2019 to \u2018Inference Suboptimality in Variational Autoencoders\u2019 because we believe it better reflects the content of the paper.\n\nOrganization: We\u2019ve moved some less relevant sections to the appendix, such as the description of AIS and our section on VAE under/overfitting.\n\nAbstract: We\u2019ve updated the abstract given the new contributions.\n", "title": "Updated Paper"}}}