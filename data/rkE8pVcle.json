{"paper": {"title": "Learning through Dialogue Interactions by Asking Questions", "authors": ["Jiwei Li", "Alexander H. Miller", "Sumit Chopra", "Marc'Aurelio Ranzato", "Jason Weston"], "authorids": ["jiwel@fb.com", "ahm@fb.com", "spchopra@fb.com", "ranzato@fb.com", "jase@fb.com"], "summary": "We investigate how a bot can benefit from interacting with users and asking questions.", "abstract": "A good dialogue agent should have the ability to interact with users by both responding to questions and by asking questions, and importantly to learn from both types of interactions. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow such interactions between a learner and a teacher. We investigate how a learner can benefit from asking questions in both offline and online reinforcement learning settings, and demonstrate that the learner improves when asking questions. Our work represents a first step in developing such end-to-end learned interactive dialogue agents.\n", "keywords": ["Natural language processing"]}, "meta": {"decision": "Accept (Poster)", "comment": " This paper is a clear accept. Reviewers were both positive and confident about their assessments. Paper introduces a simulator and synthetic question answering task where interactions with the teacher are used for learning. Reviewers felt paper was well written with clear descriptions of tasks, models and experiments. Reviewer did comment on limitations due to the simple factoid QA framework explored for which hand crafted rules seems sufficient to solve the problem."}, "review": {"HJFZRRALg": {"type": "rebuttal", "replyto": "SJrTmF5Ig", "comment": "Thanks, fixed.", "title": "Typos"}, "B10HFyNLg": {"type": "rebuttal", "replyto": "BksctemUl", "comment": "Thanks, we've made a few more updates to the paper in line with your comments above.\n\n> It seems like the human experiments are carried out in the supervised learning framework? Is that correct? If yes, please \n> clarify this and the reasons you do it. Overall, I think it's reasonable as it would allow you to run other models against the \n> same dataset in the future. \n\nYes, it is in the supervised framework. As you say, it allows simplicity of, and reproducibility of experiments. We have clarified this in the paper.\n\n>It's interesting that human feedback helps a lot on Task 8, but makes only a small difference on Task 4. \n> Can you comment on that? Could one reason be that Task 4 involves binary feedback from the user (a yes/no response), \n>  while Task 8 involves interpreting factual information? \n\nYes, this is line with the original results from the simulator data. Binary feedback about which facts are relevant to answer a question is a lot weaker information than the direct label information given in Task 8. \n\n>Minor notes: \n> - You mention the AMT dialogues have a lot of \"noise\". Can you clarify this? Are you referring to word misspellings, >grammatical errors or misinformation (e.g. user is giving incorrect responsible)? Have you thought about ways of adding >this noise into your simulator?\n\nAll of the above (misspellings, grammar, mislabeled answers) although we were thinking more of the last one (we observed all of them in the data). We did not attempt to correct any of these. Yes, it would be possible to add some of these things to the simulator, but we are not sure it would help understanding...but maybe we could make the performance more aligned with real data results and/or use this to improve results on real test data using simulated data alone?\n\n> - \"training set was times smaller due to data collection costs\" -> \"training set was smaller due to data collection costs\"\n\nCorrected.\n\n> - With the additional AMT results, I strongly encourage you to shorten the paper by moving some of your result tables (maybe Table 2) to the appendix. This would also give you space to discuss the results in depth.\n\nDone.\n\n> - Finally, it's interesting to note the differences between training with simulated synthetic data and testing on human \n> generated data versus training and testing on human generated data - in particular for Task 8. In fact, this illustrates show\n> how different the simulations actually are from real interactions with humans.\n\nAgreed.\n\n", "title": "Re: Mechanical Turk Data and Experiments Now Added!"}, "rkHJQkQIg": {"type": "rebuttal", "replyto": "rkOw_vlrl", "comment": "Note that we have now made a major update to the paper, adding human experiments using Mechanical Turk!\n\nPlease see comments to the other reviewers, and the updated paper, in particular sections 4.2 (data description) and 6.2 (results), as well as further details and analysis in the appendix.", "title": "Mechanical Turk Data and Experiments Now Added!"}, "Hy8JfJmIx": {"type": "rebuttal", "replyto": "ByBwoq4Bl", "comment": "> \" I think it's reasonable to also expect this paper to include human evaluations.\"\n> \"For example, how about experimenting with a question-answering task in the movie domain?\"\n\nWe have now updated the paper to include human experiments using Mechanical Turk!\n\nWe collected real human language data with 10,000 episodes of training, 1000 for validation,\nand 2500 for testing for two of the tasks (4 and 8). We have compared the same learning algorithms on this new data and, despite accuracy results being lower  (which is expected, as we are now using fewer, but real examples), we arrive at the same main conclusion: the presented algorithms can learn to perform better by asking questions.\n\nPlease see the updated paper, in particular sections 4.2 (data description) and 6.2 (results), as well as further details and analysis in the appendix.\n\nWe hope that this addresses your main concern.", "title": "Mechanical Turk Data and Experiments Now Added!"}, "rkEs-kXIe": {"type": "rebuttal", "replyto": "HJQUz1zNe", "comment": "> \"Of course, the paper would be much more convincing with human experiments. \"\n\nWe have now updated the paper to include human experiments using Mechanical Turk!\n\nWe collected real human language data with 10,000 episodes of training, 1000 for validation,\nand 2500 for testing for two of the tasks (4 and 8). We have compared the same learning algorithms on this new data and, despite accuracy results being lower  (which is expected, as we are now using fewer, but real examples), we arrive at the same main conclusion: the presented algorithms can learn to perform better by asking questions.\n\nPlease see the updated paper, in particular sections 4.2 (data description) and 6.2 (results), as well as further details and analysis in the appendix.\n\nWe hope that this addresses your main concern.\n\n\n", "title": "Mechanical Turk Data and Experiments Now added!"}, "ryfz7oZBx": {"type": "rebuttal", "replyto": "H1Ds5pt4l", "comment": "We think there are two ways to go given a reasonable paper length: depth or breadth. We preferred to have breath of exploration, focusing our investigation on a large variety of simulated tasks so we could understand better the problem. Human eval comes next in a subsequent paper as a study worth trying, which we did not know a priori. \n\nNote that several papers have proposed synthetic learning tasks and yet they have been very useful and well regarded, at least looking at the number of citations they have so far received:\n\n\u201cAn Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation\u201d, Larochelle et al., ICML'07 (419 citations on Google Scholar)\n\n\u201cToward AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\u201d, Weston et al., ICLR '16 (159 citations on Google scholar).\n\nValue Iteration Networks, Tamar et al., NIPS '16 (best paper award).\n\n(Indeed, much of the RL literature contains simulated problems.)\nGenerally researchers explore toy problems when the task is challenging and new, and when the understanding is currently lacking, so real data at that stage is less helpful.\n", "title": "Re: Response to Review (AnonReviewer1)"}, "rkOw_vlrl": {"type": "rebuttal", "replyto": "rk4SsHsNg", "comment": ">-- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper's contributions? \n\nvanilla-MemN2N is a standard baseline so we should compare to it.\nCont-MemN2N is a simple improvement introduced in this paper which can make a big difference.\n\n> -- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems. \n\nMissing words in this task are always in the non-entity words. Missing entities are Tasks 5-9. Still the unknown misspelled words can make the question ambiguous (e.g. do you want to know the director or writer of a movie?).\n\n> -- The first point on Page 10 \"The performance of TestModelAQ is worse than TestAQ but better than TestQA.\" is not true for Task 2 from the numbers in Tables 2 and 4. \n\nHas been fixed and re-uploaded. Thanks.\n\n> -- What happens if the conversational history is smaller or none? \n\nThe results are likely somewhat better. The code is now released so many variations can be tried.\n\n>-- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. \n\nWhile it's true the good student does have the necessary information already, when asking the question and getting the right answer the correct response is more immediately evident later, rather than trying to fish it out of the known facts. You can think of this as stronger features given to a classifier, if you like, which helps. Hence, the good student still benefits from question asking in these tasks.\n\n> -- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this? \n\nThe reason why the poor student can still do fairly well is that we introduce typos to some of the non-entity words in the questions but not all of them. The student can still to some extent understand (or guess correctly) the meaning of some of the questions. This is in line with Table 3, we are using the Cont-MemN2N in the online experiments as well as it worked better in the offline experiments, and indeed works relatively well in this setting -- but still works better when asking questions.\n\n\n> -- Figure 1, Task 2 AQ, last sentence should have a negative response \"(-)\" instead of positive as currently shown. \n\nHas been fixed and re-uploaded. Thanks.\n\n", "title": "Re: ICLR 2017 conference paper220 AnonReviewer3"}, "SJpFcC8Nx": {"type": "rebuttal", "replyto": "Hyk3MjWEe", "comment": "As far as we know, there is very little in the literature (see related work) on what the reviewer agrees is an important direction:  dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone).\nTo start in a new direction like this you have to have to start with a first approach. One has to see in a relatively simple setup if it works at all, and understand where it does and does not work. \nNow that we have showed some modest success on the tasks we have developed we hope: (i) both we and other researchers can develop better techniques and test them in this setup; and (ii) we can continue on to more difficult, real-world challenges.\nSo far, we are happy to show the positive result that a bot asking questions can significantly improve results in the setup we tested on.\n\nReply to other comments:\n- \u201cinteractive dialogue agents\u201d meaning bots that can both answer questions and ask questions,  where learning is exhibited from the latter. We have now reworded and clarify this.\n- R1 states \u201cA major limitation of the experiments is that the questions the agent can ask are specified a priori\u201d \u2014 however, that is not correct.  In the TestModelAQ setting the model has to get the form of the question correct as well. E.g. in the  Question Verification and Knowledge Verification tasks there are many possible ways of forming the question and some of them are correct \u2014 the model has to choose the right question to ask. E.g. it should ask \u201cDoes it have something to do with the fact that Larry Crowne directed by Tom Hanks?\u201d rather than \u201cDoes it have something to do with the fact that Forrest Gump directed by Robert Zemeckis?\u201d when the latter is irrelevant (the candidate list of questions is generated from the known knowledge base entries with respect to that question). We have now clarified this further in the text.\n-  However, in the RL experiment we did not use TestAQ so R1 is correct in that case. We have now removed the \"what to ask\" phrasing there.\n- FP (forward prediction) is very important to measure because our goal is to find a model that can learn during dialogue. This means asking questions, and learning from the user's responses, which is what this method does (the reward-based method doesn't really use the user's responses). The FP method outperforms the reward-based method on some of the tasks which is a very encouraging sign.\n", "title": "Response to Review (AnonReviewer1)"}, "SkdVGfvVl": {"type": "rebuttal", "replyto": "HJQUz1zNe", "comment": "Yes, please also see our response to AnonReviewer1 concerning the nature of the dataset.\nThe advantage of simulated datasets is that one can break down the analysis, e.g. we have identified types of questions that can be asked, and now we can test which models are capable of benefitting from them. We can also test a reinforcement learning setting easily without incurring impractical data collection costs during model development. Now that we have some (limited) success future work can try other models (many of our results can still clearly be improved e.g. they are in the 10 - 20% error region when they could be <5% at least) and develop more challenging/real datasets.\n\nFor the first set of tasks (Question Clarification) we only reported experiments in the \u201ctypo\u201d domain for simplicity. As R1 comments, we already have lot of experiments, so we didn't add more. However, we actually did implement other 'misunderstanding the surface form' variants but left them out for simplicity and space reasons. In particular, for each question type, we have a set number of ways of asking it (different phrase templates) and we tried leaving out 1 or 2 (can be a hyperparameter) such phrases that are only seen in test, and that the bot can ask a question about to receive a rephrasing that it does know about. This is a setting that also closely mimics a real world situation.\n", "title": "Response to Review (AnonReviewer4)"}, "SyImZnyEl": {"type": "rebuttal", "replyto": "rkE8pVcle", "comment": "Dear all, \n\nWe have released the data, code, and simulator described in the paper at https://github.com/facebook/MemNN/tree/master/AskingQuestions\n\nWe also updated the paper (https://openreview.net/pdf?id=rkE8pVcle) where we run each experiment in the supervised setting for 10 times to avoid training variations. \n", "title": "Code/data release, paper update"}, "HkLVy7fXx": {"type": "rebuttal", "replyto": "HyqkbTgQl", "comment": "For Task 7 and 8, the results are off by one column. Sorry for the mistake. We just corrected them and uploaded the updated results. \n\nFor Tasks 4 and 9, the fact that the result of mix does not lie between AQ and QA is because of training variation and it disappears when we reran each experiment for 10 times and reported the best result. We have updated the result\n\nWe use the same  dataset as described  in https://arxiv.org/abs/1511.06931. The training/dev/test sets respectively contain 181638/9702/9698 examples. The  accuracy metric corresponds to the percentage of times the student gives correct answers to the teacher's questions.\n", "title": "RE: Clarification questions"}, "r1O5iiyXl": {"type": "rebuttal", "replyto": "Hk8R-QRGl", "comment": "\n- No, it does not require a KB. Memory Networks, which we use as our basic architecture, have been shown to work well from both raw text and KBs, see this paper for a study:  https://arxiv.org/abs/1606.03126, e.g. Memory Network variants obtain state-of-the-art results on WikiQA which uses raw text, not a KB.\n\n- Yes, the model can potentially recover from missing KB entries because that is only used as context  \u2014 that is,  the final output can be any of the possible candidate responses (in this case, all possible entities) and is only conditioned on the dialog and context. Of course it is harder though.\n\n- Yes, it would be straight-forward to switch from a KB to a raw text setting in our paper, e.g. using Wikipedia as the knowledge source in our experiments, similar to the setup in the paper already mentioned ( https://arxiv.org/abs/1606.03126). However, this distinction is not the focus of the paper.\n\n- It is worth mentioning that the \"Knowledge Acquisition\" tasks deal specifically with the issue of missing entities in the retrieval step when there are unknown words. One can learn about these entities by asking questions to improve performance, i.e. this is a way for \u201c the model to recover from missing elements in the knowledge base\u201d.", "title": "Re: Knowledge Bases"}, "BkD3twXmx": {"type": "rebuttal", "replyto": "BkYhsYlXl", "comment": "Yes, to some degree it has to guess based on the entity mentioned plus any clues in the other words it can understand (e.g. a \u201cwho\u201d vs. \u201cwhat\u201d question, the word \u201cin\u201d appearing, etc.).  Character level models would  only help further if the new word is similar to an existing one (in the example given this is the case, but it might not be). Still they are definitely worth investigating in the future, too.\n\nIt's important to note that it's not just about how difficult the question is to ask, but whether the model can make use of that knowledge (the response from the teacher to the question asked) afterwards.   We find that asking questions performs better than not asking them (i.e. Train AQ+Test AQ  gives better results than  Train QA + Test QA or other combinations).\n\n", "title": "Re: question clarification tasks"}, "HyqkbTgQl": {"type": "review", "replyto": "rkE8pVcle", "review": "In the offline supervised setting, since TrainMix is a combination of TrainQA and TrainAQ, I would expect the performance for TrainMix to always be somewhere in between TrainQA and TrainAQ. But, that doesn't seem to be the case in Tasks 4 and 8 (when testing on TestQA), and Task 7 (when tested on both TestQA and TestAQ) for vanilla-MemN2N, and in Task 9 for cont-MemN2N when tested on TestAQ. Why?\n\nAlso, what are the sizes of train and test sets being used? What is the accuracy metric for numbers reported in Tables 2, 3 and 4?\nThe paper introduces a simulator and a set of synthetic question answering tasks where interaction with the \"teacher\" via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions.  \n\n-- The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction.\n-- The paper studies three different types of tasks where the agent can benefit from user feedback.\n-- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings.\n\nOther comments/questions: \n-- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper's contributions?\n-- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems.\n-- The first point on Page 10 \"The performance of TestModelAQ is worse than TestAQ but better than TestQA.\" is not true for Task 2 from the numbers in Tables 2 and 4.\n-- What happens if the conversational history is smaller or none? \n-- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. \n-- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this?\n-- Figure 1, Task 2 AQ, last sentence should have a negative response \"(-)\" instead of positive as currently shown. \n\nPreliminary Evaluation: \nA good first step in the research direction of learning dialogue agents from unstructured user interaction. ", "title": "Clarification questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rk4SsHsNg": {"type": "review", "replyto": "rkE8pVcle", "review": "In the offline supervised setting, since TrainMix is a combination of TrainQA and TrainAQ, I would expect the performance for TrainMix to always be somewhere in between TrainQA and TrainAQ. But, that doesn't seem to be the case in Tasks 4 and 8 (when testing on TestQA), and Task 7 (when tested on both TestQA and TestAQ) for vanilla-MemN2N, and in Task 9 for cont-MemN2N when tested on TestAQ. Why?\n\nAlso, what are the sizes of train and test sets being used? What is the accuracy metric for numbers reported in Tables 2, 3 and 4?\nThe paper introduces a simulator and a set of synthetic question answering tasks where interaction with the \"teacher\" via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions.  \n\n-- The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction.\n-- The paper studies three different types of tasks where the agent can benefit from user feedback.\n-- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings.\n\nOther comments/questions: \n-- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper's contributions?\n-- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems.\n-- The first point on Page 10 \"The performance of TestModelAQ is worse than TestAQ but better than TestQA.\" is not true for Task 2 from the numbers in Tables 2 and 4.\n-- What happens if the conversational history is smaller or none? \n-- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. \n-- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this?\n-- Figure 1, Task 2 AQ, last sentence should have a negative response \"(-)\" instead of positive as currently shown. \n\nPreliminary Evaluation: \nA good first step in the research direction of learning dialogue agents from unstructured user interaction. ", "title": "Clarification questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkYhsYlXl": {"type": "review", "replyto": "rkE8pVcle", "review": "Unless I'm mistaken, your models seem to use word embeddings (or the average of surrounding word embeddings). Since it's not at the character level, it's unclear how the model is able to do question verification (i.e. Figure 1, bottom right), other than completely guessing based on the entity mentioned. Is that correct? \n\nOn the other hand, the question paraphrase AQ setting seems like very easy behavior to learn (if you encounter an unknown word, ask the clarification question, then proceed normally).The goal of this paper is to analyze the behaviour of dialogue agents when they must answer factoid questions, but must query an oracle for additional information. This can be interpreted as a form of interaction between the dialogue agent and a \u2018teacher\u2019.\n\nThe problem under investigation is indeed very important. The authors create a synthetic environment in which to test their agent. The main strength of the paper is that the paper tests many different combinations of environments, where either some knowledge is missing (and the agent has to query for it), or there is some misspelling in the teacher\u2019s question, and different ways the agent can ask for extra information. \n\nI am a bit concerned that many of the tasks are too easy (e.g. the AQ question paraphrase), and I am also concerned that the environment presented is very limited, and quite far (in terms of richness of linguistic structure) from how real humans would interact with chatbots. I think the paper would be better positioned as testing the basic reasoning capabilities of agents/ their ability to do question answering, rather than dialogue. However, I think the \u2018ground-up\u2019 approach that starts with simple environments is indeed worthy of analysis, and this paper makes an interesting contribution in that direction. Of course, the paper would be much more convincing with human experiments. \n\nAdditional notes:\nI think the simulation, in the synthetic environment, for the first mistake a learner can make during dialogue: \u201cthe learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question\u201d, is particularly limited since only word misspellings are considered (and the models used don\u2019t work at the character level), which is of course only a tiny fraction of ways an agent can misunderstand the context. I would be particularly interested to see some discussion of how the authors plan to scale this up to more realistic settings.\n\nEDIT: I have updated my score to reflect the addition of the Mechanical Turk experiments", "title": "question clarification tasks", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJQUz1zNe": {"type": "review", "replyto": "rkE8pVcle", "review": "Unless I'm mistaken, your models seem to use word embeddings (or the average of surrounding word embeddings). Since it's not at the character level, it's unclear how the model is able to do question verification (i.e. Figure 1, bottom right), other than completely guessing based on the entity mentioned. Is that correct? \n\nOn the other hand, the question paraphrase AQ setting seems like very easy behavior to learn (if you encounter an unknown word, ask the clarification question, then proceed normally).The goal of this paper is to analyze the behaviour of dialogue agents when they must answer factoid questions, but must query an oracle for additional information. This can be interpreted as a form of interaction between the dialogue agent and a \u2018teacher\u2019.\n\nThe problem under investigation is indeed very important. The authors create a synthetic environment in which to test their agent. The main strength of the paper is that the paper tests many different combinations of environments, where either some knowledge is missing (and the agent has to query for it), or there is some misspelling in the teacher\u2019s question, and different ways the agent can ask for extra information. \n\nI am a bit concerned that many of the tasks are too easy (e.g. the AQ question paraphrase), and I am also concerned that the environment presented is very limited, and quite far (in terms of richness of linguistic structure) from how real humans would interact with chatbots. I think the paper would be better positioned as testing the basic reasoning capabilities of agents/ their ability to do question answering, rather than dialogue. However, I think the \u2018ground-up\u2019 approach that starts with simple environments is indeed worthy of analysis, and this paper makes an interesting contribution in that direction. Of course, the paper would be much more convincing with human experiments. \n\nAdditional notes:\nI think the simulation, in the synthetic environment, for the first mistake a learner can make during dialogue: \u201cthe learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question\u201d, is particularly limited since only word misspellings are considered (and the models used don\u2019t work at the character level), which is of course only a tiny fraction of ways an agent can misunderstand the context. I would be particularly interested to see some discussion of how the authors plan to scale this up to more realistic settings.\n\nEDIT: I have updated my score to reflect the addition of the Mechanical Turk experiments", "title": "question clarification tasks", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hk8R-QRGl": {"type": "review", "replyto": "rkE8pVcle", "review": "The approach you propose seems to require a knowledge base (KB). In other words, the human designers of the dialogue system need to hand-craft what elements constitutes knowledge for the model. It could be problematic if certain types of elements are not represented in that knowledge base. Is there any way for the model to recover from missing elements in the knowledge base? And also, have you thought of ways to extend this approach to less structured domains (e.g. to assume that no knowledge base is available)?This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.\n\nThe motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.\n\nBecause of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.\n\n\nOther comments:\n- The abstract uses the phrase \"interactive dialogue agents\". What is meant by \"interactive\" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive?\n- A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states \"For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)\". This means the agent learns *when* to ask questions but not *what* questions to ask.\n- Related to the previous comment, in the sub-section \"ONLINE REINFORCEMENT LEARNING (RL)\" the paper states \"We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask.\". Please clarify this by removing the part \"what to ask\".\n- The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the \"TrainAQ(+FP)\" and \"TrainMix\" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix.\n- Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.\n\n--- UPDATE ---\n\nFollowing the discussion below and the additional experiments provided by the authors, I have increased my score to 8.", "title": "Knowledge Bases", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Hyk3MjWEe": {"type": "review", "replyto": "rkE8pVcle", "review": "The approach you propose seems to require a knowledge base (KB). In other words, the human designers of the dialogue system need to hand-craft what elements constitutes knowledge for the model. It could be problematic if certain types of elements are not represented in that knowledge base. Is there any way for the model to recover from missing elements in the knowledge base? And also, have you thought of ways to extend this approach to less structured domains (e.g. to assume that no knowledge base is available)?This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.\n\nThe motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.\n\nBecause of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.\n\n\nOther comments:\n- The abstract uses the phrase \"interactive dialogue agents\". What is meant by \"interactive\" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive?\n- A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states \"For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)\". This means the agent learns *when* to ask questions but not *what* questions to ask.\n- Related to the previous comment, in the sub-section \"ONLINE REINFORCEMENT LEARNING (RL)\" the paper states \"We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask.\". Please clarify this by removing the part \"what to ask\".\n- The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the \"TrainAQ(+FP)\" and \"TrainMix\" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix.\n- Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.\n\n--- UPDATE ---\n\nFollowing the discussion below and the additional experiments provided by the authors, I have increased my score to 8.", "title": "Knowledge Bases", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}