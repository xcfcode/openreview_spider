{"paper": {"title": "Learning Human Postural Control with Hierarchical Acquisition Functions", "authors": ["Nils Rottmann", "Tjasa Kunavar", "Jan Babic", "Jan Peters", "Elmar Rueckert"], "authorids": ["rottmann@rob.uni-luebeck.de", "tjasa.kunavar@ijs.si", "jan.babic@ijs.si", "mail@jan-peters.net", "rueckert@ai-lab.science"], "summary": "This paper presents a computational model for efficient human postural control adaptation based on hierarchical acquisition functions with well-known features. ", "abstract": "Learning control policies in robotic tasks requires a large number of interactions due to small learning rates, bounds on the updates or unknown constraints. In contrast humans can infer protective and safe solutions after a single failure or unexpected observation. \nIn order to reach similar performance, we developed a hierarchical Bayesian optimization algorithm that replicates the cognitive inference and memorization process for avoiding failures in motor control tasks. A Gaussian Process implements the modeling and the sampling of the acquisition function. This enables rapid learning with large learning rates while a mental replay phase ensures that policy regions that led to failures are inhibited during the sampling process.    \nThe features of the hierarchical Bayesian optimization method are evaluated in a simulated and physiological humanoid postural balancing task. We quantitatively compare the human learning performance to our learning approach by evaluating the deviations of the center of mass during training. Our results show that we can reproduce the efficient learning of human subjects in postural control tasks which provides a testable model for future physiological motor control tasks. In these postural control tasks, our method outperforms standard Bayesian Optimization in the number of interactions to solve the task, in the computational demands and in the frequency of observed failures. ", "keywords": ["Human Postural Control Model", "Hierarchical Bayesian Optimization", "Acquisition Function"]}, "meta": {"decision": "Reject", "comment": "The paper proposes hierarchical Bayesian optimization (HiBO) for learning control policies from a small number of environment interaction and applies it to the postural control of a humanoid. Both reviewers raised issues with the clarity of presentation, as well as contribution and overall fit to this venue. The authors\u2019 response helped to clarify these issues only marginally. Therefore, primarily due to lack of clarity, I recommend rejecting this paper, but encourage the authors to improve the presentation as per the reviewers\u2019 suggestions and resubmitting."}, "review": {"rkxtaO2mcr": {"type": "review", "replyto": "S1eYchEtwH", "review": "After rebuttal:\n\nThank you to the authors for responding to my review.\n\n1) The title of the conference is \"... on Learning Representations\". As I stated in the review (\"no, e.g., neural networks are employed\"), neural networks are an *example* of, but do not subsume, all representation learning methods. Therefore, I agree that papers that do not cover neural networks are welcome at the conference. However, as I stated in the review, my evaluation of the method proposed in the submission is that it does not concern representation learning (\"The employed features in Table 3 are handcrafted\"). I believe this evaluation is defensible, but of course the final evaluation is up to the chairs. However, I note that the authors did not respond directly to my evaluation that the method is not engaging in representation learning.\n\n2-7) As the other reviewer notes, the paper lacks clarity in many places, and does not sufficiently discuss prior work, including in postural control (there is one citation in the references that is not mentioned in the main text), hierarchical Bayesian optimization within or without a Gaussian processes framework (https://scholar.google.com/scholar?hl=fr&as_sdt=0%2C5&q=hierarchical+bayesian+optimization&btnG=), or experience replay (https://scholar.google.com/scholar?hl=fr&as_sdt=0%2C5&q=replay+machine+learning&btnG=). Therefore, it is difficult to ascertain the research contribution.\n\nAs such, I stand by my evaluation that this submission is not ready for publication at ICLR.\n\n===========================\n\nBefore rebuttal:\n\nThe submission presents a hierarchical Bayesian optimization (HiBO) approach to solving a postural control task expressed as a proportional-derivative (PD) controller.\n\nStrengths:\n- The HiBO approach outperforms the non-hierarchical BO approach on the task of postural control.\n\nWeaknesses:\n- The paper does not make use of representation learning (no, e.g., neural networks are employed) and is therefore out-of-place at ICLR. The employed features in Table 3 are handcrafted.\n- The task (simulating human postural control) is not well-situated in the context of prior work using HiBO for robotics, so the contribution remains unclear.\n- It is not clear why this problem should be formulated as contextual policy search (i.e., to what the context variable refers).\n- Only one baseline (Bayesian optimization (BO)) is reported. This baseline corresponds to the ablation of the HiBO method (i.e., the omission of the context variable), and so does not represent, more broadly, an alternative approach.\n- The concept of \"mental replay\" is briefly introduced, but no reference is made to prior work in imagined rollouts, and no ablation study on the impact of this component is performed.\n\nMinor points:\n- It is unclear why the problem setting should be labeled as \"psychological\" postural control.\n- There are several missing references (\"?\") in the text.", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 2}, "SJxH6hmPjr": {"type": "rebuttal", "replyto": "rJlNlmY5tH", "comment": "Dear reviewer, \n\nthank you for taking a look at our paper. However, \n\n1) all variables used in the paper are clearly defined and listed (e.g., in Table 1). In page 7, after Equation 12, the concrete policy implementation for the task is discussed. \n\n2) \"1, The paper is awfully written.\u201d \nIt is not sufficient to publish such a statement without concrete references. If you refer to point 3) then we strongly disagree.\n\n3) \"I don't see anywhere explaining how the states x_t, commands u_t, \\theta, and feature \\phi are related? \u201c \nIn Equation 1, the mentioned variables are put in relation. Moreover, problem statements of our form are common in reinforcement learning and machine learning. Given your expertise (Experience Assessment: I have published one or two papers in this area.) this should be known. \n\n4) \"What is \\phi ? It is super wired why the feature parameter \\phi is jointly maximized with the policy parameter. Because I don't understand the formulation, I can hardly understand anything else.\u201d\nPlease see our answer to point 1). We discuss a standard optimization problem definition which is well known in the reinforcement learning and machine learning community. \n\n5) \"From my very limited understanding on the formulation, the proposed HIBO is trivial.\u201d \nCan you provide further details on that statement or is this speculative? \n\n6) \"The experiments are limited. The paper only conducts one experiment on the Humanoid control balancing. And they paper only compares with the EI acquisition, while the state-of-art acquisition MES should be also be compared with.\u201d\nWe evaluated our approach on a challenging non-linear postural control task. We compared to the most closely related approach that is Bayesian optimization and evaluated several acquisition functions and kernels (EI, UCB, EI+ARD, UCB+ARD). We did not observe any statistical significant difference and only reported the results for EI. However, we will add a summary statement to the results and details to a supplement.  \nIt is important to note that our proposed hierarchical acquisition function can be implemented with any acquisition function (EI, LCB, PI, MES, etc.). However, we thank the reviewer for the link to [1] and will also eval this acquisition function.   \n\n7) \"The proposed mental replay is not well justified, qualitatively or empirically.\u201d\nThe implemented mental replay is a common and well known practice in reinforcement learning. We will add references to related work. \n\nOur questions to the reviewer:\n\nA1) Your \"Review Assessment: Checking Correctness Of Derivations And Theory: N/A\u201d was selected because of your limited experience in the field? \n\nA2) How can we interpret your input on \"Review Assessment: Thoroughness In Paper Reading: N/A\u201d? You had limited time to review the paper?  \n\n\n[1] Wang, Zi, and Stefanie Jegelka. \"Max-value entropy search for efficient Bayesian optimization.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.", "title": "Reply to the Official Blind Review #1"}, "B1gaVnmPjB": {"type": "rebuttal", "replyto": "rkxtaO2mcr", "comment": "Dear reviewer, \n\nthank you for your review. However, we are a bit shocked about the quality of the review. In the following, we discuss your points of critics:  \n\n1) \"- The paper does not make use of representation learning (no, e.g., neural networks are employed) and is therefore out-of-place at ICLR. The employed features in Table 3 are handcrafted.\" \nICLR is not \"just about\" neural networks, as you can read here: https://iclr.cc/Conferences/2020/CallForPapers\n\n\"We consider a broad range of subject areas including feature learning, metric learning, compositional modeling, structured prediction, reinforcement learning, ...\" \n\nWe informed the program chairs about that issue. To discredit a paper due to an obviously wrong interpretation of  the conference topics is not acceptable. \n\n2) \"The task (simulating human postural control) is not well-situated in the context of prior work using HiBO for robotics, so the contribution remains unclear.\" \n\nHIBO was never published prior to this submission and no prior HIBO work exists. Further, postural control is considered as one of most challenging control problems in reinforcement learning and motor control.\n\n3) \" It is not clear why this problem should be formulated as contextual policy search (i.e., to what the context variable refers).\" \nThe mathematical framework of contextual policy search allows us to derive a general hierarchical model that can be applied to any other contextual policy search task. The context variables denote salient features of the motor skills that we intent to learn and can be adapted if needed. Therefore, they are referred as \"context variables\". However, we are open for suggestions of alternative mathematical representations if you provide details. \n \n4) \"Only one baseline (Bayesian optimization (BO)) is reported. This baseline corresponds to the ablation of the HiBO method (i.e., the omission of the context variable), and so does not represent, more broadly, an alternative approach.\" \nBO is a state of the art approach and most closely related. It is ideally suited to evaluate the benefits or drawbacks of an extension to hierarchical acquisition functions. Note that in most contextual policy search approaches the context is fixed and assumed to be known which is not the case in our approach. \n\n5) \"- The concept of \"mental replay\" is briefly introduced, but no reference is made to prior work in imagined rollouts, and no ablation study on the impact of this component is performed.\"\nAlthough that mental replay is a common and well known practice in reinforcement learning, we will add references to it. \n\n6) \"- It is unclear why the problem setting should be labeled as \"psychological\" postural control.\" \nThe term 'psychological' refers only to the experiment involving humans and is a standard terminology in human motor control. We will clarify that.  \n\n7) \"There are several missing references (\"?\") in the text.\"\nNo, there are not \"several\"  missing references (\"?\") in the text that we submitted! There is a singe missing ref \"?\" in page 5 to [1] due to a typo. We thank the reviewer for this comment. \n\n[1] Nakano, Eri, et al. \"Quantitative examinations of internal representations for arm trajectory planning: minimum commanded torque change model.\" Journal of Neurophysiology 81.5 (1999): 2140-2155.\n", "title": "Reply to the Official Blind Review #2"}, "rJlNlmY5tH": {"type": "review", "replyto": "S1eYchEtwH", "review": "How to quickly learn control policies with minimized number of environment interactions have long been an important problem. To tackle this problem, this paper proposed a \"hierarchical Bayesian optimization (HIBO)\" algorithm to optimize the \"feature parameter \\phi\" (which I don't know what that is) and the \"policy parameter \\theta\" hierarchically. Under the formulation of maximizing reward J(\\theta), the algorithm firstly uses EI to select \\phi. Given the selected \\phi, the algorithm selects the policy parameter \\theta. The proposed algorithm is evaluated on a Humanoid Postural Balancing task, and shows achieves high rewards faster than the standard EI acquisition. However, the paper is awfully written such that I cannot understand what the \"feature parameter \\phi\" is. Given my limited understanding, I think the paper should be rejected.\n\nStrengths,\n1, The paper deals with an interesting task: Humanoid Postural Balancing. A Humanoid is expected to learn how to balance as quick as possible to reduce the interactions with the environments, which suits well with Bayesian optimization.\n\nWeakness,\n1, The paper is awfully written. The problem statement subsection is unreadable. I don't see anywhere explaining how the states x_t, commands u_t, \\theta, and feature \\phi are related? What is \\phi ? It is super wired why the feature parameter \\phi is jointly maximized with the policy parameter.  Because I don't understand the formulation, I can hardly understand anything else.\n2, From my very limited understanding on the formulation, the proposed HIBO is trivial.\n3, The experiments are limited. The paper only conducts one experiment on the Humanoid control balancing. And they paper only compares with the EI acquisition, while the state-of-art acquisition MES should be also be compared with.\n4, The proposed mental replay is not well justified, qualitatively or empirically.", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 3}}}