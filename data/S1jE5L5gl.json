{"paper": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"], "summary": "Relaxed reparameterization trick for discrete stochastic units.", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a neat general method for relaxing models with discrete softmax choices into closely-related models with continous random variables. The method is designed to work well with the reparameterization trick used in stochastic variational inference. This work is likely to have wide impact.\n \n Related submissions at ICLR:\n \"Categorical Reparameterization with Gumbel-Softmax\" by Jang et al. contains the same core idea. \"Discrete variational autoencoders\", by Rolfe, contains an alternative relaxation for autoencoders with discrete latents, which I personally find harder to follow."}, "review": {"Hy9lH0ILl": {"type": "rebuttal", "replyto": "S1jE5L5gl", "comment": "We thank the reviewers for their time and helpful suggestions, and we address their comments in turn.\n\nAnonReviewer1.\n\nThe temperatures used to produce the Tables in the paper are all reported in the Appendix.\n\nRegarding your question on the uniqueness of the Gumbel, we certainly could have been clearer. The statement should have been, \"In the n-ary case, given a fixed alpha, we can see that the Gumbel-Max trick holds even when we restrict it to some subset of the indices. That is, if we consider the argmax over a restricted subset, it should be distributed as a discrete random variable with the Gibbs distribution normalized over just that subset. It turns out, the Gumbel is uniquely characterized by this property (Yellott, 1977). This is important, since it gives us a guaranteed recipe for implementing a sampler for any discrete distribution as the maximization of additively perturbed logits.\" In retrospect, the original statement we made in the draft does not add much to the discussion, and we plan to remove it.\n\nAnonReviewer2.\n\nWe addressed the question of VIMCO versus Concrete when comparing linear and non-linear models in a response to a question of AnonReviewer1 below. We don\u2019t have anything to add to that answer at the moment.\n\nRegarding the question of large m. Indeed, we expect VIMCO to perform better for large values of m, but as the training time grows roughly linearly with m, using more than 25 samples is quite rare in practice.\n\nThanks for detecting all of the typos and clerical issues as well as for flagging difficult paragraphs. We will fix the typos and try to clarify the writing for the final draft.\n\nAnonReviewer3.\n\nA bias / variance analysis of this method is a great future question, though we think that a generic analysis is unlikely to be possible.  We note in passing that it isn't always the case that generic score function estimators with baselines have higher variance than their reparameterization counterparts, see (Gal, 2016). Even an analysis for a specific loss function proves challenging, since it would require deriving expectations with respect to the concrete distribution. The \"normalization\" term in the density of the concrete distribution makes computing even first order moments extremely challenging.\n\nReferences.\n\nJohn Yellott. The Relationship Between Luce's Choice Axiom, Thurstone's Theory of Comparative Judgment, and the Double Exponential Distribution. 1977.\n\nYarin Gal. Uncertainty in Deep Learning. 2016. \n", "title": "Author response"}, "rk6ghr_Xe": {"type": "rebuttal", "replyto": "SJlaHXRfg", "comment": "Thanks for your question. We haven\u2019t come up with a completely satisfying answer, but we do have a hypothesis.\n\nWe suspect that the use of gradient information by the concrete relaxation explains the difference in performance. Comparing the linear/non-linear single stochastic layer experiments (models 200H-784V and 200H~784V), we can see that changing the activity of a single latent variable has an independent linear effect on the logit parameters of the observation distribution. In the non-linear case, changing the activity of a single latent variable has a non-linear non-independent effect on the logit parameters of the observation distribution. In this case gradient information will carry richer counterfactual information. VIMCO does not exploit this gradient information, while concrete relaxations do. Concrete relaxations result in biased gradients of the discrete graph, whereas VIMCO is unbiased. We suspect that the balance between using gradients and being unbiased favours VIMCO in the linear case and concrete in the non-linear case.", "title": "Reasoning for the results of VIMCO vs. Concrete"}, "SkSRoSOmx": {"type": "rebuttal", "replyto": "HygUM_oMl", "comment": "Thanks for your question. Our method can be applied to essentially any model with discrete latent variables, since it simply involves relaxing them into continuous variables during training and then discretizing them at test time.\n\nAn inverse CDF of a discrete distribution is a piecewise constant function, which means that using it for a reparameterization does not produce usable gradients. In the Discrete VAE paper this issue is avoided by smoothing out each discrete variable by using it to select between two continuous latent variables and working with the inverse CDF of the resulting mixture, with the discrete variable marginalized out. In that approach the final models contain mixtures of discrete and continuous latent variables, while the test time models of our approach contain only discrete latent variables.", "title": "Comparison to inverse CDF"}, "SJlaHXRfg": {"type": "review", "replyto": "S1jE5L5gl", "review": "Is there any explanation about why VIMCO performs better in linear models in Table 1? Similarly, why is VIMCO failing in the Table shown in Figure 4?The authors describe the concrete distribution, a continuous approximation to\ndiscrete distributions parameterized by a vector of continuous positive numbers\nproportional to the probability of each discrete result. The concrete\ndistribution is obtained by using the softmax function to approximate the\nargmax operator. The paper is clearly written, original and significant.\nThe experiments clearly illustrate the advantages of the proposed method.\n\nSome minor questions:\n\n\"for the general n-ary case the Gumbel is a crucial 1 and the Gumbel-Max trick cannot be generalized\nfor other additive noise distributions\"\n\nWhat do you mean by this? Can you be more specific?\n\nWhat is the temperature values used to obtain Table 1 and the table in Figure 4.\n", "title": "Reasoning for the results of VIMCO vs. Concrete", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJM-7wZEl": {"type": "review", "replyto": "S1jE5L5gl", "review": "Is there any explanation about why VIMCO performs better in linear models in Table 1? Similarly, why is VIMCO failing in the Table shown in Figure 4?The authors describe the concrete distribution, a continuous approximation to\ndiscrete distributions parameterized by a vector of continuous positive numbers\nproportional to the probability of each discrete result. The concrete\ndistribution is obtained by using the softmax function to approximate the\nargmax operator. The paper is clearly written, original and significant.\nThe experiments clearly illustrate the advantages of the proposed method.\n\nSome minor questions:\n\n\"for the general n-ary case the Gumbel is a crucial 1 and the Gumbel-Max trick cannot be generalized\nfor other additive noise distributions\"\n\nWhat do you mean by this? Can you be more specific?\n\nWhat is the temperature values used to obtain Table 1 and the table in Figure 4.\n", "title": "Reasoning for the results of VIMCO vs. Concrete", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HygUM_oMl": {"type": "review", "replyto": "S1jE5L5gl", "review": "Thank you for an interesting read. \n\nHow does your method compare with the reparameterisation through inverse CDF? For example, see https://arxiv.org/abs/1609.02200Thank you for an interesting read.\n\nI think this paper has proposed a very useful method, which significantly simplifies the implementation of gradients for discrete random variables. Using this trick quite a lot of discrete variable-based methods will be significantly easier to implement, e.g. a GAN-style generator for text (see the recent arxiv preprint arXiv:1611.04051).\n\nI've got one suggestion to make the paper even better, but maybe the authors want to leave it to future work. I think compared to lots of variance reduction techniques such as NVIL and VIMCO, this relaxation trick has smaller variance (from empirical observation of the reparameterisation trick), but in the price of introducing biases. It would be fantastic if the authors can discuss the bias-variance trade-off, either in theoretical or experimental way. My bet will be that here the variance dominates the stochastic estimation error of the gradient estimation, but it would be great if the authors can confirm this.\n\n**to area chair: concurrent paper by Jang et al. 2016**\nIt seems there's a concurrent submission by Jang et al. I havent' read that paper in detail, but maybe the conference should accept or reject both?", "title": "Comparison to inverse CDF", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BycCSky4x": {"type": "review", "replyto": "S1jE5L5gl", "review": "Thank you for an interesting read. \n\nHow does your method compare with the reparameterisation through inverse CDF? For example, see https://arxiv.org/abs/1609.02200Thank you for an interesting read.\n\nI think this paper has proposed a very useful method, which significantly simplifies the implementation of gradients for discrete random variables. Using this trick quite a lot of discrete variable-based methods will be significantly easier to implement, e.g. a GAN-style generator for text (see the recent arxiv preprint arXiv:1611.04051).\n\nI've got one suggestion to make the paper even better, but maybe the authors want to leave it to future work. I think compared to lots of variance reduction techniques such as NVIL and VIMCO, this relaxation trick has smaller variance (from empirical observation of the reparameterisation trick), but in the price of introducing biases. It would be fantastic if the authors can discuss the bias-variance trade-off, either in theoretical or experimental way. My bet will be that here the variance dominates the stochastic estimation error of the gradient estimation, but it would be great if the authors can confirm this.\n\n**to area chair: concurrent paper by Jang et al. 2016**\nIt seems there's a concurrent submission by Jang et al. I havent' read that paper in detail, but maybe the conference should accept or reject both?", "title": "Comparison to inverse CDF", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1Lg5Xngl": {"type": "rebuttal", "replyto": "S1TA00ilg", "comment": "Hi David, thanks for the comment. Can you double check the current draft, specifically the third paragraph of the Related Work section? This was updated for the ICLR submission, but our arxiv revision is outdated until Monday.", "title": "straight-through"}, "S1TA00ilg": {"type": "rebuttal", "replyto": "S1jE5L5gl", "comment": "How this relates to the straight-through estimator (see Bengio et al: https://arxiv.org/abs/1308.3432) seems worth a mention.", "title": "Straight-through estimator?"}}}