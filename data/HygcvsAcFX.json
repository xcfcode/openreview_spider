{"paper": {"title": "Optimal margin Distribution Network", "authors": ["Shen-Huan Lv", "Lu Wang", "Zhi-Hua Zhou"], "authorids": ["lvsh@lamda.nju.edu.cn", "wangl@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "summary": "This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.", "keywords": ["Optimal margin distribution", "Deep neural network", "Generalization bound"]}, "meta": {"decision": "Reject", "comment": "The paper proposed an optimal margin distribution loss and applied PAC-Bayesian bounds that are from Sanov large deviation inequalities to give generalization error bounds for such a loss. Some interesting empirical results are shown to support the proposed method. \n\nThe majority of reviewers think the paper\u2019s empirical results are encouraging, although still in premature stage. The theoretical analysis is a kind of being standard. After reading the authors\u2019 response and revision, the reviewers do not change much of their opinions and think the paper better undergoes systematic further study on their proposal for big improvement.  \n\nBased on current ratings, the paper is therefore proposed to borderline lean rejection. \n"}, "review": {"S1lXsaCBnm": {"type": "review", "replyto": "HygcvsAcFX", "review": "The paper presents an improvement on the previous work by [Neyshabur et el, ICLR 2018].\nMore precisely, an emprical generalization bound is provided by using PAC-Bayesian empirical \nbounds. To obtain the claimed improvement over the works [Barlett et al, NIPS 2017] and \n[Neyshabur et el, ICLR 2018], the authors have paid attention carefully (by putting some \nconditions) on the change of the layers (layer and interlayer cushion) as well as the activation \ncontraction. It is also worth noting that the paper is using a differrent loss function comparing \nto [Neyshabur et el, ICLR 2018], which the author called Optimal Margin Distribution Loss.\nAlthough the results seem interesting, the analysis is not convincible for me.\nA plus point is that the paper presents interesting numerical experiments showing the promising of the approach.\n\nMajor comments:\n1) The statement of the Theorem 1 is not clear: \nis it just under the assumptions of the lemmas\nor is it under all definitions and lemmas?\n2) The proof of Theorem 1 is not clear:\n how do you get the inequality (5)?\nhow do you get an upper bound on the KL divergence?\n This is not trivial for me!\n3) What is \\rho in Theorem 1 and in Definition 2?\n4) Your remark after Theorem 1 is not clear for me.\n  you claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1?\n a simple counter example would fit better the explanation here, I guest.\n\nMinor comments:\n1) The Lemma 1 and 2 are almost the same to Lemma 1 and 2 in [Neyshabur et el, ICLR 2018]\nwithout precisely citations. I wonder how do you obtain your Lemma 1?\n2) page3, after formula (1), your loss will first DECREASING, not \"increasing\".\nCheck the sentence \"Fig. 1 shows, equation 1 will produce a linear loss increasing progressively with the margin distance....\"\n", "title": "just seem interesting", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1gKXwNHCm": {"type": "rebuttal", "replyto": "S1lXsaCBnm", "comment": "Thank you for your review and we will make the response issues clearly in revised version.\n\n##The contribution in our paper##\nOur paper theoretically proves that margin distribution plays an important role in the generalization of deep learning. Specifically, we propose a well-designed loss function inspired by [Gao, AIJ 2013; Zhang, ICML 2017], and it can effectively alleviate the overfitting of deep models. To the best of our knowledge, this is the first work that introduces margin distribution into the analysis of deep learning. We notice that some recent works [Barlett, NIPS 2017; Neyshabur, ICLR 2018] have considered this problem, but they only focus on minimum margin, which is significantly different from our paper.\n\nThe PAC-Bayesian framework is a convenient technique for margin theory analysis. In fact, other techniques like Rademacher complexity [Koltchinskii, TIT 2001; Koltchinskii, ANN STAT 2002; Bartlett, MACH LEARN 2002; Barlett, NIPS 2017] could also be used to derive similar results. In our paper the PAC-Bayesian technique is an analysis approach rather than our main contribution. As emphasized before, the theoretical contribution is relating the generalization gap to margin distribution.\n\n##Major comments##\n\nQ1. \u201cThe statement of the Theorem 1 is not clear: is it just \u2026 definitions and lemmas?\u201d:\nA. Theorem 1 is under all the lemmas and definitions in our paper.\n\nQ2. \u201cThe proof of Theorem 1 is not clear: how do you get the inequality (5)? how do you get an upper bound on the KL divergence?\u201d:\nA. The inequality (5) is an extension of matrix version Hoeffding\u2019s inequalities [Tropp, FOCS 2012; Mackey, ANN STAT 2014]. Note that the KL divergence is between two normal distributions with different mean but the same variance, so it can be bounded by |\\vw|^2 / 2 \\sigma^2.\n\nQ3. \u201cWhat is \\rho in Theorem 1 and in Definition 2?\u201d:\nA. As presented at the beginning of the Section 3: \u201cand $\\rho$ be an upper bound on the number of output units in each layer\u201d.\n\nQ4. \u201cyou claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1?\u201d:\nA. The impact of norms in deep learning is not similar to the linear models, we can\u2019t directly normalize it to the value $1$. And the most common method to control the norm of weights is called weight decay, which tries to control the weights by a preset decay with a parameter. Because of its data-independence, the performance is not satisfied, and many more efficient methods to preventing the overfitting problem have been proposed, such as dropout and batch normalization. If we try to optimize the product of spectral norms directly as a regularization, we can find that the weights update of each layer is related to the product of spectral norms of other layers, the calculation cost for spectral norm is too large, and the weight decay does not consider this correlation.\n\n##Minor comments\n\nQ1. \u201cThe Lemma 1 and 2 are almost the same to Lemma 1 and 2 \u2026 you obtain your Lemma 1?\u201d:\nA. Lemma 1 is obtained in a similar way with PAC-Bayesian work [Neyshabur, ICLR 2018] and we have cited this paper in Sec. 3: \u201c... we have to relate this PAC-Bayesian bound to the expected perturbed loss just like [Neyshabur, ICLR 2018] derive the Lemma 1 in their paper.\u201d But for Lemma 2, we make a lot of nontrivial modification due to the introduction of margin variance.\n\nQ2. \u201cpage3, after formula (1), your loss will first DECREASING, not \"increasing\".\u201d:\nA. We have refined the misleading description.\n\nThe whole paper has been carefully improved. \n\nThank you very much for your help!\n", "title": "Thank you for your review"}, "BkeJdPEr0Q": {"type": "rebuttal", "replyto": "r1gKXwNHCm", "comment": "Reference:\n[Gao, AIJ 2013] Wei Gao and Zhi-Hua Zhou. On the doubt about margin explanation of boosting. Artificial Intelligence, 203:1\u201318, 2013.\n[Zhang, ICML 2017] Teng Zhang and Zhi-Hua Zhou. Multi-class optimal margin distribution machine. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 4063\u20134071, 2017.\n[Barlett, NIPS 2017] Peter L. Bartlett, Dylan J. Foster, and Matus J. Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241\u20136250, 2017.\n[Neyshabur, ICLR 2018] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018.\n[Koltchinskii, TIT 2001] Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information Theory, 47(5):1902\u20131914, 2001.\n[Koltchinskii, ANN STAT 2002] Vladmir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics, 30, 2002.\n[Bartlett, MACH LEARN 2002] Peter L. Bartlett, St\u00b4ephane Boucheron, and G\u00b4abor Lugosi. Model selection and error estimation. Machine Learning, 48:85\u2013113, September 2002a.\n[Tropp, FOCS 2012] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389\u2013434, 2012.\n[Mackey, ANN STAT 2014] Lester Mackey, Michael I. Jordan, Richard Y. Chen, Brendan Farrell, Joel A. Tropp. Matrix Concentration Inequalities via the Method of Exchangeable Pairs. The Annals of Probability, 42: 906\u2013945. 2014.\n", "title": "Reference in our reply"}, "HyxjEHVBAm": {"type": "rebuttal", "replyto": "S1xq2nxq2X", "comment": "Thank you for your review and we will make the response issues clearly in revised version.\n\nQ1. \u201cwhether this theorem holds for other activations \u2026 instead of only the ReLU activation for all the layers\u201d:\nA. Current analysis only holds for ReLU, however, it\u2019s not difficult to generalize to other activations since only the Lipschitz property is required.\n\nQ2. \u201cCan an activation satisfying Definition 3 have a similar bound to Theorem 1\u201d:\nA. Definition 3 is a necessary but not sufficient condition. If other conditions are also satisfied, a similar bound can be achieved.\n\nQ3. \u201cdoes the convolutional layer simplify the bound in Theorem 1\u201d:\nA. Of course Theorem 1 may be simplified by considering some special cases, but it\u2019s beyond the scope of the paper.\n\nWe have also fixed the typos and the whole paper has been carefully improved.\n\nThank you very much for your help!\n", "title": "Thank you for your review"}, "r1e4WH4S07": {"type": "rebuttal", "replyto": "ryerWMf93m", "comment": "Thank you for your review and we will make the response issues clearly in revised version.\n\nQ1. \u201clacks an in-depth study of the properties of this handcrafted loss\u201d and \u201cThe impact of loss hyper-parameters (r, \\gamma, \\mu) should be discussed thoughtfully\u201d:\nA. We have added an extra appendix A to clearly explain the intuition behind this loss function. \n\nQ2. \u201cthe provided PAC-Bayes generalization \u2026 shed no light on the benefit of the hinge and quadratic parts\u201d:\nA. Our paper proves that the generalization of deep learning heavily depends on the margin distribution. To optimize the margin distribution, we designed this ODN loss function. As a result, this well-designed loss function alleviates the overfitting problem of deep models efficiently. And we introduce how the hinge and quadratic part is derived from the intuition of margin distribution in appendix A.\n\n\nQ3. \u201cwould like the mathematical expression of the \"hinge loss\" and the \"soft hinge loss\" models to be explicitly written\u201d:\nA. We have explicitly presented the \"(soft) hinge loss\" in section 4.1. \n\nWe have also fixed the typos and the whole paper has been carefully improved.\n\nThank you very much for your help!\n", "title": "Thank you for your review"}, "SJlzk7uV3m": {"type": "rebuttal", "replyto": "H1eRD8ag3m", "comment": "Thanks for the comments. Our responses are proved below.\n\nQ1. Regarding the margin definition\nTo our knowledge, the margin definition in section 2 is widely used in multi-class learning setting, just to name a few, the classic book [Mohri M. 2012] (cf. chapter 8.2), the top journal / conference papers [Crammer K. JMLR 2001], [Zhang T. ICML 2017], [Peter L. B. NIPS 2017]. Therefore, the margin definition is not proposed by Elsayed et al., and we treat it as a common knowledge. Note that Elsayed et al. also use this definition without citation.\n\nQ2. Regarding the linear approximation\nThanks for pointing out the missing reference and we will cite Elsayed's paper in the revised version. However, we want to clarify that the novelty of our paper is irrelevant to this linear approximation. To be specific, we summarize the difference between our paper and Elsayed's paper in the next question.\n\nQ3. The contributions of our paper and Elsayed's paper are totally different\nThe main contribution of Elsayed's work is to introduce the hinge loss into deep models across all layers to improve the empirical performance. In contrast, our contribution is:\n1. Our paper proves that the margin distribution plays an important role in the generalization theory of deep learning;\n2. For alleviating the overfitting problem in deep learning, we designed this ODN loss function to optimize the margin distribution.\nTherefore, our paper and Elsayed's work have totally different motivations, as we do theoretical analysis for the margin distribution of deep learning frameworks, while they mainly focus on the empirical improvement.\n\n[Mohri M. 2012] Mohri M, Rostamizadeh A, Talwalkar A. Foundations of machine learning[M]. MIT press, 2012.\n[Crammer K. JMLR 2001] Crammer K, Singer Y. On the algorithmic implementation of multiclass kernel-based vector machines[J]. Journal of machine learning research, 2001, 2(Dec): 265-292.\n[Zhang T. ICML 2017] Teng Zhang and Zhi-Hua Zhou. Multi-class optimal margin distribution machine. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 4063\u20134071, 2017.\n[Peter L. B. NIPS 2017] Peter L. B, Dylan J. F, and Matus J. T. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241\u20136250, 2017.\n\n", "title": "Sorry for the missing of this reference, but the contributions of these two works are totally different."}, "ryerWMf93m": {"type": "review", "replyto": "HygcvsAcFX", "review": "I consider that improving the generalization capability of neural networks on small dataset is an important line of research, and the method proposed here empirically provides great results.\n\nThe proposed margin loss (Equation 1) is said to be \"specially adapted for accelerating the convergence velocity of networks by [the authors]\". I would like this statement to be explained better, or at least backed by empirical evidence. In the current state, I consider that the paper lacks an in-depth study of the properties of this handcrafted loss. Few is said on the benefits of having both a linear behavior for points inside the margin and a quadratic loss for far points. The impact of loss hyperparameters (r, \\gamma,\\mu) should be discussed thoughtfully; at some points in the paper, r and \\gamma are referred as margin mean and margin variance parameters, but this interpretation is not explained. Moreover, almost nothing is said about \\mu.\nBy considering a simplified loss function, the provided PAC-Bayes generalization bound (Theorem 1) consider solely the flat loss region [r-\\gamma, r+\\gamma], but shed no light on the benefit of the hinge and quadratic parts. I conceive that this might be hard to study theoretically, but the authors should at least provide a empirical study of these. \n\nThe empirical experiments show great evidence that the proposed method successfully improve generalization capability of neural networks on small datasets compared to classical methods. I appreciate the Inter/intra class variance study of Tables 2 and 3. I would like the mathematical expression of the \"hinge loss\" and the \"soft hinge loss\" models to be explicitly written (it is not clear in the text if the soft hinge uses an hyperparameter). In the same spirit of my above comments, I would like to see how each loss hyperparameters impacts the results, instead of having access solely to the parameter values selected by the validation process.\n\nTypos and minor comments:\n- Abstract: \"And our ODN model also outperforms the other three loss models...\" Which three loss models?\n- Section 3: \"Specially, define L_0 as r=\\theta...\" I think it should be r=0\n- Section 4.1: model-s => models\n- Page 7 (and elsewhere): Table. 2 => Table 2\n- Please specify that \"Xent\" stands for cross-entropy\n- Figure 3: Please use larger font sizes\n- Proof of Lemma 2: Equation. 4 => Equation 4 \n", "title": "Promising work, but an in-depth study of the handcrafted margin loss function is lacking", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1xq2nxq2X": {"type": "review", "replyto": "HygcvsAcFX", "review": "This paper presents a PAC-Bayesian bound for a margin loss.\n\nTheorem 1 seems specific to ReLU activations. I wonder whether this theorem holds for other activations since most deep neural networks can use different activations at different layers instead of only the ReLU activation for all the layers. In Section 3, only Definition 3 is related to the activation. Can an activation satisfying Definition 3 have a similar bound to Theorem 1? Moreover, since the convolutional layer is a simplified case of the fully connected layer discussed in Section 3, does the convolutional layer simplify the bound in Theorem 1?\n\nThere are some typos in this paper.\n\u201cTo derive a expected risk bound\u201d: a -> an\n\u201cused to formalize error-resilience in Arora et al. (2018) as following:\u201d: following: -> follows.\n\u201cthe deep network from layer i to layer j\u201d, \u201cinjected before level i\u201d: i,j should be in the math mode.\n\u201cdependent on the network structure .\u201d there is an additional blank space after \u2018structure\u2019.", "title": "PAC-Bayesian analysis for DNNs", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}