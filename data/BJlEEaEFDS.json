{"paper": {"title": "Towards an Adversarially Robust Normalization Approach", "authors": ["Muhammad Awais", "Fahad Shamshad", "Sung-Ho Bae"], "authorids": ["awais@khu.ac.kr", "fahad.shamshad@itu.edu.pk", "shbae@khu.ac.kr"], "summary": "Investigation of how BatchNorm causes adversarial vulnerability and how to avoid it. ", "abstract": "Batch Normalization (BatchNorm) has shown to be effective for improving and accelerating the training of deep neural networks. However, recently it has been shown that it is also vulnerable to adversarial perturbations. In this work, we aim to investigate the cause of adversarial vulnerability of the BatchNorm. We hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerability in the BatchNorm layer. We empirically proved this by experiments on various neural network architectures and datasets. Furthermore, we introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BatchNorm.", "keywords": ["robustness", "BatchNorm", "adversarial"]}, "meta": {"decision": "Reject", "comment": " This paper presents an empirical analysis of the reasons behind BatchNorm vulnerability to adversarial inputs, based on the hypothesis that such vulnerability may be caused by using different statistics during the inference stage as compared to the training stage. While the paper is interesting and clearly written, reviewers point out insufficient empirical evaluation in order to make the claim more convincing."}, "review": {"SkgvVW1htH": {"type": "review", "replyto": "BJlEEaEFDS", "review": "This paper addresses a limitation of BatchNorm: vulnerability to adversarial perturbations. The authors propose a possible explanation of this issue and correspondingly an alternative called RobustNorm to tackle this problem. Specifically, the authors observe that the statistics of BatchNorm for training and inference are different, resulting in different data distributions for training and inference. To solve this problem, the authors propose to use min-max rescaling instead of normalization. In addition, the running average is calculated with mean and the running mean of the denominator during inference. Experimental results show significant improvement of robustness and also comparable accuracy for clean data.\n\nThe paper is well-written and the contributions are stated clearly. The explanation of vulnerability is reasonable. The proposed solution is simple but effective.\n\nHowever, I have several concerns:\n*The authors verify that the running average is the main culprit of vulnerability to adversarial attack, but provide no further investigation of why this happens. A possible solution is the drift in input distributions, but the manuscript does not state clearly how is the distribution changed. Further experiments would have made this claim more convincing.\n*The proposed method involves a hyper-parameter \\rho, but it may result in problematic issues. The variance of input is of the same order of magnitude as (max(x)-min(x))^2. If \\rho is set to other value, the magnitude of gradient will change drastically during back-propagation. Although \\rho can be set to 0.2, it still seems ad-hoc. Experiments on more datasets and the sensitivity of the proposed method to \\rho would have validated the claims of the authors.\n", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}, "SyxQ0cXatS": {"type": "review", "replyto": "BJlEEaEFDS", "review": "Review: This paper investigates the reason behind the vulnerability of BatchNorm and proposes a Robust Normalization. They experimentally show that it is the moving averages of mini-batch means and variances (tracking) used in Normalization that cause the adversarial vulnerability. Based on this observation, they propose a new normalization method not only achieves significantly better results under a variety of attack methods but ensures a comparable test accuracy to that of BatchNorm on unperturbed datasets. The paper is clearly written, easy to read.\n \nStrengths:\n \nExplore the cause of adversarial vulnerability of the BatchNorm and assume that the tracking mechanism used in original BatchNorm leads to the vulnerability from experiment results.\nPropose a new and simple normalization method and perform extensive experiments to validate the efficacy of proposed method.\n \nWeaknesses:\nThough extensive experiments have been done by revealing what leads the vulnerability and the effectiveness of proposed method. The results seem unconvincing with respect to different datasets, since Cifar10 and Cifar100 are inherently connected. Would you mind performing some experiments on ImageNet? Since adversarial training on ImageNet is time-consuming, can you show us the result of Natural Training of different models with different norms on ImageNet and compare their robustness under different attack?\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "SyxnlNl0tB": {"type": "review", "replyto": "BJlEEaEFDS", "review": "This paper proposes an interesting perspective that BatchNorm may introduce the adversarial vulnerability, and probes why BatchNorm performs like that (the tracking part in BatchNorm). In experiment, the robustness of the networks increases by 20% when removing the tracking part, but the test accuracy on the clean images drops a lot. Afterwards, the authors propose RobustNorm, which performs better than BatchNorm for both natural and adversarial scenarios.\n\nDetailed Comments: \n+ The paper is well written. The paper structure is clear and figures are well illustrated.\n+ The paper understands and carefully investigates BatchNorm in a interesting and important direction. After the investigation, the improved version RobustNorm shows more potential.\n+ The experimental results seem good. The RobustNorm performs better than BatchNorm for both natural and adversarial scenarios.\n- More results on ImageNet would be better to verify the proposed RobustNorm method.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "HylCJXVXYr": {"type": "rebuttal", "replyto": "BJgbQgVXYB", "comment": "Thank you for pointing this out.\nWe have used standard ICLR latex template in overleaf. We will try to fix this in our final submission. \n\nWe would love to hear more feedback from you. ", "title": "Thanks"}, "rklX-V2tOB": {"type": "rebuttal", "replyto": "BJlEEaEFDS", "comment": "As suggested by Anthony Wittmer in the comments section of the paper [1], \"BatchNorm is the cause of adversarial vulnerability\",  a litmus test for our hypothesis, \"tracking in BatchNorm is a cause of its adversarial vulnerability\", is to test it for normalizations that do not require tracking. We have tested this for Group and LayerNorm. Both of these normalizations don't require tracking for test time statistics.  The results are shown in the following two figures. From figures, it is clear that normalization without tracking is less vulnerable to adversarial attacks. We believe this can also help solve the problem mentioned in section 5 of the paper. \n\nhttps://ibb.co/B37LFFS\nThe figure shows the effect of different attacks on the accuracy of networks trained with different normalizations. All the normalizations that don't require have much better adversarial accuracy.\n\nhttps://ibb.co/JQvYjjB\nEffect of increasing $\\epsilon$ for different adversarial attacks on networks with different norms. The first sub-figure shows results for BatchNorm with tracking and the rest of the figure shows results for normalizations without tracking.\n\n\n\n[1] \\url{https://openreview.net/forum?id=H1x-3xSKDr&noteId=SklNaNu2vr}\n", "title": "Litmus Test of Paper's Hypothesis "}, "HklMcg3tuS": {"type": "rebuttal", "replyto": "BylDR1y3Pr", "comment": "Thank you for your comment. Here we will answer your question one by one.\n\n- ``\"however, as an aside, that robustness also be assessed on unseen attacks and corruptions\". By unseen attacks and corruption, do you mean attacks other than PGD on which we have adversarially trained? If so, yes, we have included results for many attacks and Gaussian noise. For instance, table 2 shows the effect of Noise, GradientSign, BIM-$\\ell_{\\infty}$ and PGD-$\\ell_{\\infty}$. Similarly, figure 4 shows results for two additional attacks, Grad and PGD-$\\ell_{2}$ along with different $\\epsilon$ levels ranging from 0.003/1 to 0.9/1. Similarly, other results also show the effect of unseen attacks.\n\nIf by unseen attacks, you mean BlackBox settings, we also have tested one such setting based on transferability property [6]. For this experiment, we have crafted iterative PGD-$\\ell_{\\infty}$ noise for Renet20 with the BatchNorm layer and used it for ResNet38 with different normalizations. Please note that for the creation of adversarial samples, we have used only the BatchNorm layer to comply with BlackBox settings. As suggested by [7], we have only used a non-targeted attack. We will add more results in the final version of the paper. \n\n-----------------------------------------------------------------------------------\n                          BN                      BN w/o Tracking            RN\n-----------------------------------------------------------------------------------\nRenset38         43.96                   47.12                               52.71\n------------------------------------------------------------------------------------\n\n-   For a fair comparison, we used the same network with the same settings except for the normalization layer. This means, for the un-normalized network, we removed the BatchNorm layer. We did not use fixup initialization and used standard initialization used by most of the researchers. We agree that fixup initialization can increase accuracy in some cases and can eliminate the use of the BatchNorm layer. Although we have not experimented with fixup initialization, based on the experiments in the paper [5], it can be helpful for the reduction in adversarial vulnerability. But according to the Fixup paper, we also need to use multiplier and bias factors and some kind of regularization to match the accuracy of BatchNorm. \n    \nAlso, this, in a way, supports our hypothesis that tracking is a cause of adversarial vulnerability.  Because fixup initialization solves the problem (mysterious problem with many interpretations such as alleviation of internal Covariate  shift(ICS) [4], correction of activations to avoid their explosion [1], making loss landscape smoother [2] or regularization [3], etc.) in a way which do not make input distribution at train and test time different. \n\n- For a fair comparison, we have used uniform settings across our experiments. For data preprocessing, we have used random crops with padding of 4 and random horizontal flips. Similarly, we have used Pytorch default values for all variables which means eps = 1e-5 and tau = 0.1. Please note that we have not tuned these settings for our experiments to get better results. \n\n- We do agree with your observation. By switching from misclassification to targeted objective, adversarial accuracy improves. We have used the misclassification objective for all of our experiments. Thank you for highlighting it, We will add relevant experiments in the appendix. Yes, accuracy with BIM-$\\ell_{\\infty}$ attack decreases to zero for BatchNorm as well as BatchNorm w/o tracking as the value of $\\epsilon$ is increased to a very high level. For other attacks, this is not the case. Also, note that RobustNorm is still resistive to the attacks with such high levels of adversarial noise. \n\n\n[1] Bjorck, Nils, et al. \"Understanding batch normalization.\" Advances in Neural Information Processing Systems. 2018.\n\n[2] Santurkar, Shibani, et al. \"How does batch normalization help optimization?.\" Advances in Neural Information Processing Systems. 2018.\n\n[3] Luo, Ping, et al. \"Towards understanding regularization in batch normalization.\" (2018).\n\n[4] Ioffe, Sergey. \"Batch renormalization: Towards reducing minibatch dependence in batch-normalized models.\" Advances in neural information processing systems. 2017.\n\n[5] Galloway, Angus, et al. \"Batch Normalization is a Cause of Adversarial Vulnerability.\" arXiv preprint arXiv:1905.02161 (2019).\n\n[6] Papernot, Nicolas, Patrick McDaniel, and Ian Goodfellow. \"Transferability in machine learning: from phenomena to black-box attacks using adversarial samples.\" arXiv preprint arXiv:1605.07277 (2016).\n\n[7] Liu, Yanpei, et al. \"Delving into transferable adversarial examples and black-box attacks.\" arXiv preprint arXiv:1611.02770 (2016).\n    ", "title": "Re: A few questions "}}}