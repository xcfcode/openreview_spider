{"paper": {"title": "Sample Importance in Training Deep Neural Networks", "authors": ["Tianxiang Gao", "Vladimir Jojic"], "authorids": ["tgao@cs.unc.edu", "vjojic@cs.unc.edu"], "summary": "", "abstract": "The contribution of each sample during model training varies across training iterations and the model's parameters. We define the concept of sample importance as the change in parameters induced by a sample. In this paper, we explored the sample importance in training deep neural networks using stochastic gradient descent. We found that \"easy\" samples -- samples that are correctly and confidently classified at the end of the training -- shape parameters closer to the output, while the \"hard\" samples impact parameters closer to the input to the network. Further, \"easy\" samples are relevant in the early training stages, and \"hard\" in the late training stage. Further, we show that constructing batches which contain samples of comparable difficulties tends to be a poor strategy compared to maintaining a mix of both hard and easy samples in all of the batches. Interestingly, this contradicts some of the results on curriculum learning which suggest that ordering training examples in terms of difficulty can lead to better performance.", "keywords": ["Deep learning", "Supervised Learning"]}, "meta": {"decision": "Reject", "comment": "The reviewers provided detailed, confident reviews and there was significant discussion between the parties. \n \n Reviewer 2 and 3 felt quite strongly that the paper was a clear reject. Reviewer 1 thought the paper should be accepted.\n \n I was concerned with the two points raised by R3 and don't feel they were adequately addressed by the author's comments:\n \n - Dependence of the criteria on the learning rate (this does not make sense to me); and\n - Really really poor results on CIFAR-10 (and this is not being too picky, like asking them to be state-of-the-art; they are just way off)\n \n I engaged R1 to see how they felt about this. In reflection, I side with the majority opinion that the paper needs re-work to meet the ICLR acceptance bar."}, "review": {"S1IhYTmvg": {"type": "rebuttal", "replyto": "BJHSrzWPe", "comment": "We believe that we have miscommunicated the intent of the experiments in Figure 4, through careless summarization. Specifically, the word \"same\" is jarring.\n\nWe agree that the question of whether the NLL and SI are equal can be trivially answered. They are definitionally distinct mathematical objects. One is a function and the other is a sum of the squares of the gradients of that function. \n\nWhat we aimed to answer is whether the NLL is highly predictive of sample importance. \n\nIn the experiments, NLL becomes correlated with the SI in the late stage of training, but not so in the early ones. We believe this is an interesting observation.\n\nTo address the concern of the reviewer, we changed the manuscript to say \"correlated\" rather than \"same\". This reflects the intent of the experiment. ", "title": " We changed the manuscript to say \"correlated\" rather than \"same\""}, "S1R9QprIx": {"type": "rebuttal", "replyto": "rkCNNdNEe", "comment": "Sample importance is a direct measurement of parameter changes that contributed from a sample at a specific iteration during training. The reviewer claims that \u201cgradients tend to have higher norms early in training than at convergence\u201d, which is true for \u201ceasy\u201d samples, but not for \u201chard\u201d samples. Summing the gradients norm across time is just one of several the measurements we use and it is not the focus of our paper. To study the different contribution from samples to different parameters at different training stages, we measured the stage-wise, layer-wise sample importance. For example, one of the most important findings is that the parameters in lower-level layers are changed more by \u201chard\u201d samples during late stage. \nThe \u201cinput Fisher\u201d norm suggested by the reviewer, is related to the saliency [1] in computer vision. It speaks directly to the sensitivity of the classifier to the input features rather than the existence of an input. In our paper, we care about how the change in the network parameters depends on the contribution of input during training. However, the sensitivity of the network output with regard to the input reveals only the current state of the network, but nothing about the network training. \n\n\nThe average sample importance of output layer reach maximum during early stage and keeps dropping as training goes on (Fig 2, left figures).  Hence, we have the argument of parameters in output layers are primarily learned in the early stage of training. We can see that the later in the training stage, the smaller the average sample importance in output layer parameters. For SI measure in Fig.6, we care more about the layer-wise, stage-wise specific sample importance rather than overall sample importance. The most important message from the Fig.6 is that the sample importance has different stage-wise patterns in different sample groups. \n\n\n[1] Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. \"Deep inside convolutional networks: Visualising image classification models and saliency maps.\" arXiv preprint arXiv:1312.6034 (2013).\n", "title": "Sample importance is a direct measurement of the parameter change in the network"}, "Sy4gXaS8x": {"type": "rebuttal", "replyto": "BkgXHiNNx", "comment": "The \u201cEasy\u201d vs \u201cHard\u201d samples have been discussed in Curriculum Learning (CL) [1] and Self-paced Learning (SPL) [2]. In CL, \u201ceasy\u201d and \u201chard\u201d samples are defined manually, or by the classification error on a pre-trained classifier. In SPL, \u201ceasy\u201d and \u201chard\u201d samples are defined based on their training error at each training step. Most of the previous methods defined the \u201ceasy\u201d and \u201chard\u201d based on an error-based metric. In our paper, we made a comparison between the samples with similar sample importance pattern over the training and their training error. We treat the samples with small training NLL as \u201ceasy\u201d samples and large training NLL as \u201chard\u201d samples. We found that \u201ceasy\u201d samples contributes more during early stages and in higher layers while \u201chard\u201d samples contribute more during late stages and in lower layers.\n\nSample importance is the square of the parameter change contributes from a particular sample. It directly measures how much the parameters are changed due to the existence of a sample at a particular iteration. In FIg.1 we showed that Sample importance is preserved between initializations of the network. The noise introduced to the model from different initialization is very small. \n\n[1] Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41\u201348. ACM, 2009.\n[2] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In Advances in Neural Information Processing Systems, pp. 1189\u20131197, 2010.\n\n", "title": "\"Easy\" and \"Hard\" samples have different impact on the training of the network"}, "rJO5GTB8e": {"type": "rebuttal", "replyto": "rkp4cGtVx", "comment": "\\phi^t_{i} is a vector of parameter affectability, which is the contribution of ith sample to the change to all the parameters in iteration t. Specifically, \\phi^t_{i,j} is the parameter affectibility to the jth parameter in the network from sample i. We are sorry for this unclear definition and we have revised this in the new version.\n\nQ_d has already been defined in the original text in page 3 line 6, \u201cwhere Q_d is a set consists of the indexes of all parameters in layer d.\u201d\n\nWe defined the sample importance as the l2-norm of the change in the parameters contributed from a sample at a specific iteration, of course, the sample importance is affected by learning rate as it reflects that change in parameters at a particular iteration from the sample. In order to make the SI between different training stages comparable, we used fixed learning rate. Hence, the different behaviors of SI in different training stages are not affected by learning rate.\n\nIn order to make the analysis comparable between MNIST and CIFAR, we only use feed forward neural network for analysis, as feed forward neural network is the most basic and general structure for deep learning. We will generalize our results and findings further for other architectures like convolutional networks.\n\nThese are two direct observations in our paper. However, they are not the focus of our paper. We focused more on the samples that contribute more to different layers and different training stages. For example, in Fig. 5 and Fig. 6 we found that \u201ceasy\u201d samples contributes more to the early stage of training and \u201chard\u201d samples contributes more to the late stage of the training. These observations are not easily obtained without our experiments.\n\nThe sample importance is not the same as the NLL of a sample, but it becomes more correlated during late stage as we shown in Figure 4. Intuitively, if the NLL of a sample is large, there is much room for the network to adjust its parameters to reduce the NLL of this sample, while if the NLL is small or close to zero, there is a very tiny room for network to fit this sample as it is already perfectly trained. Hence, such question is not intuitively obvious, like the first question raised by AnonReviewer1, which demands an example for the case where SI can be not related to NLL.   \n\nThanks for your suggestion. We have revised the Fig.7 to the new scale in the new version.", "title": "Our paper focuses on layer-wise, stage-wise sample importance rather than overall importance"}, "ryt-5hWEg": {"type": "rebuttal", "replyto": "r1q2KOgEe", "comment": "The exact text from the paper:\n\"As different datapoints correspond to different loss function values\n(e.g., some are more difficult to classify), gradient amplitudes and directions may also appear to\nbe significantly different. This motivated the use of momentum both for gradient directions and\namplitudes to smooth the incoming information.\" \nThus, this is not the authors motivation for batch selection as your comment suggests, but their interpretation of why momentum was introduced. \nThe next sentence says: \"We propose to online select batches (both size and datapoints) for an algorithm A to maximize its progress (e.g., defined by the objective function f or cross-validation error) over the resource budget (e.g., the number of evaluated datapoints or time).\"\nI don't think it makes sense to rename batch selection as batch construction for the sake of renaming. Instead, it seems reasonable to focus on the actual novelty of your paper - the study of different batch selection strategies.", "title": "re: Relationship with the batch selection paper"}, "r1q2KOgEe": {"type": "rebuttal", "replyto": "H1h0oek4l", "comment": "Thank you for pointing out the workshop paper. This paper focuses on building sampling algorithms for batches based on the training loss of the samples. Specifically, authors state  motivation: \"As different datapoints correspond to different loss function values (e.g., some are more difficult to classify), gradient amplitudes and directions may also appear to be significantly different.\" In our paper, we show substantial disagreement between negative log likelihood and sample's importance. In response to another review, we also give a simple example of how such disagreement can arise. We will amend our paper to add a reference to the paper you mentioned and highlight this distinction.", "title": "Relationship with the batch selection paper"}, "H1h0oek4l": {"type": "rebuttal", "replyto": "r1IRctqxg", "comment": "Your work is closely related to batch selection: \"Online batch selection for faster training of neural networks\" from ICLR Workshops 2016 where the authors provided \"an initial study of possible online batch selection strategies\". In your work, batch selection is denoted as \"batch construction\" and you study a broader set of selection strategies. \n", "title": "Batch Selection reference is missing"}, "r1C7wv2Xg": {"type": "rebuttal", "replyto": "S1SBwgyXg", "comment": "The overall sample importance is not inflated by the oscillation between local minima. Firstly, we observed that the training objective is continuously dropping even at the last epoch. There is no oscillation in the training objective. Secondly, we tracked the gradients across the iterations for each sample. The gradients computed on easy samples converge to zero quickly. Hence, easy samples\u2019 importance drops in later epochs. The gradients computed on hard samples keep the same sign until the end of the training. As we are using early stopping for training, the objective is far from reaching a local minimum where we could see an oscillation effect.\n\n\nIn Figure 4, we show that overall sample importance is correlated with (but not the same as) the training negative log likelihood. Specifically, change in the parameter due to a sample\u2019s presence is not equal to its negative-log-likelihood. Overall sample importance is a coarse measurement of the total change in parameters contributed by a sample. In section 3.6, we found that ordering batches based on overall sample importance do not reduce the test error comparing to regular (random) batch ordering.\nHowever, the key observation in our paper is that epoch-specific sample importance to each layer. Specifically, the hard samples have the biggest impact on shaping the parameters in lower layers -- layers close to the input. This epoch-layer-specific sample importance is a direct measurement of the sum of squares of the change in parameters in each epoch. The clusters in Figure 5 and 6 are clustered based on epoch-layer-specific sample importance.", "title": "The overall sample importance is not inflated by the oscillation between local minima"}, "Sy_RLP37g": {"type": "rebuttal", "replyto": "BJ776TyQx", "comment": "The graphs with solid colors are cumulative and it is a decomposition of the overall importance. The right interpretation of the bottom right graph of figure 2 is (i) importances are approximately equal for every class. Trucks and cars have approximately equal sample importance. All the solid-fill graphs follow this convention.\n\n\nYour explication is correct. The output layers are trained first since the gradient flows backward. It is true that easy samples are learned before the hard ones. This makes our main observation from the result: hard samples are the most important factor in shaping the parameters in low-level layers.", "title": "The graph with solid colors are cumulative"}, "BJlhLPnQe": {"type": "rebuttal", "replyto": "H1m181bXg", "comment": "To establish terminology we just go over some details of back-prop. We also include a specific example where NLL and SI are very different. Please forgive the verbosity. \n\n\nThe negative log-loss is -L = -q*log(f(x,theta)), in classification task, q=1 for the true label, and f(x,theta) is the predicted probability of the true class, where x is the input, theta is the parameter. The gradient, in this case, can be written as: g = -1/f *df/dtheta. Using back-propagation, we know that the gradient for a particular layer l is df/dw^l = C^L * w^L * C^{L-1} * w^{L-1} * \u2026 * C^{l} * h^{l}, where C^l is the derivative of the activation function in the lth layer, and h^l is the input to the lth layer. Hence, the gradient for a particular parameter can be decomposed into the product of negative inverse probability -1/f, activation derivatives C^L \u2026 C^{l+1} before lth layer and parameters w^L \u2026 w^{l+1} before lth layer and the input to the lth layer, h^l. \n\n\nThis indicates that the gradient is not only related to NLL but also related to 1) the parameter weight that scales w in each layer in the current network, 2) the activation pattern (C and h) for particular samples. Here we present two examples for these two situations:\n\n\n1) NLL can be different from SI in different epochs as parameters are changing. An example is Figure 5 \u201ceasy samples\u201d cluster. In the very early epochs, NLL is large, but all the weights are initialized to be small. Hence, the gradients, which contains the product of the parameters, are small. As training goes on, NLL becomes smaller, but parameters are becoming large. Hence, there is an increase in SI in the middle early stage. When the NLL is converging close to zero, and there is no room for gradient to improve the objective, f becomes very close to 1, and output softmax function is saturating (for example, a binary softmax is equivalent to sigmoid function, which has derivative df/dx = f(1-f), and it will be close to 0 if f is close 1), and SI will be small again. This example illustrated that NLL is not predictive of SI when parameters are different.\n\n\n2) NLL is not predictive of SI when the sample input is different. As we noted, the activation pattern and input will also affect the gradient magnitude even when parameters are fixed. Hence, a sample with larger NLL might have smaller SI comparing to another sample, even if the parameters remain the same. Here is a simple synthetic example to illustrate this: Let us consider a simple binary linear logistic regression, where f(x,w) = sigmoid(x*w). The gradient for w is: g = dNLL/dw = -1/f * f(1-f) * x. (suppose this is a positive class sample). Let\u2019s consider a setting, where w = [-1000,-0.1], and two samples x_a = [0.01,0], x_b = [0,10]. Hence, NLL_a  = - log sigmoid(x_a*w) = -log sigmoid(-10) \u2248 10, which is a very large value, while the SI_a = (g_a)^2 = (-1/sigmoid(-10) * (sigmoid(-10)*(1-sigmoid(-10)))*0.01)^2  \u2248 (-0.001)^2 = 1e-6, which is a very small value. However, for x_b, NLL_b =  - log sigmoid(x_b*w) = -log sigmoid(-1)  \u2248 1.31, and SI_b = (g_b)^2 = (-1/sigmoid(-1) * (sigmoid(-1)*(1-sigmoid(-1)))*10)^2  \u2248 (-7.3)^2 approximately 53, which is very large. This example shows that sample with high NLL can have small SI, and sample with small NLL can have large SI.", "title": "NLL is not predictive of SI clarification."}, "H1m181bXg": {"type": "review", "replyto": "r1IRctqxg", "review": "With the standard cross-entropy loss for discriminative classification, the log-loss is q*log(p) and the gradient is p-q, where p is predicted and q is the target.\nso when NLL or log-loss is high, the gradient or SI should also be high. Therefore, can the authors clarify why this is observed in practice:  \"NLL is not predictive of the SI\".\nThis paper examines the so called \"Sample Importance\" of each sample of a training data set, and its effect to the overall learning process.\n\nThe paper shows empirical results that shows different training cases induces bigger gradients at different stages of learning and different layers.\nThe paper shows some interesting results contrary to the common curriculum learning ideas of using easy training samples first. However, it is unclear how one should define \"Easy\" training cases.\n\nIn addition, the experiments demonstrating ordering either NLL or SI is worse than mixed or random batch construction to be insightful.\n\nPossible Improvements:\nIt would be nice to factor out the magnitudes of the gradients to the contribution of \"sample importance\". Higher gradient (as a function of a particular weight vector) can be affected weight/initialization, thereby introducing noise to the model.\n\nIt would also be interesting if improvements based on \"Sample importance\" could be made to the batch selection algorithm to beat the baseline of random batch selection.\n\n\nOverall this paper is a good paper with various experiments examining how various samples in SGD influences the various aspect of training.", "title": "Clarification on \"NLL is not predictive of the SI\"", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkgXHiNNx": {"type": "review", "replyto": "r1IRctqxg", "review": "With the standard cross-entropy loss for discriminative classification, the log-loss is q*log(p) and the gradient is p-q, where p is predicted and q is the target.\nso when NLL or log-loss is high, the gradient or SI should also be high. Therefore, can the authors clarify why this is observed in practice:  \"NLL is not predictive of the SI\".\nThis paper examines the so called \"Sample Importance\" of each sample of a training data set, and its effect to the overall learning process.\n\nThe paper shows empirical results that shows different training cases induces bigger gradients at different stages of learning and different layers.\nThe paper shows some interesting results contrary to the common curriculum learning ideas of using easy training samples first. However, it is unclear how one should define \"Easy\" training cases.\n\nIn addition, the experiments demonstrating ordering either NLL or SI is worse than mixed or random batch construction to be insightful.\n\nPossible Improvements:\nIt would be nice to factor out the magnitudes of the gradients to the contribution of \"sample importance\". Higher gradient (as a function of a particular weight vector) can be affected weight/initialization, thereby introducing noise to the model.\n\nIt would also be interesting if improvements based on \"Sample importance\" could be made to the batch selection algorithm to beat the baseline of random batch selection.\n\n\nOverall this paper is a good paper with various experiments examining how various samples in SGD influences the various aspect of training.", "title": "Clarification on \"NLL is not predictive of the SI\"", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJ776TyQx": {"type": "review", "replyto": "r1IRctqxg", "review": "Can you first confirm that graphs with solid colors (such as the bottom-right graph of figure 2) are cumulative. in other words,  the right interpretation of the bottom right graph of figure 2 is (i) that  importances are approximately equal for every class (The legend seems to contradict this) and not (ii) that trucks are 8 times more important than airplanes. Do all solid-fill graphs follow this convention ?\n\nIt seems intuitively normal that output layers be trained first since the gradient flows backwards. Also it is to be expected that easy samples are learned before that hard ones.\n\nDo you think this would be a reasonable explication of the results you observe ? If not why ? Putting aside the last results (which tend to show that curriculum learning is probably a bad idea, something we can agree on), would you give a different explication for your results ? (what explication and why of course)\nThe paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term \\phi^t_{i,j} is never defined, only \\phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context).\n\nThe paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR)\n\nThe study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions\n - \u201cthe overall sample importance is different under different epochs\u201d => yes the norm of the gradient is expected to vary\n - \u201cOutput layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops\u201d => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables\n\nThe question of Figure 4 is absurd \u201cIs Sample Importance the same as Negative log-likelihood of a sample?\u201d. Of course not.\n\nThe results are very bad on CIFAR which discredits the applicability of those results.\n\nOn Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20%\n\nDespite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented.", "title": "confirm graph interpretation, ", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkp4cGtVx": {"type": "review", "replyto": "r1IRctqxg", "review": "Can you first confirm that graphs with solid colors (such as the bottom-right graph of figure 2) are cumulative. in other words,  the right interpretation of the bottom right graph of figure 2 is (i) that  importances are approximately equal for every class (The legend seems to contradict this) and not (ii) that trucks are 8 times more important than airplanes. Do all solid-fill graphs follow this convention ?\n\nIt seems intuitively normal that output layers be trained first since the gradient flows backwards. Also it is to be expected that easy samples are learned before that hard ones.\n\nDo you think this would be a reasonable explication of the results you observe ? If not why ? Putting aside the last results (which tend to show that curriculum learning is probably a bad idea, something we can agree on), would you give a different explication for your results ? (what explication and why of course)\nThe paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term \\phi^t_{i,j} is never defined, only \\phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context).\n\nThe paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR)\n\nThe study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions\n - \u201cthe overall sample importance is different under different epochs\u201d => yes the norm of the gradient is expected to vary\n - \u201cOutput layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops\u201d => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables\n\nThe question of Figure 4 is absurd \u201cIs Sample Importance the same as Negative log-likelihood of a sample?\u201d. Of course not.\n\nThe results are very bad on CIFAR which discredits the applicability of those results.\n\nOn Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20%\n\nDespite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented.", "title": "confirm graph interpretation, ", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1SBwgyXg": {"type": "review", "replyto": "r1IRctqxg", "review": "Can the authors convince me that the overall importance is a sensible measure ? If g_i^t > g_j^t, then clearly example i has a greater impact on \\theta_{t+1} then x_j (ignoring issues of Euclidian vs Fisher metric). The magnitude of gradients will change significantly during learning however, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally is problematic. Furthermore, a high learning may not even convergence and instead oscillate between local minima, in which case the \"total/overall importance\" would be meaningless.(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning\n", "title": "Overall importance", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkCNNdNEe": {"type": "review", "replyto": "r1IRctqxg", "review": "Can the authors convince me that the overall importance is a sensible measure ? If g_i^t > g_j^t, then clearly example i has a greater impact on \\theta_{t+1} then x_j (ignoring issues of Euclidian vs Fisher metric). The magnitude of gradients will change significantly during learning however, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally is problematic. Furthermore, a high learning may not even convergence and instead oscillate between local minima, in which case the \"total/overall importance\" would be meaningless.(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning\n", "title": "Overall importance", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}