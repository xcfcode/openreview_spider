{"paper": {"title": "DARTS: Differentiable Architecture Search", "authors": ["Hanxiao Liu", "Karen Simonyan", "Yiming Yang"], "authorids": ["hanxiaol@cs.cmu.edu", "simonyan@google.com", "yiming@cs.cmu.edu"], "summary": "We propose a differentiable architecture search algorithm for both convolutional and recurrent networks, achieving competitive performance with the state of the art using orders of magnitude less computation resources.", "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.", "keywords": ["deep learning", "autoML", "neural architecture search", "image classification", "language modeling"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper introduces a very simple but effective method for the neural architecture search problem. The key idea of the method is a particular continuous relaxation of the architecture representation to enable gradient descent-like differentiable optimization. Results are quite good. Source code is also available. A concern of the approach is the (possibly large) integrality gap between the continuous solution and the discretized architecture. The solution provided in the paper is a heuristic without guarantees.  Overall, this is a good paper. I recommend acceptance."}, "review": {"H1l5BS-ueV": {"type": "rebuttal", "replyto": "S1eYHoC5FX", "comment": "Dear Reviewers,\n\nIn response to the negative anonymous comments that we have received, we would like to reiterate that our claims are valid, and the publicly available implementation is correct. Throughout the reviewing process, we have done our best to address all questions we have received, and we will strive to continue improving the paper.", "title": "Generic Response"}, "SJesF7dIg4": {"type": "rebuttal", "replyto": "HklGrBN8gE", "comment": "This is not a bug, but a strategy to reduce the memory consumption when (1) parameters within all incoming ops are of the same shape and (2) we know that for each node only one of its predecessors will be retained (as in the case of RNNs) and the algorithm always has the option to zero out the others. It has been mentioned in sect. A.1.2, and we will explain it in more detail in the next revision.\n\n> \"In the code, there are only N connection weight Ws\"\nLike ENAS, each node in our derived recurrent cell has only a single predecessor, hence there should be N ops (W's) in total.", "title": "Our implementation is correct"}, "H1eNn09uAm": {"type": "rebuttal", "replyto": "rJeh6xB5nQ", "comment": "Thank you for the feedback.\n\n> \u201cIt seems that the justification of equations (3) and (4) is not immediately obvious\u201d\nIn this work we treat \\alpha as a high-dimensional hyperparameter. The bilevel formulation offers a mathematical characterization of the standard hyperparameter tuning procedure, namely to find hyperparameter \\alpha that leads to the best validation performance (eq. (4)) after regular parameters w are trained until convergence on the training set (eq. (3)) given \\alpha.\n\n> \"it is not that clear why iterating between test and validation set is the right thing to do\"\nUsing two separate data splits for \\alpha and w as in the bilevel formulation should effectively prevent hyperparameter/architecture from overfitting the training data. Advantage of doing so has also been empirically verified by our experiments. Please refer to \u201cAlternative Optimization Strategies\u201d in sect. 3.3 of the revised draft.\n\nFrom the algorithmic point of view, each architecture gradient step consists of two subroutines:\n(i) Obtaining w^*(\\alpha), namely weights trained until convergence for the given architecture, by solving the inner optimization eq (4). This can normally be achieved by taking a large number of gradient descent steps of w wrt the training loss.\n(ii) Descending \\alpha wrt the validation loss defined based on w^*(\\alpha). \nOur iterative algorithm is a truncated version of the above by approximating the optimization procedure in (i) using only a single gradient step.\n\n> \u201cI think architecture pruning literature is relevant too\u201d\nYes, network pruning and (differentiable) architecture search are related despite somewhat different goals. The former aims to learn fine-grained sparsity patterns (e.g. which neurons or channels should be kept) that best approximate a given unpruned network. The latter aims to learn macro-level sparsity patterns that represent an architecture.", "title": "Response to AnonReviewer1"}, "HyxF8gidA7": {"type": "rebuttal", "replyto": "HJg9ETOFn7", "comment": "Thank you for the feedback.\n\n> Regarding the initialization of \\alpha\nWe use zero initialization which implies equal amount of attention (after taking the softmax) over all possible ops. At the early stage this ensures weights in every candidate op to receive sufficient learning signal (more exploration). This detail has been added to the revised draft.\n\n> \u201cI think (5) is misleading as it is because of k-1.\u201d\nThank you for the suggestion. This has been fixed in the revised sect. 2.3.", "title": "Response to AnonReviewer3"}, "H1gA2JsOAQ": {"type": "rebuttal", "replyto": "rkgEU1jdCX", "comment": "> \u201chow did you choose the hyperparameters of DARTS\u201d (Q8)\nWhile Adam with a small learning rate (3e-4) and the default first momentum 0.9 works well for recurrent cells, the same setup leads to slow progress for conv cells (\\alpha would remain near-uniform in 50 epochs). We thus (1) increased the learning rate by an order of magnitude to 3e-3 and (2) lowered the momentum from 0.9 to 0.5 to alleviate instability due to the increased learning rate.\n\nTo better understand the effect of different momentums, we have now repeated our CIFAR-10 experiments using momentum 0.9 instead. The newly obtained cells achieve 2.89% test error with 3.5M params (1st order) and 2.91% with 3.3M params (2nd order). These are comparable with our previous results based on momentum 0.5. \n\n> \u201cI am wondering whether the authors have a reply to this\u201d (Q9)\nIn DARTS we use a deterministic architecture encoding, where \\alpha is a continuous variable with well-defined gradients. While being conceptually simple, the method may suffer from bias due the discrepancy between \\alpha and the derived discrete architecture.\n\nThe key idea of SNAS is to replace the deterministic encoding in DARTS with a stochastic one. This modification makes architecture derivation more straightforward as \\alpha is now a discrete random variable by definition. Unlike DARTS, gradients wrt (the distribution of) \\alpha are no longer well-defined, hence Gumbel-softmax estimator is used to enable a differentiable optimization procedure. As a result, the estimated gradients are biased as long as the temperature is not zero.\n\nAs far as the empirical results are concerned, the two methods perform similarly on CIFAR-10, though the DARTS cell transfers slightly better to ImageNet. The ability of DARTS to learn the architectures of recurrent cells has also been empirically verified by its strong performance for language modeling (Table 2), whereas that of SNAS requires future investigation.\n\n> \u201cA derivation, or at least a clearer motivation for the algorithm would be useful.\u201d (2nd part of Q9)\nPlease refer to our response to AnonReviewer1 and our revised sect. 2.3.", "title": "Response to AnonReviewer2 (2/2) "}, "rkgEU1jdCX": {"type": "rebuttal", "replyto": "r1ekErZ53Q", "comment": "Thank you for the detailed comments and questions. We have fixed the missing references (Q2) and presentational issues (Q4, Q10) in the revision. Below we focus on the major points:\n\n> Regarding discretization schemes (Q1)\nThe current discretization scheme can be viewed as a heuristic to minimize the per-node rounding error, as described in the revised sect. 2.4. While refining this part was not our primary focus, it indeed deserves further study. We have also added a remark in the draft to make readers aware of this potential limitation. \n\nTo reduce the rounding error, in our preliminary experiments we tried annealing the softmax temperature to enforce one-hot selection, but did not observe clear differences in terms of the quality of the derived cells. Note that a large rounding error does not necessarily imply poor performance, since the current discretization mechanism only depends on the ranking among the strengths of the incoming edges.\n\n> \u201csince ENAS is 8 times faster one could even run it 8 times\u201d (Q3)\nWe agree it would be informative to compare DARTS and ENAS given the same search cost (e.g., 4 GPU days). Following your suggestion, we repeated the search process of ENAS for 8 times on CIFAR-10 using the authors' implementation and their best setup. We then used the same selection protocol as for DARTS by training the candidate cells for 100 epochs using half of the CIFAR-10 training data to get the validation performance on the other half. The best ENAS cell out of 8 runs achieves 2.91% test error using 4.2M params in the final evaluation, which is slightly worse than 4 runs of DARTS (2.76% error using 3.3M params). These new results have been included in Table 1 of the revised draft.\n\n> \u201cOne big question I have is where the hyperparameters come from\u201d (Q5, Q6).\nLet us explain our reasoning for each of these hyperparameters in detail:\n\nFor convolutional cells:\n\nOur setup of #cells (8->20), #epochs (600) and weight for the auxiliary head (0.4) in the final evaluation exactly follows Zoph et al., 2018. The #init_channels is enlarged from 16 to 36 to ensure a comparable model size (~3M) with other baselines. Given those settings, we then use the largest possible batch size (96) for a single GPU. The drop path probability was tuned wrt the validation set among the choices of (0.1, 0.2, 0.3) given the best cell learned by DARTS.\n\nWe treat droppath, auxiliary towers and cutout as additional augmentations only for the final evaluation. Learnable affine parameters in the batch normalisation are disabled during the search phase to avoid arbitrary rescaling of the nodes, as explained in sect A.1.1. They are enabled in the evaluation phase to ensure fair comparison with other baseline networks.\n\nFor recurrent cells:\n\nWe always use the same #units for both embedding and hidden layers, which is enlarged from 300 to 850 in the final evaluation to make our #params (~23M) comparable with other models in the literature. We then use the largest possible batch size (64) to fit our model in a single GPU. The l2 weight decay was tuned on the validation set given the best recurrent cell. We do not trigger ASGD during the search phase for simplicity and also to accommodate our current approximation scheme which does not take into account model averaging (though it can be modified to support it).\n\nBatch normalisation is useful during architecture search to prevent gradient explosion (Sect 3.1.2). Similar to the case of convnets, learnable affine params are disabled to avoid node rescaling, as explained in A.1.1 and A.1.2. Once the cell is learned, batch normalisation layers are omitted in the final evaluation for fair comparison with existing language models which usually do not involve normalisation. Our usage of batch normalisation for RNN architecture search follows ENAS.\n\n> \u201chow the best of the 24 random samples in random search is evaluated\u201d (Q7):\nThe same script is used for cell selection of DARTS and random search. All the hyperparameters, except #epochs, are identical to those in our final evaluation pipeline.", "title": "Response to AnonReviewer2 (1/2)"}, "r1lmv05d0Q": {"type": "rebuttal", "replyto": "S1eYHoC5FX", "comment": "We thank all reviewers and public commenters for their feedback. The draft has been updated and major changes include:\n+ Fixed some claims, typos and missing references.\n+ Revised sect. 2.3 to better explain the motivation of our algorithm.\n+ Revised sect. 2.4 to make the description of our discretization scheme more intuitive.\n+ Highlighted the selection and evaluation costs on top of Table 1 & 2.\n+ Added results of repeating ENAS for 8 times in Table 1.\n+ Added results of simultaneously optimizing w and \\alpha over the same set instead of two separate data splits in sect. 3.3.\n+ Changes addressing the public comments.", "title": "Draft Update"}, "rJeh6xB5nQ": {"type": "review", "replyto": "S1eYHoC5FX", "review": "(Disclaimers: I am not not active in the sub-field, just generally interested in the topic, it is easy however to find this paper in the wild and references to it, so I accidentally found out the name of the authors, but had not heard about them before reviewing this, so I do not think this biased my review).\n\nDARTS, the algorithm described in this paper, is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture is, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. DARTS has \"indicator\" weights that indicate how active components are during training, and then alternatively trains these weights (using the validation sets), and all other weights (using the training set). Those indicators are then chosen to select the final sub-graph.\n\nMore detailed comments:\n\nIt seems that the justification of equations (3) and (4) is not immediately obvious, in particular, from an abstract point of view, splitting the weights into w, and \\eta to perform the bi-level optimizations appears somewhat arbitrary. It almost looks like optimizing the second over the validation could be interpreted as some form of regularization. Is there a stronger motivation than that is similar to more classical model/architecture selection?\n\nThere are some papers that seem to be pretty relevant and are worth looking at and that are not in the references:\n\nhttp://proceedings.mlr.press/v80/bender18a.html \nhttps://openreview.net/forum?id=HylVB3AqYm (under parallel review at ICLR, WARNIGN TO REVIEWERS: contains references to a non anonymized version of this paper )\n\nI think architecture pruning literature is relevant too, it would be nice to discuss the connection between NAS and this sub-field, as I think there are very strong similarity between the two.\n\nPros:\n* available source code\n* good experimental results\n* easy to read\n* interesting idea of encoding how active the various possible operations are with special weights\n\nCons\n* tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture, in particular it was tested on two data set on which they train DARTS models, which they then show to transfer to two other data sets, respectively\n* shared with most NAS papers: does not really find novel architectures in a broad sense, instead only looks for variations of a fairly limited class of architectures\n* theoretically not very strong, the derivation of the bi-level optimization is interesting, but I believe it is not that clear why iterating between test and validation set is the right thing to do, although admittedly it leads to good results in the settings tested\n", "title": "Well exposed incremental improvement to architechture tuning that gives state-of-the-art models on two classic (but old) benchmarks", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "r1ekErZ53Q": {"type": "review", "replyto": "S1eYHoC5FX", "review": "This paper proposes a novel way to formulate neural architecture search as a differentiable problem.\nIt uses the idea of weight sharing introduced in previous papers (convolutional neural fabrics, ENAS, and Bender et al's one shot model) and combines this with a relaxation of discrete choices between k operators into k continuous weights. Then, it uses methods based on hyperparameter gradient search methods to optimize in this space and in the end removes the relaxation by dropping weak connections and selecting the single choice of the k options with the highest weight. This leads to an efficient solution for architecture search. Overall, this is a very interesting paper that has already created quite a buzz due to the simplicity of the methods and the strong results. It is a huge plus that there is code with the paper! This will dramatically increase the paper's impact. \nIn my first read through, I thought this might be a candidate for an award paper, but the more time I spent with it the more issues I found. I still think the paper should be accepted, but I do have several points of criticism / questions I detail below, and to which I would appreciate a response.\n\nSome criticisms / questions:\n\n1. The last step of how to move from the one-shot model to a single model is in a sense the most interesting aspect of this work, but also the one that leaves the most questions open: Why does this work? Are there cases where we lose arbitrarily badly by rounding the solution to the closest discrete value or is the performance loss bounded? How would other ways of moving from the relaxation to a discrete choice work? I don't expect the paper to answer all of these questions, but it would be useful if the authors acknowledge that this is a critical part of the work that deserves further study. Any insights from other approaches the authors may have tried before the mechanism in Section 2.4 would also be useful.\n\n2. The related work is missing several papers, namely the entire category of work on using network morphisms to speed up the optimization process, Bender et al's one shot model, and several early papers on neural architecture search (work on NAS did not only start in 2017 but goes back to work in the 1990s on neuroevolution that is very similar to the evolution approach by Real). This is a useful survey useful for further references: https://arxiv.org/abs/1808.05377\n\n3. I find a few of the claims to be a bit too strong. In the introduction, the paper claims to outperform ENAS, but really the paper doesn't give a head-to-head comparison. In the experiments, ENAS is faster and gives slightly worse results. The authors state explicitly that their method is slower because they run it 4 times and pick the best result. One could obviously also do that with ENAS, and since ENAS is 8 times faster one could even run it 8 times! This is unfair and should be fixed. I don't really care even if it turns out that ENAS performs a bit better with the same budget, but comparisons should be fair and on even ground in order to help our science advance -- something that is far too often ignored in the ML literature in order to obtain a table with bold numbers in one's own row.\nLikewise, why is ENAS missing in the Figure 3 plots for CIFAR, and why is its performance not plotted over time like that of DARTS?\n\n4. The paper is not really forthcoming about clearly stating the time required to obtain the results:\n- On CIFAR, there are 4 DARTS run of 1 day each\n- Then, the result of each of these is evaluated for 100 epochs (which is only stated in the caption of Figure 3) to pick the best. Each of these validation runs takes 4 hours (which, again, one has to be inferred from the fact that random search can do 24 such evaluations in 4 GPU days), so this step takes another 16 GPU hours.\n- Then, one needs to train the final network for 600 epochs; this is a larger network, so this should take another 2-3 GPU days.\nSo, overall, to obtain the result on CIFAR-10 requires about one GPU week. That's still cheap, but it's a different story than 1 day.\nLikewise, DARTS is *not* able to obtain 55.7 perplexity on PTB in 6 hours with 4 GPUs; again, there is the selection step (probably another 4*6 hours?) and I think training the final model takes about 2 GPU days. These numbers should be stated prominently next to the stated \"search times\" to not mislead the reader.\n\n5. One big question I have is where the hyperparameters come from, for both the training pipeline and the final evaluation pipeline (which actually differ a lot!).\nFor example, here are the hyperparameters for CIFAR, in this format: training pipeline value -> final evaluation pipeline value:\n#cells: 8 -> 20\nbatch size: 64 -> 96\ninitial channels: 16 -> 36\n#epochs: 50 -> 600\ndroppath: no -> yes (with probability 0.2)\nauxiliary head: no -> yes (with weight 0.4)\nBatchNorm: enabled (no learnable parameters) -> enabled\n\nThe situation is similar for PTB:\nembedding size: 300 -> 850\nhidden units per RNN layer: 300 -> 850\n#epochs: 500 -> 8000\nbatch size: 256 (SGD) -> 64 (ASGD), sped up by starting with SGD\nweight decay: 5e-7 -> 8e-7\nBatchNorm: enabled (no learnable parameters) -> disabled\n\nThe fact that there are so many differences in the pipelines is disconcerting, since it looks like a lot of manual work is required to get these right. Now you need to tune hyperparameters for both the training and the final evaluation pipeline? If you have to tune them for the final evaluation pipeline, then you can't capitalize at all on the fact that DARTS is fast, since hyperparameter optimization on the full final evaluation pipeline will be order of magnitudes more expensive than running DARTS.\n\n6. How was the final evaluation pipeline chosen? Before running DARTS the first time, or was it chosen to be tuned for architectures found by DARTS?\n\n7. A question about how the best of 4 DARTS runs is selected, and how the best of the 24 random samples in random search is evaluated: is this based on 100 epochs using the *training* procedure or the *final evaluation* procedure? Seeing how different the hyperparameters are above, this should be stated.\n\n8. A few questions to the authors related to the above: how did you choose the hyperparameters of DARTS? The DARTS learning rate for PTB is 10 times higher than for CIFAR-10, and the momentum also differs a lot (0.9 vs. 0.5). Did you ever consider different hyperparameters for DARTS? If so, how did you decide on the ones used? Is it sensitive to the choice of hyperparameters? In the author response period, could you please report the  \n(1) result of running DARTS on PTB using the same DARTS hyperparameters as used for CIFAR-10 (learning rate 3*e-4 and momentum (0.5,0.999)) and\n(2) result of running DARTS on CIFAR-10 using the same DARTS hyperparameters as used for PTB (learning rate 3*e-3 and momentum (0.9,0.999))?\n\n9. DARTS is being critizized in https://openreview.net/pdf?id=rylqooRqK7#page=10&zoom=180,-16,84\nI am wondering whether the authors have a reply to this.\nThe algorithm for solving the relaxed problem is also not mathematically derived from the optimization problem to be solved (equations 3,4), but it is more a heuristic. A derivation, or at least a clearer motivation for the algorithm would be useful.\n\n10. Further comments:\n- Equation 1: This looks like a typo, shouldn't this be x(j) = \\sum_{i<j} o(i,j) x(i) ? Even if the authors wanted to use the non-intuitive way of edges going from j to i, then o(i,j) should still be o(j,i).\n- Just above Equation 5: \"the the\"\n- Equation 5: I would have found it more intuitive had \\alpha_{k-1} already just been a generic \\alpha here.\n- It would be nice if the authors gave the explicit equations for the extension with momentum in the appendix for completeness.\n- The authors should include citations for techniques such as batch normalization, Adam, and cosine annealing.\n\n\nDespite these issues (which I hope the authors will address in the author response and the final version), as stated above, I'm arguing for accepting the paper, due to the simplicity of the method combined with its very promising results and the direct availability of code.", "title": "Very exciting; but also issues when you look into the details", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HJg9ETOFn7": {"type": "review", "replyto": "S1eYHoC5FX", "review": "The authors introduce a continuous relaxation for categorical variables so as to utilize the gradient descent to optimize the connection weights and the network architecture. It is a cool idea and I enjoyed the paper. \n\nOne question, which I think is relevant in practice, is the initialization of the architecture parameters. I might be just missing, but I couldn't find description of the initial parameter values. As it is gradient based, it might be sensitive to the initial value of alpha. \n\nIn (5), the subscript for alpha should be removed as it defines a function of alpha. I think (5) is misleading as it is because of k-1. (and remove one \"the\" in \"minimize the the validation\" in the sentence above (5))", "title": "Very interesting and promising approach", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyeSHfVcnQ": {"type": "rebuttal", "replyto": "HJek5PHFh7", "comment": "> \"Could you please give an explanation?\" (on the role of zero ops for edge selection)\nSince the zero op has been taken into account in the denominator of the edge strength (defined in sect. 2.4), edges with large weights on the zero ops are less likely to be selected.\n\nOur implementation follows the intuitions above. In particular, strengths of the zero ops are included for row-wise normalization of W (L154-155). The normalized W will then affect the output of L142 to determine the selected edges.\n\n> \"Do you have some thoughts on this phenomenon?\"\nIt is tempting to replace our current discretization scheme with temperature annealing + argmax. However, we found it nontrivial to come up with a suitable annealing schedule to simultaneously ensure (1) the temperature is low enough to yield a near-discrete architecture (thus getting rid of the \u201cmixing effect\u201d that you are referring to) (2) the temperature is high enough so that \\alpha does not get stuck at some suboptimal region, e.g., solution with lots of zeros. We leave more investigations on this direction as an interesting future work.", "title": "Response to the follow-up"}, "B1eNZMVcnX": {"type": "rebuttal", "replyto": "SkxmNjGq27", "comment": "Thanks for the suggestion. We will revise our writing accordingly.", "title": "Good point!"}, "r1e0Zyrt3X": {"type": "rebuttal", "replyto": "r1lYDFO_37", "comment": "Thank you for the questions.\n\n> \"why ZERO operation is omitted in both edge and operation selection?\"\nWe\u2019d like to point out that the zero op does play a role in determining the predecessors for each node (edge selection). Please refer to the edge strength defined in sect 2.4.\n\nOnce the predecessors are determined, the zero operations are no longer used in argmax (op selection) for two reasons:\n(1) To make our derived networks comparable with NAS/PNAS/ENAS/AmoebaNets, which all assume a fixed sparsity level, i.e., exactly two predecessors per node via *non-zero* ops.\n(2) The strengths of zero ops can be underdetermined, as will be explained below.\n\n> \"why ZERO operation tends to have largest logit?\"\nNote the behavior of the network is not sensitive to the output scale of the mixed ops due to the presence of batchnorm. This makes the strength of the zero operation underdetermined, because we can always add some incremental value to the logit of a zero op (which is equivalent to rescaling the mixed op it belongs to, according to eq (2) in sect. 2.2) with a little effect on the final classification outcome. \n\nThe above is not an issue with our current discretization scheme, which is based on the relative importance among non-zero ops only (once the active predecessors are decided). We will add more discussions on this topic in the revised paper.", "title": "Clarification regarding zero ops"}, "SkgH5LvMo7": {"type": "rebuttal", "replyto": "rkxzCbNziX", "comment": "First, we'd like to emphasize that the hyperparameters provided in our scripts were chosen based on a random subset of the training data (as the validation set) rather than the test data, though we used the 50K/10K training/test split in our released code (i.e., cnn/train.py for the final run) and printed out the errors on both sets. This is to make it easier for people to reproduce the expected test learning curves and the reported test error of *the model at the very end of training*.\n\nSecondly, we'd like to point out that training the final model using all the 50K images to obtain the test error on the 10K images is a common practice. Please refer to ResNet [1] (Sect. 4.2), DenseNet [2] (Sect. 4.1: \u201cFor the final run we use all 50,000 training images and report the final test error at the end of training\u201d), their official implementations, as well as the codebases of NAS and ENAS. Note the 45K/5K split is recommended for model selection (architecture search and hyperparameter tuning) but not for the final run.\n\nFinally, we agree that this is an important detail that should be included in the paper. We also plan to refactor our code to ensure the users do not mistakenly tune their models wrt the test set. Thanks for bringing it up and please let us know if you have any other concerns.\n\n[1] He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[2] Huang, Gao, et al. \"Densely Connected Convolutional Networks.\" CVPR. Vol. 1. No. 2. 2017.", "title": "Test set was never used as validation set"}, "B1xH2iLk2m": {"type": "rebuttal", "replyto": "S1l9xl2Rim", "comment": "Thank you for the comments. We respectfully disagree with your statement that \u201cthe loss is wrong.\u201d. The reasons are as follows:\n\n(1) Our architecture encoding is deterministic and we don\u2019t maintain any probability distribution over architectures. Hence \u201cexpectation of loss over all possible architectures\u201d in your statement is not even well-defined, not to mention the statistical consistency.\n(2) eq. 3 is just the paraphrase of \u201cfinding a (deterministic) architecture that minimizes its final validation loss.\u201d. No stochasticity is involved.\n(3) The continuous architecture \\alpha is nothing but a high-dimensional hyperparameter. While bi-level optimization is new to the field of architecture search, formulations similar to eq. 3 have been well-studied for hyperparameter search [1,2,3].\n(4) Effectiveness of eq. 3 has been empirically verified by extensive experiments.\n\n[1] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In ICML, pp. 2113\u20132122, 2015.\n[2] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In ICML, 2016.\n[3] Luca Franceschi, Paolo Frasconi, Saverio Salzo, and Massimilano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. ICML, 2018.\n", "title": "The criticism is invalid"}, "Syla8iL1hQ": {"type": "rebuttal", "replyto": "ryef0ciRjQ", "comment": "Thank you for the comments.\n\n> \u201cSelecting the k strongest predecessors to derive the final architecture cannot ensure the discrete one is the best\u201d\nWe retained 2 predecessors per node in order to make our derived cells comparable with the ones in prior works (NAS/PNAS/ENAS/AmoebaNets). This is for fair comparison but by no means the optimal discretization strategy. \n\n> \u201cActually, the \"quantization error\" might leads the final architecture to be totally different with the one from the training procedure.\u201d\nIt\u2019s expected that continuous relaxation would come with a tradeoff between efficiency and bias. Quantization error of such kind can be reduced, e.g., by annealing the softmax temperature throughout the search process, forcing the \\alpha\u2019s to approach one-hot vectors. Improving our current discretization strategy at the end of search is an interesting direction orthogonal to our main focus, i.e. the overall framework of differentiable architecture search.\n\n> \u201cALL of ops for reduce cell is max pooling and most of ops for normal cell is sep_conv_3x3\u201d\nFirst, this is incorrect. Our learned reduction cell contains not only max pooling but also skip connections; our learned normal cell contains not only sep_conv_3x3, but also skip connections and dilated convs. Please refer to Figure 4 & 5.\n\nSecondly, it\u2019s actually interesting that the algorithm learns to introduce more translation invariance in the reduction cell (through multiple pooling ops) and to come up with a densely connected normal cell (through 3x3 sep convs and skip connections). Both design patterns are existent in successful architectures designed by human experts.\n\n> \u201cIt is really wired.\u201d\nWhile visual judgements about cells in Figure 4 & 5 can be subjective, please note (1) effectiveness of those cells has been quantitatively verified by their competitive performance on both CIFAR-10 and ImageNet; (2) the algorithm can learn to leverage a more diverse set of ops when necessary. Please refer to our recurrent cell in Figure 6 with strong results on PTB. ", "title": "Better discretization is possible but orthogonal to our focus"}, "r1lW2xUCiX": {"type": "rebuttal", "replyto": "Hkxeq34sjQ", "comment": "Thank you for the questions.\n\n> \"it would be better to provide results on using the same set to optimize w and alpha\"\nThe results using this strategy are already presented in the 2nd paragraph of sect 3.3. The corresponding cell yielded 4.16 \u00b1 0.16% test error.\n\n> \"also compare the alternating update manner with the simultaneous updating\"\nFollowing your suggestion, we further treated \\alpha as part of conventional parameters and optimized it simultaneously with w. The resulting cell yielded 3.56 \u00b1 0.10% test error. \n\nTo summarize, both schemes are worse than the original bilevel formulation (2.76 \u00b1 0.09% test error), which we attribute to overfitting \u2014 note \\alpha is \"tuned\" directly on the training set in the suggested heuristics. We will expand our sect 3.3 to include more discussions.", "title": "Please refer to sect 3.3"}, "Bylnj2t5j7": {"type": "rebuttal", "replyto": "ryxFEjq_sQ", "comment": "Thanks for the question. We will include complexity analysis in the revised paper.\n\nAs for ConvNets, each of our discretized cell allows \\prod_{k=1}^4 ((k+1)*k)/2)*(7^2) = ~10^9 possible DAGs (recall we have 7 non-zero ops, 2 input nodes, 4 intermediate nodes with 2 predecessors each) without considering graph isomorphism. Since we jointly learn both normal and reduction cells, the total #architectures is approximately (10^9)^2 = 10^18. This is greater than the ~5.6*10^14 of PNAS (reported in their sect 3.1) which learns only a single type of cell.\n\nAlso note that we retained the top-2 predecessors per node only in the very end, and our continuous search space before this final discretization step is even larger. Specifically, each relaxed cell (a fully connected graph) contains 2+3+4+5 = 14 learnable edges, allowing (7+1)^14 = ~4*10^12 possible configurations (+1 to include the zero op indicating a lack of connection). Again, since we are learning both normal and reduction cells, the total number of architectures covered by the continuous space before discretization is (4*10^12)^2 = ~10^25. The above assumes that we retain only 1 of the 8 ops per edge, as done in our experiments. The search space can be substantially enlarged without additional computation overhead by retaining multiple ops per edge (e.g. by replacing the current argmax during discretization with top-K selection). We leave the exploration of this enriched space as our future work.\n\n> \"networks searched per GPU hour?\"\nThis metric is not directly applicable to DARTS, which optimizes architectures in continuous space in contrast to most prior works that enumerate architecture samples.", "title": "Complexity analysis"}, "BkllDdQUo7": {"type": "rebuttal", "replyto": "H1gSF654im", "comment": "> \u201cthere is a paper about NAS not mentioned\u201d\nThanks for mentioning about the BlockQNN paper. We will cite it as a method under the RL category.\n\n> Regarding definitions of w\u2019 and w_k\nw\u2019 means the one-step unrolled w, whose definition is given underneath eq (6). w_k means the actual numerical value of w at step k. We\u2019ll make these more clear in the revision. \n\n> \u201cI am kind of curious about the motivation of formula 5\u201d\nPlease refer to section 2.3. The motivation is to descent the architecture wrt the optimal w* instead of the current suboptimal w. The former is expensive but can be approximated by the latter after taking a gradient step. While the idea of unrolling is new to the NAS literature, similar techniques can be found in unrolled GAN [1] and MAML [2].\n\n> \u201cthe comparison between the vanilla GD and current formula 5?\u201d\nWe do have provided results to compare formula 5 (DARTS 2nd order in Table 1 & 2), vanilla GD (DARTS 1st order in Table 1 & 2) and coordinate descent (2nd paragraph in section 3.3).\n\n[1] Metz, Luke, et al. \"Unrolled generative adversarial networks.\" arXiv preprint arXiv:1611.02163 (2016).\n[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" arXiv preprint arXiv:1703.03400 (2017).", "title": "Thank you for the questions"}, "H1gHvB8Wi7": {"type": "rebuttal", "replyto": "S1eKIEw55Q", "comment": "You are welcome. We also conducted architecture search using 20 cells (with initial #channels reduced from 16 to 6 due to memory budget) without adjusting other hyperparameters. The resulting cell achieved 2.88 +/- 0.09% test error on CIFAR-10. We will include those additional results and related discussion in the revised paper.", "title": "Additional results using 20 cells"}, "HkxGFGpfcX": {"type": "rebuttal", "replyto": "Hkxx5G9f57", "comment": "Thank you for the comments.\n\n>> Regarding the number of operations\nThe #ops in our convnet experiments is the same (eight) as in PNAS [1] and is greater than 6 used in ENAS [2]. We didn\u2019t try larger numbers due to the memory constraints of a single GPU. We will include the #ops as a column in our Tables to better reflect these details.\n\n>> \"the search space is much larger than DARTS\"\nThis is not correct. While the controller in NAS must sample exactly 2 connections per node, DARTS is simultaneously exploring all possible connections within a fully-connected supergraph. Although we kept the top-k (k=2) connections in the derived discrete architecture (sect. 2.4) for fair comparison with NAS, with DARTS k could be other numbers greater than 2.\n\n>> \"Most previous NAS works seem not to use dilated convolutions.\"\nThis is not correct. Dilated convolutions are used in most prior works. Please refer to NASNets [3], AmoebaNets [4] and PNASNets [1]. \n\n>> \"Would you mind to discuss the effect of the network depth during searching?\"\nSince \\alpha is shared among cells at different layers, backprop wrt \\alpha behaves similarly to BPTT. Searching with a deeper network might thus require different hyper-parameters due to the increased number of layers (steps) to back-prop through. \n\n[1] Liu, Chenxi, et al. \"Progressive neural architecture search.\" arXiv preprint arXiv:1712.00559 (2017).\n[2] Pham, Hieu, et al. \"Efficient Neural Architecture Search via Parameter Sharing.\" arXiv preprint arXiv:1802.03268 (2018).\n[3] Zoph, Barret, et al. \"Learning transferable architectures for scalable image recognition.\" arXiv preprint arXiv:1707.070122.6 (2017).\n[4] Real, Esteban, et al. \"Regularized evolution for image classifier architecture search.\" arXiv preprint arXiv:1802.01548(2018).", "title": "Clarifications."}, "BkeB4E5f97": {"type": "rebuttal", "replyto": "BygL-vufqQ", "comment": "SMBO (used in PNAS) and MCTS are discrete search algorithms. Both do not offer an explicit notion of gradient over (the continuous representation of) the architecture as in DARTS.\n\nThe goal of the performance predictor/surrogate model in SMBO is to guide the search within the discrete space. This alone does not make the search algorithm itself differentiable.\n", "title": "Thank you for the comments."}}}