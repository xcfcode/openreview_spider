{"paper": {"title": "Sampling Generative Networks", "authors": ["Tom White"], "authorids": ["tom.white@vuw.ac.nz"], "summary": "Demonstrates improved techniques for interpolation and deriving + evaluating attribute vectors in latent spaces applicable to both VAE and GAN models.", "abstract": "We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation.  Binary classification using attribute vectors is presented as a technique supporting quantitative analysis of the latent space. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.\n", "keywords": ["Unsupervised Learning", "Deep learning", "Computer vision"]}, "meta": {"decision": "Reject", "comment": "This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation."}, "review": {"H1D8j4TDx": {"type": "rebuttal", "replyto": "SyF9R2qvl", "comment": "Ah, thanks for running that experiment - that's an interesting observation.  I suppose that the gap between these numbers will shrink as the dimension increases, but it's good to know the different between these numbers is still appreciable even in 100 dimensions.  You've convinced me that linear interpolation biases the samples towards the origin.\n\nPerhaps a plot of these two distributions on the same axes would be informative.", "title": "interesting"}, "SyF9R2qvl": {"type": "rebuttal", "replyto": "S1D_ww9we", "comment": "Agreed crossing near the origin when interpolating two random vectors would be a rare event. However, this effect occurs as samples approach the origin and is not limited to the immediate vicinity of the origin. For reference, the approximate magnitude of random normal (mean=0, sd=1) vector in 100 dimensional space is 10, but the expected length of the midpoint of two such vectors is approximately 7.\n", "title": "approaching the origin"}, "S1D_ww9we": {"type": "rebuttal", "replyto": "r1fpydeVg", "comment": "You say that \"I have found a dead zone where modes collapse as you approach the origin of the latent space.\"  But linearly interpolating between two random Normal vectors wouldn't get close to the origin very often, would it?  I think that that would only happen if the second random vector happened to be the negative of the first.", "title": "But wouldn't linear interpolation also miss the origin?"}, "BJBU-p7Vl": {"type": "rebuttal", "replyto": "HJyvnP74e", "comment": "Thanks for taking the time to review my paper. I generally agree with your assessment - the paper is a mixture of ideas that I tried to assemble into a summary and the justification for spherical interpolation is more empirically than theoretically grounded (and thus, I should probably use more or better visual examples).\n\nA couple of notes specific to your review. Slerp interpolation does not actually strictly assume that the points q1 and q2 lie on the same sphere - it can interpolate across vectors with different distances to the origin. I also agree that the original paper lacked quantitative evaluation - in an updated revision I added a quantitative assessment of the attribute vectors (Section 3.3) showing that these vectors can be the basis of binary classifiers which can be more throughly evaluated.", "title": "re: a mixture of many things"}, "ByBEvYXNe": {"type": "rebuttal", "replyto": "HJyvnP74e", "comment": "(I also do work on decoder-based generative models, so I thought I'd offer a comment)\n\nI agree with your comment that in theory, these models should map the center of the latent space to a meaningful sample. However this paper convincingly demonstrates that this isn't the case in practice. I've also observed the same issue consistently in my models. One possible explanation for this is what while, as you note, the density of an isotropic Gaussian is highest at the center, because the volume is low there, that region is never sampled from during training and consequently the model never learns to decode that \"type\" of vector (i.e. one with a small norm). On the other hand, even though any other given similarly-sized region is also very unlikely to be sampled from even once during training, the model nonetheless learns to generalize from other \"similar\" sampled points (i.e. points which are similar along some of the axes) so that it maps that region to plausible-looking data.\n\nI think that the paper's contributions here (identifying the problem and demonstrating a good workaround for it) are substantial, regardless of whether or not it convincingly establishes an underlying cause.", "title": "Re. spherical linear interpolation"}, "HJmH8dM4x": {"type": "rebuttal", "replyto": "HJz_280ee", "comment": "Thanks Tara for your comment. My more recent revision is in ICLR 2017 format.", "title": "Re: ICLR Paper Format"}, "r1fpydeVg": {"type": "rebuttal", "replyto": "ByX8EiA7l", "comment": "Thanks for your comment. It is possible I am using terminology inconsistently and I would be interested if others also concur with your distinction. Let me instead try to explain this motivation within the context of a Generative Adversarial Network with a uniform prior, since in practice this is where this issue comes up. If my reasoning is flawed here, I am of course very grateful to learn this.\n\nThe GAN generator uses vectors from this distribution during training to generate samples. So at train time the generator is optimized to generate samples based on a region within this thin shell around a hypersphere. If we later try to sample from a point halfway between two such points, we are effectively asking the generator to generate a sample from a vector drawn from a set which is effectively disjoint from the distribution of vectors it was trained on. This seems inconsistent, and in practice I have found a dead zone where modes collapse as you approach the origin of the latent space.\n", "title": "re: spherical interpolation with uniform prior, follow up"}, "H1Bft90ml": {"type": "rebuttal", "replyto": "BkP_hHhQe", "comment": "Thanks for your comment. This is an interesting experiment that I had not previously considered.", "title": "Re: Shallow-shell assumption for the approx. posterior"}, "BJ8yIcAXe": {"type": "rebuttal", "replyto": "r1VS31C7x", "comment": "Because in high dimensional space uniform random vectors will have roughly the same magnitude.\n\n>>> import numpy as np\n>>> random_points = np.random.uniform(low=-1, high=1, size=(1000,100))\n>>> lengths = map(np.linalg.norm, random_points)\n>>> print(\"Mean length is {:3.2f} and std is {:3.2f}\".format(np.mean(lengths), np.std(lengths)))\nMean length is 5.76 and std is 0.26\n\nThe central limit theorem says this magnitude will converge toward a normal distribution as you increase the dimensionality of the vector.", "title": "spherical interpolation with uniform prior"}, "BkP_hHhQe": {"type": "review", "replyto": "SypU81Ole", "review": "I (obviously) agree that we expect a majority of the probability mass to be concentrated near a shallow shell in the high dimensional latent space and that this fact merits discussion because it is often overlooked and not taken into consideration.\n\nBut did you actually test whether this assumption indeed holds for the (average) approximate posterior q(h|x), and not just for the prior p(h)? This could otherwise indicate an interesting mismatch between the prior and the model. A simple histogram over the L2 norms of samples from the approx. posterior might be sufficient here.In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.\n\nI find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it\u2019s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical/empirical) insight and it does not have the scientific quality and depth I\u2019ve seen in many other ICLR submissions. But it does more than just describing useful \u201ctricks\u201d. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)\n", "title": "Shallow-shell assumption for the approx. posterior", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ByVgIvXEe": {"type": "review", "replyto": "SypU81Ole", "review": "I (obviously) agree that we expect a majority of the probability mass to be concentrated near a shallow shell in the high dimensional latent space and that this fact merits discussion because it is often overlooked and not taken into consideration.\n\nBut did you actually test whether this assumption indeed holds for the (average) approximate posterior q(h|x), and not just for the prior p(h)? This could otherwise indicate an interesting mismatch between the prior and the model. A simple histogram over the L2 norms of samples from the approx. posterior might be sufficient here.In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.\n\nI find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it\u2019s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical/empirical) insight and it does not have the scientific quality and depth I\u2019ve seen in many other ICLR submissions. But it does more than just describing useful \u201ctricks\u201d. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)\n", "title": "Shallow-shell assumption for the approx. posterior", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1lChdd7x": {"type": "rebuttal", "replyto": "BkzKKYB7l", "comment": "Thanks for your comment on format - the pdf is now in ICLR_2017 format.\n\nYes, in practice all random vectors in high dimensions with a gaussian or uniform prior lie very near a hyphersphere. This is non-intuitive but valid assumption for uniform distributions, which is why it merits discussion. Theta is the angle between the two vectors two be interpolated. Here is python function that implements slerp.\n\n# val ranges from [0,1]. interpolates from low to high.\ndef slerp(val, low, high):\n    theta = numpy.arccos(numpy.dot(low/numpy.linalg.norm(low), high/numpy.linalg.norm(high)))\n    so = numpy.sin(theta)\n    return numpy.sin((1.0-val)*omega) / so * low + numpy.sin(val*omega)/so * high\n\nNote that both the formula in the paper and this implementation handle elevation changes and so don't strictly assume that the initial points lie on the same hypersphere.", "title": "formatting and interpolation"}, "BkzKKYB7l": {"type": "review", "replyto": "SypU81Ole", "review": "The format of this paper is not ICLR format.\n\nThe spherical interpolation makes an implicit assumption that all data points lie on a sphere, which maybe a better assumption than linear space for Gaussian distributed data but not for uniform distributions.  Also the theta parameter in spherical interpolation looks arbitrary, I wonder how is it chosen?This paper proposed a set of different things under the name of \"sampling generative models\", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.\n\nThe spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances to the origin.  The author's justification for this comes from the well known fact that in high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube.  This is true because in high-dimensional space, the outer shell takes up most of the volume in space, and the inner part takes only a very small fraction of the space, in terms of volume.  This does not mean the density of data in the outer shell is greater than the inner part, though.  In a uniform distribution, the data density should be equal everywhere, a point on the outer shell is not more likely than a point in the inner part.  Under a Gaussian model, the data density is on the other hand higher in the center and much lower on the out side.  If we have a good model of data, then sampling the most likely points from the model should give us plausible looking samples.  In this sense, spherical interpolation should do no better than the normally used linear interpolation.  From the questions and answers it seems that the author does not recognize this distinction.  The results shown in this paper seem to indicate that spherical interpolation is better visually, but it is rather hard to make any concrete conclusions from three pairs of examples.  If this is really the case then there must be something else wrong about our understanding of the learned model.\n\nAside from these, the J-diagram and the nearest neighbor latent space traversal both seems to be good ways to explore the latent space of a learned model.  The attribute vector section on transforming images to new ones with desired attributes is also interesting, and it provides a few new ways to make the GAN latent space more interpretable.\n\nOverall I feel most of the techniques proposed in this paper are nice visualization tools.  The contributions however, are mostly on the design of the visualizations, and not much on the technical and model side.  The spherical interpolation provides the only mathematical equation in the paper, yet the correctness of the technique is arguable.  For the visualization tools, there are also no quantitative evaluation, maybe these results are more art than science.", "title": "formatting and others", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJyvnP74e": {"type": "review", "replyto": "SypU81Ole", "review": "The format of this paper is not ICLR format.\n\nThe spherical interpolation makes an implicit assumption that all data points lie on a sphere, which maybe a better assumption than linear space for Gaussian distributed data but not for uniform distributions.  Also the theta parameter in spherical interpolation looks arbitrary, I wonder how is it chosen?This paper proposed a set of different things under the name of \"sampling generative models\", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.\n\nThe spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances to the origin.  The author's justification for this comes from the well known fact that in high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube.  This is true because in high-dimensional space, the outer shell takes up most of the volume in space, and the inner part takes only a very small fraction of the space, in terms of volume.  This does not mean the density of data in the outer shell is greater than the inner part, though.  In a uniform distribution, the data density should be equal everywhere, a point on the outer shell is not more likely than a point in the inner part.  Under a Gaussian model, the data density is on the other hand higher in the center and much lower on the out side.  If we have a good model of data, then sampling the most likely points from the model should give us plausible looking samples.  In this sense, spherical interpolation should do no better than the normally used linear interpolation.  From the questions and answers it seems that the author does not recognize this distinction.  The results shown in this paper seem to indicate that spherical interpolation is better visually, but it is rather hard to make any concrete conclusions from three pairs of examples.  If this is really the case then there must be something else wrong about our understanding of the learned model.\n\nAside from these, the J-diagram and the nearest neighbor latent space traversal both seems to be good ways to explore the latent space of a learned model.  The attribute vector section on transforming images to new ones with desired attributes is also interesting, and it provides a few new ways to make the GAN latent space more interpretable.\n\nOverall I feel most of the techniques proposed in this paper are nice visualization tools.  The contributions however, are mostly on the design of the visualizations, and not much on the technical and model side.  The spherical interpolation provides the only mathematical equation in the paper, yet the correctness of the technique is arguable.  For the visualization tools, there are also no quantitative evaluation, maybe these results are more art than science.", "title": "formatting and others", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJz_280ee": {"type": "rebuttal", "replyto": "SypU81Ole", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format for your submissions to be considered. Thank you!", "title": "ICLR Paper Format"}}}