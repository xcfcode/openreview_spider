{"paper": {"title": "Stiffness: A New Perspective on Generalization in Neural Networks", "authors": ["Stanislav Fort", "Pawe\u0142 Krzysztof Nowak", "Stanis\u0142aw Jastrzebski", "Srini Narayanan"], "authorids": ["stanislav.fort@gmail.com", "powalnow@google.com", "staszek.jastrzebski@gmail.com", "srinin@google.com"], "summary": "We defined the concept of stiffness, showed its utility in providing a perspective to better understand generalization in neural networks, observed its variation with learning rate, and defined the concept of dynamical critical length using it.", "abstract": "We investigate neural network training and generalization using the concept of stiffness. We measure how stiff a network is by looking at how a small gradient step on one example affects the loss on another example. In particular, we study how stiffness depends on 1) class membership, 2) distance between data points in the input space, 3) training iteration, and 4) learning rate. We experiment on MNIST, FASHION MNIST, and CIFAR-10 using fully-connected and convolutional neural networks. Our results demonstrate that stiffness is a useful concept for diagnosing and characterizing generalization. We observe that small learning rates reliably lead to higher stiffness at a given epoch as well as at a given training loss. In addition, we measure how stiffness between two data points depends on their mutual input-space distance, and establish the concept of a dynamical critical length that characterizes the distance over which datapoints react similarly to gradient updates. The dynamical critical length decreases with training and the higher the learning rate, the smaller the critical length.", "keywords": ["stiffness", "gradient alignment", "critical scale"]}, "meta": {"decision": "Reject", "comment": "While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.\n\nConcerns raised include lack of sufficient motivation for the approach, and problems with clarity of the exposition."}, "review": {"H1xhRSnvsB": {"type": "review", "replyto": "H1e31AEYwB", "review": "This paper introduces \u201cstiffness\u201d, a new metric to characterize generalization in neural networks. Stiffness is a pretty simple concept and is relatively straightforward to compute. The authors evaluate this metric on standard datasets using two relatively small neural networks. On the whole, the paper is written clearly and explains its methodology in simple language.\n\nI have a few observations:\n1. The equivalence between equation 2 and equation 3 is mentioned in passing but no explanation is provided. Th equivalence is not clear so I would encourage the authors to provide a short proof.\n2.  Since stiffness depends on the gradients obtained on points in the input space, which in turn depends on the loss, why would a practitioner training a neural network turn to stiffness to diagnose overfitting instead of just looking at the values of the training and validation losses? Indeed, the authors themselves say that a network has overfitted when training and validation losses diverge. The paper fails to motivate why stiffness is better than just looking at losses during training. \n3. The authors mention \u201cThe train-val stiffness is directly related to generalization, as it corresponds to the amount of improvement on the training set transferring to the improvement of the validation set. \u201d. Typically, generalization is evaluated on a held out test set so I fail to understand what the authors mean by this statement. We would expect validation error to underestimate test error  so while they are related, train-val stiffness would not necessarily characterize generalization. It would be interesting to see a train-test stiffness graph to test the authors claim.\n4. The paper fails to motivate the the utility of the concept of \u201cDynamical Critical distance\u201d. Since the primary goal of  paper is to understand generalization, I would like the authors to clarify the motivation to study this quantity. What additional insight does this provide with respect to generalization?\n5. The term \u201cdynamical critical distance\u201d is not used uniformly. For example, it is mentioned as \u201cdynamical critical scale\u201d in section 3.3 and \u201cdynamical critical length\u201d in section 4.2.\n6. While the paper on the whole is written in a clear fashion, I found section 4.4 to be particularly confusing. The authors should consider rewriting that section to make it clearer.\n\nIn summary, the concept of stiffness seems to closely follow training and validation losses and any problem diagnosed using stiffness would therefore be also diagnosed via examining the loss values. This along with other concerns mentioned above mean that I cannot recommend this paper for publication.", "title": "Official Blind Review #5", "rating": "3: Weak Reject", "confidence": 2}, "H1gwW5Ynjr": {"type": "rebuttal", "replyto": "rJguWFm3sr", "comment": "---------------------------\nThank you for your detail points. We believe many of them are related to the two misconceptions we tried to clarify above. We would like to respond to your last point: \n\n\u201c\u2026.. with a larger learning rate the volume around an input vector (characterised by a small or zero change in loss), is smaller. We agree that the initial statement of ours is wrong. \u2026.. motivation and use of such an observation is not entirely clear to us.\u201d \n\nThank you having a closer look and updating your review. We believe that there is a great value in understanding the specific details of functions we learn when we train deep neural networks on real data using gradient descent. Our work\u2019s primary aim is to provide a new and interesting metric and characterize its behavior. We find stiffness and its input-space-distance behavior interesting, because it tells us about how local can the changes to the learned function be. If the dynamical critical length were really large, we could not learn any detailed class labeling on the input space. If it were too small, we would not be able to generalize. The fact that, according to this metric, the learning rate we choose leads to quite different functions learned, even though the more coarse-grained metrics such as loss do not seem to suggest so, is interesting to us. Our goal is to provide a piece of the puzzle in explaining and understanding how DNNs learn.", "title": "A response to the response to the Rebuttal [2/2]"}, "HkeqAKK2sr": {"type": "rebuttal", "replyto": "rJguWFm3sr", "comment": "Thank you for your quick and detailed reply. We appreciate that you are engaging in a discussion with us. While we primarily agree with your points on clarity of exposition to people unfamiliar with the subfield, we found several large points of misunderstanding in your response, primarily concerning the very definition of stiffness and the followup use of it in the definition of the critical dynamical scale. We will address those here.\n\n-----------------------------\nStiffness, as we define it in Eq. 4 and Eq. 5 (we look at two very related variants of it), is the *product between loss changes between two examples*. It is *not* the difference between them. If one example changes its loss by dL1, and the other one by dL2, stiffness ~ dL1 x dL2.\n\nWe illustrate this in Figure 1. If both dL1 and dL2 have the same sign (e.g. if both L1 and L2 go down), the stiffness is high. If they do not care about each other, stiffness is 0. If they anti-correlate, i.e. the decrease of L1 increases L2, the stiffness is negative.\n\nYour point:\n\u201cStiffness between two identical samples is 0. Arguably, as the cosine distance of input samples grows as their absolute stiffness increases.\u201d\n\nThis is certainly not true, from the definition. Between two identical examples, dL1 = dL2, and g1 = g2, therefore both definitions (Eq. 2 and 3) give you stiffness of 1, not 0. This is also clear from the illustrative Figure 1, where you can see that if X1 = X2, they have to change the loss the same, therefore they have stiffness of 1. \n\nWe believe this might be a significant source of your confusion in the followup dynamical critical scale discussion.\n\n-----------------------------\nYour point:\n\u201cStiffness between two identical samples is 0. Arguably, as the cosine distance of input samples grows as their absolute stiffness increases. (Though we don't recall that this is explicitly mentioned anywhere. Do you assume that stiffness is monotonic? We don't think that this is necessarily the case.) \n\ndynamical critical length (DCL): A learned linear estimate of the maximum distance between any two input points where where stiffness is (still) 0.\u201d\n\nWe believe a part of this was cleared up by our previous point. Stiffness between two identical images X1 = X2 is 1. The farther X2 is from X1, the less stiff you would expect them to be. We measure this change of stiffness with distance between X1 and X2 and show our empirical results in Figures 5, 9 and 10. What you see there is the amount of stiffness between examples (y axis) going down from 1 as you increase the (cosine) distance between the two inputs X1 and X2 (x axis).\n\nWe define the dynamical critical length to be the distance at which, for the first time, the stiffness between 2 examples reaches zero -- typically, the stiffness will be >0 for smaller distances (and be 1 as we approach the 0 distance).\n\n---------------------------\nYour point:\n\u201c It is not clear to us what aspect of this linear estimate is \"dynamic\" or \"critical\" but we understand that this might relate to established terminology that we are not aware of. \u201c\n\nThe \u201ccritical\u201d part is that this is the distance at which, for the first time (i.e. at no smaller distances typically) gradient updates will *not* induce correlated changes in loss between inputs X1 and X2. For smaller distances, the dL1 and dL2 and correlated on average. For larger distances, they are not strongly correlated. This dynamical critical length is \u201ccritical\u201d in the sense that these two behaviours change there.\n\nThe \u201cdynamical\u201d part is to distinguish it from the correlation length of the neural network outputs themselves. Let\u2019s say the logits have a certain value at point X and for points at distance d from X there is no significant change in the logits. That would the normal (\u201cstatic\u201d) correlation length and it is dealt with in the spectral bias of NNs literature. In our case, we are interested in the correlations of *changes* of the loss (which is a proxy for the NN fn for us). In that sense, the length is \u201cdynamical\u201d. We use this term in order to distinguish it from the more usual correlation length, which characterizes the function outputs, and not their changes on gradient updates.\n\n", "title": "A response to the response to the Rebuttal [1/2]"}, "SyxlE4TRcH": {"type": "review", "replyto": "H1e31AEYwB", "review": "This paper introduces the concept of stiffness: a measure of the change in the loss of sample A due to a gradient step based on sample B. It analyses the expected dynamic for A, B samples from the same and different classes, as well as, samples from the train and test sets.\n\nTo better understand the dynamics of optimization in neural networks is an open and important problem and the paper is clearly motivated in this regard. The proposed method is straight forward and I am not aware of a similar method. \n\nIn addition to that, the paper also introduces \"dynamical critical length \u03be\" which is the stiffness of A, B samples based on the cosine similarity of the respective inputs (section 2.4). A linear estimator of when this length becomes 0 is also introduced. Confusingly this is also called the \"dynamical critical length \u03be\" in section 4.2. Later on the term \"dynamical scale \u03be\" and \"dynamical critical scale \u03be\" seem to be used interchangeably. Figure 6 mentions the \"critical length \u03c7\" on the y-axis which seems to be a typo as no such measure was introduced.\n\nThe equivalence between eq. 2 and the two parts of eq. 3 is not obvious. We'd appreciate if the authors would provide a proof of such. \n\nOverall, the paper is written in a simple language but paragraphs remain surprisingly hard to understand. An example of such is e.g. section 4.4: What do the authors mean by \"characteristic distance\" between two input points? What is \"the typical scale of spatial variation\" of a function? etc.\n\nThe paper concludes that:\n\n1.) there is a link between generalization and stiffness\n2.) stiffness decreases with the onset of overfitting\n3.) \"general gradient updates with respect to a member of a class help to improve loss on data points in the same class\"\n4.) \"The pattern breaks when the model starts overfitting to the training set, after\nwhich within-class stiffness eventually reaches 0\"\n5.) This is observed for different models on different datasets\n6.) \"we observed that the farther the datapoints and the higher the epoch of training, the less\nstiffness exists between them on average\"\n7.) \"the higher the learning rate, the smaller the \u03be\"\n\nVerdict: Reject\n\nThe conclusions are self-evident. The paper fails to demonstrate the usefulness of stiffness and most results are expected and provide little to no insights into the optimization dynamics of deep neural networks. In fact, the reasoning in this paper is almost tautological (conclusions 1-6).\n\nE.g. if the A, B samples used to compute stiffness are separately drawn from the train and test set then stiffness is a proxy for the difference between the train error and the test error after another gradient step. The authors then compute stiffness at different points of the optimization procedure and conclude that stiffness decreases when the network starts to overfit. Since overfitting is the point in training where train error and test error diverge it is obvious that this can also be observed with regards to \"stiffness\". Hence, the reasoning is circular.\n\nConclusion 7 is slightly different in that it observes that larger learning rates result in smaller \u03be which, given the previous paragraph, we can rewrite into the statement \"larger learning rates generalise better\". This is a well known empirical observation and has been discussed thoroughly (e.g. on connection with flat and sharp minima or learning rate decay schedules). \n\nDisclaimer: This review was done on short notice. ", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 2}, "Syx3n1xhor": {"type": "rebuttal", "replyto": "H1xhRSnvsB", "comment": "------------------------\n\u201cIn summary, the concept of stiffness seems to closely follow training and validation losses and any problem diagnosed using stiffness would therefore be also diagnosed via examining the loss values. This along with other concerns mentioned above mean that I cannot recommend this paper for publication.\u201d\n\nWe do not agree with your characterization of our contribution. It is of course true that we could look at losses directly and it is indeed what we do -- we defined stiffness as the correlation between loss changes. That, however, is not the main contribution of our paper. We study how this concept depends on class membership, the stage of training, and the learning rate used for training. Those results could certainly not be inferred from the total loss and (according to us) deserve a closer look.", "title": "Response to Review #5 2/2"}, "r1gYF1x3oS": {"type": "rebuttal", "replyto": "H1xhRSnvsB", "comment": "Thank you for your review and comments. Your review was released to us only in the middle of the rebuttal period and we therefore didn't have the expected time to prepare a reaction. We would like to address several of the points your brought up.\n\n-------------------------\n\u201c1. The equivalence between equation 2 and equation 3 is mentioned in passing but no explanation is provided.\u201d\n\nThe connection between Equation 2 and Equation 3 is very simple and we therefore believed it did not require a detailed proof, however, we\u2019re happy to explain it in multiple steps. It is a Taylor expansion to the first order.\n\nThe steps are as follows:\nThe derivative of the loss L at image X1 with respect to the weight vector W is the gradient vector g1. If we look at the Taylor expansion of the change of loss due to a vector change of weights w, we obtain the dot product delta L = g1 dot w to the first order in the Taylor series. In particular, for a weight change induced by a small gradient step -epsilon*g1, we get delta L = - epsilon g1 dot g1. This is true as long as epsilon -> 0, which we take in the paper.\n\n-------------------------\n\u201c2. Since stiffness depends on the gradients obtained on points in the input space, which in turn depends on the loss, why would a practitioner training a neural network turn to stiffness to diagnose overfitting instead of just looking at the values of the training and validation losses? \u201c\n\nStiffness as we defined it is related to the transfer of gains in performance from one input datapoint to another. It is defined by looking at the correlation between loss changes on different inputs, rather than the total loss at once. As such, it is a much finer metric than the total loss. However, you could look at loss changes on individual images and it would be equivalent -- that is exactly how we defined the concept of stiffness in the first place in Equations 1 - 5. Its connection to the gradient alignment is a mathematical consequence and it is a useful way to look at it, as it makes a direct connection to other works on gradient and Hessians in neural networks. However, you can definitely think about the correlation between changes in loss alone -- we call the particular product stiffness, since it geometrically relates to how easily \u201cbendable\u201d the learned NN function is. \n\n----------------\n\u201c3. The authors mention \u201cThe train-val stiffness is directly related to generalization, as it corresponds to the amount of improvement on the training set transferring to the improvement of the validation set. \u201d. Typically, generalization is evaluated on a held out test set so I fail to understand what the authors mean by this statement.\u201d\n\nThe misunderstanding here is between the validation set and the test set nomenclature. We used the term validation set to mean the held-out, never-trained-on dataset, and therefore used it to characterize generalization. You could easily call this the test set, since the important distinction wrt the train set is that not a single gradient step was ever taken wrt to a single image in it. We feel this point is only a matter of wording and we\u2019ll try to make it clearer in the paper.\n\n----------------\n\u201c4. The paper fails to motivate the the utility of the concept of \u201cDynamical Critical distance\u201d. Since the primary goal of paper is to understand generalization, I would like the authors to clarify the motivation to study this quantity. What additional insight does this provide with respect to generalization? \u201c\n\nWe firmly believe that our concept of dynamical critical length is of interest and well motivated, as it captures how localized changes to the learned NN function are when a gradient step with respect to a particular example is applied. It is very related to the rich literature on the spectral bias of neural networks. In our case, we study what could be seen as the dynamical equivalent of the spectral bias -- i.e. on which length scales in the input space do changes to the learned function stop affecting the rest of the function.\n\nThe additional insight you are asking for is that this allows us to measure how localized changes to the learned function are. If you apply the gradient on input X1, the inputs X whose loss will change in a correlated manner are at a typical distance \u03be or closer. We measure how this distance changes both with training time and the learning rate used. This gives us an additional diagnostic tool which is in turn directly useful in studying how independent the effects of gradient updates are between different images.\n\n------------------------\n\u201c5. The term \u201cdynamical critical distance\u201d is not used uniformly. \u2026. The authors should consider rewriting that section to make it clearer. \u201c\n\nThank you for having a closer look. We will correct the typos and make sure to provide greater uniformity in the relevant subsection.\n\n", "title": "Response to Review #5 1/2"}, "BJxHnTyhor": {"type": "rebuttal", "replyto": "SyxlE4TRcH", "comment": "---------------------------\n\u201cHence, the reasoning is circular.\u201d\n\nWhile we understand your point of view, i.e. that in general, when studying the stiffness between train and val examples and provided that both train and val losses go down with training time, you would expect stiffness to be high on average, this is true only *on average*. In our analyses, we go significantly beyond that along several dimensions and we will detail why we do not believe our reasoning is tautological:\n\n(a) While the on average the train-val stiffness can be expected to be high before overfitting, this does not explain the behavior of the val-val stiffness. The val-val stiffness is very different since not a single gradient step is taken with respect to a single validation image, yet the val-val stiffness mimics very closely the behavior of the train-val stiffness. \n\n(b) What you say is true *on average*, but does not address the stiffness behavior in detail, which is the bulk of our paper. We study the stiffness between examples based on their class membership and our results would not be predicted by simply stating that on average the stiffness must be high before overfitting. The same goes for the stiffness between different classes being negative but only slightly in magnitude.\n\n(c) The dependence of stiffness on the distance between images would not be predicted apriori, and neither would the decrease in the \"dynamical critical scale \u03be\" with training time.\n\n------------------------\n\u201cConclusion 7 is slightly different in that it observes that larger learning rates result in smaller \u03be which, given the previous paragraph, we can rewrite into the statement \"larger learning rates generalise better\". This is a well known empirical observation and has been discussed thoroughly (e.g. on connection with flat and sharp minima or learning rate decay schedules). \u201c\n\nWe do not fully agree with your characterization of our results. It cannot simply be translated to \"larger learning rates generalise better\". In fact, the larger learning rates leading to functions that are influenced more locally by gradient updates (i.e. an input A will change the loss at input space positions within distance \u03be, where \u03be is smaller for large LRs) could be naively characterized as being exactly the opposite. A more local modification of the function could lead to *less* generalization. We therefore do not believe that your characterization is correct and while we do not have a good theoretical explanation yet, it certainly is not as simple a question as you made it sound.", "title": "Response to Review #4  2/2"}, "SyxtyTkhsH": {"type": "rebuttal", "replyto": "SyxlE4TRcH", "comment": "Thank you for your review and comments. We appreciate that your review was done on a very short notice and thank you for it. We would like to address and dispute several of the points your brought up.\n\n---------------------------\n\u201cOverall, the paper is written in a simple language but paragraphs remain surprisingly hard to understand. An example of such is e.g. section 4.4: What do the authors mean by \"characteristic distance\" between two input points?\u201d \u2026. \u201c What is \"the typical scale of spatial variation\" of a function?\u201d\n\nWe would like to clarify the points of confusion in Subsection 4.4. The \u201ccharacteristic distance\u201d point you bring up is a part of a longer statement:\n\u201cA natural question arises as to whether the characteristic distance between two input points at which stiffness reaches zero defines the typical scale of spatial variation of the learned function.\u201d\nSo the first \u201ccharacteristic distance\u201d refers to the distance between two points in the input space where the gradient step with respect to one of them will not influence the other one. This is the quantity we call \"dynamical critical length/scale \u03be\" and which we empirically measure in real networks trained on real data in Figures 5, 6, 9, 10 and 11. \n\nThe \"the typical scale of spatial variation\" of a function is its dominant Fourier mode in the input space -- i.e. the length scale in the input space over which the predictions change significantly. This scale is related to the concept often called the spectral bias of neural networks and there is a large literature on the topic.\n\nOur point in Subsection 4.4 was to make clear that the scale on which input points do not influence each other under gradient updates = \"dynamical critical length/scale \u03be\", and the scale over which the predictions change significantly in the input space = \"the typical scale of spatial variation\", are not necessarily the same.\n\nWe appreciate that this might have been harder to understand based on our description and will try to rephrase the paragraph for clarity.\n\n---------------------------\nDifferent terms used for \u03be and typos\n\"dynamical critical length \u03be\" in section 4.2. Later on the term \"dynamical scale \u03be\" and \"dynamical critical scale \u03be\" \u2026.. \"critical length \u03c7\"\n\nThank you for spotting that we use the words \u201cscale\u201d and \u201clength\u201d interchangeably. We will adopt a single one to ensure clarity. You are right that in Figure 6 \u03c7 should have been \u03be. This was a typo on our part and we will change it to  \u03be.  \n\n--------------------------\n\u201cThe equivalence between eq. 2 and the two parts of eq. 3 is not obvious. We'd appreciate if the authors would provide a proof of such. \u201c\n\nThe connection between Equation 2 and Equation 3 is very simple and we therefore believed it did not require a detailed proof, however, we\u2019re happy to explain it in multiple steps. It is a Taylor expansion to the first order.\n\nThe steps are as follows:\nThe derivative of the loss L at image X1 with respect to the weight vector W is the gradient vector g1. If we look at the Taylor expansion of the change of loss due to a vector change of weights w, we obtain the dot product delta L = g1 dot w to the first order in the Taylor series. In particular, for a weight change induced by a small gradient step -epsilon*g1, we get delta L = - epsilon g1 dot g1. This is true as long as epsilon -> 0, which we take in the paper.\n\n--------------------------\n\u201cThe conclusions are self-evident.\u201d\n\nWhile the self-evidence of our conclusions is a matter of subjective judgement, we do not believe that our results are in fact self-evident and our discussions with fellow researchers support this. It is hard to rebut this point, however, if you do not provide links to specific sources in literature.", "title": "Response to Review #4  1/2"}, "SygPusJnoH": {"type": "rebuttal", "replyto": "BJgSdPqotS", "comment": "Thank you for your review and comments. We hope that you will champion our paper.\n\nWe provide a detailed response to the points you brought up below.\n\n------------\n\u201cFirst, the authors study several configurations like train-train, train-val and val-val. However, these configurations are still in-domain analysis, the data distribution is quite similar.\u201d\n\nWe studied the stiffness between input images from the training set and the validation set precisely in order to directly quantify the transfer of performance improvement gained on the training set to the unseen validation set. We agree that those distributions are hopefully very similar, however, they are not identical. In that sense, this constitutes a weak version of the out-of-distribution performance experiment you were suggesting. \n\nWe agree that adding an experiment where the domain gap is large might be interesting, however, the focus of our paper was not on transfer learning (where this is a common regime), but rather on learning on a specific dataset itself. The transfer we were concerned with was from one example to another, i.e. within dataset generalization. If we, for example, looked at the stiffness between train images and random noise images, the interpretation of that metric would be very difficult, as it is not a priori known what kind of behavior would even be desirable there. It could even be the case that you do not want to transfer any performance to random out-of-distribution images, as this could limit your performance on the actual distribution.\n\n---------------------\n\u201cSecond, the datasets. I understand that for theoretically analysis, small datasets are quick to converge and easy to demonstrate. However, this submission focuses on generalization problem during transfer learning. Hence, it needs at least a bigger dataset, like ImageNet, to show it really works.\u201d\n\nWe understand that looking at ImageNet would strengthen our case, however, the stiffness calculations are very computationally demanding and the consistent appearance of the effects on MNIST, Fashion MNIST and CIFAR-10 is, according to us, a good indication of their generality. In addition, the goal of our paper is to introduce a metric and show its usefulness, for which we believe CIFAR-10 can be sufficient. Nonetheless, we will try to show our results on larger datasets.\n", "title": "Response to Review #1"}, "BJgSdPqotS": {"type": "review", "replyto": "H1e31AEYwB", "review": "This submission introduces a metric, termed stiffness, to evaluate the generalization capability of neural networks. The metric is novel and straightforward, it measures how stiff a network is by looking at how a small gradient step on one example affects the loss on another example. The authors study several configurations  on three small datasets. They demonstrate that stiffness is a useful concept for diagnosing and characterizing generalization. \n\nI give an initial rating of weak accept because (1) The paper is well motivated and well written. Studying generalization is important for neural networks. (2) It seems from experiments that stiffness is a useful metric to indicate models' generalization capability. However, I have a few concerns. \n\nFirst, the authors study several configurations like train-train, train-val and val-val. However, these configurations are still in-domain analysis, the data distribution is quite similar. It can not support author's claims well. Adding an experiment where domain gap is large will make the submission stronger, such as train-test, cross-dataset or challenging tasks like semantic segmentation. \n\nSecond, the datasets being used are very small. I understand that for theoretically analysis, small datasets are quick to converge and easy to demonstrate. However, this submission focuses on generalization problem during transfer learning. Hence, it needs at least a bigger dataset, like ImageNet, to show it really works. ", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}}}