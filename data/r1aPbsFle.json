{"paper": {"title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling", "authors": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher"], "authorids": ["inanh@stanford.edu", "khosravi@stanford.edu", "rsocher@salesforce.com"], "summary": "", "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.", "keywords": ["Natural language processing", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "pros:\n - nice results on the tasks that justify acceptance of the paper\n \n cons:\n - In my opinion its a big stretch to describe this paper as a novel framework. The reasons for using the specific contrived augmented loss is based on the good results it produces. I view it more as regularization.\n - The \"theoretical justification\" for coupling of the input and output layers is based on the premise that the above regularization is the correct thing to do. Since that's really not justified by some kind of theory, I think its questionable to call this simple observation a theoretical justification.\n - Tying weights on the inputs and output layers is far from novel."}, "review": {"SJ-dhUewg": {"type": "rebuttal", "replyto": "B179uBlPl", "comment": "\n--Response about the issue regarding eqn. 3.5--\nWe're sorry that we did not provide an update, but actually our initial formulation was correct, and our response to the review was incorrect. Note that y~ sums up to 1 , and the factor in front of y^ becomes 1 when summed over all the i's. We are sorry we were negligent about following up with this issue, although minor. But in conclusion, the arguments in the paragraph are valid.\n\n\n-- Response to comments about the motivation behind using same L matrices in eqns 2.1 and 3.6 --\n*We assume throughout that it is acknowledged that L matrices in eqns 3.6 and 3.7 are the same*\nWe feel that there might be a slight miscommunication. The use of the embeddings of the model itself to improve learning in a self contained manner is in fact the core of our work, and motivation behind it is as explained in our introductory paragraphs and section 3. We do not claim to further theoretically motivate this idea in our work, and we actually agreed with your comments in our first reply above. Overall, our main theoretical contribution is the results we present assuming this framework, which is in fact motivated in section 3, based on previous work such as [1],[2], as well as the widely recognized metric features of trained word vectors. We would be more than happy to further elaborate on this point should there be any remaining concerns. \nAt this point, for the sake of completeness of our argument, we would like to elaborate once more on the alternative scenario where L in 2.1 and L in 3.6 are not the same (Let's call the one in eqn 3.6 L' keeping the convention from our previous discussion). There are two cases:\n1) L' is static, and not learned. In this case, the output classifier matrix, W, will get close to L'. As a result, in the extreme case where W is actually L', this means it would make sense to tie W and L' together. We have a discussion of this case above which explains why this may not be a good idea.\n2) L' is learned with other parameters. This actually is probably a rather boring case because of the following: L' comes into the problem only through the augmented loss term, and this term forces W and L' to be close together, as argued above. With no other force acting on L', the optimizer will probably want to choose L'=W (that is, L' is set to whatever W is) with very little effort, since this choice will minimize the augmented loss term. Overall, although this would exactly to lead to a scenario which you pointed out, it would not be worth pursuing for the reason above.\n\n[1] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015\n[2] Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learning with a wasserstein loss, 2015", "title": "discussion"}, "rJdDBE4Vx": {"type": "rebuttal", "replyto": "rksVX4X4e", "comment": "Thank you very much for your kind assessment.\n\nWe list below the responses to your major and minor comments:\n\nMajor comments:\n\nComment: The construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.\nResponse: Although the analysis in the paper may naturally lead one to believe that this is the case, we would like to note that this work was not initiated with the intention of finding an explanation for sharing the embeddings and the classifier. In fact, in our early report in [1], we did not establish such a tight analytical link between the augmented loss and reusing word embeddings although we introduced them both. We also certainly hope that our work could be primarily appreciated for the particular framework which exploits metric similarity of words which it introduces. At this point, would like to also explain our motives for eqns 3.6 and 3.7, namely the calculation of y~. Eqn 3.6 is meant to select the word vector corresponding to the target token. Eqn 3.7 is a very elementary way to capture the word similarities through inner products and then normalize the similarity vector to be interpreted as a probability distribution; this normalization was in fact used in [2] followed by a KL distance term for a different problem. From our perspective (which certainly includes some authorship bias), the fact that this simple and plausible method leads to tying (even if seemingly obviously) points to the intuitiveness of shared embeddings in language processing tasks.\n\nComment: It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters.\nResponse: This is exactly right; and in fact, we do not claim that metric used for the augmented loss should clearly be the word embeddings of the model. Our method is simply by choice, guided by observations. As a matter of fact, very early on in our work (this is when we did not have tying in mind, and were experimenting with the augmented loss idea) and inspired by the work in [3], we used both word2vec and gloVe embeddings (both 300 dimensional) as static metrics. However, the performance was significantly inferior to that when using the word embeddings of the model. In the interest of full disclosure, we should mention that we did not meticulously tune the models with static embeddings after not being able to see improvement (and we only tried them on PTB); however, the results did not seem promising. Instead, we decided to adopt the idea of information transfer within the model, and this lead to the current framework. The experimental performance of tying alone on large networks might slightly overshadow this core idea, but it was this idea that enabled us to reach tying, and link our two improvements.\n\nComment: In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done.\nResponse: Actually, not quite - although the above is not an incorrect conclusion. We will address this comment in the context of sharing embeddings. One takeaway from Section 4 is that the output projection matrix, W, will tend to the matrix used for capturing the word metric (in the subspace sense). In this case, this matrix is L'. Therefore, under the same assumptions, one would expect W to get close to L', and it could be tempting to consider using W=L' -this is the \"not quite\" part-. However, this may not be feasible from a practical perspective, because we potentially wish the classifier to be relevant to the current dataset, and not too much to an externally learned metric, for compatibility reasons -this is the \"not incorrect\" part-. You are certainly right in that in order to tie the embeddings and the classifier, one would need to use the the word embeddings of the model itself, and not an external embedding matrix. Then again, the former is what our framework is based on, and it is the setting where we can make such plausible technical arguments.\n\n\nMinor comments:\n\nComment: Third line in abstract: where model -> where the model, Second line in section 7: into space -> into the space\nResponse: Just corrected this in our internal version. We appreciate it, thank you.\nComment: Shouldn't the RHS in Eq 3.5 be \\sum \\tilde{y_{t,i}}(\\frac{\\hat{y}_t}{\\tilde{y_{t,i}}} - e_i) ?\nResponse: That is correct; in fact y~ should not have been factored out. We fixed it as a direct multiplication with e_i. Thank you very much for pointing this out.\n\nReferences:\n[1] Hakan Inan and Khashayar Khosravi. Improved learning through augmenting the loss, 2016\n[2] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015\n[3] Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learning with a wasserstein loss, 2015\n\n**This comment was slightly edited after initial posting for clarity**", "title": "Response to review"}, "H1TJ2Yb4x": {"type": "rebuttal", "replyto": "rkPvoNl4x", "comment": "Thank you very much for your kind review.\n\nWe address your major and minor comments below.\n\nResponse to the major comments:\nThe augmented loss is meant to be a light machinery on top of inner products in the euclidean space of word embeddings. Provided that we stick to the notion of word similarity for estimating a target distribution, we believe that this method (i.e. computing inner products and normalizing via softmax) is a simple and natural way to obtain a y~. Having obtained a distribution, we believe that minimizing a KL divergence term is a natural next step. We would like to also note here that this method was used by authors in [1] for obtaining an augmented loss term, although their method is proposed for a different problem. Despite being simple and not having many parameters, we believe it is an expressive term. We certainly agree that augmented loss might not be as effective on large datasets, but we argue that this is also due to large datasets having enough information to mitigate the supervision problem (we discuss this also below in response to a reviewer question). As a final remark regarding the efficacy of the augmented loss, in addition to PTB we observe non-trivial improvements with the augmented loss on Wikitext-2, which is a larger dataset (results of which we will include soon in a forthcoming revision).\n\nTo comment on different ways for introducing the augmented loss, we believe that this is a very well made point and that it is definitely worth pursuing given the improvement which we are able to achieve by investigating it in a certain direction. This would potentially be pursued in two steps: \n    (1) Decide how to find a y~. Although we believe that computing inner products and applying softmax is a natural way for exploiting the metric similarity, one can proceed in other ways. Frankly, we did not initially intend to reach tying by selecting a loss function (we did not build this link in our earlier work); however our feeling is that any method which uses euclidean distance between words could easily lead to tying. The reasoning is the following: The classification step of the language model is essentially a computation of inner products between the output hidden state and the classifier weights. Choosing classifier weights as word vectors is a compelling choice to enforce similarity since the loss function aims to distribute its predictions according to word similarity. Of course, to show this mathematically, one needs to be able to show something similar to eqn. 4.5 in our paper (i.e. L =~ W). Based on our derivation, one can possibly attain such an identity provided that the mapping from the word embeddings to y~ is differentiable and could be well represented by its first order approximation (e.g. its derivative is Lipschitz with a controllable constant). The idea, also exploited in our derivation, is to be able to obtain a linear mapping from L to W.\n    (2) Using a loss other than KL or cross entropy. This could be very interesting; in fact one can directly construct a loss function with the metric space (word embeddings) built into it (a very simple example is generalized euclidean distance). However, practically speaking, one should be careful to control the interaction between the two loss terms. For instance, depending on the phase in training one can take over and cause instabilities or null results. My collaborators and I have also been interested in exploring different options, and we would be very glad to see other works researching losses which exploit metric similarities in language processing. \n\n\nResponses to minor comments:\nComment:Can you clarify if y~ is conditioned on the t example or on the entire history.\nResponse: y~ is indeed conditioned on the history, and not just the t'th example. This is a recurring mistake in the paper, and we have made the necessary corrections in our internal version.\nComment: Eq. 3.5: i is enumerated over V (not |V|) \nResponse: Also corrected, thank you for pointing this out.\n\n\nReferences:\n[1] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015\n", "title": "Response to review"}, "SkSy6KzVx": {"type": "rebuttal", "replyto": "r10SA2ZVg", "comment": "Thank you very much for your comments.\n\nWe address your comments/questions separately below.\n\nComment: The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.\nResponse: We're pleased to be able to offer a convincing mechanism for tying the embeddings and the classifier; indeed, this framework is our main contribution. Our work investigates this new framework in three directions, leading to the following: (1) Tying is a natural outcome of applying this framework (2) For relatively small models , using an additional loss function improves learning (3) Tying alone leads to significant improvements in a broader context. Regarding the third point: Although we certainly agree that using a shared representation for inputs and outputs is not a new idea, we find it worth mentioning that the possibility of improvement by doing so was not particularly explored previously. Rather, shared representations were employed as constituent elements of the underlying models. \n\n\nComment: I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.\nResponse: We would like to thank you for raising this point previously; we have since been working on optimizing all our models (hyper-parameters) for the wikitext-2 dataset, and we have achieved satisfactory results which are in line with our predictions. Hopefully we will add the results without causing much additional reading burden for the reader. We really appreciate your feedback on this matter.\n\n\nQuestion: Have you considered using character or sub-word units in that context?\nAnswer: This is a very good question; however we predict that the framework won't provide as significant gains in character based models. Our reasoning is as follows: Our loss framework aims to exploit the metric attached to the elements in the vocabulary. In the case of characters, the vocabulary is smaller and it perhaps carries less meaning. For instance, one might expect \"terrific\" and \"great\" to be in very close proximity in English language, but it's not as reasonable to expect a particular pair of characters to be very close together, with certain exceptions. And even in the case where it might be possible (some languages might have rules with regard to character ordering), there usually isn't enough volume in the set of characters to offer very serious gains. Also, the small vocabulary size will possibly reduce the effect of tying significantly owing to losing the advantage of reducing the redundant complexity. Finally, the large effective increase in the training set by reducing to characters will potentially eliminate the advantage of using the augmented loss (since AL is most effective when model can use better estimated output statistics). Most arguments in this paragraph are also valid for sub-word units, but of course, the situation could be different with languages which contain many prefixed and suffixed words, and when the tokenization is done according to the language rules. ", "title": "Response to review"}, "B1jLbwkXg": {"type": "rebuttal", "replyto": "SJZzwp2zx", "comment": "Dear reviewer,\nThank you very much for the questions. We list below the three answers to the three questions.\n\nQ1) Have you tried your loss on a different dataset than PTB? (maybe now considered small for this task)\nA1)Yes, we did do limited experiments on a different dataset which is not reported in the paper:\nWe tested our 2-layer LSTM model with 200 units on Wikitext 2 (>2x larger dataset, >3x larger vocabulary compared to PTB), very recently published in [1]. We used the same configuration as the one reported in our paper, only further tuning the dropout weights. Below were the word level perplexities on the validation and test sets, using the same naming convention as in our paper:\n--Validation--\nVD-LSTM : 128.9\nVD-LSTM+AL:125.3\nVD-LSTM+RE:122.7\nVD-LSTM+REAL:119.6\n--Test--\nVD-LSTM : 122.7\nVD-LSTM+AL:119.8\nVD-LSTM+RE:116.1\nVD-LSTM+REAL:114.1\n\nWe would like to acknowledge that for VD-LSTM and VD-LSTM+AL we might not have tuned dropout weights perfectly, judging from a large observed gap between training and validation perplexities during training. Although we weren't able to get much better performance by tuning them further, this fact might still account for some of the perplexity gap between models that use RE (i.e. tying) and those that don't. It should, however, still be a fair comparison for VD-LSTM vs VD-LSTM+AL, and VD-LSTM+RE vs VD-LSTM+REAL.\n\n The reasons we presented our work on PTB are the following: (i) PTB is a well recognized dataset which majority of prominent work (especially the most recent RNNLM models) on word level language modeling has extensively used for benchmark. Although we agree that it is not a large dataset, its extensive utilization allows to include most of the previous work and make adequate comparisons. (ii) Keeping in mind the length limitation for a paper, we wanted to present our work in depth and in a coherent manner, while keeping it compact. These said, we would be happy to include more results if requested.\n\n\nQ2) There are many hyper-parameters (tau, beta, etc) in the proposed loss; how are they chosen (and what are the values for the reported results)?\nA2) Firstly, we agree that we were negligent with some of the details on paramtere choices, which we will improve in a future revision. Thank you very much for pointing this out.\n We have observed that setting the weight of the augmented loss (called alpha in the paper) to a constant multiple of the temperature (tau) works for all the model sizes we have experimented with, and this constant is set to 0.5 in our experiments. Temperature (tau) is set to 20. With this said, we have not observed sudden deteriorations in the performance with respect to moderate variations in tau and alpha. However, setting the the weight of the augmented loss to very large values impede learning since the embedding space is formed throughout optimization, and forcing a wrong metric space from the beginning through a large augmented loss leads to poor optimization. The augmented loss exerts its effect after a rough estimate of the embedding space is learned, and gradients from the conventional loss become smaller.\nBeta is a parameter mentioned in Section 6.2 which is explicitly used for the empirical validation experiments, and its value is swept from 0 to 1 in Figure 1(a).\n\n\nQ3) How important is tying with respect to training set size?\nA3) This is a very interesting question, and it could perhaps be best addressed with the following experiment: Randomly subsample several times from the training set and compare the resulting averaged perplexities for tied vs. non-tied cases at different sample sizes. Unfortunately, this is not within the scope of our work, so we currently can't offer a very rigorous answer to this. On the other hand, to give an answer in a broader sense, we can argue given the performance on datasets of different sizes that tying is a useful mechanism of our framework regardless of the size of the dataset. It offers gains due to (1) enforcing the metric similarity between word vectors, as investigated in our work (2) Reducing the model complexity by eliminating a large number of parameters, at the same time retaining the representational power according to our framework. To comment on the significance of tying compared to the augmented loss with respect to the training set size, we believe that tying is more significant for larger training sets. An important explicit mechanism of the augmented loss especially for small networks is to improve supervision from the dataset (as explained in Section 3), and large datasets partly mitigate the supervision problem. Tying, on the other hand, is still important because of the aforementioned two bullet points.\n\nReferences:\n[1] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.arXiv preprint arXiv:1609.07843, 2016", "title": "Response to questions"}, "SJZzwp2zx": {"type": "review", "replyto": "r1aPbsFle", "review": "- Have you tried your loss on a different dataset than PTB? (maybe now considered small for this task)\n- There are many hyper-parameters (tau, beta, etc) in the proposed loss; how are they chosen (and what are the values for the reported results)?\n- How important is tying with respect to training set size?\nThis paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.\nExperiments on PTB shows significant improvement.\nThe idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.\nI was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.\nHave you considered using character or sub-word units in that context?\n\n", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r10SA2ZVg": {"type": "review", "replyto": "r1aPbsFle", "review": "- Have you tried your loss on a different dataset than PTB? (maybe now considered small for this task)\n- There are many hyper-parameters (tau, beta, etc) in the proposed loss; how are they chosen (and what are the values for the reported results)?\n- How important is tying with respect to training set size?\nThis paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.\nExperiments on PTB shows significant improvement.\nThe idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.\nI was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.\nHave you considered using character or sub-word units in that context?\n\n", "title": "Questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByEWBgaZe": {"type": "rebuttal", "replyto": "SJp7eUn-e", "comment": "Thanks for the clarification.\n\nIt seems like they didn't include it in the journal since in their formulation it didn't help either:\n\"The direct architecture was found about 2% better than the cycling architecture.\"\n\nAs Hakan wrote below, a major point of the paper is a theoretical framework for why this makes sense and that particular formulation leads to a state of the art language model.\n", "title": "Thanks"}, "HyWbWjhZl": {"type": "rebuttal", "replyto": "SJp7eUn-e", "comment": "Hello Andriy,\n\nThank you very much for your comments. \nRegarding the cycling architecture reference [1], I agree that input and output word representations are shared in the sense that output words enter model through word embeddings. On the other hand, rather than using an \"embedding\" matrix prior to softmax, the authors instead calculate a scalar output for each target word through further non-linear mapping before softmax. This differs than the common use of the output embedding matrix, which is to construct a log-odds (or logits) vector prior to normalization via softmax. \nRegarding [2] and section 3 of [3], I agree with the interpretation of the log-bilinear model as a feedforward language model with tied input and output embeddings. As Richard also said, we shall mention it in related work. On this note, we would like to mention that our work aims to introduce tying of embedding and classifier weights not simply as an standalone architectural novelty, but rather as a natural consequence of the overarching framework, which is our main contribution. We do hope that our theory and the accompanying empirical validation will make a compelling reason for people to use tied embeddings in various NLP tasks as well as provide people potentially familiar with the notion with an intuitive mechanism as to why it could be good idea to do it.", "title": "tying embeddings"}, "SJp7eUn-e": {"type": "rebuttal", "replyto": "HyU-Z43Zl", "comment": "Hi Richard,\n\nThanks for your response.\n\nThe listing of model parameters you are quoting is for the \"direct architecture\" which, unlike the \"cycling architecture\" I was referring to, indeed uses different embedding matrices. The text you quoted is from the JMLR version of the paper that contains only the direct architecture. The cycling architecture is described in the (earlier) NIPS version of the paper, which you can find here: https://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf\n\nAs for reference [2], in addition to the RBM language models, it also contains the first formulation of the log-bilinear model, which, though described in terms of energies in the paper, is really a feedforward model with a softmax output. You can find its feedforward formulation in Section 3 of [3]. After that paper came out, I noticed that LBL performed better on APNews if I untied the input and target word embeddings, resulting in the form of the model you are probably more familiar with.\n\nSo to summarize, both Bengio's \"cycling architecture\" and the original formulation of the log-bilinear model are feedforward neural language models that use the same embedding matrix for the input and target words.\n\n[3] Mnih, A., & Hinton, G., A Scalable Hierarchical Distributed Language Model. NIPS 2008", "title": "Feedforward language models with tied input and target word embedding matrices"}, "HyU-Z43Zl": {"type": "rebuttal", "replyto": "BysAsCKWe", "comment": "Hi Andriy,\n\nThanks for your comment.\nYou are right that in your RBM model there wasn't really a discriminative softmax classifier and those models can be seen as tying all things word-related into one vector. We will add it to related work.\n\nI'm not sure there's any prior feedforward model that does this though. In the feedforward model of [1] it says:\n\nThe free parameters of the model are \nthe output biases b (with |V| elements), \nthe hidden layer biases d (with h elements), \n* the hidden-to-output weights U (a |V| \u00d7h matrix), \nthe word features to output weights W (a |V| \u00d7 (n \u2212 1)m matrix), \nthe hidden layer weights H (a h \u00d7 (n \u2212 1)m matrix), and \n* the word features C (a |V| \u00d7m matrix).\n\nI think they are listed as separate parameters here, no?", "title": "Tying word vectors and softmax"}, "BysAsCKWe": {"type": "rebuttal", "replyto": "r1aPbsFle", "comment": "I just wanted to point out that is used to be common to use the same embedding matrix for the input and target words in a neural language model. For example, the \"cycling architecture\" proposed in Yoshua Bengio's first language modelling paper [1] does that, as do all three models proposed in [2].\n\n[1] Bengio, Y., Ducharme, R., Vincent, P. and Jauvin, C. A Neural Probabilistic Language Model. NIPS 2000\n[2] Mnih, A., & Hinton, G. Three new graphical models for statistical language modelling. ICML 2007", "title": "Tying the input and output word representations used to be common"}}}