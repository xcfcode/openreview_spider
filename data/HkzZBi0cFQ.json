{"paper": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "summary": "", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper proposes an 8-bit quantization strategy for rapid DNN deployment. 3 reviewers all rated this paper as marginally below acceptance threshold due to lack of novelty. 8 bit quantization (including channel-wise) is a well studied task. The paper lacks comparison with peer work. "}, "review": {"rJlAQp8Zk4": {"type": "rebuttal", "replyto": "S1gnBtlFRX", "comment": "Thank you very much for the valuable comments. We will improve the manuscript in the final version by adding further analysis and results as you suggested.", "title": "Thank you very much for the valuable comments."}, "B1eFz3plRX": {"type": "rebuttal", "replyto": "HkltNd_637", "comment": "Thank you very much for your comments. Competing methods in other papers require retraining or needs to cope with high accuracy loss when quantized in a layer-wise fashion. The proposed method is the first of its kind to resolve these issues by incorporating channel-wise quantization and moment-analysis method which DOES NOT require retraining or the training dataset. Na\u00efve channel-wise quantization requires adding huge number of HW shifters and providing values for them which make it unrealistic for implementation (please see Figure 1 (b) in the revised manuscript). The biggest contribution of our paper is the HW-friendly channel-wise quantization by manipulating the kernels prior to inference. For your reference, Figure 1 has been modified to make the distinction clearer.", "title": "Response to Reviewer"}, "r1xlSn6l0X": {"type": "rebuttal", "replyto": "ryebGUDO2X", "comment": "Thank you very much for your helpful comments and suggestions. \n1. Notations have been added to Algorithm 1 and modified Figure 1 along with its description, as suggested.\n2. The channel-wise quantization algorithm is calculated statically as show in Algorithm 1 and exemplified in Figure 1. Algorithm 1 demonstrates all aspects of \u201cpre-coordinating the fractional lengths of the weights.\u201d Therefore, should be able to be reproduced by anyone and didn\u2019t think it would be necessary to write it out in words. \n3. The layer-wise moment-analysis results have been added to the paper in Table 1 and 2. Channel-wise out-performs layer-wise in most cases without the need for additional HW. The overhead introduced in channel-wise quantization is just a simple pre-processing step of determining the fraction lengths by executing Algorithm 1.\nWe would like to emphasize that our paper proposes a practical solution for channel-wise quantization. Na\u00efve channel-wise quantization requires adding huge number of HW shifters and providing values for them which make it unrealistic for implementation (please see Figure 1 in the revised manuscript). The proposed method does not require such cost by manipulating the kernels prior to inference. For your reference, Figure 1 has been modified to make the distinction clearer.\n", "title": "Response to Reviewer"}, "HygNgn6eRQ": {"type": "rebuttal", "replyto": "Hklvh2J_6m", "comment": "Thank you very much for your helpful comments. We have addressed them as follows:\n\n1. Results of layer-wise quantization with moment-analysis method were added\n: The layer-wise quantization with moment-analysis method could remove the effect of outliers. To show this clearly, we added the results of applying moment-analysis method to layer-wise quantization in Table 1 and 2 in the revised manuscript. However, even if outliers were to be removed, channel-wise method consistently outperforms layer-wise one. Channel-wise method in comparison to the layer-wise method both with moment-analysis (or MAX) clearly demonstrates that channel-wise has the advantage.\n\n2. Novelty of the paper\n: The main contribution of the paper is manipulation of the weights prior to inference for channel-wise quantization as shown in Figure 1 and Algorithm 1. Na\u00efve channel-wise implementation requires shifters to be in place for each channel which would cause huge hardware cost (please see Figure 1 (b) in the revised manuscript). Thus, it is not a practical solution at all in HW perspective. The proposed method performs channel-wise quantization WITHOUT the need for any additional HW and we believe there\u2019s novelty in that it enables channel-wise quantization for real-world HW. We modified Figure 1 and improved manuscript to reflect this point clearly. \n", "title": "Response to Reviewer"}, "Hklvh2J_6m": {"type": "review", "replyto": "HkzZBi0cFQ", "review": "The paper proposes channel-wise 8-bit quantization rather than layer-wise. It further takes advantage of work using moment analysis instead of just MAX values to avoid susceptibility to outliers. The main take-away seems to be that channel-wise set ups limit the need for outlier removal and the care with which you select your data subset when performing quantization.\n\nPros:\n- using channel-wise quantization (with MAX values or moment-analysis) yields improvement over layer-wise MAX approaches\n- limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)\n- shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channel-wise quantization\n\nCons:\n- unclear how much is gained over layer-wise and MAX value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channel-wise methods are the clear winner\n- unclear if the layer-wise set up with moment-analysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channel-wise set up; a few more experiments are important to determine specifically if improvement is with respect to channel-wise or moment-analysis since only layer-wise MAX results are presented\n- clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability\n\nOverall:\nThe paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channel-wise quantization (and moment-analysis), but the overall novelty is limited. With the limited novelty, the primary benefits appear to be the ease of quantization for rapid deployment and channel-wise setups. Comparisons with stronger baseline numbers when using layer-wise methods would give a more complete picture. In addition, having these stronger tuned baseline numbers on even more networks would be great to show that the channel-wise method has clear impact across the board, even with respect to well-tuned layer-wise baselines. These results could give better support for the importance of the novelty.", "title": "promising method for 8-bit quantization without sensitivity to outliers, limited in novelty and presentation clarity", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkltNd_637": {"type": "review", "replyto": "HkzZBi0cFQ", "review": "This paper proposes an new 8-bit quantization strategy for rapid deployment. \n\n8-bit quantization has attracted many attentions recently. And it is already well used in GPU servers (cudnn), phones, ARM chips and various ASIC neural network chips. In these situations, almost no performance drop is observed for classification and detection tasks.\n\nSo, the novelty of this paper is limited.", "title": "Novelty is limited", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryebGUDO2X": {"type": "review", "replyto": "HkzZBi0cFQ", "review": "This paper proposes a technique for channel-wise quantization of CNNs\nto 8-bit, fixed point precision. The authors propose several\ntechniques for analyzing the statistical properties of output channel\nactivations in order to select the best fractional bit length for each\nchannel. Experimental results on eleven different CNN architectures\ndemonstrate that the approaches proposed result in significantly less\naccuracy loss when compared to a layer-wise baseline.\n\nThe paper has the following strengths:\n\n 1. The experimental results on eleven different architectures (of\n    varying depth and breadth) are convincing, and are consistently\n    better than layer-wise MAX for choosing fractional bit length.\n\nThe paper has the following weak points:\n\n 1. There is not much coherence between the description of the\n    approach in section 2.1, Figure 1, Algorithm 1, and\n    Figure 2. Notation is used in Algorithm 1 which is never defined.\n 2. Related to the previous point, the proposed technique has a lot of\n    moving parts and I don't feel that it would be easy to reproduce\n    the results of the paper. There are some vague statements, like\n    \"We resolve this complication by pre-coordinating the fractional\n    lengths of the weights\", which require significantly more\n    precision. This issue -- one of the main issues with channel-wise\n    versus layer-wise quantization -- is never returned to in the\n    definition of the method.\n 3. The experimental comparison with layer-wise quantization is\n    somewhat lacking. Is layer-wise MAX the state-of-the-art in CNN\n    quantization? The results comparing channel-wise and layer-wise\n    MAX are already convincing, but are the moment-analysis approaches\n    not equally applicable to layer-wise quantization?\n    State-of-the-art results that are less sensitive to outliers\n    should be included in Table 1. A comparison with layer-wise\n    approaches would be nice to have also in Figure 4 to show\n    sensitivity to profiling set size.\n\nThe experimental results in the paper are impressive, and the analysis\nmotivating the approach is convincing. However, there are presentation\nand clarity issues in the technical development, and the comparative\nanalysis is lacking broader comparisons with the state-of-the-art (to\nbe fair, the authors recognize that layer-wise MAX as a baseline is\nparticularly susceptible to outliers). These two aspects combined,\nhowever, lead me to the opinion that this work is just not quite ready\nfor publication at ICLR.\n", "title": "Interesting approach to channel-wise CNN quantization with adaptive bit allocation, with evaluation on eleven modern CNNs, comparison with simple layer-wise baseline", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}