{"paper": {"title": "GRF: Learning a General Radiance Field for 3D Scene Representation and Rendering", "authors": ["Alex Trevithick", "Bo Yang"], "authorids": ["amt6@williams.edu", "~Bo_Yang7"], "summary": "An implicit neural function to represent and render complex 3D scenes only from 2D views.", "abstract": "We present a simple yet powerful implicit neural function that can represent and render arbitrarily complex 3D scenes in a single network only from 2D observations. The function models 3D scenes as a general radiance field, which takes a set of 2D images with camera poses and intrinsics as input, constructs an internal representation for each 3D point of the scene, and renders the corresponding appearance and geometry of any 3D point viewing from an arbitrary angle. The key to our approach is to explicitly integrate the principle of multi-view geometry to obtain the internal representations from observed 2D views, such that the learned implicit representations empirically remain multi-view consistent. In addition, we introduce an effective neural module to learn general features for each pixel in 2D images, allowing the constructed internal 3D representations to be general as well. Extensive experiments demonstrate the superiority of our approach.", "keywords": ["3D scene representation", "novel view synthesis", "neural rendering"]}, "meta": {"decision": "Reject", "comment": "The paper presents an extension of recent implicit representations for view synthesis, such as NeRF. The presented formulation accepts an image set as input at test time, and can thus in principle be applied to new scenes. The idea is sound, but reviewers had concerns with the presentation and the experimental results. The work is primarily evaluated on the simplistic ShapeNet domain, which a number of reviewers found unconvincing. Concerns remain even after the authors' responses, and the AC agrees that the work can benefit from further investment before it is published."}, "review": {"M3pPPfQC6N": {"type": "rebuttal", "replyto": "20n5tfLbgbT", "comment": "**Concern 1**\\\nThanks you for your detailed replies. However, I still think some of my concerns are not well addressed. The main reason is still lack or more experimental results to support the argument made in this ... ... and the writing about methodology still needs to be improved. \n\nFor example, it is still quite confusing for me to understand the result and setting of group 2 on novel-scene generalization. How is NeRF trained for those novel scenes? Is a single nerf trained to fit all the novel scenes or several nerf are trained to fit separate scenes?  \n\n**Response**\\\nThank you for the comments. In our revised paper, extensive experimental results (quantitative and qualitative) are provided to clearly support, 1) the novelty and contribution of our GRF, 2) the advantage of our GRF over SRNs, 3) the advantage of our GRF over original NeRF, and 4) the effectiveness of the CNN module and attention module. All arguments in the entire paper are well presented and validated.\n\nAs to the experiment Group 2, to quote our sentences directly (sec 4.2, page 7 and 8), \"For comparison, we also train NeRF on each of the four novel scenes with 100, 1k, 10k iterations from scratch.\" Note that, NeRF can only be trained on a single scene at a time. \n\n**Concern 2**\\\nIf multiple models are trained, the results seem to be a little bit lower comparing to the results reported in the original NeRF paper. Also, it is not very convincing to show that GRF is better in generalization than NeRF when NeRF is not converging. \n\n**Response**\\\nFundamentally, as we clearly specify in the paper, the experiments in Group 2 are to validate that our GRF can learn faithful geometries extremely quickly given the same number of supervision signals, thanks to the generality of the features learned from the other 4 scenes. To the best of our knowledge, there is no method that can generate faithful novel views of out-of-distribution objects from high-resolution images (in a single forward pass or just a few iterations of training) as our GRF can here.\n\nThe reviewer may miss this key point of our experiments, thereby saying that \"... not convincing when NeRF is not converging\". This is irrelevant to our experiments. \n\n\n**Concern 3**\\\nOn the other hand, it would be great to include one baseline like what did in the GRAF paper, like using some other kind of conditional input or like SRN using a hyper-network to predict the parameter for different NeRF network.  \n\n**Response**\\\nAs we clearly responded earlier and discussed in the revised paper, the section 4.4, page 9, our ablated model of \"Remove Pixel Local Features\" is a \"conditional embedding + NeRF network\", where the global features for each object in cars are learned. This ablation performs SEVEN psnr worse, fundamentally because such feature embeddings can never encode the rich and precise pixel-level local features learned by GRF for such a huge number of different scenes.\n\nTo sum up, we thank the reviewer again for this repeated comment, but it has already been fulfilled in our manuscript.\n\n\n**Concern 4**\\\nOne other comment is still about the implicit function term. ... .... function and they make it every clear that they have learnt a continuous volumetric function which is not implicit at all comparing to a sign-distance field or ........ definition of implicit function.\n\n**Response**\\\nThere are always technique terms that are widely used in a specific area, while they may not have exactly the same meaning from the perspective of different researchers.\n\nAs the reviewer points out, NeRF learns a continuous volumetric function, while DeepSDF (CVPR'19) learns a continuous signed distance function. The inputs of both methods are the location of 3D query point (i.e, xyz), except that they have different meanings for the outputs. Basically, both can be called implicit functions, primarily because the word IMPLICIT here is widely used to differentiate from the EXPLICIT 3D functions such as voxel grids, point clouds, and meshes. This may be a bit confusing for the audience out of the field of 3D vision and learning. The cited wiki page is interesting, but the pure mathematical definition looks a bit far from the domain of 3D scene representation. \n\nHowever, to avoid possible confusion, we are happy to replace the term \"implicit function\" with \"neural function for implicit representation\", if it is suggested to do so.\n\n**Final**\n\nOverall, we truly appreciate all the efforts and feedback from the reviewer, and we enjoy a lot in the discussion. However, it seems that the expertise of the reviewer may not exactly fall in the area of our manuscript, according to the interesting and surprising comments. Also, the lower confidence score reported by the reviewer seems consistent with this interpretation.\n\nWe hope our responses are helpful for the audience, peer reviewers, and area chairs to have further discussion about the contribution of our manuscript to the community. ", "title": "We highly appreciate but strongly disagree with the comments"}, "7a5hBLcLg2G": {"type": "rebuttal", "replyto": "yN0jjbpIF-J", "comment": "We thank the reviewer for the support of our paper, and hope your valuable comments can be helpful for the audience and peer reviewers to accurately evaluate our contributions to the community.", "title": "Response to Reviewer's Advocation"}, "mcBOcIYzf9r": {"type": "rebuttal", "replyto": "7H72zZ-0GC7", "comment": "Dear R#1:\n\nWe thank you for careful reading through all our responses and the revised paper. It is also great to receive your comments back again. We would like to take this opportunity to further discuss your remaining concerns and clarify the novelty/contribution of our paper.\n\n**1**\\\nRemaining Concerns. We agree that your initial requirements are not all fulfilled, especially the possible experiments on the large-scale 3D scene dataset \"Tanks and Temples\". Yet, we fully discussed the reasons in the paper why our GRF is somehow limited to achieve such super exciting technical maturity. Moreover, we have provided compelling evidence for the generality of our approach in the revised paper. Apart from that, we would also like to kindly ask what are the remaining concerns regarding possible inaccurate arguments in the paper. We are definitely happy to further improve it accordingly, and we believe this is achievable considering that there is enough time to incorporate them into the final version.\n\n**2**\\\nOur Novelty/Contribution. To the best of our knowledge, following the great success of NeRF, there are two papers successfully published very recently, NSVF (NeurIPS'20 spotlight) and GRAF (NeurIPS'20). NSVF uses sparse voxels, along with the original continuous 3D points, to represent 3D scenes, significantly improving the querying speed and efficiency. GRAF leverages the powerful rendering capability of NeRF to build a great \n GENERATIVE model which is able to maintain the geometry. However, neither NVSF nor GRAF is extending NeRF to learn a GENERAL model that can learn common features from multiple scenes and then generalize to new scenes. \n\n\n In stark contrast, being also built on the successful volume rendering of NeRF, our GRF is a intuitively simple yet valid method to discover the general and shareable pixel-level patterns for multi-scene/novel-scene representation and reconstruction. This is key to the future of machine intelligence. Our code will be made available on GitHub for reproducing all experimental results.\n We believe this would advance the field of 3D scene understanding and have true impact for the community. \n\n\nIt might be helpful to note that the GENERATIVE model (GRAF) is completely different from the GENERAL model (GRF) in terms of the final objective and the design principle. To avoid the confusion of names, we are open to change a new name if it is suggested to do so.\n \nOverall, we highly appreciate all efforts of the reviewer to help us improve the manuscript. It would be great if the reviewer could reconsider the rating based on our latest updates.", "title": "Response to Reviewer's Comments"}, "7H72zZ-0GC7": {"type": "review", "replyto": "cAvgPMAA3hb", "review": "This paper presents a new neural learner for learning generic scene radiance function. Compared with existing methods such as NeRf which are scene-specific requiring per scene based training, the proposed new method is potentially able to generalize to novel unseen scenes or objects.   Specifically, the method represents scene as a neural radiance filed in terms of spatial coordinates of camera centre and a query point. An attention model is used for aggregating back-projected image features at every query location, and the aggregated feature vectors are then used for predicting its RGB-alpha radiance.  \n\nThe work is well motivated, aiming to relax the restriction of existing per-scene based radiance field learning methods  (NeRF, for example).  Experiments show some improved performance in novel view/radiance field synthesis , however I do no f find there are convincing tests provided or conducted to validate the \"better generalizability\" claim.   Overall, the current results are insufficient to validate the method's generality to novel scenes .More comments are given below:\n\nPros:  \n+ The work is well motivated.  \n+ The idea of using attention model to address multi-view consistency issue is interesting,  which appears to be sound and promising.  \n+ extensive experimental study, lots of ablation studies and comparisons. \n\nCons: \n-One key intention of this paper is to improve the network's generalibility to unseen, novel scenes.   However, throughout the reported experiments,   I do not find  adequate experimental evidences to  support this claim.    From the paper it seems the proposed method has only been tested on some unseen scenes in the ShapeNet dataset, however with very similarly-looking objects in simple and similar poses.    What are the results on the other two datasets, and what are the training-testing split on unfamiliar scenes?  Since the major motivation of this paper is to handle complex novel unseen scenes, I would suggest the authors conduct experiments on outdoor scenarios, such as the 'Tanks' and 'Temples' dataset commonly used in recent related work on neural novel view scene rendering.\n\n- It is mentioned that two different attention models (AttSets and Slot Attention) are used depending on training dataset. However it is unclear which model was used for testing which datasets.  How did you compare in PSNR/SSIM metrics on each dataset?   Also, there is no description of how these attention mechanisms help the process.  In particular, since 3D point visibility of source view features under target view is one of the key problems for any novel view synthesis work.  How the employed attention module solves this issue is unclear.  In short,  I did not see how this attention module handles the visibility problem, other than (as the authors said) it helps to \" aggregate multiple source view features\"-- which seems to me to be an obvious outcome. \n- The authors  also claimed that the attention module is invariant to the input view number and orders;  Yet,  there is not any  evidence (either theoretic analysis,  or experimental) provided in the paper.   Is it an inherent property of the Attention modules ?  \n\n- From reading Table-1,  it seems SRNs sometimes outperforms GRF.   The advantage of the proposed GRF is not entirely clear.  \n\n- overall, a well motivated paper, but the execution of the ideas is not convincing. \n", "title": "A generic scene radiance field neural estimator with better generalization ability than NeRF.   Well motivated idea but poorly executed. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "lqMrVcEZ8sh": {"type": "rebuttal", "replyto": "Qk0GBDPv5K", "comment": "**Concern 1**\\\nThe term of implicit representation is interchangeably used to\ndescribe implicit function and explicit 3D representation parameterized\nby MLPs.\\\n**Response**: \\\nWe use implicit function to describe the network architecture itself,\nwhile implicit representation to describe the learned features in\ngeneral. More information about these terms may be found in the paper\n\"Local Deep Implicit Functions for 3D Shape, CVPR'20\". In the revised\npaper, we have carefully checked the consistency of the terms.\n------------------------------------------------------------------------\n**Concern 2**\\\nComparison between GRF and GRAF: In GRAF, the radiance field is also\nconditioned on a shape code as well as an appearance code. But in the\nrelated work section, the author states that GRAF is unable to\ngeneralize to novel scenarios which seems to be an unfair claim.\\\n**Response**:\\\nThanks for the advice. We have rephrased the sentences accordingly.\n------------------------------------------------------------------------\n**Concern 3**\\\nMissing details about volumetric rendering: The paper did not talk\nabout how rendering is performed(possibly volumetric rendering). And\nboth in NeRF and NSVF, sampling strategy and volumetric rendering both\nplay important roles on achieving high-fidelity rendering results. It is\nunclear here how ray marching is formulated.\\\n**Response**:\\\nIn the revised paper, we have updated the section 3.5, clearly\nspecifying that we strictly follow NeRF for the sampling strategy and\nvolumetric rendering.\n------------------------------------------------------------------------\n**Concern 4**\\\n Experiments on ShapeNetV2: the author just reported SRN's result\nhere, but didn't compare with some other method like NVS which is also\ndirectly applicable. Besides, other methods like NeRF could be modified\nto train on multiple objects like including a conditional embedding to\nNeRF jsut like GRAF or like in SRN use hyper-networks. Lack results here\ncould be potentially undermining the claim of GRF being more general and\nrobust.\\\n**Response**:\\\nThese are helpful points. On the ShapeNetV2 (cars and chairs) dataset,\nSRNs is currently the state-of-the-art approach for novel view\ngeneration. To the best of our knowledge, there is no other NVS (novel\nview synthesis) method reporting results on this particular dataset.\nAlthough some methods may be trained from scratch on this dataset, it is\nnon-trivial to conduct the experiments considering the extremely\nexpensive computation.\n\nIt is insightful to point out that the conditional embedding to NeRF can\nbe used for comparison. In fact, in our ablation study, we analyze that\nthe removal of Point Local Features is a baseline of \\\"Conditional\nembedding + NeRF\\\". In the revised paper, we update the section 4.4\naccordingly.\n------------------------------------------------------------------------\n**Concern 5**\\\n Experiments on real-world complex scenes: The training setup here is\na bit unclear to me. From just the description there, it is hard for me\nto tell whether GRF is training one model on all those scenes or\ntraining separate models for different scenes. It would be great if the\nauthor could make that clear.\\\n**Response**:\\\nWe thank the reviewer for pointing out this ambiguity. We have changed\nthe language to make it explicit where single-scene learning is\nhappening, and where multi-scene learning is happening.", "title": "Response to Reviewer 2"}, "KFNtGeN4M4": {"type": "rebuttal", "replyto": "7H72zZ-0GC7", "comment": "**Concern 1**\\\nOne key intention of this paper is to improve the network's\ngeneralibility to unseen, novel scenes. However, throughout the reported\nexperiments, I do not find adequate experimental evidences to support\nthis claim. From the paper it seems the proposed method has only been\ntested on some unseen scenes in the ShapeNet dataset, however with very\nsimilarly-looking objects in simple and similar poses.\n\nWhat are the results on the other two datasets, and what are the\ntraining-testing split on unfamiliar scenes?\n\nSince the major motivation of this paper is to handle complex novel\nunseen scenes, I would suggest the authors conduct experiments on\noutdoor scenarios, such as the 'Tanks' and 'Temples' dataset commonly\nused in recent related work on neural novel view scene rendering.\n\n**Response**:\\\nThese points are highly valuable for our paper. As requested, we\nconducted additional experiments on the Synthetic-NeRF dataset to\nthoroughly evaluate the generalization capability of our GRF across\nnovel scenes.\n\nIn particular, we train a single GRF model on four different scenes, and\nthen directly test it on the remaining completely different four scenes.\nThe results show that our GRF can indeed generalize well across novel\nscenes.\n\nIn addition, we conduct extra experiments to demonstrate that the\nlearned GRF can greatly benefit the single-scene learning, significantly\nbetter than the NeRF trained from scratch given a sparse amount of\ntraining signals. All the details are presented in Section 4.2, page 7\nand 8.\n\nAs to the suggested large-scale 3D scenes (Tanks and Temples), it would\nbe interesting to explore. However, it is non-trivial to directly\nevaluate our GRF on it, because of the complexity of geometry and the\nexpensive computation required. In the revised paper, this is fully\ndiscussed in section 4.3, the last paragraph of page 8, and we leave it\nfor future exploration.\n\n------------------------------------------------------------------------\n**Concern 2**\\\nIt is mentioned that two different attention models (AttSets and Slot\nAttention) are used depending on training dataset. However it is unclear\nwhich model was used for testing which datasets. How did you compare in\nPSNR/SSIM metrics on each dataset?\\\n**Response**:\\\nAs noted in the appendix, we use AttSets for the ShapeNet dataset, and\nSlot Attention for the synthetic/real-world datasets. We use AttSets in\nthe former because it can quickly aggregate information from multiple\nviews, allowing the rendering of the millions of images needed for the\nShapeNet benchmark. We also empirically find that the models for\nsynthetic/real-world datasets converge quicker with Slot Attention.\nComparison of the PSNR/SSIM metrics all strictly follows the baselines\nsuch as NeRF.\n------------------------------------------------------------------------\n**Concern 3**\\\nAlso, there is no description of how these attention mechanisms help the\nprocess. In particular, since 3D point visibility of source view\nfeatures under target view is one of the key problems for any novel view\nsynthesis work. How the employed attention module solves this issue is\nunclear. In short, I did not see how this attention module handles the\nvisibility problem, other than (as the authors said) it helps to \\\"\naggregate multiple source view features\\\"-- which seems to me to be an\nobvious outcome.\n\n**Response**:\\\nThese are insightful questions. To thoroughly investigate how the\nattention module aids the network to deal with the visual occlusion, we\nconduct additional experiments and analyze in depth in the appendix\n(A.4). In particular, we track the largest attention score of the pixel\npatch and visualize the maximal attention score map for every pixel of a\nrendered image. Our results show that the attention module can indeed\ndeal with the visual occlusion as it is expected.\n------------------------------------------------------------------------\n**Concern 4**\\\nThe authors also claimed that the attention module is invariant to the\ninput view number and orders; Yet, there is not any evidence (either\ntheoretic analysis, or experimental) provided in the paper. Is it an\ninherent property of the Attention modules ?\n\n**Response**:\\\nThe property of being invariant to the input views is theoretically\nanalyzed in the papers AttSets and Slot-Att. To avoid the confusion, we\nrephrased the sentence in the revised paper.\n------------------------------------------------------------------------\n**Concern 5**\\\nFrom reading Table-1, it seems SRNs sometimes outperforms GRF. The\nadvantage of the proposed GRF is not entirely clear.\n\n**Response**:\\\nIn the revised paper, we have added additional results in section 4.1,\npage 6 and 7. We show that SRNs cannot generalize at all without\nretraining on novel scenes, while our GRF can.\n", "title": "Response to Reviewer 1"}, "cRvHunFUQhi": {"type": "rebuttal", "replyto": "DfG_ZIgUWat", "comment": "**Concern 1**\\\n\\\nAs depicted, the aggregator that assembles the 2D features of each\n3D point from multiple views handles the visual occlusion implicitly via\nan attention process without depth scans. How does it work on unseen\nscenes? The network may have no knowledge about the structure of the\nnovel scenes. Are there any failure examples of such cases?\\\n\\\n**Response**:\\\nThis is a fundamental question. Given sparse views of an unseen scene,\nthe CNN module of our trained GRF can extract hierarchical pixel\nfeatures including the pixel local patterns and the possible larger\nshape information. These features are usually believed to be common and\nshared across different objects and scenes. This allows our model to\nhave a certain level of generalization capability across unseen scenes.\n\nBasically, the attention mechanism is designed to select the most\nimportant pixel features among many for rendering a novel pixel. This\nmodule is likely to assign higher attention scores to visually similar\nand visible pixel patches.\n\nIn order to evaluate the effectiveness of the attention module, we\nconduct an experiment to visualize and analyze the learned attention\nscores on novel objects. Details are presented in the appendix (A.4). It\nshows that the attention module can truly drive the network to focus on\nthe visible pixel local features among the multiple intersected light\nrays. This result is also added into the section Ablation Study.\n------------------------------------------------------------------------\n**Concern 2**\\\nRendering a query 3D point p requires the (r_p, g_p, b_p) and the\ndensity d_p. As described in Eq. (6), the color channels are estimated\nthrough MLPs with learned feature $\\bar{F}_p$ and the query viewpoint\n$\\mathcal{V}_p$ as input. Nonetheless, the density function in Eq. (5)\ndoes not require the query viewpoint as input. I think the density of 3D\npoints for volume rendering should be dependent on the viewing ray as\nwell. How to explain the difference between Eq. (5) and Eq. (6)?\n\n**Response**:\\\nHere is the clarification. The volumetric density $d_p$ is modeled as a\nfunction of the point features $\\mathbf{\\bar{F}}_p$ only, because this\nallows the estimated density $d_p$ at the query point $p$ to remain\nunchanged under different query viewpoints, thus maintaining consistent\ngeometry for each point. We update the paper accordingly in section 3.5,\nthe last paragraph of page 5.\n------------------------------------------------------------------------\n**Concern 3**\\\nIn section 4.1, the performance of SRN is better than the proposed\nmethods in almost all situations. The authors give an explanation that\nSRN requires to be retrained on novel scenes to optimize the latent\ncode. I suggest that the authors should evaluate the performance of SRN\non novel scenes without retraining process to validate the argument.\\\n\n**Response**:\\\nThanks for the valuable suggestion. In the revised paper, we conducted\nadditional experiments as suggested in section 4.1, the last paragraph\nof page 6. The qualitative results are presented in Figure 4, clearly\ndemonstrating the advantage of our GRF over SRNs when directly being\ntested on novel objects.\n------------------------------------------------------------------------\n**Concern 4**\\\nThe evaluation of generalization for the novel scene of the proposed\nmodel is still not convincing enough, which needs improvements.\n\nThe experiments only conduct on the novel scene of the same category\nwhich share the similar features. I consider that SRN can also handle\nthis situation by the learned latent code and hypernetwork. The authors\nare suggested to train the proposed GRF on a large amount of data with\ndifferent categories of objects to learn the general feature for\nattention mechanism, and evaluate the model on the new object to\nvalidate the generalization.\n\n**Response**:\\\nWe agree with the reviewer that our GRF can be trained on a large amount\nof objects belonging to different categories. However, we empirically\nfind that such experiments would take up to a few months to finish on a\nmodern GPU. Alternatively, we conduct additional experiments on the\nSynthetic-NeRF dataset to evaluate the generalization across novel\nscenes. In particular, our GRF is trained on four different scenes, and\nthen directly tested on the remaining completely different four scenes.\nThe results show that our GRF can indeed generalize well across novel\nscenes. The details are presented in the revised paper, the section 4.2\npage 7 and 8.\n------------------------------------------------------------------------", "title": "Response to Reviewer 4"}, "JRVc8laZzV": {"type": "rebuttal", "replyto": "XtQL1ZtCHx", "comment": "**Concern 1**\\\nNotational inconsistency and clarification relating to 'viewpoint' and 'posed images'\\\n\\\n**Response**:\\\nWe thank the reviewer for pointing out the inconsistency of \"viewpoint\"\nand the symbols. In the revised paper, we made the following\nmodifications: 1) The \"viewpoint\" is clearly defined as the camera\nlocation xyz. 2) All \" posed 2D images\" are replaced by \"2D images with\ncamera poses and intrinsics\". 3) Near Figure 2, the discussion about\nviewpoint is updated with clear specifications. 4) Equation 1, Figure 1\nand the symbols are all updated with clear definition. The whole paper\nnow has consistent meanings for the viewpoint.\n\n\n------------------------------------------------------------------------\n\n**Concern 2**\\\nIn the motivation of this paper, there is special emphasis on\nrepresenting \\\"scenes\\\", yet the experiments are on \\\"single objects\\\",\njust as other methods are criticized for being limited to. Can the\nproposed method be used to represented a full indoor room (not just a\n360\u00b0 video; but view from anywhere in any direction)?\n\n**Response**:\\\nThis is a good point. In principle, our GRF formulates the 3D structure\nvia per 3D query point, which is agnostic to the complexity of scenes.\nThe success on both Synthetic-NeRF and the real-world datasets also\ndemonstrates that our GRF can indeed represent a certain level of\ncomplex 3D scenes. However, it is still challenging to directly evaluate\non large-scale 3D scenes such as full indoor rooms or extreme large\noutdoor space. In order to avoid the confusion, we made the following\nchanges in the revised paper: 1) In Introduction, the first paragraph of\npage 2, we specify \\\"the complex 3D scenes\\\" as \\\"multiple objects with\ncluttered background\\\". 2) In Section 4.3, the last paragraph of page 8,\nwe analyse the difficulties of recovering large-scale 3D scenes and it\nis left for future exploration.\n\n------------------------------------------------------------------------\n\n**Concern 3**\\\nI would also like to get a sense of how stable / gracefully degrading\nthis method is to the K input images. For example, if I only input\nimages of the front of an object, what will happen when viewing the\nback? What if most of the images are from one direction, will this bias\naffect views elsewhere?\\\n\\\n**Response**:\\\nThis is an interesting question. We conduct an experiment accordingly\nand the results are presented in the appendix (A.5). 1) In the extreme\ncase, i.e., 1-view reconstruction, our GRF is still able to recover the\ngeneral 3D shape of the unseen object, including the visually occluded\nparts, primarily because our CNN model learns the hierarchical features\nincluding the high-level shapes. 2) Given more input views, the\noriginally occluded parts tend to be observed from some viewing angles,\nand then these parts can be reconstructed better and better. This shows\nthat our GRF is indeed able to effectively identify the corresponding\nuseful pixel features for more accurately recovering shape and\nappearance.\n\n------------------------------------------------------------------------\n\n**Concern 4**\\\nDoes the method generalize to scenarios with $\\neq$ K input images?\\\n\\\n**Response**:\\\nSince the used attention modules (AttSets or Slot-Att) are able to\naggregate an arbitrary number of feature vectors, our GRF can naturally\ntake any number of input images without needing the zero vector trick.\nWe have conducted four groups of experiments for K = (1,2,5,10) in\ntesting, where the model is trained with 5 images per object. Details\nare shown in the appendix (A.5). The results show that with less input\nimages, the quality of shapes indeed degrades, especially the visually\noccluded parts.\n\n------------------------------------------------------------------------\n\n**Concern 5**\\\nIt is not really fair to write that \\\"mose methods \\... require ground\ntruth 3D geometry for supervision\\\". NERF/IDR/SIREN and when considering\nnoisy point clouds SAL/SAL++ do not need ground truth 3D geometry.\\\n\\\n**Response**:\\\nWe thank the reviewer for pointing out the related work. In the revised\npaper, we rephrased the sentence and briefly discussed SAL/SAL++ in the\nsection Related Work.\n\n------------------------------------------------------------------------\n**Concern 6**\\\nThe paper writes \\\"This simple design of GRF follows the principle of\nclassic multi-view geometry (Hartley & Zisserman, 2004), therefore\nguaranteeing the learned implicit representations meaningful and\nmulti-view consistent.\\\" I do not see how this setup provides any formal\nguarantee.\n\n\\\"for very query\\\" -- $>$ \\\"for every query\\\"\\\n\\\n**Response**:\\\nWe agree that the word \"guaranteeing\" is not appropriate. It is replaced\nby more suitable words such as \"empirically remaining\" and \\\"leading\nto\\\" throughout the paper to avoid the confusion. In addition, we\nfurther discuss the reasons in section 3.1, the last paragraph of page\n3. Typos are corrected.", "title": "Response to Reviewer 3"}, "Qk0GBDPv5K": {"type": "review", "replyto": "cAvgPMAA3hb", "review": "Summary:\n\nThe paper proposed an extension on Neural radiance field for better generalization across novel scenes by introducing multi-view pixel aligned features as additional input to NeRF. To fuse multi-view information and implicit reason about occlusion, an attention aggregation module is applied.\n\nStrengths:\n\n+ The idea of using pixel aligned feature for making NeRF generalize to novel scenes is interesting.  \n\nConcerns:\n\n1) the term of implicit representation is interchangeably used to describe implicit function and explicit 3D representation parameterized by MLPs.\n\n2) Comparison between GRF and GRAF: In GRAF, the radiance field is also conditioned on a shape code as well as an appearance code. But in the related work section, the author states that GRAF is unable to generalize to novel scenarios which seems to be an unfair claim.  \n\n3) Missing details about volumetric rendering: The paper did not talk about how rendering is performed(possibly volumetric rendering). And both in NeRF and NSVF, sampling strategy and volumetric rendering both play important roles on achieving high-fidelity rendering results. It is unclear here how ray marching is formulated. \n\n4) Experiments on ShapeNetV2: the author just reported SRN's result here, but didn't compare with some other method like NVS which is also directly applicable. Besides, other methods like NeRF could be modified to train on multiple objects like including a conditional embedding to NeRF jsut like GRAF or like in SRN use hyper-networks. Lack results here could be potentially undermining the claim of GRF being more general and robust.\n\n5) Experiments on real-world complex scenes: The training setup here is a bit unclear to me. From just the description there, it is hard for me to tell whether GRF is training one model on all those scenes or training separate models for different scenes. It would be great if the author could make that clear.\n\nMinors:\n\nIn the last paragraph of section 3.4, very query 3D point -> every query 3D  point?\n", "title": "Recommendation to weakly Reject", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "DfG_ZIgUWat": {"type": "review", "replyto": "cAvgPMAA3hb", "review": "Pros:\n+ This paper proposes a method for synthesizing 3D scenes in novel view by treating the network as a general radiance field. It seems that the method is more like traditional manner based on multiple view geometry. It enables the generalization ability of rendering unseen test data in contrast to the most related work, NeRF (Mildenhall et al., 2020).\n+ The proposed GRF can empirically generalize to novel scenes. Experimental results show that the proposed method achieves better performance on several large-scale datasets.\n+  In general, this paper is well-written and easy to read.\n\nConcerns:\n- The novelty is limited as the concept of this paper is highly similar to NeRF, despite a significant improvement is the general feature for novel view rendering. Yet I have few questions about general features for 3D points.\n1.  As depicted, the aggregator that assembles the 2D features of each 3D point from multiple views handles the visual occlusion implicitly via an attention process without depth scans. How does it work on unseen scenes? The network may have no knowledge about the structure of the novel scenes. Are there any failure examples of such cases?\n2.  Rendering a query 3D point p requires the (r_p, g_p, b_p) and the density d_p. As described in Eq. (6), the color channels are estimated through MLPs with learned feature \\bar{F}_p and the query viewpoint \\mathcal{V}_p as input. Nonetheless, the density function in Eq. (5) does not require the query viewpoint as input. I think the density of 3D points for volume rendering should be dependent on the viewing ray as well. How to explain the difference between Eq. (5) and Eq. (6)?\n\n- The evaluation of generalization for the novel scene of the proposed model is still not convincing enough, which needs improvements.\n1.  In section 4.1, the performance of SRN is better than the proposed methods in almost all situations. The authors give an explanation that SRN requires to be retrained on novel scenes to optimize the latent code. I suggest that the authors should evaluate the performance of SRN on novel scenes without retraining process to validate the argument.\n2.  The experiments only conduct on the novel scene of the same category which share the similar features. I consider that SRN can also handle this situation by the learned latent code and hypernetwork. The authors are suggested to train the proposed GRF on a large amount of data with different categories of objects to learn the general feature for attention mechanism, and evaluate the model on the new object to validate the generalization.", "title": "This paper proposes a neural rendering model named General Radiance Field (GRF) to achieve 3D representation learning and novel view synthesis. Different from the previous neural rendering methods NeRF and SRN that directly learn the mapping from pose information to color, GRF learns to aggregate the 2D features of different observations via attention mechanism given a 3D location and view pose. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "XtQL1ZtCHx": {"type": "review", "replyto": "cAvgPMAA3hb", "review": "This paper presents an extension of NeRF. The key idea is to represent a 3D\nscene as a collection of K \"posed\" images. A network is trained to take these\nimages, viewing information and a query point as input and output density (soft\noccupancy) and RGB reflectance color. In contrast to NeRF, this methods learns a\na mapping from the input images to feature vectors and thus promises to\ngeneralize across inputs (whereas NeRF uses weight-encoding). On a detailed\nlevel, this method also uses multiview consistency during training, though there\nis not an ablation I could find that demonstrates the effectiveness of this\ndelta.\n\nI recommend accepting this paper to ICLR. I like the idea of using the K input\nimages as parameters controling the scene. My main criticisms are in the\nexposition of the paper and the experiments. I hope the exposition can be\nimproved in revisions and am comfortable leaving the experimental shortcomings\nto future work.\n\nMy main source of confusion reading this paper centers around the input\n\"viewpoint\". The following comments are made in order of the paper's exposition\nand hopefully highlight the path of my confusion:\n\nPlease define what's meant by \"posed 2D images\".\n\nAre the viewpoints V1, ... Vk corresponding to the K input images really just\nthe 3D position of the camera focal point? Or does this also include the view\ndirection and camera intrinsics? If it does include other information then\nperhaps use a different symbol for the query viewpoint.\n\nNear Figure 2, the discussion of \"viewpoint\" for the image is really confusing.\nDo all pixels of an input share the same viewpoint position (seems to be the\ncase. This seems awkward, but best matches the written explanation.\n\nOr is the image \"placed\" into the 3D scene according to a pinhole camera's\ntransformation matrix, so that each pixel gets a unique 3d position associated\nto it? This seems more appropriate, but would not match the text or explanation.\n\nThe reprojection step appears to assume access to more than the camera center.\n\nMuch later the paper says Vk \"including extrinsics and intrinsics\". This is a\nfairly abusive notation. This should be clarified early on and the confusion\nissue with the image augmentation with viewpoint position remains.\n\nIf the intro/abstract made it clear that the input is K images and full camera\ninformation and a different symbol was used for camera info and query viewpoint,\nmuch of this confusion would go away.\n\nIn the motivation of this paper, there is special emphasis on representing\n\"scenes\", yet the experiments are on \"single objects\", just as other methods are\ncriticized for being limited to. Can the proposed method be used to represented\na full indoor room (not just a 360\u00b0 video; but view from anywhere in any\ndirection)?\n\nThe examples in the appendix are more like photos+depth type examples than what\nI would consider a full \"scene\". Perhaps tone down/clarify what is claimed in\nthe introduction.\n\nI would also like to get a sense of how stable / gracefully degrading this\nmethod is to the K input images. For example, if I only input images of the\nfront of an object, what will happen when viewing the back? What if most of the\nimages are from one direction, will this bias affect views elsewhere?\n\nDoes the method generalize to scenarios with \u2260 K input images? I suppose one\ncould use the zero vectors trick for the <K images, but I wonder about\ndegradation. What about >K?\n\nIt is not really fair to write that \"mose methods ... require ground truth 3D\ngeometry for supervision\". NERF/IDR/SIREN and when considering noisy point\nclouds SAL/SAL++ do not need ground truth 3D geometry.\n\nThe paper writes \"This simple design of GRF follows the principle of classic\nmulti-view geometry (Hartley & Zisserman, 2004), therefore guaranteeing the\nlearned implicit representations meaningful and multi-view consistent.\" I do not\nsee how this setup provides any formal guarantee. In the appendix it appears\nthat following NeRF this paper predicts the density from the query position and\nnot the viewing direction. Is that simply inherited here? The multiview aspects\nduring training simply harmonize with this choice but don't provide any\nguarantee that I can understand.\n\n\"for very query\" --> \"for every query\"\n", "title": "Accept", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}