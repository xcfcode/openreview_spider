{"paper": {"title": "IEA: Inner Ensemble Average within a convolutional neural network", "authors": ["Abduallah Mohamed", "Xinrui Hua", "Xianda Zhou", "Christian Claudel"], "authorids": ["abduallah.mohamed@utexas.edu", "xinruihua@utexas.edu", "xianda@utexas.edu", "christian.claudel@utexas.edu"], "summary": "We inner ensemble the features of a convolutional neural layer, it increases the network accuracy and generates distinct features.", "abstract": "Ensemble learning is a method of combining multiple trained models to improve model accuracy. We propose the usage of such methods, specifically ensemble average, inside Convolutional Neural Network (CNN) architectures by replacing the single convolutional layers with Inner Average Ensembles (IEA) of multiple convolutional layers. Empirical results on different benchmarking datasets show that CNN models using IEA outperform those with regular convolutional layers and advances the state of art. A visual and a similarity score analysis of the features generated from IEA explains why it boosts the model performance.", "keywords": ["Ensemble Convolutional Neural Networks"]}, "meta": {"decision": "Reject", "comment": "The method under consideration uses parallel convolutional filter groups per layer, where activations are averaged between the groups, forming \"inner ensembles\".\n\nReviewers raised a number of concerns, including the increased computational cost for apparently little performance gain, the choice of base architecture (later addressed with additional experiments using WideResNet and ResNeXt), issues of clarity of presentation (some of which were addressed). One reviewer was unconvinced without direct comparison to full ensembles. Another reviewer raised the issue of a missing direct comparison to the most similar method in the literature, maxout (Goodfellow et al, 2013). Authors rebutted this by claiming that maxout is difficult to implement and offering vague arguments for its inferiority to their method.\n\nThe AC agrees that a maxout baseline is important here, as it is extremely close to the proposed method and also trivially implemented, and that in light of maxout (and other related methods) the degree of novelty is limited.  The AC also concurs that a full ensemble baseline would strengthen the paper's claims. In the absence of either of these the AC concurs with the reviewers that this work is not suitable for publication at this time."}, "review": {"B1exMlxcCQ": {"type": "rebuttal", "replyto": "BygANjA5FX", "comment": "We updated a 3rd version correcting some grammatical errors. ", "title": "Updated version"}, "H1xLpthtR7": {"type": "rebuttal", "replyto": "ryx_iuvFnX", "comment": "We want to thank the reviewer for his review and dedicating time for our work. \n\n>> Regards first and second point\n\nFor the first and second point we updated our work with extra experiments including CIFAR-10 + and CIFAR-100 + on state of the art methods models like WideResNet and ResNeXt. We show that by adapting IEA into these models, WideResNet trained on CIFAR-100+ is able to improve from 18.85 to 18.03 error rate and with ensemble of IEA models we achieve an error rate of 15.84. Also, in ResNeXt IEA  trained on CIFAR-100+ achieveses a performance of 17.44 which on par with the mean of 10 runs in their paper valued at 17.44 error rate and when we ensemble IEAs we obtain an error rate of 16.44. Similar performance weer shown on CIFAR-10. Please note that we also re-trained normal WideResNet and ResNeXt following each work settings and we obtained a less results than what was in the papers. Yet, same configurations were used to train the IEA version of these two models and we achieved the aforementioned results. \n\n>> Regards the third point \n\nWe understand your concern regards increasing the parameters size $m-1$ times, we highlighted this in our paper and we follow the same lead of MaxOut and other deep models that expands in depth and width and with the current advancements of GPUs and parallelization, computational time is no longer a bottleneck. Also, we computed the inference time of our model for example WideResNet-IEA inference time on GTX1080Ti is 2.36 ms which is on par with a regular DenseNet model on the same GPU with inference time of 2.09 ms. We also added in our paper the inference time of each model. \n\n>> Conclusion\n\nWe hope that our update regards the performance enhancement of our model and the study of inference time accompanied with the unique features generated from the usage of IEA which is shown in the visualization section meets the innovation standard and shows the feasibility of this method.\n\n\n", "title": "Larger dataset results with a general increase in performance and covering the concerns "}, "S1Q3ytntR7": {"type": "rebuttal", "replyto": "BygZMqts3Q", "comment": "We want to thank you for putting such an effort in providing us with this review.\n\n>>Regards clarity of the concept\n\nThank you very much for this remark, in our paper  IEA is indeed applied after a nonlinear operation,  in style IEA itself is a plug and play method inside deep models. \n\n>> Regards adapting the m and k for enhancing the readability\n\nThanks for mentioning this, we adapted this approach in the updated version of the paper. \n\n>> Regards the comparison with ordinary ensemble \n\nOur goal is not to state that the Inner ensemble is better than outer ensemble. Our main goal is to introduce this operator and show its ability to boost the performance wherever it used. \n\n>> Regards SOTA and expanding the tests to larger datasets\n\nWe used the rebuttal period to test IEA on CIFAR-100+ and CIFAR-10+. Our results on CIFAR-100+ show that using IEA inside WideResNet improves  the error rate from 18.85 according to  WideResNet paper to 18.03. Also, for WideResNet when we have the case of (m=3,k=3) we obtain an error rate of  15.84.  We also tested IEA on ResNeXt and we had an error rate of 17.44  which is on the par with the error rate reported in their paper of 17.31. In the case of (m=3,k=3) for ResNeXt, we obtain an error rate of 16.44 which also exceeds their work. Please note that we also re-trained normal WideResNet and ResNeXt following each paper training settings and we obtained a results that is less than what was reported in their the papers. Yet, by using the same configurations in their papers the IEA version of these WideResNet and ResNext achieved the aforementioned results. We also achieved similar results on CIFAR-10+. \n\n>>Conclusion\n\n\nWe hope that our new results which shows a performance increase by using IEA and the addition of inference measurements, editing the paper structure to increase clarity matches your expectations for what this work should look like. \n\n>> Other notes\nOther notes:\n\n\nWe fixed \\citep  problem. \n\nWe changed the FCL acronym this, it\u2019s much better, thanks again for suggesting this. \n\nWe added the results of training on CIFAR-10+ and CIFAR-100+, for ImageNet we couldn\u2019t train ourmodels due to lack of resources and time. We also added training, testing error curves and inference time.\n\nThanks again\n", "title": "Exceeding SOTA, larger datasest tests and general improvements"}, "SkeqRwnF0Q": {"type": "rebuttal", "replyto": "BkgksQPZ6m", "comment": "\n\nDear reviewer, \n\nHonestly, We thank you for such a detailed review and pointing out such an important work. \n\n>>Regards the first paragraph of your review \n\nBy using IEA combined with augmentation and dropout we were able to bypass current SOTA on CIFAR100 and we show in the updated version of the paper these results on both WideResNet and ResNeXt. We also show that the IEA in some cases bypass an ensemble of m networks. To clear something, we don\u2019t directly compare IEA with ensemble rather than introducing a new operator that can enhance the current models wherever it applied. We also show that the ensemble of IEAs boost these models results more.\n\nOur intuition for choosing mean is as follow: Average ensemble of models are used essentially to reduce the variance, in other terms to increase models generalization ability. We hypothesize that the overall variance of the model can be decomposed into sub-layers within the model and each layer contributes somehow into this variance. If we used inner ensemble average we will reduce the sub-variances of each layer resulting in an overall variance reduction per model. To support our hypothesis is correct we show that features generated by IEA are unique and it suppresses the features that causes the model ambiguity or variance and this is discussed in the visualization section. For feedforward layers, we didn\u2019t test IEA on it as our main focus is towards convolutional layers. \n\n>>Regards Maxout\n\n\u201cMax out in essence choose a winner unit that will receive the gradients and other units will have an error of zero and in corner case when there is a tie for winner, the function is not differentiable, because the left-sided derivative does not match the right-sided derivative for each of the elements that ties\u201d [1]. This will require some special implementation to solve this issue also it will leave the network with ambiguous filters that will increase the overall variance because they weren\u2019t trained on all of the training set. Unlike in our approach, were we use the average, the gradients are distributed equally amongst the filters and this what leads to unique features as shown in the visualization section. We also changed the abstract following your directions.\n\n>>Regards related work\n\nWe added a literature review following your directions.\n\n>>Regards results\n\nWe added results in our updated version that shows performance boost on WideResNet and ResNeXt too. For other domains like text and audio we believe that IEA will also boost the performance following the same leads on vision task, still we strongly agree that having results on non-image settings will be a positive thing for the work, yet we currently don\u2019t have the means of doing such experiments. \n \n\n>>Regards similarity with ResNet and DesnsNet \n\n\nIn the literature review we discussed this. But IEA in essence as you mentioned is a plug and play. For example in ResNet the activation function is $H(x) = x+ F(x)$ and for DenseNet we can write it as $F_i(x) = F_{i-1}(F_{i-2}(...F_0(x))) $. IEA itself cares about F(x) and treats it as average of multiple convolutional neural layer, so IEA is an addon to these architectures more than something to compare against. Also, our latest results supports the idea of IEA.\n\n>>Regards the visualization section\n\nWe removed the ensemble selection section as we agree on this.  The visualization section is concerned with comparison of IEA versus ordinary CNN. We also investigated more by using the similarity score introduced in the paper. Our main aim of this section is to have an intuition behind IEA and why it reduces the model variances. We added a comparison between the MaxOut features and IEA features and we had some interesting findings in the updated version of the paper, thanks again for suggesting this. \n\n>>Conclusion \n\nWe hope that our updates matches your expectations and your concerns were covered, also we would like to thank you a lot for such a feedback! \n\n\n\n[1]https://www.reddit.com/r/MachineLearning/comments/2vl8hp/a_question_about_maxout_who_gets_the_error/coj1o5y/\n\n", "title": "Covering the concerns, updated results and raising SOTA bar"}, "BkgksQPZ6m": {"type": "review", "replyto": "BygANjA5FX", "review": "IEA proposes to use multiple \"parallel\" convolution groups, which are then averaged to improve performance.\n\nThis fundamental idea of ensembles combined with simple functions has been explored in detail in Maxout (Goodfellow et.  al., https://arxiv.org/abs/1302.4389) in the context of learning activation functions, and greater integration with dropout regularization.\n\nUnder the lens of comparison to Maxout (which should be cited, and is a key comparison point for this work), a number of questions emerge. Does IEA also work for feedforward layers? Does IEA give any performance improvement or have some fundamental synergy with the regularizers used here? Is the performance boost greater than simply using an ensemble of m networks directly (resulting in the equivalent number of parameters overall)? The choice of the mean here seems insufficient for creating the types of complexity in activation which are normally desirable for neural networks, so some description of why a simple mean is a good choice would be beneficial since many, many other functions are possible.\n\nCrucially Maxout seems much too close to this work, and I would like to see an indepth comparison (since it appears to be use of mean() instead of max() is the primary difference). I would also significantly reduce the claims of novelty, such as \"We introduce the usage of such methods, specifically ensemble average inside Convolutional Neural Networks (CNNs) architectures.\" in the abstract, given that this is the exact idea explored in other work including followups to Maxout.\n\nFor example, MNIST performance here matches Maxout (.45% for both, but Maxout uses techniques known in 2013). CIFAR-10 results are better, but again Maxout first appeared 5 years ago. There are more recent followups that continued on the line of work first shown in Maxout, and there should be some greater comparison and literature review on these papers. The CIFAR-10 baseline numbers are not ideal, and since IEA is basically \"plug and play\" in existing architectures, starting from one of these settings instead (such as Wide ResNet https://arxiv.org/abs/1605.07146) and showing a boost would be a stronger indication that this method actually improves results. In addition, there are a number of non-image settings where CNNs are used (text or audio), and showing this idea works on multiple domains would also be good.\n\nThere seems to be a similarity between ResNet and this method - specifically assuming the residual pathway is convolution with an identity activation, the summation that combines the two pathways bears a similarity to IEA. With multiple combined paths (as in Dense ResNet) this equivalence seems stronger still. A discussion of this comparison in greater detail, or even derivation of IEA as a special setting or extension of ResNet (coupled with stronger performance on the datasets) would help ground the work in prior publication.\n\nThe section on visualization and inspection of IEA features seems interesting, but too brief. A greater exploration of this, and possible reduction or removal of the ensemble selection section (which didn't have a clear contribution to the message of the paper, in my opinion) would strengthen the work - and again, comparisons to activations learned by Maxout and followups would make this inspection much stronger.\n\nMy key concerns here are on relation to past work, greater comparison to closely related methods, and improvement of baselines results. Given the close similarity of this work to Maxout and others, a much stronger indication of the benefits and improvements of IEA seems necessary to prove out the concepts here.", "title": "A number of missing comparisons, needs stronger empirical results", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BygZMqts3Q": {"type": "review", "replyto": "BygANjA5FX", "review": "This work proposes an ensemble method for convolutional neural networks wherein each convolutional layer is replicated m times and the resulting activations are averaged layerwise.\n\nThere are a few issues that undermine the conclusion that this simple method is an improvement over full-model ensembles:\n\t1. Equation (1) is unclear on the definition of C_layer, a critical detail. In the context, C_layer could be weights, activations before the nonlinearity/pooling/batch-norm, or activations after the nonlinearity/pooling/batch-norm. Averaging only makes sense after some form of non-linearity, otherwise the \u201censemble\u201d is merely a linear operation, so hopefully it\u2019s the latter.\n\t2. The headings in the results tables could be clarified. To be sure that I am understanding them correctly, I\u2019ll propose a new notation here. Please note in the comments if I\u2019ve misunderstood! Since \u201cm\u201d is used to represent the number of convolutional layer replications, let\u2019s use \u201ck\u201d to represent the number of full model replications. So, instead of \u201cCNL\u201d and \u201cIEA (ours)\u201d in Table 1 and \u201cEnsemble of models using CNL\u201d and \u201cEnsemble of models using IEA (ours)\u201d in Table 2, I would recommend a single table with these headings: \u201c(m=1, k=1)\u201d,  \u201c(m=3, k=1)\u201d,  \u201c(m=1, k=3)\u201d,  and \u201c(m=3, k=3)\u201d, corresponding to the columns in Tables 1 and 2 in order. Likewise for Tables 3-6.\n\t3. Under this interpretation of the tables---again, correct me if I\u2019m wrong---the proper comparison would be \u201cIEA (ours)\u201d versus \u201cEnsemble of models using CNL\u201d, or  \u201c(m=3, k=1)\u201d versus \u201c(m=1, k=3)\u201d in my notation. This pair share a similar amount of computation and a similar number of parameters. (The k=3 model would be slightly larger on account of any fully-connected layers.) In this case, the \u201couter ensemble\u201d wins handily in 4 of 5 cases for CIFAR-10.\n\t4. The CNL results, or \u201c(k=1,m=1)\u201d, seem to not be state-of-the-art, adding more uncertainty to the evaluation. See, for instance, https://www.github.com/kuangliu/pytorch-cifar. Apologies that this isn\u2019t a published table. A quick scan of the DenseNets paper and another didn\u2019t yield a matching set of models. In any case, the lack of data augmentation may account for this disparity, but can easily be remedied.\n\nGiven the above issues of clarity and that this simple method seems to not make a favorable comparison to the comparable ensemble baseline (significance), I can\u2019t recommend acceptance at this time. \n\nOther notes:\n\t* The wrong LaTeX citation function is used, yielding the \u201cauthor (year)\u201d form (produced by \\citet), instead of \u201c(author, year)\u201d (produced by \\citep), which seems to be intended. It\u2019s possible that \\cite defaults to \\citet.\n\t* The acronyms CNL and FCL hurt the readability a bit. Since there is ample space available, spelling out \u201cconvolutional layer\u201d and \u201cfully-connected layer\u201d would be preferred.\n\t* Other additions to the evaluation could or should include: a plot of test error vs. number of parameters/FLOPS/inference time; additional challenging datasets including CIFAR-100, SVHN, and ImageNet; and consideration of other ways to use additional parameters or computation, such as increased depth or width (perhaps the various depths of ResNet would be useful here).\n", "title": "Issues of clarity and comparison", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryx_iuvFnX": {"type": "review", "replyto": "BygANjA5FX", "review": "This paper describes a new op: Inner Average Ensemble (IAE). This op is constructed by $m$ convolution ops followed by an averaging op. The author claims using this IAE op is able to improve CNN classification performance. The experiments include MNIST and CIFAR-10 with a few network structures. \n\nFirst of all, this new proposed op is not efficient. Replace a traditional conv layer with one IAE layer it will introduce $m$ times more parameters, while, the performance gain from the authors\u2019 experiment is relatively small, which indicates, most learning capacity is wasted.\n\nSecondly, only MNIST and CIFAR-10 is not convincing that this structure change will be widely useful.\n\nThirdly, this work is not practical to apply on real tasks, because it introduced $m - 1$ times more computation. \n\nOverall, I am not convinced this structure change meets innovation standard. \n\n", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}