{"paper": {"title": "Overcoming Multi-model Forgetting", "authors": ["Yassine Benyahia*", "Kaicheng Yu*", "Kamil Bennani-Smires", "Martin Jaggi", "Anthony Davison", "Mathieu Salzmann", "Claudiu Musat"], "authorids": ["yassine.benyahia1@gmail.com", "kaicheng.yu@epfl.ch", "kamil.bennani-smires@swisscom.com", "martin.jaggi@epfl.ch", "anthony.davison@epfl.ch", "mathieu.salzmann@epfl.ch", "claudiu.musat@swisscom.com"], "summary": "We identify a phenomenon, neural brainwashing, and introduce a statistically-justified weight plasticity loss to overcome this.", "abstract": "We identify a phenomenon, which we refer to as *multi-model forgetting*, that occurs when sequentially training multiple deep networks with partially-shared parameters; the performance of previously-trained models degrades as one optimizes a subsequent one, due to the overwriting of shared parameters. To overcome this, we introduce a statistically-justified weight plasticity loss that regularizes the learning of a model's shared parameters according to their importance for the previous models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight plasticity in neural architecture search preserves the best models to the end of the search and yields improved results in both natural language processing and computer vision tasks.", "keywords": ["multi-model forgetting", "deep learning", "machine learning", "multi-model training", "neural architecture search"]}, "meta": {"decision": "Reject", "comment": "\npros:\n- nicely written paper\n- clear and precise with a derivation of the loss function\n\ncons:\n\nnovelty/impact:\nI think all the reviewers acknowledge that you are doing something different in the neural brainwashing (NB) problem than is done in the typical catastropic forgetting (CF) setting.  You have one dataset and a set of models with shared weights; the CF setting has one model and trains on different datasets/tasks.  But whereas solving the CF problem would solve a major problem of continual machine learning, the value of solving the NB problem is harder to assess from this paper...  The main application seems to be improving neural architecture search.  At the meta-level, the techniques used to derive the main loss are already well known and the result similar to EWC, so they don't add a lot from the analysis perspective.  I think it would be very helpful to revise the paper to show a range of applications that could benefit from solving the NB problem and that the technique you propose applies more broadly."}, "review": {"rJe_jDnHC7": {"type": "rebuttal", "replyto": "r1fE3sAcYQ", "comment": "We thank the reviewers for their valuable comments and for taking the time to review our paper. We have uploaded a revised version that addresses the reviewers\u2019 main concerns. In particular\n\n- We have renamed brainwashing \u201cmulti-model forgetting\u201d to account for the fact that the literature has become more liberal in using the term \u201cforgetting\u201d;\n\n- We have clarified the novelty of our approach. Specifically, one could not simply heuristically modify the EWC loss to fit our multi-model setting. Our derivation shows that an additional term encoding the interactions between two models arises from our scenario;\n\n- We have added a discussion of (Xu & Zhu, NIPS 2018);\n\n- We have incorporated novel experiments based on the NAO strategy of Luo et al., NIPS 2018. These experiments demonstrate the benefits of our WPL loss in another neural architecture search approach, and show that WPL improves over the path dropout strategy of Bender et al., ICML 2018. \n\n\nAltogether, we believe that these modifications significantly strengthen our paper and further highlight the generality of our approach.\n", "title": "Revised paper uploaded"}, "HJe5Bv3BAm": {"type": "rebuttal", "replyto": "rygpqz4uhQ", "comment": "Thank you for your time and positive feedback! We believe that addressing your comments has led to a stronger version of our paper.\n\n>Relation to [1] Bender et al., ICML 2018\n\nThank you for pointing us to this interesting work. [1] indeed highlights the problems arising from training a one-shot model corresponding to multiple architectures with shared parameters, and circumvents them by randomly dropping paths during training.  However, this differs significantly from our work, where we derive a mathematical solution to address this problem. In fact, both solutions can be used jointly, and this is what is done in our experiments. Indeed, in all our architecture search experiments, ENAS relies on path dropout with a probability of 0.5, both when incorporating WPL and when not. Therefore, our experiments show that our approach can further improve the results of the strategy used in [1]. We have revised our paper so as to explain our use of path dropout and give proper credit to [1].\n\n\n> Relation to [2] Luo et al., NIPS 2018 (NAO)\n\nWe became aware of NAO shortly after the ICLR deadline. In essence, the contribution of this work is to replace the reinforcement learning portion of ENAS with a gradient based auto-encoder. This can still suffer from multi-model forgetting, and again is thus orthogonal to our work. To demonstrate this, we incorporated WPL in the NAO framework and re-ran our RNN experiments with this new search method. The details of these experiments are provided in the appendix of our revised paper. These experiments illustrate the effectiveness of our approach with respect to both [1] and [2]. In short, we observed that\nthe use of WPL reduces multi-model forgetting  in NAO, as in ENAS, and this for various dropout rates;\nwhile increasing the dropout rate indeed limits the multi-model forgetting effect, the resulting model consistently benefits from using WPL.\nWe believe that these experiments confirm that our paper addresses an important issue, occurring in many neural architecture search strategies that use shared model representations. This further strengthens our contribution.\n\n\n> Relatively low performance of ENAS on CIFAR-10\n\nTo implement our approach, we used a publicly available PyTorch implementation of ENAS for the RNN case that we further developed. For CIFAR-10, we extended this implementation to the CNN case. The choice of reimplementation of ENAS  was motivated by the simplicity and flexibility of PyTorch. \n\nTo evaluate the final cells obtained by ENAS-WPL and ENAS we trained them in a fair training, without any hyperparameter tuning. The mismatch in scores is solely due to a difference in hyperparameter tuning, both for search and training from scratch, since ENAS final training is highly optimized. However, we believe this not to be a real issue, since our point is truly to demonstrate the benefits of accounting for multi-model forgetting, which our experiments do.\n\n", "title": "Response to Reviewer #1"}, "rkeiZwhr0X": {"type": "rebuttal", "replyto": "H1e4uaHj2X", "comment": "We thank the reviewer for their useful comments and for taking the time to review our paper. Below, we address their main concerns and have revised our paper accordingly.\n\n>Branding: Brainwashing vs forgetting\n\nIn the literature, \u201cforgetting\u201d traditionally refers to the scenario where one aims to train a single model on two different datasets. By contrast, we aim to train multiple models on a single data, which motivated our use of the term \u201cbrainwashing\u201d. We agree with the reviewer, however, that the term \u201cforgetting\u201d has recently started being used in a looser sense. Therefore, we have revised our paper to refer to our approach as \u201cmulti-model forgetting\u201d.\n\n\n>Novelty over EWC\n\nThere is clear technical novelty in our paper, which stems from the fact that, while EWC aims to maximize the posterior probability p(\\theta | D1, D2), we maximize p(\\theta_1, \\theta_2, \\theta_s | D), where \\theta_1 and \\theta_2 denote the parameters specific to each model and \\theta_s those shared by both models. Heuristically modifying the EWC loss to fit our two-model scenario would be mathematically unjustified, and we therefore had to derive the equations for our formalism so as to reach WPL loss.\n\nOur derivation led to a new term in Equation (3), v^T \\Omega v, which encodes the interaction between the two models. This term will never appear in EWC, nor in any single-model forgetting formulation. The fact that WPL looks similar to EWC loss is then only due to our use of a Laplace approximation of this term with the diagonal Fisher information matrix as covariance. However, other approximations, such as a Laplace one with a full covariance matrix, will lead to loss functions that differ fundamentally from the EWC one. We have clarified this in the revised paper and believe our mathematical formulation of the parameter sharing scenario and its general solution in Equation (3) to be solid technical contributions.\n\n\n>Relation to Xu & Zhu, NIPS 2018.\n\nThis paper addresses a fundamentally different problem from the one we tackle. In essence, given Model A trained on Dataset A, Xu & Zhu, NIPS 2018, use an NAS-like strategy to train Model B on a different Dataset B. While Model B shares some parameters with Model A, absolutely no forgetting occurs, because the parameters of Model A are fixed. As such, this work does not address forgetting, but rather aims to compensate for the sub-optimality of Model A\u2019s parameters for Dataset B via NAS. While interesting, this idea is orthogonal to ours. In fact, this method could benefit from relying on WPL when searching for the best Model B. We thank the reviewer for pointing us to this work, which we now discuss in our revised paper.\n", "title": "Response to Reviewer #3"}, "ByeOqI3r07": {"type": "rebuttal", "replyto": "rkg-raP03Q", "comment": "We thank the reviewer for taking the time to review our paper.  Below, we address the main concern.\n\n> Incremental advances\n\nOur work is not incremental. While weight sharing is popular, multi-model forgetting has been neither explicitly acknowledged, nor carefully studied. There is clear technical novelty in our paper, which stems from the fact that, while EWC aims to maximize the posterior probability p(\\theta | D1, D2), we maximize p(\\theta_1, \\theta_2, \\theta_s | D), where \\theta_1 and \\theta_2 denote the parameters specific to each model and \\theta_s those shared by both models. Heuristically modifying the EWC loss to fit our two-model scenario would be mathematically unjustified, and we therefore had to derive the equations for our formalism so as to reach WPL loss.\n\nOur derivation led to a new term in Equation (3), v^T \\Omega v, which encodes the interaction between the two models. This term will never appear in EWC, nor in any single-model forgetting formulation. The fact that WPL looks similar to EWC loss is then only due to our use of a Laplace approximation of this term with the diagonal Fisher information matrix as covariance. However, other approximations, such as a Laplace one with a full covariance matrix, will lead to loss functions that differ fundamentally from the EWC one. We have clarified this in the revised paper and believe our mathematical formulation of the parameter sharing scenario and its general solution in Equation (3) to be solid technical contributions.\n\nFurthermore, we believe that our new experiments using WPL in the NAO framework of Luo et al., NIPS 2018,  further confirm that our paper addresses an important issue that occurs in many neural architecture search strategies that use shared model representations. \n", "title": "Response to Reviewer #2"}, "rkg-raP03Q": {"type": "review", "replyto": "r1fE3sAcYQ", "review": "There is certainly additional novelty in that this paper focuses on models performing same/identical tasks (compared the results from the catastrophic forgetting paper), and because this model more clearly delineates the parameters that are shared across the models, vs those that are not. But both of those advances feel incremental.\n ", "title": "The technique in this paper feels more or less identical to the ideas from Kirkpatrick et al (catastrophic forgetting). The difference seems to be one of application (different tasks vs same task), and as such feels like an incremental advance.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "H1e4uaHj2X": {"type": "review", "replyto": "r1fE3sAcYQ", "review": "\n- This \"neural brainwashing\" is catastrophic forgetting. Technically speaking, this is catastrophic forgetting. \n\n- Also, some works targeting NAS (which I reckon should as well be cited due to being quite related) have targeted similar forgetting issues, e.g. Xu and Zhu, NIPS 2018 \"Reinforced continual learning\". It is nice to enrich the literature with new terms, when there is a need to. In my opinion, in this particular case, neural brainwashing is catastrophic forgetting. \n\n- Forgetting is not necessarily an \"individual problem\", sticking to the language used in the third paragraph of the first page. The same applies to \"single-model forgetting\". \n\n- page 1 \"Our work is the first of which we are aware to identify neural brainwashing and to propose a solution.\": According to the authors' argument, this is the case. Again, mine is different.\n\n- Novelty w.r.t. works tackling catastrophic forgetting, most notably EWC, is minimal. Also, comparing to other state-of-the-art algorithms targeting catastrophic forgetting can further enrich the experiments. \n\n- 3.1.1. On a technical level, there is no inherent difference, between EWC and the proposed algorithm. \n\n- Writing can improve, both in terms of the flow and the language. There are also a few typos, e.g. in the first line of the caption of Figure 2.\n\n- Apart from the aforementioned issue (comparing to other state-of-the-art catastrophic forgetting algorithms), the experiments are rigorously prepared. ", "title": "Not sufficiently novel", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rygpqz4uhQ": {"type": "review", "replyto": "r1fE3sAcYQ", "review": "This paper discusses the phenomena of \u201cneural brainwashing\u201d, which refers to that the performance of one model is affected via another model sharing model parameters. To solve the issue, the authors derived a new loss out from maximizing the posterior of the parameters. With the new loss, the neural brainwashing is largely diminished. \nThe derived new loss looks meaningful to me and I think this is a valuable work for handling the weights coadaptation between two neural models, which with no doubt will bring great interests within the community of neural architecture search.\n\nHere are some comments on the aspects that this paper can be improved: \n\n1)\tA very important related work [1] is missed in this paper. [1] discussed the properties of \u201cone-shot model\u201d, which means that several different architectures are unified into the same model by sharing model weights. Furthermore, [1] discussed \u201cneural brainwashing\u201d (although not with the same name) and how to handle it in a very simple way (by randomly dropping path). This definitely should be a baseline to compare with. In addition, a very recent work [2] also leverages model sharing to conduct neural architecture search.\n\n2)\tAlthough I understand that to improve accuracy of NAS is not the main goal of this paper, the baseline number to be improved over is too weak. For example, 4.87 of CIFAR10 in ENAS. Per my own hands on experience, it does not need too many hyperparameter tuning of ENAS to obtain < 4% error rate. Please provide more convincing baseline numbers and supporting evidences of the better performance of WPL in NAS.\n\n\n[1] Bender, Gabriel, et al. \"Understanding and simplifying one-shot architecture search.\" International Conference on Machine Learning. 2018.\n[2] Luo, Renqian, et al. \"Neural architecture optimization.\" NIPS (2018).\n", "title": "a good work on one shot model training", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1eaAIv6F7": {"type": "rebuttal", "replyto": "H1xJ3lgTF7", "comment": "We thank the reader for commenting on the paper so quickly. We believe that the problem statement for brainwashing is very different from the forgetting. Throughout the paper (e.g.,  paragraph 2 of the Introduction, Section 3.1.1), we explain this difference, i.e., catastrophic forgetting happens when training a single model for multiple tasks, while the brainwashing occurs when training  multiple models on a single task.\n\nIndeed, In \u00ab overcoming catastrophic forgetting \u00bb, the authors maximize the posterior probability p(\\theta | D1, D2) while in \u00ab overcoming neural brainwashing \u00bb, we maximize p(\\theta_1, \\theta_2, \\theta_s | D), with \\theta_s refering to the shared parameters between two different models. Mathematically speaking, the two problems are fundamentally different. Catastrophic forgetting does not deal with parameter sharing across different models and only considers a single model with parameters \\theta.  In our paper, we focus on tackling the problem where multiple models are sharing part of their architectures. We formulate our final loss through a completely different mathematical derivation and coincidentally ends in a similar formalism.", "title": "brainwashing is different from catastrophic forgetting"}}}