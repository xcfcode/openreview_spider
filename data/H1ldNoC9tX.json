{"paper": {"title": "Classification from Positive, Unlabeled and Biased Negative Data", "authors": ["Yu-Guan Hsieh", "Gang Niu", "Masashi Sugiyama"], "authorids": ["yu-guan.hsieh@ens.fr", "gang.niu@riken.jp", "sugi@k.u-tokyo.ac.jp"], "summary": "This paper studied the PUbN classification problem, where we incorporate biased negative (bN) data, i.e., negative data that is not fully representative of the true underlying negative distribution, into positive-unlabeled (PU) learning.", "abstract": "Positive-unlabeled (PU) learning addresses the problem of learning a binary classifier from positive (P) and unlabeled (U) data. It is often applied to situations where negative (N) data are difficult to be fully labeled. However, collecting a non-representative N set that contains only a small portion of all possible N data can be much easier in many practical situations. This paper studies a novel classification framework which incorporates such biased N (bN) data in PU learning. The fact that the training N data are biased also makes our work very different from those of standard semi-supervised learning. We provide an empirical risk minimization-based method to address this PUbN classification problem. Our approach can be regarded as a variant of traditional example-reweighting algorithms, with the weight of each example computed through a preliminary step that draws inspiration from PU learning. We also derive an estimation error bound for the proposed method. Experimental results demonstrate the effectiveness of our algorithm in not only PUbN learning scenarios but also ordinary PU leaning scenarios on several benchmark datasets.", "keywords": ["positive-unlabeled learning", "dataset shift", "empirical risk minimization"]}, "meta": {"decision": "Reject", "comment": "The paper proposes an algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework.\n\nThe reviewers and AC commonly note the critical limitation of practical value of the paper and results are rather straightforward.\n\nAC decided the paper might not be ready to publish as other contributions are not enough to compensate the issue."}, "review": {"ByxOA-y9T7": {"type": "rebuttal", "replyto": "SJga3o753Q", "comment": "Effectively we have mentioned that our problem setup can be viewed as a special case of dataset shift but we did not establish any connection between our method and any other algorithms dealing with the dataset shift problem.\n\nThe reweighting technique is a popular solution to many related problems like covariate shift, imbalanced data or label noise. As a matter of fact, if we consider the case where \"P and N distributions are disjoint, suppose that p(s=+1|x)>0 almost surely and set \u03b7=0\", we recover the classic covariate shift reweighting scheme where no U data intervene in the second step of the algorithm when we estimate the classification risk. These conditions are however very restricted and in general our problem should not be compared with that of covariate shift.\n\nPseudo-labeling is a well-known semi-supervised learning method and can also be applied to domain adaptation. The second step of our algorithm has some similarity with classic pseudo-labeling methods. In particular, we use a different base classifier (not the one trained with P and N data) to carefully assign pseudo labels to unlabeled samples and beyond this we also assign weights to each sample.\n\nThe literature on dataset shift and related topics is so extensive that we cannot review all of the them here, but we believe the above two are the most related to our approach. Notice that as far as we know, no existing method designed for dataset shift is well-suited to solve the PUbN classification problem that we study in this paper.", "title": "5. Response to [Relation to dataset shift]"}, "HkgJWb15am": {"type": "rebuttal", "replyto": "SJga3o753Q", "comment": "This part is already addressed in the introduction of the paper. In many PU learning problems, it is possible to collect some biased N data. However the presence of these non-representative N data are often ignored and we must resort to pure PU learning because few algorithms are able to leverage them in the learning process. Our proposed method then successfully incorporates these data into PU learning and improves the classification performance.\n\nOne motivational situation is when the N population is formed by many subpopulations. It is normal that different subpopulations have different probabilities to get labeled as N. In [1], the authors mentioned the social media text classification problem where users are asked to manually labeled some relevant posts and irrelevant posts to a certain topic. Due to the inherent diversity of all the potentially irrelevant posts, the labeled N training posts cover only a small number of irrelevant topics and are therefore \"biased\".\n\n----\n[1] G. Fei and B. Liu. Social media text classification under negative covariate shift. In Proceedings\nof the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 2347\u20132356, 2015.", "title": "4. Response to [The original motivation for having biased negative data are not explained very clear]"}, "HJxGYGk5TQ": {"type": "rebuttal", "replyto": "HkxQIauh2Q", "comment": "Thank you very much for your feedback and approving that our paper is well written.\n\nTo address the two weak points:\n\nWe agree that better methods can be developed given stricter conditions or if the problem becomes more specific, but this is a trade-off that one always needs to face: a general setup with a good but not perfect algorithm or an ad hoc algorithm that works really well but only for rather restricted situations.\n\nFurthermore, we think there are also problems for which our method can be readily applied, namely the ones that are mentioned in our paper and in the replies to other reviewers.\nAt the same time, the problem that we formulate and the approach that we consider can also inspire people who want to pursue in similar directions to design algorithms under stricter conditions or for more specific problems.", "title": "Thank you for your feedback"}, "S1eRDg1qp7": {"type": "rebuttal", "replyto": "SJga3o753Q", "comment": "We presume by the assumption in section 2.4 you are talking about the relation p(s=+1|x,y=+1)=1.\n\nThis is in fact just a problem of notation. In other words, it is supposed that the marginal distribution p is a mixture of three component distributions q_1, q_2, q_3:\n\np = a*q_1+b*q_2+c*q_3 with a, b, c >= 0 and a+b+c=1.\n\nHere q_1 is the P distribution, q_2 is the biased N distribution and (b*q_2+c*q_3)/(b+c) is the N distribution.\nEach time when x is drawn, we set the value of y and s according to the following rule\n- If x comes from q_1, y=s=+1,\n- If x comes from q_2, y=-1, s=+1,\n- If x comes from q_3, y=s=-1.\n(Here we would just like to give an intuition so we do not formulate the things mathematically.)\n\nWe see the only assumption that is made is that the full N distribution is a mixture of the bN distribution and an unknown distribution q_3 (this is similar to the \"selection condition\" in sample selection bias). We believe that this is satisfied in many read-world scenarios because the bN samples are often collected/identified from a larger N data pool, which is the case in all the examples that are mentioned in our paper.\n\nIn the very general case, bN and N distributions can differ arbitrarily and it is true that we cannot verify whether the above assumption is satisfied or not because we do not have access to samples that are sampled from the unbiased N distribution. Our algorithm is not guaranteed to work when the mixture assumption is violated. In fact, \u03c1 cannot even be defined. One possible scenario is when some bN data belong to a latent category that does not appear in the true N distribution. Below are some preliminary experimental results:\n\nMNIST:\n[Positive [0, 2, 4, 6]; Negative [1, 3, 5, 7]; biased Negative [5, 7, 8, 9] (uniformly)]\n------------------------------------\nnnPU       | 3.97 \u00b1 0.66\nPUbN\\N  | 3.26 \u00b1 0.67\nPUbN      | 3.39 \u00b1 0.77\n------------------------------------\n\nCIFAR-10:\n[Positive [airplane, automobile, ship, truck]; Negative [bird, cat, deer, dog]; biased Negative [deer, dog, frog, horse] (uniformly)]\n------------------------------------\nnnPU       | 12.63 \u00b1 0.76\nPUbN\\N  | 11.17 \u00b1 0.39\nPUbN      | 10.51 \u00b1 0.70\n------------------------------------\n\nReported are classification errors in percentage. Since \u03c1 is ill-defined we arbitrarily choose \u03c1=0.25 to run our method. We see that in these two preliminary experiments the classifier learned with bN data works at least as well as that learned without bN data. This is because the U data are always exploited in our algorithm as long as \u03c1 is not too large and \u03b7 is not too small.\n\nWe would however like to caution against applying our method directly without any modification if one knows that the [mixture assumption/selection condition] will be violated with great probability (though this should be rare). As already mentioned above, our algorithm is not guaranteed to converge to optimal solution and risks to perform worse than simple PU learning in this case.\n\n- Unbiased N data\n\nOur algorithm is designed particularly to deal with the bias of N data with help of the presence of U data. The U data are only used to correct the fact that our N data are biased, and no further assumption about the optimal model or the underlying distribution is made. Therefore, when the N data are unbiased, it falls back to classic supervised learning where U data are totally ignored. Ideas that allow to exploit U data to improve the classifier in the classic semi-supervised setting (N data being unbiased) can also be brought back to the PUbN case by simply replacing the PN risk by the PUbN risk. ", "title": "3. Response to [I'm not convinced by the main assumption]"}, "r1euaUPDTm": {"type": "rebuttal", "replyto": "SJga3o753Q", "comment": "Semi-supervised baseline methods are not suitable in our case.\n\nIn fact, as we have already highlighted in our abstract, the PUbN classification problem that we study is quite different from the semi-supervised learning setting because the labeled N distribution may not cover the whole N distribution. Take our MNIST experiment for example, in the first learning task only 1, 3 and 5 are labeled as N samples while the full N distribution also include 7 and 9.\n\nAs a result, most of the semi-supervised learning algorithms cannot be directly applied to our problem. Ex: if we use entropy regularization for the above MNIST learning task with traditional PN risk, the accuracy is only around 80%; in fact, the regularization term cannot help at all if some subpopulation of data never appear in the labeled set.\n\nPNU learning is an exception because it relies partially on PU learning so it still somehow works in our problem. On the other hand, it is straightforward to combined different regularization-based semi-supervised learning algorithms with PUbN learning as it suffices to add the corresponding regularization term.", "title": "2. Response to [Choice of baseline methods is also limited]"}, "HygdSLPv6m": {"type": "rebuttal", "replyto": "SJga3o753Q", "comment": "We suppose that you are asking why adding bN data improves upon \"nnPU\".\n\nThis should be quite intuitive, because data explicitly labeled as N, even biased, should always carry some information and if we add them in the learning process it would certainly be beneficial, and that is also what motivated us to start studying this problem.\n\nSurely if this idea can be illustrated through figurer it will be clearer how bN data really helps learning a better classifier. We have therefore revised our paper to add corresponding content in Section 4.3 while the original Section 4.3 is now moved to appendix. We plot the representations learned by PUbN and nnPU classifiers and show that as expected, PUbN classifier can better separate P and bN data.\n\n- For the question \"why does our method performs better than nnPNU\"\nWe think that is because the nnPNU algorithm does not take into account the fact that our labeled N data are biased.\n\n- The original section 4.3\nIt studies the behavior of our algorithm when no bN data are available, so we do not think that it is related to you question. It is now moved to appendix because it is less related to the main problem studied by our paper.", "title": "1. Response to [Why adding biased negative data will further improve upon nnP(N)U.]"}, "BkeevSvD6m": {"type": "rebuttal", "replyto": "SJga3o753Q", "comment": "Dear Reviewer,\n\nThank you very much for your insightful comments. Instead of answering all the questions at once, we will address the issues that you raise point by point, and also revise the paper accordingly.", "title": "Thank you for your insightful comments, we will address your concerns as quickly as possible"}, "ryeWcKMPaX": {"type": "rebuttal", "replyto": "H1ggpksxaQ", "comment": "Dear Reviewer2,\n\nThank you very much for taking your time to review our paper. Your concise summary of our paper shall be very helpful to anybody who is interested in our work. Below we address the two concerns raised at the end of the review. \n\n** 1. [Nature of the estimator of the posterior probability \u03c3\u0302] **\n\n- As for the learning algorithm, as mentioned in your comment, we use s as label and trained a probabilistic classifier to separate s=+1 and s=-1 by leveraging PU learning algorithms. See section 3.1, Estimating \u03c3:\n[In other words, here we regard X_P and X_bN as P and X_U as U, and attempt to solve a PU learning problem by applying nnPU.]\n\n- As for the model, we use always the same model for \u03c3\u0302 and the main classifier. See section 4.1:\n[For simplicity, in an experiment, \u03c3\u0302 and g always use the same model and are trained for the same number of epochs.]\n\n- By the way, we will put our code on github after the reviewing process to ensure that all the experiments in our paper can be easily reproduced by anybody who is interested.\n\n** 2. [In appendix B, choosing \u03c3\u0302=0 will minimize the criterion] **\n\n- This is not true, the criterion \\hat{J} is minimized when \u03c3\u0302~\u03c3. The criterion is the empirical approximation of E_{x~p(x)}[|\u03c3\u0302(x)-\u03c3(x)|^2] plus some constant that is independent of \u03c3\u0302. Due to the presence of this constant term, which is negative, \\hat{J} can be negative and is not minimized when \u03c3\u0302=0 in which case we have \\hat{J}=0.\n\nFinally, we would like to thank you again for your detailed feedback and great summary of our paper.", "title": "Thank you very much for your review, code to reproduce our results will be publicly available"}, "H1ggpksxaQ": {"type": "review", "replyto": "H1ldNoC9tX", "review": "The authors \frst present standard binary (positive negative or PN) classi\fca-\ntion, followed by positive unlabeled (PU) classi\fcation, that they motivate with\nexamples, such as one-class remote sensing classi\fcation. The new setting that\nthey introduce and study is called positive unlabeled biaised negative (PUbN\nclassi\fcation) and adds a biaised negative sample to PU learning. They give\nmotivating examples and compare this setting to the existing literature. A con-\nvincing case is made regarding the di\u000berence between the PUbN problem and\nthe known problems of semi-supervised learning and dataset shift.\nThey start by recalling the notations and nature of standard binary classi\f-\ncation, PU classi\fcation and the nnPU (non-negative PU) strategy, as in the\nprevious PU learning papers. Then, they present the semi-supervised setting\nunder the name PNU learning, which simply studies the minimization of a con-\nvex combination of the PN risk and the PU risk. As in PU learning, a correction\nexists to avoid considering the estimate of the negative risk to be negative, re-\nferred to as nnPNU.\nFinally, the authors introduce PUbN learning as the problem in which we\nonly have access to negatives that follow the law p(x\\mid y = -1; s = +1), where s\nis a latent variable that formalizes the bias.\nAs in PU learning, the authors derive an unbiased estimator of the risk that\ninvolves only distributions for which data is available. However, they need\nto reweight the P and bN distribution by the unknown posterior probability\n\u001bsigma(x) = p(s = +1\\mid x) of s. Considering s as the label, the problem of learning\na probabilistic classi\fer separating the elements for which s = +1 and s = \udbc0\udc001\ncan be seen as a PU learning problem, which gives an estimator ^\u001b of sigma,\nand makes the method practical.\nThey derive estimation error bounds, that depend on the mean squared\ndi\u000bfference between \u001bsigma and sigma^\u001b and a term of order n^-1/2 where n is the cardinal\nof the smallest sample. They considered the function ^\u001b as a \fxed function in\ntheir bounds, which implies that the bounds are only true if some of the data is\nkept for the estimation of sigma^\u001b. Finally, they present a variant of their algorithm\nfor PU learning, named PUbNnN where unlabeled instances are not all given\nthe same weight, but weighted according to sigma hat\u001b. The experiments use neural networks with stochastic optimization, on the classic datasets MNIST, CIFAR-10 and 20 Newsgroup. They report better per-\nformance using their technique on all datasets. The authors documented their\nexperiences thoroughly in the appendix. However, I did not \ffind information\nabout the nature of the estimator of the posterior probability sigma^\u001b, which is im-\nportant for reproducibility. Furthermore, in appendix B, choosing sigma^\u001b = 0 will\nminimize the criterion . Finally, they proceed to justify the dominance of\nthe variant of their method over usual nnPU learning.", "title": "Paper correct and carefully written but results rather straightforward", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HkxQIauh2Q": {"type": "review", "replyto": "H1ldNoC9tX", "review": "This paper studied classification problem, with Positive, Unlabeled and biased Negative labeled data. The paper presents a two-step method, where the first-step is instance weighting and the second-step is standard binary classification. The paper shows theoretical proofs on the error estimation. Experiments on several well-known data sets are conducted and compared. \n\nThe good things of the paper are clear. \n\n1.\tTechnical sound with statistical foundation\n2.\tTheoretical foundation\n3.\tProblem is general\n4.\tPaper is general well written.\n\nSome weak points as well\n1.\tApplication value is not so big, as there is no real application problem and the experiments are based on simulation.\n2.\tAlthough the studied problem is reasonable, the setup is a bit too general and need rather strict condition to have a good method. \n", "title": "This paper studied classification problem, with Positive, Unlabeled and biased Negative labeled data. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJga3o753Q": {"type": "review", "replyto": "H1ldNoC9tX", "review": "This paper has proposed a new algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework.\n\nThe paper was written in clarity and easy to follow overall. However, the original motivation for having biased negative data are not explained very clear. The relation to dataset shift was very interesting, but it\u2019s unclear what\u2019s the exact connection between the proposed algorithm and the dataset shift. Maybe the authors can elaborate a little more on their point here in the future revision.\n\nThe paper has made some assumption about the relation between the latent random variable and the label in section 2.4. In the experiment, data sets are generated following the exact assumption. That\u2019s not surprising to see that the proposed algorithm that fits the assumption will perform better than the previous methods without this assumption. In practice, there\u2019s no way to really verify this assumption. Thus, it\u2019s more interesting to see how the algorithm performs under the more generic semi-supervised learning setting, with unbiased, or biased negatives that don\u2019t really fit the exact assumption in this paper.\n\nMoreover, I\u2019d like to see more intuition on why adding biased negative data will further improve upon nnPNU. The author provided some explanation in section 4.3, which seems just observations on the FPR and FNR, rather than the fundamental explanation for the advantage of this algorithm.\n\nChoice of baseline methods is also limited. The original paper [1] for PNU has included a bunch of benchmark algorithms for semi-supervised learning. The authors should also include more benchmark algorithms for comparison, e.g. those listed in Section 5.2 in [1].\n\n[1] Sakai, Tomoya, et al. \"Semi-supervised classification based on classification from positive and unlabeled data.\" arXiv preprint arXiv:1605.06955 (2016).", "title": "I'm not convinced by the main assumption, it still needs more work to get published.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}