{"paper": {"title": "Latent Topic Conversational Models", "authors": ["Tsung-Hsien Wen", "Minh-Thang Luong"], "authorids": ["thw28@cam.ac.uk", "thangluong@google.com"], "summary": "Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.", "abstract": "Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global \u201ctopic\u201d distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n", "keywords": ["conversational modeling", "dialogue", "chitchat", "open-domain dialogue", "topic model", "neural variational inference", "human evaluation", "latent variable model", "gaussian reparameterisation trick"]}, "meta": {"decision": "Reject", "comment": "This paper combines existing models to detect topics and generate responses, and the resulting model is shown to be slightly preferred by human evaluators over baselines. This is quite incremental and the results are not impressive enough to stand on their own merit."}, "review": {"By_Kd9frz": {"type": "rebuttal", "replyto": "rkTnC_MVG", "comment": "Thank you for the comments. We used Adam with an initial learning rate of 0.1. The convergence varies case-by-case, but most of the models start to converge around 7-8th epoch.", "title": "Learning rate and epoch"}, "S1B9wqGrz": {"type": "rebuttal", "replyto": "HyY3SM9NM", "comment": "The main contribution of the paper is three-fold as mentioned in this post (\u2013 General comments on Contributions\u2013): \n1) We were first to be able to jointly learn the neural topic and seq2seq models.\n2) The paper offers a better understanding/training of latent models for languages.\n3) Both an extensive evaluation and a comprehensive analysis were conducted to validate the results.", "title": "Contribution/Novelty of the paper"}, "H1E8RNcxz": {"type": "review", "replyto": "S1GUgxgCW", "review": "This paper proposed the combination of topic model and seq2seq conversational model.\nThe idea of this combination is not surprising but the attendee of ICLR might be interested in the empirical results if the model clearly outperforms the existing method in the experimental results.\nHowever, I'm not sure that the empirical evaluation shows the really impressive results.\nIn particular, the difference between LV-S2S and LTCM seem to be trivial.\nThere are many configurations in the LSTM-based model.\nCan you say that there is no configuration of LV-S2S that outperforms your model?\nMoreover, the details of human evaluation are not clear, e.g., the number of users and the meaning of each rating.\n\n", "title": "The evaluation and details of experiments are not sufficient.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rypOZF5eG": {"type": "review", "replyto": "S1GUgxgCW", "review": "The paper proposes a conversational model with topical information, by combining seq2seq model with neural topic models. The experiments and human evaluation show the model outperform some the baseline model seq2seq and the other latent variable model variant of seq2seq.\n\nThe paper is interesting, but it also has certain limitations:\n\n1) To my understanding, it is a straightforward combination of seq2seq and one of the neural topic models without any justification.\n2) The evaluation doesn't show how the topic information could influence word generation. No of the metrics in table 2 could be used to justify the effect of topical information.\n3) There is no analysis about the model behavior, therefore there is no way we could get a sense about how the model actually works. One possible analysis is to investigate the values $l_t$ and the corresponding words, which to some extent will tell us how the topical information be used in generation. In addition, it could be even better if there are some analysis about topics extracted by this model.\n\nThis paper also doesn't pay much attention to the existing work on topic-driven conversational modeling. For example \"Topic Aware Neural Response Generation\" from Xing et al., 2017.\n\nSome additional issues:\n\n1) In the second line under equation 4, y_{t-1} -> y_{t}\n2) In the first paragraph of section 3, two \"MLP\"'s are confusing\n3) In the first paragraph of page 6, words with \"highest inverse document frequency\" are used as stop words?", "title": "interesting combination of seq2seq and neural topic models, but weak evaluation", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1ahhhJWM": {"type": "review", "replyto": "S1GUgxgCW", "review": "I enjoyed this paper a lot. The paper addresses the issue of enduring topicality in conversation models. The model proposed here is basically a mash-up between a neural topic model and a seq2seq-based dialog system. The exposition is relatively clear and a reader with sufficient background in ML should have no following the model. My only concern about the paper is that is very incremental in nature -- the authors combine two separate models into a relatively straight-forward way. The results do are good and validate the approach, but the paper has little to offer beyond that.  ", "title": "topic modeling + seq2seq", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJeDIJaXG": {"type": "rebuttal", "replyto": "S1GUgxgCW", "comment": "The authors would like to notify reviewers about the newest update of the paper where a quick analysis of the learned topic gate $l_t$ has been added to the paper based on reviewer1's request.", "title": "Notification of the newest paper update."}}}