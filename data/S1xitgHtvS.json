{"paper": {"title": "Making Sense of Reinforcement Learning and Probabilistic Inference", "authors": ["Brendan O'Donoghue", "Ian Osband", "Catalin Ionescu"], "authorids": ["bodonoghue85@gmail.com", "iosband@google.com", "cdi@google.com"], "summary": "Popular algorithms that cast \"RL as Inference\" ignore the role of uncertainty and exploration. We highlight the importance of these issues and present a coherent framework for RL and inference that handles them gracefully.", "abstract": "Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts \u2018RL as inference\u2019 and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular \u2018RL as inference\u2019 approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.\n", "keywords": ["Reinforcement learning", "Bayesian inference", "Exploration"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "The paper explores in more detail the \"RL as inference\" viewpoint and highlights some issues with this approach, as well as ways to address these issues. The new version of the paper has effectively addressed some of the reviewers' initial concerns, resulting in an overall well-written paper with interesting insights."}, "review": {"ohHi5BPwY": {"type": "rebuttal", "replyto": "gD2zMD1Ou", "comment": "I do not think we were aware of your paper, but we will make sure to investigate this!\n\nDo you have a summary for how you feel this work is related? Maybe we can have this discussion directly via email?\n\nMany thank!", "title": "Thank you for the reference"}, "S1gAoVaTYH": {"type": "review", "replyto": "S1xitgHtvS", "review": "The paper at hand presents an alternative view on reinforcement learning as probabilistic inference (or equivalently maximum entropy reinforcement learning). With respect to other formulations of this view (e.g. Levine, 2018; I am referring to the references of the paper here), the paper identifies a shortcoming in the disregard of the agent\u2019s epistemic uncertainty (which seems to refer to the uncertainty with respect to the underlying MDP). It is argued, that algorithms based on the prevailing probabilistic formulation (e.g. soft Q-learning) suffer from suboptimal exploration.\nThe paper thus compares maximum entropy RL to K-learning (O\u2019Donoghue, 2018), which is taken to address the issue of suboptimal exploration due to its temperature scheduling and its inclusion of state-action pair counts in the reward signal. \n\nAs its technical contribution, the paper re-interprets K-learning via the latent variable denoting optimality employed in Levine (2018) and introduces a theorem bounding the distance between the policies of Thompson sampling and K-learning. Empirical validation of the claims is provided via experiments on an engineered bandit problem and the tabular MDP (i.e. DeepSea from Osband et al., 2017), as well as via soft Q-learning results on the recently suggested bsuite (Osband et al., 2019).\n\nI consider this paper a weak reject. This is in light of me finding it very hard to follow the papers main claims and arguments, even though it positions itself as communicating connections (\u201cmaking sense\u201d) in prior work, rather than presenting a novel algorithm. While this is in part due to the complicated issue and math being discussed (and the paper probably catering to a very narrow audience), the paper in its current state does seem to hinder understanding as well.\n\nOn the positive side, I do appreciate the intention of the paper, namely to connect RL as probabilistic inference, Thompson sampling and K-learning. In my opinion, this can be taken as a valuable addition to the current understanding of these approaches. Also, I like the experiments as they are specifically constructed to support the claims of the paper.\nOn the negative side, vague language, missing assumptions and lax notation seem to hinder the understanding of the paper to a considerable extend: e.g. it is stated, that \u201cwe connect the resulting algorithm with [\u2026] K-learning\u201d. However, I do not recognize a new algorithm being provided. Instead the paper argues in favor of K-learning. The assumptions that come with K-learning are not mentioned. The restriction of K-learning to tabular RL is taken to be understood implicitly (whereas RL as probabilistic inference seems applicable with function approximation also, which is not mentioned in the comparison). The paper always talks of shortcomings (plural) of RL as probabilistic inference, but only provides one argument (suboptimal exploration) with respect to this. RL as probabilistic inference is introduced in a different form as in prior literature (i.e. Equation 6), while the derivation in the Appendix spanning the differences in notation being hard to follow due to (maybe minor?) notational issues (e.g. x and y seem to have replaced s\u2019 and a; further down there is a reference to Equation 7, however probably it is meant to be 8 and even that with some leap in notation).\nThe paper would benefit from better proof-reading, where mistakes in a very dense argumentation make it hard to follow (e.g. I do not understand the sentence \u201cThe K-learning expectation (7) is with respect to the posterior over Q[\u2026] to give a parametric approximation to the probability of optimality.\u201d)\n\nLiterature wise, the paper draws heavily from two unpublished papers (Levine, 2018; O\u2019Donoghue, 2018). While this makes it harder to arrive at a high confidence level with respect to the paper\u2019s claims, I would not argue this to be critical.\nI would consider raising my score, if the authors would improve the accessibility of the paper by polishing the argumentation and notation. \n\nConfidence: low. It is very likely, that I have misunderstood key arguments and derivations. Also, I did not attempt to follow all of the technical derivations.\n\n\n======\npost rebuttal comment:\n\n\nI changed the score of my review in light of the rebuttal.\nThe changes made to the paper overall address my concerns.\nI do consider the additional explanations and re-phrasings as well as the improved notation a nice improvement of the paper.\nWhile I did not read all of the appendix, Section 5.1 is much more readable and understandable in the new version.\n\nIn light of this paper probably being published, I share some typos/inconsistencies I still noticed:\n\np. 4: the solving for -> solving for the\np. 7: s_{h+1} -> s' (in Table 3) ?\np. 7: table -> Table; tables -> Tables\np. 7: (Fix position of K:) \\pi_h(s)^K -> \\pi_h^K(s) ((also in Appendix))\np. 9: (2x) soft-Q learning -> soft Q-learning; Q Networks -> Q-Networks; Soft Q -> soft Q-learning\n\n\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "rkevha9_ir": {"type": "rebuttal", "replyto": "S1xZvxv4sS", "comment": "Thanks again to the reviewers, we have just uploaded a new revision which addresses most of the reviewers concerns.", "title": "Uploaded new revision"}, "ryx5s-wNoB": {"type": "rebuttal", "replyto": "ryez-G1Lqr", "comment": "Thank you for your review and for your suggestions. We completely agree that the experiments needed to be improved, in particular the bsuite ones. We have put a significant amount of work into improving them, and making it clear what the point of each one was, as well as presenting the results of the experiments more clearly. Moreover, we have added new experiments using function approximation for both K-learning and Thompson sampling which suggest that the claims we make in the paper in the tabular case carry over to the function approximation case, at least empirically.\n\nWe do not make any claims about the advantages of K-learning or Thompson sampling over each other, they appear to have similar performance empirically, and the best RL Bayesian regret bounds for each algorithm are identical (up to logarithmic terms). We have tried to be clearer about this in the manuscript. The reason we mention them is to highlight the differences (and similarities where appropriate) between these two techniques and the RL as inference framework, which is unable to handle epistemic (Bayesian) uncertainty.\n\nThank you also for the minor remarks and your eagle eye in spotting typos, we have corrected or clarified all of them!\n", "title": "Re: Official Blind Review #3 "}, "HJg7MbPNjH": {"type": "rebuttal", "replyto": "S1gAoVaTYH", "comment": "Thank you for your careful and thorough review. We have tried to address your major concern, which was the lack of accessibility of the paper, by\n\nMaking the assumptions clearer\nFixing the notation\nImproving the clarity of the language overall.\n\nWe have also tried to make it clear that we are not proposing a new algorithm, merely highlighting a shortcoming about RL as inference, and contrasting with alternative approaches. Moreover, it is not our intention to claim that K-learning is by any means the solution to the exploration problem, just that it, along with Thompson sampling (and other algorithms that we don\u2019t focus on) do actually explore in a directed manner. We also make the connection of K-learning to the current \u2018RL as inference\u2019 framework as it is currently understood in the literature. The reason we do this is primarily because the RL as inference framework has inspired many new and interesting algorithms in many subfields of RL, including hierarchical RL, options / skills, multi-agent, empowerment etc. We hope that highlighting a major issue that the framework suffers from, and demonstrating that a fix is possible, that the performance of these algorithms can be improved further with (hopefully) relatively little change.\n\nAs for the tabular vs function approximation issue, we do have a section entitled \u2018Why is RL as inference so popular?\u2019, in which we say \u2018Further [the RL as inference derived algorithms] are often easy to implement and amenable to function approximation\u2019. We want to stress that we understand that the RL as inference framework actually has a lot of value, but that the issue of sub-optimal exploration needs to be addressed. It is an open question as to how best to implement Thompson sampling and K-learning with function approximation - we have made this point more clear, though in response to your concern we have added some experiments with function approximation for both K-learning and Thompson sampling. These experiments suggest that it is at least possible.\n\nYou are correct that we have slightly modified the presentation of RL as inference. This is to make it easier to compare with the other techniques we compare against. However, the formulation is not new. The paper by Deisenroth et al. 2013 \u201cA survey on policy search for robotics\u201d uses the same presentation. In section 2.4.2.2 the authors of that paper propose P(O = 1| tau) \\propto exp(R(tau)), where R(tau) is the reward along the entire trajectory tau (they use an overloaded R instead of O to denote \u2018optimality\u2019, but other than that it is identical). The point here is that although the presentations differ ultimately the framework is the same. We have updated the comment to be clearer on this, and fixed the derivation of soft Q-learning in the appendix.", "title": "Re: Official Blind Review #2 "}, "ryx8nxwNsr": {"type": "rebuttal", "replyto": "B1eIQQVUtS", "comment": "Thank you for your detailed review, and for your kind words! In response to your concerns:\n\nYou are right, this was confusing. Indeed the *true* MDP is sampled only once and is the same for all learning thereafter, though Thompson sampling samples an MDP from the posterior at each iteration as part of the learning. We have clarified this earlier in the paper.\n\nWe have tried to merge all three algorithms into a single table, but it\u2019s quite dense. We\u2019ll keep experimenting with it and hopefully we\u2019ll have a good compromise by the time we resubmit a revision, which we\u2019re hoping to upload in the next couple of days, though it might just be the case that three separate tables is cleanest unfortunately!\n\nWe have made efforts to clean up the notation, especially with respect to the derivation of soft Q-learning.\nWe have added a form of deep K-learning and Bootstrapped DQN to the results of section 4.3 to suggest that our claims likely carry over to the function approximation case.\n\nYou are right, that\u2019s not the correct reference at that point, we have updated it to the Eysenbach 2018 paper.\n", "title": "Re: Official Blind Review #1"}, "S1xZvxv4sS": {"type": "rebuttal", "replyto": "S1xitgHtvS", "comment": "We would like to thank the reviewers for their careful consideration of our paper. All three reviewers highlighted important points that, once addressed, will improve the quality of our manuscript. We would also like to thank the reviewers for their kind words on the value of the paper. In particular both R1 and R3 both highlight the overall clarity of exposition and how the minimal examples highlight the issues we want to address, and R2 noted that the paper will be a valuable addition to the current understanding of RL and inference.\n\nWe have responded below to each reviewer and we have outlined the changes to the paper we will make in response. We are hoping to upload a new revision in the next couple of days. Overall, we are putting in a significant effort to improve clarity, notation, and accessibility. Furthermore, we have significantly improved the experiments, which we agree were confusing. In particular, we have added a deep implementation of K-learning and a deep variant of Thompson sampling (both using neural net function approximation) to the experimental results. The take home message is that both these implementations improve over soft-Q-learning when it comes to exploration, even when using function approximation.\n", "title": "Thank you to the reviewers"}, "B1eIQQVUtS": {"type": "review", "replyto": "S1xitgHtvS", "review": "This paper criticizes the \u2018RL as inference\u2019 paradigm by highlighting its limitations and shows that a variant of this framework - the K-learning algorithm (O'Donoghue et. al., 2018) does not have these limitations. The paper first clarifies some points of confusion regarding RL as inference, namely the fact that RL was originally an inference problem all along. A simple example is used to demonstrate that the RL as inference framework (Levine, 2018) fails to choose the optimal actions that resolve epistemic uncertainty, whereas the K-learning algorithm does select the optimal action. Further, a connection is made which reveals that K-learning is an approximate version of Thompson sampling - the strategy of using as single posterior sample of parameters given data for greedy actions which originated in bandit settings. Some empirical results are provided highlighting the cases where Soft Q-learning (Levine, 2018) fails but Thompson sampling and K-learning do not.\n\nI vote for accepting this paper as it brings to light an important limitation of the popular RL as Inference framework with a didactic example which, to the best of my knowledge, has not been shown before.\n\nThe paper does a great job at succinctly introducing a simple bandit problem where the bayes-optimal policy is to take a first action that is supposed to immediately resolve all epistemic uncertainty and then exploit the optimal action repeatedly for future plays. However, this simple problem is designed in such a way that there are several other sub-optimal actions which make the RL as inference algorithm have an exponentially low probability of selecting the optimal action. This implies that RL as inference, unlike Thompson sampling, does not in fact take into account epistemic uncertainty.\n\nFeedback to authors:\n- The introduction of family of MDPs caused a lot of confusion about the problem setting. I was not sure if a new MDP is sampled from \\phi at every episode in L or a single MDP is sampled and kept the same throughout. This was clarified later on in the middle of section 2.1, but it could have been introduced more carefully earlier on,\n- The tables 1-3 summarizing algorithms are useful but it would be great if there could be a side by side comparison of all three in a single table.\n- The notation is very dense and I see that efforts were made to avoid this, but it still feels inaccessible.\n- I am not sure of the role of experiments in section 4.3, if there is no comparison to K-learning. I understand that the authors leave it to future work but then the experiments feel out of place.\n- \u201c... RL as inference has inspired many interesting and novel techniques, as well as delivered algorithms with good performance on problems where exploration is not the bottleneck (Gregor et al., 2016)\u201d. I think this sentence is false, Gregor et. al. do not employ RL as inference anywhere in their paper. Also, I don\u2019t think the point of their paper was to show good performance on any problem. Maybe this was mixed up with Eysenbach, 2018, a successor paper which uses RL as inference?\n\n \n\nReferences:\nO'Donoghue, Brendan. \"Variational Bayesian Reinforcement Learning with Regret Bounds.\" arXiv preprint arXiv:1807.09647 (2018).\n\nLevine, Sergey. \"Reinforcement learning and control as probabilistic inference: Tutorial and review.\" arXiv preprint arXiv:1805.00909 (2018).\n\nGregor, Karol, Danilo Jimenez Rezende, and Daan Wierstra. \"Variational intrinsic control.\" arXiv preprint arXiv:1611.07507 (2016).\n\nEysenbach, Benjamin, et al. \"Diversity is all you need: Learning skills without a reward function.\" arXiv preprint arXiv:1802.06070 (2018).", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 2}, "ryez-G1Lqr": {"type": "review", "replyto": "S1xitgHtvS", "review": "The authors develop a criticism of the \"RL as inference\" standard approximations and propose a simple modification that solves its main issues while keeping hold of its advantage. Even though this modification ends up relating to a previously published algorithm, I judge the submission to be worthwhile publishing for the following contributions:\n- clarity/didacticism of the exposition, the minimal problem, the positioning,\n- the theorem,\n- the (hopefully to be completed) experiments\n\nThe experiments are my main criticism of the paper, in particular the bsuite ones that was absolutely impenetrable for me: not only the experiments but also the results. I hope this will be completed in the final version. It was also a bit unclear to me the advantage of K-learning over Thomson sampling methods.\n\nMinor remarks and typos:\n- famliy => family\n- I would not say that frequentist RL is the worst-case, but more high-probability (it's the worst case within the concentration bounds).\n- the agent in then => the agent is then\n- KL has 2 meanings in the notations: K-learning and KL divergence. For clarity, I suggest to use only K for K-learning (for instance).", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}}}