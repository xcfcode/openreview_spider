{"paper": {"title": "Self-Supervised Variational Auto-Encoders", "authors": ["Ioannis Gatopoulos", "Jakub Mikolaj Tomczak"], "authorids": ["johngatop@gmail.com", "~Jakub_Mikolaj_Tomczak1"], "summary": "We present a novel class of generative models, called self-supervised Variational Auto-Encoder, where we improve VAEs by applying deterministic and discrete transformations of data.", "abstract": "Density estimation, compression, and data generation are crucial tasks in artificial intelligence. Variational Auto-Encoders (VAEs) constitute a single framework to achieve these goals. Here, we present a novel class of generative models, called self-supervised Variational Auto-Encoder (selfVAE), that utilizes deterministic and discrete transformations of data. This class of models allows performing both conditional and unconditional sampling while simplifying the objective function. First, we use a single self-supervised transformation as a latent variable, where a transformation is either downscaling or edge detection. Next, we consider a hierarchical architecture, i.e., multiple transformations, and we show its benefits compared to the VAE. The flexibility of selfVAE in data reconstruction finds a particularly interesting use case in data compression tasks, where we can trade-off memory for better data quality, and vice-versa. We present the performance of our approach on three benchmark image data (Cifar10, Imagenette64, and CelebA).", "keywords": ["generative modeling", "deep learning", "deep autoencoders"]}, "meta": {"decision": "Reject", "comment": "Reviewers appreciated the model and the ideas presented and found them very interesting.\n\nThe main reason for rejection is the extent of the empirical work.  Unfortunately, and I think what is a bad sign for the ICLR community, the authors could not do adequate empirical work due to their computational resources.  Not belonging to an organisation with extensive computational resources myself, I am in strong symparthy with the authors, though I do not see any way this can be satisfactorily accounted for in reviewing.  Several reviewers commented on the datasets, the extent of evaluations, and the comparisons made with prior work.  For instance, the small CIFAR10 images are not ideal to demonstrate the technique and comparative results with the other data sets are limited.\n\nThe reviewers had a number of concerns on the theoretical work and these were well discussed by the authors.\n\nIn summary, this is promising research but needs more empirical work.\n\n\n"}, "review": {"s2btPNPN3I_": {"type": "review", "replyto": "zOGdf9K8aC", "review": "\n## Summary\n\nThe paper presents a self-supervised variational auto-encoder called selfVAE. The work proposes the use of downscaling and edge detection as simpler representations of the input images to be reconstructed. The model should then learn to improve the low dimensional approximations to recover the higher dimensional ones in a hierarchical fashion. \n\n## Quality & Clarity\n\nThe paper is generally quite difficult to follow and the purpose, contributions and experiments are not presented clearly enough. The figures are not discussed in order, and the paper often references figures that are far away.\n\nThere are a number of grammatical errors in the paper.\n\n## Outcome\n\nThe message of the paper generally was quite unclear and it could do with restructuring to assist readers.", "title": "Difficult to follow", "rating": "5: Marginally below acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "ijZNyPSULiT": {"type": "rebuttal", "replyto": "ZPKmNgEG8bx", "comment": "We would like to thank the reviewer for very interesting and insightful comments. They helped us to significantly improve the paper.\n\n**Point 1**\n\nWe realized that after submitting the paper. It is true, it is not necessary to consider $y=d(x)$ as a part of a variational posterior. Instead, we can consider a joint distribution $p(x,y)$ that we can factorize as $p(y|x) p(x)$, where $p(y|x) = \\delta(y - d(x))$. As a result, we can obtain both x\u2019s and y\u2019s as training data. However, while modeling, we are interested in the other factorization, namely, $p(x,y) = p(x|y) p(y)$, and then we can apply the variational inference with z\u2019s and u\u2019s as latent variables. We have rewritten the paper accordingly.\n\n**Point 2**\n\nYes, we also realized this after submitting the paper. There were multiple errors. In the new version of the paper, we have corrected them.\n\n**Point 3**\n\nThis is true, we have removed the awkward statement in the paragraph with contributions. At the very beginning of this project, we decided to first train a vanilla VAE that achieves bpd on CIFAR10 as close as possible to flow-based models (e.g., RealNVP). Therefore, we put a lot of effort into that. This included checking various architectures of encoders and decoders, as well as various priors. Once we determined the best performing model, we adapted it to the selfVAE while keeping a similar number of parameters for a fair comparison. As a result, we always trained the vanilla VAE first, and then, afterward, we trained the selfVAE. \nOur very initial experiments indicated that the difference between using a bijective prior vs. the standard Gaussian prior in selfVAE was similar to the same situation in a vanilla VAE. Therefore, we skipped carrying out similar experiments with selfVAE due to limited computational resources.\n\n**Point 4**\n\nIn this section, we indeed list different manners of utilizing our approach for generating and reconstructing images. Since a vanilla VAE allows us to generate and reconstruct images in a single way, we wanted to highlight that incorporating self-supervised representations gives us more flexibility. In the new version of the paper, we added additional comments that these new ways of reconstructing open new perspectives for the compression task. Moreover, we have removed the corresponding figure, because we realized it could confuse a reader.\n\n**Point 5**\n\nDue to the double-blind policy, we cannot explain our situation in depth. However, we want to highlight that we had limited access to computational resources. We are aware that this is not a good excuse, therefore, we did our best to gather as much empirical evidence as possible. As a result, we have experimented on three datasets while two of them contain 64x64 images. We are almost certain that our approach would work even better on larger images. Nevertheless, at this point, we have no empirical evidence to support our belief.\nUnfortunately, we were unable to train larger models than 35-40M parameters. Currently, some papers report SOTA bpd for VAEs with over 100M parameters like NVAE or BIVA on CIFAR10. We include them in our comparison even though we cannot compare properly with them.\nWe agree that providing only the VAE as a baseline for CelebA and Imagenette might be misleading. After an extensive literature search, we were able to find the bpd score for CelebA only in the RealNVP paper. Surprisingly, CelebA is widely used for the qualitative assessment, however, almost no-one provides the bpd. In the case of Imagenette, we are afraid that no-one else provides the bpd score (we were unable to find any paper).", "title": "Response to AnonReviewer1"}, "TfZQ_wuGrga": {"type": "rebuttal", "replyto": "EZPCIMRKXV_", "comment": "In general, we would like to thank you for your detailed comments. We realized that the paper contains errors and it is ambiguous in some parts. Therefore, we have rewritten it significantly and reformulated the equations.\n\n**Point Hierarchical self-supervised tasks**\n\nBoth CelebA and Imagenette utilize a series of downscaling transformations (x: 64x64 -> y1: 32x32 -> y2: 16x16). Therefore, we used hierarchical selfVAE in these experiments.\nWe did not use sketching in a hierarchical model, because sketching is equivalent to edge detection. Applying an edge detection transformation to an image consisting of detected edges does not provide any new information. However, it is possible to combine various transformations, e.g., downscaling and then sketching. Unfortunately, we did not have enough computational resources to carry out these experiments.\n\n**Point How to train/balance operations**\n\nWe realized that this figure is rather confusing, and, therefore, we have removed it. The whole subsection on \u201cGeneration and Reconstruction in selfVAE\u201d is not about training, it indicates that selfVAE allows us to perform reconstruction and generation in different ways. The training procedure is the same as in any other VAE, namely, we update weights of the encoder, the decoder, and the prior by optimizing the appropriate ELBO.\n\n**Point Performance of self-supervised tasks**\n\nWe have tested all models (i.e., selfVAEs) on the density estimation task only. We do not have any experiments on tasks like super-resolution. It is an interesting research direction, and we are sure that our framework could provide a great platform for such tasks.\n\n**Point What is RE and KL in Table & FID scores**\n\nWe realized that both RE and KL could be misleading. Therefore, we have removed them from the paper.\nWe have used the WGAN since it is typically reported as a GAN-based baseline in other papers on the likelihood-based models. We did an additional search for a work that achieves state-of-the-art FID scores. As a result, we have replaced the WGAN with the following model:\n- Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33.\nThis model (DDPM) achieves FID that is better than any other GAN, and it is the likelihood-based model. Therefore, for coherence of the comparison, we excluded the WGAN and we decided to exclude any GAN-based model from the table. Nevertheless, we are thankful for providing a couple of relevant papers.\nDue to the double-blind policy, we cannot explain all details of how the project was carried out. Unfortunately, due to some circumstances, we were unable to calculate the scores.\n\n**Point No other models on CelebA**\nIt is rather surprising, but it is very hard to find a paper that reports bpd on CelebA. There is a vast of papers that use this dataset, but they provide only samples or FID scores. Eventually, after an extensive literature search, we were able to find a score (bpd) in the RealNVP that we included in the new version of the paper. We fully agree that having at least one baseline positions our results better.\n\n**Point on Imagenette64**\n\nWe have indicated in the paper that it is a version of the ImageNet dataset provided by FastAI. However, it is obtained in a slightly different manner than the ImageNet64. First, downscaling is done differently. Second, it is a subset of ImageNet64. We did our best, but we were unable to find any other paper that used Imagenette for density estimation.\nWe have used Imagenette instead of ImageNet due to limited computational resources. We are aware that this is not the best excuse, but we decided that it is better to provide another dataset that consists of 64x64 images rather than working on simpler models like MNIST or Omniglot that contain 28x28 images.\n", "title": "Response to AnonReviewer3"}, "-kYPs1asE2D": {"type": "rebuttal", "replyto": "s2btPNPN3I_", "comment": "The only suggestion following from this review is that the paper is hard to follow. We have rewritten many parts of the paper and we do hope that it is more readable now.", "title": "Response to AnonReviewer4"}, "0o04AuiIvJ": {"type": "rebuttal", "replyto": "kmPCa5FOsV", "comment": "First of all, we would like to thank you for your kind words and insightful comments. Please find our response below:\n\n**Point 1**\n\nThis is true that in many papers the decoder, i.e., $p(x|\\ldots)$, is modeled as the Gaussian distribution. Unfortunately, this is not correct in the case of images that are represented by integers in $\\{0, 1, \\ldots, 255\\}$. Variational Auto-Encoders are so-called prescribed latent variable models in which all probability distributions must be defined upfront. Moreover, all distributions must be properly chosen for random variables. By \u201cproperly\u201d we mean appropriately to the values that a random variable can take. For instance, Gaussian distribution, which has the support between $-\\infty$ and $+ \\infty$, mustn\u2019t be used for integers. We are aware that it is a common practice to use Gaussians for images, which results in the MSE loss in deep learning packages, but it is not theoretically grounded. Therefore, we used the discretized logistic distribution, which is appropriate for integers, following other papers in the literature, e.g.:\n- Salimans, T., Karpathy, A., Chen, X., & Kingma, D. P. (2017). Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. ICLR 2017.\n\n**Point 2**\n\nIt is a very interesting remark. In the very beginning of the project, we checked how selfVAE works while being trained in two stages (i.e., for one minibatch, first the encoder and the decoder were updated, and then the bijective prior was updated) against selfVAE being trained in a standard manner (i.e., all weights were updated at once). It turned out that there was no difference, and even the standard training seemed to give better results. Therefore, we used the standard training.\nThe flow-based model is considered hard to train when the target data are of high-dimensionality (i.e. natural images). However, in our case, the target distribution is of lower dimensionality (1024 dimensions) which allows up to incorporate the best of both worlds; a smaller, fast data-dependent prior that does not collapse (please see for ablation study). Given the deterministic transformations, we experienced that the model scales easily while incorporating the bijective prior, without any effort to tune any hyper-parameters. To conclude, given this very reasonable remark, our model was specially designed to scale efficiently and unhesitatingly.\nMoreover, another possible explanation of this result is the following. A vanilla hierarchical VAE can suffer from posterior collapse because z\u2019s on top obtain data that are already processed through multiple stochastic layers. However, in our approach, we provide processed data through deterministic and discrete transformations (e.g., downscaled images), therefore, on top, we have a direct connection to data, and learning a bijective prior p(u) is less problematic.\n\n**Point 3**\n\nIn our opinion, the strength of our approach is that we can use either a conditional generation or an unconditional generation. Therefore, regarding your question, we can use both. In the paper, we highlight whether we present unconditional or conditional samples. \n\n**Point 4**\n\nPartially, we agree with this comment. Applying our approach to CIFAR10 has a rather limited effect because the images are very small (32x32). However, results on 64x64 images (CelebA, Imagenette) are promising and indicate that both multiple downscaling and sketching have a positive effect. Nevertheless, we would love to apply our approach to larger images, but we simply did not possess enough computational power to accomplish that. We are almost certain that our approach would shine even more on 256x256 or larger images.\n\n**Point 5**\n\nThank you for pointing out the paper. We are aware of this paper since it is one of the first successful papers on modeling both image and its label using VAEs. The main difference between the conditional VAE (CVAE) and our selfVAE lies in the quantities we model. In our case, we focus on y that is a transformation of $x$, $y=d(x)$, while in the CVAE case, y is a label (or a segmentation). Further, CVAE models the conditional likelihood, $p(y|x)$, while we model the joint distribution, $p(x,y)$. We have also rewritten our model slightly to make this distinction even more apparent.", "title": "Response to AnonReviewer2"}, "ZPKmNgEG8bx": {"type": "review", "replyto": "zOGdf9K8aC", "review": "This paper focuses on the task of generating high-quality data with generative models. To be specific, the authors proposed a variant of variational autoencoder (VAE) model, named self-supervised VAE. The intuition behind this model is that by breaking down the complex generation task into simpler/smaller ones, complex models can be trained steadily with the guidance from the simpler-level task. To his end, a hierarchical generative model with multiple-level latent variables is proposed, in which lower-level latent variables are governed by lower-level data features. The lower-level feature is generally obtained by a determined and discrete transformation, like down scaling. In addition, to further the modeling capability, a flow-based prior is proposed to fit the data distribution. Experiments were conducted to evaluate the performance of the proposed generative model.\n\n\nStrength:\n1. The idea of guiding the complex image generation with easer tasks is interesting, and is maybe the right way to accomplish complex tasks.\n\n2. the ELOB directed in Eq.2 is intuitive and insightful. It also provides me theoretical support for the fact that employing two-level modeling and downscale transformation to generate a more vivid image is reasonable. \n\n\nWeakness:\n1. From a technical perspective, the proposed method is just the combination of flow-based VAE and auxiliary VAE. By using 3 auxiliary variables, the authors infer one of them by a discrete and determined variational distribution q(y|x) to simplify the training objective, where the downscale image y plays an important role in this model. My question is why not regard y as observed data and then model the joint distribution p(x,y).\n\n2. There are some mistakes in the derivation of Eq 2. In appendix A.4, during computing the entropy of q(w|x), the authors expresses it as E_{q(w|x)}[\\log q(w|x)] = E_{q(z|y,x)}[\\log q(z|y,x)] + ... . However, the first term in RHS is completely wrong. Actually, it should be E_{q(z|y,x)q(y|x)}[\\log q(z|y,x)]. It seems that the authors use an equation in many places, that is E_{q(z|y,x)}[\\log q(z|y,x)] = E_{q(z|y,x)q(u|y)q(y|x)}[\\log q(z|y,x)]. But this equation is not ture, because the term q(z|y,x) insided the expectation is dependent on variable y. Besides, in the choice of distribution p(y|u) and p(x|z,y), they are set to be a mixture discrete logistic distribution. For each image x or y,  are their pixels assumed to be i.i.d ? If so, you miss \\prod_{y_j \\in y} outside the \\sum_{i=1}^{I} in the distribution definition.\n\n3. Bijective prior (RealNVP) is proposed in other works, and here simply employing it should not be regard as a contribution of this paper. Moreover, the authors only compare the effectiveness of different priors (i.e. Gaussian, mixture Gaussian, and RealNVP) on vanilla VAE and confirm the superiority of using an adaptive prior. However, I want to know what is the performance of self-supervised VAE if only using a standard Gaussian prior.\n\n4. Section 3.3 is not presented well and the idea behind the sentences is hard to follow. What are the differences between these generation and reconstruction methods, and what application scenarios are corresponding to them? They are just simply listed, without providing any analysis of the logic behind them.\n\n5. The experimental results cannot support the superiority of the proposed model in both of the quantity and quality comparisons. From the generated images, I cannot see too much difference between the SelfVAE and the vanilla VAE model, without to naming the more superior generative models, like GLOW, GANs etcs. Also, for the quantity comparison, the model is only compared with the outdated vanilla VAE in CelebA and Imagenet64, more recent generative models should be included here.\n\n", "title": "A interesting idea, but the results look not competitive", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "EZPCIMRKXV_": {"type": "review", "replyto": "zOGdf9K8aC", "review": "###################################\nPros:\n\n$\\bullet$ VAEs can ignore some dimensions of the latent code. Enforcing the posterior distributions to consider desired factors of variations in the input can be fulfilled by either making it more structured (i.e., quantization as in VQ-VAE-2) or introducing additional constraints. This paper tackles this problem by applying the latter, two self-supervised tasks: edge maps and downscaled versions of inputs.\n\n$\\bullet$ The idea of adding self-supervised tasks to improve latent representation is very interesting. When learning a more structured latent representation, image superresolution, or sketch-to-image networks are also trained.\n\n###################################\nCons:\n\n$\\bullet$ *Hierarchical self-supervised tasks* In section 3.4, multiple transformations explained. However, none of the experiments are conducted as a consecutive set of transformations. Does 3-level downscale mean a single downscaling three times or generating from $u$ using four different networks and match each of these levels with $z$? If yes, why does not the selfVAE-sketches model apply in a similar way hierarchically?  \n\n$\\bullet$ *How to train/balance operations* In Figure 3, there are several modes of operations given. How did you balance these modes during training?\n\n$\\bullet$ *Performance of self-supervised tasks:* What is the effect of self-supervised tasks' performance on the quality of latent representations? Considering the literature in image superresolution and sketch-to-image, did you use a pretrained auxiliary generator?\n\n$\\bullet$ *What is RE and KL in Table?* Are they the summation of both reconstruction ($RE_x, RE_y$) and KL divergence ($KL_z, KL_u$) terms in the loss (Eq.2)?  The reason why previous methods' RE/KL values were omitted should be stated. Similarly, why were the FID scores on CelebA and ImageNet-64 not given?   \n\nFurthermore, the state-of-the-art FID scores on CIFAR-10 is better than the methods compared in Table 1. For instance, some examples of FID scores on CIFAR-10 are 18.9 in MoML [1], 29.3 in WP-GAN [2], 29.3 in spectrally normalized GAN [3], 26.4 in adversarial score matching [4], and so on.\n\n[1] https://arxiv.org/pdf/1806.11006.pdf\n[2] https://arxiv.org/pdf/1706.08500.pdf\n[3] https://arxiv.org/pdf/1802.05957.pdf\n[4] https://arxiv.org/pdf/2009.05475.pdf\n\nAs the results on CelebA and Imagenet-64 were not compared with previous literature, it is difficult to understand whether the contribution w.r.t. vanilla VAE is due to the self-supervised task or merely the use of an additional stochastic variable ($u$) and networks. \n\nMinor issue: \"Imagenette64\" might cause confusion, I did not see this dataset name before. I suppose that it is \"ImageNet resized to 64x64\" as in PixelCNN paper. References for all datasets should be added.\n\n###################################\nReasons for score: \nOverall, I rate towards rejection. Even though the idea of bijective priors and doing this through self-supervised tasks is a novel approach, my major concern is that it is beyond the state-of-the-art in CIFAR-10, not compared to any other method on CelebA and ImageNet-64. Hopefully, the authors address my concerns above in the rebuttal period. ", "title": "This paper describes a framework that combines Variational Autoencoders (VAE) with self-supervised transformations by adding latent variables such as downscaling and edge detection. The main idea is to match the latent distribution of the original and transformed (downscaled or edge detection) data. Experimental results are done on Cifar-10, ImageNet-64, and CelebA datasets.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "kmPCa5FOsV": {"type": "review", "replyto": "zOGdf9K8aC", "review": "This paper targets richer and higher-quality generation with VAE. Two techniques are adopted to achieve the goal: 1). bijective model to enrich data generation with flexible prior. 2). presenting compressed variants of the input data, i.e. self -supervision as additional condition $y$, for reconstruction. The two techniques interact through a hierarchical sampling process, $... y\\sim p(y|u)\\rightarrow z\\sim p(z|u,y)$, thus benefits VAE generation with data-dependent prior and condition generation. \n\nThe idea novel and reasonable. the paper is clearly presented. Here are some of my concerns.\n\n1.  The author specifically argues the transformation $x \\rightarrow y$ to be 'non-trainable', i.e. the mapping between x and y is deterministic.  BUT, will modeling $q(x|z,y)$ with discretized logistic distribution, affect the generation quality, since the likelihood is classicly assumed to be Gaussian distributed? \n\n2. The HIERARCHICAL SELF-SUPERVISED VAE is presented here to show the model can adopt multi-scaling information to benefit generation step-by-step. However, I  am afraid, in this way, the inference would be much difficult since the flow-based bijective operation is hard to train already. \n\n3. Is the conditional information, e.g. the sketches, also need in the test phase? or unconditional generational setting is adopted here.\n\n4.  It seems the experiments are not conducted on High-quality datasets. To me, the presented results can not obviously demonstrate the achievements of the model. \n\n5. Can you please explain the connection between your self-supervised VAE to the general conditional VAE model in [1].\n\n[1] Sohn, Kihyuk, Honglak Lee, and Xinchen Yan. \"Learning structured output representation using deep conditional generative models.\" Advances in neural information processing systems. 2015.", "title": "Interesting work, but some details need to be further clarified.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}