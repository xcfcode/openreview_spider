{"paper": {"title": "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow", "authors": ["Xue Bin Peng", "Angjoo Kanazawa", "Sam Toyer", "Pieter Abbeel", "Sergey Levine"], "authorids": ["jasonpeng142@hotmail.com", "kanazawa@eecs.berkeley.edu", "sdt@berkeley.edu", "pabbeel@cs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "summary": "Regularizing adversarial learning with an information bottleneck, applied to imitation learning, inverse reinforcement learning, and generative adversarial networks.", "abstract": "Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from raw video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods.", "keywords": ["reinforcement learning", "generative adversarial networks", "imitation learning", "inverse reinforcement learning", "information bottleneck"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a simple and general technique based on the information bottleneck to constrain the information flow in the discriminator of adversarial models. It helps to train by maintaining informative gradients. While the information bottleneck is not novel, its application in adversarial learning to my knowledge is, and the empirical evaluation demonstrates impressive performance on a broad range of applications. Therefore, the paper should clearly be accepted.\n"}, "review": {"S1ljRntcT7": {"type": "rebuttal", "replyto": "rJx9PNrv3X", "comment": "Thank you for the insight and feedback. We have included additional experiments to further compare with previous techniques, along with some additional clarifications.\n\nRe: additional citations\nThank you for the pointers, we have included the additional citations.\n\nRe: GP for other task\nWe have conducted additional motion imitation experiments with GAIL - GP and VAIL - GP [Figure 4, Table 1]. We also added experiments incorporating GP for the inverse RL tasks [Figure 7]. As in image generation, GP does indeed significantly improve the performance of GAIL. However, VAIL still performs better on most of the tasks, and VAIL - GP achieves the best performance overall.\n\nRe: content of batches used to compute KL divergence\nWe have added additional information to the paper to clarify the content of each batch [Section 4 above equation 11]. Each batch of data used to compute the expected KL contains an equal number of real and fake samples. The encoder maps each input sample to an individual distribution in Z. The KL divergence is computed separately for the distribution of each input, and then averaged across the batch, as opposed to computing the KL divergence across samples within a batch. Therefore, if the real and fake distributions are mapped to different parts of the manifold, it should result in a large KL.\n\nRe: saliency maps\nWe have added a colormap to Figure 5. The colors on the saliency map represent the magnitude of the discriminator\u2019s gradient with respect to each pixel and color channel in the input image. The gradients are visualized for each color channel, which results in the different colors. The same procedure is used to compute the gradients for GAIL.\n", "title": "Reply to AnonReviewer1"}, "BJeNdhYq67": {"type": "rebuttal", "replyto": "Bkx6mnnK3Q", "comment": "Thank you for the insight and feedback, we have included new experiments in the paper, along with some additional clarifications.\n\nRe: Adapt beta based on gradient magnitudes\nYes, it might be possible to formulate a similar constraint for adaptively updating beta according to the gradient magnitudes. A constraint on the gradient norm can be added, then a Lagrangian can be constructed in a similar manner to yield an adaptive update for beta.", "title": "Reply to AnonReviewer2"}, "B1eIr2t5Tm": {"type": "rebuttal", "replyto": "Byl41tz9nX", "comment": "Thank you for the insight and suggestions. We have added additional experiments and clarifications to the paper that aim to address each of your concerns -- we would really appreciate it if you could revisit your review in light of these additions and clarifications.\n\nRe: GP for other tasks\nWe have conducted additional motion imitation experiments with GAIL - GP and VAIL - GP [Figure 4, Table 1]. We also added experiments incorporating GP for the inverse RL tasks [Figure 7]. As in image generation, GP does indeed significantly improve the performance of GAIL. However, VAIL still performs better on most of the tasks, and VAIL - GP achieves the best performance overall.\n\nRe: How are VGAN and GP combined\nWe have added an additional section [Appendix B] that provides more information on how VDB and GP is combined. We use the reparameterization trick, as is done in VAEs, to backprop through the encoder to compute the gradient of the discriminator with respect to the inputs. There is a manually specified coefficient that weights the GP term in the objective, and we use the same value for the coefficient as [Mescheder et al., 2018] for image generation.\n\nRe: Combining VGAN and GP enhances performance\nThe VDB and GP are complementary techniques since the VDB helps to prevent vanishing gradients and GP prevents exploding gradients. Therefore both methods regularize the gradients, but under different criteria.\n\nRe: Spectral norm\nWe have included additional image generation experiments with spectral normalization [Figure 8]. Spectral normalization does show significant improvement over the vanilla GAN on CIFAR-10 (FID: 23.9), but our method still achieves a better score (FID: 18.1). The original spectral normalization paper [Miyato et al., 2018] reported an FID of 21.7 on CIFAR-10.\n", "title": "Reply to AnonReviewer3"}, "BJxmQSxU6Q": {"type": "rebuttal", "replyto": "SylvFMGra7", "comment": "Thank you for your comment. \n\nThe authors of the paper are not active on reddit and we do not have control over what reddit users post about our paper.\n\nWe used a batch size of 8 in our work, and we mention this in the paper for completeness, and since this is a bit different from Meschederer et al., who used a batch size of 24 with 4 GPUs. We do not state that the batch size from Meschederer et al. is \u201cextremely large\u201d in our paper, we state that it is \"larger\" than 8, which is factually true (it\u2019s not clear how to state this in any other way\u2026). We did not claim that the smaller batch size of 8 is a contribution of our work, and we did not claim that our paper is the first to train high-resolution GANs without progressive growing of resolution. We do have results for a network trained for 300k iterations and we will add these results to the paper.\n\nWe will refine the wording for the image generation experiments to further avoid these misinterpretations.", "title": "Clarification"}, "Byl41tz9nX": {"type": "review", "replyto": "HyxPx3R9tm", "review": "This paper proposed a constraint on the discriminator of GAN model to maintain informative gradients. It is completed by control the mutual information between the observations and the discriminator\u2019s internal representation to be no bigger than a predefined value.  The idea is interesting and the discussions of applications in different areas are useful. However, I still have some concerns about the work:\n1.\tin the experiments about image generation, it seems that the proposed method does not enhance the performance obviously when compared to GP and WGAN-GP, Why the combination of VGAN and GP can enhance the performance greatly(How do they complementary to each other), what about the performance when combine VGAN with WGAN-GP?\n2.\tHow do you combine VGAN and GP, is there any parameter to balance their effect?\n3.\tThe author stated on page 2 that \u201cthe  proposed information bottleneck encourages the discriminator to ignore irrelevant cues, which then allows the generator to focus on improving the most discerning differences between real and fake samples\u201d, a proof on theory or experiments should be used to illustrate this state.\n4.\tIs it possible to apply GP and WGAN-GP to the Motion imitation or adversarial inverse reinforcement learning problems? If so, will it perform better than VGAN?\n5.\tHow about VGAN compares with Spectral norm GAN?\n", "title": "a constraint on the discriminator of GAN model to maintain informative gradients", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Bkx6mnnK3Q": {"type": "review", "replyto": "HyxPx3R9tm", "review": "The paper \"Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow\" tackles the problem of discriminator over-fitting in adversarial learning. Balancing the generator and the discriminator is difficult in generative adversarial techniques, as a too good discriminator prevents the generator to converge toward effective distributions. The idea is to introduce an information constraint on a intermediate layer, called information bottleneck, which limits the content of this layer to the most discriminative features of the input. Based on this limited representation of the input, the disciminator is constrained to longer tailed-distributions, maintaining some uncertainty on simulated data distributions. Results show that the proposal outperforms previous researches on discriminator over-fitting, such as noise adding in the discriminator inputs. \n\nWhile the use of information bottleneck is not novel, its application in adversarial learning looks inovative and the results are impressive in a broad range of applications. The paper is well-written and easy to follow, though I find that it would be nice to give more insights on the intuition about information bottleneck in the preliminary section to make the paper self-contained (I had to read the previous work from Alemi et al (2016) to realize what information bottleneck can bring). My only question is about the setting of the constaint Ic: wouldn't it be possible to consider an adaptative version which could consider the amount of zeros gradients returned to the generator ? ", "title": "Inovative technique, Impressive results", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJx9PNrv3X": {"type": "review", "replyto": "HyxPx3R9tm", "review": "Summary:\nThe authors propose to apply the Deep Variational Information Bottleneck (VIB) method of [1] on discriminator networks in various adversarial-learning-based scenarios. They propose a way to adaptively update the value for the b\u00eata hyper-parameter to respect the constraint on I(X,Z). Their technique is shown to stabilize/allow training when P_g and P_data do not overlap, similarly to WGAN and gradient-penalty based approaches, by essentially pushing their representation distributions (p_z) to overlap with the mutual information bottleneck. It can also be considered as an adaptive version of instance noise, which serves the same goal. The method is evaluated on different adversarial learning setup (imitation learning, inverse reinforcement learning and GANs), where it compares positively to most related methods. Best results for \u2018classical\u2019 adversarial learning for image generation are however obtained when combining the proposed VIB with gradient penalty (which outperforms by itself the VGAN in this case).\n\n\nPros :\n- This paper brings a good amount of evidence of the benefits to use the VIB formulation to adversarial learning by first showing the effect of such approach on a toy example, and then applying it to more complex scenarios, where it also boosts performance. The numerous experiments and analyses have great value and are a necessity as this paper mostly applies the VIB to new learning challenges. \n\n- The proposition of a principled way of adaptively varying the value of B\u00eata to actually respect more closely the constraint I(X,Z) < I_c, which to my knowledge [1] does not perform, is definitely appealing and seems to work better than fixed B\u00eatas and does also bring the KL divergence to the desired I_c.\n\n- The technique is fairly simple to implement and can be combined with other stabilization techniques such as gradient penalties on the discriminator.\n\n\nCons:\n\n- In my view, the novelty of the approach is somewhat limited, as it seems like a straightforward application of the VIB from [1] for discriminators in adversarial learning, with the difference of using an adaptive B\u00eata.\n\n- I think the B\u00eata-VAE [2] paper is definitely related to this paper and to the paper on which it is based [1] and should thus be cited as the authors use a similar regularization technique, albeit from a different perspective, that restricts I(X,Z) in an auto-encoding task.\n\n- I think the content of batches used to regularize E(z|x) w.r.t. to the KL divergence should be clarified, as the description of p^tilde \u201cbeing a mixture of the target distribution and the generator\u201d (Section 4) can let the implementation details be ambiguous. I think batches containing samples from both distributions can cause problems as the expectation of the KL divergence on a batch can be low even if the samples from both distributions are projected into different parts of the manifold. This makes me think batches are separated? Either way, this should be more clearly stated in the text.\n\n- The last results for  the \u2018traditional\u2019 GAN+VIB show that in this case, gradient penalty (GP) alone outperforms the proposed VGAN, and that both can be combined for best results. I thus wonder if the results in all other experiments could show similar trends if GP had been tested in these cases as well. In the imitation learning task, authors compare with instance noise, but not with GP, which for me are both related to VIB in what they try to accomplish. Was GP tested in Imitation Learning/Inverse RL ? Was it better? Could it still be combined with VIB for better results? \n\n- In the saliency map of Figure 5, I\u2019m unclear as to what the colors represent (especially on the GAIL side). I doubt that this is simply due to the colormap used, but this colormap should be presented.\n\nOverall, I think this is an interesting and relevant paper that I am very likely to suggest to peers working on adversarial learning, and should therefore be presented. I think the limited novelty is counterbalanced by the quality of empirical analysis. Some clarity issues and missing citations should be easy to correct. I appreciate the comparison and combination with a competitive method (Gradient Penalty) in Section 5.3, but I wish similar results were present in the other experiments, in order to inform readers if, in these cases as well, combining VIB with GP leads to the best performance.\n\n[1] Deep Variational Information Bottleneck, (Alemi et al. 2017)\n[2] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (Higgins et al. 2017)\n", "title": "Good showcase of the application and benefits of the VIB in GANs, minor corrections suggested.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}