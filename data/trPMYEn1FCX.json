{"paper": {"title": "GENERATIVE MODEL-ENHANCED HUMAN MOTION PREDICTION", "authors": ["Anthony Bourached", "Ryan-Rhys Griffiths", "Robert Gray", "Ashwani Jha", "Parashkev Nachev"], "authorids": ["ucabab6@ucl.ac.uk", "rrg27@cam.ac.uk", "r.gray@ucl.ac.uk", "ashwani.jha@ucl.ac.uk", "p.nachev@ucl.ac.uk"], "summary": "", "abstract": "The task of predicting human motion is complicated by the natural heterogeneity and compositionality of actions, necessitating robustness to distributional shifts as far as out-of-distribution (OoD). Here we formulate a new OoD benchmark based on the Human3.6M and CMU motion capture datasets, and introduce a hy- brid framework for hardening discriminative architectures to OoD failure by aug- menting them with a generative model. When applied to current state-of-the-art discriminative models, we show that the proposed approach improves OoD ro- bustness without sacrificing in-distribution performance, and can theoretically facilitate model interpretability. We suggest human motion predictors ought to be constructed with OoD challenges in mind, and provide an extensible general framework for hard- ening diverse discriminative architectures to extreme distributional shift.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper proposes a method for out-of-distribution modeling and evaluation in  the human motion prediction task. Paper was reviewed by four expert reviewers who identified the following pros and cons.\n\n> Pros:\n- New benchmark for testing out of distribution performance [R1]\n- Compelling performance with respect to the baselines [R1,R4]\n- Paper is well written and easy to  follow  [R2]\n- Generative model in the context  of  out-of-distribution modeling of human motion is novel [R1,R2,R4]\n\n> Cons:\n- Lack of support for interpretability claim  [R1]\n- Validity and usefulness of the metric [R1]\n- Lack of \"effectiveness\" of the proposed approach [R2,R4]\n- Technical contributions are not significant [R3,R4]\n- Experimental validation lacks comparisons to other state-of-the-art in motion prediction  methods [R3] \n- Lack of evaluation on additional datasets and for the main task [R4]\n\nAuthors tried to address the comments in the rebuttal, but  largely unconvincingly to the  reviewers.  On balance, reviewers felt that negatives outweighed the positives and unanimously suggest rejection. AC concurs and sees no reason to overturn this consensus. \n"}, "review": {"sK5JWeWRW_C": {"type": "review", "replyto": "trPMYEn1FCX", "review": "Summary:\nThis paper proposes a method and benchmark for out-of-distribution modeling and evaluation of human motion. They evaluate against state-of-the-art human motion methods, and show favorable performance against them.\n\n\nPros\n+ Generative model formulation for human motion prediction\n+ Benchmark for testing out of distribution performance in Human 3.6M and CMU-Mocap\n+ Proposed generative model outperforms baselines\n\n\nComments / Suggestions:\n- Interpretability claim:\nThe authors talk about facilitating interpretability in the abstract, however, I fail to find any clear experiments suggesting this. For example, I cannot find analysis of the different dimensions in the learned latent space or anything of that nature. I see section B in the supplementary material discusses interpretability, but I fail to find any clear cut results about this. Can the authors clarify how this claim is reflected in the paper?\n\n- Evaluations:\nThe evaluations provided in this paper are based on euclidean distance measured with respect to the ground truth. While this metric is reasonable, it may also not be enough to evaluate a generative model of motion (e.g., there are multiple plausible futures given a single past). Given that there are clearly defined actions in the used datasets, I would suggest using a metric that measures the generated sequences as a whole. For example, one can train a motion recognition network which given a motion tells us what type of motion we are observing. The authors could train this type of network and test it on their generated motion to see if the predicted / generated motion is recognized as the right category. Another similar evaluation would be FID, where the authors can see if the predicted / generated motion distribution in feature space is close to the ground truth distribution.\n\n- Differences with Kipf & Welling, 2016:\nThe authors mention that they adopt VGAE from Kipf and Welling, but I fail to find where the authors mention what are the specific differences of their method in comparison to Kipf & Welling, 2016. Can the authors clarify this or point out where the specific differences are mentioned?\n\n\nConclusion:\nThe proposed benchmark is interesting and useful for out-of-distribution evaluations, however, some evaluations may be missing to make this more comprehensive. The differences between the method used by the authors and the related work need to be clarified. I am willing to change my score if the authors successfully address the issues mentioned above.\n\n###########################\n  Post Rebuttal Comments\n###########################\n\nAfter reading the rebuttal, I am keeping my original score. For my first concern, they authors mention space as being a limitation for not providing analysis on the \"surveyable\" latent space, but as far as I know, additional experiments addressing my concern could have been added to the supplementary material. For my second concern, the authors talk about excelling in synthetic fidelity, however, there are fidelity measures for generative models that were not used in this submission. MSE is not a fidelity metric. I suggest that the authors address the concerns raised by the reviewers in future submissions, and it's highly likely that the work will be more solid.\n", "title": "Review", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "UaNKPz8sASK": {"type": "rebuttal", "replyto": "6KUeTkR9rOk", "comment": "-- OoD Benchmark--\nWe disagree. Any good benchmark should seek to replicate the circumstances and demands of the real-world\ntask while being maximally sensitive to the performance differences between competing models. The critical point\nwe make at the outset of our paper is that the first property is very difficult to satisfy owing to the extreme\nheterogeneity and complex compositionality of human actions, and interacts with the more important second\nproperty. Any modest selection of classes is likely to contain a blend of degrees of similarity, so that the extent to\nwhich any one class is out-of-sample with respect to the set will vary a great deal. When a set of models are\nevaluated within a leave-one-class-out framework, performance will then be brittle, dependent on the accidental\nhomology between the test class and one or more of the training classes. Crucially, performance in such\ncircumstances will be less sensitive to the generalising power of the test model than when evaluated within the\nconverse framework\u2014testing on all after training on one\u2014for generalisation will be easier in being informed by a\ngreater diversity of training data. The latter framework helpfully accentuates the difference between the training\nand testing data, and broadens the range of contrasts being evaluated. Equally, given that an action of a given\ndistinctive morphology might be relatively rare, data efficiency is an important concern here, and ought to be\nstress-tested by varying the quantity of training data as well as its composition.\n\nWe agree that the AMASS dataset would be a good candidate for further benchmarks for OoD motion, but a\ncomparison across different sets of instances of supposedly the same action is not the critical contrast, as we\nhave already argued. What we need is a comparison across explicitly labelled actions, for the fundamentals of\nreal-world action render all performance subject to unquantifiable and likely substantial distributional shifts. This\nis what our benchmark sets out to achieve.\n\n-- Quantifying the OoD--\nThe protocol is not the same: we modify it so as to quantify the difference between in- and out-of-distribution\nperformance for two models that differ only in one of them being enhanced with a generative model. The same\napproach can be taken for other models in other contexts.\n\nOn the reviewers questions:\n- On comparison to Askan et al.\nThe cardinal characteristics of human action we draw attention to in the introduction suggest deep generative\narchitectures are likely to provide the best means of modelling it. But the focus of this paper is the felicity of the\nsimpler, often computationally more economical, approach of enhancing a conventional discriminative model with\ngenerative machinery. We call it a framework because it is transferable across an array of discriminative\narchitectures, at least when implemented with neural networks of sufficient flexibility. That others may achieve\nhardening to OoD problems via kindred mechanisms\u2014in Aksan et al.\u2019s work the reviewer cites via explicit\nprobabilistic modelling of joint dependence\u2014seems to us to reinforce the general point we are making here, and\nto justify the introduction of the OoD benchmark I think we are all agreed is overdue.\n\n- On how can sequential deterministic models be addressed:\nWhile our focus is on the current SOTA model, which happens to be a GCN model, the approach is potentially\napplicable to any discriminative model flexibly implemented as a neural network. Weight sharing with an auxiliary,\nsubservient generative model will always be possible, and would encourage the discriminative model's sequence\nto contain a richer description of the input. Additionally, an auxiliary generative model that generates a sequence\nin tandem with, but separate from, the discriminative model could share its uncertainty measures and alternative\nfutures at each point in the generation of the sequence. Whereas in our case we encourage the data flowing\nthrough a discriminative model to double as the representation in a latent variable model, in a sequential\ndeterministic model, a fully or partially generated sequence could similarly serve a dual role as the seed for a\ngenerative sequential model\u2019s sequence.\n\nHow these tricks are used very much depends on the experimental setup, and while it is natural to consider the\nanalogous approaches that may be applicable to sequential models, the considerations and metrics relevant to\nthe juxtaposition of feedforward and sequential models are not. We believe that to include this investigation would\nincrease the complexity of this paper to the point of obfuscating its main contribution.\n\n-- Additional comments --\nAmended as requested and may now be viewed in the pdf. Note we cite Martinez et al., already, but have added others: many thanks.", "title": "We are grateful for the reviewer\u2019s comments, which we address in detail below."}, "humwfu3bgwS": {"type": "rebuttal", "replyto": "k6EK7t1UUkU", "comment": "On the contribution: The judgment of publication significance is a subjective, editorial matter. But the reviewer states our contribution only partially. The value of our work is less in the specific implementation of a generatively-enhanced model of\nmotion\u2014though the implementation is novel\u2014than in the demonstration of the general value of the approach in\nthe domain of action modelling. We do this for reasons the reviewer agrees with\u2014the complex compositionality\nand constitutional indeterminacy of human motion\u2014which have not been adequately discussed in the literature,\nand have a bearing on all models of human action. The lack of sufficient awareness of the problem is reflected in\nthe absence of an established OoD benchmark, which we provide here: another aspect of our contribution the\nreviewer describes as highly valuable.\n\nOn the experiments: Our objective is to demonstrate the value of enhancing a discriminative model with a generative one: the correct comparison is therefore with the same model, but without the generative machinery. We have chosen the state-\nof-the-art model at the time of submission: space precludes evaluation of a range of models, but demonstrating\nan effect on less successful models would naturally be weaker, for they leave more room for improvement. In\nreporting model performance we follow established practice in the literature, but have now conducted additional experiments to provide confidence intervals for the h3.6m dataset results. Which may now be viewed in the pdf.", "title": "We are grateful for the reviewer\u2019s comments, which we address in detail below."}, "XDz_nfKRCMr": {"type": "rebuttal", "replyto": "blQNVIlMJGk", "comment": "The results show that our approach matches or exceeds the current state-of-the-art in individual tasks, and\nexceeds the state-of-the-art overall. A choice between the two architectures of similar size and training\ncharacteristics would naturally find in favour of ours, for that is what the evaluation data compel. But our aim here\nis less to build a state-of-the-art model of motion prediction than to show that the use of a relatively simple\ngenerative model to enhance a discriminative model of a radically different architecture improves OoD\nperformance even when the discriminative architecture is already heavily optimised. It is the felicity of that simple\ngeneral architectural move that we wish to highlight here, for it has implications for the design of other models in\nthe field, indeed any model deployed on the same task. We have amended the text to make this clear.\n\nWe have also conducted additional experiments to provide confidence intervals for the h3.6m dataset results. Which may now be viewed in the pdf.", "title": "We are grateful for the reviewer\u2019s comments, which we address in detail below."}, "obHLqSSHhs5": {"type": "rebuttal", "replyto": "sK5JWeWRW_C", "comment": "Interpretability: Our point is that the introduction of a succinct, surveyable latent space can facilitate the identification of\ncharacteristic patterns of motion variability by rendering them intuitively apprehensible. This is a theoretical claim\nimplied by the fundamentals of the approach, and here we merely draw attention to an additional benefit our\napproach could potentially bring. Limited space precludes detailed empirical exploration of its value. We have\namended the text to clarify this point.\n\nEvaluations: Our objective is to demonstrate that a state-of-the-art discriminative predictive model of motion can be hardened\nto OOD challenge by the addition of a relatively simple generative network. The primary measure of performance\nmust therefore be the fidelity of motion prediction\u2014not action synthesis or action recognition\u2014for that is our task.\nA generative model that excels in synthetic fidelity and disentanglement of the constituent actions is theoretically\nlikely to perform well, but since the device is here deployed subserviently to a discriminative model, the\nappropriate metric is that of the target model it serves to enhance. Indeed, Myronenko\u2019s work suggests a\nrelatively crude generative model of limited synthetic power can nonetheless promote a basic discriminative\narchitecture to state-of-the-art. What seems to us striking here is that the addition to discriminative architectures\nof fairly simple generative machinery can be remarkably useful, achieving state-of-the-art performance without\nthe architectural complexities and training demands an accomplished generative model involves. We have\namended the text to clarify this point.\n\nDifferences with Kipf & Welling, 2016: Our generative model is a variational autoencoder (Kingma & Welling) with graph convolutional layers in place of dense layers, except for immediately around the (dense) layers that produce a sample from q(z|x). Kipf & Welling's application is a link prediction task in citation networks and thus it is desired to model only connectivity in the latent space. Here we model connectivity, position, and temporal frequency. We have amended the text to clarify this. \n\nThank you: we hope our replies are satisfactory.", "title": "We are grateful for the reviewer\u2019s comments, which we address in detail below."}, "6KUeTkR9rOk": {"type": "review", "replyto": "trPMYEn1FCX", "review": "## Summary:\n---\nThis paper raises and studies concerns about the generalization of 3D human motion prediction approaches across unseen motion categories. The authors address this problem by augmenting existing architectures with a VAE framework. More precisely, an encoder network that is responsible for summarizing the seed sequence is shared by two decoders for the reconstruction of the seed motion and prediction of the future motion. Hence, the encoder is trained by using both the ELBO of a VAE and the objective of the original motion prediction task. \n\n## Pros:\n--- \nThe paper has a novel and interesting direction as robustness to distribution shifts has not been studied before in 3D human motion modeling. It is implemented around one of the SoTA models based on Graph Convolutional Networks (GCN) using discrete cosine transformation (DCT) features extracted from the motion sequence. To simulate the out-of-distribution scenario, the baselines and the proposed extension are trained on a single action category such as walking and tested on the remaining actions such as eating and sitting. Experiment results on the H3.6M and CMU datasets show that the proposed approach is useful on out-of-distribution (OoD) test cases. \n\n## Cons:\n--- \nI have two main concerns on the proposed benchmark and the models. \n \n-- OoD Benchmark--\n- It looks like there is a significant underfitting problem. The performance of GCN on the walking category is 0.56 at 400 ms while the in-distribution (ID) performance with OoD training is 0.66 (Table 1). The training split for the OoD setup proposed by the authors is possibly too small. I also can not grasp the motivation for selecting a training set \u201cas small in quantity, and narrow in domain as possible\u201d (Section 3). While there is not enough or barely enough training samples, the comparisons might be misleading. We do not know how the proposed extension behaves on the standard task. The authors should compare their models on the main task as well.\n \n- Motion samples from different categories (i.e., walking, eating, etc.) can still be useful for the models in learning the 3D human motion prior. In fact, it has been shown by Martinez et al. (2017) [4] that training motion models with _all_ available actions improves the performance significantly compared to a single-action models as done in this paper. While the proposed approach outperforms the baselines in average performance, it is not always or substantially better on the fine-grained actions. \n \n- It is a tedious setup, but a leave-one-action-out strategy can be more reliable. In my opinion a better option would be training on one dataset and testing on another one. This would allow for an evaluation of the existing (and even pre-trained) models directly where the proposed extension would remain as the only factor for evaluation. In the context of H3.6M and CMU datasets, this might not be straightforward due to different skeletal configurations. Yet there exists a much larger benchmark for 3D motion prediction: AMASS [1]. This would be a suitable candidate for this task as it is a collection of several diverse mocap datasets with different motion categories. It would be very easy to train on a subset of datasets and test on the remaining ones as all the datasets follow a unified skeletal configuration. Note that this is only a suggestion to improve the current work and I am not asking for running experiments on AMASS for the rebuttal as it would drastically change the submission.\n \n-- Quantifying the OoD--\n- The existing architectures are augmented with a VAE latent space and a decoder, which is not technically novel. However, regularization of the representation space and reconstruction of the inputs as auxiliary tasks seem like helpful to the motion prediction task and a good contribution under the \u201climited\u201d training/evaluation protocol. At the same, time the proposed evaluation protocol is not orthogonal to the task but instead it just follows the existing task protocol. In other words, it is not an independent metric/method/framework for assessing the existing models\u2019 OoD performance. \n\nI am asking the following questions as the proposed approach is presented as a \u201cframework\u201d:\n\n- The authors hypothesize that motion prediction in generative modeling frameworks can alleviate the OoD problems. I find it too broad as generative modelling can be applied in various frameworks. Although it is conceptually very different, [1] uses an auto-regressive model, which is a generative model by design, and trains the model by predicting both the seed (i.e., loosely reconstruction) and the future frames similar to the proposed approach. Can we say that they also deal with the OoD problems implicitly? How do the authors position their \u201cframework\u201d compared to this line of work? \n\n- The authors only focus on the Seq2seq-based methods for motion prediction and choose a baseline with an implicit temporal model (i.e., using DCT to encode the motion sequences). Hence, the proposed approach seems to be limited to this GCN-based architecture. How can the sequential deterministic models [1, 3, 4] be addressed? \n \n-- Additional comments --\n- Figures should be improved. Especially the text is hardly readable. \n\n- I find it very hard to follow Section 3. It was clear only after I read the section A in the appendix. It would be clearer if some of the findings are discussed in Section 3 already. \n\n- The losses in the tables are too high compared to the actual task. I am not sure if there is a qualitative difference between the models as the authors did not present any qualitative results. \n\n- Missing related work on 3D motion modelling. I list a s,all collection of SoA representatives below:\n \n[1] Aksan, Emre, Manuel Kaufmann, and Otmar Hilliges. \"Structured prediction helps 3d human motion modelling.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n[2] Gui, L. Y., Wang, Y. X., Liang, X., & Moura, J. M. (2018). Adversarial geometry-aware human motion prediction. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 786-803).\n[3] Pavllo, D., Grangier, D., & Auli, M. (2018). Quaternet: A quaternion-based recurrent model for human motion. arXiv preprint arXiv:1805.06485.\n[4] Martinez, J., Black, M. J., & Romero, J. (2017). On human motion prediction using recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2891-2900).\n", "title": "Review - Generative Model-Enhanced Human Motion Predicition", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "k6EK7t1UUkU": {"type": "review", "replyto": "trPMYEn1FCX", "review": "The paper presents, firstly, a new benchmark (based on Human3.6M and CMU datasets) for human activity and motion with a high degree of out-of-distribution examples, and secondly a hybrid framework for human motion prediction which is more robust to out-of-distribution samples. \n\nOn a positive note, the presented view of human activity as highly compositional and without a clear ontology of actions and sub-actions is highly relevant, and the observed issues with the state of the art methods are completely correctly characterized. The proposed behchmark is also highly valuable to the community.\n\nHowever, the paper suffer from two flaws that renders it unfit for publication in ICLR in its current form. \n* Firstly, the contribution - to combine GCN with the approach of (Myronenko 2018) to regularize the training with a generative model that takes unlabeled samples into accound, and to replace their VAE framework with a corresponding VGAE one -  is not on its own significant enough to serve as a basis for an ICLR paper. \n* Secondly, the experiments are not adequate in that the method is only compared to a GCN without the generative model - and not with any of the other state-of-the-art in motion prediction. Moreover, results are presented without standard deviations which makes it hard to determine if the improvements are significant.\n\nThese two flaws leads to a Reject recommendation, but the authors are highly encouraged to expand the experiments to empirically verify that the proposed contribution is significant enough to warrant publication, and resubmit to a later conference. While addressing the second flaw, it would also help convince the reader of the significance of the method contribution.", "title": "Review of \"GENERATIVE MODEL-ENHANCED HUMAN MOTION PREDICTION\"", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "blQNVIlMJGk": {"type": "review", "replyto": "trPMYEn1FCX", "review": "This paper presents a generative model to solve the OoD problem in human motion prediction. It extends the GCN and attention-GCN works with VGAE for predicting human motions that are  different from ones used in training. Experiments are performed on H36M and CMU benchmarks for illustrating the efficacy of the proposed approach.\n\nPros:\n1. The paper is good in writting and easy to follow the idea\n2. The perspective of using generative model to deal with OoD problem in human motion is novel.\n\nCons.\n1. My major concern is the effectiveness of the proposed approach. From the results shown in Table 3 to Table 5, we could find that the proposed approach fail to solve the OoD problem for some actions when comparing with the baseline attention-GCN. For example, in Table 5, the proposed generative model achieve poor performance than attention-GCN, such as Discussion, Posing, Purchases, Walking Dog and Walking Together (5 out of 14 acitions). These experiments could not provide convinced results to depict the efficacy of the proposed approach. I think the authors should provide more explanations on this which should make this paper stronger. ", "title": "The proposed approach is reasonalbe for dealing with Out-of-Distribution (OoD) problem of human motion prediction. But the experimental results are unconvinced. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}