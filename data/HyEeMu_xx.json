{"paper": {"title": "Progressive Attention Networks for Visual Attribute Prediction", "authors": ["Paul Hongsuck Seo", "Zhe Lin", "Scott Cohen", "Xiaohui Shen", "Bohyung Han"], "authorids": ["hsseo@postech.ac.kr", "zlin@adobe.com", "scohen@adobe.com", "xshen@adobe.com", "bhhan@postech.ac.kr"], "summary": "Progressive attention model that accurately attends to the target objects of various scales and shapes through multiple CNN layers.", "abstract": "We propose a novel attention model which can accurately attend to target objects of various scales and shapes in images. The model is trained to gradually suppress irrelevant regions in an input image via a progressive attentive process over multiple layers of a convolutional neural network. The attentive process in each layer determines whether to pass or suppress features at certain spatial locations for use in the next layer. We further employ local contexts to estimate attention probability at each location since it is difficult to infer accurate attention by observing a feature vector from a single location only. The experiments on synthetic and real datasets show that the proposed attention network outperforms traditional attention methods in visual attribute prediction tasks.", "keywords": ["Deep learning", "Computer vision", "Multi-modal learning"]}, "meta": {"decision": "Reject", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. Authors have conducted additional experiments and provided comparisons to other existing models. However, reviewer scores are not leaning sufficiently towards acceptance.\n \n The effectiveness of this approach on realistic data still remains unclear in the context of existing approaches. I agree that the reported improvement on Visual Genome over the baseline is non-trivial. But evaluating an existing state-of-the-art VQA approach (for instance) would help better place the performance of this approach in perspective relative to state-of-the-art. \n \n Incorporating reviewer comments, and more convincing demonstration of the model's capabilities on realistic data will help make the paper stronger."}, "review": {"ryJ8Xf_we": {"type": "rebuttal", "replyto": "B1VPg2Lve", "comment": "STN-M model often attends to \"improper regions\" that cover a very large area in the image or are even larger than the whole image. This kind of poor attention quality results in 30.70% mAP w/ prior setting, which is the lowest among all the compared algorithms.", "title": "Response"}, "B1VPg2Lve": {"type": "rebuttal", "replyto": "BJ3hwmPBg", "comment": "Dear Authors,\n\nYou say \"We mainly compare our algorithm with SAN and HAN since STNs could not learn a proper attention process on VG.\"\n\nCan you clarify if this improper attention process resulted in poor performance of STN-M on VG? Quantitatively, what was the accuracy of STN-M on VG? How does it compare to other methods in Table 2?\n\nThanks.", "title": "Clarification about STN-M on VG"}, "rkvUdVlwl": {"type": "rebuttal", "replyto": "By_P1QeDe", "comment": "Thank you for your review and positive recommendation. We apologize that we misunderstood your original suggestion of experimenting VQA methods on our dataset. While the rebuttal period is due very soon, we will try to conduct the comparative experiments using state-of-the-art VQA approaches and update the paper with the new results later before the final camera-ready. Thanks again for your constructive comments.", "title": "Response to the comment"}, "SylGjyvLx": {"type": "rebuttal", "replyto": "HJKt06-Ng", "comment": "Thank you for your review and constructive comments. The updates of the paper and the responses are summarized below.\n\n1. More results for VG\nWe added true-positive ratio (TPR) of the attentions on VG to Table 2. TPR of each model is measured with the GT bounding boxes because we cannot obtain the GT segmentation masks. The results show that the attention quality of the proposed method is better than the other baseline models. \nAs suggested, we also added more qualitative results of VG in Appendix D. In the qualitative results of VG, we now present the GT attribute of each example and the predicted probability of the GT attribute for each model in addition to the attention maps.\n\n2. Experiments on VQA\nWe wrote about this in the above response. Please refer to the second part of our response to AnonReviewer1\u2019s review.\n\n3. Other minor comments\nWe have edited the typo, Figure 11 and Appendix A based on your comments.", "title": "Response to review"}, "B1ww9kPLl": {"type": "rebuttal", "replyto": "SyYWBfzNl", "comment": "Thank you for your review and constructive comments. Below, we summarize our responses to the review.\n\n1. Missing citation and minor typos\nAs suggested, we added (Graves, 2013) to the references and introduced it in the related work section and edited the typos.\n\n2. Experiments on VQA\nVQA takes a free-form question as an input, so it is different from the attribute prediction task where the input is just an object and the output is designed to be its attribute. Although attribute prediction task can be regarded as a special VQA question type, but general VQA problems contain a lot more variety of questions. On the other hand, for the attribute prediction task, the target objects are often small in size, so precise attention with detailed spatial extent on the target objects are essential for correctly predicting their attributes. However, for many types of questions in VQA, precise attention covering an object extent is often not required. Even with these differences, we have conducted additional experiments on VQA with a na\u00efve extension of PAN (using skip-thought vector to encode questions and treat it as an attention target) but this na\u00efve extension did not give much performance gain. In order to apply the proposed model more appropriately for VQA, we need to use natural language processing to parse and understand questions and model multi-step attention processing pipeline such as attention for a target object, attention combination, attention transformation as in the work of \u2018Neural Module Networks\u2019, which we can explore as a future work.", "title": "Response to review"}, "BJ3hwmPBg": {"type": "rebuttal", "replyto": "SynYYsrNe", "comment": "Thank you for your review and comments. Here are our responses to your comments.\n\n1. comparison to STN with multiple transformer layers (STN-M)\nThe proposed progressive attention model can attend to the precise region of the target object by predicting attention maps with fine-grained shapes. In contrast, the shape of the attended regions in STN is constrained by a predefined transformation type such as affine transformation. The same limitation exists even with multiple transformation layers (STN-M). Note that the ability to attend to precise spatial support of object is crucial for predicting some attributes e.g. colors. In addition, the proposed method can also attend to a variable number of isolated regions (i.e. multi-modal target distribution) simultaneously whereas the number of attended regions of STN is fixed when the network is designed. Also, STN is developed in the context of classification tasks while our model is designed for attribute prediction.\nAs you suggested, we conducted experiments on using STN with multiple transformer layers (STN-M) for our task and the results are reported in Table. 1a. Although STN-M performs better on simple settings such as MREF than other baselines (reported in our paper), but is still far from the accuracy of the proposed method. Moreover, the performance of STN-M drops significantly as the task becomes more difficult and is even lower than the other baselines on MBG. Finally, STN-M could not learn proper spatial attention process on Visual Genome but learned a coding system of the query to fit the query-specific biases through transformations. The transformer layers generated padded images of different sizes and rotations to encode the query vector. We included these experiments related to STN-M in the updated paper.\n\n2. comparison to dasNet [Deep Networks with Internal Selective Attention through Feedback Connections]\nWhile a channel-wise attention process is applied to multiple layers of the network in dasNet, there is no \u201cspatial\u201d attention. Our task requires to have an attention to local regions specified by the query. More importantly, extending dasNet to our tasks is not trivial for the following reasons: First, to build a dasNet for our target task, we need to assume that a pretrained attribute classifier is available prior to learning attention. Second, query module needs to be integrated in the network for feature manipulation. Finally, the network is not end-to-end trainable while the task itself involves the attention process. We added this paper in the related work section.\n\n3. localization networks of baselines\nThe attention functions in our experiments are depicted in Figure 5b for MNIST Reference and in Appendix Figure 9c for VG and are shared among all the networks including baselines. Note that the local context of $\\mathcal{F}^l_{i, j}$ is only used in PAN-CTX.\n\n4. performance improvements on Visual Genome\nThe performance improvement is much larger on the MNIST Reference experiment than the VG experiment as the former benefits more directly from accurate attention maps while the latter has higher uncertainty on attribute prediction even if the resulting attention map is reasonable. Note that as shown in Table. 2, the proposed method achieves 29.38% mAP while the baseline (SAN) achieves 27.62% on VG. It appears to be a small improvement, but it is still 6.37% relative improvement on mAP. \n", "title": "Rebuttal"}, "SySHKES7g": {"type": "rebuttal", "replyto": "BysBSHGml", "comment": "1) We added some examples of failure cases within the appendix of the paper. Please refer to Figure 12 in Appendix C.\n2) Our visualization of attention maps with rescaling makes sense as the relative strengths of feature activations at different spatial locations remain the same after rescaling. Note that, to draw attended maps for PANs, we did not accumulate rescaled attention maps. Instead, we first accumulated all the raw attention maps and then rescaled the accumulated map once to obtain final maps for visualization.\n\nThanks,\nPaul", "title": "Answers to the questions"}, "HyuXjzMXx": {"type": "rebuttal", "replyto": "S1isLgg7g", "comment": "We did not try the option because the attention process significantly changes the distribution of the feature representations. Fine-tuning the intermediate layers of the network was necessary for this reason. When attention in the network trained with attention is ignored during testing, the performance degraded significantly and is just as good as random choices.\n\nThanks,\nPaul", "title": "Answer for the clarification"}, "BysBSHGml": {"type": "review", "replyto": "HyEeMu_xx", "review": "1) One of the nice advantages of models with attention is added interpretability. Do you have any interesting examples of failure in your model?\n2) Although your visualization method is pretty consistent with work on this type of attention model, do you have comments on how valid your visualization technique of rescaling is for your particular model? \n\nThis paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset.\n\nOverall I think this is a well executed paper, with good experimental results and nice qualitative visualizations. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches.  \n\nAn important missing citation is Graves 2013 which had an early version of the attention model. \n\nMinor typo:\n\"It confins possible attributes..\" -> It confines..\n\"ImageNet (Deng et al., 2009), is used, and three additional\" -> \".., are used,\"", "title": "questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SyYWBfzNl": {"type": "review", "replyto": "HyEeMu_xx", "review": "1) One of the nice advantages of models with attention is added interpretability. Do you have any interesting examples of failure in your model?\n2) Although your visualization method is pretty consistent with work on this type of attention model, do you have comments on how valid your visualization technique of rescaling is for your particular model? \n\nThis paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset.\n\nOverall I think this is a well executed paper, with good experimental results and nice qualitative visualizations. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches.  \n\nAn important missing citation is Graves 2013 which had an early version of the attention model. \n\nMinor typo:\n\"It confins possible attributes..\" -> It confines..\n\"ImageNet (Deng et al., 2009), is used, and three additional\" -> \".., are used,\"", "title": "questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1isLgg7g": {"type": "review", "replyto": "HyEeMu_xx", "review": "Hi,\n\nwhile I don't believe this will work better: \nDid you try to keep the intermediate layers fixed and only trained the attention?\n\nThanks.The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer. Instead, the features are reweighted in each layer with the predicted attention.\n\n1.\tContribution of approach: The approach to use attention in this way is to my knowledge novel and interesting.\n2.\tQualitative results: \n2.1.\tI like the large number of qualitative results; however, I would have wished the focus would have been less on the \u201cnumber\u201d dataset and more on the Visual Genome dataset.\n2.2.\tThe qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results.\n3.\tQualitative results:\n3.1.\tThe paper presents results on two datasets, one simulated dataset as well as Visual Genome. On both it shows moderate but significant improvements over related approaches.\n3.2.\tFor the visual genome dataset, it would be interesting to include a quantitative evaluation how good the localization performance is of the attention approach.\n3.3.\tIt would be interesting to get a more detailed understanding of the model by providing results for different CNN layers where the attention is applied.\n4.\tIt would be interesting to see results on more established tasks, e.g. VQA, where the model should similarly apply. In fact, the task on the numbers seems to be identical to the VQA task (input/output), so most/all state-of-the-art VQA approaches should be applicable.\n\n\nOther (minor/discussion points)\n-\tSomething seems wrong in the last two columns in Figure 11: the query \u201c7\u201d is blue not green. Either the query or the answer seem wrong.\n-\tSection 3: \u201cIn each layer, the each attended feature map\u201d -> \u201cIn each layer, each attended feature map\u201d\n-\tI think Appendix A would be clearer if it would be stated that is the attention mechanism used in SAN and which work it is based on.\n\n\nSummary:\nWhile the experimental evaluation could be improved with more detailed evaluation, comparisons, and qualitative results, the presented evaluation is sufficient to validate the approach. The approach itself is novel and interesting to my knowledge and speaks for acceptance.\n", "title": "Clarification", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJKt06-Ng": {"type": "review", "replyto": "HyEeMu_xx", "review": "Hi,\n\nwhile I don't believe this will work better: \nDid you try to keep the intermediate layers fixed and only trained the attention?\n\nThanks.The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer. Instead, the features are reweighted in each layer with the predicted attention.\n\n1.\tContribution of approach: The approach to use attention in this way is to my knowledge novel and interesting.\n2.\tQualitative results: \n2.1.\tI like the large number of qualitative results; however, I would have wished the focus would have been less on the \u201cnumber\u201d dataset and more on the Visual Genome dataset.\n2.2.\tThe qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results.\n3.\tQualitative results:\n3.1.\tThe paper presents results on two datasets, one simulated dataset as well as Visual Genome. On both it shows moderate but significant improvements over related approaches.\n3.2.\tFor the visual genome dataset, it would be interesting to include a quantitative evaluation how good the localization performance is of the attention approach.\n3.3.\tIt would be interesting to get a more detailed understanding of the model by providing results for different CNN layers where the attention is applied.\n4.\tIt would be interesting to see results on more established tasks, e.g. VQA, where the model should similarly apply. In fact, the task on the numbers seems to be identical to the VQA task (input/output), so most/all state-of-the-art VQA approaches should be applicable.\n\n\nOther (minor/discussion points)\n-\tSomething seems wrong in the last two columns in Figure 11: the query \u201c7\u201d is blue not green. Either the query or the answer seem wrong.\n-\tSection 3: \u201cIn each layer, the each attended feature map\u201d -> \u201cIn each layer, each attended feature map\u201d\n-\tI think Appendix A would be clearer if it would be stated that is the attention mechanism used in SAN and which work it is based on.\n\n\nSummary:\nWhile the experimental evaluation could be improved with more detailed evaluation, comparisons, and qualitative results, the presented evaluation is sufficient to validate the approach. The approach itself is novel and interesting to my knowledge and speaks for acceptance.\n", "title": "Clarification", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}