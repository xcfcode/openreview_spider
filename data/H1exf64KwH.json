{"paper": {"title": "Exploring Model-based Planning with Policy Networks", "authors": ["Tingwu Wang", "Jimmy Ba"], "authorids": ["tingwuwang@cs.toronto.edu", "jba@cs.toronto.edu"], "summary": "how to achieve state-of-the-art performance by combining policy network in model-based planning", "abstract": "Model-based reinforcement learning (MBRL) with model-predictive control or\nonline planning has shown great potential for locomotion control tasks in both\nsample efficiency and asymptotic performance. Despite the successes, the existing\nplanning methods search from candidate sequences randomly generated in the\naction space, which is inefficient in complex high-dimensional environments. In\nthis paper, we propose a novel MBRL algorithm, model-based policy planning\n(POPLIN), that combines policy networks with online planning. More specifically,\nwe formulate action planning at each time-step as an optimization problem using\nneural networks. We experiment with both optimization w.r.t. the action sequences\ninitialized from the policy network, and also online optimization directly w.r.t. the\nparameters of the policy network. We show that POPLIN obtains state-of-the-art\nperformance in the MuJoCo benchmarking environments, being about 3x more\nsample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC.\nTo explain the effectiveness of our algorithm, we show that the optimization surface\nin parameter space is smoother than in action space. Further more, we found the\ndistilled policy network can be effectively applied without the expansive model\npredictive control during test time for some environments such as Cheetah. Code\nis released.", "keywords": ["reinforcement learning", "model-based reinforcement learning", "planning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a model-based policy optimization approach that uses both a policy and model to plan online at test time. The paper includes significant contributions and strong results in comparison to a number of prior works, and is quite relevant to the ICLR community. There are a couple of related works that are missing [1,2] that combine learned policies and learned models, but generally the discussion of prior work is thorough. Overall, the paper is clearly above the bar for acceptance.\n\n[1] https://arxiv.org/pdf/1703.04070.pdf\n[2] https://arxiv.org/pdf/1904.05538.pdf"}, "review": {"H1e3tFvstH": {"type": "review", "replyto": "H1exf64KwH", "review": "Summary\n\nThis work provides a novel model-based reinforcement learning algorithm for continuous domains (Mujoco) dubbed POPLIN. The presented algorithm is similar in vein to the state-of-the-art PETS algorithm, a planning algorithm that uses state-unconditioned action proposal distributions to identify good action sequences with CEM in the planning routine. The important difference compared to PETS is the incorporation of a parametric state-conditioned policy (trained on real data) in the planning routine to obtain better action-sequences (CEM is used to learn the \"offset\" from the parametric policy). The paper presents two different algorithmic ablations where CEM either operates in action space or parameter space (POPLIN-A and POPLIN-P respectively), in combination with different objectives to learn the parametric policy. The method is evaluated on 12 continuous benchmarks and compared against state-of-the-art model-based and model-free algorithms, indicating dominance of the newly proposed method.\n\nQuality\n\nThe quality of the paper is high. This is an experimental study and the number of benchmarks and baselines is far above average compared to other papers in that field. One minor point is that averaging experiments over 4 seeds only is usually not optimal in these environments, but in light of the sheer amount of baselines and benchmarks excusable. While the experimental results are impressive, the authors mention that asymptotic performance in Walker2d and Humanoid might not match the asymptotic performance of model-free baselines. This could be stated more clearly. Also, there are no Humanoid experiments in the paper despite mentioned in the text (2nd paragraph in Section 5.1)?\n\nClarity\n\nThe clarity of the paper can be in parts improved upon. For example, how does the \"policy control\" ablation (mentioned in Section 4.3) work precisely, i.e. the interplay between executing the parametric policy in the real world and harnessing the environment model? I assume the policy distillation techniques in Section 4.4 are different alternatives for the second-to-last lines in the pseudocodes from the appendix? Which one is the default used in Section 5.1? On a minor note, above Equation (7), a target network is mentioned---where does the target network occur in Equation (7)? There are some plots that do not mention the name of the environment, e.g. in Figure (4), but also some in the appendix. Furthermore, it could be stated more clearly that the reward function is assumed to be known. If the authors improve the clarity of their paper significantly, I am willing to increase my score further (presupposing that no severe issues arise in the discussion phase).\n\nOriginality\n\nAdding a parametric policy to PETS is not the most original idea, but clearly a gap in the current literature.\n\nSignificance\n\nThe experiments and the empirical results make the paper quite significant.\n\nUpdate\n\nThe authors improved the clarity of the paper. I therefore increase to 8. Section 4.4 paragraph \"Setting parameter average (AVG)\" can still be improved---does this go together with POPLIN-P-Uni from Section 4.2?", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 3}, "B1e6w2V2or": {"type": "rebuttal", "replyto": "H1exf64KwH", "comment": "We thank the reviewers for the valuable suggestions and acknowledgement of the paper.\nWe updated our paper and summarize the revisions as follows:\n\n1. We greatly improve the clarity of the paper.\n\nWe are sorry for the confusion caused in our earlier version of the paper, which, for example, didn\u2019t specify the model variants used in the experiment section, and has inconsistent use of words.\n\n2. We added more papers in the related work section.\n\nSome of the concurrent or latest research are now included in the paper.\n\nWe also refer to the responses to each reviewer, and we are hoping for further feedback.\n", "title": "General response and update of the paper"}, "BklYenE3iB": {"type": "rebuttal", "replyto": "SJeV9O1dYB", "comment": "We thank the reviewer for the valuable suggestions, and answer the following questions raised in the helpful review.\n\nQ1. Readability of the plots in Figure 7 and missing text doesn't describing environments tested.\n\nWe are sorry for the confusion. In the latest revision, we fixed the captions needed to describe the figures. \n\nQ2. I'd suggest some normalized performance ratio over the final performance of the base algorithm, to show the contribution of all the ingredients of the method.\n\nWe thank the reviewer for the valuable suggestion. In the latest revision, we updated the normalized performance in the appendix.\nDue to the space limit, we summarize some of the representative results here:\n\n                     |      Cheetah     |         Ant         |       Hopper    |     Swimmer   \nPOPLIN-P    | 0.944\u00b10.079    | 0.932\u00b10.128   |  0.912\u00b10.112  |   0.936\u00b10.086\nPOPLIN-A    | 0.395\u00b10.057    | 0.459\u00b10.175   |  0.582\u00b10.175  |   0.962\u00b10.018\nPETS            | 0.363\u00b10.002    | 0.466\u00b10.091   |  0.566\u00b10.113  |   0.916\u00b10.032\nRS                | 0.072\u00b10.005    | 0.214\u00b10.015   |  0.092\u00b10.006  |   0.156\u00b10.024\nMBMF         | 0.025\u00b10.002    | 0.054\u00b10.02     |  0.355\u00b10.2      |   0.377\u00b10.114\nTRPO           | 0.028\u00b10.003    | 0.129\u00b10.01     |  0.164\u00b10.116  |   0.22\u00b10.028\nPPO             | 0.023\u00b10.01      | 0.128\u00b10.02     |  0.527\u00b10.187  |  0.489\u00b10.037\nGPS             | 0.067\u00b10.051    | 0.178\u00b10.085   |  0.406\u00b10.037  |   0.023\u00b10.016\nMETRPO      | 0.004\u00b10.043    | 0.113\u00b10.007   |  0.777\u00b10.091  |   0.664\u00b10.262\nTD3              | 0.074\u00b10.061    | 0.348\u00b10.114   |  0.876\u00b10.181  |   0.28\u00b10.327\nSAC              | 0.184\u00b10.006    | 0.219\u00b10.059   |  0.689\u00b10.134  |   0.612\u00b10.173\nRandom      | 0.037\u00b10           | 0.191\u00b10.019   |  0.042\u00b10.104  |   0.069\u00b10.032\nMax, Min     |  13000, -800   |    2500, 0        |   2500, -3000  |    360, -40    \n\nQ3. For the results presented in section 5.1, it's unclear to me which exact variation of POPLIN-A and POPLIN-P have been selected.\n\nSorry for the confusion. It is also brought by other reviewers as well. We use POPLIN-A-BC-Replan and POPLIN-P-Sep respectively. We updated them in the latest revision.\n\nQ4. Overall, the SOTA performance in MuJoCo is held by MFRL methods, but typically require more training (up to 1M).\n\nYes we agree that our algorithm as well as other MBRL algorithms plateau after a few hundred thousand samples. We rephrase our paper in the latest revision accordingly to avoid over-claiming.\nWe also refer to the general response and the updated paper for more details.\n", "title": "Response to Reviewer 3"}, "BJeLws4nsS": {"type": "rebuttal", "replyto": "H1e3tFvstH", "comment": "We thank the reviewer for the valuable suggestion. In the latest revision, we improve the clarity of the paper accordingly and apologize again for the confusion caused by our writing in the original revision.\n\nQ1. Walker2d and Humanoid might not match the asymptotic performance of model-free baselines. This could be stated more clearly.\n\nWe thank Reviewer 2 for the suggestion. We agree that it should be stated more clearly, and we updated in the newest revision.\n\nQ2. Also, there are no Humanoid experiments in the paper despite mentioned in the text (2nd paragraph in Section 5.1)?\n\nSorry for the confusion. In section 5.1, we were meant to mention humanoid as the tasks which POPLIN cannot solve efficiently. Therefore we didn\u2019t put the results in the paper.\nWe rephrase the sentences to remove ambiguity in the latest revision.\n\nQ3. how does the \"policy control\" ablation (mentioned in Section 4.3) work precisely?\n\nWe apologize for the confusion of \u201cpolicy control\u201d and \u201cMPC control\u201d in our paper.\n\u201cMPC Control\u201d: To generate the data that is used to train the policy network, we always use \u201cMPC planning\u201d, which uses the distilled policy network, learnt dynamics and plans into the future at every time-steps.\n\u201cPolicy Control\u201d: We are also interesting to see how powerful the distilled policy network along is, without the use of expensive planning with learnt dynamics.\nTo get the \u201cpolicy control\u201d performance, we load the saved policy network weights at different iterations. The data generated is not used in \u201cMPC control\u201d or training.\n\nQ4. Policy distillation techniques in Section 4.4 and pseudocodes.\n\nYes, policy distillation techniques are in Section 4.4 are different alternatives for the second-to-last lines in the pseudocodes. For the benchmarking results in Section 5.1, POPLIN-A-BC-Replan and POPLIN-P-Sep-AVG variants are respectively used in the paper. We also provide the results of the  ablation study of different distillation techniques in Section 5.4.\n\nQ5. On a minor note, above Equation (7), a target network is mentioned---where does the target network occur in Equation (7)? \n\nIt should have been just \u201cnetwork\u201d instead of \u201ctarget network\u201d. The \u201ctarget\u201d was redundant and clearly caused confusion. We were meant to indicate we don\u2019t have noise randomly generated during training.\n\nQ6. There are some plots that do not mention the name of the environment, e.g. in Figure (4), but also some in the appendix. \n\nWe thank the reviewer for pointing out. They are fixed in the latest revision.\n\nQ7. Furthermore, it could be stated more clearly that the reward function is assumed to be known.\n\nIn the latest revision, we now state more clearly about the reward function.\n\nQ8. If the authors improve the clarity of their paper significantly, I am willing to increase my score further.\n\nWe apologize again for our lack of clarity. We thank the reviewer to the valuable suggestions.\nWe uploaded the updated paper in the openreview system, which we hope has greatly improved the readability of the paper.\n\n", "title": "Response to Reviewer 2. We hope that the current revision has greatly improved its clarity"}, "Hylu2qNhsH": {"type": "rebuttal", "replyto": "Hkxyvbc6Fr", "comment": "We thank the reviewer for the suggestion and updated accordingly in the latest revision.\n\nQ1. In the example of arm in the first paragraph of Section 4.2, although the mean is 0, the randomness in sampling will be the tie breaker. So \"failing\" is probably not the best word here. \n\nWe agree that \u201cfailing\u201d is not the most appropriate word. In the newest version, we rephrase it into \u201cit is hard to model the bimodal action distribution in traditional planner\u201d.\n\n\nQ2. Why use deterministic policy network?\n\nIn CEM, the stochasticity is partially modeled by the planner by maintaining a population of candidates.\nOne of the reasons we didn\u2019t have an explicit stochastic policy network (which sample actions from a distribution) is that it might make the CEM updates unstable and hard to converge. \nIn the paper we didn\u2019t test the idea, but we think stochastic policy network is definitely worth a try in POPLIN.\n\nQ3. Why optimize all parameters in the policy network, which makes optimization more difficult? \n\nThe case where we only optimize the last layer (covariance) of the policy can be regarded as a special case of POPLIN-A.\nWe believe there\u2019s a trade-off between capacity and optimization difficulty.\nThe more parameters or the deeper the network, the more expressive but also more difficult to optimize the planner can be.\nEmpirically, we found that for low dimensional environments, it does turn out having more parameters does not help the planner, and only brings more computation cost. We also refer to Q5 below for more details.\n\nQ4. Comparison to MBPO. \n\nWe thank the reviewer for bringing this baseline. We added it into the related work section in the last revision and modified the claim in the paper.\nMBPO and POPLIN were developed with very different techniques (Dyna for MBPO and MPC for POPLIN), and the environments / reward functions are defined differently.\nWe have similar performance in many tasks, and MBPO seems to be very effective for Ant and Walker. We believe the performance will be even better if we combine the Dyna updates from MBPO and the MPC in POPLIN.\n\nQ5. What's the architecture of policy network? More importantly, how many parameters does the policy network have?\n\nWe experimented with different sizes of MLP, including [32] (best for POPLIN-P), [64], [32, 32] (best for POPLIN-A), and [64, 64]. Very deep network is both very computationally infeasible and hard to optimize.\n\nQ6. In Ablation Study, what does \"imaginary data\" mean? \n\nSorry for the confusion. The imaginary data refers to the planned trajectories generated in rollouts. They are predicted by the learnt dynamics, and can potentially helpful to train the policy network.\n \n\nQ8. A lot of default values need to be specified: What's the policy distillation method used in POPLIN-A/P in Table 1? Does POPLIN-A mean POPLIN-A-Replan or POPLIN-A-init? Does POPLIN-P mean POPLIN-P-Sep or POPLIN-P-Uni? \n\nSorry for the confusion. We use POPLIN-A-BC-Replan and POPLIN-P-Sep respectively. We updated them in the latest revision.\n", "title": "Response to Reviewer 1"}, "SJeV9O1dYB": {"type": "review", "replyto": "H1exf64KwH", "review": "Contributions\n\nThis paper proposes a MBRL algorithm for continuous action space domains that relies on a policy network to propose an initial trajectory, which is then refined, either in the action space or in the policy space, using standard CEM.\n3 options are evaluated to update the policy network: a GAN approach, a regression approach (behaviour cloning) and a direct modifications of the policy based on the perturbation found by CEM.\n\n\nReview\n\nThe presentation of the method is thorough, as well as the motivations and the experiments. However, the presentation of the experimental result could gain in clarity. For example, plots in Figure 7 are totally unreadable when printed on paper, and even after zooming in the pdf it's difficult to tell what's going on. I think the text doesn't even mention on which environment these curves are obtained.\nI'd suggest some normalized performance ratio over the final performance of the base algorithm, to show the contribution of all the ingredients of the method.\n\nFor the results presented in section 5.1, it's unclear to me which exact variation of POPLIN-A and POPLIN-P have been selected.\n\n\nFinally the abstract reads \"We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments\" (similar claims are made elsewhere in the paper). I'd tone this down a little, since the said performance ist state-of-the-art only for the 200k frames regime, and as it's hinted at the end of 5.1, I assume the proposed approach suffers from the same shortcoming as other competing MBRL methods which is that their performance tends to plateau after a few hundred thousand samples. Overall, the SOTA performance in MuJoCo is held by MFRL methods, but typically require more training (up to 1M)\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 1}, "Hkxyvbc6Fr": {"type": "review", "replyto": "H1exf64KwH", "review": "This paper presents POPLIN, a novel model-based reinforcement learning algorithm, which trains a policy network to improve model-prediction control. The paper studies extensively how to utilize the policy, by planning in action space or planning in parameter space and how to train the policy, by behavioral cloning, by GAN or by averaging the results of CEM. The experiments show that the proposed algorithm can perform very well in MuJoCo tasks. \n\nOverall, the paper is well-written and the method is novel. The extensive experiments make the paper more convincing. \n\nQuestions:\n1. In the example of arm in the first paragraph of Section 4.2, although the mean is 0, the randomness in sampling will be the tie breaker. So \"failing\" is probably not the best word here. \n2. It seems that the policy network is deterministic. Why?\n3. Reparametrizable policy often requires fewer (noise) parameters to be optimized over. For example, suppose the policy outputs a multi-variate Gaussian distribution with diagonal covariance in R^10, then we only need to optimize over 10 parameters (the Gaussian noise).  Why optimize all parameters in the policy network, which makes optimization more difficult? \n4. Sec 5.1 Para 2: To my knowledge, the state-of-the-art model-based RL algorithm in MuJoCo environments is MBPO (https://arxiv.org/abs/1906.08253, NeurIPS 2019). \n5. What's the architecture of policy network? More importantly, how many parameters does the policy network have? It's really interesting to see that CEM works for such a high dimensional space. In an environment where a larger network is required, the optimization seems to be more difficult. \n6. In Ablation Study, what does \"imaginary data\" mean? \n7. I'm also curious to see how the objective of CEM improves. \n\nMinor comments:\n\n1. Sec 5.1 Para 2 L11: efficient -> efficiently. \n2. Are you talking about Figure 3 at Sec 5.2? If so, could you please add a link to the figure? \n3. A lot of default values need to be specified: What's the policy distillation method used in POPLIN-A/P in Table 1? Does POPLIN-A mean POPLIN-A-Replan or POPLIN-A-init? Does POPLIN-P mean POPLIN-P-Sep or POPLIN-P-Uni? \n4. Sec 4.1 Eq (2): \\delta_0...\\delta_\\xi are \\xi+1 sequences. \n5. Sec 5.3 Para 3: multi-model -> multi-modal. ", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}}}