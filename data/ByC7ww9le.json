{"paper": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"], "summary": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "Three knowledgable reviewers recommend rejection. While they agree that the paper has interesting aspects, they suggest a more convincing evaluation. The authors did not address some of the reviewer's concerns. The AC strongly encourages the authors to improve their paper and resubmit it to a future conference."}, "review": {"BymlhFHVg": {"type": "rebuttal", "replyto": "SkkQgS-4l", "comment": "Thank you for you time reviewing our paper. Let me try to clarify some of your concerns.\n\nFirst, we discuss properties of the learned TransGaussian embedding in Appendix B. In fact, the variance parameters associated with each relation result in an interesting block pattern (Figure 3). This shows that the proposed TransGaussian embedding can disentagle different properties into different subsets of dimensions as shown in Figure 4. This is a feature that clearly distinguishes TransGaussian from the rotationally invariant TransE model.\n\nSecond, it is true that the order of relations is lost in the current formulation. This is the problem we discuss in Section 5 in relation to Neelakantan et al. (2015a). We hope to address this issue using RNNs in the future.\n\nFinally, regarding motivation, our motivation for introducing the Gaussian attention model is that it can compactly represent a set of items. A typical attention model deals with a soft version of an item. Our Gaussian attention model deals with a soft version of a set. As operations on sets, we support composition and conjunction. The nice thing about our Gaussian model is that both of these operations can be carried out in closed form (namely convolution and product). We can think of extending NTM and memory networks so that they can deal with sets instead of items but that is beyond the scope of this paper.", "title": "Clarification"}, "BkrTFttQl": {"type": "rebuttal", "replyto": "HJhyJ2lQe", "comment": "\"Why not compare TransGaussian on the benchmarks used by this kind of method (like TransE) usually like FB15k?\"\n\nWe definitely should test our model on other datasets as well. Yet, large scale question and answering datasets on knowledge bases are mostly about Simple Q&A where the answer to a question is associated with an atomic fact. e.g. the SimpleQuestion dataset proposed in Bordes et al. (2015) and Serban et al. (2016), \"Generating Factoid Questions With Recurrent Neural Networks-The 30M Factoid Question-Answer Corpus\".\n\nThere are datasets which require a model to map a sentence to a logical form, such as Geo808 and Jobs640. To tackle these datasests, we would need to translate from the set of logical representations typically used for these datasets to the set of smooth Gaussian operations (composition and conjunctions) we have presented in this paper. We are optimistic that we can support many useful operations beyond simple Q&A using our Gaussian attention model.\n\n\n\"(1) Why is there a drop in performance from SINGLE to COMP for TransE? This is not expected.\"\n\nIn Table 2, at least for path queries, TransE (comp) is better than TransE (single).\n\nThe weak performance of TransE (comp) compared to TransE (single) on conjunctive queries mainly comes from not being able to model the two inverse relations (#8 and #9) well. Our hypothesis is that TransE lacks the complexity to take full advantage of the compositional training. We agree that it was also unexpected for us, but please note that no previous work evaluated compositionally trained TransE on conjunctive queries.\n\n\n\"(2) TransGaussian is actually worse than TransE in the SINGLE setting. Is there an explanation?\"\n\nWe hypothesize that since TransGaussian is more flexible than TransE, it needs stronger regularization. That is, compositional training provided the necessary constraint to train TransGaussian efficiently.\n", "title": "Responses"}, "HJhyJ2lQe": {"type": "review", "replyto": "ByC7ww9le", "review": "- Why not compare TransGaussian on the benchmarks used by this kind of method (like TransE) usually like FB15k?\n- In Table 2: \n(1) Why is there a drop in performance from SINGLE to COMP for TransE? This is not expected.\n(2) TransGaussian is actually worse than TransE in the SINGLE setting. Is there an explanation?This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them, centered around the use of multivariate gaussian likelihood instead of inner products to score attention. This is supposed to allow more control on the attention by dealing with its spread.\n\nThis is a dense paper centered around a quite complicated model. With the supplementary material, this makes a 16p paper. It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.\n\nI like the idea of controlling the spread of the attention. This makes sense. However, I do not feel that this paper is convincing enough to justify its use compared to usual inner products.\n\nFor several reasons:\n- These should be more ablation experiments to separate the different pieces of the model and study their influence separately. The only interesting point in that sense is Table 8 in Appendix B. We need more of this. \n- In particular, a canonical experiments comparing Gaussian interaction vs inner product would be very useful. \n- Experiments on existing benchmarks (for KB completion, or QA) would help. I agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (WorldCup2014). But this should come in addition to experiments on existing data.\n- Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark). Unfortunately, TransGaussian does not perform well compared to simpler TransE. This, along with the poor results of TransGaussian (SINGLE) of Table 2, indicate that training TransGaussian seems pretty complex, and hence question the actual validity of this architecture.\n\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkDXxTrNe": {"type": "review", "replyto": "ByC7ww9le", "review": "- Why not compare TransGaussian on the benchmarks used by this kind of method (like TransE) usually like FB15k?\n- In Table 2: \n(1) Why is there a drop in performance from SINGLE to COMP for TransE? This is not expected.\n(2) TransGaussian is actually worse than TransE in the SINGLE setting. Is there an explanation?This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them, centered around the use of multivariate gaussian likelihood instead of inner products to score attention. This is supposed to allow more control on the attention by dealing with its spread.\n\nThis is a dense paper centered around a quite complicated model. With the supplementary material, this makes a 16p paper. It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.\n\nI like the idea of controlling the spread of the attention. This makes sense. However, I do not feel that this paper is convincing enough to justify its use compared to usual inner products.\n\nFor several reasons:\n- These should be more ablation experiments to separate the different pieces of the model and study their influence separately. The only interesting point in that sense is Table 8 in Appendix B. We need more of this. \n- In particular, a canonical experiments comparing Gaussian interaction vs inner product would be very useful. \n- Experiments on existing benchmarks (for KB completion, or QA) would help. I agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (WorldCup2014). But this should come in addition to experiments on existing data.\n- Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark). Unfortunately, TransGaussian does not perform well compared to simpler TransE. This, along with the poor results of TransGaussian (SINGLE) of Table 2, indicate that training TransGaussian seems pretty complex, and hence question the actual validity of this architecture.\n\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1mEdE3fe": {"type": "rebuttal", "replyto": "SJZ1i73fl", "comment": "1) Please take a look at Table 5 in Appendix. The questions were generated using multiple templates per question.\n2) That is right. See bottom of page 7. We wrote \"Note that TransE can be considered as a special case of TransGaussian where the variance matrix is the identity and hence, the scoring formula Eq. (7) is applicable to TransE as well.\"\n3) Sure. Scoring function (1) has two parametes (mean and variance). In general, an attention network computes both the mean and the variance in a context dependent manner. Thus it can control not only the direction of the attention but also the sharpness of the focus. In the proposed knowledge base embedding model the mean and the variance are independent parameters for each relation. You can see in Figure 3 in Appendix that the learned variance is low for coordinates corresponding to the relation and high for other coordinates. This is natural because when you query for a Forward player, the variance should be high in all but the coordinates corresponding to the position of the player.\n4) In Section 2.1, we wrote \"if \\Sigma_r is \ufb01xed to the identity matrix, we are modeling the relation of subject v_s and object v_o as a translation \\delta_r, which is equivalent to the TransE model (Bordes et al., 2013).\" but this is only to relate our model to TransE. We don't fix \\Sigma_r as identity.\n5) This was only because of convenience and we should definitely test our model on other datasets as well.", "title": "Response to your questions"}, "SJZ1i73fl": {"type": "review", "replyto": "ByC7ww9le", "review": "Hi, I would have some doubts about the paper:\n1) How did you generate natural language questions for the worldcup dataset?\n2) When comparing to Guu et al. in the QA experiments did you just change the TransGaussian scoring function with the TransE (single) and TransE (comp)?\n3) In the abstract you state that with TransGaussian you can control the focus of the attention, it is not clear from the paper how would you do it, can you elaborate on this?\n4) In section 2.1 you stated that \\Sigma_r is fixed to the identity matrix, but after you state that the relation matrices are actually learnt. Could you elaborate on it?\n5) Is there a particular reson why you tested your model on a new dataset and you did not test it on estabilished benchmarks?\nSUMMARY.\n\nThe paper propose a new scoring function for knowledge base embedding.\nThe scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.\nThe proposed function is tested on two tasks knowledge-base completion and question answering.\n\n----------\n\nOVERALL JUDGMENT\nWhile I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.\nRegarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.\nPlus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.\nRegarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.\nFinally, the paper lack of discussion of results and insights on the behavior of the proposed model.\n\n\n----------\n\nDETAILED COMMENTS\n\n\nIn section 2.2 when the authors calculate \\mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?\n", "title": "Some questions", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkkQgS-4l": {"type": "review", "replyto": "ByC7ww9le", "review": "Hi, I would have some doubts about the paper:\n1) How did you generate natural language questions for the worldcup dataset?\n2) When comparing to Guu et al. in the QA experiments did you just change the TransGaussian scoring function with the TransE (single) and TransE (comp)?\n3) In the abstract you state that with TransGaussian you can control the focus of the attention, it is not clear from the paper how would you do it, can you elaborate on this?\n4) In section 2.1 you stated that \\Sigma_r is fixed to the identity matrix, but after you state that the relation matrices are actually learnt. Could you elaborate on it?\n5) Is there a particular reson why you tested your model on a new dataset and you did not test it on estabilished benchmarks?\nSUMMARY.\n\nThe paper propose a new scoring function for knowledge base embedding.\nThe scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.\nThe proposed function is tested on two tasks knowledge-base completion and question answering.\n\n----------\n\nOVERALL JUDGMENT\nWhile I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.\nRegarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.\nPlus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.\nRegarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.\nFinally, the paper lack of discussion of results and insights on the behavior of the proposed model.\n\n\n----------\n\nDETAILED COMMENTS\n\n\nIn section 2.2 when the authors calculate \\mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?\n", "title": "Some questions", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}