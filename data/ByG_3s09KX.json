{"paper": {"title": "Dopamine: A Research Framework for Deep Reinforcement Learning", "authors": ["Pablo Samuel Castro", "Subhodeep Moitra", "Carles Gelada", "Saurabh Kumar", "Marc G. Bellemare"], "authorids": ["psc@google.com", "smoitra@google.com", "cgel@google.com", "kumasaurabh@google.com", "bellemare@google.com"], "summary": "In this paper we introduce Dopamine, a new research framework for deep RL that is open-source, TensorFlow-based, and provides compact yet reliable implementations of some state-of-the-art deep RL agents.", "abstract": "Deep reinforcement learning (deep RL) research has grown significantly in recent years. A number of software offerings now exist that provide stable, comprehensive implementations for benchmarking. At the same time, recent deep RL research\nhas become more diverse in its goals. In this paper we introduce Dopamine, a new research framework for deep RL that aims to support some of that diversity. Dopamine is open-source, TensorFlow-based, and provides compact yet reliable\nimplementations of some state-of-the-art deep RL agents. We complement this offering with a taxonomy of the different research objectives in deep RL research. While by no means exhaustive, our analysis highlights the heterogeneity of research\nin the field, and the value of frameworks such as ours.", "keywords": ["reinforcement learning", "software", "framework", "reproducibility"]}, "meta": {"decision": "Reject", "comment": "The paper presents Dopamine, an open-source implementation of plenty of DRL methods. It presents a case study of DQN and experiments on Atari. The paper is clear and easy to follow.\n\nWhile I believe Dopamine is a very welcomed contribution to the DRL software landscape, it seems there is not enough scientific content in this paper to warrant publication at ICLR. Regarding specifically the ELF and RLlib papers, I think that the ELF paper had a novelty component, and presented RL baselines to a new environment (miniRTS), while the RLlib paper had a stronger \"systems research\" contribution. This says nothing about the future impact of Dopamine, ELF, and RLlib \u2013 the respective software."}, "review": {"Hkx2TztkTX": {"type": "rebuttal", "replyto": "ByG_3s09KX", "comment": "We would like to thank all the reviewers for their comments.\n\nWe feel ICLR is the right venue for this type of contribution, as it is providing a stable, reproducible, and reliable framework for others to use.\nSimilar frameworks have been previously introduced at comparable conferences: ELF at NIPS 2017 and RLLib at ICML 2018.", "title": "Response to reviews"}, "rJgSH8Io37": {"type": "review", "replyto": "ByG_3s09KX", "review": "This paper introduces and details a new research framework for reinforcement learning called Dopamine. The authors give a brief description of the framework, built upon Tensorflow, and reproduce some recent results on the ALE framework. \n\nPros:\n1. Nice execution and they managed to successfully reproduce recent deep RL results, which can be challenging at times.\n\nCons:\n1. Given that this is a paper describing a new framework, I expected a lot more in terms of comparing it to existing frameworks like OpenAI Gym, RLLab, RLLib, etc. along different dimensions.  In short, why should I use this framework? Unfortunately, the current version of the paper does not provide me information to make this choice. Other than the framework, the paper does not present any new tasks/results/algorithms, so it is not clear what the contribution is.\n\n\nOther comments:\n1. The paragraphs in sections 2.1 and 2.2 (algorithmic research, architecture research, etc.) seem to say pretty much the same things. They could be combined, and the DQN can be used as a running example to make the points clear.\n2. The authors mention tests to ensure reliability and reproducibility. Can you provide more details? Do these tests account for semantic bugs common while implementing RL algorithms?", "title": "Needs refinement", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SygEf9Dvhm": {"type": "review", "replyto": "ByG_3s09KX", "review": "Summary:\nThe authors present an open-source framework TensorFlow-based named Dopamine to facilitate the task of researchers in deep reinforcement learning (deep RL). It allows to build deep RL using existing components such as reinforcement learning agents, as well as handling memory, logs and providing checkpoints for them.\nEmphasis is given on providing a unified interface to these agents as well as keeping the framework generic and simple (2000 lines of code).\nThe framework was demonstrated on Atari games notably using Deep Q-network agents (DQN).\nThe authors provide numerous examples of parameter files that can be used with their framework.\nPerformance results are reported for some agents (DQN, C51, Rainbow, IQN).\n\nGiven the actual trends in deep learning works, unified frameworks such as that proposed is welcome.\nThe automatization of checkpointing for instance is particularly useful for long running experiments.\nAlso, trying to reduce the volume of code is beneficial for long-term maintenance and usability.\n\nMajor concerns:\n* This type of contribution may not match the scope of ICLR.\n* In the abstract and a large fraction of the text, the authors claim that their work is a generic reinforcement learning framework. However, the paper shows that the framework is very dependent on agents playing Atari games. Moreover, the word \"Atari\" comes out of nowhere on pages 2 and 5.\nThe authors should mention in the beginning (e.g. in the abstract) that they are handling only agents operating on Atari games.\n* The positioning of the paper relative to existing approaches is unclear: state of the art is mentioned but neither discussed nor compared to the proposal.\n* The format of the paper should be revised:\n                - Section 5 (Related Works) should come before presenting the author's work. When reading the preceding sections, we do not know what to expect from the proposed framework.\n                - All the code, especially in the appendices, seems not useful in such a paper, but rather to the online documentation of the author's framework.\n* What is the motivation of the author's experiments?\n                - Reproduce existing results (claimed on page 1)? Then, use the same settings as published works and show that the author's framework reaches the same level of performances.\n                - Show new results (such as the effect of stickiness)? Then the authors should explicitly say that one of the contributions of the paper is to show new results.\n* The authors say that they want to compare results in Figure 3. They explain why the same scale is not used. To my opinion, the authors should find a way to bring all comparisons to the same scale.\n\nFor all these reasons, I think the paper is not ready for publication at ICLR.", "title": "The contribution is not ready to be published in ICLR", "rating": "3: Clear rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "B1lGa1pUn7": {"type": "review", "replyto": "ByG_3s09KX", "review": "Review: This paper proposed \"Dopamine\", a new framework for DeepRL.  While this framework seems to be useful and the paper seems like a useful guide for using the framework, I didn't think that the paper had enough scientific novelty to be an ICLR paper.  I think that papers on novel frameworks can be suitable, but they should demonstrate that they're able to do something or provide a novel capability which has not been demonstrated before.  \n\nStrengths: \n\n-Having a standardized tool for keeping replay buffers seems useful.  \n\n-The Dopamine framework is written in Python and has 12 files, which means that it should be reasonably easy for users to understand how it's functioning and change things or debug.  \n\n-The paper has a little bit of analysis of how different settings effect results (such as how to terminate episodes) but I'm not sure that it does much to help us in understanding the framework.  I suppose it's useful to understand that the settings which are configurable in the framework affect results?  \n\n-The result on how sticky actions affect results is nice but I'm not sure what it adds over the Machado (2018) discussion.  \n\nWeaknesses: \n\n-Given that the paper is about documenting a new framework, it would have been nice to see more comprehensive baselines documented for different methods and settings.  \n\n-I don't understand the point of 2.1, in that it seems somewhat trivial that research has been done on different architectures and algorithms.  \n\n-In section 4.2, I wonder if the impact of training mode vs. evaluation mode would be larger if the model used a stochastic regularizer.  I suspect that in general changing to evaluation mode could have a significant impact.  \n", "title": "A useful framework, but may not have enough research novelty", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}