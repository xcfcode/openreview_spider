{"paper": {"title": "Learning the Step-size Policy for the Limited-Memory Broyden-Fletcher-Goldfarb-Shanno Algorithm", "authors": ["Lucas N. Egidio", "Anders Hansson", "Bo Wahlberg"], "authorids": ["~Lucas_N._Egidio1", "anders.g.hansson@liu.se", "~Bo_Wahlberg1"], "summary": "A framework to automatically learn a policy from data that generates step sizes for the Limited-Memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm and performs better than heuristically tuned ADAM and RMSprop in tests on MNIST dataset.", "abstract": "We consider the problem of how to learn a step-size policy for the Limited-Memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm. This is a limited computational memory quasi-Newton method widely used for deterministic unconstrained optimization but currently avoided in large-scale problems for requiring step sizes to be provided at each iteration. Existing methodologies for the step size selection for L-BFGS use heuristic tuning of design parameters and massive re-evaluations of the objective function and gradient to find appropriate step-lengths. We propose a neural network architecture with local information of the current iterate as the input. The step-length policy is learned from data of similar optimization problems, avoids additional evaluations of the objective function, and guarantees that the output step remains inside a pre-defined interval. The corresponding training procedure is formulated as a stochastic optimization problem using the backpropagation through time algorithm.  The performance of the proposed method is evaluated on the training of classifiers for the MNIST database for handwritten digits and for CIFAR-10. The results show that the proposed algorithm outperforms heuristically tuned optimizers such as ADAM, RMSprop,  L-BFGS with a backtracking line search and L-BFGS with a constant step size. The numerical results also show that a learned policy can be used as a warm-start to train new policies for different problems after a few additional training steps, highlighting its potential use in multiple large-scale optimization problems.", "keywords": ["Unconstrained optimization", "Step-size policy", "L-BFGS", "Learned optimizers"]}, "meta": {"decision": "Reject", "comment": "The paper presents a novel procedure to set the steps-size for the L-BFGS algorithm using a neural network.\nOverall, the reviewers found the paper interesting and the main idea well-thought. However, a baseline that was proposed by one of the reviewers seems to be basically on par with the performance of the proposed algorithm, at least in the experiments of the paper. For this reason, it is difficult to understand if the new procedure has merit or not. Also, the reviewers would have liked to see the same approach applied to different optimization algorithms.\n\nFor the reasons, the paper cannot be accepted in the current form. Yet, the idea might have potential, so I encourage the authors to take into account the reviewers' comments and resubmit the paper to another venue."}, "review": {"AXMyWDcep89": {"type": "review", "replyto": "AcH9xD24Hd", "review": "**Summary**:\n\nThe paper presents a novel steps-size adaptation for the L-BFGS algorithm inspired by the learning-to-learn idea. The step-size policy is determined by two linear layers which compare a higher dimensional mapping of curvature information which is trained to adapt the step-size.\n\n\n**Reasons for score**:\n\nWhile the idea of learning to predict a suitable step-size is intriguing and is definitely worth pursuing I am not convinced that the proposed algorithm results in an active policy that usefully adapts the step-size. There are too many concerns that I think needs to be addressed and it is not clear if the speed up improves over the reliability of a line search. I therefore vote to reject the paper in its current form.\n\n**Pros**:\n\n- It is clear that a lot of thought has gone into the project to come up with the policy. I think it might have merit but requires additional tests.\n\n- The figures were first difficult to understand but once the content had been explained in the text the benefit of the chosen presentation became clear.\n\n\n\n**Concerns**:\n\n- My main concern is best visualized in Figure 3. Both the learned policy and the BTLS seem to mostly favour a step of 1 which raises several questions. \n\t1) What would be the results of using no adaptation and rely on a step of 1 (or 0.9) constantly as a baseline? \n\t2) The $\\pi$-algorithm mostly uses a step-size of 1 which happens top be the upper boundary $\\tau_M$, which means it is not clear if the policy network has learned that 1 is a good step or if it has not learned at all and the results are just due to the clipping. What would happen if $\\tau_M>1$ for example? \n\t3) Given that both the BTLS and $\\pi$ mostly use $t_k=1$, is there any intuitive explanation as to why the results between the two algorithms differ by so much in figure 3? Are there additional figures where the BTLS similarly outperforms the competition (it did reach $10^{-5}$ first in ~60% of the tasks according to table 1, column 1)? This despite the fact that BTLS is at least 1 forward pass more expensive per iteration than the policy (for a fully connected network I think that is ~50% of the iteration cost).\n\n- The benefit of using the double network for the policy is not clear to me. What would be the result of using a single linear layer instead or a recurrent network that monitors temporal changes to the curvature information that is used as input?\n\n- Given that the input and output dimensionality of the policy network is of low dimension it would be interesting to see what the weights and biases look like for respective policy layers. By looking at the weights it would be possible to see what curvature information makes the network decide to adjust the step-length. Does the policy learn a cosine behaviour similar to the proposed possibility in the appendix? \n\n- Could the policy be used for another optimization algorithm for which $-g_t^\\intercal d_k>0$, such as RMSprop or GD? It might be easier to understand the influence of the policy in such a setting.\n\n\nComparably minor points:\n\n- Section 3 first paragraph ends with a statement regarding $\\rho_i$ to keep the Hessian approximation s.p.d by ignoring iterations. Is this used in the implementation? \n- Table 1: According to what metric is \"first\" defined (iteration, time, function evaluations)? It would be good to mention the average value of this metric for each optimizer at the end of $K=800$ inner steps.\n- In Section 6 it says that the learning rate of Adam and RMSprop was chosen to yield fast convergence. I understand this as quickly reaching a threshold, not achieving best performance for allocated time. Is this correct? That could help explain why so many settings in table 1 and figure 4 fail to converge. Personally I think the first-order algorithms should be allowed to change the learning rate between problems to prevent them from not converging (ex. RMSprop).\n- Eq.7 s.t. -> with, or is the outer problem actually constrained and I missed something?\n\n\n-------------------\n\n\n**Post Rebuttal**\n\nI have considered the revised article, additional reviews and rebuttal and decided to slightly raise my score but I am still in favor of rejecting the paper. Below is a summary of my reasoning.\n\n--------\n\nThe authors have provided a good rebuttal and I am overall pleased with the detailed response, additional experiments and figures, and overall exhibited transparency. \nUnfortunately my assumption about $t_k$ seemed correct when considering the additional L-BFGS-B results, which indicate that using standard $t_k=1$ is a really strong baseline that proved difficult to beat.\n\nI would suggest finding another set of problems where $t_k=1$ is not so good for L-BFGS or consider adapting another first-order algorithm for which it is clear that the step-length needs to change between tasks and architectures.", "title": "Official Review #2", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "p8mpCMet0wM": {"type": "rebuttal", "replyto": "YV1hoR5RwuU", "comment": "**Authors**\n\nWe would like to thank the reviewer for evaluating our work and sharing\nher/his thoughts here.\n\n**Reviewer**\n\n_Week part: 1) The paper\u2019s goal is limited to design the step-length\npolicy for one specific optimization algorithm L-BFGS. If the original\nL-BFGS algorithm is not effective for certain problems, then the learned\nstep-length policy may be un-useful as well._\n\n**Authors**\n\nThe reviewer is correct that the paper focus in L-BFGS specifically but\nwe would like to highlight that L-BFGS is generally the algorithm of\nchoice in large-scale deterministic optimization problems. However,\nthere are caveats concerning the step-size selection and this is the\nproblem we tackled in this paper.\n\n**Reviewer**\n\n_2\\) Based on Table 1\u2019s metric of gradient length smaller than\n$\\backslash$epsilon, the learned step-length policy seems not better\nthan Adam even in the higher dimension setup in which the Adam is using\nthe pre-tuned learning rate. The result in Fig. 5 seems better for the\nlearned step-length policy, but that uses the smallest value over\ntraining as compared value, which may not be \u2019fair\u2019. Because Adam,\nRMSProp, and L-BFGS are different algorithms, which may result in\ndifferent (local) minima in different problems._\n\n**Authors**\n\nNotice that in the revised version of the paper the results are\ndifferent from the previous. For the problem scale in which the policy\nwas trained it performs better than the competitors but the same is not\ntrue for an arbitrary different problem. We tried to motivate that by\ncarrying out an additional experiment. Also, regarding the analysis of\nthe attained final value, a new discussion was added in Appendix\u00a0C.\nHowever, notice that in the context of deterministic nonlinear\noptimization it is a good idea to keep the best iterate visited so far\nand allow the algorithm to explore other areas of the decision space.\nThat is why the minimum over the iterations was taken into account. To\nbe fair, the same criterion was used for the competitors.\n\n**Reviewer**\n\n_3\\) The performance test setup is not realistic in neural network model\ntraining. The setup for training the model (inner optimization) along\nwith step-length policy model (outer optimization) is with a fixed 1000\nimages (randomly selected from 60 batches of data) over T = 16 outer\niteration and 10 epochs. When comparing different algorithms\u2019\nperformance, it also splits the test dataset into 10 batches of 1000\nimages. And record performance for each fixed batch dataset, which is\nnot realistic in usual neural network model training. Usually, different\nbatches are randomly selected at each time step as opposed to being\nfixed. This makes the claimed performance doubtful in realistic setting._\n\n**Authors**\n\nWe agree with the reviewer that training using mini-batches is generally\npreferable in this context but notice that the goal of this evaluation\nis not to tackle state-of-the-art machine learning problems but rather\nbenchmark the algorithms against a deterministic nonlinear optimization\nproblem. For this framework, and especially in large-scale settings, we\nunderstand that L-BFGS is the algorithm of choice, even though there is\nno general consensus on how to choose its step sizes. We tried to\nhighlight that at the beginning of the experimental section.\n\n**Reviewer**\n\n_3\\. Based on the strong and week points of this paper, I tend to reject\nit._\n\n_4\\. Supporting arguments Please refer to my above arguments._\n\n_5\\. Questions_\n\n_1. Can author/s please explain the first two plots in Figure 4? It seems\nto me that both Adam and RMSProp are faster reaching to the required\ngradient norm precision. The precision seems different between these two\nalgorithms and the author/s claimed one. Why not use the same precision\nrequirement so that we can compare these algorithms directly?_\n\n**Authors**\n\nIn Figure 4 each bullet denotes the \u201cfinal time\u201d $t_f$ at which a\nprecision $||g_k||<\\epsilon$ was attained by two different algorithms\nfor a specific task. Basically, bullets above the blue line indicate\ntasks for which the algorithm in the x-axis performs better and\n*vice-versa*. If you consider the lowest precision $\\epsilon = 10^{-3}$,\nADAM is attaining it before our algorithm. However, higher values of\nprecision are not attained by ADAM and RMSprop in the course of $K$\niterations, and for those cases we consider $t_f=\\infty$. These cases\nare represented in the two boxes surrounding the main plot. These plots\ncompare algorithms directly and different precision values were adopted\nto compare how the stop criteria affect the performance.\n\n**Reviewer**\n\n_2\\. Can author/s provide any feedback on the third point of \u201cweak part\u201d?_\n\n**Authors**\n\nWe hope the feedback we provided above is enough.\n\n**Reviewer**\n\n_3\\. Can this proposed architecture/idea be used as a step-size policy for\nnot just L-BFGS?_\n\n**Authors**\n\nYes, and we have added some discussion in this sense to Section 3.", "title": "Author's response to AnonReviewer1 "}, "DFR-iATAya": {"type": "rebuttal", "replyto": "AHmhtI6uF2Q", "comment": "\n**Reviewer**\n\n_\\*\\* Description_\n\n_This paper makes two separate contributions to machine learning: (1) It\nproposes a neural network to learn the step-size policy of L-BFGS and\n(2) it uses the algorithm for training a deep neural network._\n\n_\\*\\* Pros_\n\n_In practical terms it provides a solution to the problem of finding an\nappropriate step-size policy. It also demonstrates that the policy out\nperforms ADAM and RMSprop on a rather strangely chosen problem._\n\n**Authors**\n\nWe would like to thank the reviewer for evaluating our manuscript and\nproviding useful comments.\n\n**Reviewer**\n\n_\\*\\* Cons_\n\n_Learning how to optimise by examining related problems is often a\nfruitless endeavour because small changes in a problem can drastically\nchange the behaviour of optimisation algorithms. It would have been nice\nto see more convincing evidence that learning a step-size polices\ngeneralises across at few more problem domains._\n\n**Authors**\n\nFollowing the suggestions of this and another review, we added an\nexample to investigate how the proposed strategy generalizes to other\nclasses of problems. More specifically, we took the already learned\npolicy (for training MLPs in MNIST) and used it for training CNNs in\nCIFAR-10. The straight generalization capacity was not evident in this\ncase, as the reviewer indicated. However, after a few additional\ntraining steps on this other class of problems provided a policy that is\nas good as its competitors, which indicates that learning can be quickly\ntransferred between problem domains.\n\n**Reviewer**\n\n_For me, the evaluation of their algorithm for training a neural network\nwas slightly unconvincing. Possibly this was just chosen as an example\noptimisation problem and the application shouldn\u2019t be taken too\nseriously (a comment to that effect would be useful). Obviously the use\nof full-batch training is not that practical for most problems. For\nneural network training, robustness to the noise produced by mini-batch\ntraining is important to understand. Although ADAM and RMSProp are\nstate-of-the-art optimisers for minibatch training when compared on\nfull-batch it would be useful to compare with other standard approaches\nsuch as conjugate gradient, etc. The choice of problem was puzzling.\nClearly an MLP on MNIST is not representative of modern machine\nlearning. It left the question of whether a small network was\ndeliberately chosen because the algorithm does not scale to really large\nnetworks. Again a comment about this would have strengthened the paper._\n\n**Authors**\n\nThe chosen example was inspired by one of the experiments in\nAndrychowicz et al. (2016). However, the goal of this evaluation is not\nto tackle state-of-the-art machine learning problems but rather\nbenchmark the algorithms against a deterministic nonlinear optimization\nproblem. For this framework, and especially in large-scale settings, we\nunderstand that L-BFGS is state-of-the-art even though there is no\nconsensus on how to choose its step sizes.\n\n\nThe idea to also use conjugate gradient within our comparisons was\nconsidered but, after doing some research, the preconditioning\nrecommended in the book \u201cConjugate Gradient Algorithms in Nonconvex\nOptimization\u201d by Radoslaw Pytlak for our setting uses an L-BFGS\niteration at each step which, in our understanding, would result in an\nalgorithm that relies on L-BFGS to perform well. Therefore, we did not\nadd this comparison but added some comments regarding this point.\n\nFinally, the proposed policy scales without difficulty to large problems\nas it is $O(n+n_h)$. For instance, the problem for $(n_l,n_u)=(4, 800)$\nhas $n=128,317,600$ parameters.\n\n**Reviewer**\n\n_I was left with the impression that the authors were being slightly\nselective in their choice of problems for showing the utility of their\nmethod. I would have liked to see more conclusive evidence in this\ndirection and a clearer discussion of the regime where this method is\nlikely to perform well._\n\n**Authors**\n\nUnfortunately, the theory on learned optimizers is yet scarce to the\nbest of our knowledge so formal guarantees of convergence and\nperformance are to be developed. However, to provide more evidence of\nthe policy performance, we added a second problem where CNNs are trained\nto classify CIFAR-10 images and we drew similar conclusions to the first\nexperiment. We agree with the reviewer that extensive experimentation\nwould be more enlightening, and this is also to be done in future work.\n\n**Reviewer**\n\n_\\*\\* Typos_\n\n_The paper is comprehensible, but would gain from being proof read by a\nnative speaker. This is not a consideration that affects the valuation.\nExamples of rewordings that would make the paper flow slightly better\nare_\n\n_p2 l4: good &lt;\u2013 well p2 l8: exempts &lt;\u2013 frees p2 l23: seek &lt;\u2013\nsearch p3 section 3 paragraph 2 line 2: it does ... &lt;\u2013 there does\n...\\_\n\n**Authors**\n\nThank you for putting your time into this, it was very much helpful!\n", "title": "Author's response to AnonReviewer4"}, "2hnrTcLJULC": {"type": "rebuttal", "replyto": "MVAcQqG4p0X", "comment": "\n**Authors**\n\nThis is a good suggestion but, unfortunately, the higher-dimension of\nthe new input vector prevents us from obtaining a good graphical\nvisualization of the policy behavior. Moreover, presenting the 204\nvalues defining the parameters in $\\theta$ may not be very insightful.\n\n**Reviewer**\n\n\u2013 _Could the policy be used for another optimization algorithm for which\n$-g_t^Td_k>0$, such as RMSprop or GD? It might be easier to understand\nthe influence of the policy in such a setting._\n\n**Authors**\n\nThe extension of these results to the stochastic framework is a very\ninteresting future work and we stressed that in the revised version of\nthe paper. Our work, however, focuses on the study of the deterministic\noptimization problem for which there is no good alternative to line\nsearches in the literature, to the best of our knowledge.\n\n**Reviewer**\n\n\u2013 _Section 3 first paragraph ends with a statement regarding $\\rho_i$ to\nkeep the Hessian approximation s.p.d by ignoring iterations. Is this\nused in the implementation?_\n\n**Authors**\n\nYes, we have used this and we have made it clearer in the revised\nversion.\n\n**Reviewer**\n\n\u2013 _Table 1: According to what metric is \u201cfirst\u201d defined (iteration, time,\nfunction evaluations)? It would be good to mention the average value of\nthis metric for each optimizer at the end of $K=800$ inner steps._\n\n**Authors**\n\nWe have added Appendix C that gathers similar data as presented in the\nmain text but with the defined index taking into account only the final\nvalue of the objective function. We drew similar conclusions for both\ndefinitions.\n\n**Reviewer**\n\n\u2013 _In Section 6 it says that the learning rate of Adam and RMSprop was\nchosen to yield fast convergence. I understand this as quickly reaching\na threshold, not achieving best performance for allocated time. Is this\ncorrect? That could help explain why so many settings in table 1 and\nfigure 4 fail to converge. Personally I think the first-order algorithms\nshould be allowed to change the learning rate between problems to\nprevent them from not converging (ex. RMSprop)._\n\n**Authors**\n\nThe reviewer is correct but what Fig. 4 is showing is whether the\nalgorithms attained the desired precision within the pre-specified\nhorizon $K$ and, if so, how much time did it take. This does not mean\nthat $t_f=\\infty$ implies in divergence, necessarily. Our interpretation\nfrom this comparison is that RMSprop and Adam, even when tuned for\nachieving the fastest convergence, fail to overcome the competitors in\nthis setting in terms of reaching gradient-base stopping criteria first.\nWe do agree that these algorithms should be allowed to change their\nlearning rates between different problems to make the best out of them\nbut that would not be a general tunning and therefore, not comparable\nwith the proposed policy which is not trained task-wise.\n\n**Reviewer**\n\n\u2013 _Eq.7 s.t. -&gt; with, or is the outer problem actually constrained and\nI missed something?_\n\n**Authors**\n\nIn this case, we have adopted a formal representation of the\noptimization problem where the iterates could be regarded as decision\nvariables (as they depend on the policy parameters). However, in\npractice, the \u201cs.t.\u201d can be regarded as a \u201cwith\u201d for implementation\npurposes.", "title": "Author's response to AnonReviewer2, part 2"}, "MVAcQqG4p0X": {"type": "rebuttal", "replyto": "AXMyWDcep89", "comment": "**Authors**\n\nWe would like to thank the reviewer for evaluating our work, for her/his\npositive comments, and for providing constructive feedback. We hope that\nthis revised version and our response help to remediate her/his\nconcerns.\n\n**Reviewer**\n\n1. _What would be the results of using no adaptation and rely on a step\nof 1 (or 0.9) constantly as a baseline?_\n\n**Authors**\n\nIn our revised version, we also compare our policy with a \u201cbaseline\nL-BFGS\u201d which adopt constant step sizes of 1. We also added more\nfeatures to the architecture which resulted in a policy that avoids step\nsizes too close to 1 at some iterations. It seems that the new input\nvector allows the policy to become aware of update directions that may\nresult in huge increases, allowing it to perform better than BTLS and\nthe baseline for the problem on which it was trained.\n\n**Reviewer**\n\n2. _The $\\pi$-algorithm mostly uses a step-size of 1 which happens top be\nthe upper boundary $\\tau_M$, which means it is not clear if the policy\nnetwork has learned that 1 is a good step or if it has not learned at\nall and the results are just due to the clipping. What would happen if\n$\\tau_M>1$ for example?_\n\n**Authors**\n\nIn the revised version, a different policy behavior is observed but, to\ninvestigate this point raised by the reviewer, we also tried letting it\nlearn $\\tau_M$. However, in our tests, a maximum step size too close to\n1 was obtained and then we decided to fix it as, generally, $t_k=1$ is a\ngood step size for this algorithm and the initial value for the BTLS.\n\n**Reviewer**\n\n3. _Given that both the BTLS and $\\pi$ mostly use $t_k=1$, is there any\nintuitive explanation as to why the results between the two algorithms\ndiffer by so much in figure 3? Are there additional figures where the\nBTLS similarly outperforms the competition (it did reach $10^{-1}$ first\nin $\\sim$60% of the tasks according to table 1, column 1)? This despite\nthe fact that BTLS is at least 1 forward pass more expensive per\niteration than the policy (for a fully connected network I think that is\n$\\sim$50% of the iteration cost)._\n\n**Authors**\n\nIn the revised version the BTLS and $\\pi$ are performing differently\nbut, in the last version, a possible better selection of step sizes in\nearly iterations could explain the cases where it performs better.\nIndeed, in the current version, the policy also prefers to be more\ncautious in the first steps what can be important to extract theoretical\ninsights of how an optimal step-size policy should perform. Regarding\nthe transparency, to provide a more uniform sampling of the results we\nhave added 10 similar plots to Fig. 3 corresponding to the 10 first\ntasks in the test set. These figures are in Appendix B.\n\n**Reviewer**\n\n\u2013 _The benefit of using the double network for the policy is not clear to\nme. What would be the result of using a single linear layer instead or a\nrecurrent network that monitors temporal changes to the curvature\ninformation that is used as input?_\n\n**Authors**\n\nThe proposed policy is based on the idea of comparing the scalar\nprojection of two vectors and then clipping the resulting value. We have\ntwo layers in parallel to compute two vectors and apply this operation.\nIf only one layer were adopted, a second constant or learned vector\nwould be necessary. However, learning a constant vector or providing an\narbitrary one have the same results (as one can always apply a linear\ntransformation to the output of the first linear layer by redefining its\nparameters). On the other hand, a second layer gives an additional\ndegree of freedom to the policy with no much larger computational cost,\nas the two layers are run in parallel (in the implementation we use only\none layer and split its output into two). Finally, we tried both cases\n(one layer + constant vector and two layers) and got better results with\nthis topology.\n\n\n**Reviewer**\n\n\u2013 _Given that the input and output dimensionality of the policy network\nis of low dimension it would be interesting to see what the weights and\nbiases look like for respective policy layers. By looking at the weights\nit would be possible to see what curvature information makes the network\ndecide to adjust the step-length. Does the policy learn a cosine\nbehaviour similar to the proposed possibility in the appendix?_\n\n", "title": "Author's response to AnonReviewer2, part 1"}, "V-WINISaZDB": {"type": "rebuttal", "replyto": "Be-ZHgVcbfw", "comment": "\n**Reviewer** _The paper studies a problem of learning step-size policy for L-BFGS\nalgorithm. This paper falls into a general category of meta-learning\nalgorithms that try to derive a data-driven approach to learn one of the\nparameters of the learning algorithm. In this case, it is the learning\nrate of L-BFGS. The paper is very similar in nature to the papers of\nRavi & Larochelle, MAML and Andrychowicz._\n\n_My biggest issue with this paper is the evaluation. The paper itself\ncites in the introduction: \u201c...for large-dimensional problems this\nprocedure is likely to become the bottle-neck of the optimization task\u201d.\nHowever, the paper doesn\u2019t not provide necessary evaluation to help\nresolving this issue. In fact, it is not very clear at all that the\nproposed method would work on a more general scenario when the\nevaluation dataset is wildly different from training the dataset._\n\n**Authors**\nFirst, we would like to thank the reviewer for her/his assessment of our\nwork and thoughtful comments. In the revised version of our manuscript,\nwe added a second experiment where we learned our policy by training MLP\nclassifiers for MNIST images and, subsequently we applied the policy to\na different problem which is training a CNN for CIFAR-10 images. Indeed\na direct generalization of our policy to this different problem is not\nassured but, after a few additional training steps on other classes of\nproblems, the policy can quickly learn how to perform well on these\nproblems. This shows that in our framework, although learning cannot be\ngeneralized from one class to another, it may be efficiently\ntransferred.\n\nThe correspondent part is:\n\n>\u201c Finally, as a last experiment, we applied the learned policy and these\ncompetitors to a class of tasks comprising the training of a\nConvolutional Neural Network (CNN) to classify images in CIFAR-10, see\n(Krizhevsky et al., 2009). The adopted architecture is described in\nZhang (2016) but sigmoid activation functions were replaced by ReLU to\nmake this problem even more distant from the one $\\pi$ was trained on. A\ntraining and a test set of tasks, $\\mathcal{T}\\_{train}^C $ and\n$\\mathcal{T}\\_{test}^C$, were built similarly to $\\mathcal{T}\\_{train}$\nand $\\mathcal{T}\\_{test}$ but using images in CIFAR-10 instead.\nEvaluating these algorithms in $\\mathcal{T}\\_{test}^C$ and computing the\nindex $\\mathcal{I}\\_a(f)$ for each task allows us to present the first\nbox plot in Fig.\u00a06. This figure indicates that $\\pi$ do not perform as\ngood as before in these problems. This could be expected as a different\narchitecture directly affects the nature of the objective function. To\ninvestigate whether the learned policy $\\pi$ can be used as a warm-start\nfor the training a new policy $\\pi^C$, we perform additional training\nsteps on $\\pi$ corresponding to 10 epochs in the training set\n$\\mathcal{T}\\_{train}^C$, but eliminating 5/6 of its tasks. This is done\nto show that even with very low effort placed in this retraining phase\nand considering fewer learning data, we can benefit from previous\nlearning to speed-up new training procedures. The corresponding results\nare presented in the second box plot of Fig.\u00a06, which shows that the new\npolicy $\\pi^C$ performs comparably to the competitors. Certainly,\nfurther investigation is required but this suggests that some learning\ncan be transferred across distinct problem domains.\u201d\n\n\n**Reviewer**\n_I\u2019m also a bit surprised to see this paper tackling specifically L-BFGS\nalgorithm, instead of more general case of learning rate parameter for\nany gradient basis algorithm. I would be curious to learn what is so\nspecial about L-BFGS that made the authors chose it. After all, the\npaper (and L-BFGS) deals with deterministic optimization problems on a\nfull batch which limits the applicability of the paper._\n\n\n**Authors**\nWe chose L-BFGS because as it is often the algorithm of choice in\ndeterministic unconstrained optimization. The examples we present may\nnot be state-of-the-art machine learning problems, which rely more often\nupon stochastic optimization approaches but the choice is to point the\ngap in the literature regarding the selection of a step-sizes for this\nalgorithm to perform as good and as automatic as possible.", "title": "Author's response to AnonReviewer3"}, "YV1hoR5RwuU": {"type": "review", "replyto": "AcH9xD24Hd", "review": "1. Paper contribution summary\n    This paper proposes a neural network architecture with local gradient/quasi gradient as input to learn the step-length policy from data for the specific L-BFGS algorithm. The step-length policy can be learned from the data with similar optimization problems. The designed neural network architecture for step-length policy is using inner product as input to allow the policy to be independent of the problem size, and the logarithm operation is used to enable products and division among powers of the inputs.\n    The numerical example shows that the learned policy is comparable to L-BFGS with backtracking line search and potentially better generalization to the same problem with higher dimension.\n\n2.  Strong and weak points of the paper\n    Strong part: 1) The network architecture for the step-length policy is very interesting, and may be useful for other problems with similar needs. 2) The numerical experiment shows that the learned step-length policy is comparable to the same algorithm with backtracking line searched step-length, which looks promising.\n    Week part: 1) The paper's goal is limited to design the step-length policy for one specific optimization algorithm L-BFGS. If the original L-BFGS algorithm is not effective for certain problems, then the learned step-length policy may be un-useful as well. 2) Based on Table 1's metric of gradient length smaller than \\epsilon, the learned step-length policy seems not better than Adam even in the higher dimension setup in which the Adam is using the pre-tuned learning rate. The result in Fig. 5 seems better for the learned step-length policy, but that uses the smallest value over training as compared value, which may not be 'fair'. Because Adam, RMSProp, and L-BFGS are different algorithms, which may result in different (local) minima in different problems. 3) The performance test setup is not realistic in neural network model training. The setup for training the model (inner optimization) along with step-length policy model (outer optimization) is with a fixed 1000 images (randomly selected from 60 batches of data) over T = 16 outer iteration and 10 epochs. When comparing different algorithms' performance, it also splits the test dataset into 10 batches of 1000 images. And record performance for each fixed batch dataset, which is not realistic in usual neural network model training. Usually, different batches are randomly selected at each time step as opposed to being fixed. This makes the claimed performance doubtful in realistic setting.\n\n3. Based on the strong and week points of this paper, I tend to reject it.\n\n4. Supporting arguments\n    Please refer to my above arguments.\n\n5. Questions\n    1) Can author/s please explain the first two plots in Figure 4? It seems to me that both Adam and RMSProp are faster reaching to the required gradient norm precision. The precision seems different between these two algorithms and the author/s claimed one. Why not use the same precision requirement so that we can compare these algorithms directly?\n    2) Can author/s provide any feedback on the third point of \"weak part\"?\n    3) Can this proposed architecture/idea be used as a step-size policy for not just L-BFGS?", "title": "Interesting submission about step-length policy learning", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "AHmhtI6uF2Q": {"type": "review", "replyto": "AcH9xD24Hd", "review": "** Description\n\nThis paper makes two separate contributions to machine learning: (1) It proposes a neural network to learn the step-size policy of L-BFGS and (2) it uses the algorithm for training a deep neural network.\n\n** Pros\n\nIn practical terms it provides a solution to the problem of finding an appropriate step-size policy.  It also demonstrates that the policy out performs ADAM and RMSprop on a rather strangely chosen problem.\n\n** Cons\n\nLearning how to optimise by examining related problems is often a fruitless endeavour because small changes in a problem can drastically change the behaviour of optimisation algorithms.  It would have been nice to see more convincing evidence that learning a step-size polices generalises across at few more problem domains.\n\nFor me, the evaluation of their algorithm for training a neural network was slightly unconvincing.  Possibly this was just chosen as an example optimisation problem and the application shouldn't be taken too seriously (a comment to that effect would be useful).  Obviously the use of full-batch training is not that practical for most problems.  For neural network training, robustness to the noise produced by mini-batch training is important to understand.  Although ADAM and RMSProp are state-of-the-art optimisers for minibatch training when compared on full-batch it would be useful to compare with other standard approaches such as conjugate gradient, etc.  The choice of problem was puzzling.  Clearly an MLP on MNIST is not representative of modern machine learning.  It left the question of whether a small network was deliberately chosen because the algorithm does not scale to really large networks.  Again a comment about this would have strengthened the paper.\n\nI was left with the impression that the authors were being slightly selective in their choice of problems for showing the utility of their method.  I would have liked to see more conclusive evidence in this direction and a clearer discussion of the regime where this method is likely to perform well.\n\n** Typos\n\nThe paper is comprehensible, but would gain from being proof read by a native speaker.  This is not a consideration that affects the valuation.  Examples of rewordings that would make the paper flow slightly better are\n\np2 l4: good <-- well\np2 l8: exempts <-- frees\np2 l23: seek <-- search\np3 section 3 paragraph 2 line 2: it does ... <-- there does ...\n", "title": "Review on Step Size Policy Network for L-BFGS", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Be-ZHgVcbfw": {"type": "review", "replyto": "AcH9xD24Hd", "review": "The paper studies a problem of learning step-size policy for L-BFGS algorithm. This paper falls into a general category of meta-learning algorithms that try to derive a data-driven approach to learn one of the parameters of the learning algorithm. In this case, it is the learning rate of L-BFGS. The paper is very similar in nature to the papers of Ravi & Larochelle, MAML and Andrychowicz.\n\nMy biggest issue with this paper is the evaluation. The paper itself cites in the introduction:  \"...for large-dimensional problems this procedure is likely to become the bottle-neck of the optimization task\". However, the paper doesn't not provide necessary evaluation to help resolving this issue. In fact, it is not very clear at all that the proposed method would work on a more general scenario when the evaluation dataset is wildly different from training the dataset.\n\nI'm also a bit surprised to see this paper tackling specifically L-BFGS algorithm, instead of more general case of learning rate parameter for any gradient basis algorithm. I would be curious to learn what is so special about L-BFGS that made the authors chose it. After all, the paper (and L-BFGS) deals with deterministic optimization problems on a full batch which limits the applicability of the paper.\n\n\n\n\n", "title": "Learn to learn approach for L-BFGS step size parametrization", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}