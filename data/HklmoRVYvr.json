{"paper": {"title": "Long History Short-Term Memory for Long-Term Video Prediction", "authors": ["Wonmin Byeon", "Jan Kautz"], "authorids": ["wonmin.byeon@gmail.com", "jkautz@nvidia.com"], "summary": "We propose a new recurrent unit, Long History Short-Term Memory (LH-STM) which incorporates long history states into a recurrent unit to learn longer range dependencies.", "abstract": "While video prediction approaches have advanced considerably in recent years, learning to predict long-term future is challenging \u2014 ambiguous future or error propagation over time yield blurry predictions. To address this challenge, existing algorithms rely on extra supervision (e.g., action or object pose), motion flow learning, or adversarial training. In this paper, we propose a new recurrent unit, Long History Short-Term Memory (LH-STM). LH-STM incorporates long history states into a recurrent unit to learn longer range dependencies. To capture spatio-temporal dynamics in videos, we combined LH-STM with the Context-aware Video Prediction model (ContextVP). Our experiments on the KTH human actions and BAIR robot pushing datasets demonstrate that our approach produces not only sharper near-future predictions, but also farther into the future compared to the state-of-the-art methods. ", "keywords": ["LSTM", "video", "long-term prediction"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a new recurrent unit which incorporates long history states to learn longer range dependencies for improved video prediction. This history term corresponds to a linear combination of previous hidden states selected through a soft-attention mechanism and can be directly added to ConvLSTM equations that compute the IFO gates and the new state. The authors perform empirical validation on the challenging KTH and BAIR Push datasets and show that their architecture outperforms existing work in terms of SSIM, PSNR, and VIF.\nThe main issue raised by the reviewers is the incremental nature of the work and issues in the empirical evaluation which do not support the main claims in the paper. After the rebuttal and discussion phase the reviewers agree that these issues were not adequately resolved and the work doesn\u2019t meet the acceptance bar. I will hence recommend the rejection of this paper. Nevertheless, we encourage the authors improve the manuscript by addressing the remaining issues in the empirical evaluation."}, "review": {"HJgpVslatS": {"type": "review", "replyto": "HklmoRVYvr", "review": "The paper proposes a type of recurrent neural network module called Long History Short-Term Memory (LH-STM) for longer-term video generation. This module can be used to replace ConvLSTMs in previously published video prediction models. It expands ConvLSTMs by adding a \"previous history\" term to the ConvLSTM equations that compute the IFO gates and the candidate new state. This history term corresponds to a linear combination of previous hidden states selected through a soft-attention mechanism. As such, it is not clear if there are significant differences between LH-STMs and previously proposed LSTMs with attention on previous hidden states. The authors propose recurrent units that include one or two History Selection (soft-attention) steps, called single LH-STM and double LH-STM respectively. The exact formulation of the double LH-STM is not clear from the paper.  The authors then propose to use models with LH-STM units for longer term video generation. They claim that LH-STM can better reduce error propagation and better model the complex dynamics of videos. To support the claims, they conduct empirical experiments where they show that the proposed model outperforms previous video prediction models on KTH (up to 80 frames) and the BAIR Push dataset (up to 25 frames).\n\nOverall I believe there are serious flaws with the paper that prevent acceptance in its current form.\n\nFirst, I believe the paper starts from the wrong assumption, namely that current video prediction models are limited by their capacity to limit the propagation of errors and to capture complex dynamics. Instead, it is well known that the main difficulty for longer term video prediction is to manage the increasing uncertainty in future outcomes. Stochastic models such as SVG-LP or SAVP are currently the state-of-the-art in video generation, with deterministic models not being able to generate more than a few non-blurry frames of video. While the authors mention that they do not focus on future uncertainty here, it is not clear how the proposed model helps to generate better longer-term videos when it does not deal with what actually makes long-term video generation difficult. In addition, it's misleading to claim that current models produce high quality generations for \"only one or less than ten frames\", especially without defining high quality. Models such as SVG [1] or SAVP[2] can produce non-blurry videos for 30-100 frames for the BAIR dataset, for example. \n\nThe experiments are missing 1) SVG as a baseline, 2) metrics that correlate with human perception such as LPIPS or FVD [3] and 3) qualitative samples that compare to stochastic models. Deterministic models can achieve very high PSNR/MSE/SSIM scores but produce very bad samples, as these scores are maximized by blurry predictions that conflate all possible future outcomes. This is highly apparent when looking at samples, and metrics that correlate better with human perception are usually better to compare video prediction methods. Comparisons to SAVP are found in Table 1 and 3 but there are no figures comparing samples from this model to the proposed model. The samples from the proposed model on the BAIR Push dataset for example (found in the appendix) are of significant lower quality than those reported from SAVP or SVG, and at the same time they are not longer-term than the predictions from these models. Consequently, the experimental section does not correctly assess how this model can generate better longer-term prediction than current models and it also does not give an accurate assessment of the model with respect to the current state-of-the-art.\n\nTo sum up, the paper does not adequately address how the proposed model allows for longer-term video generation. It is missing critical qualitative comparisons to state-of-the-art models such as SVG and it is unclear how the proposed model is different from a ConvLSTM with attention on previous hidden states.\n\n[1] Stochastic Video Generation with a Learned Prior. E.Denton and R. Fergus. ICML 2018\n[2] Stochastic Adversarial Video Prediction. Lee et al. Arxiv 2018\n[3] Towards Accurate Generative Models of Video: A New Metric & Challenges. Unterthiner et al. Arxiv 2018\n\n\n--- Post-discussion update ---\nThe authors have addressed a number of points raised by the reviewers and I'm raising my score to a weak reject from a reject. There are important remaining issues with the experimental section and the conclusions reached from their results, and therefore I still think the paper is below the acceptance bar.", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "H1xIL0o2jB": {"type": "rebuttal", "replyto": "rJxGfT9hsH", "comment": "The effectiveness of our paper compared to attention over inputs instead of states are shown in Section 4.2. \nIt is true that attention on LSTM in language modeling is popular, however this has not bee explored in video prediction. Considering such high-dimensional spatio-temporal data has different properties, we believe that our contribution is important for the community.\nThanks for the suggestions. We will consider them in the next update. ", "title": "Response to Reviewer #3"}, "SJeb-wohsH": {"type": "rebuttal", "replyto": "rJlUqJonoB", "comment": "There appears to have been a misunderstanding. We did not average the predictions. Our reported numbers are the median score out of the scores of all 100 predictions. \nIf one out of many samples produce a perfect prediction, it is not really useful in reality. High median score means the model will more likely produce good prediction on average. \nThe next suggestion is a good idea. We will consider this evaluation in the next update. Nevertheless, we believe that our current metric is more useful than the comparison based on best samples in previous work. ", "title": "Misunderstanding"}, "ryxzNTRoiS": {"type": "rebuttal", "replyto": "SyeHF9HbsH", "comment": "We have uploaded a GIF showing sample comparisons of 7 videos to an anonymous sharing website. \nLink: https://postimg.cc/nCC5k2nc \nThe first row is the ground truth. \nThe second, third, and last row show outputs of Double LH-STM, SAVP-Deterministic, and SAVP-VAE, respectively. \nEach column corresponds to a separate test set video sample. \nWe hope this addresses your concern. ", "title": "video"}, "rkg4ojRssH": {"type": "rebuttal", "replyto": "HklmoRVYvr", "comment": "We have now updated the main text of the paper as well based on all comments from the reviewers.  In particular, we have added a discussion about stochasticity to the introduction that clarifies our focus. ", "title": "paper updated"}, "H1gUksXcsS": {"type": "rebuttal", "replyto": "BJlXGS0FiS", "comment": "We believe that having a better architecture and dealing with model uncertainty are both important for video prediction, and both are unsolved problems. In this paper, we focus on the first part.\nWe agree that the real world is not deterministic but not completely stochastic either. We do not claim that our model can solve the problem of unknown influence over time. Instead, our model extracts the information from the past and maintains consistency over time. We would like to show that this is also an important factor for long-term prediction. As mentioned earlier, we will add a discussion about this to the introduction, and will gladly remove any part of the paper that gives the wrong impression about the importance of stochasticity. ", "title": "Response about stochasticity"}, "rkls7c75sS": {"type": "rebuttal", "replyto": "HklVnt0KjS", "comment": "1. The significant difference between our paper and others is: where the attention is used and what information is attended to. \n\nAttention modules in these papers soft-select video frames/sub-volumes directly. This process is independent from ConvLSTM. This mechanism is similar to 'input history' in our paper Section 4.2 (Fig. 4 left).  \n\nIn our paper, the attention soft-selects the hidden states inside of ConvLSTM. One of the major components in ConvLSTM/LSTM is a hidden state ($H_{k-1}$. The idea in our paper is that LSTM module can also use its own past information from the states $H_{k-m:k-2}$ for the time step $k$. The attention here is used to extract the past information by comparing it with the last state $H_{k-1}$ (see Fig. 1 (c) and Fig. 2). \n\nWe believe that these states compress spatio-temporal information from the entire video better to select the relevant past information for video prediction. The impact of our proposed attention instead of using 'input' is shown in Tab. 1. \n\nThe closest work we believe is [Cheng2016] using a standard LSTM for language models (referred in our paper, Section 2). We will add [4] and [5] as the relevant previous works as well. \n\n2. We meant the performance comparison between SVG and SAVP-VAE are already reported in the SAVP paper (page 10-12 in their Arxiv paper) in terms of realism. Therefore, we did not report the numbers separately in this paper. We will refer SVG and their performance compared to SAVP-VAE in our paper. \n\n3. We do not claim that our model can predict 'longer' then other papers, but learns long-term dependencies for videos. Other papers such as SAVP or SVG measured their performance up to 30-40 frames in their papers. We followed the same, then extended the evaluation to 80 frames. Only one figure in SVG shows some samples predicting 100 frames, and there are samples on the websites, but there are no systematic evaluations over the complete test set like we provide. \n\nThe 'Longer prediction' in Section 4.3 refers longer than 40 frame prediction. We will correct the language in our paper, so the readers will not get the impression that our model can predict longer than others. ", "title": "Response to Reviewer #3"}, "SklElF75oB": {"type": "rebuttal", "replyto": "S1eCthRFjr", "comment": "SAVP-VAE numbers in their paper are obtained by first generating 100 output samples per input and then selecting the best generated sample for each input based on the highest score between the output and the ground truth. This evaluation measures the best prediction the model *can* do, given 100 tries. However, it does not provide an overall picture of the generations from the model. In real world scenarios, the ground truth is not available, so the 'best' samples cannot be picked as done in such evaluations. Therefore, we re-computed the scores for stochastic models based on their 'median' output sample (instead of best) among the 100 randomly generated ones, compared to the ground truth (stated in our paper, page 6 footnote). This strategy measures how well the model can be expected to perform on average, and so it is a more representative score. We will update the paper with a detailed discussion of our evaluation. ", "title": "About SAVP-VAE evaluation with LPIPS metric"}, "SygbgGVFsH": {"type": "rebuttal", "replyto": "HklmoRVYvr", "comment": "We have updated the paper with more results (see Fig. 5-8 and 10-19).\n\n1. SAVP-VAE results have been included for all comparisons. [reviewer 2 and 3]\n\n2. LPIPS metric has been added  for all additional results. [reviewer 3]\n\n3. Separating the data with different action classes. (Fig 5, 6 and Fig. 12, 13) [reviewer 1]\nIn addition to the results with all actions (Fig. 5), we also show per-frame results with only 'boxing', 'waving', and 'clapping' (Fig. 6) action classes for 80 frame prediction. Double LH-STM results are generally better than SAVP-Deterministic and -VAE on all metrics. This shows that our model is not just copying the background when predicting more than 40 frames. We additionally provide per-frame comparisons for each action class in Fig. 12-13. \n\n4. Separating the data with large and small motion similar to Villegas et al., 2017a. (Fig. 10, 11) [reviewer 1]\nWe split the video samples into five bins by motion magnitude computed as averaged L2 norm between target frames. Overall, across all metrics, all models perform worse when the motion is larger. For PSNR and SSIM, Double LH-STM achieves the best performance except for the largest motion. For LPIPS, Double LH-STM performs better for small motions (first two bins). SAVP-Deterministic performs the best on this metric for larger motions.\n\n5. More samples for both datasets. (Fig. 8, 14-16 for the KTH action dataset and Fig. 17-19 for the robot push dataset) [reviewer 1,2,3]\nOn KTH action dataset, samples in Fig. 8, 14 and 15 show that motion and human shape predicted by Double LH-STM are the closest to the ground truth. On robot push dataset, random robot motion makes long-term prediction extremely hard with deterministic models as reviewer 3 pointed out. However, the overall motion prediction with Double LH-STM and SAVP-Deterministic is still reasonable compared to SAVP-VAE. We also included the samples with one of the largest motions for both datasets (Fig 16 and 19). When motion is larger, all models fail to produce reasonable predictions, whether they are deterministic or stochastic. \n\n6. The exact formulation of the double LH-STM has been added. (Eq. 5, 6) [reviewer 3]\n\nThis update only includes extra Figures and Equations requested by reviewers, but we have not updated the main text yet. We will update it soon.  \nWe welcome additional suggestions or questions. ", "title": "Paper updated with requested results and equations"}, "SJg1ihBWsB": {"type": "rebuttal", "replyto": "rkgTTMcKKB", "comment": "We appreciate the reviewer\u2019s constructive feedback. \n\n1. Novelty\nDespite similarities among various attention-based approaches, specific details of the attention mechanism can make models more expressive and easier to train. We believe the differences between our model and E3D-LSTM are crucial.\n\nIn the attention mechanism of E3D-LSTM, the gate vector R_{k} (recall gate) is first computed with values in (0, 1) due to sigmoid. This recall vector is then compared to past cell states (unbounded) via dot product to compute the attention weights, which are finally applied to select the relevant past states.\n$\\text{Attention}(R_{k}, C_{k-m:k-1}) = \\text{Softmax}(R_{k} \\cdot C_{k-m:k-1}) \\cdot C_{k-m:k-1}$, where $R_{k}$ is the recall gate.\n\nIn contrast, in our model, we first transform all hidden states H to H^tilda. The most recent transformed hidden state (H^tilda_{k-1}) is then compared to those in the past to compute the attention weights. We believe that this is better than E3D-LSTM because the vectors being compared are of the same 'type' (transformed hidden states) and range of values, making our attention mechanism more natural and intuitive.\n$\\text{Attention}( \\tilde{H}_{k-1}, \\tilde{H}_{k-m:k-2} ) = \\text{Softmax}(\\tilde{H}_{k-1} \\cdot \\tilde{H}_{k-m:k-2}) \\cdot \\tilde{H}_{k-m:k-2}$, where $\\tilde{H}$ indicates the transformed hidden state.\n\nThe Double LH-LSTM is a very specific method of compressing accessible past information into a single state that significantly improves results without requiring more parameters. Thus it builds upon the Single LH-STM in a unique way.\n\n2. Comparison with other existing methods for long-term prediction\nWe provided the comparisons not only with Context-VP but also with SAVP-deterministic and -VAE [Lee et al., 2018] in Table 3 and with SAVP-deterministic in Figs 5 and 8.\nWe did not directly compare with SVG-LP [Denton et al. 2017] as SAVP-VAE produces similar results to it (reported in [Lee et al., 2018] Section 4.4)\n\nWe will respond to the rest of feedback soon. ", "title": "Response to Reviewer #2"}, "HkeBAjr-iS": {"type": "rebuttal", "replyto": "HJgpVslatS", "comment": "1. \"it is unclear how the proposed model is different from a ConvLSTM with attention on previous hidden states\".\nConvLSTM with attention on previous hidden states would be similar to our single LH-STM in time direction, but to our knowledge, there is no previous work that uses such a module. Could you provide any reference we have missed?\nIn addition, we also proposed Double LH-STM which produces better results without requiring more parameters. This modification has not been explored in previous works with attention.\n\n2. \"SVG as a baseline\"\nWe did not directly compare with SVG since SAVP-VAE [2] produces similar results to it (reported in [2] Section 4.4).\n\n3. \"they are not longer-term than the predictions from these models.\"\nWe followed a similar experimental setup to SAVP [2]. SAVP predicts 30 frames on KTH and 28 frames on BAIR. We reported 40 and 80 frames on KTH and 25 frames on BAIR.\n\n4. \"The exact formulation of the double LH-STM is not clear from the paper.\"\nDouble LH-STM is divided into two blocks: History LSTM (H-LSTM) and Update LSTM (U-LSTM). The formulation of H-LSTM is shown in Eq 5. U-LSTM is the same as the standard LSTM (Eq 2) except the hidden state (H_{k-1}). The hidden state is replaced with the output of H-LSTM (H'_{k-1}). We will add the U-LSTM formulation in the paper.\n\nWe will respond to the rest of the feedback as soon as possible. ", "title": "Rest of response to Reviewer #3"}, "SyxmMjSWoS": {"type": "rebuttal", "replyto": "HJgpVslatS", "comment": "We agree that stochasticity is a crucial challenge and it is very important to find better ways of incorporating uncertainty into future prediction. However, we emphasize that it is not the only challenge we need to overcome. It is not settled that the network architectures currently in use are sufficiently powerful and efficient for long-term prediction. Regardless of whether the model produces stochastic or deterministic outputs, it needs to extract important information from spatio-temporal data and retain this information longer into the future efficiently. If it unables to do so, its uncertainty about the future will increase even if the future is completely predictable given the past. Therefore, developing better model architectures is still important.\n\nTo support our claims, we point the reviewers to Fig. 4 of the SAVP paper [2]. Lee et al. [2] reported in the evaluation of the KTH dataset that even the predictions of SAVP-deterministic model do not degrade over time but can still be blurry. Having stochasticity in the model alone (SAVP-VAE) does not reduce blurriness in the predictions. Because of the ability of our model, we can produce sharper predictions compared to both SAVP-deterministic and -VAE.\n\nOn the robot push dataset, the concern of stochasticity is valid due to random motion in the videos. It is true that the output samples in this dataset are blurrier than those reported from the SAVP paper [2]. However, please note that the stochastic model samples shown in the SAVP paper are selected based on the highest VGG cosine similarity score between the output and the ground truth. In general, stochastic models can be expected to perform better for such random motion. However as we noted earlier, stochastic models are also likely to benefit from more powerful architecture just like deterministic models do in our evaluation. This is an important avenue to be explored.\n\nWe will add this important discussion about stochasticity to the introduction in our paper.", "title": "The importance of stochasticity for long-term video prediction"}, "SyeHF9HbsH": {"type": "rebuttal", "replyto": "r1xjMAW0tH", "comment": "We thank the reviewer for constructive comments. \n\n1. \"Larger number of parameters in comparison to ContextVP-4\"\nBoth Single and Double LH-STM use ContextVP as the base model.\nWe adjusted the number of neurons, so all models have similar model size (\u224817M) (stated in Appendix A).\nDue to the different design of the model, we could not make the exact same number of parameters (Single is 7.5% larger than ContextVP).\nTo follow the rule of thumb for GPU parallelization, we kept the layer size divisible by 32.  \nDouble LH-STM has the smallest number of parameters (6.3% less than ContextVP) but produces the best performance. This indicates that the performance boost is not due to the parameters.  \n\n2. Why retrain for longer sequences?\nDue to lack of GPU memory, we could not use the original resolution for 80 frame prediction.\n\nWe will respond to the rest of the comments as soon as possible.", "title": "Response to Reviewer #1"}, "rkgTTMcKKB": {"type": "review", "replyto": "HklmoRVYvr", "review": "This paper presents a new RNN unit based on ConvLSTM for long-term video prediction. The proposed method is technically correct but lacks enough originality. I tend to reject this paper due to the following three reasons:\n\n1. The major novelty of this paper is the LH-STM unit, which applies a temporal attention approach to historical hidden states. This module is very similar to the Recall gate of the E3D-LSTM [Wang et al. 2018b]. Besides, the Double LH-STM looks like an incremental extension of the Single LH-STM. As mentioned, it is technically correct, and yet has limited novelty for an ICLR paper.\n\n2. The authors mainly compared the proposed network with the Context-VP model in the experiments (Fig 5, Fig 8, and Table 3), which is not enough. As far as I know, there are other existing methods for long-term video prediction, e.g. [Denton et al. 2017]. \n\n3. Another problem of the experiments is that Lee et al. [2018] proposed a stochastic model for video prediction, but the authors only compared the LH-STM with its deterministic version.\n\n4. In Figure 11, there is no significant improvement by using LH-STM. Also, the authors might include more compared models on the pushing dataset.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 3}, "r1xjMAW0tH": {"type": "review", "replyto": "HklmoRVYvr", "review": "Summary:\nThis paper proposes a new LSTM architecture called LH-STM (and Double LH-STM). The main idea deals with having a history selection mechanism to directly extract what information from the past. The authors also propose to decompose the history and update in LH-STM into two networks called Double LH-STM. In experiments, the authors evaluate and compare their two architectures with previously proposed models. They show that their architecture outperforms previous in the PSNR, SSIM and VIF metrics.\n\n\nPros:\n+ New architecture that can use all computed state history in a sequence\n+ Outperforms previous methods in the used metrics\n\nWeaknesses / comments:\n- Larger number of parameters in comparison to ContextVP-4\nIn Table 1, the authors present a comparison to previous works and the respective number of parameters used for most of the methods. It is mentioned in the paper that Single LH-STM uses an architecture similar to ContextVP-4. Since the architectures are similar and the proposed method is added to this architecture, can the authors make sure that Context VP-4 and Single LH-STM have around the same number of parameters? This can give a more direct comparison in performance to make sure that the parameter boost is not the reason for performance boost.\n\n\n- Why retrain for longer sequences?\nThe authors have experiments where they attempt to predict past 40 frames into the future. However, they mention that for this experiment, they train another network that takes in 64x64 pixels. Optimally, they should just let the 128x128 network predict past 40 frames. Can the authors comment on why they don\u2019t just do this? Is long-term prediction limited to how many previous states you can store in the GPU?\n\n\n- PSNR, SSIM, and VIF could be biased to blurriness and perfect background reconstruction\nIt has been shown before that PSNR and SSIM can be biased to blurriness and perfectly copying the background (Villegas et al., 2017b). Therefore, these metrics should be complemented with other metrics such as actual humans look at the videos and evaluated them for realism. The fact that these metrics look good in this paper can be due to blurriness. There is clear evidence of blurry predictions in the comparison Figures in the main paper and supplementary material. In addition these metrics, and VIF can also be biased to perfectly copying the background. I suggest the authors separate the data into videos with large motion and little motion similar to Villegas et al., 2017a. This way we can better evaluate this method on how well it predicts videos with large motion in comparison with videos where copying the background is enough.\n\n\n- Testing for 80 frame sequences is not very meaningful in this dataset.\nThe KTH dataset contains the action categories of running, walking, and jogging which most of these videos do not go up up 40 frames while the person still being in the frame. Therefore, after frame 40, the method is just required to copy the background and it will look like the future is perfectly predicted.. Can the authors clarify if they only tested on handwave, handclap and boxing? Handwave, handclap and boxing are the only categories that will still have a human present in the video at frame 80.\n\n\n- No video files are provided.\nFinally, this method does not provide any videos to better evaluate the proposed network. Looking at videos is necessary to observe temporal consistency and blurriness happening in the video. Or humans appearing and disappearing randomly.\n\n\nConclusion:\nThe proposed method is novel and interesting, but the experimental section has many issues as discussed above. If the authors successfully address these issues, I am willing to increase my score.\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 4}}}