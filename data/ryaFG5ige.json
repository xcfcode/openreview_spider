{"paper": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"], "summary": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "keywords": ["Deep learning", "Supervised Learning", "Optimization"]}, "meta": {"decision": "Reject", "comment": "The reviewers agree that the paper pursues an interesting direction to explore active example selection for CNN training, but have unanimously raised serious concerns with regards to overall presentation which needs further improvement (I still see spelling/grammatical errors/sloppy notation in the latest draft). Some sections in the paper are hard to follow. With regards to technical motivation, the link between depth and need for active example selection is alluded to, but not properly explained in the paper. The PCs think that this paper has too many areas in need of improvement to be accepted to the conference."}, "review": {"rktZK_DUe": {"type": "rebuttal", "replyto": "B1qrOdwIx", "comment": "Another experiments demonstrating that sampling a larger subset D does not affect the accuracy is available in the apendix of the paper.", "title": "Supplementary experiments"}, "B1qrOdwIx": {"type": "rebuttal", "replyto": "BJv2ZLQVe", "comment": "We are grateful to AnonReview3 for the interest and feedback. We have revised the paper to both improve the overall organization and stress the distinction between our contributions and previous works like the one from Graves which is now described in the related works. We have also revised the notations and mathematical formulations. The new version is now available on openreview. \nWe have validated our method on traditional datasets for the active learning community which are USPS and MNIST. Those are relatively small datasets compared to ImageNet, thereby not requiring networks as deep as for bigger datasets. The theory and the scalability of our method however holds for deeper networks. \nThe experiments made appear that curriculum learning is not a good active learning strategy for both tested datasets. As for the uncertainty selection, it works really well on MNIST while it fails on USPS.  While MNIST is a pretty clean database, USPS contains more outliers and noisy samples rendering it more difficult in terms of accuracy even though both databases are designed to assess digit classification. As other works we mentioned in the related work section, we are led to explain uncertainty selection to select useless samples with the amount of outliers and noisy samples in USPS.", "title": "Proof of concept for active learning with CNNs"}, "BJPXuuDUx": {"type": "rebuttal", "replyto": "r1HGpiBNg", "comment": "We are grateful to AnonReview4 for the interest and feedback. We have corrected the typos and grammatical errors and modified the structure of the text: we have trimmed the descriptions of unrelated active learning methods in the related work section and extended the discussion on Bayesian inference for active learning. We also included a detailed description of Graves' work on variational inference for neural networks. We have modified the paper to stress out the distinction between previous works (Graves) and our contributions, in particular to highlight our own proposition to get prior and posterior distributions of the weights from statistical assumption on the Maximum Likelihood Estimator. The new version is now available on openreview. ", "title": "Interesting ideas but not well explained"}, "S1Hqv_vUg": {"type": "rebuttal", "replyto": "HyRPClKNl", "comment": "We are grateful to AnonReview2 for the interest and feedback. We have paid a great attention to rewriting the paper, in particular straightening up the mathematical notations and restructuring the presentation of several sections. We specifically focused on introduction and related works so as to make clearly appear the novelty of our contribution. The new version is now available on openreview. As we focus on an active learning strategy, assessing the benefit of this very strategy on a dataset pre-requires to have the complete implementation of the training process on the full dataset for the right deep architecture. It will be very interesting to assess how the strategy performs on bigger datasets. Owing to the limited time frame, we could not get these results yet.", "title": "Very interesting but hard to follow"}, "Sy5hbH-Vg": {"type": "rebuttal", "replyto": "Bk1NfRJXe", "comment": "We thank warmly AnonReview2 for his feedbacks and interests!\n\nFor a lack of time before the deadline, we run 5 experiments for each baseline\non MNIST while we run 10 experiments for USPS. We update the papers so the\nbaselines on MNIST are compared on a basis of 10 runs. Figures correspond\nto the curves averages over several tests.\n\n", "title": "Experiments"}, "rJ-tMSbEe": {"type": "rebuttal", "replyto": "BJctfh1Xg", "comment": "We thank warmly AnonReview4 for his feedbacks and interests!\n\nThe sampling distribution is initialized randomly, we also sampled the initial training set randomly for every runs.\n\nIndeed the approximation of Q(\\beta) is crucial for the importance sampling of the data. \nPapers proposing active learning with second order information (1, 2, 3) assume that the current MLE estimator w hat, is already\na good approximation to the true parameter and we follow that guide of line.\n\n\nreferences\n(1) Zhang, T., & Oles, F. (2000). The value of unlabeled data for classifica-\ntion problems. In Proceedings of the Seventeenth International Conference on\nMachine Learning,(Langley, P., ed.) (pp. 1191-1198).\n(2) Hoi, S. C., Jin, R., Zhu, J., & Lyu, M. R. (2006, June). Batch mode active\nlearning and its application to medical image classification. In Proceedings of\nthe 23rd international conference on Machine learning (pp. 417-424). ACM.\n(3) Chaudhuri, K., Kakade, S. M., Netrapalli, P., & Sanghavi, S. (2015). Conver-\ngence rates of active learning for maximum likelihood estimation. In Advances\nin Neural Information Processing Systems (pp. 1090-1098).", "title": "Approximation of Q(\\beta)"}, "ry3rbSbEg": {"type": "rebuttal", "replyto": "BJWRG6xmx", "comment": "For a sake of concision, as the\ninitial submission is already two pages more that a standard submission should\nbe, we did not present further comparisons with simple approach. We updated\nour paper with comparison with two naive baselines : uncertainty sampling and\ncurriculum sampling both based on the likelihood score of a sample (because\nthe true label of a sample is unknown, the likelihood is computed based on the\nprediction of the current network). Uncertainty will select samples on which the\nclassifier is uncertain about, that means samples with the highest negative log\nlikelihood. On the contrary, curriculum sampling selects the sample on which\nthe classifier is the more certain about. Figures 1 and 2 have been updated accordingly.", "title": "Naive baseline"}, "HkX1WHZNl": {"type": "rebuttal", "replyto": "S1RBQaxQx", "comment": "The time required for using a minibatch to update the model by backprop is\nmuch faster: the mean time for processing a minibatch (forward and backprop\npass) on USPS (8 samples) is 9.65475 ms with a Titan-X (GTX 980 M) with 8\nGB RAM GPU memory. Because finding the optimal subset is a combinatorial\noptimization problem and it is necessary to test every solution to check\nwhether one solution is optimal, the original problem is EXPTIME complete. We\npropose a polynomial heuristic, which runs approximately in Nlog(N) given N\nthe number of unlabelled samples evaluated as potential queries.", "title": "Time complexity"}, "BJ1eeHWEg": {"type": "rebuttal", "replyto": "H1gu7Te7g", "comment": "We call groundtruth the average accuracy on 10 runs of the test error obtained\nwith the full annotated data shuffled randomly. Note that we do not optimize\nthe hyperparameters specifically for the size of the current annotated training\nset.", "title": "Groundtruth meaning"}, "HJenJHZEg": {"type": "rebuttal", "replyto": "SJuxNag7e", "comment": "We thank warmly AnonReview3 for his feedbacks and interests !\n\nOur code is now available on the github account mducoffe, it is developed with\nTheano and Blocks. If the approximations mentionned are related to the KFAC\nrepresentation, the author\u2019s version is supposed to be also published soon.", "title": "Is our code available ?"}, "BJWRG6xmx": {"type": "review", "replyto": "ryaFG5ige", "review": "You mention existing work on the simple approach of uncertainty sampling. Did you try this as a baseline?This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these \"deep\", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.\n\nThe paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. \n\nThe paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.\n\nI have one more question: why is it necessary to first sample a larger subset D \\subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)\n", "title": "Simple active learning baseline?", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "BJv2ZLQVe": {"type": "review", "replyto": "ryaFG5ige", "review": "You mention existing work on the simple approach of uncertainty sampling. Did you try this as a baseline?This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these \"deep\", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.\n\nThe paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. \n\nThe paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.\n\nI have one more question: why is it necessary to first sample a larger subset D \\subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)\n", "title": "Simple active learning baseline?", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "Bk1NfRJXe": {"type": "review", "replyto": "ryaFG5ige", "review": "In the Experiments of figure 2 what criterion did you use to decide to use 5 or 10 runs of experiments ? Does figure 2 correspond to only one active learning test or are the curves averages over several tests ?The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated.\n\nThe Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow.\n\nThe paper is written in poor English and is sometimes a bit painful to read.\n\nAlternative Active learning strategies and techniques do not need to be described with such detail. On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).", "title": "Experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "HyRPClKNl": {"type": "review", "replyto": "ryaFG5ige", "review": "In the Experiments of figure 2 what criterion did you use to decide to use 5 or 10 runs of experiments ? Does figure 2 correspond to only one active learning test or are the curves averages over several tests ?The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated.\n\nThe Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow.\n\nThe paper is written in poor English and is sometimes a bit painful to read.\n\nAlternative Active learning strategies and techniques do not need to be described with such detail. On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).", "title": "Experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "BJctfh1Xg": {"type": "review", "replyto": "ryaFG5ige", "review": "In the approximation of Q(\\beta), which is crucial for the importance sampling of the data in the authors' framework of active learning, they assumed the current MLE estimator w_hat, is already a good approximation to the true parameter \\theta_Y^*; how can this be justified? How is the sampling distribution initialized?Quality:\nThe paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network. \nHowever, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.\n\nClarity:\nThe paper needs major improvements in terms of clarity. The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper. For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology. \nThe related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons. In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11.\n\nOriginality & Significance:\nThe authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)\n\nI think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning. But the paper needs to be improved in terms of presentation.\n\n", "title": "A strong assumption?", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "r1HGpiBNg": {"type": "review", "replyto": "ryaFG5ige", "review": "In the approximation of Q(\\beta), which is crucial for the importance sampling of the data in the authors' framework of active learning, they assumed the current MLE estimator w_hat, is already a good approximation to the true parameter \\theta_Y^*; how can this be justified? How is the sampling distribution initialized?Quality:\nThe paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network. \nHowever, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.\n\nClarity:\nThe paper needs major improvements in terms of clarity. The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper. For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology. \nThe related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons. In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11.\n\nOriginality & Significance:\nThe authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)\n\nI think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning. But the paper needs to be improved in terms of presentation.\n\n", "title": "A strong assumption?", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}