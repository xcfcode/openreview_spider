{"paper": {"title": "Dimensionality Reduction for Representing the Knowledge of Probabilistic Models", "authors": ["Marc T Law", "Jake Snell", "Amir-massoud Farahmand", "Raquel Urtasun", "Richard S Zemel"], "authorids": ["law@cs.toronto.edu", "jsnell@cs.toronto.edu", "farahmand@vectorinstitute.ai", "urtasun@cs.toronto.edu", "zemel@cs.toronto.edu"], "summary": "dimensionality reduction for cases where examples can be represented as soft probability distributions", "abstract": "Most deep learning models rely on expressive high-dimensional representations to achieve good performance on tasks such as classification. However, the high dimensionality of these representations makes them difficult to interpret and prone to over-fitting. We propose a simple, intuitive and scalable dimension reduction framework that takes into account the soft probabilistic interpretation of standard deep models for classification. When applying our framework to visualization, our representations more accurately reflect inter-class distances than standard visualization techniques such as t-SNE. We show experimentally that our framework improves generalization performance to unseen categories in zero-shot learning. We also provide a finite sample error upper bound guarantee for the method.", "keywords": ["metric learning", "distance learning", "dimensionality reduction", "bound guarantees"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero-shot learning and distillation. The authors provide an extensive theoretical and empirical analysis, showing performance improvements in zero shot learning and finite sample error upper bounds. The reviewers generally agree this is a good paper that should be published."}, "review": {"HylnV5jchQ": {"type": "review", "replyto": "SygD-hCcF7", "review": "Authors propose a method of embedding training data examples into low-dimensional spaces such that mixture probabilities from a mixture model on these points are close to probability predictions from the original model in terms of KL divergence. Authors suggest two use-cases of such an approach: 1) data visualization, and 2) zero-shot learning. For the visualization use-case, authors compare against other dimensionality reduction methods with qualitative analysis on a synthetic problem, as well as evaluation metrics such as Neighborhood-Preservation Ratio and Clustering Distance Preservation Ratio. For zero-shot use-case, they take pre-trained models on two zero-shot tasks, and improve the accuracy by using probability outputs from pre-trained models as target.\n\nRegarding the benefit of using the proposed method for visualization, the DRPR is making a strong assumption that representations of data points that belong to the same class form a uni-modal, Gaussian distribution (since authors don't experiment with distance functions other than L2). This inductive bias comes with a strong benefit when the assumption is true - as demonstrated in the toy dataset experiment - but when it is not true, the visualization would strongly distort the underlying structure of the model. And I don't believe this is a realistic assumption, because there has to be a reason that most deep-learning based classification models in the literature don't always use a model like (3) or Prototypical Networks instead of typical fully-connected + softmax layer, unless the data size is small and we need stronger inductive bias to improve the performance of the model.  That is, we usually don't think unimodality is the right assumption, even with learned representations. I suspect that the while DRPR might be good at visualizing relationships between class labels - especially which class can be easily confused with another - but would be worse at faithfully representing each data point, especially the ambiguity of class labels on individual ones. I would argue, however, that faithful representation of each data point is more important for scatter plots than relationship between classes, because the latter can be more effectively analyzed with other methods such as confusion matrices. As it is typical in most dimensionality reduction papers, I would encourage authors to consider more types of synthetic datasets which nonlinearity and multimodality are critical to be learned. I don't believe quantitative evaluation in Table 1 and 2 are very meaningful, because DRPR's objective function is much better aligned with these metrics than others. \n\nZero-shot experiments show a promising lift over the baseline pre-trained models. The kind of bias we should be careful about, however, is that when we distillate one model into another, the performance generally improves even when the same exact model is both the teacher and the student: (Furlanello et al, ICML 2018 https://arxiv.org/abs/1805.04770 ). Therefore, it would be interesting to compare against distillation with baseline models themselves.\n\nPros:\n* Extensive theoretical and empirical analysis\n* Simple idea that generalizes to multiple use-cases, which implies robustness of the approach as a methodology\n\nCons:\n* Unimodal assumption is likely not realistic, which would result in misleading visualization of data\n* Visualization analysis focuses on how class-relationships are preserved rather than faithful representation of each data point, which is a wrong target\n* Synthetic experiment is conducted on a single, too simplistic one; more examples are needed to understand the capabilities of the model in more detail\n* The bias of knowledge distillation is not controlled", "title": "strong inductive bias of the model may not be appropriate for visualization", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkgXAnlcRQ": {"type": "rebuttal", "replyto": "H1gTSCXcnX", "comment": "We thank you for your positive review and helpful suggestions. We address the comments in the following and have updated the paper accordingly:\n\n- \u201cComments: - When trying to understand the proposed method, I found it useful to expand out the full objective function and derive the gradients w.r.t. to f_i. If my maths were correct, the gradient of the objective w.r.t. f_i can be written as the difference between the expected gradient of the divergence w.r.t Y and the expected gradient of the divergence w.r.t. the posterior cluster assignment probabilities. Though not surprising in and of itself, the authors might consider including this equation as it really helped me understand what the learning algorithm was doing.\u201d\n\nWe thank the reviewer for this idea. As suggested, we have added the gradient of our optimization problem wrt the example f_i in the new Equation (5). To simplify its formulation, we consider that the matrix of centroids M does not depend on F (which is the case in the zero-shot learning task) and that priors are all equal. The gradient does depend on both Y and the posterior cluster assignment probabilities.\nMore exactly, the magnitude of the gradient depends on both of theses scores (which means that a cluster with high score Y_ic will be given more importance). \nThe gradient tries to make f_i closer to each centroid while separating it from all the centroids depending on their predicted scores as well.\n\nWe have added a new paragraph called \u201cGradient interpretation\u201d which discusses the gradient.\n\n- \u201cThe authors might consider adding a more complete description of the zero-shot learning task. My understanding of the task was that there are text descriptions of each category and at test time new text descriptions are added that were not in the training set. The goal is to map an unseen image to a class based on the text descriptions of the classes. A couple of sentences explaining this in the first paragraph of section 4.2 would help those who are not familiar with this zero-shot learning setup.\u201d\n\nThank you for this clear and succinct description of our zero-shot learning scenario. We have added this to the first paragraph of Section 4.2 as suggested.", "title": "Response to AnonReviewer2"}, "Syxw33eqRQ": {"type": "rebuttal", "replyto": "HylnV5jchQ", "comment": "We thank you for sharing your concerns about the paper. We will clarify your concerns regarding the assumptions made by DRPR and classification and distillation problems for zero-shot learning.\n\n- \u201c DRPR is making a strong assumption that representations of data points that belong to the same class form a unimodal distribution. I don't believe this is a realistic assumption.\u201d \n\nWhile DRPR makes some assumptions on the distribution in the learned low-dimensional space, we would like to emphasize that DRPR makes no assumption on the input probability distribution in general.\nIt is true that the toy dataset illustrates a special case where both the original and low-dimensional representations follow a unimodal Gaussian distribution (for each cluster). \nThe goal of the toy experiment was to illustrate some weaknesses of t-SNE on a problem that is easy to visualize. We chose this toy dataset because it was easy to visualize the original 3D points themselves (input of t-SNE in Fig. 2 (b)), and also the soft assignment scores of the different points wrt the different clusters in the original space (input of t-SNE in Fig. 2 (c)). These soft probability scores are the target of our model.\n\nOur algorithm is similar to t-SNE in the sense that it assumes some distribution of the data in the low-dimensional space: t-SNE considers a Student-t distribution to compute similarity between pairs of points. Any kind of probability distribution can be used for the input space: t-SNE considers by default a conditional probability based on a Gaussian similarity between pairs of points, but any other kind of distribution can be given as input. \nDRPR is given target probability scores that can be computed from any distribution. In the zero-shot learning task, those scores indeed come from Gaussian mixtures. However, the targets in the visualization experiment come from neural networks trained with cross entropy and do not follow a Gaussian distribution\n\n- \u201cMost deep-learning based classification models in the literature use softmax, we need stronger inductive bias to improve the performance of the model.\u201d \n\nWe agree that, in the usual classification task where the training and test categories are the same, learning a fully connected layer + softmax regression leads to state-of-the-art performance. However, the output dimensionality of the learned model is high and it is then difficult to interpret what has been learned by the model. Applying visualization techniques such as t-SNE has been proposed to interpret such complex models. Nonetheless, to the best of our knowledge, no visualization techniques exploit the fact that softmax classifiers have soft probabilistic interpretations.\n\n- \u201cWhen we distillate one model into another, the performance generally improves even when the same exact model is both the teacher and the student (Furlanello et al., ICML 2018). Therefore, it would be interesting to compare against distillation with baseline models themselves.\u201d\n\nWe thank the reviewer for this suggestion. We have compared our method to two distillation strategies proposed in (Furlanello et al., ICML 2018) in the zero-shot learning task. We used it with the Prototypical Networks as the teacher since it obtains the best performance.\nIn the first strategy, for each example, we preserve only the predicted category of the teacher model and convert the target as a one-hot vector. This actually corresponds to applying a second \u201clayer\u201d of prototypical network which is a special case of our method when the targets are hard assignments.\nIn the second strategy, we preserve the predicted category and permute the scores of the category with lower scores.\nIn both cases, the accuracy scores decreased relative to using only the Prototypical Network (without our method): about 55% on Birds and 60% on Flowers.\n\nThis difference may be explained by the fact that (Furlanello et al., ICML 2018) consider a task where categories are the same during training and test. Applying wrong predictions based on a pre-trained teacher does not seem to affect accuracy.\nIn our case, the set of training, validation and test categories are all different. Preserving relevant relative scores seems more crucial.\n\n- \u201cVisualization analysis focuses on how class-relationships are preserved rather than faithful representation of each data point, which is a wrong target \u201d\n\nThe \u201cwrong\u201d target depends on the task.\nEvery dimensionality reduction framework has some criterion that is seeked to be optimized. PCA finds a linear transformation that maximizes the variance. t-SNE tries to preserve local neighborhood in both the high and low-dimensional spaces, which results in a poor preservation of inter-cluster distances. There is not a clear definition of \u201cfaithful representation\u201d since it always depends on what is meant to be represented. \nThe goal of DRPR is to find a low-dimensional space that best preserves the soft predicted scores of a classifier.", "title": "Response to AnonReviewer1"}, "B1gOV3x90Q": {"type": "rebuttal", "replyto": "ryxpkAks3m", "comment": "We would like to thank you for the feedback on the paper. We try to clarify your concerns regarding the convergence guarantee and why we need to compute prior at each iteration.\n\n- \u201cThe algorithm that is presented is quite natural, though no guarantees that it will converge to something relevant were given.\u201d\n\u201cDoes this algorithm really minimize the discrepancy?\u201d\n\nIt is not clear what \u201csomething relevant\u201d refers to, so we try to explain the result of Theorem 1 in the appendix. The theorem provides a finite-sample upper bound on the quality of the minimizer of the empirical loss, as defined in Eq. (6). \nThe quality of the solution is measured according to the discrepancy (Delta_n), which is the expected KL-divergence between the teacher and the student. The teacher is defined by  \\phi, and provides a distribution over k clusters.\nBut since we only have access to n data points, the student minimizes the empirical discrepancy \\Delta_n. The minimizer is \\hat{g}, and it induces a distribution psi_\\hat{g}.\nTheorem 1 shows that the expected KL-divergence between teacher \\phi and student psi_\\hat{g} is upper bounded by two terms:\nThe best possible student within the function class G, from which \\hat{g} is selected.\nSome estimation error terms that depend on the number of samples n and some properties of the function space G.\n\nWe emphasize that our theoretical result does not concern the convergence of the optimization procedure, as noted in Footnote 5.\n\nA related question of the reviewer is that whether the algorithm really minimizes the discrepancy: The answer is that the algorithm minimizes the empirical version of the discrepancy. But the theory shows that doing so leads to a guarantee on the quality of resulting estimator, according to the true discrepancy (which has expectation, instead of a finite number of data points).\n\n- \u201cEspecially, easiness of substitution of \\bar{Y}_{ic} with Y_{ic} in the algorithm is unclear (roughly speaking, the latter means that E-step is omitted in EM).\u201d\n\nWe thank the reviewer for pointing out this lack of clarity in our first submission. We assume that the reviewer refers to the statement \u201cIt is worth noting that we never apply the EM algorithm during training\u201d. We do apply the E-step of the EM algorithm at each gradient descent iteration, but only once unlike the standard EM algorithm.\n\nMore precisely, at each iteration, we know the optimal desired values of the M-step variables (i.e. the centroid matrix M and the prior vector \\pi) as a function of the ground truth assignment matrix Y and the current representations of the mini-batch F. We then use these optimal values of M and \\pi (formulated in step 4 of Algorithm 1) to compute the E-step, which corresponds to the predicted assignment matrix \\Psi formulated in Eq. (3) (which you are denoting as \\bar{Y}_{ic}). \nBy definition of our optimization problem in Eq. (4), we minimize the average of KL divergence between the rows of Y and the rows of \\Psi.\n\nWe have updated the sentence accordingly.\n\nWe would also like to emphasize that exploiting the target assignment matrix Y to compute the optimal M-step variables (i.e. centroid matrix) is commonly done in the supervised (hard) clustering literature [A,B].\n\n- \u201cIf matrix Y in algorithm is fixed, why we need to compute \\pi in the loop? Isn't it going to be the same?\u201d\n\nWe think that you mean that if Y stays the same at every iteration, then \\pi which depends only on Y should also stay the same at each iteration. This statement is correct.\nHowever, in the case where we train a neural network via mini-batches (e.g. in the zero-shot learning task or the hard clustering task in Section C.3), Y corresponds to the target assignment matrix of the mini-batch.\nSince each iteration then considers a different mini-batch F, the matrices Y and \\pi then also change.\n \nNonetheless, as mentioned in our paper, the priors can also be assumed to be all equal and then ignored.\nThe matrix \\pi can also be calculated according to the target assignments of the whole training dataset and then fixed.\n\n\n[A] Lajugie et al., Large-Margin Metric Learning for Constrained Partitioning Problems, ICML 2014\n[B] Law et al., Deep Spectral Clustering Learning, ICML 2017\n", "title": "Response to AnonReviewer3"}, "ryxpkAks3m": {"type": "review", "replyto": "SygD-hCcF7", "review": "The paper deals with a problem formulation adjacent to that of the sufficient dimension reduction: given training set of pairs (x_i,y_i), how to reduce the dimension of the first element, i.e. map x_i --> f(x_i), so that f(x_i)'s still have all the information to recover y_i's.\n\nIn the paper, the output y_i is a probability distribution over k labels that softly describes inclusion of example i into k classes.\n\nThey consider a nonlinear case, i.e. the mapping f is taken from a prespecified set of mappings, parameterized by Theta (e.g. neural network). Then by \"recovering y_i\" they mean that EM algorithm for {f(x_i)} will result in a clustering of the data into k soft clusters similar to given {y_i}.\n\nThe algorithm that is presented is quite natural, though no guarantees that it will converge to something relevant were given. Theoretical analysis deals with a question --- how far the empirical discrepancy could be from the true expected one. Especially, easiness of substitution of \\bar{Y}_{ic} with Y_{ic} in the algorithm is unclear (roughly speaking, the latter means that E-step is omitted in EM). If matrix Y in algorithm is fixed, why we need to compute \\pi in the loop? Isn't it going to be the same? Does this algorithm really minimizes the discrepancy?", "title": "What do we really minimize?", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "H1gTSCXcnX": {"type": "review", "replyto": "SygD-hCcF7", "review": "Summary:\n\nThis paper introduces a new supervised dimensionality reduction model. Supervision is provided in the form of class probabilities and the learning algorithm learns low-dimensional representations such that posterior cluster assignment probabilities given the representations match the observed class probabilities. The representations can be learned directly or the parameters of a neural network can be learned which maps inputs to the lower-dimensional space. The authors provide an extensive theoretical analysis of the proposed method and evaluate it on dimensionality reduction, visualization, and zero-shot learning tasks.\n\nReview:\n\nOverall, I thought this was an excellent paper. The idea is well-motivated, the presentation is clear, and the evaluations are both comprehensive and provide insight into the behavior of the proposed methods (I will not comment on the theoretical analysis, as it is entirely contained in the supplemental materials). I was honestly impressed by the shear volume of content in this paper, particularly since I found none of it to be superfluous. Frankly, this paper might be better served as two papers or a longer journal paper, but that is hardly a reason not to accept it. I strongly recommend acceptance and have only a couple of comments on presentation.\n\nComments:\n\n- When trying to understand the proposed method, I found it useful to expand out the full objective function and derive the gradients w.r.t. to f_i. If my maths were correct, the gradient of the objective w.r.t. f_i can be written as the difference between the expected gradient of the divergence w.r.t Y and the expected gradient of the divergence w.r.t. the posterior cluster assignment probabilities. Though not surprising in and of itself, the authors might consider including this equation as it really helped me understand what the learning algorithm was doing. \n\n- The authors might consider adding a more complete description of the zero-shot learning task. My understanding of the task was that there are text descriptions of each category and at test time new text descriptions are added that were not in the training set. The goal is to map an unseen image to a class based on the text descriptions of the classes. A couple of sentences explaining this in the first paragraph of section 4.2 would help those who are not familiar with this zero-shot learning setup.", "title": "Excellent paper with strong motivation, interesting proposed method, and comprehensive empirical results", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}