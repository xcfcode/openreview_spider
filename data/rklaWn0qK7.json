{"paper": {"title": "Learning Neural PDE Solvers with Convergence Guarantees", "authors": ["Jun-Ting Hsieh", "Shengjia Zhao", "Stephan Eismann", "Lucia Mirabella", "Stefano Ermon"], "authorids": ["junting@stanford.edu", "sjzhao@stanford.edu", "seismann@stanford.edu", "lucia.mirabella@siemens.com", "ermon@cs.stanford.edu"], "summary": "We learn a fast neural solver for PDEs that has convergence guarantees.", "abstract": "Partial differential equations (PDEs) are widely used across the physical and computational sciences. Decades of research and engineering went into designing fast iterative solution methods. Existing solvers are general purpose, but may be sub-optimal for specific classes of problems. In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain. We achieve this goal by learning to modify the updates of an existing solver using a deep neural network. Crucially, our approach is proven to preserve strong correctness and convergence guarantees. After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.", "keywords": ["Partial differential equation", "deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "Quality: The overall quality of the work is high.  The main idea and technical choices are well-motivated, and the method is about as simple as it could be while achieving its stated objectives.\n\nClarity:  The writing is clear, with the exception of using alternative scripts for some letters in definitions.\n\nOriginality:  The biggest weakness of this work is originality, in that there is a lot of closely related work, and similar ideas without convergence guarantees have begun to be explored.  For example, the (very natural) U-net architecture was explored in previous work.\n\nSignificance:  This seems like an example of work that will be of interest both to the machine learning community, and also the numerics community, because it also achieves the properties that the numerics community has historically cared about.  It is significant on its own as an improved method, but also as a demonstration that using deep learning doesn't require scrapping existing frameworks but can instead augment them."}, "review": {"HkgU6zj4g4": {"type": "rebuttal", "replyto": "HJlqu0rTJE", "comment": "Hi Redouane,\n\nThank you for your question! We are also happy to hear from other people trying to solve Poisson Equation.\n\n1) how many filters do you consider in your Convk and U-Net, to make it quicker than your baseline\n\nWe only use 1 filter at each layer for both Convk and U-Net models. We haven\u2019t tried adding more filters, but we expect it to give similar or better results.\n\n2) how do you deal with image boundaries? I mean how do you deal with padding (zero? no padding?) by keeping the boundary conditions of the problem.\n\nOnce we get the error terms, we pad it with zeros. We do this so that the conv layers don\u2019t reduce the input size, and it makes sense since the error of the boundary values should be zero.\n\n3) do you use TF or PyTorch, and do you think that has big impact on speed?\n\nWe use PyTorch. I am not entirely sure but I don\u2019t think it has a big impact on speed.", "title": "Reply to technical questions"}, "BJxZsitoa7": {"type": "rebuttal", "replyto": "BJg5yrONnm", "comment": "Thank you for your helpful reviews and suggestions.\n\n1) \u201cThe method seems to rely strongly on the linearity of the solver and its deformation (to guarantee the correctness of the solution). The operator H is a matrix of finite dimensions and it is not completely clear to me what is the role of the multi-layer parameterization. \n\u201cwhy is the deep network parameterization needed? Since no nonlinearities are present, isn t this equivalent to fix the rank of H?\u201d\n\nEven though composition of linear functions is still linear, using d linear layers is better than one. On a grid with n^2 vertices: one convolution layer requires O(n^2) computations and have local receptive field; one fully-connected layer requires O(n^4) computations and have global receptive field; our deep U-Net architecture has O(n^2) computations but global receptive field. Our hope is that the deep U-Net architecture learns a linear function with both good computation properties (O(n^2)) and convergence properties, which is impossible for one layer models.\n\nOur learned network H is a convolutional operator, which does not have low rank. A low rank H is unlikely to perform well because many different errors may be mapped to the same correction term, while a high rank H can correct different errors differently. Our parameterization learns a high rank H with O(n^2) computation.\n\n\n2) \u201cBased on a grid approach, the idea applies only to one- or two-dimensional problems.\u201d\n\nOur method generalizes without modification to any dimensional problems: simply replace 2-D convolution to k-D convolution.\n\n\n3) \u201cin the introduction, what does it mean that generic solvers are effective 'but could be far from optimal'?  Does this refer to the convergence speed or to the correctness of the solution?\u201d\n\nWe meant that generic solvers like Jacobi are hand-designed and theoretically correct, but may not be optimal in terms of convergence speed. Designing a solver is a trade-off between computation-per-iteration and spectral radius. We would like to have the smallest spectral radius given computation budget. We verify in our experiments: human designed solvers (e.g. Jacobi) are not Pareto optimal, and are outperformed by our learned solvers. Similar observations have also been made in other fields: learned models outperform hand designed ones, e.g. Andrychowicz et al., 2016, Song et al, 2017.\n\n\n4) \u201cother deep learning approaches to PDE solving are mentioned in the introduction. Is the proposed method compared to them somewhere in the experiments?\u201d\n\nTo the best of our knowledge, related works applying ML to PDEs directly fit the solution with deep networks, which have no correctness or generalization guarantees and are restricted to specific dimensions and geometries. Our algorithm is the first deep learning based method with provable correctness and generalization guarantees.\n\n\n5) \u201cgiven a PDE and some boundary conditions, is there any known method to choose the liner iterator T optimally? For example, since u* is the solution of a linear system, could one choose the updates to be the gradient descent updates of a least-squares objective such as || A u - f||^2?\u201d\n\nActually, this is exactly the update rule for most existing methods (conjugate gradient, Jacobi, etc). We compared with these methods (conjugate gradient, Jacobi) in experiments and outperform them. For example, if we minimize the objective 1/2 u^T A u - u^T f, given that A is symmetric, positive-definite, this objective has a unique minimizer, and the derivative is exactly Au - f. If we perform gradient descent on this objective with learning rate 1, we get exactly the Jacobi update.\n\nGradient descent may not be optimal; improving it is undergoing active research (e.g. ADAM, Adagrad, \u201cLearning to learn\u201d [1]). We tackle a special class of optimization problems and design methods with both correctness guarantees and better performance.\n\n[1] Andrychowicz, Marcin, et al. \"Learning to learn by gradient descent by gradient descent. NIPS, 2016.\n\n\n6) \u201cgiven the `interpretation of H' sketched in Section 3.3, is there any relationship between the proposed accelerated update and the update of second-order coordinated descent methods (like Newton or quasi-Newton)?\u201d\n\nOn a grid with k (= n^2) vertices, second order methods (e.g. Newton) have optimal convergence speed (solve linear equations with a single update), but poor computation complexity (O(k^3) to solve A inverse). Our method only require O(k) computation per-iteration, and we hope to achieve a good trade-off between convergence speed and computation budget by optimization.", "title": "Reply to AnonReviewer3"}, "H1eZ3PYjaX": {"type": "rebuttal", "replyto": "rklkZrg8nm", "comment": "Thank you for your helpful reviews and suggestions.\n\n1) \u201cYou need to spend considerably more space discussing the related work on using ML to improve PDE solvers. Most readers will be unfamiliar with this. You should explain what they do and how they are qualitatively different than your approach.\u201d\n\nWe will add more discussions in our updated paper. To the best of our knowledge, related works applying ML to PDEs directly fit the solution with deep networks, which have no correctness or generalization guarantees and are restricted to specific dimensions and geometries.\n\n\n2) \u201cYou do a good job 3.3 of motivating for what H is doing. However, you could do a better job of motivating the overall setup of (6). Is this a common formulation? If so, where else is it used?\u201d\n\nThis formulation is a novel idea that provides correctness guarantees by leveraging a hand designed solver: we modify the residual of a hand designed solver. Another idea that also modify the residual (but not of a hand designed solver) is conjugate gradient.\n\n\n3) \u201cI\u2019m surprised that you didn\u2019t impose some sort of symmetry conditions on the convolutions in H, such as that they are invariant to flips of the kernel. This is true, for example, for the linearized Poisson operator.\u201d\n\nGeneralization, for our model, is almost for free because of our linear ConvNet setup. Therefore, we didn\u2019t find strong reasons to restrict the network parameters and reduce dimensionality. Enforcing symmetry introduces unnecessary overhead.\n\n\n4) \u201cValid iterators converge to a valid solution. However, can\u2019t there be multiple candidate solutions? How would you construct a method that would be able to find all possible solutions?\u201d\n\nFor most PDEs with Dirichlet boundary conditions (e.g. Possion, Helmholtz), the solution is always unique. Thus, a valid iterator should converge to the unique solution. We currently consider PDEs that have unique solutions.\n\n\n==Minor comments==\n\n5) \u201cIn (9), why do you randomize the value of k? Wouldn\u2019t you want to learn a different H depending on what computation budget you knew you were going to use downstream when you deploy the solver?\u201d\n\nOur hope is to learn a generic solver for a type of PDE that can be applied to a variety of applications. Therefore, we train the model agnostic to downstream applications. Nonetheless practitioners who know their computation budget can certainly fine tune our iterator with a fixed k.\n\n6) \u201cIn future work it may make sense to learn a different H_i for each step i of the iterative solver.\u201d\n\nThank you for the suggestion. We can try in the future, e.g. there are some methods that take the history of the iteration into account, which means it has a different H for each step.\n\n\n7) \u201cWhen introducing iterative solvers, you leave it as an afterthought that b will be enforced by clamping values at the end of each iteration. This seems like a pretty important design decision. Are there alternatives that guarantee that u satisfies b always, rather than updating u in such a way that it violates G and then clamping it back? Along these lines, it might be useful to pose (2) with additional terms in the linear system to reflect G.\u201d\n\nThis is the most straightforward way to satisfy the boundary condition, and most existing iterative solvers enforce boundary conditions with this reset operation. We will also explicitly add G into our update rule in our updated paper.", "title": "Reply to AnonReviewer1"}, "BygG_etiTQ": {"type": "rebuttal", "replyto": "Bkgt-vXc3X", "comment": "Thank you for your helpful reviews and suggestions.\n\n1) \"Why didn\u2019t you try the nonlinear deep network? Is it merely for computational efficiency? I expect that nonlinear networks might result in even better estimates of H and further reduce the number of fixed-point iterations, despite each operation of H will be more expensive. There might be some trade-off here. But I would like to see some empirical results and discussions.\"\n\nThe reason we did not use nonlinear deep networks is that it\u2019s hard to prove correctness guarantees. Our linear iterator has provably correct fixed point while nonlinear iterators may have non-unique or incorrect fixed points. In addition, it is easy to prove convergence by spectral theory, while this is not the case for nonlinear operators.\n\n\n2). \"The evaluation is only on Poisson equations, which are known to be easy. Have you tried other PDEs, such as Burger\u2019s equations? I think your method will be more meaningful for those challenging PDEs, because they will require much more fine-grained grids to achieve a satisfactory accuracy and hence much more expensive. It will be great if your method can dramatically improve the efficiency for solving these equations.\"\n\nWe did additional experiments on Helmholtz equations, \\nabla^2 u + k^2 u = 0, which is known to be very challenging [1]. So far we have some preliminary results of Conv1 model in a square domain: we outperform traditional methods by a similar margin. The following show for different values of k, the ratio of computation cost compared to Jacobi in terms of layers / flops (same as Table 1). \nk = 1: 0.422 / 0.685\nk = 2: 0.396 / 0.643\nk = 3: 0.383 / 0.622\n\nWe leave more thorough analysis of the Helmholtz equation for future work.\n\n[1] Oliver G. Ernst and Martin J. Gander. Why it is Difficult to Solve Helmholtz Problems with Classical Iterative Methods. Numerical analysis of multiscale problems, 2012.\n\n\n3). \"I am a bit confused about the statement of Th 3 --- the last sentence H is valid for all parameters f and b if the iterator \\psi converges \u2026 I think it should be 'for one parameter'. \"\n\nIn Theorem 1 and Lemma 1, we showed that if our iterator is convergent, it converges to the correct solution, hence it is valid. In Theorem 3, we showed that if the iterator is valid for some f and b, then the iterator is valid for every f and b. These combined implies that the iterator is valid for every f and b if it is convergent for one f and b. We will rephrase Theorem 3 to remove the confusion.", "title": "Reply to AnonReviewer2"}, "Bkgt-vXc3X": {"type": "review", "replyto": "rklaWn0qK7", "review": "This paper develops a method to accelerate the finite difference method in solving PDEs. Basically, the paper proposes a revised framework for fixed point iteration after discretization. The framework introduces a free linear operator --- the choice of the linear operator will influence the convergence rate. The paper uses a deep linear neural network to learn a good operator. Experimental results on Poisson equations show that the learned operator achieves significant speed-ups. The paper also gives theoretical analysis about the range of the valid linear operator (convex open set) and guarantees of the generalization for the learned operator. \n\nThis is, in general, a good paper. The work is solid and results promising.  Solving PDEs is no doubt an important problem, having broad applications. It will be very meaningful if we can achieve the same accuracy using much less computational power.  Here, I have a few questions. \n\n1). Why didn\u2019t you try the nonlinear deep network? Is it merely for computational efficiency? I expect that nonlinear networks might result in even better estimates of H and further reduce the number of fixed-point iterations, despite each operation of H will be more expensive. There might be some trade-off here. But I would like to see some empirical results and discussions.\n\n2). The evaluation is only on Poisson equations, which are known to be easy. Have you tried other PDEs, such as Burger\u2019s equations? I think your method will be more meaningful for those challenging PDEs, because they will require much more fine-grained grids to achieve a satisfactory accuracy and hence much more expensive. It will be great if your method can dramatically improve the efficiency for solving these equations. \n\n3). I am a bit confused about the statement of Th 3 --- the last sentence \u201cH is valid for all parameters f and b if the iterator \\psi converges \u2026\u201d I think it should be \u201cfor one parameter\u201d. \n\nMiscellaneous:\n1)\tTypo. In eq. (7) \n2)\tSection 3.3, H(w) should be Hw (for consistency)\n", "title": "A Good and Solid Work", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rklkZrg8nm": {"type": "review", "replyto": "rklaWn0qK7", "review": "==Summary==\nThis paper is well-executed and interesting. It does a good job of bridging the gap between distinct bodies of literature, and is very in touch with modern ML ideas. \n\nI like this paper and advocate that it is accepted. However, I expect that it would have higher impact if it appeared in the numerical PDE community. I encourage you to consider this conference paper to be an early version of a more comprehensive piece of work to be released to that community.\n\nMy main critique is that the paper needs to do a better job of discussing prior work on data-driven methods for improving PDE solvers.\n==Major comments==\n* You need to spend considerably more space discussing the related work on using ML to improve PDE solvers. Most readers will be unfamiliar with this. You should explain what they do and how they are qualitatively different than your approach. \n\n* You do a good job 3.3 of motivating for what H is doing. However, you could do a better job of motivating the overall setup of (6). Is this a common formulation? If so, where else is it used?\n* I\u2019m surprised that you didn\u2019t impose some sort of symmetry conditions on the convolutions in H, such as that they are invariant to flips of the kernel. This is true, for example, for the linearized Poisson operator. \n\n==Minor comments==\n\n* Valid iterators converge to a valid solution. However, can\u2019t there be multiple candidate solutions? How would you construct a method that would be able to find all possible solutions?\n\n* In (9), why do you randomize the value of k? Wouldn\u2019t you want to learn a different H depending on what computation budget you knew you were going to use downstream when you deploy the solver? \n\n* In future work it may make sense to learn a different H_i for each step i of the iterative solver. \n\n* When introducing iterative solvers, you leave it as an afterthought that b will be enforced by clamping values at the end of each iteration. This seems like a pretty important design decision. Are there alternatives that guarantee that u satisfies b always, rather than updating u in such a way that it violates G and then clamping it back? Along these lines, it might be useful to pose (2) with additional terms in the linear system to reflect G. \n", "title": "Interesting, well-written paper", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJg5yrONnm": {"type": "review", "replyto": "rklaWn0qK7", "review": "Summary:\nThe authors propose a method to learn and improve problem-tailored PDE solvers from existing ones. The linear updates of the target solver, specified by the problem's geometry and boundary conditions, are computed from the updates of a well-known solver through an optimized linear map.  The obtained solver is guaranteed to converge to the correct solution and\nachieves a considerable speed-up compared to solvers obtained from alternative state-of-the-art methods.   \n\nStrengths:\nSolving PDEs is an important and hard problem and the proposed method seems to consistently outperform the state of the art. I ve liked the idea of learning a speed-up operator to improve the performance of a standard solver and adapt it to new boundary conditions or problem geometries. The approach is simple enough to allow a straightforward proof of correctness. \n\nWeaknesses:\nThe method seems to rely strongly on the linearity of the solver and its deformation (to guarantee the correctness of the solution). The operator H is a matrix of finite dimensions and it is not completely clear to me what is the role of the multi-layer parameterization. Based on a grid approach, the idea applies only to one- or two-dimensional problems. \n\nQuestions:\n- in the introduction, what does it mean that generic solvers are effective 'but could be far from optimal'?  Does this refer to the convergence speed or to the correctness of the solution? \n- other deep learning approaches to PDE solving are mentioned in the introduction. Is the proposed method compared to them somewhere in the experiments? \n- given a PDE and some boundary conditions, is there any known method to choose the liner iterator T optimally? For example, since u* is the solution of a linear system, could one choose the updates to be the gradient descent updates of a least-squares objective such as || A u - f||^2?\n- why is the deep network parameterization needed? Since no nonlinearities are present, isn t this equivalent to fix the rank of H?\n- given the `  interpretation of H' sketched in Section 3.3, is there any relationship between the proposed accelerated update and the update of second-order coordinated descent methods (like Newton or quasi-Newton)?", "title": "A linear method for speeding up PDE solvers with good empirical performances", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1g2mv84om": {"type": "rebuttal", "replyto": "B1e_-B_MoX", "comment": "Hello!\n\nThank you for pointing out this unpublished but relevant work! We were not aware of it and we will certainly add a reference. Deep Multigrid has some surface resemblance to our method, but there are major differences:\n\n(1) Generalization: Deep Multigrid does not generalize to different grid size or different geometries. The learned prolongation and restriction operators are fully-connected layers, which need retraining for each grid size and geometry. Our model generalizes (both by design and in experiments) to very different grid sizes and geometries after training on a single example (Figure 1).\n\n(2) Usability: Deep Multigrid only experimented on 1D grids, with no proposed generalization to 2D or 3D geometries. \nOn 1D grid, the matrix A is tridiagonal, and Au = f can be solved exactly by Gaussian elimination in O(n) time [1]. Contrastly, our method applies without modification to any dimension (by using d-dimensional convolution).\n\n(3) Flexibility: Deep Multigrid only learns prolongation and restriction operators. Our U-Net model is end-to-end: it implicitly includes smoothing, prolongation, and restriction. Our approach is simpler yet more general.\n\n(4) Experiments: Deep multigrid does not compare runtime with state-of-the-art solvers. Our method is faster (wall-clock time and number of operations) than both Jacobi Multigrid and FEniCS.\n\n\n[1] Randall J LeVeque. Finite difference methods for ordinary and partial differential equations: steady-state and time-dependent problems, volume 98. Siam, 2007.", "title": "Differences and problems with Deep Multigrid"}}}