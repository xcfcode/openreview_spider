{"paper": {"title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"], "summary": "", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "keywords": ["Natural language processing", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "Though the have been attempts to incorporate both \"topic-like\" and \"sequence-like\" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written.\n \n Pros:\n -- clean and simple model\n -- sufficiently convincing experimentation\n \n Cons:\n -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)"}, "review": {"B18BcXXO-": {"type": "rebuttal", "replyto": "ryxafJWO-", "comment": "In most of the paper the topic vector (whether the prior or the posterior) is modeled as a Gaussian, and as such its components can be negative. This seems consistent with NOT interpreting the topic vector as a mixture (or proportions) of topics.  However in Figure 2 the topic vector IS being interpreted as topic proportions, and all components are non-negative. I'm just bothered by this inconsistency. ", "title": "Is the length-K topic vector meant to represent topic proportions among K topics?"}, "ryxafJWO-": {"type": "rebuttal", "replyto": "rJbbOLcex", "comment": "In the generative model in Sec 3 you're drawing a K-dimensional \"topic vector\" theta from a normal distribution N(0,I). If these are intended to be topic proportions, how is a normal distribution suited to this, since the components of theta can be negative? I would have thought maybe a logistic-normal distribution would be more suitable for this?", "title": "Distribution of the topic vector"}, "HkadLMxvg": {"type": "rebuttal", "replyto": "SyoGQzeDx", "comment": "Thank you AnonReviewer1!", "title": "Re: Thanks for the bug correction"}, "BkQQK0KIe": {"type": "rebuttal", "replyto": "rJbbOLcex", "comment": "We thank the reviewers and the anonymous commenters for the helpful feedback and questions!\n\u00a0\nWe first summarize the main idea of this paper below:\n\u00a0\nNeural network-based language models have achieved state of the art results on many NLP tasks. One difficult problem is to capture long-range dependencies as motivated in the introduction of this paper. We propose to solve this by integrating latent topics as context and jointly training these contextual features with the parameters of an RNN language model. We provide a natural way of doing this integration by modeling stop words that are excluded by topic models but needed for sequential language models. This is done via binary classification where the probability of being a stop word is dictated by the hidden layer of the RNN. This modeling approach is possible when the contextual features as provided by the topics are passed directly to the softmax output layer of the RNN as additional bias. We illustrate the performance of this approach on two tasks and two datasets: word prediction on PTB and sentiment analysis on IMDB.\u00a0 We provide competitive perplexity scores on PTB showing more generalization capabilities (for example we only need a TopicGRU with 100 neurons to achieve a better perplexity than stacking 2 LSTMs with 200 neurons each ---112.4 vs 115.9). \"This method of jointly modeling topics and a language model seems effective and relatively easy to implement.\" quoted from AnonReviewer1.\n\u00a0\nWe have revised the paper and added the following changes:\n1- we added a line on the middle of page 7 to clarify even more how we compute the topic vector \\theta using a sliding window for word prediction.\n2- we added the test perplexity scores for TopicRNN, TopicLSTM, and TopicGRU as required by AnonReviewer3.\n3- we added the inferred distributions from some documents as required by AnonReviewer1.\n4- we added an explanation of why we passed the topics directly to the output layer at the bottom of page 4. \n\nWe answer each reviewer individually. See below.", "title": "General Answer To Reviewers and ACs"}, "H1rG20tUg": {"type": "rebuttal", "replyto": "Hy_fitzEx", "comment": "Thank you for your feedback and questions! \n\n- We added an explanation for our rationale of passing the topic vector directly to the output layer at the bottom of page 4 of the revised paper.\n\n- The RNN is trained using only the training data reviews. The labels were not used for training TopicRNN. The labels are used to train a simple classifier that uses the features extracted fromTopicRNN.\n\n- We believe the results from PTB are very encouraging. For example, with only 100 neurons we are able to achieve lower perplexity score with TopicGRU than a stack of 2 layers of LSTMS with 200 hidden units in each layer: 112.4 vs 115.9. See table 2.\n\n- Thank you for mentioning the Miyato et al. reference. We added it to the paper. We were not aware of it by the time we submitted the paper. However, note their approach is semi-supervised. They use the labels to train their network whereas we use an unsupervised approach. We added their SOTA score to table 4.\n\n- Regarding the stop word modeling, we added a note on the bottom of page 4. \n\n- TopicLSTM and TopicGRU actually perform better than TopicRNN when the number of hidden units is greater than 10. We corrected a bug on the computation of the ELBO and reported the new results on Table 2. Yes, we performed gradient clipping.\n\n-The stop word list is the one we provided in the link. As we mention in the discussion section, we leave as future work the dynamic discovery of the stop words. When the stop words are discovered dynamically a stop words list won\u2019t be needed.\n\n-We added inferred distributions from three different documents on page 6 in figure 2.  Thank you for suggesting this! As can be seen in that figure, some \u201ctopic\u201d components are a lot higher than others for different documents. ", "title": "Re: Nice work on feature extraction"}, "S1haqCt8g": {"type": "rebuttal", "replyto": "SJ1aaWHNx", "comment": "Thanks for your questions and for suggesting we add results for TopicLSTM!\n\n- In Table 2, the first two lines were results reported in Mikolov et al 2012. They run LDA separately and extract features for words using the topic matrix.\n\n- We included results from TopicRNN, TopicLSTM, and TopicGRU. Contrary to what we mentioned earlier, TopicLSTM and TopicGRU actually perform very well. We corrected a bug on the computation of the ELBO. The new results are consistent with the story and are reported in table 2. (We are also running experiments with TopicGRU/TopicLSTM on IMDB data. However, it takes some time to finish. We will add them when they are available.)\n\n- Each text is generated using one example input document. The input for IMDB was a negative review. That sentiment is reflected in the generated text. Note one can sample from the prior for the topic vector \\theta and use that as bias on the trained model.\n\n-TopicRNN is a language model. As reported at the bottom of page 5, the complexity is dominated by the computation of the softmax output layer as is the case for language models. As such, all methods for dealing with the softmax layer are also applicable to TopicRNN. We reported the computation time in the experiments section to give an idea.\n\n- We followed the procedure in Paragraph\u00a0Vector. The main comparison here is against other unsupervised neural network based approaches (ex: Le and Mikolov 2014). Note it is also possible to train the classifier directly with TopicRNN. However, we wanted to highlight TopicRNN as unsupervised feature extractor.\n", "title": "Re: review"}, "ByipKCYUx": {"type": "rebuttal", "replyto": "S1mRre84l", "comment": "Thanks for your questions.\u00a0\n\n1. We believe there was a misunderstanding of our proposed model.\u00a0Unlike LDA, TopicRNN is a sequential model\u00a0(as expressed in the generative process in the middle of page 4) and as such does not make the exchangeability assumption.\u00a0\nThe inference network that produces the topic vector \\theta used as bias takes as input Xc which is a bag of words representation of the document.\u00a0Xc excludes stop words just as done in topic modeling. This is where exchangeability is needed and maybe where the confusion is coming from. But this is the inference network for \\theta, not the actual generative model.\n\u00a0\n2. There are three main reasons behind our choice of using the topic vector as bias instead of passing it into the hidden states of the RNN.\u00a0\n\na) First, this enables us to have a clear separation of the contributions of global semantics and those of local dynamics. The global semantics come from the topics which are meaningful when stop words are excluded.\u00a0However, these stop words are needed for the local dynamics of the language model. We hence achieve this separation of global vs local via a binary decision model for the stop words. It is unclear how to achieve this if we pass the topics to the hidden states of the RNN. This is because the hidden states of the RNN\u00a0will account for all words (including stop words) whereas topics exclude stop words.\u00a0Passing the topics through the hidden states of the RNN violates this.\n\nb) Second, we show empirical evidence that our approach does better than\u00a0previous ways of integrating topic models into RNNs.\u00a0This modeling choice also allows discovering interpretable topics within a single model.\u00a0\n\nc) Finally, this modeling choice allows easier end-to-end training of the model.\u00a0And we argue that although we do not have the topics directly\u00a0going into the hidden states, they will affect the whole trained model, including the hidden states\u00a0due to our end-to-end training approach (unlike the previous work that use pre-trained topic models).\n\nWe added this note on page 4 of the manuscript.", "title": "We believe there was a misunderstanding of our proposed model."}, "rymEZGNHx": {"type": "rebuttal", "replyto": "Sk2ycnmSx", "comment": "Hi Anonymous. Thank you for your question.\n\nYes, it is unfair to use a global topic feature first and then do word prediction. That is why we don't do that for word prediction. We use a sliding window to compute \\theta as we go. The topic vector \\theta that is used from the current batch of words is estimated from the previous batch of words. We explain this in the middle of page 5 (section titled: \"Generating sequential text and computing perplexity\"). Apologies that it did not appear clear. Note this fair sliding window procedure was also used in Mikolov et al. 2012 which allows for fair comparison of test perplexities as reported in table 2.\n\nIn the sentiment analysis on IMDB we do not use a sliding window to compute \\theta since the ultimate goal in this task is classification and not word prediction.", "title": "Re: Is it unfair to use a global topic feature first and then do word prediction?"}, "Sk2ycnmSx": {"type": "rebuttal", "replyto": "rJbbOLcex", "comment": "I have a question regrading on the language modeling part. I believe it seems unfair to get a global word distribution(i.e. document topic) first and then use it to do word prediction. The RNN model would never do this and would perform not very good on the very beginning of this article. So does the ppl performance increase comes from this? \n\nWhat if the RNN model gets a global embedding first and then do the word prediction?", "title": "Is it unfair to use a global topic feature first and then do word prediction?"}, "HJ3y3H1Qx": {"type": "rebuttal", "replyto": "BJpisV1Qx", "comment": "Thanks for your questions. We think it is fair to say that TopicRNN is better than RNN in terms of perplexity.  And we also show in the paper that TopicRNN is better than many variants of LSTM in terms of sentiment classification results in Figure 4. As far as we know, this is the best result on this task. For TopicLSTM, we haven\u2019t had a very definitive conclusion yet except that it is performing slightly worse than TopicRNN. We are working on a journal version of this paper to address these comparisons including TopicLSTM vs LSTM. \n\nGiven the fact that Mikolov et al. 2012 and Ghosh et al. (the LSTM version of Mikolov et al.) showed that adding contextual bias to an RNN (vanilla RNN or LSTM) yields better perplexity scores than not adding contextual bias, we hope we can find a better way to optimize TopicLSTM so that we can confirm this.", "title": "Re: TopicLSTM"}, "rkwY3Zk7l": {"type": "rebuttal", "replyto": "rya03kRGg", "comment": "Thank you for your comment. We added the answers to your first two questions in the main text and posted a revision. \n\nRegarding your third question:\nWe are making this speculation given that TopicLSTM gave worse performance in terms of perplexity on PTB. This could be because LSTM already tries to model long-term dependencies and that might interact negatively with the topic bias. This might cause the model to get stuck in a bad local mode. With TopicRNN we only need a few steps (note this prevents vanishing/exploding gradients issues during optimization). The topic bias provided can be seen as some sort of \"memory\" for the RNN.", "title": "Re: Prereview Questions"}, "rya03kRGg": {"type": "review", "replyto": "rJbbOLcex", "review": "- How long did it take to train TopicRNN on the PTB and IMBD datasets?\n- How much worse are the TopicRNN results with LSTM cell compared to standard RNN cell? \n- Could you please elaborate a little bit more your speculation that \"topic models are more effective than LSTM at capturing global semantic information; standard RNNs and topic models are really complementary in this case.\"? Is this based on your observations on the performance of baseline contextual LSTM models?This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. \nExperiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. \nThe authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.\n\nSome questions and comments:\n- In Table 2, how do you use LDA features for RNN (RNN LDA features)? \n- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.\n- The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic \"trading\"? What about the IMDB one?\n- How scalable is the proposed method for large vocabulary size (>10K)?\n- What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines. ", "title": "Prereview Questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJ1aaWHNx": {"type": "review", "replyto": "rJbbOLcex", "review": "- How long did it take to train TopicRNN on the PTB and IMBD datasets?\n- How much worse are the TopicRNN results with LSTM cell compared to standard RNN cell? \n- Could you please elaborate a little bit more your speculation that \"topic models are more effective than LSTM at capturing global semantic information; standard RNNs and topic models are really complementary in this case.\"? Is this based on your observations on the performance of baseline contextual LSTM models?This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. \nExperiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. \nThe authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.\n\nSome questions and comments:\n- In Table 2, how do you use LDA features for RNN (RNN LDA features)? \n- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.\n- The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic \"trading\"? What about the IMDB one?\n- How scalable is the proposed method for large vocabulary size (>10K)?\n- What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines. ", "title": "Prereview Questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}