{"paper": {"title": "Efficient randomized smoothing by denoising with learned score function", "authors": ["Kyungmin Lee", "Seyoon Oh"], "authorids": ["~Kyungmin_Lee1", "syoh@add.re.kr"], "summary": "We provide efficient method to generate smoothed classifier for provable defense by exploiting score-based image denoiser", "abstract": "The randomized smoothing with various noise distributions is a promising approach to protect classifiers from $\\ell_p$ adversarial attacks. However, it requires an ensemble of classifiers trained with different noise types and magnitudes, which is computationally expensive. In this work, we present an efficient method for randomized smoothing that does not require any re-training of classifiers. We built upon denoised smoothing, which prepends denoiser to the pre-trained classifier. We investigate two approaches to the image denoising problem for randomized smoothing and show that using the score function suits for both. Moreover, we present an efficient algorithm that can scale to randomized smoothing and can be applied regardless of noise types or levels. To validate, we demonstrate the effectiveness of our methods through extensive experiments on CIFAR-10 and ImageNet, under various $\\ell_p$ adversaries.", "keywords": ["Adversarial Robustness", "Provable Adversarial Defense", "Randomized Smoothing", "Image Denoising", "Score Estimation"]}, "meta": {"decision": "Reject", "comment": "The paper proposes an improved method for randomized smoothing, reducing computationally complexity compared with some previous works. The authors propose to learn score functions to denoise the randomized image prior to feeding it to a trained classification model. More specifically,  two image denoising algorithms based on score estimation are proposed to be applied regardless of noise level/type.\n\n\nStrengths:\n- The paper shows strong quantitative results. The gap with white box smoothing is small on cifar, outperforming Salman et al. However according to the authors, the performance advantage could be mainly attributed to (1)  the use of better network architecture and (2) the multi-scale training, not the major contribution of a score-based denoiser. \n- The denoiser doesn't require access to the pre-trained classifiers.\n- The proposed method only requires training of one score network to handle various types of noise type/level, although reviewers have raised concerns about motivation to having a method that only needs one denoiser for multiple noise levels -  the computational bottleneck of randomized smoothing is the prediction time rather than training time and  using the same score function for multiple noise levels could be suboptimal.\n\nWeaknesses:\n- There are some concerns about the significance of the contribution as well as novelty of the work, as the denoising + pre-trained classifier architecture is already proposed. Specifically, the work can be seen as incremental to [1], although the work uses a score-based image denoiser whereas [1] uses a CNN based image denoiser and this work is more efficient as it requires only one score network, while [1] trained multiple denoisers with respect to each noise levels. \n- Reviewers have expressed concerns on the prediction efficiency of score-function based generative / denoising models.  The proposed method might exacerbate the weakness of randomized smoothing (i.e., slow prediction), especially in high-dimensions. \n-The reviewers are curious to see the benefit of the proposed denoiser over the state-of-the-art Gaussian denoisers (as used in [1]) under Gaussian noise setting.\n-Method seems to be effective for low-resolution images only. The gap with white box increases on Imagenet.\n\n[1]. Salman, Hadi, et al. \"Denoised Smoothing: A Provable Defense for Pretrained Classifiers.\" Advances in Neural Information Processing Systems 33 (2020).\n"}, "review": {"83YOTFFuxiH": {"type": "review", "replyto": "sI4SVtktqJ2", "review": "Update: I have read the author's response and decided to keep my review, confidence, and score.\n\n---\n\nSummary: randomized smoothing is a method to construct provably robust classifiers via additive Gaussian noises on the input. The authors propose to learn score functions as a means to denoise the randomized image prior to a trained classification model. As the denoising + pre-trained classifier architecture is already proposed, the contribution is only limited to the choice of using a score function. The justification and realization of the method is limited for two main reasons. See below. \n\n1. Efficiency: one of the most critical bottleneck of randomized smoothing methods is the slow prediction time. The score-function based generative / denoising models are known for their slow sampling time, so the proposed method undermines randomized smoothing in efficiency. \n\n2. Many design choices in this paper is not well justified. \n\n2-1) How good does the RHS of Eq. (12) approximates the gradient descent procedure? \n\n2-2) Even if the true gradient descent can be executed, the bound in Eq. (13) seems very bad in high dimension, thus the smoothed classifier will not be accurate unless the pre-trained classifier is already robust in the local region. \n\n2-3) Clearly using the same score function for multiple $\\sigma$ is suboptimal. Although the authors mentioned this part as an advantage, but it is not clearly compared to existing methods. Would existing methods fail if they use the same denoising function for multiple $\\sigma$?\n\n", "title": "Official review", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "X8BkRpXNLER": {"type": "rebuttal", "replyto": "sI4SVtktqJ2", "comment": "We uploaded a revised version that reflected reviews and instructions from reviewers and commenters. We sincerely appreciate everyone's instruction. ", "title": "Revision uploaded"}, "9K_0dxwxX8Q": {"type": "rebuttal", "replyto": "WNluhKAoKwp", "comment": "Thanks for your interest in our work. We appreciate your instructions and made some revisions to our rebuttal version. \n\n(1) We found out that the concept of empirical Bayes aligns with our proposition 3.1, and we added references to them. \n\n(2) Our work and the paper [SS20] seem to share a very similar idea, that they both use Bayes estimator for provable robust classification. Yet, we didn't conduct adversarial training since we only consider attaching denoiser to the pre-trained classifier, and we didn't experiment on the MNIST dataset. We inserted a short reference on [SS20] in section 2.2.\n\n(3) Since we consider provable methods for adversarial robust classification, we think [XWM+19] is irrelevant to our work. However, we appreciate your suggestion. \n\n[SS20] Saeed Saremi and Rupesh Srivastava. Provable Robust Classification via Learned Smoothed Densities, arXiv:2005.04504, 2020.\n\n[XWM+19] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, and Kaiming He. Feature denoising for improving adversarial robustness, arXiv:1812.03411", "title": "Response"}, "wEv3QasOcaH": {"type": "rebuttal", "replyto": "aK2z50z4Yo", "comment": "Thank you for response.\n\n1 - We admit that our framework may exacerbate the prediction time, but we claim that our method is efficient for generating the certified robust classifier. \n\n2-1. We wil revise the theorem with explications with literature that we've mentioned. \n\n2-2. To be clear, let $\\sigma_R$ be that used for noise in randomized smoothing, then for denoising we use $\\sigma_i$ such that \n$$\n\\sigma_R > \\sigma_1 > \\sigma_2 \\cdots > \\sigma_L \n$$\nTherefore, by taking $\\sigma_L$ as small as  possible, we can reduce the upper bound of the error. Therefore $\\sigma_L\\rightarrow 0$ is quite irrelevant to the effect of randomized smoothing.  We admit that $\\sigma=0.01$ might be loose, but as result shown in CIFAR-10, the empirical results reflect the theorem. \n\n2-3. Thank you for your explanation. Many image denoising literatures [1,2] pointed out that denoising autoencoder based image denoiser can't handle noise that differs to that used for training, which is one of the reason why the author of Denoised smoothing paper trained denoiser for each $\\sigma$. To justify the motivation of our work, note that the $\\sigma$ controls the robustness and accuracy tradeoff in randomized smoothing. Previous literatures that dealt with randomized smoothing used 3-4 $\\sigma$s to evaluate the performance, also the denoised smoothing paper does. But for instance, the user of vision API might want to select their own $\\sigma$ that fits to their samples. Then training new classifier or denoiser might be inefficient. However, our framework suggest that even without retraining the classifier or denoiser we can achieve certified robustness by using our denoiser. \n\n[1]. Guo, Shi, et al. \"Toward convolutional blind denoising of real photographs.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n\n[2]. Zhou, Yuqian, et al. \"When awgn-based denoiser meets real noises.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 07. 2020.", "title": "Response"}, "6AwxpIPixrP": {"type": "rebuttal", "replyto": "dC2KRx4w1jh", "comment": "We sincerely appreciate your instructive review. We list the response for your questions below: \n\nFor clarification, [1] trained denoising autoencoder (DAE) with MSE and classification loss, and our one-step denoiser is equivalent to DAE trained with MSE loss. The equivalence of DAE and denoising score matching (DSM) is studied through various literatures [] as we demonstrated in the paper. Therefore the perforemance gap between [1] and our one-step denoiser is due to architecture change and multi-scale training. We used U-net based architecture as in [2], and we've shown that the multi-scale training benefits us learning the score function easier in Appendix C. \n\nNote that we trained only one score network which learns score of perturbed data distribution with Gaussian noise. For the best of our knowledge, [1] or any other works experimented with denoiser trained with other noise types such as Laplace noise or uniform noise. For your information, soon We will add comparison for Gaussian noise settings. \n\nOverall, we sincerely thanks for kind and detailed review which helped us a lot.  \n\n[1] Block, Adam, Youssef Mroueh, and Alexander Rakhlin. \"Generative Modeling with Denoising Auto-Encoders and Langevin Sampling.\" arXiv preprint arXiv:2002.00107 (2020).\n\n[2] Vincent, Pascal. \"A connection between score matching and denoising autoencoders.\" Neural computation 23.7 (2011): 1661-1674.", "title": "Response to AnnonReviewer 3"}, "_Nn_cD6RZeY": {"type": "rebuttal", "replyto": "yDeA0JWWNA5", "comment": "Q6: p5 \"the noise makes the support of the score function to be whole space, [...] non-Gaussian or off-the-manifold samples\". I don't understand that part.\nA6: As demonstrated in [2], addition of Gaussian noise makes the support of data density to be the whole space (as Gaussian noise is well-defined in whole area). Therefore, the noise fills the low-density region and it makes the denoising score matching objective more amenable. We will revise the statement to be more accurate one. \n\nOverall, we sincerely thanks for kind and detailed review which helped us a lot.  \n\n[1] Salman, Hadi, et al. \"Denoised Smoothing: A Provable Defense for Pretrained Classifiers.\" Advances in Neural Information Processing Systems 33 (2020).\n\n[2] Song, Yang, and Stefano Ermon. \"Improved techniques for training score-based generative models.\" Advances in Neural Information Processing Systems 33 (2020).\n\n[3] Zhang, Kai, et al. \"Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.\" IEEE Transactions on Image Processing 26.7 (2017): 3142-3155.\n\n[4] Block, Adam, et al. \"Fast Mixing of Multi-Scale Langevin Dynamics underthe Manifold Hypothesis.\" arXiv preprint arXiv:2006.11166 (2020).", "title": "Response to AnnonReviewer 2 (3)"}, "WkSCX_KjEr7": {"type": "rebuttal", "replyto": "83YOTFFuxiH", "comment": "We sincerely appreciate your instructive review. We list the response for your questions below: \n \n1 - We realize that the slow prediction time is bottleneck for randomized smoothing, and it isn't efficient. However, the efficiency that we deal in this paper is about generation of smoothed classifier. We didn't change any algorithm on original randomized smoothing, therefore is independent to our work.\n On the other hand, it is true that putting denoiser in front of the classifier slows down the prediction time. We empirically found out that it tooks 2-3 time (which is similar to that of [1]) for one-step denoiser and 4-10 time(depends on the $\\sigma$) compare to original randomized smoothing (a.k.a white-box smoothing) which seems reasonable for prediction time. As ImageNet is of high-dimension the multiplicity makes prediction more slower, we leave accelerating the prediction time for future work.\n\n2-1. For our multi-step approach, there are two approximations involved. First is approximation of data distribution $p$ with $p_\\sigma$, and second one is approximation of $-\\log p_\\sigma(x)$ with score function $s_{\\theta,\\sigma}(x)$. Note that the first approximation error is bounded by $\\sigma\\sqrt{d}$ (which is shown in Appendix A) and second approximation error is generalization error which can be expressed by Radamacher complexity of used neural network for score network. We refer [2] for theoretic overview on learning score-based model (or equivalently DAE). \n\n2-2 As we demonstrated in our theorem, the bound is loose when image is of high-dimension. However, as our denoising algorithm anneals $\\sigma\\rightarrow 0$, theoretically we can achieve small error bounds using extremely small $\\sigma_L$. Furthermore, it is true that robust pre-trained classifier helps the performance. In fact, we experimented with classifier trained with very small Gaussian noise $\\sigma = 0.01$, but it didn't help the performance. Instead, as illustrated in Appendix C.2 the classifiers with better test accuracy results in better performance. \n\n2-3 We didn't really understand your question. But based on our interpretation, even though we use only one score function for multiple $\\sigma$ the score function is conditioned by predetermined sequence of $\\sigma$. Can you explain more about 'existing methods'? We hope your answer and willing to response.\n\nOverall, we sincerely thanks for kind and detailed review which helped us a lot. \n\n[1] Salman, Hadi, et al. \"Denoised Smoothing: A Provable Defense for Pretrained Classifiers.\" Advances in Neural Information Processing Systems 33 (2020).\n\n[2] Block, Adam, Youssef Mroueh, and Alexander Rakhlin. \"Generative Modeling with Denoising Auto-Encoders and Langevin Sampling.\" arXiv preprint arXiv:2002.00107 (2020).", "title": "Response to AnnonReviewer 3"}, "NXlEbtPxJqj": {"type": "rebuttal", "replyto": "9r6nr0ilwNZ", "comment": "Thanks for your interest on our work.\n\nOur work can be seen as a generalization for [1]. They used CNN based image denoiser and proposed denoised smoothing. We built upon same denoised smoothing framework, but we devised score-based image denoiser which includes the method that used in [1]. Moreover, our work requires only one score network, while [1] trained multiple denoisers with respect to each noise levels. Therefore, our method is more efficient. \n\n[1]. Salman, Hadi, et al. \"Denoised Smoothing: A Provable Defense for Pretrained Classifiers.\" Advances in Neural Information Processing Systems 33 (2020). ", "title": "Response from authors"}, "FSWaIFbDjGq": {"type": "rebuttal", "replyto": "yDeA0JWWNA5", "comment": "Q3: I do not understand well the motivation for using score-based methods. Why scored based denoising in particular and not other blind denoising models? The current method is impacted by the size of the images, while most of the existing blind denoising algorithms are not affected by the size of the image. Actually, why even focus on learning base methods? Cannot simpler methods with handcrafted priors do the job? I might also be wrong. Could you please elaborate?\nQ5: Aren't there missing reference in the related work regarding blind denoising ?\n\nA3, A5: We aren't really certain for the definition of 'blind denoising', but if we assume blind denoising refers to denoising without the knowledge on the noise, then our framework is not a blind denoising, because we select the noise type and magnitude to decide the robustness. Moreover, our multi-step denoiser require the form of noise distribution. \n\nNote that non-learning image denoising algorithm such as BM3D can be applied to our denoised smoothing framework. However, many works shown that using deep image prior can boost the performance of image denoising. As [1] did, our method is built upon deep learning based image denoiser, and we generally divide into two types. First, training CNN based image denoiser such as DnCNN[3]. Those methods learns the feature to remove noise (especially Gaussian noise) from noisy images. Second, solving optimization with MAP formulation. Those method requires prior for image restoration, which the prior can be either handcrafted or by learning process. Many works showed that the generative model based approach is prominent, however those generative models such as GAN or Inveritible neural network took so long to denoise even when only small amount of noise is inserted. It is important to note that our score-based method covers both approaches with only one neural network. \nMoreover, we acknowledge that our algorithm degrades their performance when the dataset is of high dimension(e.g. ImageNet). The problem of high-dimensionality may alleviated by longer step of denoising, i.e. more minuscule sequence of $\\sigma_i$. We suspect another approach such as striding with smaller patches to denoise, or downsampling and upsampling as done in [4]. We leave it for future work. \n\nQ4: Qualitatively speaking it seems to me that the visual performances are not very good when compared to existing denoising algorithms (maybe a quantitative comparison in term of PSNR with other algorithms would be relevant, rather than only showing qualitative results). Also the multi step denoiser seems to give a lot of artefacts in figure 1.\nA4: The visual performance might not be optimal compare to existing denoising algorithms. As mentioned above, We believe visiting more noise levels during MAP optimization processes may result in better denoising quality. However, due to the time bottleneck of randomized smoothing, we mediate the visual quality vs time tradeoff. It is definitely true that the better denoiser can perform better denoised smoothing but it really doesn't require ultimate clean and sharp image. The results from [1] supports that their classifier induced denoisers have strange artifacts but still perform well in denoised smoothing setting. ", "title": "Response to AnnonReviewer 2 (2)"}, "jdPW38u8Y8m": {"type": "rebuttal", "replyto": "yDeA0JWWNA5", "comment": "We sincerely appreciate your instructive review. We list the response for your questions below: \n\nQ1: If I understand well, one-step denoiser can only handle gaussian noise whereas multi-step can be applied to any log concave distribution? What is the advantage of one-step denoiser over multistep, does it perform better for gaussian noise?\nA1: Yes. The one-step denoiser is equivalent to a trained denoising autoencoder (DAE) which were used in [1]. DnCNN and MemNet are exemplar for such approach. On the other hand, the multi-step denoiser is a iterative algorithm that optimizes maximum a posteriori (MAP) loss with gradient approximated by score function, which allows various log-concave noise distributions. \n  The one-step denoiser is much faster than multi-step denoiser as it only requires one forward pass to achieve a denoised image. Moreover, the empirical results show that one-step denoiser performs better than multi-step denoiser for Gaussian test setting. On the other hand, multi-step denoiser is 'efficient' that can deal with various noise types. \n\nQ2: It is not clear to me why the method does not make use of the pretrained classifier but still outperforms Salman et al. which can acess the classifier? Is it only due to the denoiser's performance?\nA2: The performance gap between [1] and our can be explained with two aspects: the use of better network architecture and the multi-scale training. We used the U-net based architecture with improved convolution layer as illustrated in [2] which is developed for score-based modeling. Besides the impact of architecture, we empirically found out that multi-scale training slightly boosts the performance (Appendix C). \n Moreover, even though we didn't include in the paper as we think is irrelevant to our interest, we trained score network with classification regularization as done in [1]. We trained the loss function as following:\n$$\nL_{\\text{score}} + \\gamma D_{KL}(f(x) \\| f(\\hat{x}))\n$$, where $\\hat{x} = x -\\sigma^2s_{\\theta,\\sigma}(\\tilde{x})$ is denoised image from our one-step denoiser. We figure out that the classifier loss marginally improves the performance when $\\sigma$ is large, but overall there were no difference.\n\n", "title": "Response to AnnonReviewer 2 (1)"}, "I4WX3iIKTx-": {"type": "rebuttal", "replyto": "rV1pv8kEVg", "comment": "We sincerely appreciate your instructive review. Our work proposes score-based model for image denoising that can efficiently generate smoothed classifier without retraining classifiers. If there is any further question, let us know.", "title": "Response to AnnonReviewer 1"}, "rV1pv8kEVg": {"type": "review", "replyto": "sI4SVtktqJ2", "review": "This paper proposes a method based on denoising to protect classifiers from adversarial attacks. Unlike existing methods based on randomized smoothing with various noise distributions to retrain several classifiers, the proposed one uses denoising as the preprocess of the classifier. The experimental results demonstrate the proposed method has good performance.", "title": "I am not an expertise of the area of this paper. But I think the idea is interesting.", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "dC2KRx4w1jh": {"type": "review", "replyto": "sI4SVtktqJ2", "review": "This paper presents a denoising-based method for randomized smoothing that converts a base classifier into a smoothed one with p-robustness to adversarial examples. It considers a practical setting where the retraining/finetuning of the base classifier is largely inapplicable (e.g. the commercial classification service with only API provided to users).  To do this, it adopts a recently proposed methodology termed denoised smoothing [1] by prepending a custom-trained denoiser to the pretrained classifier. The major novelty of this work lies at the proposed denoising method using learned score function. The new denoising method only requires training one score network and is readily applicable to defend various $l_p$ adversaries, which is a key feature not available in [1].  The experiments show the proposed method outperforms the previous denoising-based approach, and is sometimes on par with the white-box approach [2] that manipulates the classifier. \n\nBasically, this is an incremental work over [1] but the contributions claimed are perceived myself (though I have to admit I'm an expert on image denoising, instead of adversarial defense) \nHowever, I do have some concerns about the method and the experiments, listed as follows:\n\n- The major advantage of the score-function-based denoiser is the flexibility to handle various noise types and levels.  I don't expect it can beat the specialized Gaussian denoiser [1] under Gaussian perturbation setting.  As it is the case on Table 1/2, I'm wondering what's the benefit of  the proposed denoiser over the state-of-the-art Gaussian denoisers (as used in [1]) under Gaussian noise setting?\n- The flexibility to tackle various $l_p$ adversary, the key feature of the proposed method is not thoroughly evaluated. In Table 2, I suggest the authors to add comparisons to [1] with denoiser trained on Gaussian noise setting, as well as ones trained with noise type aligned with the test setting. \n\n[1] Denoised Smoothing: A Provable Defense for Pretrained Classifiers, Arxiv 2020  \n[2] Certified Adversarial Robustness via Randomized Smoothing, ICML 2019\n", "title": "Good work with some incremental novel contributions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "yDeA0JWWNA5": {"type": "review", "replyto": "sI4SVtktqJ2", "review": "Summary & contribution: \n\nin this paper the authors propose an improved method for randomized smoothing (for provable defense) which performs better and is less computationally expensive than previous work. More specifically the authors propose two image denoising algorithms (based on score estimation) that can be applied regardless of noise level/type. In fact, the paper is mainly an improvement of Salman et al. (2020) with a blind denoiser. Therefore, it is not needed to train different models for different kind of attacks. Another advantage of the method is that one does not need to retrain the classifier and does not even need information from the pretrained classifier while the method from Salman et al.  requires access to the classifier.\n\n\nStrengths: \n\n-strong quantitative results. Experimental section is promising. The gap with white box smoothing is small on cifar. The method outperforms Salman et al.\n\n-Only need to train one score network to handle various types of noise type/level \n\n-Denoiser doesn't need access to the pretrained classifier.\n\nWeaknesses:\n\n-In my opinion writing can be improved. I am not very familiar with the literature and it took me some time to understand section 3. More specifically I did not understand the motivation for using score based denoisers rather than a more \"standard\" algorithm for blind denoising. \n\n-Method seems to be effective for low-resolution images only. The gap with white box increases on Imagenet.\n\nQuestions for the authors:\n\n-If I understand well, one-step denoiser can only handle gaussian noise whereas multi-step can be applied to any log concave distribution? What is the advantage of one-step denoiser over multistep, does it perform better for gaussian noise? \n\n-It is not clear to me why the method does not make use of the pretrained classifier but still outperforms Salman et al. which can acess the classifier? Is it only due to the denoiser's performance?\n\n-I do not understand well the motivation for using score-based methods. Why scored based denoising in particular and not other blind denoising models? The current method is impacted by the size of the images, while most of the existing blind denoising algorithms are not affected by the size of the image. Actually, why even focus on learning base methods? Cannot simpler methods with handcrafted priors do the job? I might also be wrong. Could you please elaborate?\n\n-Qualitatively speaking it seems to me that the visual performances are not very good when compared to existing denoising algorithms (maybe a quantitative comparison in term of PSNR with other algorithms would be relevant, rather than only showing qualitative results). Also the multi step denoiser seems to give a lot of artefacts in figure 1. \n\n-Aren't there missing reference in the related work regarding blind denoising ? \n\n-p5 \"the noise makes the support of the score function to be whole space, [...] non-Gaussian or off-the-manifold samples\". I don't understand that part. \n\n\nAt this stage I give a weak accept, but I would consider raising my score if authors answer my concerns.\n\nTypos:\n\np8 \"without any re-trianing.\"\n\np7 \"smoothingon\" \n\np4 \" matching obejctiv\" ", "title": "Promising results,  clarifications would be appreciated", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}