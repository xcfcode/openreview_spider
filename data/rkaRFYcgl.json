{"paper": {"title": "Low-rank passthrough neural networks", "authors": ["Antonio Valerio Miceli Barone"], "authorids": ["amiceli@inf.ed.ac.uk"], "summary": "Describe low-rank and low-rank plus diagonal parametrizations for Highway Neural Networks, GRUs and other kinds of passthrough neural networks. Present competitive experimental results.", "abstract": "Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. For large depths, this is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem, e.g. LSTMs, GRUs, Highway Networks and Deep Residual Networks, which are based on a single structural principle: the state passthrough. We observe that these \"Passthrough Networks\" architectures enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on several tasks, including a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.", "keywords": ["Deep learning"]}, "meta": {"decision": "Reject", "comment": "The reviewers seem to agree that the framework presented is not very novel, something I agree with.\n The experiments show that the low rank + diagonal parameterization can be useful, however. The paper could be improved by making a more tightened message, and clearer arguments. As it currently stands, however it does not seem ready for publication in ICLR."}, "review": {"BJnmF0iUl": {"type": "rebuttal", "replyto": "Bka3I09Ix", "comment": "The low-rank plus diagonal parametrization is needed to achieve success on the memory tasks (just using low-rank without the diagonal matrices fails unless the rank is so large that the total number of parameters essentially matches the baseline full-matrix parametrization). In the other tasks it doesn't seem to be necessary, but it does not hurt performance and it only adds a small amount of parameters and computational cost and it is not difficult to implement, therefore I would recommend  to always add the diagonal matrices whenever you use a low-rank parametrization.", "title": "Answer"}, "Bka3I09Ix": {"type": "rebuttal", "replyto": "SkBu1scIg", "comment": "In my above comments, whenever I wrote symmetric, I meant diagonal. I apologize for this mistake.\n\nSpecifically, when I wrote \"Further, I don't understand what the framework has to do with the low-rank + symmetric approach in the first place.\" I meant \"Further, I don't understand what the framework has to do with the low-rank + diagonal approach in the first place.\". \n\nMy point was this: Your paper deals with two main ideas: passthrough networks and replacing weight matrices with a diagonal + low-rank construct. What is the connection between these two ideas? Why would the diagonal + low-rank construct be applied specifically to passthrough networks and not to other networks? If in fact the diagonal + low-rank approach applies equally to passthrough and non-passthrough networks, then the discussion of passthrough networks seems redundant.\n\nI hope this is more clear. Again, apologies for the confusion.\n", "title": "Symmetric = Diagonal"}, "SkBu1scIg": {"type": "rebuttal", "replyto": "SyAKov3Bx", "comment": "Thanks for your comments.\n\n> However, the diagonal + low-rank model assigns the majority of its parameters to the low-rank matrix, which is NOT local. Hence, one cannot say that the authors adhere to these physical constraints.\n\nOnly the convolutional parametrizations, which correspond to cellular automata, are strictly local. Low-rank parametrizations correspond to low-badwith constraints, which may also be thought to ultimately arise from physical locality constraints, although this discussion is beyond the scope of the paper.\n\n> In section 3.1, the authors claim that 99.13% is the state-of-the-art on MNIST. LeCun achieved a lower error rate in 1998! Do you mean the state-of-the-art on randomly permuted MNIST?\n\nI meant the permutation-invariant MNIST (i.e. the task where you are not allowed to use convolutions). I've edited the paper to make it clear.\n\n>  Overall, I think section 3.1 should just be omitted, as it contains too little information to be useful.\n\nAgreed. I moved it to the appendix, and I will remove it if the reviewers agree.\n\n> I agree with reviewer three that the statement \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" makes it sound as if the framework is a contribution, which it isn't. \n\nAgreed. Removed.\n\n> Further, I don't understand what the framework has to do with the low-rank + symmetric approach in the first place. \n\nI don't mention any low-rank + symmetric approach. I can't follow this remark.\n\n> In the conclusion, you say that your approach is \"orthogonal\" to the convolutional parametrizations explored by He et. al. How is that? He et al. experiment with a low-rank structure (they call it bottleneck) between each skip connection. That seems very similar to what you do. \n\nHe et al. experiment with a bottleneck structure, but it is non-linear and spread three layers that replace the two layers of the base ResNet block, while my proposal is linear and replaces one layer with two layers. The general idea of using an information bottleneck is the same, but the details are different.\n\n> In any case, if I use the low-rank layout from He et al., it would be reasonable to think that using your low-rank layout in addition would yield at most diminishing returns, hence the methods are not orthogonal. \n\nThat specific bottleneck parametrization may be not orthogonal to mine, but the general ResNet idea is orthogonal with bottleneck parametrizations, including mine.\n\n>  Also, you talk about the convolutional parametrization in Srivastava et al in the same sentence. As far as I can tell, that paper neither dealt with convlutional nets nor with low-rank factorizations.\n\nConvolutional highway networks are mentioned in the CIFAR-10 experiments.\n", "title": "Response"}, "rymGj558x": {"type": "rebuttal", "replyto": "BJOY_CR7g", "comment": "Thanks for your comments.\n\n> the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.)\n\nFair point, I removed that sentence.\n\n> They are hardly better though, which makes it unclear why low-rank networks should be used.\n\nFor memory, addition and permuted sequential MNIST they work better than full-rank networks, presumably due to a regularization effect. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.\n\nOn language modelling, even with the additional regularization of dropout, they work pretty much as good as full-rank networks for the same state size, and better for the same parameter size, thus at the very least they could be used as a from of model compression.\n\n", "title": "Response"}, "Hk9rKccUg": {"type": "rebuttal", "replyto": "SJrNt8lVl", "comment": "I've added uRNN results in the figure and results for an additional variant of LRD-GRU.\n\nI've also added LSTM and LRD-LSTM results for the language modeling task.\n", "title": "Response"}, "Bk2xKq98g": {"type": "rebuttal", "replyto": "BkVSLugNx", "comment": "Thanks for your comments.\n\n> - In the highway network experiment, the author does not compare with a baseline\n\nThe highway network experiment was exploratory, and I didn't have time to carry out all the proper evaluation. Therefore I moved it to the appendix. May remove it from the paper if the reviewers agree.\n\n> - It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison.\n\nI now tried to replicate (Graves 2013) setup, although I still get worse results. Nevertheless, I still get an improvement using the Low-rank plus diagonal parametrization on an LSTM model, indicating that the method is robust w.r.t. different architectures.\n\n> -Author claims state-of-art in the memory task. However, their approach uses  more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. \n\nFair point, I have now included a direct comparison to a uRNN with a large state size and also some experiments with a variant of the LRD-GRU.\n\n> -  It would be informative to see how low-rank RNN performs using overall 6.5K parameters.\n\n> Overall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.\n\nHope that now it's better.\n\nI've tried in different configurations but it fails.\n\n> - It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task.\n\nAdded the curve for the memory task, although for a different state size than in the uRNN paper.\n\n> -  it is not clear when to use low-rank or low-rank + diagonal from the experiments.\n\nThe low-rank is sufficient in most cases but not on the memory task. Since the low-rank plus diagonal adds a very small amount of complexity and parameters, there seems no reasons not to always use it.\n", "title": "Response"}, "rJZBL95Lx": {"type": "rebuttal", "replyto": "rkaRFYcgl", "comment": "I added additional results on the memory tasks and language modeling task.\n\nFor the memory task, as suggested by the reviewers, I compare against uRNN models with approx. the same number of parameters as the LRD-GRU. This uRNN, much larger than the one used in the original uRNN paper, converges quickly on the simple memory task but overfits (training was terminated by early stopping in the reported graph).\n\nOn the variable sequence length task (Danihelka 2016) it trains faster (with somewhat larger oscillations) than the LRD-GRU, but on the variable lag task (Henaff 2016) it fails.\n\nI've also considered a variant of the LRD-GRU which I tried only on these two subtasks. In this variant I add weight normalization (Salimans 2016) and a weight max-row-norm constraint in order to reduce the amounts of time that NaN recovery triggers. With this modification, NaN recovery is unnecessary, and the models train much faster. In fact, it performs on par with the uRNN even on the (Danihelka 2016) task.\n\nFor the language modelling task, I did experiments with a LSTM baseline and a low-rank plus diagonal LSTM, while trying to use the same setup of (Graves 2013). I still can't replicate those results (the code is online at https://github.com/Avmb/lowrank-lstm feel free to have a look at it), but I get an improvement from the low-rank plus diagonal parametrization compared to the baseline.\n\nI also moved the experiments on the Highway Networks in the appendix, describing them as exploratory and preliminary. It may make sense to remove them completely from the paper if the reviewers agree.\n\nI removed the phrasing that implied that the framework was a novel contribution.\n", "title": "Third revision"}, "SyAKov3Bx": {"type": "rebuttal", "replyto": "rkaRFYcgl", "comment": "I would like to give some feedback on an area the reviewers did not touch on, presentational shortcomings. I think those would also have to be improved for an acceptance of the paper.\n\nComments:\n\nThe authors say \"Classical physical systems, however, consist of spatially separated parts with primarily local interactions, long-distance interactions are possible but they tend to be limited by propagation delays, bandwidth and noise. Therefore it may be beneficial to bias our model class towards models that tend to adhere to these physical constraints by using a parametrization which reduces the number of parameters required to represent them\" However, the diagonal + low-rank model assigns the majority of its parameters to the low-rank matrix, which is NOT local. Hence, one cannot say that the authors adhere to these physical constraints.\n\nIn section 3.1, the authors claim that 99.13% is the state-of-the-art on MNIST. LeCun achieved a lower error rate in 1998! Do you mean the state-of-the-art on randomly permuted MNIST?\n\nAlso, in section 3.1, you fail to compare against the same network without low-rank factorization. Also you do not mention how many parameters your model saved compared to a full-rank model. Overall, I think section 3.1 should just be omitted, as it contains too little information to be useful.\n\nI agree with reviewer three that the statement \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" makes it sound as if the framework is a contribution, which it isn't. (It is on page 2 of the original highway paper!) Also the framework is so general one almost cannot call it a framework. (We define a general function in terms of 3 new general functions.) Further, I don't understand what the framework has to do with the low-rank + symmetric approach in the first place. Any neural network or indeed any method that somehow involves matrices can have its matrices replaced with low-rank + symmetric form. I think the authors have the believe that passthrough networks do not alter the state as much per layer as other networks, and thus are more likely to have a wasteful parameter budget. However, this is just a belief and not experimentally validated. In summary, section 2 is more confusing than helpful. I would immediately start by mentioning the model you are investigating (low-rank + symmetric) and then briefly explain why you think it makes particular sense to apply it to GRU / Highway networks.\n\nThe graphs in Figure 2 are two large. One-curve graphs do not need one sixth of a page. While this is obviously just a presentation shortcoming, it nevertheless sends the message that you didn't have enough to say to fill the page space with words. Also, if you are going to use the entire page for the six graphs, at least make the axis labels of a font size comparable to that of the main text.\n\nIn section 3.2.1 and 3.2.2, you have to present your results in a table and not just in the text, as you do in section 3.2.3 and 3.2.4. If you make Figure 2 smaller (see previous paragraph), you will have plenty of space for this.\n\nDetails such as learning rate and mini-batch size should either be in the appendix or in a seperate section, not intermixed with the experimental results. That makes it harder to read.\n\nI can't find any experiment where diagonal without symmetric model outperformed the diagonal plus symmetric model. If there is no such experiment, the diagonal without symmetric model should be omitted from the paper or relegated to a side note as it adds virtually no value, as the vast majority of parameters are tied up in the diagonal matrix.\n\nIn the conclusion, you say that your approach is \"orthogonal\" to the convolutional parametrizations explored by He et. al. How is that? He et al. experiment with a low-rank structure (they call it bottleneck) between each skip connection. That seems very similar to what you do. In any case, if I use the low-rank layout from He et al., it would be reasonable to think that using your low-rank layout in addition would yield at most diminishing returns, hence the methods are not orthogonal. Also, you talk about the convolutional parametrization in Srivastava et al in the same sentence. As far as I can tell, that paper neither dealt with convlutional nets nor with low-rank factorizations.\n\nYou include a lot of very basic references in your papers, i.e. multiple references on \"neural networks were successful\", multiple on the vanishing gradient problem, etc. I think you don't need that many \"obvious\" references. (This is a very minor point.) On the other hand, I think you are missing some more important references that have looked at low-rank or related methods for neural nets, such as:\n\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc' Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148\u20132156.\n2013.\n\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with\nlow rank expansions. arXiv preprint arXiv:1405.3866, 2014.\n\nEmily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure\nwithin convolutional networks for efficient evaluation. In Advances in Neural Information Processing\nSystems, pages 1269\u20131277. 2014.", "title": "From an interested reader: Feedback"}, "BJeOaIeNl": {"type": "rebuttal", "replyto": "SJrNt8lVl", "comment": "Results are presented as plots in the original papers (uRNN, Associative LSTM and Orthogonal RNN) and they are a bit difficult to regenerate given the limited hardware resources I have currently access to (uRNN and Associative LSTM in particular are slow to train). Code for Associative LSTM and Orthogonal RNN is also not available, as far as I know.\n", "title": "Results"}, "BkxsdUlNe": {"type": "rebuttal", "replyto": "Hkcx-LeVe", "comment": "Results are SOTA on the memory tasks as far as I know.", "title": "State-of-the-art on memory task"}, "Bk3f80RXl": {"type": "review", "replyto": "rkaRFYcgl", "review": "The paper is clear, just to make the task go away.The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal) it is shown to work as well as a fully-parametrized network on a number of tasks.\n\nThe paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.)\n\nAside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks. They are hardly better though, which makes it unclear why low-rank networks should be used. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.", "title": "No question", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJOY_CR7g": {"type": "review", "replyto": "rkaRFYcgl", "review": "The paper is clear, just to make the task go away.The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal) it is shown to work as well as a fully-parametrized network on a number of tasks.\n\nThe paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.)\n\nAside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks. They are hardly better though, which makes it unclear why low-rank networks should be used. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.", "title": "No question", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJM0_tEmx": {"type": "rebuttal", "replyto": "B1Zf493zg", "comment": "I ran a LM experiment with a larger GRU baseline and it did not overfit more than the other models (thanks to recurrent dropout, I presume). In fact it performed similarly to my best low-rank+diagonal model, although with a much higher number of parameters. With that many parameters, it should be possible in principle to train low-rank and low-rank+diagonal models with a much larger state size, but I found it difficult to it on the GPUs that I have access to because of memory constraints (a low-level implementation optimized for memory consumptions could possibly help, although I didn't investigate this).\n\nI also fixed a result in table 2 (the 5459-64 GRU) and some details in appendix A.2 (learning rate and lack of character embeddings).\n", "title": "Follow-up"}, "r1-MLKNme": {"type": "rebuttal", "replyto": "rkSV6rJ7g", "comment": "> 1) Why don't you compare low-rank GRU with a GRU baseline for the memory variant and addition tasks?\n\nThe original papers report results for LSTM models, and papers that compare GRUs and LSTMs generally found that they perform comparably for the same state size, which is consistent with the results I obtained here when I did the comparisons, therefore I assumed that I could avoid doing the comparison with GRU baselines for these tasks in order to save computational time.\n\n> Memory and adding tasks are usually used to show that model can capture long-term dependencies. Do you think that the low-rank and low-rank+diagonal parametrizations help with that problem?\n\nLow-rank and low-rank+diagonal parametrizations allow to use larger state size for the same number of parameters (which is practically limited by available dataset size and computational resources), increasing the memory capacity of the model, therefore helping it to capture long-term dependencies.\n\nIn tasks on synthetic data, which is in principle unlimited, overfitting is impossible, therefore even a full-matrix model should eventually reach the performance of a low-rank or low-rank+diagonal model if trained for long enough (assuming that the optimizer doesn't get stuck at some bad critical point). In tasks on natural data, which is limited in size, a full-matrix model can overfit sooner than a low-dimensional one, resulting in lower generalization accuracy.\n\nSpecifically, adding a single node to a hidden layer parametrized by full nXn matrices adds O(n) parameters to the model, therefore when n is large, even adding a single node can change a model that underfits because it has too little parameters into one that overfits because it has too many parameters, without ever hitting close to the ideal number of parameters for the task. Low-dimensional parametrizations can help controlling the parameter number without sacrificing the state size.\n\n> 4) What is the total number of parameters for the different models PTB and the permuted sequential mnist tasks?\n\nAdded in the paper.\n\n> 5) How exactly is computed test per-char perplexity on the char-level PTB experiment and how your model compare to state-of-art models in term of bits per character ? \n\nThe test per-char perplexity is the base-2 exponential of the bits-per-character entropy. I added a citation to existing work. My model performs worse than published results, although the experimental setup is slightly different (published results usually maintain the RNN state between sentences and use a slightly different character set). In any case, my goal here is not to set a new SOTA but to show a relative improvement of the proposed parametrizations w.r.t. the full-matrix baseline.\n", "title": "Answer"}, "rkSV6rJ7g": {"type": "review", "replyto": "rkaRFYcgl", "review": "I have a few questions regarding the experimental section.\n\n\n1) Why don't you compare low-rank GRU with a GRU baseline for the memory variant and addition tasks?\n\n2) Memory and adding tasks are usually used to show that model can capture long-term dependencies. Do you think that the low-rank and low-rank+diagonal parametrizations help with that problem?\n\n4) What is the total number of parameters for the different models PTB and the permuted sequential mnist tasks?\n\n5) How exactly is computed test per-char perplexity on the char-level PTB experiment and how your model compare to state-of-art models in term of bits per character ? \nThe author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network.\n\nAuthor also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks.\n\nAn empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). \n\nHowever, there are few problems with the evaluation:\n\n- In the highway network experiment, the author does not compare with a baseline.\nWe can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer  (first layer of size $n$, second layer of size $d$, third layer of size $n$) and not in the gate functions. Also, how did you select the hyperparameter values?.\n\n- It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison.\nAlso the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline.\n\n-Author claims state-of-art in the memory task. However, their approach uses  more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size.\n\n- It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task.\n\n- it is not clear when to use low-rank or low-rank + diagonal from the experiments.\n\nOverall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.", "title": "Prereview question", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkVSLugNx": {"type": "review", "replyto": "rkaRFYcgl", "review": "I have a few questions regarding the experimental section.\n\n\n1) Why don't you compare low-rank GRU with a GRU baseline for the memory variant and addition tasks?\n\n2) Memory and adding tasks are usually used to show that model can capture long-term dependencies. Do you think that the low-rank and low-rank+diagonal parametrizations help with that problem?\n\n4) What is the total number of parameters for the different models PTB and the permuted sequential mnist tasks?\n\n5) How exactly is computed test per-char perplexity on the char-level PTB experiment and how your model compare to state-of-art models in term of bits per character ? \nThe author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network.\n\nAuthor also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks.\n\nAn empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). \n\nHowever, there are few problems with the evaluation:\n\n- In the highway network experiment, the author does not compare with a baseline.\nWe can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer  (first layer of size $n$, second layer of size $d$, third layer of size $n$) and not in the gate functions. Also, how did you select the hyperparameter values?.\n\n- It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison.\nAlso the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline.\n\n-Author claims state-of-art in the memory task. However, their approach uses  more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size.\n\n- It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task.\n\n- it is not clear when to use low-rank or low-rank + diagonal from the experiments.\n\nOverall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.", "title": "Prereview question", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1Zf493zg": {"type": "rebuttal", "replyto": "SJ6YzFhfe", "comment": "- The two proposal are \"low-rank\" and \"low-rank plus diagonal\". Diagonal-only, which would be essentially a bag-of-words model with some temporal weighting, was not considered because I conjecture that it would not be expressive enough for practical tasks.\n- I did not re-implement the uRNN and Associative LSTM models. Training curves are available in the referenced papers, and they are comparable since the experimental setups are the same.\n- For Sequential permuted MNIST, I experimented with a full-rank GRU with state size equal to 256 and it failed to converge due to training instability. Different choices of learning rate, etc. could possibly make it work, but this is larger than what is commonly used in the literature for this task so I didn't investigate it any further. For character-level language model I did not try to use baselines larger than 1000 (which is the common size found in the literature) but I expect overfitting, especially for the largest setting. GPU memory utilization could be also an issue. If I can get some GPU time in the next few days I may try to run a few more experiments on language modeling. Anyway, the main claim for this task is that our proposal obtains a better perplexity for the same number of parameters.\n\n\n", "title": "Answer"}}}