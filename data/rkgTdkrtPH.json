{"paper": {"title": "NoiGAN: NOISE AWARE KNOWLEDGE GRAPH EMBEDDING WITH GAN", "authors": ["Kewei Cheng", "Yikai Zhu", "Ming Zhang", "Yizhou Sun"], "authorids": ["viviancheng@cs.ucla.edu", "zhuyikai.zyk@gmail.com", "mzhang_cs@pku.edu.cn", "yzsun@cs.ucla.edu"], "summary": "We proposed a unified Generative Adversarial Networks (GAN) framework to learn noise-aware knowledge graph embedding.", "abstract": "Knowledge graph has gained increasing attention in recent years for its successful applications of numerous tasks. Despite the rapid growth of knowledge construction, knowledge graphs still suffer from severe incompletion and inevitably involve various kinds of errors. Several attempts have been made to complete knowledge graph as well as to detect noise. However, none of them considers unifying these two tasks even though they are inter-dependent and can mutually boost the performance of each other. In this paper, we proposed to jointly combine these two tasks with a unified Generative Adversarial Networks (GAN) framework to learn noise-aware knowledge graph embedding. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms both in regard to knowledge graph completion and error detection. ", "keywords": ["Knowledge graph embedding", "Noise aware"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a noise-aware knowledge graph embedding (NoiGAN) by combining KG completion and noise detection through the GANs framework. The reviewers find that the idea is interesting, but the comparison to SOTA is largely missing. The paper can be improved by addressing the reviewer comments. "}, "review": {"ryxPtR_2sB": {"type": "rebuttal", "replyto": "S1l96w7HKH", "comment": "We thank the reviewer for the constructive reviews. We addressed the questions and concerns of the reviewer accordingly in the following.\n\n(1) Thanks to the reviewer for pointing out the problem of data leakage in FB15K and WN18. We have conducted the experiments on the FB15K-237 and WN18RR instead. Please find the result in Table 3 in our latest version of the paper.\n\n(2) Thanks to the reviewer for pointing the issue of insufficient baselines. We have added more baseline methods, including (1) KGE models (e.g., DistMult [5] and RotatE [4]), (2) robust KGE models (e.g., attention based method [6]) and (3) KGE models with GAN (e.g., KBGAN [3]). In addition, to show that our NoiGAN can be easily generalized to various KGE models, RotatE is also added as score function for NoiGAN. Please find the result in Table 3 in our latest version of the paper. The results show that both NoiGAN-TransE and NoiGAN-RotatE consistently and significantly outperform all the baseline methods in terms of robustness. \n\n(3) The goal of NoiGAN and KBGAN is totally different. KBGAN incorporates GAN for better negative sampling to improve the quality of embeddings. The discriminator of the GAN is their final KGE model.  However, in our case, NoiGAN utilizes GAN to determine whether a triple is noisy and it is independent with our KGE model. Different from KBGAN, the discriminator in our GAN is a binary classifier, which is used to learn confidence score for each triple to enable NoiGAN to cope with noisy training data. To further show the difference between NoiGAN and KBGAN, we have added KBGAN as our baseline and report the results in Table 3 in our latest version of the paper. The results indicate that KBGAN cannot cope with noisy training data.\n\n(4) To analyze the efficiency of NoiGAN, we compare the total training time until converge against the baseline methods on FB15K-237 with 100% noise as follows.\n\nMethods                                    The whole training time until converge (min)\nTransE [8]                                                                     60\nCKRL [7]                                                                      150\nDistMult [5]                                                                  40\nRotatE [4]                                                                     60\nKBGAN [3]                                                                    30\nattention based method [6]                                   3600\nNoiGAN-TransE                                                           60\n\nWe can observe that our NoiGAN does not cost much time compared to other baseline methods.\n\n(5) Thanks to the reviewer for pointing the issue of not reporting NoiGAN performance with 0% noise. We have added these experiments as shown in Table 3 in our latest version of the paper. We can observe that for NoiGAN-RotatE, it has almost the same performance as its variant RotatE on FB15K-237 and WN18RR. It performs even better than RotatE on YAGO3-10. The major reason could be that YAGO3-10 contains more noise than FB15K-237 and WN18RR. Our NoiGAN-RotatE shows its superiority in this situation. \n\n[1]  \u201cGraphGAN: Graph Representation Learning with Generative Adversarial Nets.\u201d AAAI'18. \n[2]  \u201cIrgan: A minimax game for unifying generative and discriminative information retrieval models.\u201d SIGIR'17. \n[3]  \u201cKbgan: Adversarial learning for knowledge graph embeddings.\u201d NAACL\u201918. \n[4]  \u201cRotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space.\u201d ICLR'19. \n[5]  \u201cEmbedding Entities and Relations for Learning and Inference in Knowledge Bases.\u201d ICLR'15. \n[6]  \u201cLearning Attention-based Embeddings for Relation Prediction in Knowledge Graphs.\u201d ACL\u201919\n[7]  \u201cDoes william shakespeare really write hamlet?  knowledge representation learning with confidence.\u201d AAAI\u201918.\n[8] \u201cTranslating embeddings for modeling multi-relational data.\u201d NeurIPS\u201913\n\n\n\n\n\n", "title": "Responds to Review #1"}, "BklSeTd3jS": {"type": "rebuttal", "replyto": "ryl29S9x5H", "comment": "We thank the reviewer for the constructive reviews. We addressed the questions and concerns of the reviewer accordingly in the following.\n\n(1) Thanks to the reviewer for pointing the issue of insufficient baselines. We have added more baseline methods, including (1) KGE models (e.g., DistMult [5] and RotatE [4]), (2) robust KGE models (e.g., attention based method [6]) and (3) KGE models with GAN (e.g., KBGAN [3]). In addition, to show that our NoiGAN can be easily generalized to various KGE models, RotatE is also added as score function for NoiGAN. Please find the result in Table 3 in our latest version of the paper. The results show that both NoiGAN-TransE and NoiGAN-RotatE consistently and significantly outperform all the baseline methods in terms of robustness. \n \n(2) Thanks to the reviewer for pointing out the problem of data leakage in FB15K and WN18. We have conducted the experiments on the FB15K-237 and WN18RR instead. Please find the result in Table 3 in our latest version of the paper.\n\n(3) Thanks to the reviewers for pointing out more related works. According to [1] pointed out by the reviewer, although the authors include real-world noise in the Biological Knowledge Graph, it still introduces random noise to FB15k-237, which is the same as what we do. The major reason is that the real-world noise is unavailable for the benchmark Knowledge Graph dataset, including FB15k-237, YAGO3-10 and WN18RR. Some of the other related works also use the same strategy to introduce random noise, such as [7], [8], [9].\n\n(4) We apologize for the unclear claim. We agree that well trained KGE models are widely used for denoising when constructing a knowledge graph, such as [2] mentioned by the reviewer. However, in order to achieve a reliable well trained KGE model, the training data has to be clean. The major reason is that current KGE models highly rely on high-quality training data and thus are lack of robustness to noise [7]. Given the fact that a real knowledge graph will inevitably include many kinds of errors, such as ambiguous, conflicting and erroneous and redundant information, it is difficult for us to find an ideal clean dataset to train KGE models. To address this problem, in this paper, we proposed a novel technique to enable current embedding models to cope with noisy data.\n\n[1]  \u201cInterpretable Graph Convolutional Neural Networks for Inference on Noisy Knowledge Graphs.\u201d Workshop at NeurIPS\u201918.\n[2]  \u201cKnowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion.\u201d KDD\u201914.\n[3]  \u201cKbgan: Adversarial learning for knowledge graph embeddings.\u201d NAACL\u201918. \n[4]  \u201cRotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space.\u201d ICLR'19. \n[5]  \u201cEmbedding Entities and Relations for Learning and Inference in Knowledge Bases\u201d  ICLR'15. \n[6]  \u201cLearning Attention-based Embeddings for Relation Prediction in Knowledge Graphs\u201d, ACL\u201919.\n[7]  \u201cSparsity and noise:  Where knowledge graph embeddings fall short.\u201d ENMLP\u201917.\n[8]  \u201cDoes william shakespeare really write hamlet?  knowledge representation learning with confidence.\u201d AAAI\u201918.\n[9]  \u201cConfidence-aware negative sampling method for noisy knowledge graph embedding.\u201d ICBK\u201918.\n", "title": "Responds to Review #2"}, "rkxEN3dhsB": {"type": "rebuttal", "replyto": "HklhuRpxiH", "comment": "We thank the reviewer for the constructive reviews. We addressed the questions and concerns of the reviewer accordingly in the following.\n\n1 Using policy gradient to generate discrete data with GAN is first proposed by [7] and [2] and shows great performance in information retrieval in [2]. Afterward, this strategy has been widely adopted to learn graph representation, e.g., [1] and [3]. Following [1], [2], [3], [7],  we adopt the same strategy in our work. We agree that GAN is unstable and hard to train. Fortunately, in our work, it doesn\u2019t cause much trouble. To show that our model is stable and our results are easy to reproduce, we train NoiGAN-RotatE (soft) with the same parameters for 3 times on FB15K-237 with 70% noise and report the results on test dataset as follows:\n\nMRR  HITS@1  HITS@3  HITS@10\n0.279   0.179       0.320       0.475\n0.279   0.179       0.319       0.477\n0.279   0.179       0.319       0.474\n\nWe can observe that the results are almost the same, which shows the stability of our model.\n\n(2) To study the effect of the percentage of triples as positive training examples, we also run NoiGAN-RotatE (soft) with the percentage of triples as positive training examples as 10% 20% 40% on FB15K-237 with 70% noise, the result is as follows:\n\nPercentage of triples  MRR  HITS@1  HITS@3   HITS@10\n10%                               0.279   0.179       0.320         0.475\n20%                               0.278   0.179       0.318         0.473\n40%                               0.279   0.180       0.318         0.475\n\nWe can observe that the variation among the results is relatively small. It indicates that our NoiGAN is less sensitive to the percentage of triples as positive training examples. \n\n(3) Thanks to the reviewer for pointing out this issue. We have added more baseline methods, including (1) KGE models (e.g., DistMult [5] and RotatE [4]), (2) robust KGE models (e.g., attention based method [6]) and (3) KGE models with GAN (e.g., KBGAN [3]). In addition, to show that our NoiGAN can be easily generalized to various KGE models, RotatE is also added as score function for NoiGAN. Please find the results in Table 3 in our latest version of the paper. The results show that both NoiGAN-TransE and NoiGAN-RotatE consistently and significantly outperform all the baseline methods in terms of robustness. \n\n[1]  \u201cGraphGAN: Graph Representation Learning with Generative Adversarial Nets.\u201d AAAI'18. \n[2]  \u201cIrgan: A minimax game for unifying generative and discriminative information retrieval models.\u201d SIGIR'17. \n[3]  \u201cKbgan: Adversarial learning for knowledge graph embeddings.\u201d NAACL\u201918. \n[4]  \u201cRotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space.\u201d ICLR'19. \n[5]  \u201cEmbedding Entities and Relations for Learning and Inference in Knowledge Bases\u201d  ICLR'15. \n[6]  \u201cLearning Attention-based Embeddings for Relation Prediction in Knowledge Graphs\u201d, ACL\u201919.\n[7]  \u201cSeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\u201d, AAAI\u201917.\n", "title": "Responds to Review #4"}, "HklhuRpxiH": {"type": "review", "replyto": "rkgTdkrtPH", "review": "This paper presented a jointly learning framework based on GAN for tackling both knowledge graph completion and noise detection simultaneously. Existing works only deal with each of task independently and did not investigate the benefits of coping with both tasks together. The paper is well motivated. In order to achieve them, the paper presented a GAN framework in order to train a noising KG embedding as well the generator and discriminator. The key connections between two parts are through the confidence of a noise triple and generation of the negative sample triples. The whole framework looks quite interesting and promising. The experimental results are provided to validate the effectiveness of the proposed model. \n\nThere are two key concerns about this paper:\n\n1) It is well known that both GAN and RL are hard to train, not to mention combining them together to joint train in order to deal with data indifferenceability issue of discrete triple generation. Are the results easy to reproduce?\n\n2) Choosing 10% triples as positive training examples seems very ad-hoc. Have you studied the sensitivity of the number of percentage of triples as positive training examples on the system performance?\n\n3) I don't know too much about the methods from knowledge graph noise detection so maybe one baseline - CKRL is enough for representing state-of-the-arts. However, for knowledge graph completion task, TransE is most simple baseline and they are rich state-of-the-art methods in this line such as [1]. It is not convincing to show the advantages of the proposed NoiGAN without such comparisons. \n\n[1]  \u201cRotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space.\u201d ICLR'19. \n\n\n", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 2}, "S1l96w7HKH": {"type": "review", "replyto": "rkgTdkrtPH", "review": "This paper proposes to provide a novel noise-aware knowledge graph embedding (NoiGAN) by combining KG completion and noise detection through the GANs framework. More specifically, NoiGAN repeatedly utilizes a GAN model to 1) approximate the confidence score for facts identifying reliable data (discriminator) and 2) generate more challenging negative samples (generator). Then, it uses this confidence score and negative samples to train a more accurate link prediction model. The authors validate the proposed model through several experiments.\n\nThis paper reads well and the results appear sound. I personally find the idea of incorporating confidence score into a link prediction model to achieve a more accurate model very interesting. Furthermore, the provided experiments support their intuition and arguments outperforming considered baselines.\n\nAs for the drawbacks, I find the baselines considered in this work outdated missing many SOTA and related works in link prediction and noise detection [1,2, 3, 4, 5]. Further, I believe this work needs more experimental results and an ablation study capturing different aspects of the presented method. My concerns are as follows:\n\n\u2022\tConsidering the existing reverse relation issue in FB15K and WN18, I suggest conducting the experiments on the FB15K-237 and WN18RR from [6] instead. \n\u2022\tI suggest considering more recent link prediction models as baselines.\n\u2022\tI am wondering if the only difference between NoiGAN and KBGAN [7] is incorporating the confidence score in the link prediction loss?\n\u2022\tConsidering the fact that NoiGAN repeatedly retrains GAN and link prediction model, I suggest providing a comparison of computational complexity.\n\u2022\tI am wondering if NoiGAN can only work with pre-knowledge of noisy triples in KG? If not, why didn\u2019t you report NoiGAN performance with 0% noise in Table 3?\n\u2022\tI find utilizing few examples to evaluate the power of discriminator in distinguishing noisy triples (Table 4) not satisfactory at all. I suggest experimenting with more data and providing the per-relation breakdown performance of the discriminator.\n\nOn overall, although I find the proposed model quite novel and interesting, the paper needs more experimental results to validate the idea.\n \n[1] Pinter, Yuval, and Jacob Eisenstein. \"Predicting Semantic Relations using Global Graph Properties\".\n[2] Nathani, Deepak, et al. \"Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs\". \n[3] Bala\u017eevi\u0107, Ivana, Carl Allen, and Timothy M. Hospedales. \"TuckER: Tensor Factorization for Knowledge Graph Completion\".\n[4] Sun, Zhiqing, et al. \"Rotate: Knowledge graph embedding by relational rotation in complex space\".\n[5] Pezeshkpour, Pouya, Yifan Tian, and Sameer Singh. \"Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications\".\n[6] Dettmers, Tim, et al. \"Convolutional 2d knowledge graph embeddings.\", AAAI-18.\n[7] Liwei Cai and William Yang Wang. \u201cKbgan: Adversarial learning for knowledge graph embeddings\u201d. \n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 4}, "ryl29S9x5H": {"type": "review", "replyto": "rkgTdkrtPH", "review": "This paper proposes a GAN-oriented framework for training robust-to-noise neural link predictors. My main concern is that CKRL is the only baseline -- ignoring years of prior works in this space (see e.g. [1, 2]).\nFurthermore, [2] shows that two of the three datasets used by the authors suffer from test triple leakage in the training set.\n\nFinally, the considered datasets do not really test for the presence of noise - authors may want to check out e.g. https://arxiv.org/abs/1812.00279 (there are several works in this space, all of which were systematically ignored by this paper).\n\nFinally, authors claim neural link predictors were never used for denoising, but actually [3] use them to learn a prior distribution over triples in a probabilistic DB setting.\n\n\n[1] https://arxiv.org/abs/1806.07297\n[2] https://arxiv.org/abs/1707.01476\n[3] https://ai.google/research/pubs/pub45634", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 4}}}