{"paper": {"title": "Heterogeneous Bitwidth Binarization in Convolutional Neural Networks", "authors": ["Josh Fromm", "Matthai Philipose", "Shwetak Patel"], "authorids": ["jwfromm@uw.edu", "matthaip@microsoft.com", "shwetak@cs.washington.edu"], "summary": "We introduce fractional bitwidth approximation and show it has significant advantages.", "abstract": "Recent work has shown that performing inference with fast, very-low-bitwidth\n(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate\nresults. However, although 2-bit approximated networks have been shown to\nbe quite accurate, 1 bit approximations, which are twice as fast, have restrictively\nlow accuracy. We propose a method to train models whose weights are a mixture\nof bitwidths, that allows us to more finely tune the accuracy/speed trade-off. We\npresent the \u201cmiddle-out\u201d criterion for determining the bitwidth for each value, and\nshow how to integrate it into training models with a desired mixture of bitwidths.\nWe evaluate several architectures and binarization techniques on the ImageNet\ndataset. We show that our heterogeneous bitwidth approximation achieves superlinear\nscaling of accuracy with bitwidth. Using an average of only 1.4 bits, we are\nable to outperform state-of-the-art 2-bit architectures.", "keywords": ["Deep Learning", "Computer Vision", "Approximation"]}, "meta": {"decision": "Reject", "comment": "All of the reviewers find the approach interesting, but they have reservations regarding the practical impact and empirical evaluation. The paper needs improvement both on the motivation and on the experimental results by including more baseline methods and neural architectures.\n"}, "review": {"SkYPj5Hez": {"type": "review", "replyto": "HJDV5YxCW", "review": "This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase.\n\nThough this is an important direction to investigate, there are several issues:\n\n1. Comparison with previous results is misleading:\na.\t1-bit weights and floating point activations: Rastegari et al. got 56.8% accuracy on Alexnet, which is better than this paper 1.4bit result of 55.2%.\nb.\tHubara et al. got 51% results on 1-bit weights and 2-bit activations included also quantization first and last layer, in contrast to this paper. Therefore, it is not clear if there is a significant benefit in the proposed method which achieves 51.5% when decreasing the activation precision to 1.4bit. \n\nTherefore, it is not clear that the proposed methods improve over previous approaches.\n\n2. It is not clear to me: in which dimension of the tensors are we saving the scale factor? If it is per feature map, or neuron, this eliminates the main benefits of quantization: doing efficient binarized operations when doing Weight*activation during the forward pass?\n\n3. The review of the literature is inaccurate. For example, it is not true that Courbariaux et al. (2016) \u201cfurther improved accuracy on small datasets\u201d: the main novelty there was binarizing the activations (which typically decreased the accuracy). Also, it is not clear if the scale factors introduced by XNOR-Net indeed allowed \"a significant improvement over previous work\" in ImageNet (e.g., see DoReFA and Hubara et al. who got similar results using binarized weigths and activations on ImageNet without scale factors).  Lastly, the statement \u201cTypical approaches include linearly placing the quantization points\u201d is inaccurate: it was observed that logarithmic quantization works better in various cases. For example, see Miyashita, Lee and Murmann 2016, and Hubara et al.\n\n%%% After Author's Clarification %%%\nThis paper results seem more positive now, and I have therefore have increased my score, assuming the authors will revise the paper accordingly.\n\n", "title": "There are flaws in comparisons with previous works", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1Jn8QYeG": {"type": "review", "replyto": "HJDV5YxCW", "review": "The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights.\n\n1.  The paper misses some more recent reference, e.g. [a,b]. The author should also have a discussion on them.\n\n2. Indeed, AlexNet is a good seedbed to test binary methods. However, it is more interesting and important to test on more advanced networks. So, I wish to see a section on testing with Resnet and GoogleNet.\n\nIndeed, the authors have commented: \"AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.\" So, please show that.\n\n3. The paper wants to find a good trade-off on speed and accuracy. The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?\n\nMy concern is that one-bit system is already complicated to implement. Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice? One example is Section 4 in [Courbariaux et al. 2016].\n\n4. Is trade-off between 1 to 2 bits really important? \n\nCompared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1). Is such improvement really important?\n\nReference:\n[a]. Trained Ternary Quantization. ICLR 2017\n[b]. Extremely low bit neural network: Squeeze the last bit out with ADMM. arvix 2017", "title": "The improvement is not significant", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SklRZUJ-G": {"type": "review", "replyto": "HJDV5YxCW", "review": "This paper presents an extension of binary networks, and the main idea is to use different bit rates for different layers so we can further reduce bitrate of the overall net, and achieve better performance (speed / memory). The paper addresses a real problem which is meaningful, and provides interesting insights, but it is more of an extension.\n\nThe description of the Heterogeneous Bitwidth Binarization algorithm is interesting and simple, and potentially can be practical, However it also adds more complication to real world implementations, and might not be an elegant enough approach for practical usages. \n\nExperiments wise, the paper has done solid experiments comparing with existing approaches and showed the gain. Results are promising.\n\nOverall, I am leaning towards a rejection mostly due to limited novelty. \n\n", "title": "Review of Heterogeneous Bitwidth Binarization in Convolutional Neural Networks", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkrJS-_Qz": {"type": "rebuttal", "replyto": "H1iBLz8Xf", "comment": "We seem to be having a little bit of a terminology mismatch. In our work, all references to first layer mean the input layer, just as references to the last layer mean the output layer. We see now how this can cause confusion and will update our lingo to exclusively use input layer and output layer.\n\nWith this terminology mismatch in mind, our description of Hubara's work matches the implementation in his github repo (https://github.com/itayhubara/BinaryNet). The architectures in our work are comparable.\n\nHopefully clarifying this point allows more direct comparison to other works and highlights the significance of our results.", "title": "First Layer Vs Input Layer"}, "r1UdREX-M": {"type": "rebuttal", "replyto": "H162w9x-z", "comment": "Thanks for pointing us to this work. We weren\u2019t aware of it, and it looks interesting and related. More details below.\n\nOur two papers focus on different problems, but have quite a bit of potential to improve each other in future work. BitNet presents a method for learning the integer number of bits to binarize each layer in a network while our work presents a method for learning the number of bits for each individual parameter within a layer. In other words, BitNet is learning bitwidths with layer granularity while we're learning bitwidths with parameter level granularity.\n\nTo dig into this difference a little deeper, noting figure 1 in BitNet, we can see that during training the bitwidth of each layer is quite noisy due to the large discontinuity between bits. This is because prior to our work, there was no method to binarize a tensor to a fractional bitwidth. If the authors of BitNet were to incorporate Heterogeneous Bitwidth layers, the figure 1 of BitNet could be made smooth and continuous. This should help quite a bit with improving the stability of BitNet's learning process.\n\nBeyond the fundamental contribution of each paper, our work also differs by binarizing to lower bitwidths. The majority of layers in BitNet end up being binarized to around 6 bits, which is a huge difference in representational power than the 2 bit and below widths we focus on. Traditionally, showing good performance on 1-2 bits has been significantly harder than e.g. 4 bits and above.\n\nAdditionally, BitNet only focuses on the CIFAR and MNIST datasets. Many works have found that success on these datasets does not always transfer to more difficult datasets such as ImageNet. In contrast we provide a detailed analysis of results of ImageNet classification, including for the first time, a state-of-the-art model, Google\u2019s Mobilenet.", "title": "BitNet Comparison"}, "BJzQtmQbG": {"type": "rebuttal", "replyto": "SkYPj5Hez", "comment": "Reviewer 1 points out flaws in comparisons with related work.\n\n[Rastegari shows 56.8% accuracy, we only show 55.2% accuracy (row 4 of Table 1)] We measure the result of binarizing *all* layers of Alexnet (similar to Dong et al, from rows 1 and 2). Rastegari et al *do not binarize the first or last layer*. We consider Dong's result to be more challenging and chose to compare to it. However, we are happy to also compare to Rastegari's configuration if reviewers think it is misleading not to. Or we can just make this difference (currently noted in section 4.3) explicit in the table.\n\n[Hubara got 51% with 1-bit weights/2-bit activations when binarizing all layers, whereas we got 51.5% without binarizing first and last layer] We admit that this partly slipped by us. We chose our binarization configuration to compare to the many other pieces of work in Table 1, none of which (to our understanding) binarize first and last layers. Please note however, that Hubara uses 8-bit binarization for the first layer, which is arguably closer to \"no binarization\" than to the conventional 1- and 2-bit binarization. Further, other work (e.g. Tang et al AAAI 2017) has shown that binarizing the last layer, unlike the first, does not result in much accuracy loss. But we are happy to report Hubara's configuration if reviewers deem this as misleading.\n\n[Are scale factors stored per feature map or neuron?] No, they are stored per kernel exactly as in Rastegari et al. Scale factor multiplication can be done after the binary product by multiplying each output feature map by it's corresponding scalar. The amount of added work is equivalent to replacing ReLU activations with PReLU activations, which does not have a significant effect on network inference time. We can make this point more explicit in the Implementation section.\n\n[Misunderstandings of key innovations and contributions in related work] We are embarrassed at our misunderstanding of the literature. We thank the reviewer and will correct these and improve our understanding.", "title": "Reviewer 1 Response"}, "rkiavXQ-z": {"type": "rebuttal", "replyto": "H1Jn8QYeG", "comment": " The reviewer questions whether the performance improvements we claim are significant. In detail:\n\n[Comparison to related work] We thank the reviewer for these references. Reference A, on ternary networks, is quite similar to the work of Li et al that we reference in related work, but we will include further discussion. Reference B suggests training improvements (not binarization techniques), which we will look to adopt.\n\n[Evaluation in larger models: please show results on Resnet/GoogleNet] We selected AlexNet to illustrate most of the work, since it has been used exclusively in the community to compare approaches. However, do note that **unlike any paper so far**, we have shown results also on Mobilenet, which is the state-of-the art object recognition model as of fall 2017, from Google (contribution 4 in the intro, and last paragraph of section 4.3). Mobilenet yields comparable accuracy to Resnet and GoogleNet, but is also much faster than them, and is therefore a challenging benchmark. Perhaps we should highlight this result better?\n\n[Complexity of implementation] Gaining performance from binarized models is indeed complex, especially on a CPU. In fact, no paper provides the many crucial details, such as machine specific vectorization/tiling/loop fusion algorithms essential to gaining real-world speedup. Even Courbariaux et al, section 4, only gives a sketch in this direction. Admittedly, heterogeneous bitwidths will add to this complexity, so this is a fair concern on a CPU. However, implementation is fairly straightforward on an FPGA, because we simply lay out custom gate patterns for each bit pattern to be XNOR'd against (see e.g. https://arxiv.org/pdf/1612.07119.pdf , esp. section 4.3.2 for a similar implementation in the homogeneous bitwidth case). The custom pattern for processing 2 bits is only slightly different from the 1-bit version. Perhaps we can focus the implementation section on sketch how to perturb this standard FPGA-based design?\n\n[No speed vs accuracy number] As mentioned above, almost no paper in this area reports measured speedups, just improvements in coarsely estimated instruction counts. In the FPGA context, we could similarly report coarse estimates the number of cycles, chip real estate and power consumed. However, roughly speaking these (especially the latter two that are our goal) are simply proportional to average bitwidth of the operations programmable into hardware. We could make this explicit in the text when we discuss the implementation above.\n\n[A 1.4/2x = 0.7x reduction is not significant] Although this is a subjective call and hard to argue against, it is worth noting that our gains are *on top of* optimized binary implementations. Further, note that FPGA implementations of DNNs are now running at cloud scale e.g. in the Azure cloud (https://www.microsoft.com/en-us/research/blog/microsoft-unveils-project-brainwave/). A 30% improvement in space/energy efficiency with no accuracy loss is considered quite significant at these scales.", "title": "Reviewer 2 Response"}, "rkNfvQQbG": {"type": "rebuttal", "replyto": "SklRZUJ-G", "comment": "The reviewer describes our work as seeking to use different bit rates for different layers, and points out that the work is not novel enough overall.\n\nWe would like to point out that in fact, we are not looking to simply binarize different layers at different bitwidths (although as a baseline we do so in figure 3(a)). In that baseline, assuming we select *up front* what the bitwidth k of each layer is, we use the not-so-novel approach of simply applying standard k-bit binarization algorithms with different k to each layer. This is indeed simple to do, but figure 3(a) shows that such naive selection only provides \"linear\" increase in accuracy (e.g., using 1.5 bits on average gives only the average of 1-bit and 2-bit accuracies, which is interesting but perhaps not surprising). \n\nInstead, in our main contribution, we are asking the question \"if we *learned* what bitwidth to assign to *each* parameter (jointly with its value), could we get better-than-linear speedup\". This learning of  bitwidths  is what is novel about our goal, and techniques.\n\nLearning bitwidths requires changing the training algorithm in a non-obvious way, using the mask-generation scheme of algorithm 1, and the middle-out scheme for thresholding. We should emphasize that the operations of equation 4, Algorithm 1 and equation 6 are not performed in a one-time \"post-processing\" step, but on every forward propagation during training. We submit that this learning algorithm is quite novel.\n\nGiven that the bitwidth is such a fundamental aspect of model parameters, we hope that learning them jointly with values should be of broad interest to the ICLR community.", "title": "Reviewer 3 Response"}, "Hy2OIQXWf": {"type": "rebuttal", "replyto": "HJDV5YxCW", "comment": " We thank the reviewers for their detailed and useful feedback. Below, we make two summary points in response and follow up with some detailed responses to the issues raised.\n\nAt the highest level,  we realize from the feedback we did not make the motivation and the technical significance of our work sufficiently clear. \n\nFirst, the motivation. We are looking to adapt binarization algorithms for implementation on FPGAs. On standard FPGA-based implementations (e.g., https://arxiv.org/pdf/1612.07119.pdf ), every XNOR product is implemented as a separate hardware structure proportional to the number of bits in the product. Typically, both the real estate (number of gates) and power consumed (watts) by an implementation are proportional to the average bitwidth of the data. A (2 to 1.4 =) 30% average reduction in real estate and power **at no accuracy loss** is quite attractive. We intend to rework our introduction and implementation sections to highlight this perspective.\n\nSecond, the significance. Traditionally, ML algorithms look to optimize/learn the *value* of every parameter. We extend the optimization criterion in a simple, but fundamental, way to include the representation. We ask whether it is (a) feasible and (b) useful to *jointly* learn both the value *and the bitwdith* of every parameter.  We provide an affirmative answer on both counts, and to our knowledge, are the first to do so. This advance requires a non-obvious change to the training scheme (i.e., the middle-out scheme to select a variable-bitwidth mask, Algorithm 1). We also show experimentally (sec 4.2.1 and Figure 3a) that a simpler approach that does not pick the bitwidths in a data driven manner does not give the same bump in performance. Again, we will reframe our paper to highlight this.\n\nDetailed responses are provided below to each reviewer.", "title": "Rebuttal Summary"}}}