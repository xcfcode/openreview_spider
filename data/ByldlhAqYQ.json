{"paper": {"title": "Transfer Learning for Sequences via Learning to Collocate", "authors": ["Wanyun Cui", "Guangyu Zheng", "Zhiqiang Shen", "Sihang Jiang", "Wei Wang"], "authorids": ["cui.wanyun@sufe.edu.cn", "simonzgy@outlook.com", "shen54@illinois.edu", "tedjiangfdu@gmail.com", "weiwang1@fudan.edu.cn"], "summary": "Transfer learning for sequence via learning to align cell-level information across domains.", "abstract": "Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain. Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer. RNN uses a chain of repeating cells to model the sequence data. However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling. Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain.\n\nIn this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer. ART is in a recurrent manner that different cells share the same parameters. Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain. This strategy enables ART to capture the word collocation across domains in a more flexible way. We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis). ART outperforms the state-of-the-arts over all experiments.\n", "keywords": ["transfer learning", "recurrent neural network", "attention", "natural language processing"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents a method for transferring source information via the hidden states of recurrent networks.  The transfer happens via an attention mechanism that operates between the target and the source.  Results on two tasks are strong.\n\nI found this paper similar in spirit to Hypernetworks (David Ha, Andrew Dai, Quoc V Le, ICLR 2016) since there too there is a dynamic weight generation for network given another network, although this method did not use an attention mechanism.\n\nHowever, reviewers thought that there is merit in this paper (albeit pointed the authors to other related work) and the empirical results are solid.\n"}, "review": {"rJgMuxuieN": {"type": "rebuttal", "replyto": "S1gw3tDil4", "comment": "Thanks for your concerns.\n\nYour first concern is already addressed by AnonReviewer3. Please refer to our response to AnonReviewer3 about minimally supervised domain adaptation.\n\nFor your second concern, we will try to fine-tune the hyperparameters and see if it changes a lot before the camera ready. We will also release our source code and datasets later.", "title": "Response"}, "ryg9EAIjx4": {"type": "rebuttal", "replyto": "BkxHEW8jl4", "comment": "Hi Zheng, let's try to make it clearer and reach an accurate agreement for HATN.\n\nAs described in the response above, we used the source code and keeped hyper parameters in https://github.com/hsqmlzno1/HATN. More specifically, we initialize HATN by 300d skip-gram vectors. The dimensions of the word attention layer and the sentence attention layer are both 300. We train the model with batch_size=50, learning rate=1e-4. We use the same early-stopping policy. Besides, we use the same unlabeled data provided from https://github.com/hsqmlzno1/HATN. These settings are all from your github repository.\n\nPlease provide more details of your implementation and you results. We will consider updating the results in the camera ready version if we find the results change a lot in your settings. ", "title": "detailed experimental settings for HATN"}, "rJgs-aD6Am": {"type": "rebuttal", "replyto": "ByeMKYAtCX", "comment": "For your detailed writing advices, we have rewritten the two sentences accordingly.\n\n1.\tWe rewrote the sentence \n\u201cART discriminates between information of the corresponding position and that of all positions with collocated words.\u201d \nto \n\u201cFor each word in the target domain, ART learns to incorporate two types of information from the source domain: (a) the hidden state corresponding to the same word, and (b) the hidden states for all words in the sequence.\u201d\n\n2.\tWe rewrote the sentence \n\u201cBy using the attention mechanism (Bahdanau et al., 2015), we compute the correlation for each word pair.\u201d \nto \n\u201cART learns to incorporate information (b) based on the attention scores (Bahdanau et al., 2015) of all words from the source domain.\u201d\n\nFor more writing improvements, please refer to the previous comment or the paper.", "title": "Detailed rewritings"}, "SyeoJ4gjR7": {"type": "rebuttal", "replyto": "Skx0jUh90Q", "comment": "Thank you for your response. \n\n== Writing ==\nWe agree that there is room for writing of the original submission. We have been improving the writing quality. We believe that the latest version is much clearer now.\n\nWe made the following revisions to improve the writing:\n1. We gave more descriptions of how ART works.\ni. [Learn to Collocate and Transfer] In section 1, we rewrote paragraph of \u201clearn to collocate and transfer\u201d. We highlighted how ART incorporates two types of information and uses the attention mechanism to capture the long-term cross-domain dependency.\nii. [Architecture] In section 2, we added a paragraph to describe the architecture of ART. We elaborated how it incorporates the information of the source domain from the pre-trained model.\niii. [Model training] In section 2, we rewrote the paragraph of model training. We highlighted the model pre-training procedure and fine-tuning procedure of ART.\n2. We added the interpretations and examples for some confusing notions, such as \u201clevel-wise transfer learning\u201d, \u201ccell-level transfer learning\u201d, and \u201ccollocate\u201d.\n3. We abandoned or reduced some vague words or phrases, such as \u201cword correlation\u201d, \u201ccollocate\u201d. The revised version uses more precise expressions, such as \u201cdependencies between two words\u201d, \u201cincorporate information by their attention score\u201d.\n4. We rewrote the related work section. We compared ART with BERT and ELMo. The latter two approaches also use pre-trained models for downstream tasks.\n5. We fixed some typos.\n\n== Minimally Supervised Domain Adaptation ==\n1. For merging three domains as one source domain, we try to evaluate the effectiveness of ART when the source domain corpus is rich and the target domain corpus is scarce. We merge the rest three domains to enrich the source domain corpus. We do not differentiate samples from the three domains, which is different from standard multi-source domain adaptation. \n2. We will highlight that we use annotated target data, which is different from some baselines. The annotated target data is also used by HRN in Table 5 and other pre-training approaches.", "title": "Writing and Minimally Supervised Domain Adaptation"}, "S1g0uXesAX": {"type": "rebuttal", "replyto": "ByeMKYAtCX", "comment": "Thank you for your encouraging comments.\nWe agree that there is room for writing of the original submission. We have been improving the writing quality. We believe that the latest version is much clearer now.\n\nWe made the following revisions to improve the writing:\n1. We gave more descriptions of how ART works.\ni. [Learn to Collocate and Transfer] In section 1, we rewrote paragraph of \u201clearn to collocate and transfer\u201d. We highlighted how ART incorporates two types of information and uses the attention mechanism to capture the long-term cross-domain dependency.\nii. [Architecture] In section 2, we added a paragraph to describe the architecture of ART. We elaborated how it incorporates the information of the source domain from the pre-trained model.\niii. [Model training] In section 2, we rewrote the paragraph of model training. We highlighted the model pre-training procedure and fine-tuning procedure of ART.\n2. We added the interpretations and examples for some confusing notions, such as \u201clevel-wise transfer learning\u201d, \u201ccell-level transfer learning\u201d, and \u201ccollocate\u201d.\n3. We abandoned or reduced some vague words or phrases, such as \u201cword correlation\u201d, \u201ccollocate\u201d. The revised version uses more precise expressions, such as \u201cdependencies between two words\u201d, \u201cincorporate information by their attention score\u201d.\n4. We rewrote the related work section. We compared ART with BERT and ELMo. The latter two approaches also use pre-trained models for downstream tasks.\n5. We fixed some typos.", "title": "Writing"}, "r1xwngaY3Q": {"type": "review", "replyto": "ByldlhAqYQ", "review": "The proposed method is suitable for many NLP tasks, since it can handle the sequence data.\n\nI find it difficult to follow through the model descriptions.  Perhaps a more descriptive figures would make this easier to follow, I feel that the ART model is a very strait forward and it can be easily described in much simpler and less exhausting (sorry for the strong word) way, while there is nothing wrong with being as elaborating as you are, I feel that all those details belong in an appendix. \nCan you please explain the exact learning process?\nI didn\u2019t fully understand the exact way of collocations, you first train on the source domain and then use the trained source network when training in the target domain with all the collocated words for each training example? I deeply encourage you to improve the model section for future readers. \nIn contrast to the model section, the related work and the experimental settings sections are very thin.\nThe experimental setup for the sentiment analysis experiments is quite rare in the transfer learning/domain adaptation landscape, having equal amount of labeled data from both source and target domains is not very realistic in my humble opinion.\nMore realistic setup is unsupervised domain adaptation (like in DANN and MSDA-DAN papers) or minimally supervised domain adaptation (like you did in your POS and NER experiments).\n\nIn addition to the LSTM baseline (which is trained with target data only), I think that LSTM which is trained on both source and target domains data is required for truly understand ART gains \u2013 this goes for the POS and NER tasks as well.\nThe POS and NER experiments can use some additional baselines for further comparison, for example:\nhttp://www.aclweb.org/anthology/Q14-1002\nhttps://hornhehhf.github.io/hangfenghe/papers/14484-66685-1-PB.pdf\n\nI am not sure I understand the \u201ccell level transfer\u201d claim, did you mean that you are the first to apply inner LSTM/RNN cell transfer or that you are the first ones to apply word-level fine grained transfer, the latter has already been done:\nhttps://arxiv.org/pdf/1802.05365.pdf\nhttps://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=4531&context=sis_research\nhttp://www.aclweb.org/anthology/N18-1112\nhttps://openreview.net/pdf?id=rk9eAFcxg\n", "title": "The paper proposed to use RNN/LSTM with collocation alignment as a representation learning method for transfer learning/domain adaptation in NLP.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SklbRJ0_37": {"type": "review", "replyto": "ByldlhAqYQ", "review": "This paper presents the following approach to domain adaptation. Train a source domain RNN. While doing inference on the target domain, first you run the source domain RNN on the sequence. Then while running the target domain RNN, set the hidden state at time step i, h^t_i, to be a function 'f' of  h^t_{i-1} and information from source domain \\psi_i; \\psi_i is computed as a convex combination of the state of the source domain RNN, h^s_{i}, and an attention-weighted average of all the states h^s{1...n}. So in effect, the paper transfers information from each of source domain cells -- the cell at time step i and all the \"collocated\" cells (collocation being defined in terms of attention). This idea is then extended in a straightforward way to LSTMs as well. \n \nDoing \"cell-level\" transfer enables more information to be transferred according to the authors, but it comes at a higher computation since we need to do O(n^2) computations for each cell.\n\nThe authors show that this beats a variety of baselines for classification tasks (sentiment), and for sequence tagging task (POS tagging over twitter.)\n\nPros:\n1. The idea makes sense and the experimental results show solid \n\nCons:\n1. Some questions around generalization are not clearly answered. E.g. how are the transfer parameters of function 'f' (that controls how much source information is transferred to target) trained? If the function 'f' and the target RNN is trained on target data, why does 'f' not overfit to only selecting information from the target domain? Would something like dropping information from target domain help?\n\n2. Why not also compare with a simple algorithm of transferring parameters from source to target domain? Another simple baseline is to just train final prediction function (softmax or sigmoid) on the concatenated source and target hidden states. Why are these not compared with? Also, including the performance of simple baselines like word2vec/bow is always a good idea, especially on the sentiment data which is very commonly used and widely cited. \n\n3. Experiments: the authors cite the hierarchical attention transfer work of Li et al (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16873/16149) and claim their approach is better, but do not compare with them in the experiments. Why?\n\nWriting:\nThe writing is quite confusing at places and is the biggest problem with this paper. E.g.\n\n1. The authors use the word \"collocated\" everywhere, but it is not clear at all what they mean. This makes the introduction quite confusing to understand. I assumed it to mean words in the target sentences that are strongly attended to. Is this correct? However, on page 4, they claim \"The model needs to be evaluated O(n^2) times for each sentence pair.\" -- what is meant by sentence pair here? It almost leads me to think that they consider all source sentence and target sentences? This is quite confusing. \n\n2. The authors keep claiming that \"layer-wise transfer learning mechanisms lose the fine-grained cell-level information from the source domain\", but it is not clear exactly what do they mean by layer-wise here. Do they mean transferring the information from source cell i to target cell i as it is? In the experiments section on LWT, the authors claim that \"More specifically, only the last cell of the RNN layer transfers information. This cell works as in ART. LWT only works for sentence classification.\" Why is it not possible to train a softmax over both the source hidden state and the target hidden state for POS tagging? \n\nnits:\npage 4 line 1: \"i'th cell in the source domain\" -> \"i'th cell in the target domain\". \"j'th cell in target\" -> \"j'th cell in sourcE\".\n\n\nRevised: increased score after author response.\n", "title": "Reasonable idea but the technical details are quite unclear", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkxdELsKRm": {"type": "rebuttal", "replyto": "r1xwngaY3Q", "comment": "Thank you for your insightful and supportive comments. We have made the following revisions: (1) We added two baselines according to your comments. The results further justify the effectiveness of ART. (2) We added a new experiment for minimally supervised domain adaptation in Table 3. ART still outperforms all the competitors by a large margin. (3) We clarified the ART model and model training process in the revised paper. We will give more details below:\n\n== Writing ==\n1. High level description of the ART model. \nWe have added the following description of ART model in section 2.\n\u201cThe source domain and the target domain share an RNN layer, from which the common information is transferred. We pre-train the neural network of the source domain. Therefore the shared RNN layer represents the semantics of the source domain. The target domain has an additional RNN layer. Each cell in it accepts transferred information through the shared RNN layer. Such information consists of (1) the information of the same word in the source domain (the red edge in figure 2); and (2) the information of all its collocated words (the blue edges in figure 2). ART uses attention to decide the weights of all candidate collocations. The RNN cell controls the weights between (1) and (2) by an update gate.\u201d\n\n2. Model training. We add more details of the model training part in section 2.\nWe first pre-train the parameters of the source domain by its training samples. Then we fine-tune the pre-trained model with additional layers of the target domain. The fine-tuning uses the training samples of the target domain. All parameters are jointly fine-tuned.\n\n3. Related work.\nWe have rewritten the related work section. We compare with other cell-level transfer learning approaches and pre-trained models.\n\n== Innovation of cell-level transfer ==\nWe agree that some previous transfer learning approaches also consider cell-level transfer. But none of them considers the word collocations. As a pre-trained model, ELMo uses bidirectional LSTMs to generate contextual features. Instead, ART uses attention mechanism in RNN that each cell in the target domain directly access information of all cells in the source domain. We added more details in the related work section.\n\n== Baselines ==\nWe added two baselines, LSTM-u and FLORS, according to your comments. LSTM-u uses a standard LSTM and is trained by the union data of the source and the target domain. FLORS is a domain adaptation model for POS tagging (http://www.aclweb.org/anthology/Q14-1002). Their results are shown in Table 2 and Table 5. ART outperforms LSTM-u in almost all settings by a large margin. Note that FLORS is independent of the target domain. If the training corpus of the target domain is quite rare (Twitter/0.01), FLORS performs better. But with richer training data of the target domain (Twitter/0.1), ART outperforms FLORS by a large margin.\n\nTable 2: Classi\ufb01cation accuracy on the Amazon review dataset.\nSource\t\tTarget\t\tLSTM-u\tART\nBooks\t\tDVD\t\t0.770 \t0.870 \nBooks\t\tElectronics\t0.805 \t0.848 \nBooks\t\tKitchen\t\t0.845 \t0.863 \nDVD\t\tBooks\t\t0.788 \t0.855 \nDVD\t\tElectronics\t0.788 \t0.845 \nDVD\t\tKitchen\t\t0.823 \t0.853 \nElectronics\tBooks\t\t0.740 \t0.868 \nElectronics\tDVD\t\t0.753 \t0.855 \nElectronics\tKitchen\t\t0.863 \t0.890 \nKitchen\t\tBooks\t\t0.760 \t0.845 \nKitchen\t\tDVD\t\t0.758 \t0.858 \nKitchen\t\tElectronics\t0.815 \t0.853 \n        Average\t\t\t\t0.792 \t0.858\n\nTable 5: Performance over POS tagging.\nTask\t\t\tSource\tTarget\t\tFLORS\tART\nPOS Tagging\t        PTB\t\tTwitter/0.1\t0.763\t0.859\nPOS Tagging\t        PTB\t\tTwitter/0.01\t0.763\t0.658\n\n== Experimental settings ==\nBased on your comment, we added a new experiment for minimally supervised domain adaptation in sentence classification. For each target domain in the Amazon review dataset, we combined the training/development data of rest three domains as the source domain. We show the results in Table 3. ART outperforms the competitors by a large margin. This verifies its effectiveness in the setting of minimally supervised domain adaptation.\n\nTable 3: Classification accuracy with scarce training samples of the target domain.\nTarget\t\tLSTM\tLSTM-u\tCCT\t\tLWT\tHATN\tART\nBooks\t\t0.745 \t0.813 \t0.848 \t0.808 \t0.820 \t0.895 \nDVD\t\t0.695 \t0.748 \t0.870 \t0.770 \t0.828 \t0.875 \nElectronics\t0.733 \t0.823 \t0.848 \t0.818 \t0.863 \t0.865 \nKitchen\t\t0.798 \t0.840 \t0.860 \t0.840 \t0.833 \t0.870 \nAverage\t\t0.743 \t0.806 \t0.856 \t0.809 \t0.836 \t0.876", "title": "Response to AnonReviewer3 [new baselines, experimental settings, and clarifications]"}, "H1ltRziK0X": {"type": "rebuttal", "replyto": "SklbRJ0_37", "comment": "Thank you for your insightful and supportive comments. We have made the following revisions: (1) We added two baselines according to your comments. The results further justify the effectiveness of ART. (2) We clarified \u201ccollocate\u201d, \u201clayer-wise transfer learning\u201d, \u201cmodel training\u201d, and their related issues. We give more details below:\n\n1. Regarding computational cost:\nThe network depth only increases by 2 if we ignore the detailed operations (e.g. gates). One is caused by collocating and transferring. Another one is caused by merging the original input, the previous cell\u2019s hidden state, and the transferred information. So the time cost does not increase much.\n\n2. Regarding Con1: why does 'f' not overfit to only selecting information from the target domain? \nYour understanding is correct. Function 'f' will overfit to the target domain. All parameters will be jointly fine-tuned by the training samples of the target domain. Nevertheless, the pre-training for the source domain still helps because it provides representations of the source domain. Another recent successful example of using pre-trained models is BERT (Devlin et al., 2018), which also fine-tunes all the parameters to specific tasks. \nAnd we rewrite the model training part in section 2 to make it clearer.\n\u201cWe first pre-train the parameters of the source domain by its training samples. Then we fine-tune the pre-trained model with additional layers of the target domain. The fine-tuning uses the training samples of the target domain. All parameters are jointly fine-tuned.\u201d\n\nRegarding Con2. More simple baselines.\nFirst, we added a baseline model, LSTM-s, which directly uses parameters from the source domain to the target domain. The results are shown in Table 2. ART outperforms the baseline by a large margin.\n\nTable 2: Classi\ufb01cation accuracy on the Amazon review dataset.\nSource\t\tTarget\t\tLSTM-s\tHATN\tART\nBooks\t\tDVD\t\t0.718\t0.813\t0.870\nBooks\t\tElectronics\t0.678\t0.790\t0.848\nBooks\t\tKitchen\t\t0.678\t0.738\t0.863\nDVD\t\tBooks\t\t0.730\t0.798\t0.855\nDVD\t\tElectronics\t0.663\t0.805\t0.845\nDVD\t\tKitchen\t\t0.708\t0.765\t0.853\nElectronics\tBooks\t\t0.648\t0.763\t0.868\nElectronics\tDVD\t\t0.648\t0.788\t0.855\nElectronics\tKitchen\t\t0.785\t0.808\t0.890\nKitchen\t\tBooks\t\t0.653\t0.740\t0.845\nKitchen\t\tDVD\t\t0.678\t0.738\t0.858\nKitchen\t\tElectronics\t0.758\t0.850\t0.853\n        Average\t\t\t\t0.695\t0.783\t0.858\n\nSecond, you suggest directly concatenating the hidden states of the source and the target domains. In fact, we already proposed a very similar baseline CCT. The only difference is that CCT uses a gate to merge the two values, instead of concatenation. ART outperforms CCT in all cases.\nThird, we already used 100d GloVe vectors to initialize ART and all its ablations we proposed in this paper. The pre-trained word embeddings are also widely used by its competitors (e.g. AMN and HATN). We have added the description in section 4.\n\n\nRegarding Con 3. Experiments: the hierarchical attention transfer work of Li et al.\nWe added the comparison with HATN (Li et al 2018). The results are shown in Table 2 and Table 3. We use the source code and hyper parameters of (Li et al 2018) from the authors\u2019 Github. We changed its labeled training samples from 5600 to 1400 as with ART.\n\nThe results are shown in Table 2 above. ART still beats the baseline by a large margin. This verifies its effectiveness.\n\n== Writing ==\nRegarding Writing1.\nFirst, for the meaning of \u201ccollocate\u201d, we added more explanations and take figure 1 as an example in section 1. \n\u201cHere \u201ccollocate\u201d indicates that a word's semantics can have long-term dependency on other words. To understand a word in the target domain, we need to precisely represent its collocated words from the source domain. We learn from the collocated words via the attention mechanism. For example, in figure 1, \u201chate\u201d is modified by the adverb \u201csometimes\u201d, which implies the act of hating is not serious. But the \u201csometimes\u201d in the target domain is trained insufficiently. We need to transfer the semantics of \u201csometimes\u201d in the source domain to understand the implication.\u201d\nSecond, to avoid the ambiguity of \u201csentence pair\u201d, we rewrote the description in the revised version.\n\u201cThe model needs to be evaluated O(n^2) times for each sentence, due to the enumeration of n indexes for the source domain and n indexes for the target domain. Here n denotes the sentence length.\u201d\n\nRegarding Writing2.\n\u201cLayer-wise transfer learning\u201d indicates that the approach represents the whole sentence by a single vector. So the transfer mechanism is only applied to the vector. We cannot apply layer-wise transfer learning algorithms to sequence labeling tasks.\nWe added the descriptions in section 1. ", "title": "Response to AnonReviewer2 [new baselines and clarifications]"}, "rJeHRZoYC7": {"type": "rebuttal", "replyto": "Hye7v16q3m", "comment": "Thank you for your insightful and supportive comments. We have made the following revisions: (1) We added two baselines based on your comments. The results further justified the effectiveness of ART. (2) We added the clarification of \u201clayer-wise transfer learning\u201d, \u201ccell-level transfer learning\u201d, and \u201ccollocate\u201d in section 1. We will give more details below:\n\n==Experiments==\nWe added two baselines, LSTM-u and HATN, according to your comments. LSTM-u uses a standard LSTM and is trained by the union data of the source domain and the target domain. The HATN model is from the paper \"Hierarchical Attention Transfer Network for Cross-domain Sentiment Classification\" (Li et al 2018). We use the source code and hyper parameters of (Li et al 2018) from the authors\u2019 Github. We changed its labeled training samples from 5600 to 1400 as with ART.\n\nThe results are shown in Table 2. ART still beats the baselines by a large margin. This verifies its effectiveness.\n\nTable 2: Classi\ufb01cation accuracy on the Amazon review dataset.\nSource\t\tTarget\t\tLSTM-u\tHATN\tART\nBooks\t\tDVD\t\t0.770\t0.813\t0.870\nBooks\t\tElectronics\t0.805\t0.790\t0.848\nBooks\t    \tKitchen\t\t0.845\t0.738\t0.863\nDVD \t        Books\t\t0.788\t0.798\t0.855\nDVD\t        Electronics\t0.788\t0.805\t0.845\nDVD\t        Kitchen\t\t0.823\t0.765\t0.853\nElectronics    \tBooks\t\t0.740\t0.763\t0.868\nElectronics\tDVD\t\t0.753\t0.788\t0.855\nElectronics\tKitchen\t\t0.863\t0.808\t0.890\nKitchen\t\tBooks\t\t0.760\t0.740\t0.845\nKitchen\t\tDVD\t\t0.758\t0.738\t0.858\nKitchen\t\tElectronics\t0.815\t0.850\t0.853\n         Average\t\t\t0.792\t0.783\t0.858\n\n\n== Writing==\nWe added more detailed explanations and took figure 1 as an example to clarify the confusing parts in section 1.\n\n1. Layer-wise transfer learning: \n\u201cLayer-wise transfer learning\u201d indicates that the approach represents the whole sentence by a single vector. So the transfer mechanism is only applied to the vector. \n\n2. Cell-level transfer learning:\nART uses cell-level information transfer, which means each cell is affected by the transferred information.  For example, in figure 1, the state of \u201chate\u201d in the target domain is affected by \u201csometimes\u201d and \u201dhate\u201d in the source domain. \n\n3. Collocate: \nWe use the term \u201ccollocate\u201d to indicate that a word's semantics can have long-term dependency on another word. To understand a word in the target domain, we need to precisely capture and represent its collocated words from the source domain. We learn from the collocated words via the attention mechanism. For example, in figure 1, \u201chate\u201d is modified by the adverb \u201csometimes\u201d, which implies the act of hating is not serious. But \u201csometimes\u201d in the target domain is trained insufficiently. We need to transfer the semantics of \u201csometimes\u201d.", "title": "Response to AnonReviewer1 [new baselines and clarifications]"}, "Hye7v16q3m": {"type": "review", "replyto": "ByldlhAqYQ", "review": "== Quality of results ==\nThis paper's empirical results are its main strength. They evaluate on a well-known benchmark for transfer learning in text classification (the Amazon reviews dataset of Blitzer et al 2007), and improve by a significant margin over recent state-of-the-art methods. They also evaluate on several sequence tagging tasks and achieve good results.\n\nOne weakness of the empirical results is that they do not compare against training a model on the union of the source and target domain. I think this is very important to compare against.\n\nNote: the authors cite a paper in the introduction \"Hierarchical Attention Transfer Network for Cross-domain Sentiment\nClassification\" (Li et al 2018) which also achieves state of the art results on the Amazon reviews dataset, but do not compare against it. At first glance, Li et al 2018 appear to get better results. However, they appear to be training on a larger amount of data for each domain (5600 examples, rather than 1400). It is unclear to me why their evaluation setup is different, but some clarification about this would be helpful.\n\n== Originality ==\nA high level description of their approach:\n1. Train an RNN encoder (\"source domain encoder\") on the source domain\n2. On the target domain, encode text using the following strategy:\n  - First, encode the text using the source domain encoder\n  - Then, encode the text using a new encoder (a \"target domain encoder\") which has the ability to attend over the hidden states of the source domain encoder at each time step of encoding.\n\nThey also structure the target domain encoder such that at each time step, it has a bias toward attending to the hidden state in the source encoder at the same position.\n\nThis has a similar flavor to greedy layer-wise training and model stacking approaches. In that regard, the idea is not brand new, but feels well-applied in this setting.\n\n== Clarity ==\nI felt that the paper could have been written more clearly. The authors set up a comparison between \"transfer information across the whole layers\" vs \"transfer information from each cell\" in both the abstract and the intro, but it was unclear what this distinction was referring to until I reached Section 4.1 and saw the definition of Layer-Wise Transfer.\n\nThroughout the abstract and intro, it was also unclear what was meant by \"learning to collocate cross domain words\". After reading the full approach, I see now that this simply refers to the attention mechanism which attends over the hidden states of the source domain encoder.\n\n== Summary ==\nThis paper has good empirical results, but I would really like to see a comparison against training a model on the union of the source and target domain. I think superior results against that baseline would increase my rating for this paper.\n\nI think the paper's main weakness is that the abstract and intro are written in a way that is somewhat confusing, due to the use of unconventional terminology that could be replaced with simpler terms.", "title": "Good empirical results on transfer learning; writing could be clearer", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}