{"paper": {"title": "Batch Policy Gradient  Methods for  Improving Neural Conversation Models", "authors": ["Kirthevasan Kandasamy", "Yoram Bachrach", "Ryota Tomioka", "Daniel Tarlow", "David Carter"], "authorids": ["kandasamy@cmu.edu", "yorambac@gmail.com", "ryoto@microsoft.com", "dtarlow@microsoft.com", "dacart@microsoft.com"], "summary": "", "abstract": "We study reinforcement learning of chat-bots with recurrent neural network\narchitectures when the rewards are noisy and expensive to\nobtain. For instance, a chat-bot used in automated customer service support can\nbe scored by quality assurance agents, but this process can be expensive, time consuming\nand noisy. \nPrevious reinforcement learning work for natural language uses on-policy updates\nand/or is designed for on-line learning settings.\nWe demonstrate empirically that such strategies are not appropriate for this setting\nand develop an off-policy batch policy gradient method (\\bpg).\nWe demonstrate the efficacy of our method via a series of\nsynthetic experiments and an Amazon Mechanical Turk experiment on\na restaurant recommendations dataset.\n\n", "keywords": ["Natural language processing", "Reinforcement Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy. \n The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR."}, "review": {"SJ59aO_rl": {"type": "rebuttal", "replyto": "rJfMusFll", "comment": "Once again, we'd like to thank all reviewers for the feedback. We have updated the manuscript accordingly. The changes are done in magenta so that it would be easier to identify them.\n\n@Reviewer2:\ns' is defined in the first para of section 2.2. Also, the expectation in the statement at the end of section 2.2 is to account for the stochasticity in the rewards.", "title": "Revisions"}, "r10BxxDNe": {"type": "rebuttal", "replyto": "B1b9ZBxNe", "comment": "Thank you for raising these points.\n\nWe agree with you that their method could still work without a value function estimate, but could be slower when you convert to the backward view as you point out. When switching to the backward view, we believe the role of the VF estimate in Degris et al is more than to reduce the variance (despite their claims on page 4). In page 11, they have seamlessly replaced delta_{t} with delta_{t+1} under stationarity which implicitly assumes good estimates for delta_{t} and consequently for the VF estimate. We think both these issues contribute to the difference in performance although its not clear which is the more dominant effect.\n\nWe will update the manuscript regarding this. Please let us know if you have any outstanding concerns. We will update the manuscript by the end of the month.", "title": "re: clarification"}, "HyNGglPVx": {"type": "rebuttal", "replyto": "H1bSmrx4x", "comment": "Thank you for the review. We agree with you partially on the role of the VF estimate but still think it plays a critical role when you are in the backward view. See comment below.", "title": "re: review"}, "Bklxlxw4e": {"type": "rebuttal", "replyto": "ByVsGkMVx", "comment": "Thank you for the review.\n\nWe indeed agree that the size of the dataset is relatively small, which is why we use a small LSTM (see page 8). Also, while the responses are reasonable, they are not all amazing (the maximum score is 0.9 out of 2).  We believe that the reason why a small model still works here is because this is a very narrow domain, tackling very few issues, and with a relatively lower variation (as opposed to the Ubuntu Dialogue Corpus, for example). Indeed, seeing whether a similar approach works for larger datasets with larger model is an interesting question for future work. \n\nWe will add the related references you mentioned, and add a small discussion regarding the size of the dataset. We will update the manuscript by the end of the month.", "title": "re: Review"}, "r1bjygDNg": {"type": "rebuttal", "replyto": "ByQ-cqT7x", "comment": "Thank you for the review. \n\nWe updated only the top and softmax layers in all experiments - including the constant estimator - to keep the comparison fair (paragarah RNN Design on page 7). But yes, investigating when each estimator will work well is important. We suspect that when there is more data the GTD (or a more sophisticated) estimator would work better, but we would rather not make any strong statements or draw stronger conclusions from the current experimental setup.\n\nThank you for the other suggestions and comments. We will update the manuscript by the end of the month.", "title": "re: clearly written, natural extension of previous work"}, "B1-g9SjXl": {"type": "rebuttal", "replyto": "SkCxPHw7e", "comment": "Hi, thank you for your questions. Please see our responses below.\n1) Not quite. To convey the main intuitions, lets just assume the tabular setting. If you look at Algorithm 1 of Degris et al, you see that the reward gets factored into the updates due to the delta term. The first time their algorithm sees an episode, it only affects the update at time step T (where T is the length of the episode). The next time it sees the same episode, the updates at time T-1 and T are affected because the value function (VF) estimate is also updated. So for time t in an episode, the example has to be seen T-t times. A similar phenomenon occurs in the non-tabular setting where it takes at least a few passes through an episode before the rewards are factored into all time steps.\nThe more serious issue with the on-line version is that this relies on a reliable estimator for the value function - because, the expected rewards of the future states in a given episode are made known to the actor *only* via the VF estimate. While it is reasonable to expect a good estimator for the VF when you have large amounts of labeled data, this is not the case in settings like ours. For e.g. a simple constant estimator for the VF will not work here. We have explained this in the last paragraph of Sec 4.1 (first para of page 8).\nDegris et al show that the forward and backward view are equivalent, but the proof is in the asymptotic / large-sample regime when you have reached the limiting distribution and the VF estimate has \"converged\". The proof does not hold for finite sample and we have strong reasons to believe that it is not true, especially in limited data situations as in the paper.\n2) No, the updates are performed in mini-batches. See the sentence above the last paragraph in page 5 and line 6 of the last paragraph of page 6. We can clarify this more in the table for Algorithm 1.\n3) Yes.\n4) Yes, we do not claim any convergence properties for the proposed algorithm.\n5) Sorry, we should have been a bit more precise here. The assumption is that the state distributions s_t and s_{t+k} are the same for all k>0. This is true (once again) when you have reached the limiting distribution but not the case in ours. See the beginning of Appendix A.2 in Degris et al.\n", "title": "re: clarification"}, "SkCxPHw7e": {"type": "review", "replyto": "rJfMusFll", "review": "Could the authors please comment on / clarify the following points regarding the comparison of their proposed batch algorithm to the online version of [Degris et al 2012]:\n1) On p6 the authors claim, that the main benefit of their algorithm compared to the online version comes from the fact that in the batch setting all (terminal) rewards are known compared to the online setting. However, Degris et al show that using eligibility traces, the lambda-return can exactly be computed in a backward view. Hence, it the updated to the parameters were computed over the entire dataset using the algorithm of Degris et al (without doing any actual parameter updated meanwhile), the result would be the same as of the proposed algorithm. Hence, as I understand it, the only difference between the two algorithms is when the parameters are updated. Is this correct?\n2) The algorithm outlined on p6 suggests that for each gradient step on the parameters, the entire dataset is touched. This seems quite inefficient. An mini-batched version of this algorithm on the other hand would resemble the version of Degris quite closely. \n3) In fig 2: Does the x-axis show number of passes through the entire dataset for all algorithms?\n4) The errata of Degris et al states that the guarantees for their off-policy algorithm only holds for the tabular setting. This would exclude the RNN-based state representation chosen by the authors here; is this correct?\n5) middle of p6: \u201cIn order to tackle this, previous research either [...], or make additional assumptions about the distribution of the states (Degris et al., 2012...\u201d Which assumptions in Degris et al do the authors refer to?The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.\nThe approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \"clarification regarding batch vs. online setting\").\nThe artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. ", "title": "Clarification regarding batch vs. online setting", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1bSmrx4x": {"type": "review", "replyto": "rJfMusFll", "review": "Could the authors please comment on / clarify the following points regarding the comparison of their proposed batch algorithm to the online version of [Degris et al 2012]:\n1) On p6 the authors claim, that the main benefit of their algorithm compared to the online version comes from the fact that in the batch setting all (terminal) rewards are known compared to the online setting. However, Degris et al show that using eligibility traces, the lambda-return can exactly be computed in a backward view. Hence, it the updated to the parameters were computed over the entire dataset using the algorithm of Degris et al (without doing any actual parameter updated meanwhile), the result would be the same as of the proposed algorithm. Hence, as I understand it, the only difference between the two algorithms is when the parameters are updated. Is this correct?\n2) The algorithm outlined on p6 suggests that for each gradient step on the parameters, the entire dataset is touched. This seems quite inefficient. An mini-batched version of this algorithm on the other hand would resemble the version of Degris quite closely. \n3) In fig 2: Does the x-axis show number of passes through the entire dataset for all algorithms?\n4) The errata of Degris et al states that the guarantees for their off-policy algorithm only holds for the tabular setting. This would exclude the RNN-based state representation chosen by the authors here; is this correct?\n5) middle of p6: \u201cIn order to tackle this, previous research either [...], or make additional assumptions about the distribution of the states (Degris et al., 2012...\u201d Which assumptions in Degris et al do the authors refer to?The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.\nThe approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \"clarification regarding batch vs. online setting\").\nThe artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. ", "title": "Clarification regarding batch vs. online setting", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJdY0dlXe": {"type": "rebuttal", "replyto": "rJ-Dowkmx", "comment": "Thank you for your question.\n\nThe target sentence is the exact reverse of the input sequence, i.e. we do *not* exclude the forbidden words. Therefore, a maximum likelihood (ML) trained bot can do well on the reproducing part of the reinforcement learning (RL) reward, r_r, but not on the forbidden part, r_f. The goal then is to use the rewards to see if we can improve on r_f (and r_r).\n\nThe reason we chose this set up was because it reflects naturally occurring practical scenarios for this work where the ML objective and the RL objective are partially but not completely aligned. For e.g., we may have large quantities of past conversations where the agents' responses are good, but perhaps not entirely suitable due to reasons explained in the paper. We can initialise a bot with ML on this dataset, and then use the limited labeled dataset to fine tune this bot via RL. We have mentioned this in the 2nd para of Section 4.1 but can elaborate more if necessary.\n\nYes, the reward is independent of the decoded word order. Repeating the sentence is a sufficient but not necessary requirement to score well on r_r. We will add this detail in the main text.\n\nps: Most of us are traveling for NIPS, so we might not be able to incorporate any changes to the manuscript immediately.", "title": "re: synthetic experiment (@AnonReviewer4)"}, "rJ-Dowkmx": {"type": "review", "replyto": "rJfMusFll", "review": "Maybe I missed it in the paper, but it would be nice if you could provide some more intuition as to why you chose the precise setup for your synthetic experiment.\n\nAlso, just to make sure -- in this case the target sentence is the exact same as the input sentence, but without the forbidden words? From how you define the reward in the appendix, it seems the reward is independent of decoded word order -- is that true?This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.\n\nWhile the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. \n\nMy main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.\n\nReferences:\n\nWen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016).\n", "title": "synthetic experiment", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ByVsGkMVx": {"type": "review", "replyto": "rJfMusFll", "review": "Maybe I missed it in the paper, but it would be nice if you could provide some more intuition as to why you chose the precise setup for your synthetic experiment.\n\nAlso, just to make sure -- in this case the target sentence is the exact same as the input sentence, but without the forbidden words? From how you define the reward in the appendix, it seems the reward is independent of decoded word order -- is that true?This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.\n\nWhile the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. \n\nMy main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.\n\nReferences:\n\nWen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016).\n", "title": "synthetic experiment", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}