{"paper": {"title": "Regularizing CNNs with Locally Constrained Decorrelations", "authors": ["Pau Rodr\u00edguez", "Jordi Gonz\u00e0lez", "Guillem Cucurull", "Josep M. Gonfaus", "Xavier Roca"], "authorids": ["pau.rodriguez@cvc.uab.es", "poal@cvc.uab.es", "pep.gonfaus@visual-tagging.com", "xavier.roca@visual-tagging.com"], "summary": "We show that that models regularized with local feature decorrelation have lower overfitting.", "abstract": "Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated feature weights, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. \nIn particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout are present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN.", "keywords": ["Computer vision", "Deep learning", "Optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper presents a new regularization approach for deep learning that penalizes positive correlations between features in a network. The experimental evaluation is solid, and suggests the proposed regularized may help in learning better convolutional networks (but the gains are relatively small)."}, "review": {"B19tNcU8g": {"type": "rebuttal", "replyto": "HJxMCGlNx", "comment": ">> Although the improvement in performances is not significant the direction of research and the observations made are promising.\n\nOn the new updated version, we show better performance improvements with the new wide ResNet results. We hope these new results strengthen the significance of our work.\n", "title": "Re: Orthogonality of weight features might not always lead to improved performance"}, "S1U8Nc8Ig": {"type": "rebuttal", "replyto": "HJeb6s-4l", "comment": ">> The paper refers to these as \"local\" and \"global\" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things.\n \nWe found the words \u201clocal\u201d and \u201cglobal\u201d useful to avoid repeating long sentences like \u201cregularizing positive and negative correlations\u201d, we also explain the origin of this notation on page five: in previous regularizations all feature weights influence each other (\u201cglobal\u201d), while OrthoReg only regularizes the nearest feature weights in angle (\u201clocal\u201d). \n\n\n>> More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes.\n\nWe have added the suggested comparison in Figure 6 for Wide Resnet on Cifar10 and Cifar100. As it can be seen, OrthoReg has a lower error bound compared with regularizing the negative correlations (0.2% on Cifar10, and 0.7% on Cifar100).\n\n\n>> One of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as \"features\". However, the authors have already said they will address this.\n\nWe have fixed this issue accordingly replacing \u201cfeatures\u201d with \u201cfeature weights\u201d or \u201cfeature activations.\n\n>> The paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed.\n\nAlthough we successfully applied the proposed regularizer on ReLU networks, we agree with the reviewer that a thorough study on the effects of the non-linearities should be done. We have modified the discussion part of the paper so as to reflect this important concern.\n\n\n>> In response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the \"multi-bias\" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for \"multi-bias\", seeing as the weights are shared across sets of correlated features.\n\nWe now better understand the point, but note that there is no guarantee that networks with a high number of filter weights, and thus with a lot of redundant filters, will show a good behaviour in terms of generalization and overfitting. We have added a comment on this fact in section 2.\n\n>> The dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model).\n\nAs we explained in the paper, weight decay is shown to reduce the \u201ceffective number of parameters\u201d of neural networks [1]. Taking this into account we think it is sensible to make the distinction between those regularizations which drop weights, activations\u2026 (in essence, regularizations which reduce the capacity of the network), from those which do use all the capacity as effectively as possible. \n\nHowever, we agree this distinction may not be as clear to a reader as we claimed in the paper, thus, we change the sentence \u201cThere are two clearly defined regularization strategies in the literature\u201d for \u201cFrom the literature, two different regularization strategies can be defined\u201d.\n\n[1] Moody, J. E. (1991, December). The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. In NIPS (Vol. 4, pp. 847-854).\n\n\n>> Overall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.\n\nThanks. We hope the new consistent results on the new version of wide residual networks strengthen our claims and the contribution.\n", "title": "Re: Solid experimental validation"}, "B1deVcLIe": {"type": "rebuttal", "replyto": "HkOs0RbEx", "comment": "Thank you again!", "title": "Re: Good results on deep nets"}, "r1LhQcUUe": {"type": "rebuttal", "replyto": "ByOvsIqeg", "comment": "We thank again the reviewers for the thorough revision and the valuable feedback. We have updated the paper accordingly:\n\n1. We have applied OrthoReg on the new version of Wide ResNets, consistently reducing the error rates to 3.69%, and 18.56% on Cifar10, and Cifar100 respectively.\n\n2.  Figure 6 has been updated so as to compare the base regularization to the regularization of positively correlated feature weights. All the curves are now obtained from Wide ResNet v2.\n\nMulti-bias neural networks are now discussed in the third paragraph of section 2.\n3. The need for investigating the effects of activation functions like ReLU on the different existing regularizations is now reflected on the discussion section.\n\n4. When appearing alone, the word \u201cfeatures\u201d has been replaced for \u201cfeature weights\u201d or \u201cfeature activations\u201d accordingly.\nThe dichotomy between regularization types has been changed according to reviewer 3 comments.\n", "title": "Paper v2.0"}, "HJuMvZxmg": {"type": "rebuttal", "replyto": "S1qEzwJmx", "comment": "Thank you!!!\n\nPau", "title": "Re: Very good results"}, "Syw1PZgXx": {"type": "rebuttal", "replyto": "ryjFWPkQg", "comment": "We have not tried with multi-crop because we chose to report single-crop performance for fairness of comparison with the rest of models presented in table 3. We will explain it in the next version.\n\nThank you very much,\n\nPau", "title": "Re: Regularization helps"}, "H13jxZemg": {"type": "rebuttal", "replyto": "rkaYmG17g", "comment": "Thanks for the feedback,\n\n>> The paper seems to conflate orthogonality and correlation of the feature weights with orthogonality and correlation of the feature activations in some places. Could you clarify which one of these is meant in e.g. Section 2.2?\n\nOrthoReg is always performed on feature weights, in the next version of the paper we will change all the ambiguous sentences such as \u201cthe minimum angle between two features\u201d i section 2.2, so that it refers to \u201cfeature weights\u201d.\n\n\n>> These are not equivalent because there is always an interaction with the nonlinearity. In the ReLU case for example, relu(Wx) and relu(-Wx) will be perfectly orthogonal (one is zero iff the other is nonzero and vice versa), but the weights are negatively correlated. \n\nTanks for the insightful comment. We are aware of the difference between weight correlations and activation correlations when using ReLU. As reviewer 1 explains in the third question, ReLU activation makes even more natural avoiding to regularize negative correlations. We will accordingly include comments on the ReLU activation function in the next version of the paper. \n\n\n>> Reviewer1: Biases also play important role in de-correlation of neural responses. What can one expect the learned bias statistics to be in presence of OrthoReg?\nReveiwer3: As AnonReviewer1 points out, there is a similar interaction with the bias parameters, which is ignored when only looking at the correlation of the weights. In fact, some authors have found it useful to have multiple feature detectors with exactly the same weights, differing only in their bias: https://arxiv.org/abs/1604.00676 So it's not immediately obvious to me that having only uncorrelated / negatively correlated weights is a good thing, could you comment on this as well?\n\nAs both reviewers state, biases have a great influence on neural responses, specially in ReLU networks. We are aware that multiple biases can be beneficial when applied to feature detectors with the same feature weights. In fact, we do not regularize the biases in order to avoid the learning of similar feature detectors with opposite bias weights (as explained in https://arxiv.org/abs/1604.00676). However, there is no conflict between OrthoReg and multi-bias since our approach tackles the problem of adding more feature weights without increasing the overfitting, while multi-bias increases the representative power of the network by thresholding the same feature map at multiple levels. Note the two approaches are complementary and not opposite. We will add this clarification in the next version of the paper.\n\nWe thank the reviewer for the valuable comments,\n\nPau\n\n", "title": "Re: activations vs. weights"}, "BypMhglQx": {"type": "rebuttal", "replyto": "H1I4foAfg", "comment": "Thank you for your comments,\n\n>> In the experiments section only very deep networks were chosen to showcase the improved performance when using OrthoReg. Any particular reason for this? How do CNNs with fewer layers perform when using OrthoReg?\n\nWe chose to regularize \"very deep\" neural networks in order to prove that, contrary to other regularizations which apply to the feature maps, our proposal can be applied to state-of-the-art neural networks with dozens of hidden layers in feasible time. We already provide results on a shallower MLP and CNN models trained with MNIST, showing similar results in Section 3.1. \n\n\n>> The increase is in performance is quite little as compared to the original networks using other regularization schemes. More experimental validation might help. Can you please comment on this.\n\nNote that the improvements were found even in the presence of Dropout, weight-decay and Batch-Normalization at the same time on deep residual networks and wide residual networks. In Table 3 we show that improvements in performance of recent models trained on CIFAR are below 1%. (e.g. 0.11% improvement 100-Layer ResNet w.r.t. the ELU-Network). In this context, it can be considered that the 0.16% improvement obtained on CIFAR-10, the 0.9% on CIFAR-100, and the 0.1% on SHVN obtained with OrthoReg can be considered as remarkable. \nMore experimentation can also be found in Table 2, where we show that the margin of improvement can be increased if compared with no Dropout but the total improvement is best when both regularizations are present.\nWe encourage reviewer 1 to propose any extra experiment if still considered necessary.\n\n\n>> Reviewer1: Biases also play important role in de-correlation of neural responses. What can one expect the learned bias statistics to be in presence of OrthoReg?\nReveiwer3: As AnonReviewer1 points out, there is a similar interaction with the bias parameters, which is ignored when only looking at the correlation of the weights. In fact, some authors have found it useful to have multiple feature detectors with exactly the same weights, differing only in their bias: https://arxiv.org/abs/1604.00676 So it's not immediately obvious to me that having only uncorrelated / negatively correlated weights is a good thing, could you comment on this as well?\n\nAs both reviewers state, biases have a great influence on neural responses, specially in ReLU networks. We are aware that multiple biases can be beneficial when applied to feature detectors with the same feature weights. In fact, we do not regularize the biases in order to avoid the learning of similar feature detectors with opposite bias weights (as explained in https://arxiv.org/abs/1604.00676). However, there is no conflict between OrthoReg and multi-bias since our approach tackles the problem of adding more feature weights without increasing the overfitting, while multi-bias increases the representative power of the network by thresholding the same feature map at multiple levels. Note the two approaches are complementary and not opposite. We will add this clarification in the next version of the paper.\n\n\n>> Given that the deep networks mostly use rectified non-linearities as activation functions it seems quite natural to avoid penalizing negative correlations. Can you please comment on this.\n\nAs we show in Figures 2, and 3, and given the experimental results we argument that to avoid penalizing negative correlations is beneficial from different perspectives. We agree with the reviewer that the ReLU perspective is interesting and we will accordingly include it in the next version of the paper.\n\n\n>> It was shown previously that orthogonal weight initialization helps in training very deep networks faster. Any such observations were made when using OrthoReg?\n\nWe observed the same convergence speeds when training with and without OrthoReg. However, since OrthoReg is proven to work well as a regularization, we expect it to be useful in the form of initialization because the benefits of keeping negatively correlated features in small regularization steps do also apply when determining the initial value of orthogonal weights. This is an interesting comment that will be addressed in the future work.\n\n\nWe thank the reviewer for the valuable suggestions,\n\nPau\n\n", "title": "Re: Orthogonality of weight features might not always lead to improved performance"}, "ryjFWPkQg": {"type": "review", "replyto": "ByOvsIqeg", "review": "Have you try to test with several crops as Graham did in his paper of Fractional Max-Pooling? He arrive to 3.47% with 100 test on CIFAR-10.\n\n\n\nThe author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. ", "title": "Regularization helps", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkOs0RbEx": {"type": "review", "replyto": "ByOvsIqeg", "review": "Have you try to test with several crops as Graham did in his paper of Fractional Max-Pooling? He arrive to 3.47% with 100 test on CIFAR-10.\n\n\n\nThe author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. ", "title": "Regularization helps", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkaYmG17g": {"type": "review", "replyto": "ByOvsIqeg", "review": "The paper seems to conflate orthogonality and correlation of the feature weights with orthogonality and correlation of the feature activations in some places. Could you clarify which one of these is meant in e.g. Section 2.2?\n\nThese are not equivalent because there is always an interaction with the nonlinearity. In the ReLU case for example, relu(Wx) and relu(-Wx) will be perfectly orthogonal (one is zero iff the other is nonzero and vice versa), but the weights are negatively correlated.\n\nAs AnonReviewer1 points out, there is a similar interaction with the bias parameters, which is ignored when only looking at the correlation of the weights. In fact, some authors have found it useful to have multiple feature detectors with exactly the same weights, differing only in their bias: https://arxiv.org/abs/1604.00676 So it's not immediately obvious to me that having only uncorrelated / negatively correlated weights is a good thing, could you comment on this as well?The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations. An alternative version which penalises all correlations regardless of sign is also considered. The paper refers to these as \"local\" and \"global\" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things.\n\nThe experimental validation is quite rigorous. Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases. While these improvements may seem modest, the baselines are already very competitive as the authors pointed out. In some cases it does raise some questions about statistical significance though. More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes.\n\nOne of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as \"features\". However, the authors have already said they will address this.\n\nThe paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed.\n\nIn response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the \"multi-bias\" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for \"multi-bias\", seeing as the weights are shared across sets of correlated features.\n\nThe dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model).\n\nOverall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.", "title": "activations vs. weights", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJeb6s-4l": {"type": "review", "replyto": "ByOvsIqeg", "review": "The paper seems to conflate orthogonality and correlation of the feature weights with orthogonality and correlation of the feature activations in some places. Could you clarify which one of these is meant in e.g. Section 2.2?\n\nThese are not equivalent because there is always an interaction with the nonlinearity. In the ReLU case for example, relu(Wx) and relu(-Wx) will be perfectly orthogonal (one is zero iff the other is nonzero and vice versa), but the weights are negatively correlated.\n\nAs AnonReviewer1 points out, there is a similar interaction with the bias parameters, which is ignored when only looking at the correlation of the weights. In fact, some authors have found it useful to have multiple feature detectors with exactly the same weights, differing only in their bias: https://arxiv.org/abs/1604.00676 So it's not immediately obvious to me that having only uncorrelated / negatively correlated weights is a good thing, could you comment on this as well?The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations. An alternative version which penalises all correlations regardless of sign is also considered. The paper refers to these as \"local\" and \"global\" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things.\n\nThe experimental validation is quite rigorous. Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases. While these improvements may seem modest, the baselines are already very competitive as the authors pointed out. In some cases it does raise some questions about statistical significance though. More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes.\n\nOne of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as \"features\". However, the authors have already said they will address this.\n\nThe paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed.\n\nIn response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the \"multi-bias\" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for \"multi-bias\", seeing as the weights are shared across sets of correlated features.\n\nThe dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model).\n\nOverall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.", "title": "activations vs. weights", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1I4foAfg": {"type": "review", "replyto": "ByOvsIqeg", "review": "In the experiments section only very deep networks were chosen to showcase the improved performance when using OrthoReg. Any particular reason for this? How do CNNs with fewer layers perform when using OrthoReg?\n\nThe increase is in performance is quite little as compared to the original networks using other regularization schemes. More experimental validation might help. Can you please comment on this.\n\nBiases also play important role in de-correlation of neural responses. What can one expect the learned bias statistics to be in presence of OrthoReg?\n\nGiven that the deep networks mostly use rectified non-linearities as activation functions it seems quite natural to avoid penalizing negative correlations. Can you please comment on this.\n\nIt was shown previously that orthogonal weight initialization helps in training very deep networks faster. Any such observations were made when using OrthoReg?\nEncouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. \n\nOrthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.\n\nAlthough the improvement in performances is not significant the direction of research and the observations made are promising.", "title": "Orthogonality of weight features might not always lead to improved performance", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJxMCGlNx": {"type": "review", "replyto": "ByOvsIqeg", "review": "In the experiments section only very deep networks were chosen to showcase the improved performance when using OrthoReg. Any particular reason for this? How do CNNs with fewer layers perform when using OrthoReg?\n\nThe increase is in performance is quite little as compared to the original networks using other regularization schemes. More experimental validation might help. Can you please comment on this.\n\nBiases also play important role in de-correlation of neural responses. What can one expect the learned bias statistics to be in presence of OrthoReg?\n\nGiven that the deep networks mostly use rectified non-linearities as activation functions it seems quite natural to avoid penalizing negative correlations. Can you please comment on this.\n\nIt was shown previously that orthogonal weight initialization helps in training very deep networks faster. Any such observations were made when using OrthoReg?\nEncouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. \n\nOrthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.\n\nAlthough the improvement in performances is not significant the direction of research and the observations made are promising.", "title": "Orthogonality of weight features might not always lead to improved performance", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}