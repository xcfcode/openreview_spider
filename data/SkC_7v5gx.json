{"paper": {"title": "The Power of Sparsity in Convolutional Neural Networks", "authors": ["Soravit Changpinyo", "Mark Sandler", "Andrey Zhmoginov"], "authorids": ["schangpi@usc.edu", "sandler@google.com", "azhmogin@google.com"], "summary": "Sparse random connections that allow savings to be harvested and that are very effective at compressing CNNs.", "abstract": "Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3.", "keywords": ["Deep learning", "Supervised Learning"]}, "meta": {"decision": "Reject", "comment": "The reviewers agreed that the main contribution is the first empirical analysis on large-scale convolutional networks concerning layer-to-layer sparsity. The main concerns were that of novelty (connection-wise sparsity being explored previously but not in large-scale domains) and the importance given the current state of fast implementations of sparsely connected CNNs. The authors argued that the point of their paper was to drive software/hardware co-evolution and guide the next generation of, e.g. CUDA tools development. The confident scores were borderline while the less confident reviewer was pleased with the paper so I engaged the reviewers in a discussion post author-feedback. The consensus was that the paper presented promising but not fully developed nor convincing research."}, "review": {"Hk__R_Krl": {"type": "rebuttal", "replyto": "SJSjvTZEg", "comment": "Thank you for your thoughtful and detailed review.\n\nWe want to make two points regarding the memory cost.\nFirst, for a convolutional layer, there are two memory components: the memory needed to store activations (e.g. num_channels * width * height) vs. the memory needed to store convolutional weights (e.g. num_input_channels * num_output_channels * kernel_size). It is important to stress that, for many layers (especially for later \u201cthicker\u201d layers), the cost is often dominated by the latter component rather than the former. Thus, storing sparse matrices reduces memory requirement during inference time. Furthermore, it can also significantly reduce memory needed to store trained models.\nSecond, while memory write access can be expensive, in matrix multiplication, vast majority of the time is spent accumulating the result rather than saving it to memory. That\u2019s why in general matrix multiplication is much slower than addition. In this case, each output value requires kernel_size * num_input_channels computations plus memory/input access for all the weights, and sparse convolutions require a comparable number of memory access to their corresponding reduced-depth dense convolutions (which could be much smaller than what the full dense convolutional operator would require).\n\nRegarding the gains, we wholeheartedly agree that gains on modern GPUs still need to be realized. However, given the amount of engineering effort invested in implementation of highly efficient CUDA convolution operators, matching it will require a similar effort. Nevertheless, we believe that results like ours help guide the future hardware and software decisions about neural networks. We also note that the story is different in embedded applications where CUDA does not exist yet, but this is beyond the scope of the paper.\n\nWhile incremental training gives no better accuracy than training the full network from the beginning, it is very likely that incremental training will be more efficient. Not only is the network during early stages of incremental training is sparser, but also it has far fewer parameters than the full model.", "title": "Thank you for your review."}, "Skdl1YtBl": {"type": "rebuttal", "replyto": "SJ9WwqyEx", "comment": "Thank you for the detailed review. Regarding the sparse implementation issue, we wholeheartedly agree that gains on modern GPUs still need to be realized. However, given the amount of engineering invested in implementation of CUDA convolution operators, matching it will require a similar effort. Nevertheless, we believe that results like ours help guide the future hardware and software optimization for neural networks. We also note that the story is different in embedded applications where CUDA does not exist yet, but this is beyond the scope of the paper.\n\nIn the related work section, we provide a comprehensive overview regarding where our novelty lies in comparison to relevant ideas. Besides being the first to provide evidence and analysis on the usefulness of sparsity in large convolutional networks, our approach is very simple but at the same time preserving \u201cregularity\u201d in structure and therefore efficiency. We believe this is an important knowledge. We also introduce an incremental training technique to help speeding up the training further.", "title": "Thank you for your review."}, "SkYRTOKBx": {"type": "rebuttal", "replyto": "H1EOToHEe", "comment": "Thanks for your thoughtful review. We agree that coming up with a different/better dropout schedule is an interesting and important research direction. We will add this to the future work section.", "title": "Thank you for your review."}, "S16ikNvmg": {"type": "rebuttal", "replyto": "S1X1p-DXe", "comment": "Thank you for your comment! Responses inline:\n\n1) In general in our experiments we found that dropping connections or channels, as part of gradient descent performed slightly _worse_ than naive multiplier method (where each layer is just trimmed down by a constant fraction), even though the result at first glance looked great. We did not pursue this approach further. We find that breaking symmetry argument (see discussion in the paper, and also LeCun's paper linked in the comments) for densification over time, to be compelling justification why densification over time leads to better results. \n\nAnother point is that densification over time leads to speed-up of training (start with smaller model first and arrive at your target), where as sparsification would lead to slowing it down. (you start with bigger model than you need and then trim it down)\n\n2) The connections are activated once and for all, there is no \"zero sparsity during test time\". Test time is just as sparse (and fixed the same way) as  train time, this is a key difference with a Dropconnect method, where drop connect is used for improving training properties.\n\n3) This is a good question, as described in the paper our current schedule is uniform with a fixed coefficient. E.g. for coefficient alpha, we keep alpha \\times input connections per each output. Alpha is the same throughout the graph. It is an open question what is the \"optimal schedule\"\n\n4) In our implementation, computation graph always contains the full weight matrix, which is monotonously trimmed up-to the  density  alpha, alpha is just a scalar tensor that 1) either slowly increases during training for incremental training of full models or 2) stays constant for sparse model. Memory requirement for weight matrix@training time is typically not a constraint. Dynamic size tensors, might be an option when software stack starts supporting them.\n", "title": "re schedules"}, "S1X1p-DXe": {"type": "rebuttal", "replyto": "SkC_7v5gx", "comment": "1. The idea in incremental training is to start with a very sparse model and increase the density over time. What if you start with a full connection model and eventually remove some connections. More connections in the beginning of training can results in better parameter initialization and then removing these links can reduce complexity during test time.\n\n2. The idea in sparse random method is to deactivate the connections at random. I expect zero sparsity during test time. How do you handle reduced expectation of output of layers during test time? Do you scale the output by scalar as function of 1/sparsity-ratio?\n\n3. We expect first layers to have more lower level information and we expect to have less Drop connection ratio for these layers. What is your drop connection schedule for different layers?\n\n4. How do you handle computation graph during training time? Do you recompile the graph for different dropout schedule, (which can slow down training)?\n", "title": "DropConnection Schedule"}, "H1huVuy7g": {"type": "rebuttal", "replyto": "HyctawkQg", "comment": "Thank you for your comment. You are absolutely right that any architecture from last 10 years makes it prohibitively expensive to do random memory access. However we don't use sparse matrices, we only introduce sparsity from channel to channel. Thus, we can use all the benefits of caching becuase the basic operation (convolve image plane with k \\times k filter) is unchanged. \n\nIn other words, if naive implementation of dense convolution looks like this:\n\nfor i in input_channel\n  for o in output_channel\n    # this oepration is the expensive one since it has to go over entire image\n    result[o] += convolve(input[i]  w[i, o, ...])\n\nsimilar sparse convolution will look like this: \n\nfor i in input_channel\n  for index, o, in enumerate(output_channels_connected_to_i)\n    # this oepration is expensive  since it has to go over entire image\n    result[o] += convolve(input[i]  w[i, index, ...])\n\nin this implementation setup, the speed up will be mostly in terms of madds, since the basic operation (convoving *entire* image plane) is unchanged. We will include this in the updated version of the paper\n\nTo answer your question we do not have optimized version of sparse convolutions yet, and Cudnn implementation of dense convolutions is very fast (yet not publicly available) (e.g. X00% faster than reference GPU implementation)  so we can not report any speed up compared to that. Our naive implementation gives comparable speed up to naive dense gpu implementation. \n\nWe believe this what makes this result interesting. In particular it might guide design for hardware manufacturing and/or algorithmic research to optimize these use cases. ", "title": "dense vs sparse"}, "HyctawkQg": {"type": "review", "replyto": "SkC_7v5gx", "review": "Computational cost is reported in terms of multiply-adds.  On what real architectures (current or possible future) would this result in wall-time speedups?  On many arch implementations, elements stored in contiguous memory regions are faster to load (amortized per element) compared to spread-out random reads.  Is it possible to measure wall-time in addition to multiply-adds?\nThis paper aims to improve efficiency of convolutional networks by using a sparse connection structure in the convolution filters at each layer.  Experiments are performed using MNIST, CIFAR-10 and ImageNet, comparing the sparse connection kernels against dense convolution kernels with about the same number of connections, showing the sparse structure (with more feature maps) generally performs better for similar numbers of parameters.\n\nUnfortunately, any theoretical efficiencies are not realized, since the implementation enforces the sparse structure using a zeroing mask on the weights.  In addition, although the paper mentions that this method can be implemented efficiently and take advantage of contiguous memory reads/writes of current architectures, I still find it unclear whether this would be the case:  The number of activation units is no smaller than when using a dense convolution of same dimension, and these activations (inputs and outputs) must be loaded/stored.  The fact that the convolution is sparse saves only on the multiply/addition operation cost, not memory access for the activations, which can often be the larger amount of time spent.\n\nThe section on incremental training is interesting, but feels short and preliminary, and any gains here also have yet to be realized.  The precision is no better than for the original network, and as mentioned above, the implementation of the sparse structure is no faster than the original.\n\nOverall, the method and evaluations show that the basic approach has promise.  However, it is unclear how real gains (in either speed or accuracy) might actually be found with it.  Without this last step, it still seems incomplete to me for a conference paper.\n", "title": "Wall-time measurements?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJSjvTZEg": {"type": "review", "replyto": "SkC_7v5gx", "review": "Computational cost is reported in terms of multiply-adds.  On what real architectures (current or possible future) would this result in wall-time speedups?  On many arch implementations, elements stored in contiguous memory regions are faster to load (amortized per element) compared to spread-out random reads.  Is it possible to measure wall-time in addition to multiply-adds?\nThis paper aims to improve efficiency of convolutional networks by using a sparse connection structure in the convolution filters at each layer.  Experiments are performed using MNIST, CIFAR-10 and ImageNet, comparing the sparse connection kernels against dense convolution kernels with about the same number of connections, showing the sparse structure (with more feature maps) generally performs better for similar numbers of parameters.\n\nUnfortunately, any theoretical efficiencies are not realized, since the implementation enforces the sparse structure using a zeroing mask on the weights.  In addition, although the paper mentions that this method can be implemented efficiently and take advantage of contiguous memory reads/writes of current architectures, I still find it unclear whether this would be the case:  The number of activation units is no smaller than when using a dense convolution of same dimension, and these activations (inputs and outputs) must be loaded/stored.  The fact that the convolution is sparse saves only on the multiply/addition operation cost, not memory access for the activations, which can often be the larger amount of time spent.\n\nThe section on incremental training is interesting, but feels short and preliminary, and any gains here also have yet to be realized.  The precision is no better than for the original network, and as mentioned above, the implementation of the sparse structure is no faster than the original.\n\nOverall, the method and evaluations show that the basic approach has promise.  However, it is unclear how real gains (in either speed or accuracy) might actually be found with it.  Without this last step, it still seems incomplete to me for a conference paper.\n", "title": "Wall-time measurements?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bkt1nECze": {"type": "rebuttal", "replyto": "rJFbhosMl", "comment": "Thanks for pointing out additional references. We have updated the manuscript to incorporate both of them. However, we note that LeCun et al. do not explore different levels of sparsity and do not apply sparsity to all layers. Thus, we provide a more comprehensive picture on the power of sparsity. Moreover, unlike both references, we do not fix a specific pattern of connections (assigning a uniform number of input connections to each output filter), potentially breaking even more symmetry.", "title": "Comparison to LeCun et al. and Torch API"}, "B1Rus4CMx": {"type": "rebuttal", "replyto": "SkpGk-6zl", "comment": "Right, we did not apply our approach to AlexNet, and we simply take VGG accuracy from Fig. 5. We have modified the paper to make it more clear. Sorry for the confusion. We simply want to say that we can achieve high accuracy with a small number of parameters.", "title": "Clarification for the conclusion section"}, "SkpGk-6zl": {"type": "review", "replyto": "SkC_7v5gx", "review": "In the conclusion section, the paper says: \"For example, this simple method achieves AlexNet-level accuracy with fewer than 400K parameters and VGG level with roughly 1M parameters.\" I couldn't see these results in any of the graphs. By looking at the graphs, my understanding is there was no experiment conducted with AlexNet and the number of parameters needed to achieve VGG level accuracy is the same as the dense connected convolutional networks. Can you please clarify this statement from the conclusion? The paper experiments with channel to channel sparse neural networks.\nThe paper is well written and the analysis is useful. The sparse connection is not new but has not been experimented on large-scale problems like ImageNet. One of the reasons for that is the unavailability of fast implementations of randomly connected convolutional layers.\nThe results displayed in figures 2, 3, 4, and, 5 show that sparse connections need the same number of parameters as the dense networks to reach to the best performance on the given tasks, but can provide better performance when there is a limited budget for the #parameters and #multiplyAdds. \nThis paper is definitely informative but it does not reach to the conference acceptance level, simply because the idea is not new, the sparse connection implementation is poor, and the results are not very surprising.", "title": "clarification on the results", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJ9WwqyEx": {"type": "review", "replyto": "SkC_7v5gx", "review": "In the conclusion section, the paper says: \"For example, this simple method achieves AlexNet-level accuracy with fewer than 400K parameters and VGG level with roughly 1M parameters.\" I couldn't see these results in any of the graphs. By looking at the graphs, my understanding is there was no experiment conducted with AlexNet and the number of parameters needed to achieve VGG level accuracy is the same as the dense connected convolutional networks. Can you please clarify this statement from the conclusion? The paper experiments with channel to channel sparse neural networks.\nThe paper is well written and the analysis is useful. The sparse connection is not new but has not been experimented on large-scale problems like ImageNet. One of the reasons for that is the unavailability of fast implementations of randomly connected convolutional layers.\nThe results displayed in figures 2, 3, 4, and, 5 show that sparse connections need the same number of parameters as the dense networks to reach to the best performance on the given tasks, but can provide better performance when there is a limited budget for the #parameters and #multiplyAdds. \nThis paper is definitely informative but it does not reach to the conference acceptance level, simply because the idea is not new, the sparse connection implementation is poor, and the results are not very surprising.", "title": "clarification on the results", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJFbhosMl": {"type": "rebuttal", "replyto": "SkC_7v5gx", "comment": "This paper adds value by studying the idea of sparse random connections between input and output feature maps by trying it with newer convolutional networks, and there are some additional details. However, the basic idea is not new, it appeared in a 1998 paper by Yann LeCun and others:\n\nhttp://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n\nCheck out Table-I and the accompanying explanation. Also, Torch already has an implementation for such a random connection map:\n\nhttps://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialConvolutionMap\n\nThe authors don't reference LeCun's original paper, or the Torch API at all, this is a little surprising.", "title": "The basic idea is not new."}}}