{"paper": {"title": " Adding Recurrence to Pretrained Transformers", "authors": ["Davis Yoshida", "Allyson Ettinger", "Kevin Gimpel"], "authorids": ["~Davis_Yoshida1", "~Allyson_Ettinger1", "~Kevin_Gimpel1"], "summary": "Adding a small recurrence module to pretrained transformer language models allows maintaining performance while lowering memory cost", "abstract": "Fine-tuning a pretrained transformer for a downstream task has become a standard method in NLP in the last few years. While the results from these models are impressive, applying them can be extremely computationally expensive, as is pretraining new models with the latest architectures. We present a novel method for applying pretrained transformer language models which lowers their memory requirement both at training and inference time. An additional benefit is that our method removes the fixed context size constraint that most transformer models have, allowing for more flexible use. When applied to the GPT-2 language model, we find that our method attains better perplexity than an unmodified GPT-2 model on the PG-19 and WikiText-103 corpora, for a given amount of computation or memory.", "keywords": ["Language modeling", "Transformers", "Recurrence", "Gradient checkpointing", "Pretraining"]}, "meta": {"decision": "Reject", "comment": "In this paper, the authors propose to add recurrence to pre-trained language models such as GPT-2 or BERT. The idea is similar to the compressive transformer paper: a small module is added to the network, and used to compress the representations from the previous chunk of data from the sequence to a single vector. Then, this vector is added to the keys and values of the self-attention module when processing the next chunk. The main contribution of the paper is to show that this technique can be added to pre-trained models at fine-tuning time.  The main concerns regarding the paper are technical novelty and limited empirical results. The idea of adding recurrence to transformers was previously explored in compressive transformer, and many previous work have considered adding modules with small number of parameters at fine-tuning time. Moreover, I do not believe that the empirical section is strong enough to justify the acceptance of the paper, as the method is only evaluated on two language modeling tasks (and one early experiment on HotpotQA). The baselines are weak, and thus, the results are not convincing. For these reasons, I weakly recommend to reject the paper, and encourage the authors to make the empirical section stronger."}, "review": {"uzImIRbM8n": {"type": "rebuttal", "replyto": "taQNxF9Sj6", "comment": "Thank you to each of the reviewers for the helpful comments. We have revised the paper, making some points more clear, and also added two new experiments to address specific concerns which were raised.\n\nThe more substantial experiment is on the HotpotQA task, although due to time constraints we give preliminary results. We are encouraged by the fact that our method improves over a non-recurrent baseline out of the box, with no hyperparameter tuning or architecture search. For the final version of the paper, we will scale up these experiments to the entire dataset, as well as optimizing hyperparameters for both the baseline and recurrent versions, and doing some recurrent architecture experimentation.\n\nIn order to address Reviewer 3\u2019s concern about how well our claims about topical propagation between windows were supported, we have added quantitative evidence using an LDA topic model fit to the PG-19 training set. We showed that the JS-divergence between distributions of topics computed from a context and continuations of that context are lower for our recurrent models than from a baseline continuation.\n", "title": "Overall response"}, "aGPcOc-V6-m": {"type": "rebuttal", "replyto": "qQNEkzJ2OvR", "comment": "> My main concern with the paper is that unfortunately the evaluation is only limited to perplexity numbers on a couple of datasets. While this is a useful metric for evaluation, this alone is inadequate to demonstrate the quality of the model as a text generating system or as a language model that will be fine-tuned for target tasks or to understand how much impact the model will have in these applications.\n\nWe agree that this is an important point, and have added preliminary experiments on HotpotQA as suggested. Due to the short time frame, we were only able to run a limited evaluation, using approximately one third of the training set for 4 epochs, and doing no hyperparameter search for either the baseline or our model. However, we find that adding a recurrence module to a disjoint RoBERTa baseline adds about 1 point to both the F1 and exact match scores. We find the fact that this result occurred \u201cout of the box\u201d without any tuning to be very promising, and will scale up the experiments to the full training set, longer training, and multiple architecture evaluations for different recurrent modules for the final version of the paper.", "title": "Response to Reviewer 1"}, "_3liUUFUsoO": {"type": "rebuttal", "replyto": "9bPwIXKKFYE", "comment": "> It would be better to include the failure results stated in the end of section 3.1.\n\nAs the failures were in our preliminary experiments, we do not have rigorous experiments for them. We can run experiments and add results on these variations in the final version of the paper.\n\n> I\u2019m surprised that a key-value pair can boost the performance that much.\n\nIf one views the output of the recurrent module as an embedding of the previous windows, it may be less surprising.\n\n> The paper should add more content on differentiating with Transformer-XL. I believe the difference is more than relative embeddings. For example, each Transformer-XL layer attends to an earlier layer of previous timestep, this convolutional operation making the structure no longer \u201crecurrent\u201d.\n\nWe have added this to the discussion in Section 2.\n", "title": "Response to Reviewer 4"}, "Se9mZDWotL0": {"type": "rebuttal", "replyto": "Nd8-ViHCEzW", "comment": ">  The authors argue that topical information or so between adjacent windows is propagated. Although it is a plausible argument, it seems like it is hardly supported by table 3.\n\nBased on this feedback, we have included an additional experiment (see section 5.3.1), in which we compute an LDA topic model using a portion of the PG19 training set, then compute the JS divergence between the topic distributions for a context and an argmax decoded continuation using both the baseline and recurrent models. We find that the topics for the recurrent continuations have a lower JS divergence from the context than the baseline continuations. Hopefully this additional quantitative evaluation addresses this concern.\n\n> The authors said more complex recurrence modules do not make any significant difference. Then, the authors need to explain why the variations do not matter.\n\nTo be clear, we were not saying that there do not exist better module architectures, but that we were unable to find any, and so went with the simplest architecture that gave good results. We have rephrased the wording at the end of Section 3.1 to better reflect this.\n\n> For example, the authors fixed l_ins to be 2, without an explanation.\n\nWe did not perform hyperparameter search for this value for our final experiments, and had just left it fixed since our preliminary experiments. We have added a footnote to Section 5 clarifying this. Because we did not vary this hyperparameter in our final experiments, it could have some impact, but unfortunately we lack the computational resources to perform thorough hyperparameter tuning. Nonetheless, we will include more experimental comparisons of hyperparameters and architectures in the next version.\n\n> It is interesting to see the relationship between the overlap length and improvement using the recurrent connection. It would be better to have further discussion about the relation and different roles.\n\nCould you provide more detail on what you mean by different roles? Perhaps you mean determining what kinds of improvements are achieved by overlap vs by the recurrent connection? We suspect using overlap with the recurrent module would cause the latter to focus more on longer-distance information, but we would have to verify that with further analysis.\n\n\n> The Transformer model is also fine-tuned with the recurrent connection. So, I was wondering if the fine-tuning improves the Transformer model too. It would be interesting to compare the updated Transformer to the previous one.\n\nInteresting point. If we understand correctly, you are interested in knowing whether the transformer is getting better due to the presence of the recurrent component during fine-tuning even if we don't use the recurrent module at test time. Even when using the recurrent module, the first window in each document is processed without any information from prior windows, and we could indeed evaluate our model in a way in which we treat every window as the initial window in the document. This would hopefully address your question. We will work on including that evaluation in the next version. \n", "title": "Response to Reviewer 3"}, "qQNEkzJ2OvR": {"type": "review", "replyto": "taQNxF9Sj6", "review": "The goal of this work is to enable existing pre-trained transformers (e.g. GPT-2) to operate over long input contexts. This is achieved by breaking the input sequence into segments and processing each segment through the transformers while allowing tokens in the current segment to attend over a summary vector of the tokens in the previous segment. The summary vector is created as a weighted combination of the tokens in the summarized segment. Thus the summary vector introduces recurrence where each segment can use information from the previous segment. These modifications yield a better language model for long input texts. \n\nThe main benefits of this approach are as follows: (1) The modifications yield a better language model for long input texts, especially when compared to a tiling based approach (2) Potential for reducing memory footprint in these models by shrinking the amount of text that is to be processed in one-go. \n\nMy main concern with the paper is that unfortunately the evaluation is only limited to perplexity numbers on a couple of datasets. While this is a useful metric for evaluation, this alone is inadequate to demonstrate the quality of the model as a text generating system or as a language model that will be fine-tuned for target tasks or to understand how much impact the model will have in these applications.\n\n-- For the model to be considered as a text generation system, there needs to be some human evaluation of the generated outputs. There are a small number of examples in the paper but that is not enough for a quantitative assessment. To clearly establish the benefits of the proposed modification it would be even better to consider generation tasks where conditioning on long inputs is essential. \n\n-- How will model fare when used in a target task defined over long input contexts? The related work section includes some papers that evaluate on such tasks. For example on target task could be HotpotQA, which requires QA over ten paragraphs which easily exceed the 512 token limits. It is important to know how the proposed recurrence model compares to tiling GPT (disjoint version) or other simpler approaches on these long input tasks.\n\n-- Another key strength of the model is that it potentially allows for processing the input in smaller segments. While perplexity gains are helpful, here again there is a missed opportunity in terms of human evaluation of the generated outputs over shorter segments, and the impact of these choices in different applications. \n\n\nTo reiterate, this paper presents a very nice idea to a well-motivated problem. The executed experiments show that this idea is likely to work but the gaps in experimentation leave much room for speculation about the potential impact of this approach in end applications. \n", "title": "Interesting idea but evaluation is inadequate.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "9bPwIXKKFYE": {"type": "review", "replyto": "taQNxF9Sj6", "review": "Summary:\n\nThe paper proposed to add a recurrent component to pretrained transformers. The component pools the hidden states of a context window and passes it to the next context window as an additional input to the self-attention layer. The component reduces the memory usage at both training and inference time, and enables the Transformer model to work on a longer sequence. The component is evaluated on two language modeling datasets and outperforms baseline models.\n\nReasons for score: \n\nI vote for accepting the paper. The paper proposed a nice and simple way to make use of the existing pretrained Transformers with reduced memory usage and extended sequence length. This should benefit practitioners who want to apply these language models on a more diverse set of downstream tasks where the sequence length doesn\u2019t fit the one from the original pretrained model. The results presented in the paper are significant. The paper is well-written and easy to follow.\n\nComments:\n\n1. It would be better to include the failure results stated in the end of section 3.1. I\u2019m surprised that a key-value pair can boost the performance that much. \n2. The paper should add more content on differentiating with Transformer-XL. I believe the difference is more than relative embeddings. For example, each Transformer-XL layer attends to an earlier layer of previous timestep, this convolutional operation making the structure no longer \u201crecurrent\u201d.\n\nTypos: \n- Third line in section 3.1: \u201cat position t\u201d -> \u201cat position i\u201d\n", "title": "A nice improvement for pretrained Transformers", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Nd8-ViHCEzW": {"type": "review", "replyto": "taQNxF9Sj6", "review": "The paper proposes recurrent connections between two adjacent Transformers, which transfers the previous context to the next step. This is a practically useful technique, improving the performance (perplexity in the experiments), and worth publishing. However, I have some comments and questions about the article.\n\nSection 5.3 is an interesting question. The authors argue that topical information or so between adjacent windows is propagated. Although it is a plausible argument, it seems like it is hardly supported by table 3.\n\nThe authors said more complex recurrence modules do not make any significant difference. Then, the authors need to explain why the variations do not matter. For example, the authors fixed l_ins to be 2, without an explanation.  \n\nIt is interesting to see the relationship between the overlap length and improvement using the recurrent connection. It would be better to have further discussion about the relation and different roles. \n\nThe Transformer model is also fine-tuned with the recurrent connection. So, I was wondering if the fine-tuning improves the Transformer model too. It would be interesting to compare the updated Transformer to the previous one. \n\nIn Eq. (1), is there 1/T? ", "title": "practically useful method for a long context size ", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}