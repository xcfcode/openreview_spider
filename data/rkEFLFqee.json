{"paper": {"title": "Decomposing Motion and Content for Natural Video Sequence Prediction", "authors": ["Ruben Villegas", "Jimei Yang", "Seunghoon Hong", "Xunyu Lin", "Honglak Lee"], "authorids": ["rubville@umich.edu", "jimyang@adobe.com", "maga33@postech.ac.kr", "timelin@buaa.edu.cn", "honglak@umich.edu"], "summary": "", "abstract": "We propose a deep neural network for the prediction of future frames in natural video sequences. To effectively handle complex evolution of pixels in videos, we propose to decompose the motion and content, two key components generating dynamics in videos. Our model is built upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. By independently modeling motion and content, predicting the next frame reduces to converting the extracted content features into the next frame content by the identified motion features, which simplifies the task of prediction. Our model is end-to-end trainable over multiple time steps, and naturally learns to decompose motion and content without separate training. We evaluate the pro- posed network architecture on human activity videos using KTH, Weizmann action, and UCF-101 datasets. We show state-of-the-art performance in comparison to recent approaches. To the best of our knowledge, this is the first end-to-end trainable network architecture with motion and content separation to model the spatio-temporal dynamics for pixel-level future prediction in natural videos.", "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "Here is a summary of the reviews and discussion:\n \n Strengths\n Idea of decoupling motion and content is interesting in the context of future frame prediction (R3, R2, R1)\n Quantitative and Qualitative results are good (R1)\n \n Weaknesses\n Novelty in light of previous multi-stream networks (R3, R2)\n Not clear if this kind of decoupling works well or is of broader interest beyond future frame prediction (R3) AC comment: I don\u00d5t know if this is too serious a concern; to me the problem seems important enough -- making an advancement that improves a single relevant problem is a contribution\n Separation of motion and content already prevalent in other applications, e.g., pose estimation (R2)\n UCF-101 results are not so convincing (R3)\n Clarity could be improved, not written with general audience in mind (R2)\n Concern about static bias in the UCF dataset of the learned representations (R1, R2, R3) AC comment: The authors ran significant additional experiments to respond to this point\n \n The authors provided a comprehensive rebuttal which convinced R1 to upgrade their score. After engaging R2 and R3 in discussion, both said that the paper had improved in its subsequent revisions and would be happy to see the paper pass. The AC agrees with the opinion of the reviewers that the paper should be accepted as a poster."}, "review": {"Skh0A2atf": {"type": "rebuttal", "replyto": "SJl4Th6Ff", "comment": "I see. The score of 31.5 in their paper is computed from only the moving pixels. In figure 4, we evaluate on all pixels. If you look at the last table  in their supplementary material (Table 5), they show the evaluation on the all the pixels, and the score in the first time step is 27.2. We also have evaluation on moving pixels in the supplementary material figure 13. We use a different optical flow algorithm to obtain the moving pixels, so their score is actually higher than 31.5 for the first time step.", "title": "Clarification"}, "SJl4Th6Ff": {"type": "rebuttal", "replyto": "Byo3KrDKG", "comment": "Thanks for the prompt reply.\nI think I understood this from your paper. But even considering that the model that you tested was the 2nd best model available for download, the scores for Mathieu et al. in Fig. 4 still seem too low.\n\nAm I making a mistake in reading this, or do you obtain a PSNR of ~27 for the first frame instead of 31.5 for the second best model reported by Mathieu et al.? If it is the latter, it's unclear what is the reason for this discrepancy. \n\nYour point that MCnet performs equally well whether trained on Sports1M or UCF-101 is well taken. I just have doubts regarding the score comparisons.", "title": "Re: Clarification."}, "Byo3KrDKG": {"type": "rebuttal", "replyto": "BJ4g1HwYf", "comment": "Thanks for your question. As mentioned in the paper, we mainly show our model trained on Sports 1M and tested on UCF101 against their model trained and tested the same way (That would be their 2nd best model numerically, and the model they use to generate the qualitative evaluation. We acquired the already trained model from their website). We were not able to acquire the best model (2nd best + finetuning on UCF101) from the authors. We mention this in the paper. We attempted to train such model with their code, but didn\u2019t achieve the same results. The main message in the experiments with the UCF101 dataset, however, is that out model does not need to specifically train with such dataset to perform well (trained on Sports 1M and test on UCF101), and in fact, it performs similarly as if we trained with UCF101. Hope this answered your question.", "title": "Clarification."}, "BJ4g1HwYf": {"type": "rebuttal", "replyto": "rkEFLFqee", "comment": "For first frame prediction, Mathieu et al. report a PSNR of 32 for their best model on UCF-101. However, in your Fig. 4, the result for Mathieu et al. for the first frame is around 27. Can you please explain what is the difference between the two? ", "title": "Clarification reg. results for Mathieu et al's method  "}, "S1ThseFve": {"type": "rebuttal", "replyto": "r1xpCSXDx", "comment": "Thank you very much.", "title": "on response to rebuttal"}, "Sy5wieYDx": {"type": "rebuttal", "replyto": "B1ZC3GxDe", "comment": "Thank you very much. I will reflect your suggestion in the final version of the paper.", "title": "on Final comments"}, "B1jen53Ll": {"type": "rebuttal", "replyto": "rkEFLFqee", "comment": "We appreciate insightful and constructive comments by reviewers.\n\nWe have updated the results on UCF101 dataset. We found a better hyper-parameter setting for \\beta (previously it was 0.01, and now it is 0.001 for all of our networks and baselines). We also changed the qualitative results to videos with large motion as determined by the motion quantities measure suggested by Reviewer1. In addition, we added grids and plot high-amplitude optical flows to help visualize the motion and comparison. Finally, we have updated the videos in the project website for easier comparison of the generated videos; these videos show that our method also works quite well compared to the baseline methods for videos with large motions.\n\nPlease check our project website: https://sites.google.com/a/umich.edu/rubenevillegas/iclr2017\n\nExample prediction results on UCF 101: https://www.youtube.com/watch?v=fHH4xA_2Xn4&index=1&list=PL-3H6EwkxufGLTdE88QkhlgBsdl6b3o9c\n\nNovelty [Reviewer2, Reviewer3]\nThe contribution of this paper is a proposal of simple yet effective end-to-end architecture for pixel-level video prediction.  Motion and content separation has been considered as one of key components for dealing with video data in the task of action recognition (Simonyan and Zisserman, 2014), but such architecture has not been investigated for the task of pixel-level video prediction in an unsupervised manner. In particular, we find the motion-content separation intriguing as it is relevant to disentangling properties in neural networks, as well as the two-stream hypothesis in neuroscience (Goodale and Milner, 1992). We show that motion and content separation can be achieved without additional supervision by employing asymmetric encoders or specialized mechanisms, and our model achieves performance improvement over the previous arts and baselines. To our best knowledge, this is the first attempt that directly addresses motion and content separation in video prediction, we believe that the paper would provide useful insights for the research community.\n\nUCF101 Motion Disambiguation [Reviewer1, Reviewer2, Reviewer3]\nWe agree that there is a static bias in the UCF101 dataset which can be seen by the high performance of the simple copy-previous-frame baseline suggested by Reviewer1. To address this problem, we evaluated on only the moving areas in the target videos. We follow the same procedure as Mathieu et al. (2015) by replacing pixels below a motion threshold of .2 with zeros where the motion is normalized to [0,1]. The motion is measured using DeepFlow optical flow (Weinzaepfel et al., 2013). Additionally, we follow a suggestion from Reviewer1 and separate the data by percentiles based on measuring the overall quantity of motion by computing the l2 norm of time differences. Please take a look at section C in the supplementary material for the effect of such evaluation. These results address the concerns that our algorithm may simply copy the last observed frame; the evaluation shows that as more motion is present, our algorithm becomes better than the simple copy last frame baseline, and it is still better compared to the other baselines. \n\nTypos, clarifications, and additional comments [Reviewer2]\n-    We have added a section in the supplementary material with details about Adversarial Training (Section D).\n-    Equation (1): c is the memory cell in LSTM that retains information of the dynamics observed through time. In this case it is a tensor for convolutional LSTM. This has been addressed in the paper.\n-    We fixed the mentioned typos in the paper.\n\nReference:\nGoodale, M.A. and Milner, A.D. (1992). \"Separate visual pathways for perception and action\". Trends Neurosci. 15 (1): 20\u20135.", "title": "Rebuttal"}, "B12lQoWEe": {"type": "rebuttal", "replyto": "H1iCTjUQe", "comment": "Thank you for your insightful question. \n\n1) The main reason we completely separate the pathways is so our network learns to focus on the locally observed motion when making the next frame prediction (moving the pixels that need to be moved) and making the task or network should do easy to learn.\n\n2) Motion is not completely dependent on appearance, but more on pixel shifts. If there is some illumination change happening in the image, it can have the visual effect of color change, but motion is not actually happening.\n\n3) If we funnel appearance features in the motion encoder, it may keep our network from focusing on learning motion features. Image prediction noise in addition to the noise coming from image difference input can confuse the network and keep it from focusing on representing noise. Such noise can result in similar performance as convolutional LSTM.", "title": "On the reason for pathway separation"}, "BJY9ZAY7x": {"type": "rebuttal", "replyto": "S1zgRoUQe", "comment": "Thanks for your question!\nWe did not run into any overfitting issues. We augmented the data by doing horizontal mirroring which seems we did not mention in the paper. We will update this in the next revision.", "title": "On overfitting"}, "H1iCTjUQe": {"type": "review", "replyto": "rkEFLFqee", "review": "Why are the pathways completely separated? Isn't motion dependent on appearance? Could anything be gained by funneling appearance features earlier into the motion network?The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.\n\nThe paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation.\n\nDetails :\n\nThe paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.\n\nExample : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are \"additional\" losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.\n\nEquation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c'.\n\nThe residual nature of the layer should be made more apparent in equation (3).\n\nThere are several typos, absence of articles and prepositions (\"of\" etc.). The paper should be reread carefully.\n", "title": "Independence", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HySrJeGNl": {"type": "review", "replyto": "rkEFLFqee", "review": "Why are the pathways completely separated? Isn't motion dependent on appearance? Could anything be gained by funneling appearance features earlier into the motion network?The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.\n\nThe paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation.\n\nDetails :\n\nThe paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.\n\nExample : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are \"additional\" losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.\n\nEquation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c'.\n\nThe residual nature of the layer should be made more apparent in equation (3).\n\nThere are several typos, absence of articles and prepositions (\"of\" etc.). The paper should be reread carefully.\n", "title": "Independence", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SysikHMQe": {"type": "rebuttal", "replyto": "HJZ1EkWmg", "comment": "The motion encountered in KTH has a more cyclic (predictable) nature and less action categories compared to the motion in UCF101. Because of the highly unpredictable nature of the dataset, and the large number of human actions, our network makes more conservative predictions of the motion while at the same time trying to keep the observed image structure (content) from being lost.", "title": "Comment on very little motion"}, "BJQ2Zzf7l": {"type": "rebuttal", "replyto": "HJ-oGHIWe", "comment": "Thanks for your suggestion. We are currently attempting to use the published code for this work to train on the datasets in our submission. We will update the submission as soon as we get results using this method.", "title": "Relevant work"}, "S1cWZfG7x": {"type": "rebuttal", "replyto": "B19Da9hMx", "comment": "Thank you for your question, and helpful comments. The answers to your question are as follow:\n\n1)\n- Answered in updated submission\n\n2)\n- Answered in updated submission\n\n3)\n- The reason we experiment on KTH dataset is to explore the ability of our network to learn to predict cyclic human motion well.\n- The reason we experiment on UCF101 dataset is to compare against the state-of-the-art unsupervised video prediction algorithm proposed by Michael Matheiu et. al.\n-  As you point out, applying video prediction algorithms in datasets with sports instead of random videos does bring a dependency on the actions observed in the dataset. As a result, video prediction algorithms may internally learn to identify the individual actions and make predictions based on the identified action. Such dependencies, however, can help video prediction algorithms learn features specific to an action label which can be applied for the task of action recognition of such actions. A downfall of applying our algorithm to human action videos is that the learned features may not be as general as if we applied it on random video prediction.\n\n4)\n- We have not performed experiments for action recognition. In the VGAN paper, the discriminator network is used as a pre-trained model for action recognition. In our case, we would use our generator's learned features for the same task. As per your request, we will attempt to train a prediction model that observes 32 frames (similar to VGAN) and predicts 1, proceed to fine-tune for action recognition, and report the results once we have gathered them.\n", "title": "Answers to your questions."}, "HJZ1EkWmg": {"type": "review", "replyto": "rkEFLFqee", "review": "The proposed method shows strong results with realistic motion on KTH.  However on the less constrained UCF-101, the qualitative examples show realistic frames but very little motion, especially related to the foreground action of interest as would be desired.  Can you comment on this? This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets.\n\nThe idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction.\n\nWhile results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset.  The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question.\n\nOverall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.", "title": "static quality of MCNet on UFC-101", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJF0H7M4g": {"type": "review", "replyto": "rkEFLFqee", "review": "The proposed method shows strong results with realistic motion on KTH.  However on the less constrained UCF-101, the qualitative examples show realistic frames but very little motion, especially related to the foreground action of interest as would be desired.  Can you comment on this? This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets.\n\nThe idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction.\n\nWhile results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset.  The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question.\n\nOverall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.", "title": "static quality of MCNet on UFC-101", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "B19Da9hMx": {"type": "review", "replyto": "rkEFLFqee", "review": "Very clear and interesting paper! I have a few questions before writing my review:\n\n1) The qualitative results seem to only include (almost) static cameras. Could you please include new examples with moving cameras (fairly common in UCF-101, some KTH videos too) and discuss? I am especially interested in seeing how the good quantitative results translate in this setting, in particular for large camera movements. I wonder if your motion encoder can handle this complexity, or if it entangles this and fails (following the same reasoning you applied to justify decoupling appearance and motion). The latter case would then suggest potential improvements by separating further in two motion decoders, as often done in more traditional computer vision (where decoupling camera / background motion from the foreground helps, even when doing simple camera motion compensation).\n\n2) Related to the first question, and in order to grasp better what the actual quantitative improvements represent (although the small qualitative results partially address this already), could you please compare to a stupid baseline where all the pixels from the last frame are simply copy-pasted? From the qualitative examples, it seems that the focus on small motions might reflect a bias of the datasets (or at least their subset where frame prediction methods truly differ), in which case it would be helpful to know if the models do not simply learn to be very conservative (as there is no explicit regularization discouraging that). I personally find the good results of all frame prediction methods very static :-)\n\n3) Could you discuss a bit why you only experiment on human action recognition benchmarks and the implied potential dependency on action labels versus applying your approach on random videos, as done for instance by Vondrick et al. with their VGAN model (NIPS'16)?\n\n4) Have you done any experiment to see if your MCNet+RES model has learned any useful representation for action recognition and compared to the aforementioned VGAN results on UCF-101? I am hopeful, as your architecture and working assumptions are similar to the two-stream network of Simonyan and Zisserman (as you mention), and this model is among the state of the art in action recognition (including on UCF-101, especially the latest variants like the Temporal Segment Networks of Wang et al at ECCV'16).1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!", "title": "Pre-review questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkUoXJW4e": {"type": "review", "replyto": "rkEFLFqee", "review": "Very clear and interesting paper! I have a few questions before writing my review:\n\n1) The qualitative results seem to only include (almost) static cameras. Could you please include new examples with moving cameras (fairly common in UCF-101, some KTH videos too) and discuss? I am especially interested in seeing how the good quantitative results translate in this setting, in particular for large camera movements. I wonder if your motion encoder can handle this complexity, or if it entangles this and fails (following the same reasoning you applied to justify decoupling appearance and motion). The latter case would then suggest potential improvements by separating further in two motion decoders, as often done in more traditional computer vision (where decoupling camera / background motion from the foreground helps, even when doing simple camera motion compensation).\n\n2) Related to the first question, and in order to grasp better what the actual quantitative improvements represent (although the small qualitative results partially address this already), could you please compare to a stupid baseline where all the pixels from the last frame are simply copy-pasted? From the qualitative examples, it seems that the focus on small motions might reflect a bias of the datasets (or at least their subset where frame prediction methods truly differ), in which case it would be helpful to know if the models do not simply learn to be very conservative (as there is no explicit regularization discouraging that). I personally find the good results of all frame prediction methods very static :-)\n\n3) Could you discuss a bit why you only experiment on human action recognition benchmarks and the implied potential dependency on action labels versus applying your approach on random videos, as done for instance by Vondrick et al. with their VGAN model (NIPS'16)?\n\n4) Have you done any experiment to see if your MCNet+RES model has learned any useful representation for action recognition and compared to the aforementioned VGAN results on UCF-101? I am hopeful, as your architecture and working assumptions are similar to the two-stream network of Simonyan and Zisserman (as you mention), and this model is among the state of the art in action recognition (including on UCF-101, especially the latest variants like the Temporal Segment Networks of Wang et al at ECCV'16).1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!", "title": "Pre-review questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJ-oGHIWe": {"type": "rebuttal", "replyto": "rkEFLFqee", "comment": "This work is similar to the paper published in ICLRw2016 https://arxiv.org/abs/1511.06309, which decomposes a video into spatial component and temporal changes (motion), and also uses a convolutional LSTM to encode motion cues. The proposed architecture should be compared with the aforementioned one, and the results compared.", "title": "Missing relevant reference and comparison?"}}}