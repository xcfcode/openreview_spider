{"paper": {"title": "FastText.zip: Compressing text classification models", "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"], "summary": "Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "keywords": ["Natural language processing", "Supervised Learning", "Applications"]}, "meta": {"decision": "Reject", "comment": "The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited."}, "review": {"Sya6_wOSg": {"type": "rebuttal", "replyto": "B1Mcr6c4l", "comment": "First, we would like to thank the reviewer.\n\nThe main concern stated by Reviewer 3 is the lack of novelty of the paper. However, we are not aware of previous work using similar techniques for compression of text classification models. We are happy to examine any paper in that field that the reviewer could refer to. While we do agree that similar techniques were applied to other domains, such as computer vision, one of the main contributions of this paper is to perform a careful empirical evaluation for text classification. In particular, we evaluated many different combinations of the methods discussed in the paper, on 9 datasets with different characteristics (see Tables 5, 6, 7, 8 and 9 of the appendix).\n\n** \"From a machine learning point of view, the proper baseline to solve this problem...\"\n\nWe believe that this approach is more complex that the one described in the paper. Moreover, this technique cannot be used to compress existing models. Again, a reference providing results comparable to our approach in terms of the performance between compression rate and accuracy would be welcomed. \n\n** \"what if one simply uses fewer vocabulary elements (e.g based on subword units ...)\"\n\nWe agree that this technique could be used additionally to the ones described in the paper. We actually discuss this in the last paragraph of the \u201cFuture work\u201d section.\n\n** \"However, the machine learning contributions of the paper are marginal to me.\"\n\nWe do not believe that only the contributions cast as a machine learning objective are of interest to ICLR (see the list of relevant topics, e.g. \"Implementation issues, parallelization, software platforms, hardware\").\n\n** \"The paper cites an ArXiv manuscript by Carreira-Perpinan and Alizadeh (2016)\"\n\nWe do not cite any paper by Carreira-Perpinan and Alizadeh.\n\n** \"In Fig 2 does the square mark PQ or OPQ? The paper does not distinguish OPQ and PQ properly at multiple places especially in the experiments.\" and \"The use of (optimized) product quantization for approximating inner product is not particularly novel.\"\n\nAs stated in the first paragraph of section 4.1, \u201cwe adopt the normalized PQ (NPQ) for the rest of this study.\u201d Note, this is a variation that departs from PQ and OPQ by the way we treat the magnitude information, which to the best of our knowledge is new in this context. \n\n** \"The paper argues the wide and shallow models are the state of the art in small datasets.\"\n\nWe make the claim that \u201clinear classifiers remain competitive with more sophisticated, deeper models\u201d, supported by the results reported by Wang and Manning (2012) and Joulin et al. (2016).", "title": "Rebuttal"}, "Hk3L4jZ4e": {"type": "rebuttal", "replyto": "HJe8ExgVe", "comment": "First we would like to thank the reviewer.\n\nShe/he seems to question the potential impact research-wise:\n\nShowing that it is possible to obtain state-of-the-art text classifiers that fits in less than 100KB, is a significant achievement that has not been published before (to the best of our knowledge). \n\nModel quantization is a very active field of research and ICLR 2016 best paper award was on that subject, i.e., \u201cDeep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding\u201d by Han et al. Our work provides a simple approach that can be used as a strong baseline for any future work in this direction.\n\nWe answer her/his questions below:\n\n\"- it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\"\n\nWe will add a formula measuring the size of a model as a function of the different parameters. \n\nOn small output space the model size is mostly due to A. On YFCC100M, it is due to both A and B.\n\nThe dictionary cost around 1Mb to store which is negligible on big models but becomes significant when quantizing to hundreds of Ko.\n\nWe dump in memory the model as it will be used during testing and measure its size. It doesn\u2019t account for the size of the executable.\n\n\u201csome parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\u201d\n\nWe agree that we should extend and clarify the greedy algorithm.\n\nThe \u201coverhead of a few hundreds of KB\u201d:\nAt test time, we only have access to the hashing function that compute the indices before pruning and we need to verify if a given hashing value has been removed by the pruning. \n\nThis sum up to storing an array V of K uint_8, where V[index_after_pruning] = index_before_pruning. This leads to an overhead of a few hundreds of KB. At test time, we need to explore this array efficiently, hence the binary search.\n\nThis overhead can be removed by using Bloom filters to test if a given index has been kept or not.", "title": "Rebuttal"}, "S1bngLTQe": {"type": "rebuttal", "replyto": "Hki8CTyQg", "comment": "Thank you for your question.\n\nIn table 9 of the appendix, we measure the quality of our model with different size of embeddings.  We indeed observe that using larger embedding with compression allows to obtain models that are significantly smaller while being more accurate. We did not make it a central point of the paper as it is not the main focus and may clutter the message.\n\n", "title": "Re: Question"}, "Sk8EeIame": {"type": "rebuttal", "replyto": "SJ-gbbw7g", "comment": "Thank you for your questions.\n\nA standard linear model without quantization takes from 1.6Mb (AG) to 45Mb (dbpedia) on a dataset with a small output space. On YFCC100M where the output space is large, a linear model requires ~350Gb (since it has around 300K*300K parameters). \n\nAll the compression methods presented in this paper can be used on linear models if the output space is small. It is less clear on datasets with a large output space. \n\nConcerning the use of \u201cbag-of-words + SVM with polynomial kernel of degree 2 or 3\u201d, we are not sure what the reviewer is referring to. The most related work we are aware of, is the paper by Sanchez et al., \u201cImage Classification with the Fisher Vector: Theory and Practice\u201d where the interaction between SVM and PQ is analyzed with Fisher kernel (could be a rbf/polynomial kernel). It is used on the embeddings to capture higher order information (possibly ngrams in the case of text?). However this work does not compress the model but the representations (that is it conserve the original embeddings, increase it with a kernel and then compress the output). On the other hand, we are interested in compressing the model. If the reviewer is thinking of a different work that shows that this approach can be successfully applied to the compression of text classification models, we will add a comparison in our experiments. \n\nFinally, we focus on the comparison with character level CNNs because in theory, their number of parameters is independent of the vocabulary size, making them natural memory efficient text classifiers (Xiao & Cho, 2016). \n", "title": "Re: comparison to non-deep baselines"}, "SJ-gbbw7g": {"type": "review", "replyto": "SJc1hL5ee", "review": "It is well-known that non-deep method still performs well on the text classification task (particularly for dataset with few classes as in Section 4.1). Could the authors also report results on non-deep baselines, such as a simple bag-of-words + SVM with polynomial kernel of degree 2 or 3 (thus emulating bigrams and trigrams)? How does this kind of baseline compare to fastText.zip in terms of accuracy and model size? I believe that such baseline models can be compressed using simple tricks as well.This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.\n\nThis paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.\n\nThe paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:\n  - a straightforward variant of PQ for unnormalized vectors,\n  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,\n  - hashing tricks and bloom filter are simply borrowed from previous papers.\n\nThese techniques are quite generic and could as well be used in other works. \n\n\nHere are some minor problems with the paper:\n\n  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\n  \n  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\n  \n\nOverall this looks like a solid work, but with potentially limited impact research-wise.\n", "title": "comparison to non-deep baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJe8ExgVe": {"type": "review", "replyto": "SJc1hL5ee", "review": "It is well-known that non-deep method still performs well on the text classification task (particularly for dataset with few classes as in Section 4.1). Could the authors also report results on non-deep baselines, such as a simple bag-of-words + SVM with polynomial kernel of degree 2 or 3 (thus emulating bigrams and trigrams)? How does this kind of baseline compare to fastText.zip in terms of accuracy and model size? I believe that such baseline models can be compressed using simple tricks as well.This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.\n\nThis paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.\n\nThe paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:\n  - a straightforward variant of PQ for unnormalized vectors,\n  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,\n  - hashing tricks and bloom filter are simply borrowed from previous papers.\n\nThese techniques are quite generic and could as well be used in other works. \n\n\nHere are some minor problems with the paper:\n\n  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\n  \n  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\n  \n\nOverall this looks like a solid work, but with potentially limited impact research-wise.\n", "title": "comparison to non-deep baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hki8CTyQg": {"type": "review", "replyto": "SJc1hL5ee", "review": "Did you also explore increasing the dimensionality of embeddings (e.g., doubling)? I.e., with 8x compression, can you achieve a model that is both smaller and noticeably more accurate?The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, https://arxiv.org/pdf/1512.06473v3.pdf), which similarly incorporates fine tuning to mitigate losses due to quantization error.\n\nAs such, one criticism of the paper is that it is a more-or-less straightforward application of techniques that have already been shown to be effective elsewhere in the model compression literature, and so isn't particularly surprising or deep from a technical perspective. However, this is as far as I am aware the first work applying these techniques to text classification, and the results are strong enough that I think it will be of interest to those working on models for text-based tasks.", "title": "Question", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1PiU1UVg": {"type": "review", "replyto": "SJc1hL5ee", "review": "Did you also explore increasing the dimensionality of embeddings (e.g., doubling)? I.e., with 8x compression, can you achieve a model that is both smaller and noticeably more accurate?The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, https://arxiv.org/pdf/1512.06473v3.pdf), which similarly incorporates fine tuning to mitigate losses due to quantization error.\n\nAs such, one criticism of the paper is that it is a more-or-less straightforward application of techniques that have already been shown to be effective elsewhere in the model compression literature, and so isn't particularly surprising or deep from a technical perspective. However, this is as far as I am aware the first work applying these techniques to text classification, and the results are strong enough that I think it will be of interest to those working on models for text-based tasks.", "title": "Question", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}