{"paper": {"title": "Multi-Agent Collaboration via Reward Attribution Decomposition", "authors": ["Tianjun Zhang", "Huazhe Xu", "Xiaolong Wang", "Yi Wu", "Kurt Keutzer", "Joseph E. Gonzalez", "Yuandong Tian"], "authorids": ["~Tianjun_Zhang1", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1", "~Kurt_Keutzer1", "~Joseph_E._Gonzalez1", "~Yuandong_Tian1"], "summary": "", "abstract": "Recent advances in multi-agent reinforcement learning (MARL) have achieved super-human performance in games like Quake 3 and Dota 2. Unfortunately, these techniques require orders-of-magnitude more training rounds than humans and don't generalize to new agent configurations even on the same game. In this work, we propose Collaborative Q-learning (CollaQ) that achieves state-of-the-art performance in the StarCraft multi-agent challenge and supports ad hoc team play. We first formulate multi-agent collaboration as a joint optimization on reward assignment and show that each agent has an approximately optimal policy that decomposes into two parts: one part that only relies on the agent's own state, and the other part that is related to states of nearby agents. Following this novel finding, CollaQ decomposes the Q-function of each agent into a self term and an interactive term, with a Multi-Agent Reward Attribution (MARA) loss that regularizes the training. CollaQ is evaluated on various StarCraft maps and shows that it outperforms existing state-of-the-art techniques  (i.e., QMIX, QTRAN, and VDN) by improving the win rate by 40% with the same number of samples. In the more challenging ad hoc team play setting (i.e., reweight/add/remove units without re-training or finetuning), CollaQ outperforms previous SoTA by over 30%. ", "keywords": ["multi-agent reinforcement leanring", "ad hoc team play"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a method for collaborative multi-agent learning and ad-hoc teamwork. The paper includes extensive empirical results across multiple environments (including one of known outstanding high difficulty) and repeatedly performs favourably in comparison to a suitable set of state of the art methods. The proposed method is motivated by theoretical analysis, which was considered interesting but its connection to the method in the initial paper was weak. \n\nOverall, there are remaining concerns which have not been fully addressed in the discussion phase. The authors' responses and discussion with the reviewers should be utilised to improve the material's presentation and to clarify the theory-empirical connection in future revisions of the paper."}, "review": {"LC6KEYKa8An": {"type": "review", "replyto": "GVNGAaY2Dr1", "review": "To address the ad hoc team play, the authors propose a residual term of Q function, which additionally considers the states of nearby agents. A novel MARA loss is introduced to the residual term as a regularization to achieve the reward assignment implicitly. The proposed CollaQ could be easily built on QMIX and trained end-to-end. CollaQ outperforms other baselines on various tasks with the ad hoc team play setting. \n\nThe paper is very clear and well-structured. To the best of my knowledge, the MARA regularization is novel enough. The Ad-hoc MARL is an important problem in real-world applications but has not been fully studied. The interactive term with regularization is a practical and promising method to solve this problem and could be followed by other researchers.\n\nHowever, I still have some concerns:\n\nFirst, the theoretical analysis of reward assignment is not close to the implementation of CollaQ. There is no real assignment mechanism. A MARA loss is derived from the theoretical analysis to achieve the reward assignment implicitly, but the MARA loss could be more straightforwardly interpreted as that the Q value should be equal to the individual value when the agent cannot observe other agents. From this perspective, the complex reward assignment is not necessary. Moreover, CollaQ is built on QMIX. However, the individual value function in QMIX does not estimate a real expected return, and the value has no meaning. Is the theoretical analysis of reward assignment still valid in QMIX?\n\nI do not find any experiments to support the claim that \"agents using CollaQ would first learn to solve the problem pretending no other agents are around using Qalone then try to learn interaction with local agents through Qcollab.\" I think it is over-claimed and should be removed. Splitting the end-to-end learning process into two learning stages might harm the learning. \n\nThe visualizations in Fig 3 are helpful to understand how CollaQ works, but they are special cases. Statistical results are more convincing to verify how CollaQ influences the decision.\n\nAt the test time of StarCraft, are the IDs shuffled at each timestep or only at the first timestep of an episode?\n\n----------Update after author response----------\n\nI thank the authors for the detailed response. Most of my concerns have been addressed, and I decide to keep my score.\n", "title": "Reviews", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "GGOvotlRmL": {"type": "rebuttal", "replyto": "Hhg7_JQr_gP", "comment": "We thank the reviewer for the comments. \n\n**Q1**: We think that the intuitive explanation that MARA loss is to eliminate the ambiguity and to make the solution unique is perfectly valid. However, we would like to emphasize several things here: \n1. While (4) looks like a \u201cnatural\u201d formulation, there are a lot of other natural formulations available (e.g., how about we have a $Q^{alone}$ function that takes as much input as possible? Instead of adding $Q^{alone}$ and $Q^{collab}$, why not multiple them together?). The theory helps us understand the principle behind it and nails down the precise formulation we use here. \n2. Without the theory, there could be other possible decompositions/explanations of the algorithm. The theory provides one possible intuition/explanation behind CollaQ. The path to the solution, in addition to the solution itself, could trigger some interesting future research as well and should be regarded as a contribution of the paper. This is also recognized by other reviewers (e.g., R1: \u201cGood theoretical analysis and compliant experiment performance\u201d). For example, while currently, we don\u2019t solve Eqn. 1 directly, the formulation itself could trigger other thoughts and connections to other existing works (e.g., multi-agent centralized planning). \n3. The semantic meaning and intuition of our algorithm CollaQ are very different from previous work as we stated in the common questions. \n\nWe have already added one section to the paper explaining the intuition and connection. We plan to leave a more in-depth analysis of the two to future work. \n\nNote that reward decomposition theory is only one part of our contributions. Our practical algorithm (motivated by intuitions and theory) CollaQ achieves impressive empirical performance not only on traditional multi-agent games like StarCraft II, but also outperforms all baselines by a good margin in our newly proposed multi-agent ad hoc setting. We think that those two parts should also be considered when we are trying to evaluate the novelty/contribution of the paper. Note that this is also mentioned by other reviewers (e.g., R3: \u201cTo the best of my knowledge, the MARA regularization is novel enough. The Ad-hoc MARL is an important problem in real-world applications but has not been fully studied.\u201d, R2: \u201cthe main advance is in terms of scaling up to real-world scenarios rather than having a better explicit algorithm to manage reward decomposition at runtime in an online learning setting.\u201d). \n\n\n**Q2**: We don\u2019t require the dimension of local state $S^{local}$ to be the same for every agent. We use transformer architecture to put attention on the important part from observation $o_{i}$ that influences the agent\u2019s value function.", "title": "Author Response for R4"}, "z4CdBqVDuv": {"type": "rebuttal", "replyto": "rabHEzOzQQL", "comment": "We thank R1 for all the comments. Please also refer to the common questions above for the answer to the remaining questions\n\n**Q1**: In Theorem 1, does observation cover $S^{i}_{local}$ a limitation?\n\n**A**: We thank the reviewer for this valid concern. This is actually one of the limitations of Theorem 1. However, we found that CollaQ works well empirically even in complex games like StarCraft II. We leave this limitation to future work.\n\n**Q2**: There are too many kinds of rewards including perceived reward, local reward, external reward.\n\n**A**: Please see the common questions section. \n\n**Q3**: Can you provide a brief algorithm flowchart and pseudo codes?\n\n**A**: The actual algorithm is simple so we didn\u2019t draw an algorithm flowchart. The DQN training involves sampling episodes from a trajectory buffer and Bellman update using those samples. We adopt the DQN training paradigm (QMIX head on top) with the objective function defined in Eq. 5. \n", "title": "Author Response to R1"}, "t9r4HFHabb_": {"type": "rebuttal", "replyto": "GVNGAaY2Dr1", "comment": "We are thankful for the reviewers\u2019 insightful comments. We address common concerns here and will reply to each reviewer separately for their specific comments. \n\n**Common Questions**:\n\n**Contribution.** \n\n[Some summarization of our work here.]\n\n1. CollaQ proposes to formulate the multi-agent collaboration problem as a two-stage optimization. \n\n2. Derived from the theorem, we present a practical algorithm that achieves the SoTA performance in complex games like StarCraft II. \n\n3. We also try to solve ad hoc team play in a complicated game domain with our new framework and algorithm above. Empirical results of CollaQ on StarCraft II show good performance over the current algorithms.\n\n4. To the best of our knowledge, we are the first to study ad hoc team play in a complex game domain without requiring sophisticated online learning at test time or strong domain knowledge of possible teammates.\n\n**[R4]** To our best knowledge, we haven\u2019t seen a similar formulation for multi-agent collaboration that uses the reward assignment to model the collaborative behavior of agents in prior works and believe this is a substantial contribution to the community. \n\n**[R2]** We would like to emphasize that: 1. Our theorem on formulating the multi-agent ad hoc team play as a two-stage is by itself novel and could trigger future research. 2. Motivated by the theorem, the algorithm CollaQ (with MARA loss) is also novel and effective in realistic environments. 3. We try to solve ad hoc team play in a more complicated game domain with the novel framework and algorithm above. The experiments achieved strong results. \n\n**What\u2019s the connection between the theory and practical algorithm (CollaQ) [R2, R4]?**\n\nIn summary, we use the theory as motivation and derive a general end-to-end algorithm that can work empirically in complex/realistic environments. Besides motivating the practical algorithm, the theorem shows a bigger picture of the framework and can serve as a separate contribution to trigger future research along the direction. However, as mentioned in Sec 2.1 and 2.2, the optimization for reward assignment is complex and cannot be done efficiently. How to conduct such optimization efficiently in complex environments is beyond the scope of this paper. Thus, we didn\u2019t conduct experiments on explicitly doing optimization on reward assignment and leave this for future work. Instead, derived from the theorem, CollaQ uses a theory-inspired Q decomposition and serves as a practical algorithm that can work in realistic environments. \n\nWithout the theory, if we just start from Eq. 4, then we will  \n\n1. Completely lose the motivation why we have these two specific decompositions. Why is $Q^{alone}$ only allowed to take the individual state of agent $i$ as the input? Why does $Q^{collab}$ need to have MARA loss? These questions are well-motivated by Eq. 3 and the theorem. \n\n2. Lose a potentially bigger picture of reward assignment underlying Eqn. 4. The reward assignment formulation for multi-agent collaboration (Eqn. 1) could potentially trigger future research, even if it cannot be solved efficiently for now. \n\n**Related Works [R1, R4]**\n\nWe thank the reviewers for the related works. We\u2019ll definitely update the paper accordingly in the next revision. We just want to clarify the difference between CollaQ and certain related works on decomposition network, social influence, reward shaping:\n\nDecomposition Network: There is other literature that tries to decompose the observation space: ASN [2] decomposes the observation space of each agent trying to capture semantic meaning of actions, DyAN [1] adopts similar architecture in a curriculum domain. We would like to point out that although the network structure shares some similarities, the semantic meaning of each component is different. CollaQ is well-motivated by Theorem 1 and the derived MARA loss is also novel. Thus the algorithm CollaQ shares little similarity except for the network structure with the papers mentioned before. Fig. 3 also supports this idea.\n\nSocial Influence: the SSD [3] paper gives the agent an extra intrinsic reward when its action has huge influence on others. In contrast, CollaQ doesn\u2019t need any intrinsic reward for learning collaboration behaviors. In addition to that, CollaQ doesn\u2019t need extra monte-carlo simulation in the environments, which is not possible in games like StarCraft II. This simulation is needed for SSD to find the most influential action of an agent. \n\nSocial dilemma [5]: social dilemma often assumes each agent has their independent reward, but in our setting, the agents are fully cooperative and share the same reward.\n\nReward shaping [4]: Although our theorem is motivated by reward assignment, our practical algorithm CollaQ doesn\u2019t do reward shaping explicitly. The only reward it receives is the external reward (e.g., for SMAC, whether the team wins against the opponent team). \n\n(to be continued)", "title": "Author Response to Common Questions (1/2)"}, "zyClxPGyuA": {"type": "rebuttal", "replyto": "xk6cb4IbgjT", "comment": "We thank R4 for all the comments. Please also refer to the common questions above for the answer to the remaining questions\n\n**Q1**: How to measure the distance of states and \"nearby agents\" should be more rigorously defined.\n\n**A**: Please see the common questions part. \n\n**Q2**: What\u2019s the meaning of math? Is there any implication beyond that the more the problem decomposes into local environments, the easier it becomes to solve it distributedly?\n\n**A**: With theorem 1, we can also see that it is valid to decompose the agent\u2019s value function into $Q^{alone}$ and $Q^{collab}$. Thus we can derive our algorithm CollaQ with this important intuition. Please also check the common questions. \n\n**Q3**: Theorem 1 doesn't define $R_{max}$ and Lemma 3 doesn't define the distance between states. Please clarify notations.\n\n**A**: We thank the reviewers for pointing out. We\u2019ll fix them in the next revision of the paper.\n\n**Q4**: Why in the resource collection scenario, CollaQ only compares to IQL and not to QTRAN/VDN/QMIX?\n\n**A**: There are two reasons: 1. Since the state of resource collection is fully observable, so IQL is enough for the agent to choose action. 2. We also provide results using QMIX on resource collection in Appendix Fig.9. QMIX doesn\u2019t work well compared to IQL. \n", "title": "Author Response to R4"}, "BQYbMWpbn_S": {"type": "rebuttal", "replyto": "LC6KEYKa8An", "comment": "We thank R3 for all the comments. Please also refer to the common questions above for the answer to the remaining questions\n\n**Q1**: Do we still need the complex reward assignment if MARA loss could be more straightforwardly interpreted as that the Q value should be equal to the individual value when the agent cannot observe other agents?\n\n**A**: This is a good point. Your explanation is perfectly valid. In fact, this is actually the intuition behind the reward assignment and the Theorem just conveys this idea in a more formal way, so that it can be developed further (e.g., why we want to decompose this way). Please also see the connection between theory and the algorithm part in common questions.\n\n**Q2**: Is theoretical analysis still valid combined with QMIX?\n\n**A**: QMIX takes each individual Q function as the input. The decompositional property of each Q is not affected by how it is being used on the top level.\n\n**Q3**: Do agents using CollaQ would first learn to solve the problem pretending no other agents are around using $Q^{alone}$ then try to learn interaction with local agents through $Q^{collab}$?\n\n**A**: We thank the reviewer for pointing this out. We will revise this sentence in the next revision. \n\n**Q4**: The visualization of Fig3. is a special case. Statistical results would be preferred.\n\n**A**: We would like to emphasize that Fig. 3 is not a special case (it is actually randomly sampled) and this phenomenon happens quite often in our observation. We thank the reviewer for this suggestion and could report some statistics in the next revision of the paper. The visualizations combined with Theorem 1 and Eq. 3 could to some extent make the point.\n\n**Q5**: Are the IDs shuffled at each timestep or only at the first time step of an episode?\n\n**A**: The agent IDs are shuffled only at the first time step of an episode. In another word, each episode has a different (but fixed) ID assignment.\n", "title": "Author Response to R3"}, "FRzxMR7d46O": {"type": "rebuttal", "replyto": "UTzNZNeS31D", "comment": "We thank R2 for all the comments. Please also refer to the common questions above for the answer to the remaining questions\n\n**Q1**: There is not a major new methodological or conceptual insight.\n\n**A**: Please see the contribution part of how we view this work.\n", "title": "Author Response to R2"}, "ARhSZjlV3l": {"type": "rebuttal", "replyto": "GVNGAaY2Dr1", "comment": "\n**What\u2019s the Reward structure and how to define $S^{i}_{local}$? [R1, R4]**\n\nExternal reward: The external reward is the reward shared by all the agents given by the environment. Please refer to the Basic Setting section in Sec. 2: $r_{e}: S \\times A_{1} \\times A_{2} \\times \u2026 \\times A_{K} \\to R$. \n\nPerceived reward: Given this shared external reward, depending on a specific reward assignment, each agent can receive a perceived reward that drives its behavior. If the reward assignment is properly defined, then all the agents can act based on the perceived reward to jointly optimize (maximize) the shared external reward. \n\nLocal reward set: By defining $S_{local}$ to be the subset of states that affect heavily on each agent i\u2019s optimal perceived reward, the agent $i$ can predict the optimal value function by only observing $S_{local}$. We then call this region the \u201clocal reward set\u201d of agent $i$ (see Lemma 3 in the Appendix). \n\nDefinition of \u201cnearby\u201d agent: We define the distance between states $s_{1}$ and $s_{2}$ by the minimum number of steps the agent needs to transit from $s_{1}$ to $s_{2}$ under any possible policies. Following this definition, we similarly say that if the distance between agent 1\u2019s state and agent 2\u2019s state is small, they are nearby.\n\n**We will make things more clear in the next revision.**\n\n**References:**\n\n[1] Wang, Weixun, et al. \"From Few to More: Large-Scale Dynamic Multiagent Curriculum Learning.\" AAAI. 2020.\n\n[2] Wang, Weixun, et al. \"Action Semantics Network: Considering the Effects of Actions in Multiagent Systems.\" arXiv preprint arXiv:1907.11461 (2019).\n\n[3] Jaques, Natasha, et al. \"Social influence as intrinsic motivation for multi-agent deep reinforcement learning.\" International Conference on Machine Learning. PMLR, 2019.\n\n[4] Devlin, Sam Michael. Potential-based reward shaping for knowledge-based, multi-agent reinforcement learning. Diss. University of York, 2013.\n\n[5] Leibo, Joel Z., et al. \"Multi-agent reinforcement learning in sequential social dilemmas.\" arXiv preprint arXiv:1702.03037 (2017).", "title": "Author Response to Common Questions (2/2)"}, "xk6cb4IbgjT": {"type": "review", "replyto": "GVNGAaY2Dr1", "review": "The paper studies a team of agents that collaborate to maximize a global objective, where all agents receive the same reward value as a function of their state and actions. The approach suggested here is to assign each agent a virtual (\"perceived\") reward function such that all the virtual rewards sum up to the actual reward. Then the problem from the point of view of agent i can be solved with local value functions that depend on the local state of agent i and its perceived reward.  It is shown that such a decoupled policy exists that approximates the optimal policy if the state structure decomposes \"well enough\" into local states. Experiments demonstrate the advantages of this approach on a resource collection game on the StarCraft Multi-Agent Challange, with dramatic improvements over existing methods for the scenarios that were tested. \n\nThe paper is mostly easy to follow, and is well-written and well motivated. The idea of this paper is nice and intuitive. Being less familiar with the literature, It's hard for me to judge how novel this idea is. It seems like there must exist multi-agent works that exploit the local structure of the interaction to reduce the dimensionality. Hence, as a start, the contribution of this paper can be enhanced by extending the literature review to include works of that kind, even if under different contexts. If indeed this idea is unique enough (and had a major impact on the design of the algorithm), then this is a significant contribution. Then, the other main issue is with improving the rigor of the presentation and the math. While it's clear what the results are showing, some guessing is needed to fill in some gaps. \n\nThe reward and state structure: I guess that the \"external rewards\" referred to above (1) and the \"same reward\" that all agents share are the same thing. Please clarify and unify the definitions. Then it's not clear what's going on with the dimensions of the local states. The local states appear in the second paragraph of Section 2 and are never properly defined, and also the local state spaces S_i are not well defined. Are the cardinalities of S_i and A_i the same for all i? otherwise, the constraint in (1) isn't clear. Additionally, it's not clear how can one measure distances of states that live in different state spaces (of different agents) as is often being done in the proofs (e.g., the proof of Theorem 1, the definition of D, and so on). On that note, the idea of \"nearby agents\" should be more rigorously defined. Ignoring these gaps, it looks like Theorem 1 basically shows that the better the problem decomposes into local environments, the easier it becomes to solve it distributedly. It's not clear if the math provides any added value on top of this important yet simple observation. \n\nThe connection between theory and practice: Assigning perceived rewards to simplify the MARL problem is an elegant and appealing idea. For this reason, it's important to carefully discuss to what extent this idea actually influenced the algorithm that was tested in practice. Subsection 2.3 raises some concerns in this regard since the algorithm resorts to \"end-to-end learning of Q_i\", in what seems like a total bypass of the idea of the perceived rewards. Looking at equation (3) or (4), one can get the impression that the perceived rewards are just an interpretation of what happens \"inside\" when training the Q-values after splitting them as in (4). By itself, (4) makes a lot of sense and is very natural, so it's not clear if it isn't' easier to come up directly with (4) without knowing anything about perceived rewards. This raises the question: why isn't it possible to bypass the idea of the perceived rewards and motivate the paper based on (4), which is closer to the practical algorithm? answering this question is crucial to claim a significant contribution since otherwise the are two loosely related parts in this paper. \n\nPrecise statements: The statements of the mathematical results often lack some definitions. The statement of theorem 1 doesn't define R_{max} (but Lemma 3 does). The statement of Lemma 3 doesn't define the distance between states (but Lemma 2 does). Please make the statements more standalone and well defined. On the same note, make sure that important definitions don't randomly appear in the paper, sometimes too late (e.g., local states). \n\nExperiments: The experimental results are overall nice and promising.  My only question here is why the resource collection scenario only compares to IQL and not to QTRAN/VDN/QMIX like the StarCraft scenario? \n\nVague sentences and typos:\n\n\"each agent i is acted independently on its own state\" - acting? based on its own state? \n\n(s_1,...,s_N) (bottom of page 2) - needs to be s_K\n\n\"so that\" in Theorem 1 - such that \n\n\"We found that using the observation o_i of agent i covers s_i^{local} works sufficiently well\" - not clear. \n\n\"since 1990s\" - since the 1990s. \n\n\"We sometimes also replace... in Eq.7 by its target to further stabilize training\" (Page 12)- what does sometimes mean? how can one reproduce this?\n\n\"doesn't\" - does not \n\n\"Applying Lemma 1 and notice that all other rewards does not change\" - do not change \n\n\"Define the remote agents s_i^{remote}...\" isn't that a set of states and not of agents? please rephrase. \n\n\"the more distant between relevant rewards istes from remote agents\" - the larger the distance?  \n\n\n\n", "title": "Nice idea, but needs some serious clarification and polishing ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "UTzNZNeS31D": {"type": "review", "replyto": "GVNGAaY2Dr1", "review": "The paper focuses on reward attribution in multiagent reinforcement learning, proposing a new algorithm, essentially by splitting value functions to what agents can achieve individually and learning separately how different individual rewards interact. This is an old idea, but the paper essentially applies it to the state-of-the-art deep learning machinery, producing impressive results on the hardest games state-of-the-art algorithms can manage. \n\nAs is the case with many similar areas, the work emphasises translation of an idea to the deelp learning setting, with the usual caveats, i.e. that there is not a major new methodological or conceptual insight, and the main advance is in terms of scaling up to real-world scenarios rather than having a better explicit algorithm to manage reward decomposition at runtime in an online learning setting. In other words, we learn more about how to solve challenging games rather than about the key underlying AI problem. \n\nThe paper does not offer a huge amount of novelty, but rather presents a solid deep RL engineering approach to solving the wider problem in a specific setting. Nonetheless, the technical material is well-developed, the presentation is overall of a high quality, and the experimental results extensive (and impressive). ", "title": "Novel method for reward attribution achieves impressive results in state-of-the-art benchmark domains", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rabHEzOzQQL": {"type": "review", "replyto": "GVNGAaY2Dr1", "review": "Good theoretical analysis and compliant experiment performance\n\n1. The Limitation of Theorem 1: the authors have said that ``the optimal gap of r_i heavily depends on the size of s_i^{local}. But in experiments (including experiments in appendix), the authors only discussed the claimed optimal setting (\u201cusing the observation o_i of agent i covers $s_i^{local}$\u201d). More experiments on other size of s_i^{local} could be added to better prove the conclusion. (Whether the best choice can not or hard to be proven mathematically.)\n2. Some expression problems which may cause confusions: 1) Too many kinds of rewards including perceived reward, local reward, external reward and etc. The definitions of them are not very clear.  For example,  the perceived reward is really confused; 2) A brief algorithm flow chat and pseudo codes are needed for better understanding of how the algorithm works.\n3. As this work is eventually an MARL work in solving ad hoc team setting games by decomposing reward. Some explicit comparisons (May be in form or experiments or brief analysis) should be added with some MARL methods(SSD: Social Influnce as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning; PBRS: Reward shaping for knowledge-based multi-objective multi-agent reinforcement learning). Only the credit assignment problem in RL is discussed, the authors need to discuss more on some other related works like social dilemma in MARL or reward shaping?", "title": "Good theoretical analysis and compliant experiment performance", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}