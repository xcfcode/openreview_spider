{"paper": {"title": "Support Regularized Sparse Coding and Its Fast Encoder", "authors": ["Yingzhen Yang", "Jiahui Yu", "Pushmeet Kohli", "Jianchao Yang", "Thomas S. Huang"], "authorids": ["superyyzg@gmail.com", "jyu79@illinois.edu", "pkohli@microsoft.com", "jianchao.yang@snapchat.com", "t-huang1@illinois.edu"], "summary": "We present Support Regularized Sparse Coding (SRSC) to improve the regular sparse coding, and propose a feed-forward neural network termed Deep Support Regularized Sparse Coding (Deep-SRSC) as its fast encoder.", "abstract": "Sparse coding represents a signal by a linear combination of only a few atoms of a learned over-complete dictionary. While sparse coding exhibits compelling performance for various machine learning tasks, the process of obtaining sparse code with fixed dictionary is independent for each data point without considering the geometric information and manifold structure of the entire data. We propose Support Regularized Sparse Coding (SRSC) which produces sparse codes that account for the manifold structure of the data by encouraging nearby data in the manifold to choose similar dictionary atoms. In this way, the obtained support regularized sparse codes capture the locally linear structure of the data manifold and enjoy robustness to data noise. We present the optimization algorithm of SRSC with theoretical guarantee for the optimization over the sparse codes. We also propose a feed-forward neural network termed Deep Support Regularized Sparse Coding (Deep-SRSC) as a fast encoder to approximate the sparse codes generated by SRSC. Extensive experimental results demonstrate the effectiveness of SRSC and Deep-SRSC.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "Adding a manifold regularizer to a learning objective function is certainly not a new direction. The paper argues that using a support based regularizer is superior to using a standard graph Laplacian regularizer (which has been explored before), although this argument is not developed particularly rigorously and dominantly has to fall back on empirical evidence. The main contribution of the paper appears to be theoretical justification of an alternating optimization scheme for minimizing the resulting objective function (yet the optimization aspects of dealing with a sparse support regularizer are somewhat orthogonal to the current context). The empirical results are not very convincing since the dictionary size is relatively large compared to the dataset size; the gains with respect to l2 manifold regularizer are not consistent; and the gains using deep architectures to directly predict sparse codes are also modest and somewhat inconsistent. These points aside, the reviewers are overall enthusiastic about the paper and find it to be well written and complete."}, "review": {"SkvYCqKvx": {"type": "rebuttal", "replyto": "S1DiuduNx", "comment": "Thank you for your comment!\n\nWe have included a more clear motivation of Deep-SRSC in the beginning of Section 4 in the revised paper, namely \u201cThe goal of Deep-SRSC is to approximate the sparse codes of the input data in a fast way by feeding the data through the Deep-SRSC network, instead of running the iterative optimization algorithm for SRSC in Section 2.1. \u201d\n\nIn fact, SRSC is efficient from the perspective of the conventional optimization algorithms for the sparse coding methods. Please refer to the complexity analysis of SRSC in the subsection \u201cTime Complexity\u201d in Section 2.1 of the revised paper. Deep-SRSC is proposed as a fast approximation of SRSC with considerable (around 8.3 times) speedup for obtaining the approximate support regularized sparse codes of the new data or the test data (more details in Section 4.1 of the revised paper: Deep-SRSC As Fast Encoder). \n\nMoreover, we have presented the additional experiments in the revised paper on the MNIST and CIFAR-10 data for the clustering and semi-supervised learning tasks, which further demonstrate the effectiveness of SRSC and Deep-SRSC.\n", "title": "Thank you for your comment"}, "Hy_XYcYDx": {"type": "rebuttal", "replyto": "ByD8Z8QNg", "comment": "Thank you for your comment!\n\nWe have included the intuition of what implies the condition G_ki >=0  in Section 3. Moreover, we have presented the additional experiments in the revised paper on the MNIST and CIFAR-10 data for the clustering and semi-supervised learning tasks, which further demonstrate the effectiveness of SRSC and Deep-SRSC.\n", "title": "Thank you for your comment"}, "SkINLtNvg": {"type": "rebuttal", "replyto": "SkEO-2zPe", "comment": "Thank you for your comments and the concern on the efficiency comments in the paper. The more detailed explanation of the efficiency is presented below.\n\nThe optimization algorithm of SRSC algorithm alternatingly performs optimization over the dictionary and the sparse codes with the other variable fixed. The optimization over the dictionary for SRSC has the same efficiency as the well-known work \u201cEfficient sparse coding algorithms\u201d by Lee et al. published in NIPS 2006, and the optimization over the sparse code of each data point by the proposed PGD-style iterative method is almost as efficient as the widely used Iterative Shrinkage and Thresholding Algorithm (ISTA) method. Compared to ISTA, the extra operations required by our PGD-style iterative method are only the arithmetic operations with the time complexity 20p where p is the dictionary size. Since a compact dictionary is preferred by the extensive study of the sparse coding and dictionary learning literature and the dictionary size p is not greater than 500 throughout our experiments, our PGD-style iterative method only incurs extra operations of constant time complexity compared to ISTA while learning supported regularized sparse codes. In Section 4, we propose Deep-SRSC as a fast approximation of SRSC with considerable (around 8.3 times) speedup for obtaining the sparse codes of the new data or the test data (see more details in Section 4.1: Deep-SRSC As Fast Encoder). Furthermore, we conduct the empirical study showing that the parallel coordinate descent method, which updates the codes of a group of $P$ data points in parallel and provides $P$ times speedup over the coordinate descent method used in Algorithm 1, exhibits almost the same performance as the coordinate descent method for the clustering task on the test set of the CIFAR-10 data. Please refer to the details in the subsection ``Deep-SRSC with the Second Test Setting (Referring to the Training Data)'' in the Appendix of the revised paper. The complexity analysis of SRSC is illustrated with more details in the subsection \u201cTime Complexity\u201d in Section 2.1 of the revised paper. \n\nMoreover, it is worthwhile to be emphasized that the goal of our implementation of the SRSC optimization algorithm with CUDA C++ is to further improve the efficiency of SRSC by parallel computing through GPU, considering that the GPU platforms are becoming more and more popular in the community. Our CUDA C++ implementation of SRSC is on GitHub and free for downloading.", "title": "Thank you for your comments and the concern on the efficiency comments"}, "S11JYSh8g": {"type": "rebuttal", "replyto": "ByBttsrVg", "comment": "Thank you for your comments and the raised concerns.\n\n1 Compact dictionary learned by sparse coding methods leads to compelling empirical results, as shown by the extensive study in the machine learning and computer vision literature, so we choose the dictionary size between 100 and 500. Note that the dictionary size in our paper is comparable to that in the seminal paper \u201cLearning Fast Approximations of Sparse Coding\u201d by Karol Gregor and Yann LeCun which proposes LISTA network for fast approximation of regular sparse coding. We report the best performance among the two similarity measures as the clustering results on the test set of each data set in Section 5. The same procedure is performed for all sparse coding based methods to produce fair comparison results, and we observe that the clustering results by the two similar measures are usually similar. In our semi-supervised learning experiments (see more details in the second part of this response), a single similarity measure, i.e. the support similarity introduced in Section 5.1, is used for all the semi-supervised learning methods.\n\nRegarding to the clustering results in terms of accuracy and normalized mutual information (NMI), accuracy is in many cases the more preferable measure since it directly reflects the percentage of the data that have correct cluster labels. Our clustering results on various data sets are always promising in terms of accuracy (thank you for pointing this out), while maintaining consistently competitive NMI value. Also, SRSC produces notably better NMI results than the baseline methods on the USPS data (e.g. for the first c = 6 and 8 clusters of the USPS data in Table 1). More importantly, we conduct more clustering experiments on larger data sets, i.e. MNIST handwritten digit data and CIFAR-10 data, with results shown in Table 7 and Table 8 of the revised paper. Such results again demonstrate the effectiveness of SRSC and Deep-SRSC, and SRSC and Deep-SRSC achieve considerable improvement in terms of both accuracy and NMI on the MNIST data. \n\nSince our SRSC method is an improved sparse coding method by exploiting manifold structure of the data, we mainly demonstrate performance improvement over other sparse coding methods, i.e. regular sparse coding and L2-RSC which uses graph Laplacian to smooth the sparse codes based on the local manifold structure of the data. In our experiments of semi-supervised learning by label propagation, we add into our baseline methods the state-of-the-art semi-supervised method that is mostly relevant to our study to the best of our knowledge (see more details below). \n\n2 We have added another important application of the sparse coding methods, i.e. semi-supervised learning by label propagation (this application was shown on the USPS data in response to the comment of AnonReviewer2), and we conduct semi-supervised learning experiments on the USPS data and the CIFAR-10 data with the results illustrated in Figure 6 and Figure 7 of the revised paper. Note that SRSC and Deep-SRSC achieve significant improvement over other baseline methods based on label propagation in terms of error rate especially when the number of labeled samples for each class is small (usually with 15% to 40% improvement), revealing the advantage of support regularization that captures the locally linear manifold structure of the data. The baseline methods include manifold based similarity adaptation (MBS) by Karasuyama et al. in NIPS 2013, one of the state-of-the-art semi-supervised learning methods based on label propagation.\n\nUpdate: We use the deep neural network trained on the ILSVRC 2012 data to extract the $4096$-dimensional feature vector for each image in the CIFAR-10 data, and all the clustering methods are performed on the extracted features so as to achieve much higher performance. SRSC still outperforms other baseline methods in terms of both accuracy and NMI. Please see more details in Table 8 and the subsection \"Deep-SRSC with the Second Test Setting (Referring to the Training Data)\" in the appendix of the revised paper.", "title": "Thank you for your comments and the raised concerns"}, "B1ddeRgNl": {"type": "rebuttal", "replyto": "By38t1bQe", "comment": "Thank you for your comment and the questions regarding to the details of this paper.\n\n1. The difference between our work and \"Collaborative linear coding for robust image classification\" by Wang et al is significant. We denote the referred paper by [a]. When the dictionary (or codebook) is provided, [a] uses a correlation regularization term similar to the support regularization term proposed in this paper. In [a], the data are partitioned into several groups and the correlation regularization term has vanished support distance between the sparse codes of data from different groups. Due to the difficulty of the nonconvex and non-smooth optimization problem with the correlation regularization term, [a] makes a strong assumption on the sparse codes: the sparse codes of the data belonging to the same group have the same support. Under this strong assumption, the optimization problem over all the sparse codes is equivalent to a set of independent sub-problems and each sub-problem aims to obtain the sparse codes of the data from a single group. In order to solve the sub-problem corresponding to a specific group, an empirical method is adopted which first pre-encodes each data point of this group by regular sparse coding using the provided dictionary, then selects the dictionary atoms with strong response (i.e. sparse code elements with large absolute values) to form a sub-dictionary. The regularized sparse codes of the data from this group are then obtained by performing conventional sparse coding w.r.t. this sub-dictionary and then they have the same support w.r.t. to the original dictionary.\n\nBased on the above description of [a], our optimization algorithm in section 2.1.2 has two significant advantages. 1) We directly solve the original nonconvex and non-smooth optimization problem without the strong group assumption in [a]. 2) We provide theoretical guarantee on the proposed optimization algorithm. Prop. 1 guarantees that the proposed PGD-style iterative method decreases the value of the objective function in each iteration. Section 3 further guarantees that the sequence produced by the PGD-style iterative method converges to a sub-optimal solution which is a critical point of the objective function and close to the globally optimal solution when the support regularization term has positive coefficients (i.e. it is lower semicontinuous).\n\n2. For a data point i, if the number of its neighbors with zero k-th element of the sparse codes is larger than that with nonzero k-th element in the sparse codes, which indicates that the neighbors of data i suggest that a zero k-th element in the sparse code of data i is preferable, then G_ki >=0 and G_ki quantitatively represent penalty if the sparse code element (Z_ki) chooses to be nonzero while the neighbors of point i suggest that Z_ki=0 is more preferable. Intuitively, this situation happens when there is conflict between choosing the support of the code solely by the data point itself and the suggestion of its neighbors; if the point is an outlier or suffering from noise, the optimization can help that point make a sensible choice by considering the suggestion of its neighbors. We observe that G_ki >=0 (for all k = 1..p) happens in all the data sets we used in this paper.\n\n3. The goal of Deep-SRSC is to approximate the sparse codes of the input data in a fast way by feeding the data through the Deep-SRSC network, instead of running the iterative optimization algorithm in Section 2.1. To achieve this goal, the Deep-SRSC network is trained on the training data by minimizing the squared distance between the predicted codes of the training data by the network and their ground truth codes. We feed the test data and the proper KNN graph through the Deep-SRSC network to have the approximate codes of the test data. The network design of Deep-SRSC is in accordance with the proposed PGD-style iterative method. Since the number of layers of Deep-SRSC is much smaller than the number of iterations in the PGD-style iterative method, Deep-SRSC achieves fast approximation of the support regularized sparse codes, and the more detained analysis on the speedup achieved by Deep-SRSC is in the subsection \u201cDEEP-SRSC AS FAST ENCODER\u201d in section 4.\n\nWe use the training data (7291 images) of the USPS data to train the Deep-SRSC network in Section 5, and feed forward the test data of the USPS data and the KNN graph over the test data into the Deep-SRSC network to obtain the approximate codes of the test data. The training data here is used to train Deep-SRSC, and we use the codes of the test data predicted by the trained Deep-SRSC for clustering on the test data, so as to show the effectiveness of the codes generated by Deep-SRSC. There are broad applications with the approximate codes produced by Deep-SRSC, and we apply Deep-SRSC to semi-supervised learning problem in the revised version of this paper. In semi-supervised learning, the training data and their labels participate in the inference of the labels of the test data.\n\nWe understand your concern that the training graph is expected to be used for inferring the code of a new data point, and we greatly appreciate that you point out that the success would depend on the quality of the test samples in our current setting. Therefore, we provide the second test setting (referring to the training data) where the code of each test point is predicted by Deep-SRSC with the KNN graph over that test point and the training data. The description of our original setting (the first test setting without referring to the training data) and the second test setting (referring to the training data) are described in Section 4 of the revised paper. Moreover, we show the compelling experimental results of Deep-SRSC with the second test setting, including the prediction error of Deep-SRSC and its application to semi-supervised learning in the appendix of the revised paper.\n\n\n", "title": "Thank you for your detailed and helpful comments"}, "By38t1bQe": {"type": "review", "replyto": "HkljfjFee", "review": "1. The paper was very interesting. The motivation for the new regularization is very interesting. Could you please comment more on the differences with the work of Wang et al 2014? (cited in the manuscript)\n\n2. Could you provide an intuition of what implies the condition G_ki >=0? (assumed in the results presented in Section 3). Would that be reasonable to find in practice?\n\n3. I am a bit confused with the value of using the SRSC as an encoder, and the need of a fast approximation. It would be useful to provide an example application. The division into train and test it's not clear to me. The provided examples ( Sec. 6.1) is on clustering. In that case, I would find natural to re-compute the clusters as new data arrives, or to define classes from the previously found clusters to classify new data.\n\nFollowing on this point, if we are interested in computing new codes for a new datapoint, I would expect the training graph to also be used for inference. If I understand correctly, this is not the case. Please clarify.  Say we have a new test sample, I would expect that SRSC to use the training data to define the graph (maybe leaving the codes of the training data fixed for efficiency). Otherwise the success would depend on the quality of the test samples (needs to cover well the manifold).\nI'd like to thank the authors for their detailed response to my questions.\n\nThe paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.\n\nThe paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. \n\nThanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript.\n\nThe authors also propose a fast encoding scheme for their proposed method. \nThe authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation). While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work. \"Converting\" iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems). Is natural that this can be done, and not very surprising. Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past).\n\n\n", "title": "Three questions to better understand the work", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByD8Z8QNg": {"type": "review", "replyto": "HkljfjFee", "review": "1. The paper was very interesting. The motivation for the new regularization is very interesting. Could you please comment more on the differences with the work of Wang et al 2014? (cited in the manuscript)\n\n2. Could you provide an intuition of what implies the condition G_ki >=0? (assumed in the results presented in Section 3). Would that be reasonable to find in practice?\n\n3. I am a bit confused with the value of using the SRSC as an encoder, and the need of a fast approximation. It would be useful to provide an example application. The division into train and test it's not clear to me. The provided examples ( Sec. 6.1) is on clustering. In that case, I would find natural to re-compute the clusters as new data arrives, or to define classes from the previously found clusters to classify new data.\n\nFollowing on this point, if we are interested in computing new codes for a new datapoint, I would expect the training graph to also be used for inference. If I understand correctly, this is not the case. Please clarify.  Say we have a new test sample, I would expect that SRSC to use the training data to define the graph (maybe leaving the codes of the training data fixed for efficiency). Otherwise the success would depend on the quality of the test samples (needs to cover well the manifold).\nI'd like to thank the authors for their detailed response to my questions.\n\nThe paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.\n\nThe paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. \n\nThanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript.\n\nThe authors also propose a fast encoding scheme for their proposed method. \nThe authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation). While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work. \"Converting\" iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems). Is natural that this can be done, and not very surprising. Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past).\n\n\n", "title": "Three questions to better understand the work", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}