{"paper": {"title": "Bypassing the Random Input Mixing in Mixup", "authors": ["Hongyu Guo"], "authorids": ["~Hongyu_Guo1"], "summary": "We report a finding that one can bypass the random input mixing in Mixup to conduct effective model regularization. ", "abstract": "Mixup and its variants have promoted a surge of interest due to their capability of boosting the accuracy of deep  models. For a random sample pair, such approaches generate a set of synthetic samples through interpolating both the  inputs and their corresponding one-hot labels. Current methods either interpolate random features from an input pair or learn to mix salient features  from the  pair. Nevertheless, the former methods can create misleading synthetic    samples or remove important features from the given inputs, and the latter strategies incur significant computation cost for selecting descriptive input regions. In this paper, we show that the effort needed for the input mixing can be bypassed. For a given sample pair, averaging the features from the two inputs and then assigning it with a set of  soft labels can  effectively regularize the training. We empirically show that the proposed  approach performs on par with state-of-the-art strategies in terms of predictive accuracy. ", "keywords": ["Deep Learning", "Data Augmentation", "Mixup"]}, "meta": {"decision": "Reject", "comment": "This work proposes to improve Mixup by using soft labels, removing the need for input mixup. The reviewers found the paper was clear and found the experiments promising. The reviewers raised concerns about the lack of experiments comparing this approach to Mixup+Label smoothing, which were addressed during the rebuttal by the authors. However, the reviewers did not find the empirical evidence strong enough given that this is mostly an empirical contribution. The authors do not necessarily need to train on the full Imagenet, but it would be beneficial to evaluate on more standard settings on the dataset considered to facilitate comparison to previous work."}, "review": {"rJYZcVupnBq": {"type": "review", "replyto": "mLtPtH2SIHX", "review": "**Main Claim:**\n\nThe authors propose to use soft labels on naive mixup as an alternative to sophisticated mixup strategy. In experiments on 5 small datasets, the proposed method can achieve better accuracy than baselines.\n\n**Strong points:**\n\nThe authors propose to use soft labels to overcome the mislearning features in mixup images. The idea is simple and straightforward. \n\nExperiment results show the method works well on small datasets.\n\n\n**Weak points:**\n\nThe contribution of this work is incremental.\n\nExperiment results on ImageNet are missing. On Tiny-ImageNet, it\u2019s also helpful to show the performance of Mixup and CutMix.\n\nSome decisions are made without justification. Some details are not clearly explained. See questions.\n\n\n\n**Recommendation:**\n\nReject.\n\nThe proposed method is incremental. The method should be evaluated on ImageNet. \n\n**Questions:**\n\nIn Eq(9), a sigmoid function is applied before the softmax function. So the unnormalized logits of this distribution is in [-1, 1]. Why?\n\nWhy is \u201ctarget soft labels too far away\u201d an issue for the model? why does \u201ca mini-batch with original inputs\u201c prevent \u201cLaMix from assigning target soft labels too far away\u201d? If the trick is not applied, what will happen to the model? Will the model take more time to converge or it won't converge?\n\n\u201cFor LaMix, the added fully connected layer is just a copy of the fully-connected layer of the original network with a Softmax function on the top.\u201d Is the network pre-trained? Are the two matrices sharing weights?\n\nAs shown on Figure 2, the top 2 classes occupy a large portion of the probability. So I\u2019m curious which part actually contributes to the improvements. Is it (A) the reweighting of y_i and y_j, or (2) the introduction of other labels? I.e. After computing Eq (10), keep the value for y_i and y_j, set all other dimensions to zero, (then renormalize the distribution), and use it as the training label, what will happen?  My impression is it may solve the problem of  \u201ctarget soft labels too far away\u201d.\n\n**Comments:**\n\nEq (3) is confusing to me. I think authors can follow the convention in (Yun et al.), rewrite the equation as $x_\\lambda^{i,j}=\\Phi(x_i, x_j, lambda) * x_i + (1 - \\Phi(x_i, x_j, lambda) * x_j$\n\nhas tow forms -> has two forms\n\n**After rebuttal:**\n\nThanks to the author for providing additional experimental data. But without the results of imagenet, it is difficult to judge the effectiveness of this method on complicated data. So I decided to keep the original score.\n", "title": "An incremental work on mixup data augmentation ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "tECQdKTtcH5": {"type": "rebuttal", "replyto": "yOfuXt4gI0a", "comment": "As shown in Table 5 on Page 9, for the Tiny ImageNet LaMix obtained error rates of 34.40% and 28.85% with PraAct ResNet-18 and ResNet-50 respectively, which significantly outperformed the 41.68% and  39.09% obtained by Mixup. \n\n", "title": "LaMix significant outperformed Mixup on TinyImageNet (over 10% absolute error rate reduction)"}, "Q3b6FGr1yF0": {"type": "rebuttal", "replyto": "Ph6zhjnK-eA", "comment": "We sincerely appreciate your insightful comments on our paper. We have updated the revision PDF with a set of additional results in Section 6 of page 9.\n\nSince the second discussion phase will end soon, please let us know if you have any comments/concerns that we have not addressed up to your satisfactory. We will be happy to address them to strengthen our paper.", "title": "Updated PDF with a set of additional results in Section6 of page 9."}, "jvpQkTEIHY": {"type": "rebuttal", "replyto": "kF7r-Ky5Ty", "comment": "We sincerely appreciate your insightful comments on our paper. We have updated the revision PDF with a set of additional results in Section 6 of page 9.\n\nSince the second discussion phase will end soon, please let us know if you have any comments/concerns that we have not addressed up to your satisfactory. We will be happy to address them to strengthen our paper.", "title": "Updated PDF with a set of additional results in Section6 of page 9."}, "DXJYVrT1taO": {"type": "rebuttal", "replyto": "rJYZcVupnBq", "comment": "We sincerely appreciate your insightful comments on our paper. We have updated the revision PDF with a set of additional results in Section 6 of page 9.\n\nSince the second discussion phase will end soon, please let us know if you have any comments/concerns that we have not addressed up to your satisfactory. We will be happy to address them to strengthen our paper.\n", "title": "Updated PDF with a set of additional results in Section6 of page 9."}, "3-bZsGdrNDp": {"type": "rebuttal", "replyto": "yOfuXt4gI0a", "comment": "We sincerely appreciate your insightful comments on our paper. We have updated the revision PDF with a set of additional results in Section 6 of page 9.\n\nSince the second discussion phase will end soon, please let us know if you have any comments/concerns that we have not addressed up to your satisfactory. We will be happy to address them to strengthen our paper.\n", "title": "Updated PDF with a set of additional results in Section6 of page 9."}, "nKaxHfsjlvz": {"type": "rebuttal", "replyto": "Ph6zhjnK-eA", "comment": "Thank you for your insightful comments. \n\nOur work is inspired by Mixup and self-distillation. Our method differs from Mixup in that we average the two inputs, instead of linearly interpolating the inputs with a random ratio. In other words, the Mixup method will generate a large set of mixed inputs from a given sample pair, but our approach only has one mixed input (by average). \n\nRegarding your comments:\n\n1. Our work is motivated by two issues in Mixup. First, there is a dis-matching between mixed input and associated label of a synthetic sample in Mixup during to its random input mixing policy, thus Mixup can generate misleading synthetic samples. Second, the input mixing for learning to mixing Mixup strategies incurs significant computation cost for selecting descriptive input regions. To cope with these two problems in Mixup, our method leverages only one mixed input (by averaging the input pair), and then assigns adaptive soft labels to the mixed input. Dynamically learning a soft label will mitigate the label dis-matching issue in Mixup and also effectively make good use of the only mixed input. \n\n Simply stacking label smoothing on top of vanilla Mixup didn\u2019t help. We conducted experiments of stacking uniform label \n smoothing (with different coefficients) on top of Mixup, and here are the results (error rates) of using PreAct ResNet-18 and \n ResNet-50 on Tiny ImageNet, Cifar100 and Cifar10 (number in the bracket is the deviation of 5 runs). \n\n Tiny ImageNet:\t\n\n LaMix, Mixup,  Mixup + ULS 0.1, Mixup + ULS 0.2, Mixup + ULS 0.3, Mixup + ULS 0.4\n\n PreAct ResNet18, 34.40 (0.26), 41.68 (0.35),  42.31 (0.34), 45.19 (0.68), 60.58 (0.59), 78.48 (0.53)\n\n ResNet50, \t 28.85 (0.15), 39.09 (0.28), 42.58 (0.39), 59.25 (0.91), 81.06 (0.24), 91.82 (0.37)\n\n Cifar100: \n\n LaMix, Mixup,  Mixup + ULS 0.1, Mixup + ULS 0.2, Mixup + ULS 0.3, Mixup + ULS 0.4\n\n PreAct ResNet18, 19.38 (0.16), 21.10 (0.21), 21.51 (0.51), 21.41 (0.55), 20.94 (0.49), 20.95 (0.49)\n\n ResNet50, \t 18.60 (0.69), 19.48 (0.48), 21.58 (0.86), 20.87 (0.51), 21.64 (0.41), 21.18 (0.58)\n\n Cifar10: \n\n LaMix, Mixup, Mixup + ULS 0.1, Mixup + ULS 0.2, Mixup + ULS 0.3, Mixup + ULS 0.4\n\n PreAct ResNet18, 3.82 (0.18), 3.88 (0.32), 4.00 (0.17), 3.95 (0.13), 4.06 (0.04), 4.06 (0.03)\n\n ResNet50, \t 3.60 (0.24), 4.29 (0.28), 4.02 (0.27), 4.09 (0.10), 4.19 (0.18), 4.68 (0.79)\n\n These results show that label smoothing degraded the accuracy of Mixup in the cases of Cifar100, Cifar10, and Tiny ImageNet on \n PreAct ResNet18 and Resnet50. Such accuracy degradation is due to the two layers of label smoothing in Mixup. Research has \n shown that the mixing label in the original Mixup has the effect of label smoothing (Carratino et al, 2020, On Mixup \n Regularization). In this sense, adding another label smoothing regularizer on top of the vanilla Mixup means that the Mixup will \n have two layers of label smoothing regularizers, which can easily mess up the regularization effect, thus degrading the accuracy \n as shown in the results above.\n\n2. We are glad that you found Figure 3 promising. These figures indeed show that the dynamic soft labels were evolving to adapt to the states of the learning. \n3. We agree that our method performs on par with the SOTA method. But, our approach excludes the need to compute all the different ratios between [0, 1] for each sample pair, which is a significant reduction in terms of computation cost. Each of these mixing ratios needs to solve an optimization problem in Puzzle Mix, which is computational expensive. \n", "title": "Adaptive soft label mitigates the label mis-matching issue in Mixup and also makes good use of the only mixed input of a sample pair"}, "4qFgf7i-kqy": {"type": "rebuttal", "replyto": "kF7r-Ky5Ty", "comment": "Thank you for your insightful comments. \n\nThe motivation of using the global soft labels is that there is a dis-matching issue between mixed input and associated label of a synthetic sample in Mixup. Learning the soft label helps mitigate this issue. In a nutshell, our work is motivated by two problems in Mixup. First, there is a dis-matching between mixed input and associated label in Mixup due to its random input mixing policy. Second, the input mixing for learning to mixing Mixup strategies incurs significant computation cost for selecting descriptive input regions. To cope with these two issues in Mixup, our method leverages only one mixed input (by averaging the input pair), and then assigns adaptive soft label to the same mixed input. \n\nRegarding your questions:\n\n1. This is incorrect. W_{t} will be learned (along with the other hyperparameters of the network), before feeding into the Sigmoid, to generate the global soft label. We will make it much clearer in the revision. \n2. In Equation9, we use a Sigmoid function, so that the output range is [0, 1]. This will provide the probability of associating the mixed input with a particular label. On the other hand, Equation7 is used by the network to generate predicted classification distribution, so there is no need for a Sigmoid to squeeze the scores into the range of [0, 1]. \n3. We found that for the random input mixing methods 0.5 provided better results. For the learning to mixing method 1.0 gave better accuracy. These two numbers were heuristically found using the validation datasets. Note that, even with Beta=1.0, our method still differs from Mixup, which uses a random mixing ratio between [0, 1] to generate a set of synthetic inputs while our method uses only one mixed input for any sample pair. \n4. The hyperparameters are exactly the same as that in the Facebook codes and the Puzzle Mix codes as published on their websites. We didn\u2019t change anything for fair comparison. \n5. Thank you for suggesting applying label smoothing to Mixup. We conducted experiments of stacking uniform label smoothing (with different coefficients) on top of Mixup, and here are the results (error rates) of using PreAct ResNet-18 and ResNet-50 on Tiny ImageNet, Cifar100 and Cifar10 (number in the bracket is the deviation of 5 runs). \n\n Tiny ImageNet: \n\n LaMix, Mixup,  Mixup + ULS 0.1, Mixup + ULS 0.2, Mixup + ULS 0.3, Mixup + ULS 0.4\n\n PreAct ResNet18, 34.40 (0.26), 41.68 (0.35),  42.31 (0.34), 45.19 (0.68), 60.58 (0.59), 78.48 (0.53)\n\n ResNet50, \t 28.85 (0.15), 39.09 (0.28), 42.58 (0.39), 59.25 (0.91), 81.06 (0.24), 91.82 (0.37)\n\n Cifar100: \n\n LaMix, Mixup,  Mixup + ULS 0.1, Mixup + ULS 0.2, Mixup + ULS 0.3, Mixup + ULS 0.4\n\n PreAct ResNet18, 19.38 (0.16), 21.10 (0.21), 21.51 (0.51), 21.41 (0.55), 20.94 (0.49), 20.95 (0.49)\n\n ResNet50, \t 18.60 (0.69), 19.48 (0.48), 21.58 (0.86), 20.87 (0.51), 21.64 (0.41), 21.18 (0.58)\n\n Cifar10: \n\n LaMix, Mixup, Mixup + ULS 0.1, Mixup + ULS 0.2, Mixup + ULS 0.3, Mixup + ULS 0.4\n\n PreAct ResNet18, 3.82 (0.18), 3.88 (0.32), 4.00 (0.17), 3.95 (0.13), 4.06 (0.04), 4.06 (0.03)\n\n ResNet50, \t 3.60 (0.24), 4.29 (0.28), 4.02 (0.27), 4.09 (0.10), 4.19 (0.18), 4.68 (0.79)\n\n These results show that label smoothing degraded the accuracy of Mixup in the cases of Cifar100, Cifar10, and Tiny ImageNet on \n PreAct ResNet18 and Resnet50. Such accuracy degradation is due to the two layers of label smoothing reguarlization in Mixup. \n Research has shown that the mixing label in the original Mixup has the effect of label smoothing (Carratino et al, 2020, On Mixup \n Regularization). In this sense, adding another label smoothing regularizer on top of the vanilla Mixup means that the Mixup will \n have two layers of label smoothing regularizers, which can easily mess up the regularization effect, thus degrading the accuracy as shown in the results above. \n", "title": "Applying label smoothing to Mixup degrades accuracy."}, "f4Z1rivAa_k": {"type": "rebuttal", "replyto": "rJYZcVupnBq", "comment": "Thank you for your insightful comments.\n\nIn terms of contribution, our research shows that one can bypass the random input mixing in Mixup to conduct sufficient model regularization, and we think this message is worth noting to the research community. \n\nIn terms of our technical novelty, the use of adaptive soft label in our work is motivated by the mis-matching issue between a mixed input and its soft label in Mixup. This is due to the fact that Mixup generates soft labels using random mixing ratios for linear interpolation. Such mis-matching issue will result in generating misleading synthetic samples and causing underfitting. To cope with this challenge, our method fixes the mixed input (by averaging the input pair), and then adaptively assigns a soft label to the same mixed input. The learned soft label aims to mitigate the potentially wrong soft label generated by Mixup. \n\nWe conducted additional experiments on Tiny-ImageNet. Here is the results for Resnet-18 and ResNet-50. We will add the following results (error rates) to Table 4. \n\n Vanilla, Mixup, CutMix\n\n PreAct ResNet18,  42.61 (0.41), 41.68 (0.35), 43.01 (0.36)\n\n ResNet50, \t  40.89 (0.36), 39.09 (0.28), 41.36 (0.44)\n\n\nWe also agree that results on ImageNet will improve our paper, but experiments on ImageNet are just too expensive to us. We conducted additional experiments on Tiny ImageNet and will add the following results (error rates) into Table 1.\n\n\nVanilla, Mixup, CutMix, LaMix\n\n PreAct ResNet18,  42.61 (0.41), 41.68 (0.35), 43.01 (0.36), 34.40 (0.26)\n\nResNet50, \t  40.89 (0.36), 39.09 (0.28), 41.36 (0.44), 28.85 (0.15)\n\n\nRegarding your Questions:\n\n1. We use a Sigmoid function in Eq(9), so the output range is [0, 1] (not [-1, 1]). The reason here is that this function will provide the probability of associating the input to a particular label. \n2. Without the original samples, the method significantly degraded its accuracy. We conducted the suggested experiments and provided the results (error rates) as follows (average over five runs and their standard deviations in brackets). The reason here is that, without the original training samples, the training samples could be very different from the testing samples. \nAlso, it is interesting to see that ResNet50 obtained inferior results than PreAct ResNet18 in some cases when removing the original training samples. This indeed suggests that Resnet50 fits the mixed training samples better due to its expressive power, but those mixed training samples may be far from the testing samples due to the lack of the original training samples. \n\n Cifar100: \n\n LaMix, LaMix w/o original samples\n\n PreAct ResNet18, 19.38 (0.16), 25.74 (0.43)\n\n ResNet50, \t 18.60 (0.69), 25.57 (0.36)\n\n Cifar10: \n\n LaMix, LaMix w/o original samples\n\n PreAct ResNet18, 3.82 (0.18), 5.90 (0.31)\n\n ResNet50, \t 3.60 (0.24), 6.39 (0.59)\n\n\n3. By \u201cthe added fully connected layer is just a copy of the fully-connected layer\u201d we meant that the structure of the two are the same, namely W_{t} and W_{l}. The networks are trained end-to-end simultaneously. They are two different matrices and they don\u2019t share weights. We will make it much clearer in the revision. \n4. The contributions of the performance improvement come from both: mixing of y_{i} and y_{j} and the learned smoothing distribution P^{ij}. Without the p^{ij}, the model significantly degraded the accuracy. We conducted the experiments as suggested and provided the results (error rates) below (average over five runs with standard deviation in the bracket), which show that the model degraded the accuracy:\t\n\n Cifar100: \n\n LaMix, LaMix w/o global soft label\n\n PreAct ResNet10,  19.38 (0.16), 23.03 (0.35)\n\n ResNet50, \t  18.60 (0.69), 21.34 (0.70)\n\n Cifar10: \n\n LaMix, LaMix w/o global soft label\n\n PreAct ResNet10, 3.82 (0.18), 5.07 (0.38)\n\n ResNet50, \t 3.60 (0.24), 4.78 (0.14)\n\n\n5. Thank you for suggesting the equation from Yun et al. The suggested formula will not cover the case of Puzzle Mix due to the optimal transport component in  Puzzle Mix. \n", "title": "Added ablation studies and additional experiments as suggested."}, "aK-dcHiswZb": {"type": "rebuttal", "replyto": "yOfuXt4gI0a", "comment": "Thank you for your insightful comments.\n\n1. The use of adaptive soft label in our work is motivated by the mis-matching issue between a mixed input and its soft label in Mixup. This is due to the fact that Mixup generates soft labels using random mixing ratios for linear interpolation. Such mis-matching issue will result in generating misleading synthetic samples and causing underfitting. To cope with this challenge, our method fixes the mixed input (by averaging the input pair), and then adaptively assigns a soft label to the only mixed input. Dynamically learning a soft label will mitigate the label dis-matching issue in Mixup and also effectively make good use of the only mixed input. On the contribution side, we also think that the message of bypassing the random input mixing in Mixup is worth noting to the community.\n\n\n2. Our research is motivated by the label and input mis-matching issue in Mixup and our solution is inspired by label smoothing and self-distillation. In fact, simply stacking label smoothing on top of vanilla Mixup didn\u2019t help. We conducted experiments of stacking uniform label smoothing (with different coefficients) on top of Mixup, and here are the error rates of using PreAct ResNet-18 and ResNet-50 on Tiny ImageNet, Cifar100 and Cifar10 (number in the bracket is the deviation of 5 runs). \n\n Tiny ImageNet: \n\n LaMix, Mixup,  Mixup + ULS 0.1, Mixup + ULS 0.2, Mixup + ULS 0.3, Mixup + ULS 0.4\n\n PreAct ResNet18, 34.40 (0.26), 41.68 (0.35), 42.31 (0.34), 45.19 (0.68), 60.58 (0.59), 78.48 (0.53)\n\n ResNet50, \t 28.85 (0.15), 39.09 (0.28), 42.58 (0.39), 59.25 (0.91), 81.06 (0.24), 91.82 (0.37)\n\n Cifar100: \n\n LaMix, Mixup,  Mixup + ULS 0.1, Mixup + ULS 0.2, Mixup + ULS 0.3, Mixup + ULS 0.4\n\n PreAct ResNet18, 19.38 (0.16), 21.10 (0.21), 21.51 (0.51), 21.41 (0.55), 20.94 (0.49), 20.95 (0.49)\n\n ResNet50,  18.60 (0.69), 19.48 (0.48), 21.58 (0.86), 20.87 (0.51), 21.64 (0.41), 21.18 (0.58)\n\n Cifar10: \n\n LaMix, Mixup, Mixup + ULS 0.1, Mixup + ULS 0.2, Mixup + ULS 0.3, Mixup + ULS 0.4\n\n PreAct ResNet18, 3.82 (0.18), 3.88 (0.32), 4.00 (0.17), 3.95 (0.13), 4.06 (0.04), 4.06 (0.03)\n\n ResNet50, \t 3.60 (0.24), 4.29 (0.28), 4.02 (0.27), 4.09 (0.10), 4.19 (0.18), 4.68 (0.79)\n\n These results show that label smoothing degraded the accuracy of vanilla Mixup in the cases of Cifar100, Cifar10, and Tiny \n ImageNet. Such accuracy degradation is due to the two layers of label smoothing in Mixup. Research has shown that the mixing \n label in the original Mixup has the effect of label smoothing (Carratino et al, 2020, On Mixup Regularization). In this sense, adding \n another label smoothing regularizer on top of the vanilla Mixup means that the Mixup will have two layers of label smoothing \n regularizer, which can easily mess up the regularization effect, thus degrading the accuracy as shown in the results above. \n\n\n3. In terms of performance of our method, as shown in the experiments, Mixup did obtain inferior accuracy in some cases when comparing to CutMix, but our method outperformed both Mixup and CutMix in all testing cases.  \nWe agree that experiments on ImageNet will improve our paper, but unfortunately these experiments are too expensive to us. We conducted additional experiments on Tiny ImageNet, and the following results (error rates) will be added to Table 1 in the revision:\n\n Vanilla, Mixup, CutMix, LaMix\n\n PreAct ResNet18,  42.61 (0.41), 41.68 (0.35), 43.01 (0.36), 34.40 (0.26)\n\n ResNet50, \t  40.89 (0.36), 39.09 (0.28), 41.36 (0.44), 28.85 (0.15)\n", "title": "Mixup plus label smoothing won't work."}, "Ph6zhjnK-eA": {"type": "review", "replyto": "mLtPtH2SIHX", "review": "Summary\n\nThis paper simply combines mixup and self-distillation to achieve more adaptive soft label, which effectively regularize the training. In the manuscript, authors argue that the existed mixup-based approaches has two mainly efforts, may create misleading training samples or meet computation cost issue on creating samples. Motivated by this, they propose \"LaMix\", which can leverage the information of self-distillation, to solve those two efforts and achieve competitive performance with SOTA \"Puzzle-mixup\".   \n\n\nComment\n\n1. This paper introduces a combination method between mixup and self-distillation, and simply use an additional FC layer to have adaptive soft label, which is intuitive and clear but lack of novelty. Can you give more insightful comment about how adaptive label can help mixup soft label?\n\n2. In Figure 3, I think the provided evidence for the effect of considering adaptive soft label with mixup approach is promising.\n\n3. For experimental results, in section 3.2, the proposed \"Lamix\" seems not achieve significant improvement compared to the SOTA \"Puzzle-mix\" on CIFAR-10 and 100.\n", "title": "Simple combination of mixup and self-distillation, but marginal experimental results", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "kF7r-Ky5Ty": {"type": "review", "replyto": "mLtPtH2SIHX", "review": "Summary:\nThe previous advanced Mixup methods, such as CutMix and PuzzleMix, involve input mixing. This paper suggests a new Mixup approach, called LaMix, that does not require input mixing. The solution is combining the original target label (interpolation of two one-hot targets) and generated target labels from an additional network to use it for training. The authors argue that LaMix achieves superior performance without input mixing.\n\nReasons for score:\nThe authors should explain the reason why using global soft labels is theoretically plausible before describing the method. Although empirical results look nice, I am suspicious about the experimental settings for the comparison with other methods.\n\nPros:\n- The paper includes diverse results on probing experiments.\n- The paper is clearly written.\n\nConcerns/Questions:\n- As far as I understood, neural network parameters for the global soft label (W_t) are not trained because they are only used for training labels. Then, these parameters are just randomly initialized values. Is it right? If so, isn\u2019t the final effect sensitive to the initialization?\n- Sigmoid activation is used in equation (9), different from equation (7). However, there is no explanation about the reason for using it. My guess is to make artificial labels similar to each other.\n- I don\u2019t understand why beta in LaMix is 1.0 for Section 3.2 experiments. First, I think beta = 1.0 means not using the global soft label, and it is equivalent to standard Mixup. Second, the authors mention that setting beta to 0.5 is a good choice in Section 3.1.3 (Figure 4). Moreover, they use beta as 0.5 for the experiments of Section 3.1.1 (Table 1). The settings of Table 1 and Table 3 (model architectures and datasets) are the same except for the beta value of LaMix.\n- Could you provide hyperparameters used for other Mixup methods as a baseline? Are they well-tuned?\n- I am curious whether the combination with the global soft label can be done after input mixing. If then, I think providing these results would be helpful to check whether the regularization effect is orthogonal to input mixing.\n- To compare LaMix with label smoothing, I think the author should apply label smoothing to Mixup rather than the vanilla setting.\n\nMinor comment:\n- \u201cPuzzle Mix\u201d -> \u201cPuzzleMix\u201d", "title": "Seemingly plausible but weak support", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "yOfuXt4gI0a": {"type": "review", "replyto": "mLtPtH2SIHX", "review": "This work proposes to change the labels for the mixed examples in mixup. My major concerns are as follows.\n1.\tThe motivation of adopting soft label for mixup is not clear. Label smoothing is helpful for generic training but why it can benefit mixup?\n2.\tThe proposed method is more like a combination of mixup and label smoothing. The improvement may come from label smoothing as a generic trick rather than mixup itself.\n3.\tThe performance of proposed method is very close to mixup, where the improvement is not significant. Additional experiments on ImageNet can make the results more convincing.", "title": "Label smoothing in mixup", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}