{"paper": {"title": "Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling", "authors": ["Boris Ginsburg", "Igor Gitman", "Yang You"], "authorids": ["bginsburg@nvidia.com", "igitman@andrew.cmu.edu", "youyang@cs.berkeley.edu"], "summary": "A new large batch training algorithm  based on Layer-wise Adaptive Rate Scaling (LARS); using LARS, we scaled AlexNet  and ResNet-50 to a batch of 16K.", "abstract": "A common way to speed up training of large convolutional networks is to add  computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with a mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. However,  training with a large batch  often results in lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome these optimization difficulties, we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled AlexNet  and ResNet-50 to a batch size of 16K.", "keywords": ["large batch", "LARS", "adaptive rate scaling"]}, "meta": {"decision": "Reject", "comment": "Pros:\n+ The proposed large-batch, synchronous SGD method is able to generalize at larger batch sizes than previous approaches (e.g., Goyal et al., 2017).\n\nCons:\n- Evaluation on more than one task would make the paper more convincing.\n- The addition of more hyperparameters makes the proposed algorithm less appealing.\n- Some theoretical justifiction of the layer-wise rate scaling would help.\n- It isn't clear that the comparison to Goyal et al., 2017 is entirely fair, because that paper also had recommendations for the implementation of batch normalization, weight decay, and a momentum correction as the learning rate is scaled up, but this submission does not address any of those.\n\nAlthough the revised paper addressed many of the reviewers' concerns, they still did not feel it was quite strong enough to be accepted to ICLR.\n"}, "review": {"ry33G5FxM": {"type": "review", "replyto": "rJ4uaX2aW", "review": "This paper provides an optimization approach for large batch training of CNN with layer-wise adaptive learning rates. \nIt starts from the observation that the ratio between the L2-norm of parameters and that of gradients on parameters varies\nsignificantly in the optimization,  and then introduce a local learning rate to consider this observation for a more stable and efficient optimization. Experimental results show improvements compared with the state-of-the-art algorithm.\n\nReview:\n(1) Pros\nThe proposed optimization method considers the dynamic self-adjustment of the learning rate in the optimization based on the ratio between the L2-norm of parameters and that of gradients on parameters  when the batch size increases, and shows improvements in experiments compared with previous methods.\n\n(2) Cons\ni) LR \"warm-up\" can mitigate the unstable training in the initial phase and the proposed method is also motivated by the stability but uses a different approach. However, it seems that the authors also combine with LR \"warm-up\" in your proposed method in the experimental part, e.g., Table 3. So does it mean that the proposed method cannot handle the problem in general?\n\nii) There is one coefficient that is independent from layers and needs to be set manually in the proposed local learning rate. The authors do not have a detail explanation and experiments about it. In fact, as can be seen in the Algorithm 1, this coefficient can be as an independent hyper-parameter (even is put with the global learning rate together as one fix term).\n\niii) In the section 6, when increase the training steps, experiments compared with previous methods should be implemented since they can also get better results with more epochs.\n\niv) Writing should be improved, e.g., the first paragraph in section 6. Some parts are confusing, for example, the authors claim that they use initial LR=0.01, but in Table 1(a) it is 0.02.  ", "title": "A layer-wise learning rate is proposed. Some state-of-the-art baselines are missing in comparison. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Sk-rQ7qxf": {"type": "review", "replyto": "rJ4uaX2aW", "review": "The paper proposes a new approach to determine learning late for convolutional neural networks. It starts from observation that for batch learning with a fixed number of epochs, the accuracy drops when the batch size is too large.  Assuming that the number or epochs and batch size are fixed, the contribution of the paper is a heuristic that assigns different learning late to each layer of a network depending on a ratio of the norms of weights and gradients in a layer.  The experimental results show that the proposed heuristic helps AlexNet and ResNet end up in a larger accuracy on ImageNet data.\n  Positives:\n- the proposed approach is intuitively justified\n- the experimental results are encouraging\n  Negatives:\n- the methodological contribution is minor\n- no attempt is made to theoretically justify the proposed heuristic\n- the method introduces one or two new hyperparameters and it is not clear from the experimental results what overhead is this adding to network training\n- the experiments are done only on a single data set, which is not sufficient to establish superiority of an approach\n  Suggestions:\n- consider using different abbreviation (LARS is used for least-angle regression) \n", "title": "The paper proposes a heuristic for layer-wise learning rate selection in convolutional networks. The contribution is relatively minor and is not evaluated with sufficient depth.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1b88I5xM": {"type": "review", "replyto": "rJ4uaX2aW", "review": "This paper proposes a training algorithm based on Layer-wise Adaptive Rate Scaling (LARS) to overcome the optimization difficulties for training with large batch size. The authors use a linear scaling and warm-up scheme to train AlexNet on ImageNet. The results show promising performance when using a relatively large batch size. The presented method is interesting. However, the experiments are poorly organized since some necessary descriptions and discussions are missing. My detailed comments are as follows.\n\nContributions\uff1a\n\n1.\tThe authors propose a training algorithm based LARS with the adaptive learning rate for each layer, and train the AlexNet and ResNet-50 to a batch size of 16K. \n2.\tThe training method shows stable performance and helps to avoid gradient vanishing or exploding.\n\nWeak points:\n\nThe training algorithm does not overcome the optimization difficulties when the batch size becomes larger (e.g. 32K), where the training becomes unstable, and the training based on LARS and warm-up can\u2019t improve the accuracy compared to the baselines. \n\nSpecific comments: \n\n1.\tIn Algorithm 1, how to choose $ \\eta $ and $ \\beta $ in the experiment?\n2.\tUnder the line of Equation (3), $ \\nabla L(x_j, w_{t+1}) \\approx L(x_j, w_{t}) $ should be $ \\nabla L(x_j, w_{t+1}) \\approx \\nabla L(x_j, w_{t}) $.\n3.\tHow can the training algorithm based on LARS improve the generalization for the large batch? \n4.\tIn the experiments, what is the parameter iter_size? How to choose it?\n5.\tIn the experiments, no descriptions and discussions are given for Table 3, Figure 4, Table 4, Figure 5, Table 5 and Table 6. The authors should give more discussions on these tables and figures. Furthermore, the captions of these tables and figures confusing.\n6.\tOn page 4, there is a statement \u201cThe ratio is high during the initial phase, and it is rapidly decreasing after few epochs (see Figure 2).\u201d This is quite confusing, since Figure 2 is showing the change of learning rates w.r.t. training epochs.\n", "title": "This paper proposes a training algorithm based on Layer-wise Adaptive Rate Scaling (LARS) to overcome the optimization difficulties for training with large batch size. The authors use a linear scaling and warm-up scheme to train AlexNet on ImageNet. The results show promising performance when using a relatively large batch size. The presented method is interesting. However, the experiments are poorly organized since some necessary descriptions and discussions are missing. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJLfcihbz": {"type": "rebuttal", "replyto": "ry33G5FxM", "comment": "1. Comment : \"LR \"warm-up\" can mitigate the unstable training in the initial phase and the proposed method is also motivated by the stability but uses a different approach. However, it seems that the authors also combine with LR \"warm-up\" in your proposed method in the experimental part, e.g., Table 3. So does it mean that the proposed method cannot handle the problem in general?\"\nA: Warm-up alone is not able to mitigate the unstable training for Alexnet. LARS with warm-up can. There is also a new version of algorithm which eliminates warm-up completely\n\n2. Comment: \"There is one coefficient that is independent from layers and needs to be set manually in the proposed local learning rate. The authors do not have a detail explanation and experiments about it. In fact, as can be seen in the Algorithm 1, this coefficient can be as an independent hyper-parameter (even is put with the global learning rate together as one fix term).\"\nA: Agree. In the paper we used fixed trust coefficient $eta\" and changed learning rate. One can used instead fixed global learning rate policy which does not depend on networks, and scale up only trust coefficient . I will add the explanation to the revised paper.\n\n3. Comment \"In the section 6, when increase the training steps, experiments compared with previous methods should be implemented since they can also get better results with more epochs.\" \nA: The point of section 6 was to show that there is no \"fundamental\" limit on the accuracy of large batch training, provided we do train it long enough and regularize well (e.g. increase weight decay or add data augmentation. \n\n4. Comment 4: \"the authors claim that they use initial LR=0.01, but in Table 1(a) it is 0.02\"\n    A: typo is fixed in the revised paper. \n\n  \n", "title": "Reply to Comment 3"}, "r1rHHihWG": {"type": "rebuttal", "replyto": "Sk-rQ7qxf", "comment": "1. Comment 1: \"the methodological contribution is minor\"\n    A: We proposed a new training method, which enable training with large batch of networks which is not possible with all other methods (AFAK).   \n\n2. Comment 2: \" no attempt is made to theoretically justify the proposed heuristic\"\n   A: \" Agree, unfortunately most methods used for deep learning don't have formal proof yet\"\n\n3. Comment 3: \"the method introduces one or two new hyperparameters and it is not clear from the experimental results what overhead is this adding to network training\"\nA: there is one hyper-parameter -  trust coefficient $0<eta<1\"$ . I added the explanation how it depends  to revised paper \n\n4. Comment 4: \"the experiments are done only on a single data set, which is not sufficient to establish superiority of an approach\"\nA: We focused on Imagenet classification only  for large batch training The results in the paper are for 3 models (Alexnet, Alexnet-BN, and Resnet-50).  I will add Googlenet results for completeness. \n\n5.  Suggestions: \" consider using different abbreviation (LARS is used for least-angle regression) \"\n A: Agree, probably LARC (Layer-wise Adaptive Rate Control\" would be better, but the algorithm was already implemented in nvcaffe when we realized that there is a name collision. ", "title": "Reply to comment 2"}, "S1J7jF2-z": {"type": "rebuttal", "replyto": "S1b88I5xM", "comment": "Q: \"Weak points: The training algorithm does not overcome the optimization difficulties when the batch size becomes larger (e.g. 32K), where the training becomes unstable, and the training based on LARS and warm-up can\u2019t improve the accuracy compared to the baselines\"\nA:\n1) Standard recipe \"increase learning rate proportionally to batch size\" does not work  for such networks as Alexnet and Googlenet even with warm-up. LARS is the only algorithm (AFAK) that allows to train Alexnet with batch > 2K to the same accuracy as for small batches\n2) We added Appendix with data on  LARS performanse comparing to other \"Large Batch training\" methods for Resnet-50. \n\nSpecific comments: \n1) Q: \"In Algorithm 1, how to choose $ \\eta $ and $ \\beta $ in the experiment?\"\nA: $ $0<\\eta<1 $, and it depends on the batch size $B$. It grows with B: for example for Alexnet with B=1K  the optimal $\\eta=0.002$, with B=4K  the optimal $\\eta=0.005$, with B=4K  the optimal $\\eta=0.008$,...  Weight decay $\\beta$ is chosen as usual. We found that with large batch it's beneficial to increase weight decay to improve the regularization \n2) typo: fixed in the revised paper \n3) Q: \"How can the training algorithm based on LARS improve the generalization for the large batch?\"\nA: LARS does not replace standard regularization methods (weight decay, batch norm, or data augmentation).  But we found that with LARS we can use larger weight decay than usual,  since LARS automatically limits the norm of weights during training: $|| W(T)|| <= ||W(0)|| * exp \\int_{0}^{T} \\gamma(t) dt$. \n4) Q: In the experiments, what is the parameter iter_size? How to choose it?\n    A: iter_size is used in caffe to emulate large batch if batch does not fit into GPU DRAM.  For example if the batch which fits in GPU memory is 1K, and we want to use B=8K, then iter_size=8.\n5) and 6) We will add more explanation to the revised paper\n ", "title": "Reply to Comment 1"}}}