{"paper": {"title": "Unsupervised Model Selection for Variational Disentangled Representation Learning", "authors": ["Sunny Duan", "Loic Matthey", "Andre Saraiva", "Nick Watters", "Chris Burgess", "Alexander Lerchner", "Irina Higgins"], "authorids": ["sunnyd@google.com", "lmatthey@google.com", "andresnds@google.com", "nwatters@google.com", "cpburgess@google.com", "lerchner@google.com", "irinah@google.com"], "summary": "We introduce a method for unsupervised disentangled model selection for VAE-based disentangled representation learning approaches.", "abstract": "Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. We show that our approach performs comparably to the existing supervised alternatives across 5400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, we show that the ranking produced by our approach correlates well with the final task performance on two different domains.", "keywords": ["unsupervised disentanglement metric", "disentangling", "representation learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors address the important and understudied problem of tuning of unsupervised models, in particular variational models for learning disentangled representations.  They propose an unsupervised measure for model selection that correlates well with performance on multiple tasks.  After significant fruitful discussion with the reviewers and resulting revisions, many reviewer concerns have been addressed.  There are some remaining concerns that there may still be a gap in the theoretical basis for the application of the proposed measure to some models, that for different downstream tasks the best model selection criteria may vary, and that the method might be too cumbersome and not quite reliable enough for practitioners to use it broadly.  All of that being said, the reviewers (and I) agree that the approach is sufficiently interesting, and the empirical results sufficiently convincing, to make the paper a good contribution and hopefully motivation for additional methods addressing this problem."}, "review": {"rkehdCaaKH": {"type": "review", "replyto": "SyxL2TNtvr", "review": "The paper proposes a metric for unsupervised model (and hyperparameter) selection for VAE-based models. The essential basis for the metric is to rank the models based on how much disentanglement they provide. This method relies on a key observation from this paper [A] viz., disentangled representations by any VAE-based model are likely to be similar (upto permutation and sign).\n\nI am inclined to accept the paper for the following reasons:\n1. The proposed approach is clear and easy enough to understand and well motivated\n2. The paper has clearly outlined the assumptions and limitations of their work\n3. The reported result show that models ranked by disentanglement correlate well with the supervised metrics for the various VAE models.\n4. This metric is unsupervised and thus can utilize far more data than the supervised metric methods and can be useful even when the dataset has no labels.\n5. The supplementary material also shows that the metric correlates well with the task performance.\n\n[A] Variational Autoencoders Pursue PCA Directions (by Accident), CVPR 2019\n\n---\n\nUpdate:\n\nThanks for the thoughtful rebuttal by the authors to all the reviewers' feedback.\n\nBased on the several discussions by the other reviewers and the discussion that happened, I am inclined to lower my scores to a weak accept. ", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}, "SyewUag2jH": {"type": "rebuttal", "replyto": "Syl9XBhooH", "comment": "Dear Reviewer,\n\nThank you for changing your score. We really appreciate it. \n\nWe have taken your final comments into account and will work to develop further methods for unsupervised disentangled model selection in the future that address them. We hope, however, that in the meantime UDR can be useful for the practitioners using the current disentangling VAEs, and that this paper encourages others to come up with better unsupervised model selection methods.\n\nIn the recent version of the paper we added a paragraph in Sec. 4 (paragraph 4) that discusses whether the assumptions of Rolinek et al hold for other disentangling models apart from beta-VAE, and point to the new section in the Supplementary Materials that discusses this point in more depth. We are not sure if you have seen it. If you believe that we still have misleading statements in our paper after this change, please let us know and we will address them.\n\nThank you again for engaging in a meaningful discussion with us.", "title": "Thank you for changing your score"}, "Byx8ouwAFr": {"type": "review", "replyto": "SyxL2TNtvr", "review": "This paper proposes a criterion called Unsupervised Disentanglement Ranking (UDR) score. The score is computed based on the assumption that good disentangled representations are alike, while the representations can be entangled in multiple possible ways.  The UDR score can be used for unsupervised hyperparameter tuning and model selection for variational disentangled method.\n\nThe problem this paper focuses on is essential because we usually apply unsupervised disentangled methods to analyze the data when the labels are unavailable. However, existing metrics for hyperparameter tuning and model selection requires ground-truth labels. This paper allows measuring model performance without supervision, making the hyperparameter tuning and model selection possible in practice.\n\nIt looks like some parts of this paper need rewriting. In the abstract, it is not mentioned at all what is the proposed approach. Most paragraphs in the introduction section review the related work and background but do not introduce what assumption and strategy the proposed method adopted.\n\nIt looks like the proposed UDR is theoretically supported by Rolinek et al. (2019). However, the proof given by Rolinek et al. (2019) is for $\\beta$-VAE, where the regularization can be turned into the constraint on KL divergence. I do not think the \"polarised regime\" holds for other disentangled model, for example, TCVAE, where a biased estimation of total correlation is introduced in the objective function. Therefore, I am not convinced that I should trust the results of the UDR, which combines multiple disentangled models.\n\nThe computational process of UDR is heuristic and somewhat arbitrary. There is no theoretical guarantee that UDR should be a useful disentanglement metric.  Although the UDR is supported by some experiments, I am not convinced that it is trustworthy for more complex real-world datasets.\n\nEquation (3) looks problematic. Note that it is possible to train a Bidirectional Generative Adversarial Network (BiGAN) that can generate complex images based on a uniform distribution (Donahue et al., 2016). The encoder of the BiGAN can be considered as the inverse of the generator, which maps images back to the uniform distribution. This suggests that under the encoder-decoder framework, it is possible that latent variables can be informative even the posterior distribution matches the prior distribution. Although VAEs are trained using a different strategy, I do not see why the posterior needs to diverge from the prior distribution for informative latent representations. The encoder might simply be the inverse of the decoder under a certain scenario.\n\nIn summary, this paper focuses on solving an important problem. However, the proposed method is not well supported by theorems as it seems. The paper also appears to contain minor technical issues. Therefore, I am inclined to reject this paper.\n\nReferences\nDonahue, Jeff, Philipp Kr\u00e4henb\u00fchl, and Trevor Darrell. \"Adversarial feature learning.\" arXiv preprint arXiv:1605.09782 (2016).", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}, "HJxUH8voiS": {"type": "rebuttal", "replyto": "SyxL2TNtvr", "comment": "Dear Reviewers,\n\nWe wanted to point out that we are currently working with Bachem and Locatello to add UDR to disentanglement_lib (https://github.com/google-research/disentanglement_lib). We were hoping to have the code released by now, but unfortunately the process is taking longer than expected. We hope to have the code open sourced next week.", "title": "Code to be released on disentanglement_lib soon"}, "r1xDKSPjir": {"type": "rebuttal", "replyto": "rkxuk7PujS", "comment": "Dear Reviewer,\n\nOur models have finished running on ImageNet and CelebA. We have added a new section in the Supplementary Materials to describe the results and a line to reference these results in the main text. Our results show that despite VAEs being notoriously bad at modelling ImageNet, they were still able to learn how to disentangle the coarse representations achieved on this dataset. The models were able to disentangle CelebA well. UDR ranked the models well on both datasets.\n\nAlthough we only had time to run these experiments on the beta-VAE, which is the fastest model class to train among the ones considered in this paper, we will be willing to add extra results on the other models classes for the camera ready version of the paper. ", "title": "Added results on CelebA and ImageNet"}, "BylnVVwioB": {"type": "rebuttal", "replyto": "SJxor_UcjB", "comment": "Dear Reviewer,\n\nThank you for further suggestions on how we can improve our paper. We have added a new section to the Supplementary Materials and a few lines to the main text to both empirically verify the assumption, and to make it explicit that the assumption is mainly motivated by the observations on the beta-VAE, but happens to hold for the other model classes too.", "title": "Added empirical results to verify the assumption"}, "rkxuk7PujS": {"type": "rebuttal", "replyto": "rJl9JqrDir", "comment": "Dear Reviewer, thank you for getting back to us.\n\n1) We agree that there are no guarantees that the other disentangling models necessarily enter the \u201cpolarised regime\u201d. However, the reason why we were quite confident that this would happen in practice for the other methods (apart from DIP-VAE-I) is that the extra disentanglement terms that these approaches add to the ELBO do not act against the VAE\u2019s tendency to \"switch off\" latent dimensions. DIP-VAE-I is the only approach that has an explicit term that penalises the model for switching off latents. We have checked empirically whether the different model classes enter the polarised regime across the hyperparameter sweeps reported in our paper, and found that this is indeed the case. All of the models apart from DIP-VAE-I \u201cswitch off\u201d on average 3/10 latent dimensions across the hyperparameter sweeps. Saying this, you are also right that the UDR computations do not depend on the models being in the \u201cpolarised regime\u201d, so we have re-worded the relevant sections of the paper to avoid confusion.\n\nIn terms of your question about how UDR performs on beta-VAE only, we actually already report UDR performance broken by model class in Figs. 2 and 6 (the latter figure is in the Supplementary Materials). These figures show that UDR ranks beta-VAE models well, both qualitatively and quantitatively, and that its performance for the other model classes is very similar to its performance on beta-VAE.  We have moved some of the results from Fig. 6 to the new Tbl. 1 in the main text. The table reports the correlations between different versions of UDR and the MIG supervised metric across different datasets and model classes. It clearly demonstrates that the different versions of UDR correlate with MIG well and perform comparably regardless of whether they are applied to the beta-VAE or other disentangling VAEs.\n\n2) Thank you for suggesting that we test UDR on more naturalistic datasets than the ones presented in the paper. We have just started a hyperparameter sweep training beta-VAE models on ImageNet, Cifar10 and CelebA. These datasets are complex and contain natural images. We will validate UDR rankings qualitatively by comparing the similarity of the visual latent traversals of the best and worst ranked models on these datasets and will report the results of these experiments as soon as they are ready.\n", "title": "Made changes to the paper and started extra experiments "}, "SygVY8UXir": {"type": "rebuttal", "replyto": "rygVXyLQiB", "comment": "Thank you for getting back to us so quickly. \n\nIn terms of P, it is fine to train S=P models per hyper-parameter. Furthermore, if for whatever reason that is not possible, it is also ok to train S<P models per hyper-parameter but run the All-2-All version of UDR where the models for pairwise comparisons are samples across hyperparameter settings. The results of these different versions of UDR are very similar (e.g. see Figs. 6-7 in the Supplementary Materials).\n\nIn terms of the test on J that you are proposing, Rolinek et al actually did something similar for simple beta-VAE models in their paper (https://arxiv.org/pdf/1812.06775.pdf). They proposed a Distance to Orthogonality measure which basically compared how similar the V of the SVD decomposition was to the best matching signed permutation matrix. Unfortunately they required to run Integer Programming to approximate the permutation matrix which is computationally infeasible once the dimensionality of V becomes large, and hence we would struggle to run it directly on our models. \n\nIn terms of a more qualitative test, it is possible to see that the different models learn the same representation (up to a permutation, sign inverse and subsetting) by looking at the latent traversal plots. In these plots we change the value of one latent at a time while fixing the others, and visualise what effect this has on the reconstruction. For example, Fig. 9  in Supplementary Materials shows such latent traversals for a number of beta-VAEs trained during a hyperparameter sweep. Models 1, 2 and 4 in that plot are highly scored by the UDR and it is clear that their representations are the same up to the UDR assumptions. Models 3, 5 and 6 on the other hand are poorly scored according to UDR and their representations look quite different.\n\n", "title": "Response"}, "ryx4jaGQjB": {"type": "rebuttal", "replyto": "rkehdCaaKH", "comment": "Dear Reviewer, thank you for your feedback.", "title": "Response to Review2"}, "HkeGvpGmoB": {"type": "rebuttal", "replyto": "Byx8ouwAFr", "comment": "Dear Reviewer,\n\nThank you for your thoughtful comments. In your feedback you have brought up three major points: 1) you have expressed doubt whether the \u201cpolarised regime\u201d holds for other disentangling methods other than \\beta-VAE; 2) you were wondering what underlies the thinking behind the choice of our computational process; and 3) you were wondering about our choice of using per latent KL divergence to identify which latents are informative. We will address these points below.\n\n\n1) You have expressed doubt whether the \u201cpolarised regime\u201d holds for other disentangling methods other than \\beta-VAE, which was used as the example model in Rolinek et al. First, even the vanilla VAEs (Kingma and Welling, 2014) are known to enter the \u201cpolarised regime\u201d, which is often cited as one of their shortcomings (e.g. see Rezende and Viola, 2018). All of the disentangling VAEs considered in the paper, including TC-VAE, augment the original ELBO objective with extra terms. Hence, all of them still contain the \\beta KL term of the ELBO with \\beta => 1. This means that in theory all of them inherit the property of the original VAEs of operating in the \u201cpolarised regime\u201d. We have tested this empirically by counting the number of latents that are \u201cswitched off\u201d in each of the 5400 models considered in our paper. We found that all models apart from DIP-VAE-I entered the polarised regime, having on average 2.95/10 latents \u201cswitched off\u201d (with a standard deviation of 1.97). Note that DIP-VAE-I is the only model with an objective that explicitly penalises \u201cswitching off\u201d latent dimensions, which means that it is less suitable for disentangled representation learning in the common regime where the number of generative factors is smaller than the number of latents (as discussed in the original paper by Kumar et al, 2018). Despite DIP-VAE-I never entering the polarised regime, the results reported in our paper (e.g. in Fig. 2 or Fig. 6 in Supplementary Materials) suggest that our proposed UDR still performs well and correlates highly with the supervised metrics for this model class. \n\n2) The computational process proposed in our paper is motivated by the theoretical results presented in Rolinek et al. Unfortunately there is no computationally feasible way to calculate directly whether the SVD decomposition J=U\u03a3V of the Jacobian J of the decoder results in a V, which is a signed permutation matrix. Our approach uses a simple process to approximate this in a computationally feasible way. We have applied our proposed method to a number of datasets commonly used in the literature and have demonstrated that it performs well across 5400 models. Please let us know if you have a particular suggestion for a more complex dataset that you would like to test our metric on.\n\n3) In terms of Equation 3, the GAN objective in the BiGAN paper implicitly minimises the KL divergence between the prior p(z) and the marginalised posterior q(z). However, in Eq.3 we measure the KL divergence between the prior and the conditional posterior q(z|x). Hence, the two are not directly comparable. Eq. 3 is a way to quantify which latent dimensions are used by the network that has entered the \u201cpolarised regime\u201d. Rolinek et al define a model to be in the \u201cpolarised regime\u201d if its latents can be split into two disjoint sets of \u201cused\u201d and \u201cunused\u201d dimensions. \u201cUsed\u201d dimensions are defined as those which have inferred \\sigma^2 << 1, and the \u201cunused\u201d dimensions are defined as those which have \\mu^2 << 1 and \\sigma^2 \\approx 1 (see Sec 3.2, Definition 1 in Rolinek et al). Note that the latter would result in a small KL from a unit Gaussian prior as per Eq. 3 in our paper, thus justifying our choice to find the \u201cused\u201d and \u201cunused\u201d latents. \n\nFinally, we have modified the abstract and introduction to mention our proposed method as per your suggestion.\n", "title": "Response to Reviewer1"}, "ryx9uOzmjB": {"type": "rebuttal", "replyto": "SJgQGjb39r", "comment": "Dear Reviewer,\n\nThank you for your thoughtful comments. In your feedback you have brought up two points: 1) you were wondering whether disentangled representations would benefit subsequent tasks; 2) you were wondering about our choice of certain hyperparameters. We will address these questions below:\n\n1) In terms of whether disentangled representations would benefit subsequent tasks, we believe that it is important to consider the nature of the task, and whether it implicitly assumes any of the properties that disentangled representations possess. For example, if one is trying to solve a binary classification task based on the value of a single pixel in a high-dimensional image, it is unlikely that a disentangled representation will be useful. Indeed, a disentangled representation will most likely learn to ignore this pixel, since it doesn\u2019t contribute much to the quality of the reconstruction. On the other hand, an entangled representation learnt implicitly through a supervised objective aiming to solve the task will throw away all information apart from the value of the relevant pixel and hence will be much more informative for that particular task. \n\nOn the other hand, if one is interested in solving a large number of natural tasks in a single environment (e.g. learning to achieve different values of the score in an Atari game, generalising policies to variations in the game colour schemes, fast language binding problems, data efficient classification of object identities, colours, sizes or relations), then a disentangled representation may be of more relevance, since it will produce the semantically meaningful equivariant compositional representation that will support many variations of these tasks. Hence, we believe that disentangled representations will be useful for those tasks that require compositionality, generalisation, data efficiency or generalisation/transfer.\n\nYou were also wondering whether the real world data generative process follows the independence assumption presumed by disentangled representations. To answer this question we would like to refer you to the recently proposed alternative view on disentangled representations that moves away from considering independent generative factors (Higgins et al, 2018). The new definition suggests that disentangled representations instead reflect the compositional natural symmetry transformations. The implication of this definition is that one can move away from assuming IID training data generated by independent generative factors, and instead think about which aspects of the world can be transformed independently of each other, and how these transformations can be discovered through embodied active learning (see Caselles-Dupre et al, 2019 for a first effort in that direction).\n\n2) You were wondering about the choice of the 0.01 threshold in Eq. 3, and whether it was set using a \u201cqualitative feeling\u201d. The answer is no. This equation quantifies which latent dimensions are used by the network that has entered a \u201cpolarised regime\u201d. Rolinek et al define a model to be in a \u201cpolarised regime\u201d if its latents can be split into two disjoint sets of \u201cused\u201d and \u201cunused\u201d dimensions. \u201cUsed\u201d dimensions are defined as those which have inferred \\sigma^2 << 1, and the \u201cunused\u201d dimensions are defined as those which have \\mu^2 << 1 and \\sigma^2 \\approx 1 (see Sec 3.2, Definition 1). Note that the latter would result in a small KL from a unit Gaussian prior as per Eq. 3. Empirically we found that 0.01 was a good threshold, since the KL values have a bimodal distribution, with on average around 97% of all \u201csmall kl\u201d values lying below this threshold. \n\nIn terms of P, we do not suggest using P<5. Training P seeds per hyperparameter setting is the largest computational overhead of the UDR, however it is subsumed by the largely accepted good research practice of training a number of seeds per hyperparameter setting anyway. The rest of the UDR computations are very fast. To give you an idea, running a single pairwise comparison using UDR Lasso takes around 4 seconds on a standard CPU, and it takes around 1600 seconds on average to compute UDR Lasso with P=50 for 300 models within a hyperparameter sweep, when we parallelise the pairwise comparisons per model.\n\nFinally, how would you propose that we quantitatively evaluate whether the representations learnt by the models are the same up to the UDR assumptions apart from running the UDR itself? Maybe we misunderstood your question...\n", "title": "Answer to Reviewer4"}, "SJgQGjb39r": {"type": "review", "replyto": "SyxL2TNtvr", "review": "This paper addresses the problem of unsupervised model selection for disentangled representation learning. Based on the understanding of \u201cwhy VAEs disentangle\u201d [Burgess et al. 2017, Locatello et al. 2018, Mathieu et al. 2019, Rolinek et al. 2019], the authors adopt the assumption that disentangled representations are all alike (up to permutation and sign inverse) while entangled representations are different, and propose UDR method and its variants. Experimental results clearly show that UDR is a good approach for hyperparameter/model selection.\nOverall, I think a reliable metric for model selection/evaluation is needed for the VAE-based disentangled representation learning. According to comprehensive experimental studies performed in this paper, UDR seems to be a potentially good choice.\n\nHowever, I am not sure if very good disentangled representations must benefit (general) subsequent tasks, though the authors provide experimental evidence on fairness classification and data efficiency tasks. Actually, the data generation process in the real-world may consist of different generative factors that are not independent of each other. Though good disentangled representation provides good interpretability, it needs not to be better than entangled representation for concrete tasks. Specifically, for concrete supervised classification tasks, VAE with beta smaller than 1 (not towards disentanglement) might be the best (Alexander A. Alemi et al. 2017, Deep VIB).\n\nAnother concern is about the choice of some key \u201chyperparameters\u201d.\nFor the KL divergence threshold in equation 3, you set it to be 0.01. It looks like the choice would control how much the UDR favors a \u201csparse representation map\u201d. The larger the value, the few \u201cinformative dimensions\u201d would be considered.\nIn supplementary material, you say that \u201cuninformative latents typically have KL<<0.01 while informative latents have KL >>0.01\u201d. Is this judgment based on \u201cqualitative feeling\u201d? For me, as you are contributing a ``quantitative measurement\u201d, it is interesting and important to see how this threshold would generally affect UDR\u2019s behavior in one (or more) datasets you have tried. \nAnother hyperparameter I cared is P (number of models for pairwise comparison). In the paper, you validate the effect of P in the range [5,45]. How would P smaller than 5 affect UDR? According to Table 1, if I was using UDR, I\u2019d rather using P>=20 (or at least 10) rather than 5.\nAlso, it seems to me P would grow up due to the size of factors that generate the data. Thus, I also have a little concern about the computation cost of the proposed metric (as also mentioned by the authors).\n\nOthers concerns:\n-- As a heavy experimental paper, most experimental results are in supplementary material, while the authors spent a lot of time in the main text explaining the conclusions found in other papers.\n-- To validate the fundamental assumption of UDR, the authors might consider to quantitatively validate that, disentangled representations learned by those approaches you used in the paper are almost the same (up to permutation and sign inverse). ", "title": "Official Blind Review #4", "rating": "6: Weak Accept", "confidence": 2}, "rylxuPrq_r": {"type": "rebuttal", "replyto": "Byec_l_zuH", "comment": "Table 2 shows which of the qualities often assigned to disentangled representations are actually assessed by the different metrics. We do not make a judgement as to which of these are desirable or not and instead leave that to the metric users. However, we hope that the table can help the reader understand why the different metrics sometimes rank the same models slightly differently.\n\n", "title": "Further clarification"}, "ByxXVo1fuS": {"type": "rebuttal", "replyto": "B1l7pf8-Or", "comment": "Thank you for your comment. We followed the notation and supervised metric choices made in Locatello et al (2019) and hence use \"DCI Disentanglement\" to denote just the \"disentanglement\" part of the Eastwood and Williams (2018) metric. This is why by this definition \"DCI Disentanglement\" only measures disentanglement/modularity and does not measure compactness/completeness or explicitness/informativeness. We hope this answers your question.", "title": "Explanation of Table 2"}}}