{"paper": {"title": "Accidental exploration through value predictors", "authors": ["Tomasz Kisielewski", "Damian Le\u015bniak", "Maia Pasek"], "authorids": ["tymorl@gmail.com", "damian.lesniak@doctoral.uj.edu.pl", "maiapasek@gmail.com"], "summary": "We study the biases introduced in common value predictors by the fact that trajectories are, in practice, finite.", "abstract": "Infinite length of trajectories is an almost universal assumption in the theoretical foundations of reinforcement learning. In practice learning occurs on finite trajectories. In this paper we examine a specific result of this disparity, namely a strong bias of the time-bounded Every-visit Monte Carlo value estimator. This manifests as a vastly different learning dynamic for algorithms that use value predictors, including encouraging or discouraging exploration.\n\nWe investigate these claims theoretically for a one dimensional random walk, and empirically on a number of simple environments. We use GAE as an algorithm involving a value predictor and evolution strategies as a reference point.", "keywords": ["reinforcement learning", "value predictors", "exploration"]}, "meta": {"decision": "Reject", "comment": "The paper studies the mismatch between value estimation in RL from finite vs. infinite trajectories. This is an interesting problem, but the reviewers raised concerns regarding (1) the consistency and coherence of the story (2) the significance of theoretical analysis and (3) significance of the results. I appreciate that the authors made significant changes to the paper to address the comments. However, given the extent of changes, I think another review cycle is needed to check the details of the paper again."}, "review": {"SygD5bm2RX": {"type": "rebuttal", "replyto": "SkxaDU2OAX", "comment": "We thank the reviewer for these comments. We are happy to hear that now the paper is more readable.\n\n1. Defining an environment in which longer trajectories are more computationally expensive is a very interesting problem. We agree that such environments probably exist -- for example any environment which procedurally generates a world and has to keep it in memory might be a good candidate.\n\nOn the other hand, one could argue that such environments will not be MDPs -- computing the transition from state s should depend only on s and the chosen action, thus the simulation cost can vary for different state-action pairs, but cannot increase in time. Also, if we assume that the process is non-stationary, and modify the agent accordingly, then the effects due to the time-bound bias will probably disappear -- see the experiments in section 6.3, where we include time in the state space.\n\nIt is worth mentioning that using constant-length trajectories that are relatively short is quite popular. For example, consider the OpenAI Gym implementation of the MountainCar environment (T=200), which is for many young researchers one of the first forays into reinforcement learning, and Schulman's et al. experiments with GAE (\"Each episode was terminated after 2000 timesteps if the\nrobot had not reached a terminal state beforehand\"). Therefore, we think that our paper is valuable as a straightforward argument against this practice: we introduce the time-bound bias, analyse it on simple environments (where its influence can be isolated), and warn about its ill effects.\n\n2. We agree with the reviewer, but with one caveat -- even if the bias has small magnitude, its shape remains the same. Thus, we have to be careful not to amplify the bias later on, e.g. by normalising the advantage, as the effects on exploration might manifest again (in particular, this will happen if a random agent receives constant reward).\n\nSince rewards after more than n steps have low impact on the state's value, we could simply use n-step rollouts for value estimation, even for the early states. This approach should completely eliminate the effects on exploration, as they stem from using shorter rollouts for later states. Certainly, this\nis true for environments with constant reward, as the estimated value will also be constant. We can even choose a relatively small n, and even though the time-bound bias might still be substantial, now it should only be responsible for making the agent prefer instant rewards.\n\nPerhaps the main disadvantage of this approach is low sample efficiency -- we do not use the last n rewards to update the estimated value of the last n states. Intuitively, it should be possible, but we point out that simple solutions do not work well.\n\n3. Possibly we do not understand correctly, but we think we are doing something similar in section 6.2 (figure 7) -- we update the value predictor using sums of the rollouts, and the, properly discounted, current estimation of the last state's value.\n\nMaybe the reviewer meant n-step bootstrapping for all but the last n states? In such case, we are quite confident this would eliminate the effects on exploration, because they should appear only if we used smaller values of n for later states. This again would have the disadvantage of low sample efficiency for large n.\n\nOnce again we would like to thank the reviewer for their helpful and encouraging comments.", "title": "Further response"}, "rkgn8XzboX": {"type": "review", "replyto": "S1llBiR5YX", "review": "\nUPDATE:\n\nI have read the authors\u2019 response and the other reviews.  While the authors have made some improvements, my core criticism remains \u2013 the paper does not produce concrete theoretical or empirical results that definitively address the problems described.  In addition, there are many confusing statements throughout the paper.  For instance, the discussion of positive and negative rewards in the introduction does not conform with the rest of the literature on exploration in RL.\n\nThe authors also seemed to have missed the point of the Kearns & Singh reference.  The authors are right that the older paper is a model-based approach, but the idea is that they too were solving infinite-horizon MDPs with finite trajectories and not introducing a bias.  \n\n\nSummary:\n\nThe paper attempts to link the known mismatch between infinite horizon MDP values and finite trajectory sums to the problem of exploration.  Trajectories in environments requiring exploration (mountain car and a number-line walk) are shown and the effects of changing trajectory lengths and initial values are discussed.  Potential solutions to the problem are proposed though the authors did not deem any of the solutions satisfactory.\n\n\nReview:\n\nThe paper brings up a number of important issues in empirical reinforcement learning and exploration, but fails to tackle them in a manner that convincingly isolates the problem nor proposes a solution that seems to adequately address the issue.  Specifically, several issues seem to be studied at once here (including finite-horizon MDPs, function approximation, and exploration), relevant work from the exploration and RL community is not cited, the early experiments do not reach a formal theoretical claim, and the proposed solutions do not appear to adequately address the problem).  These issues are detailed below.\n\nFirst, the paper is considering many different issues and biases at once, including those introduced by initialization of the value function, exploration policies, function approximation, and finite/infinite length trajectories.  While the authors claim in several places that they show one bias is more important than another, no definitive experiment or theorem is given showing that finite-length trajectories are the cause of bad behavior.  While it is well known that infinite-horizon MDPs do not exactly match value functions for finite horizon MDPs, so many other factors are included in the current work (for instance the use of neural networks) that it remains unclear that the finite/infinite mismatch is an issue.\n\nThe paper also fails to cite much of the relevant work on these topics.  For instance, the use of infinite-horizon MDPs to study finite learning trajectories is often done under the guise of epsilon-optimal guarantees, with  epsilon derived from the discount factor (see \u201cNear-Optimal Reinforcement Learning in Polynomial Time\u201d).  In addition, the effects shown in mountain car when changing the values or the initialization function, mirror experiments with Q-learning that have shown that there is no one initialization scheme that guarantees optimal exploration (see Strehl\u2019s thesis \u201cProbably Approximate Correct Exploration in Reinforcement Learning\u201d ).  Overall, the paper seems to confuse the problems of value initialization and trajectory length and does not show that they are particularly related.\n\nIn addition, the early sections covering theoretical models such as Wiener Processes and Random Walks lay out many equations but do not come to a specific formally proven point.  No theorem or proof is given that compactly describes which exact problem the authors have uncovered.  Therefore, when the solutions are presented, it remains unclear if any of them actually solve the problem.\n\nFinally, several of the references are only ArXiv pre-prints.  Papers submitted to ICLR or other conferences and journals should only cite papers that have been peer-reviewed unless absolutely necessary (e.g. companion papers).\n", "title": "not formal enough", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJlYSahViQ": {"type": "review", "replyto": "S1llBiR5YX", "review": "The paper addresses the problem of truncating trajectories in Reinforcement Learning. The scope is right for ICLR. \n\nThe presentation is pretty good as far as the English goes but suffers from serious problems on the level of formulating concepts. Overall, I believe that the issue addressed by the paper is important, but I am not sure whether the approach taken by the authors addresses it.\n\nI have the following complaints.\n1. The paper suffers from a fundamental confusion about what problem it is trying to address. There are two straightforward ways to formulate the problem. First, we can formulate the task as solving an MDP with infinite trajectories and ask the question of what we can learn training from finite ones. The learned policies would then be evaluated on the original MDP (in practice using trajectories that are, say, an order of magnitude longer or, for a simple MDP, analytically). Second, we can consider the family of episodic MDPs parametrised by trajectory length T and ask the question of what we can learn by training on some values of T and evaluating on others. These problems are similar, but not the same and should be carefully distinguished. Right now, the introduction reads like the authors were trying to use the first approach but Section 4 reads like they are doing the second: \"If during training we just used times up to T, but deployed the agent in an environment with times greater than T\". Either way, the concept of bias, which appears throughout the paper, isn't formally defined anywhere. I believe that a paper that claims to address bias should have an equation that defines it as a difference between two clearly defined quantities. It should also be clearly and formally distinguished which quantities are deterministic and which are random variables. The introduction seems to (implicitly?) define bias as a random variable, section 3.1 seems to talk about \"bias introduced by initializing the network\". As the paper stands now, the working definition of bias used by the authors seems to be that some quantity is vaguely wrong. I do expect a higher standard of clarity in a scientific paper,\n\n2. The paper confounds the problem of learning the value function, specifying the initial estimates of the value function and exploration. The analysis of exploration is entirely informal and suffers from the lack of clear problem formulation as per (1). Of course, one can influence exploration by initialising the value function in various ways, and this may respond differently to different truncations (different values of T), but I don't see how it is related to the \"bias\" problem that the paper is trying to address. In any case, I wish the authors either provided a formal handle on exploration or shift the focus of the paper and remove it,\n\n3. I don't see what hypothesis the experiments are trying to test. Clearly, if I train my agent on a different MDP and test it on a different one, I get a mismatch. The lack of clear definitions as per (1) comes back with a vengeance.\n\n4. Section 1.1 seems to exist purely to create a spurious impression of formality, which bears little relevance for what the paper is actually about. RL, as traditionally formulated, uses discrete time-steps so the Brownian motion model developed in this section doesn't seem very applicable - it is true that it is a limiting case of a family of discrete chains, but I don't see how this produces any insights about RL - chains are easy to simulate, so why not test on a chain directly? In any case, the result shown in Section 1.1 is entirely standard, can be found in any textbook on stochastic processes and was likely introduced purely to cover for the lack on any substantial theory that comes form the authors.\n\nTo summarize: if the authors address these points, there is a possibility that the ideas presented in this draft may somehow lead to a paper at some later point. However, I feel that the required changes would be pretty massive and don't see how the authors could make it during the ICLR revision phase - the problems aren't details or technicalities but touch the very substance of what the paper is trying to do. Basically, the whole paper would need to be re-written.\n\nanother minor point:  sloppy capitalization in section 2.1\n\n===================\n\nFor reasons outlined in my comment below, I updated the score to 5 (for the new heavily updated version).\n", "title": "Papers suffers from confounding of concepts, requires overhaul; in this state it is clearly not publishable.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1gFNcyU0Q": {"type": "rebuttal", "replyto": "rkgn8XzboX", "comment": "We thank the reviewer very much for their comments, they were very helpful in improving our\nwork. We revised the paper to make it more formal and focused on the issue we\nwant to present. In a newly added chapter 2 The time-bound bias we provide a formal definition of the bias and a clear statement of the problem.\n\n\u201cFirst, the paper is considering many different issues and biases at once, including those introduced by initialization of the value function, exploration policies, function approximation, and finite/infinite length trajectories. While the authors claim in several places that they show one bias is more important than another, no definitive experiment or theorem is given showing that finite-length trajectories are the cause of bad behavior. While it is well known that infinite-horizon MDPs do not exactly match value functions for finite horizon MDPs, so many other factors are included in the current work (for instance the use of neural networks) that it remains unclear that the finite/infinite mismatch is an issue.\u201d\n\nOur only goal is to analyse the effects of the time-bound bias. The last paragraph of chapter 2 explains which other biases we need to take into consideration. There, we describe the steps we\u2019ve taken to ensure that they are negligible in our setting - i.e. after every update of the agent the value network is trained long enough to reflect the value estimation produced by the Every-visit Monte Carlo estimator.\nWe could replace GAE with tabular methods, but we think it is quite fun to show that GAE fails to solve one-dimensional gridworld with truncated trajectories, in contrast to the policy gradient.\n\n\u201cThe paper also fails to cite much of the relevant work on these topics. For instance, the use of infinite-horizon MDPs to study finite learning trajectories is often done under the guise of epsilon-optimal guarantees, with epsilon derived from the discount factor (see \u201cNear-Optimal Reinforcement Learning in Polynomial Time\u201d). In addition, the effects shown in mountain car when changing the values or the initialization function, mirror experiments with Q-learning that have shown that there is no one initialization scheme that guarantees optimal exploration (see Strehl\u2019s thesis \u201cProbably Approximate Correct Exploration in Reinforcement Learning\u201d ). Overall, the paper seems to confuse the problems of value initialization and trajectory length and does not show that they are particularly related.\u201d\n\nWe have introduced a short discussion of the related work, including the article \u201cNear-Optimal Reinforcement Learning in Polynomial Time\u201d. We are particularly grateful for pointing out this paper, because it gave us an opportunity to underline what the focus of our work is.\nThe reviewer\u2019s remark about initialisation of the value network is, if we understand correctly, a result of confusion caused by lack of clarity in the submitted version of our paper. We apologize for this. The only time we want to discuss value network bias is the case of the MountainCar+1 environment, where the effects caused by the time-bound bias should be nonexistent, but the agent still (but rarely) explores. We try to explain this phenomena in terms of the value network bias, but this is unrelated to the main concern of this paper - we moved the discussion to the Appendix. This is also why we don\u2019t reference the paper concerning initialisation schemes.\n\n\u201cIn addition, the early sections covering theoretical models such as Wiener Processes and Random Walks lay out many equations but do not come to a specific formally proven point. No theorem or proof is given that compactly describes which exact problem the authors have uncovered. Therefore, when the solutions are presented, it remains unclear if any of them actually solve the problem.\u201d\n\nIn a newly added subsection 3.1 Bias prevalence we connect the results of discrete random walk analysis with other environments. We argue that since the time bound bias is continuous with respect to the reward and policy, then as long as rewards are of a similar order of magnitude and are either usually positive or usually negative, the value estimator should end up with a sharp peak close to the starting state, thus resulting in exploration boost or discouragement. The rest of the paper tries to check these claims empirically, and explain the mechanism.\n\n\u201cFinally, several of the references are only ArXiv pre-prints. Papers submitted to ICLR or other conferences and journals should only cite papers that have been peer-reviewed unless absolutely necessary (e.g. companion papers).\u201d\n\nWe also thank the reviewer for pointing out the ArXiv citations -- two of the papers cited that way were actually accepted to previous editions of ICLR, but we failed to cite them appropriately. In the current version the only pre-print we cite is the OpenAI Gym whitepaper.\n", "title": "Response"}, "HJx1-qyURQ": {"type": "rebuttal", "replyto": "BJlYSahViQ", "comment": "We would like to thank the reviewer very much for their comments, they were extremely helpful in improving our work.\n\nTo address the complaints:\n\n1. We edited the paper to make the concepts more clear. In a newly added chapter 2. The time-bound bias we provide, among others, a formal definition of the bias.\nWe always assume that we are trying to solve an infinite MDP, while learning on trajectories of length T. We\u2019ve removed all the confusing parts and further clarified in chapter 2.\n\n2. Our only goal is to analyse the effects of the time-bound bias. The last paragraph of chapter 2 explains which other biases we need to take into consideration. We argue that they are negligible in our setting. The \"bias introduced by initializing the network\" becomes relevant only if we significantly lessen the time-bound bias - a short discussion about the value network biases was moved to the Appendix C.\n\n3. We trained and tested all the agents on the same environments. After each modification of the environment the agent and the value network were trained again from scratch. The experiments show the influence of the time-bound bias on exploration - this is why we modify the environments slightly, and compare the changes in the agent\u2019s policy (after retraining) with those we would expect to happen in the infinite-time case. For example, adding a constant value to all rewards in the MountainCar environment does not change the objective nor the optimal policy, but the GAE agent behaves completely differently. The figures show only the history of training, but in our opinion this is sufficient to illustrate the effects we investigate.\n\n4. \u201cSection 1.1 seems to exist purely to create a spurious impression of formality (...) and was likely introduced purely to cover for the lack on any substantial theory that comes from the authors.\u201d\nThis remark is particularly hard for us to address. We will try our best to explain the reasons behind introducing the Wiener process analysis, and let the reviewer judge our true intentions.\nThe continuous case is a good approximation of the discrete random walk, and we are able to explain why the bias generates a sharp peak by calculating the one-sided derivative at the beginning of the walk. We simply think that is an even more basic example, and the final equations provide additional insight.\nOn the other hand, we fully agree that this chapter is redundant, and disrupts the paper\u2019s flow, so we decided to move it to the Appendix.\n\nAs the reviewer suggested, to improve clarity of the paper changes in the structure needed to be quite massive, but we, hopefully, managed to address all the mentioned problems, without the need to modify the experiments or the conclusions. We are fully aware that this fact alone is sufficient to disqualify our submission, nevertheless we would be extremely grateful for any further comments.\n", "title": "Response"}, "rJgv5FJU07": {"type": "rebuttal", "replyto": "BJgXLXSp3m", "comment": "We thank the reviewer very much for their comments. We edited the paper to make the\nconcepts clearer.\n\n\n\u201cThe comparison of policy gradient methods is too old.\u201c\nWe use the policy gradient mainly to show that, despite being basic, it can still outperform\nalgorithms with value predictors if the time-bound bias degrades their performance.\nWe also use the ES as a reference algorithm, that performs well and is insensitive to the time-bound bias.\n\n\u201cThe experimental environment is very simple in reinforcement learning tasks, and the authors should look for more complex environments for comparison.\u201d\nUsing simple environments is the only way to ensure that the observed effects are solely caused by the time-bound bias. At the end of section 3. Random walk we have added a paragraph explaining what other biases we need to consider, and why they\u2019re negligible in this setting.\n\n\u201cThe experiment results are hard to interpret.\u201d\nWe tried our best to clarify in the current version. Hopefully this makes the results more clear-cut.\n\nQ1:\nThis condition was introduced just for simplicity, as varying rewards would lengthen the analysis without helping to convey the core idea. In a newly added subsection 1.1 Influence on exploration we discuss two factors that cause the exploration effects. Firstly, if the missing suffix (after truncating the trajectory to length T) of the total discounted reward in state \u2018s\u2019 is positive (resp. negative), then the bias lowers (resp. increases) the learned value of \u2018s\u2019. Secondly, the later the state \u2018s\u2019 appears in trajectories, the stronger the bias. Combination of these two factors causes the sharp peak in estimated value function.\nLater, in a newly added subsection 3.1 Bias prevalence, we also point out that the effects are local in time, in the sense that if an agent ends up in a previously unexplored region of state space that exhibits the required properties, the value estimator of the new states will be similarly biased.\nPractical effects of non-constant reward are shown in section 5.2 Axis walk.\n\nQ2:\nWe tried clarifying this in the revised version. At the end of the section 3 Random walk we say that the optimal policy for the (initially) estimated value is to always stay at 0, therefore the generalized policy iteration will be strongly biased towards this particular policy. Additionally, learning using an advantage function will have a similar effect, as in the case of constant reward the preference stems only from the value function.\nLater we show empirically that the advantage estimator used by GAE also causes the exploration effects.\n\n\nQ3:\nVariant of learning algorithm used in this case -- either the ES, GAE or policy gradient. The colours in the figures correspond to these algorithms.\n\nOnce again we thank the reviewer for the comments.", "title": "Response"}, "BJgXLXSp3m": {"type": "review", "replyto": "S1llBiR5YX", "review": "Paper summary: This paper focuses on the case where the finiteness of trajectories will make the underlying process to lose the Markov property and investigates the claims theoretically for a one-dimensional random walk and Wiener process, and empirically on a number of simple environments. \n\nComments: The language and the structure of the paper are not on a very good scientific level. The paper should be proofread as it contains a lot of grammatical mistakes. \n\nGiven the assumption that every state's reward is fixed, the theoretical analysis is trivial.\n\nThe comparison of policy gradient methods is too old. The authors should look for more advanced methods to compare.\n\nThe experimental environment is very simple in reinforcement learning tasks, and the authors should look for more complex environments for comparison. The experiment results are hard to interpret. \n\n\n\nQ1: In the theoretical analysis, why should the rewards for each state be fixed?\n\nQ2:Why use r_t \u2013 (V(s_t)-\\gammaV(s_{t+1})) as the advantage function?\n\nQ3: What does the \u201cvariant\u201d mean in all figures? \n\nTypos: with lower absolute value -> with lower absolute values \n", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}