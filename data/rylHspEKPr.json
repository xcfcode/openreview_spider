{"paper": {"title": "Learning to Represent Programs with Property Signatures", "authors": ["Augustus Odena", "Charles Sutton"], "authorids": ["augustusodena@google.com", "csutton@inf.ed.ac.uk"], "summary": "We represent a computer program using a set of simpler programs and use this representation to improve program synthesis techniques.", "abstract": "We introduce the notion of property signatures, a representation for programs and\nprogram specifications meant for consumption by machine learning algorithms.\nGiven a function with input type \u03c4_in and output type \u03c4_out, a property is a function\nof type: (\u03c4_in, \u03c4_out) \u2192 Bool that (informally) describes some simple property\nof the function under consideration. For instance, if \u03c4_in and \u03c4_out are both lists\nof the same type, one property might ask \u2018is the input list the same length as the\noutput list?\u2019. If we have a list of such properties, we can evaluate them all for our\nfunction to get a list of outputs that we will call the property signature. Crucially,\nwe can \u2018guess\u2019 the property signature for a function given only a set of input/output\npairs meant to specify that function. We discuss several potential applications of\nproperty signatures and show experimentally that they can be used to improve\nover a baseline synthesizer so that it emits twice as many programs in less than\none-tenth of the time.", "keywords": ["Program Synthesis"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors propose improved techniques for program synthesis by introducing the idea of property signatures. Property signatures help capture the specifications of the program and the authors show that using such property signatures they can synthesise programs more efficiently.\n\nI think it is an interesting work. Unfortunately, one of the reviewers has strong reservations about the work. However, after reading the reviewer's comments and the author's rebuttal to these comments I am convinced that the initial reservations of R1 have been adequately addressed. Similarly, the authors have done a great job of addressing the concerns of the other reviewers and have significantly updated their paper (including more experiments to address some of the concerns). Unfortunately R1 did not participate in subsequent discussions and it is not clear whether he/she read the rebuttal. Given the efforts put in by the authors to address different concerns of all the reviewers and considering the positive ratings given by the other two reviewers I recommend that this paper be accepted. \n\nAuthors,\nPlease include all the modifications done during the rebuttal period in your final version. Also move the comparison with DeepCoder to the main body of the paper."}, "review": {"HJge9Qkx9r": {"type": "review", "replyto": "rylHspEKPr", "review": "I would like to be able to recommend accepting this paper, but I can't.  It describes two contributions to the community that could be valuable:\n\n1: searcho, a programming language designed for studying program synthesis via search over programs\n2: A method of automatically choosing interesting features for guided program search. \n\nThe paper does not give enough evidence to ascertain the value of either contribution.  There are now a large number of program synthesis works using ML, and a huge literature on program synthesis without ML.   While many of ML works use a DSL as the testbed, surely the authors' feature selection method can be applied in some of these DSLs, allowing comparisons with current art?   On the other hand, many of the ML methods don't require the DSL used in the works that introduce them.  Can searcho + your set of training and test programs distinguish between these methods, and establish a benchmark?  Are there some more realistic tasks/applications that searcho makes reasonable, not readily approached with previous DSLs used for testing these algorithms?   I think this paper could be valuable if it could demonstrate either in what ways the new feature selection algorithm is  an improvement over prior ML methods, or that searcho is valuable as a benchmark or for approaching interesting applications.\n\n\n######################################################################################\nedit after author response:  raising to 6, I think the deepcoder experiments are useful in contextualizing the contribution of the authors' feature selection method.   \n\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "r1xnqi9noB": {"type": "rebuttal", "replyto": "rylHspEKPr", "comment": "We've added a description of the DeepCoder comparison experiment to the appendix. \n\nWe plan to move it into the main text after some wrangling.", "title": "Revision"}, "r1l4aLe9oS": {"type": "rebuttal", "replyto": "Skxqu-xjcH", "comment": "Hi and thanks again,\n\nWe've run a new experiment that we believe largely addresses your 3rd point.\nWe wonder if, in light of this new experiment and our previous response (which addresses your 1st and 2nd points),\nyou might consider increasing your score slightly?\nIn light of the other two reviews, a score of 1 seems perhaps a bit harsher than is warranted?\n\nPlease let us know if there are any other questions we can answer.\n", "title": "New Experiment"}, "H1l-7Slqir": {"type": "rebuttal", "replyto": "HJge9Qkx9r", "comment": "Hi and thanks again for the review!\n\nWe believe we've addressed (or at least made substantial progress toward addressing) your main criticism:\nPlease see our comment titled: 'New Experiment: Comparison with DeepCoder' and let us know if you have\nany questions about that comment or about our original response.\nWe would be happy to expand on them or run additional comparisons as time permits.\n", "title": "New Experiment"}, "HklF9bg9ir": {"type": "rebuttal", "replyto": "rylHspEKPr", "comment": "We have conducted an experiment to compare premise selection using Property\nSignatures to the premise selection algorithm from [1].\nThis required considerable modifications to the experimental procedure:\n\nFirst, since the premise-selection part of DeepCoder can only handle Integers\nand lists of Integers, we restricted the types of our training *and* test\nfunctions. In particular, we read through [1] and found four\nfunction types in use:\n\nf :: [Int] -> [Int]\ng :: [Int] -> Int\nh :: ([Int], [Int]) -> Int\nk :: ([Int], Int) -> Int\n\nThe types of f and g  are taken directly from\n[1]. The types of h and k are inferred from examples given in the\nappendix of [1]. Their DSL does not technically have tuples,\nbut we have wrapped the inputs of their 'two input functions' in tuples for\nconvenience. \n\nSecond, since DeepCoder can only handle integers between -255 and 255, we\nfirst re-generated all of our random inputs (use for 'hashing' of generated\ntraining functions) to lie in that range. We then generated random training functions\nof the above four types. We then made a data set of training functions\nassociated with 5 input-output pairs,\nthrowing out pairs where any of the outputs\nwere outside the aforementioned range, and throwing out functions where\n*all* outputs contained some number outside that range.\n\nThird, of the examples in our test set with the right types, we modified their\ninput output pairs in a similar way.\nWe filtered out functions that could not be so modified.\nAfter doing so, we were left with a remaining test suite of 32 functions.\n\nFinally, we trained a model to predict functions-to-use from learned embeddings\nof the input-output pairs, using the architecture described in [1].\nWe didn't see a description of how functions with multiple inputs had their\ninputs represented, so we elected to separate them with a special character,\ndistinct from the null characters that are used to pad lists.\n\nCompared with the Property Signatures method, this technique results in far\nfewer synthesized test set programs.\nWe did 3 random restarts for each of DeepCoder, Property Signatures, and the\nRandom Baseline (recall that the random baseline itself is already a relatively\nsophisticated synthesis algorithm - it's just the configurations that are\nrandom).\nThe DeepCoder runs synthesized an average\nof 3.33 test programs, while the Property Signature runs (trained on the same\nmodified training data and tested on the same modified test data) synthesized\n16.33. The random baseline synthesized 3 programs on average.\n\nA priori, this might seem like a surprisingly large gap, but it actually fits with\nwhat we know from existing literature.\nIt's well known that deep learning techniques perform poorly under distribution shift in general, \nand [2] observe something similar for program synthesis:\nwhich is that DeepCoder-esque techniques tend to generalize poorly to a\na test set where the input-output pairs come from a different distribution\nthan they do in training.\nThis is the case in our experiment, and it will be the case in any realistic\nsetting, since the test set will be provided by users of the synthesis tool.\nProperty Signatures are (according to our experiments) much less sensitive to such shift.\nThis makes intuitive sense: whether an input list is half the length of an output list (for instance) is\ninvariant to the particular distribution of members of the list.\n\nNote that even if Property Signatures did not outperform DeepCoder on this\nsubset of our test set, they would still constitute an improvement due to their\nallowing us to operate on arbitrary programs and input types.\n\n\n[1] DEEPCODER: LEARNING TO WRITE PROGRAMS \n  ( https://openreview.net/pdf?id=ByldLrqlx )\n\n[2] Synthetic Datasets for Neural Program Synthesis\n  ( https://openreview.net/forum?id=ryeOSnAqYm )\n\nPS:\nWe are updating the draft to include these results, but we thought it would be easier\nto follow a comment here than to try and see the difference between 2 PDFs.\n", "title": "New Experiment: Comparison with DeepCoder"}, "Byldv8pesB": {"type": "rebuttal", "replyto": "BJxYtKBlcr", "comment": "Thanks very much for the review!\n\nWe will use > for quotes and respond point-by-point below:\n\n> it is quite unclear to me how they simplify the generation of programs that combine smaller/simpler programs\nWe will expand the explanation of this part in the text.\n\n> Sect 3.2 on how to learn useful properties is rather vague and it would need a much more detail explanation.\nYou're right - we will fix this.\n\n\n> the description of the experiments seem rather shallow\nYeah, we were really fighting with the space restrictions here.\nWe will expand the description substantially and move things to an appendix as necessary.\n", "title": "Thank you for the review!"}, "H1xC3Npeir": {"type": "rebuttal", "replyto": "HJge9Qkx9r", "comment": "Thanks very much for the review.\nPlease see our comment titled OVERALL RESPONSE regarding comparisons to prior work.\nThrough a combination of arguments made here and the additional experiments we are running, we hope to convince you that a slightly higher score is merited.\nIn this comment, we will use > for quotes and respond point-by-point.\n\n> huge literature on program synthesis without ML.\nAgreed, but since our baseline is essentially a (faster and more general) implementation of [1], \nwe would claim that we have made a serious effort to compare to work from this literature.\nDo you find this claim unsatisfying?\n\n> On the other hand, many of the ML methods don't require the DSL used in the works that introduce them.\nPoint taken, and as mentioned in our other comment, we are setting up an experiment right now that will \ncompare premise-selection with property signatures to premise-selection using the algorithm from [2].\nPlease let us know if there is some other comparison you'd like to see and we will do our best.\n\n*However*, we would argue that there are several important ways in which this work adds to current art \nthat can be verified without actually running experiments in the way that is common in the ML literature.\nThat is, there are things that our system can do 'by construction' that previous ML-based methods can't do.\nFor example, DEEPCODER (the most relevant prior art, probably?)\n\na) Has no way to deal with unbounded data types,\nb) Requires hard-coding a new neural network architecture for all compound data-types,\nc) Has no way to deal with polymorphic functions\n\nand Property Signatures deal with all of these things 'for free'.\nTo be (just a little bit) glib, there's a sense in which we have run a comparison \nwith DeepCoder already (on almost all of the 13 types we used), and it just 'returned NaN' when\nthere was no way for it to handle the type under consideration.\nAgain, we are actually going to run the experiments for the few types where it makes sense and report back,\nbut we would claim that your review as it currently stands does not give us enough credit for points like\na), b) and c) above.\n\n> Are there some more realistic tasks/applications that searcho makes reasonable, not readily approached with previous DSLs used for testing these algorithms?\nWe would argue that the answer is yes, because Searcho is Turing complete and comes w/ a distributed search implementation, but we might \nbe misunderstanding you on this point?\n\nFinally, we are pretty optimistic about the value of the ideas in Section 5, but it doesn't seem like any of the reviewers found that section interesting.\nWe would be especially grateful to have more specific feedback on that section.\n\n[1] Synthesizing data structure transformations from input-output examples\n  ( https://www.cs.rice.edu/~sc40/pubs/pldi15.pdf )\n[2] DEEPCODER: LEARNING TO WRITE PROGRAMS \n  ( https://openreview.net/pdf?id=ByldLrqlx )\n", "title": "Thanks for the review!"}, "rkxbx93xsS": {"type": "rebuttal", "replyto": "Skxqu-xjcH", "comment": "Thanks very much for the review!\nPlease see our comment titled OVERALL RESPONSE, regarding your point 3.\nOverall we believe that there are several misunderstandings (our fault for not being more clear).\nWe hope that you will consider raising your score slightly in response to our clarifications.\n\nWe will quote using > and respond point by point:\n\n====================    Intro   ==============================\n> which is simply a bag-of-word representation with only 0 or 1 as each element\nThis is not quite true: there are 3 possible elements: ALL_TRUE, ALL_FALSE, or MIXED.\nYou could think of them as -1, 0 and 1.\nWe don't mean to be pedantic, but we think this is an important point and possibly points to a larger misunderstanding:\nThe value for a given property applies across all possible input/output pairs of the relevant type.\nSo if you have the property 'is the output list twice the length of the input list', it will be MIXED for the function\nthat concatenates a list to itself, since that function takes the empty list to the empty list, but it takes [1,2,3] -> [1,2,3,1,2,3].\nApologies if you already understood this.\n\n====================    Point 1) ==============================\n> Potentially it could have an exponential number of possible properties \nWe think there is a misunderstanding here.\nFor most type signatures, there are in fact infinitely many possible properties, since properties are just programs.\nBut there's no need to actually use them all: think of property signatures as analogous to random projections.\nWe get a decent representation of the semantics of a program from a small-ish number of property signatures.\n\n====================    Point 2) ==============================\n> All I saw are \"randomly sample and generate\".\nThis misses a few really important parts.\nFirst, we aren't just randomly sampling from all possible properties, we're sampling\nlowest-cost-first, which means that we will get simple properties before more complicated properties.\nSecond, there's a filtering stage that you may have missed?\nGiven a test set of program specifications, you can just only keep properties \nthat help you to distinguish between multiple test specifications.\n\n> But it is definitely not feasible for any complex function, not to mention project.\nThis is a very strong assertion.\nIt depends what you mean by complex, but we would argue that the following function \nis pretty complex (from Listing 1 in the paper): \n\nfun unique_justseen(xs :List<Int>) -> List<Int> {\n  let triple = list_foldl_<Int, (List<Int>, Int, Bool)>(\n    xs,\n    (nil<Int>, 0, _1),\n    \\(list_elt, (acc, last_elt, first)){\n      cond_(or_(first, not_equal_(list_elt, last_elt)),\n        \\{(cons_(list_elt, acc), list_elt, _0)},\n        \\{(acc , list_elt, _0)})\n    });\n  list_reverse_(#0(triple))\n};\n\nAnd we were able to learn to synthesize this function.\n\nMore generally, it's probably wrong to envision program synthesis tools \nthat synthesize whole projects from scratch: it seems much more likely that\nusers of future tools will synthesize projects function-by-function, \nin which case your concern would not apply.\n\n====================    Point 3) ==============================\n> There are very little baselines to compare against\nPlease see our comment titled OVERALL RESPONSE for this.\nWe should have been more clear, but our main baseline is in fact an implementation of prior work.\n", "title": "Thanks for the review!"}, "Hkln52sgiB": {"type": "rebuttal", "replyto": "rylHspEKPr", "comment": "Thanks for the reviews!\n\nWe'll respond to each reviewer in detail, but here we address one main point \nthat was mentioned by Reviewers 1 and 3:\n\nReviewers 1 and 3 were concerned about a lack of comparison to prior work:\nWe should have made this more clear, but our main experiment *does* already have a comparison to prior work,\nsince the baseline synthesizer is essentially the algorithm from [1].\nWhether this represents the 'state-of-the-art' is up for debate, since synthesis papers tend to all use\nvery different benchmarks, but we don't know of anything that has obviously superseded it?\n\nRegardless, we are happy to add more such comparisons, and are currently working on a comparison \nwith the 'DeepCoder' algorithm from [2]. \nWe will add the results here before the end of the reviewer response period.\nPlease let us know if there are other comparisons you would like to see.\n\n[1] Synthesizing data structure transformations from input-output examples\n  ( https://www.cs.rice.edu/~sc40/pubs/pldi15.pdf )\n[2] DEEPCODER: LEARNING TO WRITE PROGRAMS \n  ( https://openreview.net/pdf?id=ByldLrqlx )\n", "title": "OVERALL RESPONSE: We *do* compare to prior work (but we will add even more such comparisons)"}, "BJxYtKBlcr": {"type": "review", "replyto": "rylHspEKPr", "review": "** Summary\nThe paper studies the problem of program synthesis from examples and it proposes the notion of property signature as a set of \"features\" describing the program that can be used to simplify the synthesis by combining together sub-programs with similar features.\n\n** Evaluation\nThe paper is outside my scope of research, so my review is an educated guess.\n\nThe use of properties to summarize some features about the program and the possibility to evaluate them directly from samples seems a very interesting idea. As the authors mentioned, this may help in creating useful features and useful architectures to simplify the learning task. The concept of property signatures is very well introduced and explained. The authors also provide an extensive comparison to related work. The empirical results seem promising in showing how property signatures make the synthesis much faster and better.\n\nThe downsides of the paper are:\n- While it is clear how to build property signatures, it is quite unclear to me how they simplify the generation of programs that combine smaller/simpler programs.\n- Sect 3.2 on how to learn useful properties is rather vague and it would need a much more detail explanation.\n- Although the authors release code for the paper, the description of the experiments seem rather shallow and it requires a much higher level of detail on how the learning process is set up and executed.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}, "Skxqu-xjcH": {"type": "review", "replyto": "rylHspEKPr", "review": "This paper proposed the concept of \"property signatures\" , which are learned to represent programs. The property signatures are essentially some key attributes that one may summarize from a given set of input-output pairs, which the target function has. Then a program can be generated by evaluating these property signatures vectors (which is simply a bag-of-word representation with only 0 or 1 as each element). Much discussions have been given to discuss why and how these properties may be useful and very little real experiments are conducted quantitatively compared with existing works. Although this paper is quite interesting, I think this paper is in its very early stage and there are a lot of serious concerns I have for using this approach to synthesize the real complex programs. \n\n1) First of all, the notion of property signatures are easy to understand and is very natural. Just like human beings, when we write a program, we first think about the possible attributes of this program may have given a set of input-output pairs for both correctness and completeness. However, this is also the hard part of this idea. Potentially it could have an exponential number of possible properties as the program goes more complex and complex. It will quickly become computationally intractable problem. \n\n2) When I read the middle of paper, I would eager to know how authors can effectively find a good set of properties of a target program from a given input-output pairs. However, when I eventually reached the Section 4, I was kindly disappointed since I did not see any effective and principle way to get them. All I saw are \"randomly sample and generate\". This may be Ok for a very simple program given a set of simple input-output pairs. But it is definitely not feasible for any complex function, not to mention project. I think this is the key for the proposed idea since how to construct a good set of property signatures is crucial to treat them as the inputs for any program synthesis task later. \n\n3) There are very little baselines to compare against even though authors listed \"substantial prior work on program synthesis\". I understand the existing works may have their limitation in both what they can do and how well they can do. But it is still important to compare with directly on the same set of benchmarks. Otherwise, it is hard to be convincing that this approach is indeed superior compared to existing ones.  ", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 2}}}