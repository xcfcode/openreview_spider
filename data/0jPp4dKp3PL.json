{"paper": {"title": "Integrating linguistic knowledge into DNNs: Application to online grooming detection", "authors": ["Jay Morgan", "Adeline Paiement", "Nuria Lorenzo-Dus", "Anina Kinzel", "Matteo Di Cristofaro"], "authorids": ["~Jay_Morgan1", "~Adeline_Paiement1", "n.lorenzo-dus@swansea.ac.uk", "a.l.kinzel@swansea.ac.uk", "mdc@infogrep.it"], "summary": "Incorporating Corpus Linguistic knowledge in Deep Learning models to create accurate and interpretable models.", "abstract": "Online grooming (OG) of children is a pervasive issue in an increasingly interconnected world. We explore various complementary methods to incorporate Corpus Linguistics (CL) knowledge into accurate and interpretable Deep Learning (DL) models. They provide an implicit text normalisation that adapts embedding spaces to the groomers' usage of language, and they focus the DNN's attention onto the expressions of OG strategies. We apply these integration to two architecture types and improve on the state-of-the-art on a new OG corpus.", "keywords": ["Machine Learning", "Corpus Linguistics"]}, "meta": {"decision": "Reject", "comment": "Most reviewers did not feel that this paper was ready for publication. I thank the authors for answering all the concerns of the reviewers, running new experiments and submitting a revised version, however, this was not not enough to alleviate the reviewers' concerns, notably relating to the handling of the ethical consideration in the writing of the manuscript."}, "review": {"4di_quBQOT8": {"type": "review", "replyto": "0jPp4dKp3PL", "review": "##########################################################################\n\nSummary:\n\nThis paper proposes two families of methods for integrating external knowledge\ninto neural networks aimed at classifying instances of online grooming. The\nfirst family focuses on incorporating knowledge of word semantic similarity into\ntheir representations. The second, on different attention mechanisms for\nincorporating knowledge about theoretical stages of online grooming.\n\nThe authors perform a solid amount of experiments to assess the differences\nbetween their suggested methods and baseline models.\n\n\n##########################################################################\n\nReasons for score:\n\nHowever, the way the paper is written and structured make it difficult to\nunderstand. While looking at the results, it is hard to really assess the\ncontribution of each suggested strategy, and how they compare to each other.\n\nFinally, the paper presents some serious conceptual gaps that undermine its\noverall credibility.\n\n##########################################################################\n\nPros:\n\n- Fair amount of experiments for assessing impact of each proposed strategy.\n- The ideas for modeling different word variants could be built upon. I\n  especially like the idea of Elastic Pulling for combining the semantic\n  information of word representations. This idea would be useful to the research\n  community focusing on combining word representations.\n\n\n##########################################################################\n\nCons:\n\n- Some serious conceptual gaps, and wrong claims. .\n- The paper is overall unclear and difficult to understand.\n- The task the paper is addressing is not well specified.\n- The methods are also not clearly explained.\n- Results are difficult to interpret and analyses are lacking.\n- Lack of ethical considerations for a system that could be used in law\n  enforcement. I would have liked to see a more detailed discussion on the\n  societal implications that systems aiding law enforcement could have, and in\n  particular, which measures should be taken for avoiding the prosecution of\n  innocent people by systems like this.\n\n#########################################################################\n\nComments and suggestions for the authors:\n\n- The term \"text normalisation\" is spread throughout the paper, but is never\n  concretely defined, and is not immediately inferable.\n\n- I was not familiar with the \"word semantics representation\" noun phrase. After\n  reading the paper I am pretty sure that you mean \"vector space model\", or word\n  embeddings. Why not use these terms that will probably be more familiar to\n  potential readers?\n\n- In Figure 1 (left), you wrote Embedding; at the right you wrote WSR. Are these\n  equivalent?\n\n- I understand that you are doing classification at the conversation level, but\n  in page 3, in the \"Base models\" paragraph, you mention that \"with the WSR's\n  embedding provided as input to the OG classifier in place of a sentence\n  embedding\". In order to classify a conversation, you need a vector\n  representation of it. How is this obtained? In other words, how are you\n  aggregating the contextualized word representations (i.e., the output of the\n  LSTM or the XLNet encoder), into a single vector representation of the\n  conversation? Are you using the last hidden state of the LSTM or a pooling\n  method? Are you using the [CLS] token of XLNet or something else?\n\n- You mentioned that your dataset contains full conversations with an average of\n  431 messages per conversation. Are all the conversation turns separated by the\n  [SEP] token? What is the average message length? What max input length did you\n  use as a hyperparameter? Did you use the same text input for Model 1 and Model\n  2?\n\n- Saying XLNet is the SoTA for NLP is a false statement (p. 3 second-last\n  paragraph). First of all, NLP encompasses several tasks and there is no single\n  model superior to all the others in every task. Second, XLNet has already been\n  beaten in several tasks. See the Glue benchmark\n  (https://gluebenchmark.com/leaderboard) for a few examples. You mention that\n  XLNet is the SoTA of NLP again in p. 6; sec. 5.1; second paragraph.\n\n- Saying that XLNet iteratively refines word embeddings from a WSR similar to\n  that of LIU et al. (2017) is only tangentially true and is misleading, in my\n  opinion. Recurrent models such as those relying on LSTMs, are profoundly\n  different to those based on transformers such as XLNet.\n\n- I am not sure that there is a clear correspondence between using recurrent\n  models such as LSTMs and better handling of class imbalance. How is a\n  two-layer LSTM going to help handling class imbalance?\n\n- You mentioned several times that \"text normalisation\" was a strong point of\n  your contribution, but in the last paragraph of section 4 (p.4) you say that\n  you do not apply \"text normalisation\". I understand that you might be\n  referring to different kinds of normalisation, but I think this terminology is\n  confusing. I suggest using more precise language to clearly differentiate\n  what you are referring to.\n\n- What do you mean by \"intersection tests\"?\n\n- I suggest using the word \"representations\" rather than \"coordinates\" for\n  referring to the vector representation of a word (p. 4; elastic pulling\n  paragraph).\n\n- The correspondence between rows in Table 1 and the strategies discussed in\n  section 4.2, are not immediately apparent.\n\n- When not using GloVe embeddings, how did you initialize your word\n  representations?\n\n- You mention in Table 1 that bold are improved results, but improved with\n  respect to what?\n\n- You report precision and recall in Table 2, but not in Table 1. I think it\n  would be valuable to include these in Table 1, rather than the distance\n  reduction and average resulting distance metrics which are only valid for a few\n  strategies.\n\n- You could check the following papers for more context on combining\n  representations of different word variants:\n  * [Attention-based Conditioning Methods for External Knowledge Integration](https://www.aclweb.org/anthology/P19-1385/)\n  * [Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics](https://www.aclweb.org/anthology/P13-1149/)\n  * [Composition in Distributional Models of Semantics](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01106.x)\n\n\n#########################################################################\n\nSome typos:\n\n- p. 4; Manifold Learning - \"by building, using manifold learning, a new space\"\n  could be better written as \"by building through manifold learning a new space\"\n  or \"by building a new space through manifold learning\"\n\n- p. 4 - \"it is possible reduce dimensionality\" -> \"it is possible to reduce\n  dimensionality\"\n\n- p.5, sec. 4.2, first paragraph - \"collocates and std the span\". Not sure what\n  std is supposed to mean here.\n\n\n", "title": "Some good ideas, but needs more work", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "uxZqjPxNt6n": {"type": "rebuttal", "replyto": "u7SUegBnXHM", "comment": "The pending test for base model 2 is now completed and we have updated the paper with the results, which are similar to those of base model 1: 0.110 (0.072) average (std) energy for instances of OG processes, slightly lower than the energy across all conversations at 0.120 (0.088).", "title": "Additional test 2"}, "u7SUegBnXHM": {"type": "rebuttal", "replyto": "JilQqSw6dnl", "comment": "Thank you for your understanding. We now have completed the pending test for base model 1 (the job for base model 2 is still in a waiting queue).\n\nWe have computed the average and std of attention energy at the locations of our labelled instances of OG processes. The values are 0.0009 (0.0002), lower than energy across all conversations at 0.0016 (0.0128). Thus,  the contexts that the model learnt to focus on are not related to our labelled instances of OG processes. This is an indication that the model was not able to discover on its own the sub-goals that the CL analysis of Lorenzo-Dus et al. (2016) identified, and their associated language. This knowledge is therefore an added value for the model, as also demonstrated by the improved results.\n\nWe have added this test and discussion in the updated version of the paper.", "title": "Additional test"}, "pLn88agtAxJ": {"type": "rebuttal", "replyto": "4di_quBQOT8", "comment": "- *The correspondence between rows in Table 1 and the strategies discussed in section 4.2, are not immediately apparent.*\n\nPlease, kindly refer to the headings of the paragraphs that describe each strategy, both in sections 4.1 and 4.2. These headings have been used in the second column of Table 1.\n\nThe stimulating attention and stimulating LSTM input gates strategies have several variants, as detailed in their respective paragraphs. The names of these variants are indicated in bold (and now also underlined) in the paragraphs, and they are used in the third column of Table 1. As indicated in section 5.1, and now also in 4.2, the two variants \u201csupervision\u201d and \u201cexcitation\u201d are not mutually exclusive and may be used together, which is indicated by \u201csuperv. + excit.\u201d in the third column of Table 1.\n\n- *When not using GloVe embeddings, how did you initialize your word representations?*\n\nPlease kindly refer to the first paragraph of Section 5 for all the details on initialisation and training of models. Base model #1, including its word embedding, is trained from random weights on our dataset.\n\n- *You mention in Table 1 that bold are improved results, but improved with respect to what?*\n\nSince the aim of this experiment is to assess the effect of each proposed strategy, the improvements are with respect to not using these augmentation strategies, i.e. with respect to the non-augmented base models (rows 1, 14, and 18 of Table 1).\n\n- *You report precision and recall in Table 2, but not in Table 1. I think it would be valuable to include these in Table 1, rather than the distance reduction and average resulting distance metrics which are only valid for a few strategies.*\n\nPrecision and recall for the experiment of Table 1 are provided in the sup. materials. We have edited Table 1 in the revised manuscript to include them.\n\n- *You could check the following papers for more context on combining representations of different word variants*\n\nWe thank the reviewer for these suggestions, they are indeed relevant to our study. We have added the most closely relevant paper (due to space constraints) to the related works section of the revised manuscript.\n\n- *Some typos*\n\nThank you, we have made these corrections.", "title": "Response to reviewer - part 4"}, "qolJLTrIZXt": {"type": "rebuttal", "replyto": "4di_quBQOT8", "comment": "- *Saying XLNet is the SoTA for NLP is a false statement (p. 3 second-last paragraph). First of all, NLP encompasses several tasks and there is no single model superior to all the others in every task. Second, XLNet has already been beaten in several tasks. See the Glue benchmark (https://gluebenchmark.com/leaderboard) for a few examples. You mention that XLNet is the SoTA of NLP again in p. 6; sec. 5.1; second paragraph.*\n\nThis shortcut was indeed badly phrased, and it has been corrected in the revised manuscript. We meant to say that transformer models, especially XLNet, are the SoTA in many NLP tasks and a popular model, and XLNet is, therefore, a strong baseline to work from and also to compare against.\n\n- *Saying that XLNet iteratively refines word embeddings from a WSR similar to that of LIU et al. (2017) is only tangentially true and is misleading, in my opinion. Recurrent models such as those relying on LSTMs, are profoundly different to those based on transformers such as XLNet.*\n\nThis is a misunderstanding, and we have reformulated this paragraph in the revised manuscript to prevent this happening in the future. We did not mean that the refined contextualised embedding of XLNet after the self-attention layers is similar to the word embedding of Liu et al. (2017). We were actually referring to the initial word embedding that captures word semantic similarly to the word embedding of Liu et al. (2017), before any self-attention layer, and independently of the location of the words within sentences. This is the embedding that we work on.\n\n- *I am not sure that there is a clear correspondence between using recurrent models such as LSTMs and better handling of class imbalance. How is a two-layer LSTM going to help handling class imbalance?*\n\nWe observed that the original XLNet failed to train satisfactorily on our dataset and always output the same class, with 0.392 F-score. We were able to solve this issue by using an LSTM to further contextualise the word embeddings and extract a conversation embedding. The reason for this behaviour remains to be investigated. It is not the first time that a transformer model is combined with an LSTM, see for example [1] where this approach obtained good results. Using an LSTM to extract a conversation embedding has the extra advantage of allowing the use of our LSTM-based knowledge integration strategies. We now try to better present this decision process in the revised paper.\n\n[1] Ma, Tweets Classification with BERT in the Field of Disaster Management, 2019\nhttp://web.stanford.edu/class/cs224n/reports/custom/15785631.pdf\n\n- *You mentioned several times that \"text normalisation\" was a strong point of your contribution, but in the last paragraph of section 4 (p.4) you say that you do not apply \"text normalisation\". I understand that you might be referring to different kinds of normalisation, but I think this terminology is confusing. I suggest using more precise language to clearly differentiate what you are referring to.*\n\nIn the last paragraph of section 4, we describe the text preparation steps that are applied to the text prior to the DNNs. It is correct that we do not apply an external text normalisation to the text before the DNN, since the DNNs include their own text normalisation.\nWe have updated the terminology in the revised manuscript to avoid such confusion in the future.\n\n- *What do you mean by \"intersection tests\"?*\n\nThe global support of the RBF allows considering all words for propagating the influence of each pulling operation, rather than operating on a local support which would require identifying those words that are located within each considered neighbourhood. This identification of neighbouring words would typically be done using intersection tests. The description has been amended in the revised paper to avoid the use of this term.\n\n- *I suggest using the word \"representations\" rather than \"coordinates\" for referring to the vector representation of a word (p. 4; elastic pulling paragraph).*\n\nThe pulling operation is a spatial displacement of the words\u2019 representations within the embedding space. The equations that define this displacement do use coordinates, therefore it is not possible to avoid the use of this word. However, we have amended this paragraph to clearly define the coordinates of a word\u2019s representation in the embedding space.", "title": "Response to reviewer - part 3"}, "VBEYTYISYj": {"type": "rebuttal", "replyto": "4di_quBQOT8", "comment": "- *The term \"text normalisation\" is spread throughout the paper, but is never concretely defined, and is not immediately inferable.*\n\nThe definition of text normalisation, provided in the introduction (originally: \u201cText normalisation methods were proposed to reduce variance and simplify learning\u201d), has been expanded in the revised manuscript to stress that text normalisation is concerned with choice of word and/or spelling.\n\n- *I was not familiar with the \"word semantics representation\" noun phrase. After reading the paper I am pretty sure that you mean \"vector space model\", or word embeddings. Why not use these terms that will probably be more familiar to potential readers?*\n\nWSR indeed refers to a word embedding (before LSTM or self-attention layers). With this formulation, we wished to stress the difference with the contextualised word embedding after LSTM or self-attention. The purpose of the embedding (capturing word semantic) is central to the motivation of our modification method, so this name seemed suitable. We have updated the terminology throughout the revised paper, stressing the difference between the embedding for semantic and the contextualised embedding whenever this may be ambiguous.\n\n- *In Figure 1 (left), you wrote Embedding; at the right you wrote WSR. Are these equivalent?*\n\nThank you for pointing out this possibly confusing term, the terminology has been harmonised to \u201cword embedding\u201d throughout the revised paper.\n\n- *I understand that you are doing classification at the conversation level, but in page 3, in the \"Base models\" paragraph, you mention that \"with the WSR's embedding provided as input to the OG classifier in place of a sentence embedding\". In order to classify a conversation, you need a vector representation of it. How is this obtained? In other words, how are you aggregating the contextualized word representations (i.e., the output of the LSTM or the XLNet encoder), into a single vector representation of the conversation? Are you using the last hidden state of the LSTM or a pooling method? Are you using the [CLS] token of XLNet or something else?*\n\nIn base model #1, both the last hidden state of the LSTMs, and all previous states, are provided to the attention mechanism in order to create an optimised embedding (which is computed for the full conversation rather than per message, so it is a conversation embedding). This follows the procedure suggested by the authors of the attention mechanism in (Luong et al., 2015). The attention mechanism outputs a conversation embedding of the same size as the LSTM\u2019s hidden state, namely 256. This is now detailed more in the revised paper.\nIn base model #2, the last hidden state of the LSTM is used as conversation embedding.\n\n[CLS] is used at the beginning of each conversation for XLNet, and [SEP] is used in between messages of a conversation for both architectures. The use of [CLS] is now explicit in the revised paper.\n\n- *You mentioned that your dataset contains full conversations with an average of 431 messages per conversation. Are all the conversation turns separated by the [SEP] token? What is the average message length? What max input length did you use as a hyperparameter? Did you use the same text input for Model 1 and Model 2?*\n\nPlease, kindly refer to Section 4, paragraph \u201cInput to the models\u201d (just before 4.1), where it is explained that all messages are separated by the [SEP] tokens, with no consideration of the speaker\u2019s identity.\n\nStatistics on message lengths etc. were provided in the sup. materials due to lack of space. We added some numbers in the revised paper, and clarified the sup. material tables. Numbers have changed slightly because, in the sup. materials, we initially computed conversation statistics by combining several conversations of two same users. Since different chat sessions / conversations are classified separately, we now provide separate statistics for easier interpretation. There is an average of 215 messages and 1110 words per classified grooming conversation. Average message length is 4.68 words. For non-grooming conversations, there is an average of 13 messages and 104 words per conversation, with an average message length at 7.04 words.\n\nConversations are truncated to the last 2,000 words. 12 / 8e-5 % of OG / non-OG conversations needed this truncation.\n\nModels 1 and 2, both with and without augmentations, were trained and tested on the exact same text, to allow for fair comparisons.", "title": "Response to reviewer - part 2"}, "YRBgJQF7Imn": {"type": "rebuttal", "replyto": "4di_quBQOT8", "comment": "We thank the reviewer for their insights. We are sorry that the reviewer finds the methods and discussions of experimental results hard to understand. In the revised paper, we have expanded (as much as page limit allows) the descriptions and discussions to alleviate this issue. We especially expanded the discussions around the ablation study, which evaluates and compares each suggested strategy. Next, we respond in turn to their raised questions and comments. We hope that this will address the criticism about \"conceptual gaps\". If not, we would appreciate a more explicit description of these gaps, to allow us to address them.\n\n- *The task the paper is addressing is not well specified.*\n\nThe task is the classification of conversations between instances of online grooming (positive class), and neutral (negative class) conversations. In the revised submission, we have expanded the second sentence of the paper (originally: \u201cThe aim of this work is to detect instances of OG through classification of whole conversations.\u201d) to clarify this.\n\n- *Lack of ethical considerations*\n\nAs discussed in the introduction and in the dedicated discussion in Section 6 \u201cPerspectives for  OG  prevention\u201d, the system was co-developed with specialised law enforcement, and its \u201cintended usage is to facilitate triage by law enforcement\u201d. It is not intended to replace a human decision. Within this usage scenario, there is, therefore, no risk of innocents to be automatically prosecuted, since this decision is always made by a trained police officer after reviewing the conversation.\n\nIn fact, not all courts would accept linguistic evidence. Our law enforcement partners only aim at using this system when they already have enough evidence, as part of their toolkit to analyse large quantities of seized digital materials including conversations.\n\nThe co-development of the system with law enforcement ensures that their own security protocols will be followed throughout the usage of the system, to preserve integrity. In particular, the system would not be provided to any police officer, but only to trained operators, using robust mechanisms that are already being in use by law enforcement in the context of OG investigations. Therefore, while a human review of the flagged conversations might in general suffer from biases and prejudices, security protocols are already in place to mitigate these natural human biases.\n\nThe aim of our work is *not* to address the possible biases in the human decision. However, our method may indirectly help in achieving a fairer handling of the flagged conversations. Indeed, the visualisation provided by our method eases the analysis of key parts of the conversation. Additionally, the triaging would allow operators to focus on a few flagged conversations while spending less time on the others. This reduced workload and associated lowered time pressure, associated with the visualisation, may allow a more thorough and fairer investigation.\n\nThe \u201cPerspectives  for  OG  prevention\u201d section has been expanded in the revised manuscript to reflect this discussion.\n\nWhile one may argue that our system may be diverted from its intended use and deployed more largely online without following the law enforcement\u2019s robustness and security protocols, we wish to point out that this is the case for many AI systems, and therefore that this should be discussed at the higher level of the whole AI field and together with experts of other disciplines such as jurists. Indeed, the question of liability and responsibility arises in many AI applications, for example when an AI system may perturb stock exchange markets and economy or may endanger or even take the life of passengers of automatic cars. The AI-assisted detection of online grooming is indeed concerned with these questions of unintended usage and liability around AI. However, for a fuller and clearer debate, these questions should be (and are [1-3]) discussed more globally.\n\n[1] Liability for Artificial Intelligence and other emerging digital technologies, European Commission Report from the Expert Group on Liability and New Technologies \u2013 New Technologies Formation, 2019\nISBN 978-92-76-12959-2, doi:10.2838/573689\nhttps://ec.europa.eu/transparency/regexpert/index.cfm?do=groupDetail.groupMeetingDoc&docid=36608\n\n[2] A. Bertolini, Artificial Intelligence and Civil Liability, Report from the European Parliament's Committee on Legal Affairs, 2020\nhttps://www.europarl.europa.eu/RegData/etudes/STUD/2020/621926/IPOL_STU(2020)621926_EN.pdf\n\n[3] J.K.C. Kingston, Artificial Intelligence and Legal Liability, International Conference on Innovative Techniques and Applications of Artificial Intelligence, 2016\nhttps://arxiv.org/ftp/arxiv/papers/1802/1802.07782.pdf", "title": "Response to reviewer - part 1"}, "--xgRVQ8YT4": {"type": "rebuttal", "replyto": "nmxzn-nfFVl", "comment": "We thank the reviewer for their insights. We respond next to their raised questions and comments:\n\n*Although the considered task is indeed very important, a weak point of the paper is that it considers a single domain, and the method looks tailored to such domain. The paper asserts that the same methodology could be applied to different scenarios in the domain of chat conversations, but this is not confirmed by the experimental evaluation. For example, what about the process of identification of \"variants\": is such process hand-made? Would it be possible to have more details on such part?*\n\nAs briefly indicated in Section 3, the CL analysis that produced our annotations involves a heavy use of manual analysis by CL experts. Indeed, expert knowledge doesn\u2019t come for free. However, in such multidisciplinary studies, it is often the case that expert CL knowledge already exists.\n\nThe word variants that were (manually) identified and used for this study would be largely re-usable in other contexts and applications of analysing chat conversations. Indeed, a significant number of these variants are related to digital language rather than specifically to online grooming. Only a few variants could be more specifically linked to online grooming, where they would aim to minimize the sexual meaning of some terms for example. We reckon that these few samples would not prevent the re-use of the whole set of variants in other applications.\n\nThe evaluation of the discriminative aspect of the variants for a given classification can be done easily and automatically, following the procedure described at the beginning of Section 4.1, using empirical occurrences in positive and negative conversations.\n\nIn conclusion, the first strategy of integrating knowledge on discriminative word variants into DNNs, is easily reusable in other applications of analysing chat conversations.\n\nThe decomposition of a conversation\u2019s aim into subgoals has been the focus of many social science studies. For example, for extreme ideology groups, such as radical right hate speech and radicalisation, a large corpus of works have identified strategies for persuasion / manipulation through conversations [1-5]. This established baseline of knowledge may be used for our second strategy of integrating knowledge into DNNs through decomposing conversations into subgoals.\n\nThe identification of frequent 3-word collocates is automated, as described in (Lorenzo-Dus et al., 2016). The association of their occurrences to the identified subgoals is the only task that may require additional manual work.\n\nFor these reasons, the prior knowledge integration methods that we present are not only tailored to online grooming detection, but they could also be used for other classification tasks of chat conversation, such as detecting radicalisation for example. The existing CL knowledge of these applications can be exploited to reach this goal in a multidisciplinary context.\n\nThis discussion has been provided in the new version of the paper.\n\n[1] Saridakis, I. and Mouka, E. (2020) A corpus study of outgrouping in Greek radical right computer-mediated discourses, Journal of Language Aggression and Conflict, 8: 188 \u2013 231; DOI: https://doi.org/10.1075/jlac.00038.sar\n\n[2] Baker, P. et al (forthcoming, February 2021) The Language of Violent Jihad, Cambridge: Cambridge University Press.\n\n[3] Brindle, A. (2016) The Language of Hate: A Corpus Lingusitic Analysis of White Supremacist Language, London: Routlege.\n\n[4] Nouri, L. & Lorenzo-Dus, N. (2019). Investigating Reclaim Australia and Britain First\u2019s use of social media: Developing a new model of imagined political communities online, Journal for Deradicalization, 18: 1-37.\n\n[5] Lorenzo-Dus, N. & Nouri, L. (2020) The discourse of the US alt-right online \u2013 a case study of the Traditionalist Worker Party blog, Critical Discourse Studies, Critical Discourse Studies, DOI: 10.1080/17405904.2019.1708763", "title": "Response to reviewer"}, "forJAV2ZHCS": {"type": "rebuttal", "replyto": "5BOnM4u3n88", "comment": "We thank the reviewer for their insights. We respond in turn to their raised questions and comments:\n- *When reading the descriptions of the linguistic sub-models in the DNN, one has the question if we could compare subsets of a DNN that is trained without these explicit sub-models. and may have learned these representations for normalisation etc. vs the integrated CL knowledge.*\n\nIn our ablation studies, we did perform comparisons where no or only part of the CL knowledge was used for enhancing the DNN. These experimental results are shown in Table 1. The improved results when integrating CL knowledge demonstrate that the non-augmented DNNs could not fully discover this knowledge on their own.\n\nFor the selective text normalisation, in particular, our experimental results demonstrate that the non-augmented DNNs fail to discover the knowledge on discriminative word variants. Indeed, fine-tuning on our dataset did not allow the DNNs to identify on their own the variants that, at the same time, have the same meanings and are not discriminative of groomer language. These variants were kept separate in the word embeddings, as indicated by the reported distances for non-augmented models in Table 1.\n\nIn the revised submission, we have added a discussion on the inability of the two non-augmented DNNs to discover on their own the same level of CL knowledge that we use to augment the models.\n\n- *Could we work to extract interpretable pieces fo the DNN that will then be comparable to the proposed CL augmentations?*\n\nUsing machine learning to discover new knowledge is an interesting research field. However, this would be an entirely different study. Here, we are interested in how we can exploit the knowledge that experts already have, in order to augment DNNs and improve their results.\n\n- *The above is important as work on the PAN-12 dataset has tried to reconcile the NLP approach with also understanding the behaviour of sexual predators, so if we can learn how the DNNs are extracting information, we can better create interpretability models that can be more general for NLP + DNNs.*\n\nIn our study, we leave the understanding of sexual predators to CL experts, who have published the results of their analysis e.g. in the cited paper (Lorenzo-Dus et al., 2016). However, we demonstrate that augmenting the DNNs using this expert knowledge, in a multidisciplinary research context, does help in improving the interpretability of the model (see Section 5.2 \u201cVisualisation\u201d).\n\n- *The work does well to show the gains we get from including these priors. I think we would be better suited if we also understood in the base models, how much of the priors were learnt.*\n\nAs discussed previously, the knowledge on discriminative word variants could not be discovered by the non-augmented DNNs, and this is highlighted in the revised paper.\n\nTesting whether the non-augmented DNNs could identify the existence of sub-goals and the expression of their related contexts is more difficult without introducing this knowledge in the process. As a simple test, in a new experiment, we could visualise the attention energies of the attention module of base model 1, and of the last self attention layer of base model 2, to check whether the model learnt on its own to focus on the contexts of some sub-goals. This new experiment will take a few days (due to current technical issues with our local computation resources), and we will update this response with the results when we get them.\n\n- *Please also include a note about some of the ethical considerations when dealing with the PAN-12 dataset and how the data was created.*\n\nMore discussion has been added in Section 3. The data itself being freely available online, its use does not raise any peculiar ethical concern. Its initial collection by the PJ website (we are not involved in this process) was debated and discussed for example in (Chiang & Grant, 2019; Schneevogt et al., 2018), which are cited in our paper. The annotation of the data by CL experts was performed following the method developed in another study (Lorenzo-Dus et al., 2016) and that is cited in our paper. This annotation being mostly manual, due care has been taken to preserve the mental health of the CL experts.", "title": "Response to reviewer"}, "nmxzn-nfFVl": {"type": "review", "replyto": "0jPp4dKp3PL", "review": "This paper presents an approach to natural language processing which integrates corpus linguistics knowledge within deep neural networks (namely, an LSTM-based architecture with attention). The approach is tailored and evaluated on a specific application, namely online grooming detection.\n\nThe approach is based on (1) the normalization of word embeddings by exploiting word semantics representations and word variants; (2) the decomposition of conversation analysis to identify subgoals, by exploiting online grooming processes (or phases); (3) the use of attention to modulate the input gate of LSTM cells. Another significant contribution of the paper is a novel corpus, which extends a previous one (PAN2012). The proposed neural architecture presents several variants in otder to incorporate linguistic knowledge within the model, and the paper reports about such an ablation study.\n\nIn the experimental evaluation, linguistic knowledge is injected within two base models, namely one based on LSTMs, and the other one on XL-Net. Performance is shown to be improved with respect to the state-of-the-art.\n\nAlthough the considered task is indeed very important, a weak point of the paper is that it considers a single domain, and the method looks tailored to such domain. The paper asserts that the same methodology could be applied to different scenarios in the domain of chat conversations, but this is not confirmed by the experimental evaluation. For example, what about the process of identification of \"variants\": is such process hand-made? Would it be possible to have more details on such part?\n\n- Pag. 7, \"This may be due to this capturing of language subtleties helping with distinguishing OG conversations...\" -> this sentence should probably be rephrased\n- Pag. 7, \"have same aim\" -> \"have the same aim\"", "title": "Combining deep networks and corpus linguistics for online grooming detection", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "5BOnM4u3n88": {"type": "review", "replyto": "0jPp4dKp3PL", "review": "Summary\n\nThis work proposes the approach of integrating priors into a DNN in the form of Linguistic sub-models that capture characteristics of OG. The authors use the example of the PAN-12 dataset for sexual predators to use information about linguistics behaviour for the grooming phases. The work then goes to highlight the augmentations that are done on baseline DNN models to include these CL characteristics. The authors then go on to show the impact of these augmenations on performance of classification on the PAN-12 dataset.\n\nQuestions\n \n- When reading the descriptions of the linguistic sub-models in the DNN, one has the question if we could compare subsets of a DNN that is trained without these explicit sub-models. and may have learned these representations for normalisation etc. vs the integrated CL knowledge. \n\nCould we work to extract interpretable pieces fo the DNN that will then be comparable to the proposed CL augmentations?\n\nThe above is important as work on the PAN-12 dataset has tried to reconcile the NLP approach with also understanding the behaviour of sexual predators, so if we can learn how the DNNs are extracting information, we can better create interpretability models that can be more general for NLP + DNNs.\n\n- The work does well to show the gains we get from including these priors. I think we would be better suited if we also understood in the base models, how much of the priors were learnt.\n\n- Please also include a note about some of the ethical considerations when dealing with the PAN-12 dataset and how the data was created.", "title": "Integration vs. Extraction", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}