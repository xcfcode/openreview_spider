{"paper": {"title": "Improving Exploration of Deep Reinforcement Learning using Planning for Policy Search", "authors": ["Jakob J. Hollenstein", "Erwan Renaudo", "Justus Piater"], "authorids": ["jakob.hollenstein@uibk.ac.at", "erwan.renaudo@uibk.ac.at", "justus.piater@uibk.ac.at"], "summary": "We employ a sample-based planning method for more directed exploration and efficiency in policy learning", "abstract": "Most Deep Reinforcement Learning methods perform local search and\ntherefore are prone to get stuck on non-optimal\nsolutions. Furthermore, in simulation based training, such as\ndomain-randomized simulation training, the availability of a simulation\nmodel is not exploited, which potentially decreases\nefficiency. To overcome issues of local search and exploit\naccess to simulation models, we propose the use of kino-dynamic\nplanning methods as part of a model-based reinforcement learning\nmethod and to learn in an off-policy fashion from solved planning\ninstances. We show that, even on a simple toy domain, D-RL\nmethods (DDPG, PPO, SAC) are not immune to local optima and\nrequire additional exploration mechanisms. We show that our\nplanning method exhibits a better state space coverage, collects\ndata that allows for better policies than D-RL methods without\nadditional exploration mechanisms and that starting from the\nplanner data and performing additional training results in as\ngood as or better policies than vanilla D-RL methods, while also\ncreating data that is more fit for re-use in modified tasks.\n", "keywords": ["reinforcement learning", "kinodynamic planning", "policy search"]}, "meta": {"decision": "Reject", "comment": "The paper is about exploration in deep reinforcement learning. The reviewers agree that this is an interesting and important topic, but the authors provide only a slim analysis and theoretical support for the proposed methods. Furthermore, the authors are encouraged to evaluate the proposed method on more than a single benchmark problem."}, "review": {"rklbNiVniB": {"type": "rebuttal", "replyto": "Bkefy-y0Fr", "comment": "\u2022 Thank you for your review!\n\u2022 @1.) Eventually, probably the most interesting final metric is task success and therefore achieved return \u2013 but that strongly depends on the task.\n\nWithout assumptions on how the reward is structured (with respect to the state space), it is not possible to exclude portions of the state space and without excluding regions of the state space it is not possible to explore more intelligently. \n\nIn dynamical systems, the distance between two state space vectors cannot be measured by simply taking the euclidean distance \u2013 this is due to the dynamics (under-actuation, obstacles). A simple example is a pendulum swing-up, the mountain car example or a robot in a maze. While the target location (in the state space) might be close in terms of euclidean distance, it may not be possible to reach that point (not enough torque and force, or a blocking wall). As such it is hard to define guiding assumptions for the exploration. As such uniform exploration of the state space appears to be a crude but reasonable approach.\n\n\u2022 @1.) Curse of dimensionality: The method will suffer from the curse of dimensionality, however, this is also true for other methods - probably ways to deal with this problem are a) to reduce the high dimensional problem to a lower dimensional one, or b) to use heuristics and solve it only approximately.\n\nOne benefit of RRT and the local steering method is that even in high dimensional spaces the tree will span the state space coarsely if the dynamics allow that.\n\n\u2022 @1.) RRT/RRT*: The difference between RRT and RRT* is that RRT* finds optimal paths from the initial point to the target points in the state space, while the paths found by RRT are not optimal (i.e. not the shortest paths) - however, RRT* requires additional environment steps to perform this optimization - whereas we mostly want to use RRT to find ways to reach large areas of the state space and optimize around the most promising regions.\nThis is also visible in Figure 5, where training is done from 50k RRT steps (full exploration) and then slowly replaced by samples from SAC - thereby fading from pure exploration to exploitation. Although unfortunately we did not highlight this aspect well.\n\n\u2022 @2.) While gradient-based algorithms suffer from local optimality, it does not feature prominently in D-RL research. And given the large amount of excitement around D-RL and the impressive success (e.g. the OpenAI work using the Shadow Dexterous Hand, although this was achieved with great effort - thirteen thousand years of experience) - we felt it beneficial to show that this is a problem that actually happens in practise and therefore is relevant.\n\u2022 We will include more experiments in a future extension of this paper.", "title": "Comment "}, "S1ez8cE2jB": {"type": "rebuttal", "replyto": "ByxPXHFptS", "comment": "\u2022 Thank you for your review!\n\u2022 We eventually want to apply our method on robotics tasks and therefore we focus on continous state/continuous action spaces.\n\u2022 The paper by Zhan et al. \u201919 (\u201cTaking the scenic route: Automatic exploration for video games\u201d) shows an interesting idea to extend the use of RRT even to domains like the Atari games: they use features of a neural network as a low-dimensional continuous embedding of an (Atari/similar) game state (i.e. the image). They use a simplified version of RRT that samples a target point, restores the closest state stored in the tree and tries to reach that target\npoint. However usually RRT uses a local steering method to reach that target point \u2013 while Zhan et al. use a (random) action sequence irrespective of the target point.\nSince we want to target to robotics tasks, where action and feature spaces are inherently continuous and discretization becomes infeasible, we need to be able to deal with such action spaces. Moreover and related to the next comment, random steering will often not be beneficial: either it cannot be done at all, or it is too inefficient.\n\u2022 MCTS: Since we want to eventually apply our method on robotic tasks, we are focusing on continuous domains, we focus on planning methods that are able to deal with such domains and therefore sampling based planners. AlphaGo is an impressive showcase of an extended version of MCTS - however MCTS needs extensions to be applicable in continuous domains, such as\nfor example (but not limited to) HOOT (Mansley et al.,\u201cSample-Based Planning for Continuous Action Markov Decision Processes\u201d), to be applicable to continuous action domains.\nA second aspect is that MCTS typically does not use the information often available in these continuous domains: the locally linearised dynamics - which the local steering method of RRT exploits.\nWe therefore chose RRT as a reasonably effective, yet reasonably simple to implement planning method - although we do not foresee any reason why other planning methods would not work as long as they are applicable to kino-dynamic domains, and produce environment interactions.\n\u2022 We will add more tasks in the next extension.", "title": "Comments on Questions"}, "HJeltOV2jS": {"type": "rebuttal", "replyto": "SygxmJz8YH", "comment": "\u2022 Thanks for your review!\n\u2022 Our proposed method uses a planning method (in this implementation RRT) in the learning and data collection phase - which is then used to learn a policy. During execution the policy\nis used thereby eliminating the planner time from policy execution.\n\u2022 The time taken by the planner during the offline data collection phase is not evaluated in our paper yet - we will add that in a future extension.", "title": "Comment"}, "SygxmJz8YH": {"type": "review", "replyto": "rJe7CkrFvS", "review": "This paper suggested that conventional deep-reinforcement learning (D-RL) methods struggle to find global optima in toy problem when two local optima exist.  The authors proposed to tackle this problem using planning method (Rapidly Exploring Random Tree, RRT) to expand the search area. Since the collected data are not correlated with reward, it is more likely to find the global optima in toy problem with two local optima . As to the planning time problem,  they proposed to synthesize the planning results into a policy. \n\nThe experiments proved that the proposed method performs better in the aforementioned toy problem, and  has advantage in adapting dynamic environment. However, the authors failed to provide sufficient analyis and theoretical support for the proposed method, plus it did not address the weakness of the RRT method-the problem of planning time. ", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 1}, "ByxPXHFptS": {"type": "review", "replyto": "rJe7CkrFvS", "review": "The paper aims to improve exploration in DRL through the use of planning. This is claimed to increase state space coverage in exploration and yield better final policies than methods not augmented with planner derived data.\n\nThe current landscape of DRL research is very broad, but RRT can only directly be applied in certain continuous domains with continuous action spaces. With learned embedding functions, RRT can be applied more broadly (see \"Taking the Scenic Route: Automatic Exploration for Videogames\" Zhan 2019). The leap from RRT-like motion planning to the general topic of \"planning\" for policy search is not well motivated explained with respect to the literature. Uses of Monte Carlo Tree Search (as in AlphaGo) seem obviously related here.\n\nThis reviewer moves to reject the paper primarily on the grounds of overinterpreting experimental results from a single, extremely simple example RL task. In a domain so small, we can't tease out the role of exploration, we aren't engaging with the \"deep\" of DRL, and we are only considering one specific kind of planning. The implicit claims of general improvement to exploration and improved downstream policies are not supported by the experimental results. At the same time, no theoretical argument is attempted that would make up for the very narrow nature of the experiments.\n\nQuestions for the authors:\n- If HalfCheetah is used to motivate the work, and it is so easily available in the open source offerings from OpenAI, why isn't one (or many more) tasks of *at least* this complexity considered? MountainCar is one of the gym environments with a 2D phasespace compatible with the kinds of plots used in this paper.\n- Could the authors taxonomize the landscape of planning and provide a specific argument for focusing on RRT? (RRT is a fun algorithm, but how will you draw the attention of other researchers who are currently focused on Atari games?)", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 3}, "Bkefy-y0Fr": {"type": "review", "replyto": "rJe7CkrFvS", "review": "The paper is mostly easy to read and I enjoyed reading it. The authors address an important issue of exploration in reinforcement learning and the used of a model-based planner is certainly a promising direction. However, I do have a number of concerns.\n\n1. On Q1. I think the key question here is this -- should state-space coverage be the only measure for effective exploration? The classical dilemma of explore-or-exploit in reinforcement learning is relevant here. From Figure 3, it seems that RRT tends to explore uniformly rather than \"intelligently\". For problems where there is absolutely no information guiding the exploration process this might be desirable, but then the search complexity will suffer from the curse of dimensionality and there is no evidence in this work that this is a good strategy. Perhaps switching from RRT to RRT* helps but the authors chose not to do it.\n\n2. On Q2. Perhaps I missed something here but other than special cases (e.g. convex problems) almost all gradient-based algorithms suffer from local optimality. I am not sure Q2 is a good question to ask here.\n\n3. On Q3. It seems that SAC from scratch is the best-performing approach here. This particular setting is hardly convincing in motivating the re-use of examples across tasks.\n\nThe above concerns, plus the fact that only one particularly simple task is being investigated here, prevent me from recommending acceptance. \n", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 4}}}