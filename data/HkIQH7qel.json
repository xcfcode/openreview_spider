{"paper": {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "authors": ["Kenton Lee", "Tom Kwiatkowksi", "Ankur Parikh", "Dipanjan Das"], "authorids": ["kentonl@cs.washington.edu", "tomkwiat@google.com", "aparikh@google.com", "dipanjand@google.com"], "summary": "We present a globally normalized architecture for extractive question answering that contains explicit representations of all possible answer spans.", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "keywords": ["Natural language processing"]}, "meta": {"decision": "Reject", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, most reviewers are not leaning sufficiently towards acceptance. In particular, it is unfortunate that authors can not evaluate their model on the leaderboard due to copyright issues. The role of standard datasets and benchmarks is to allow for meaningful comparisons. Evaluation on non-standard splits defeats this purpose. Fortunately, sounds like authors are working on getting their model evaluated on the leaderboard. Resolving that and incorporating reviewers' feedback will help make the paper stronger."}, "review": {"S1nJIhrIl": {"type": "rebuttal", "replyto": "HkIQH7qel", "comment": "We thank all three reviewers for the valuable comments and suggestions. \n\nWe agree with reviewer 1 that the lack of test results is not ideal and sadly we do not yet have a manner in which we can run on the hidden test set, as not all of our code is open-sourced. However, we hope that the significant dev set size of 10k items, along with the cross-validation results add some reassurance that our hyperparameter tuning scheme has not overfit the data.\n\nReviewer 3 points out that the results in this paper are no longer state of the art. It is true that there are other papers on the leaderboard that have now surpassed our results, largely through ensembling. However, we believe that our paper is the only work to specifically study the impact of different span representations and we agree with Reviewer 3 that our findings should be complementary to other recent work on this dataset. We have added some extra quantitative and qualitative analysis of the differences between the span classifier and the endpoints predictor to illustrate the manner in which the quality of endpoint predictions degrade for longer sentences, in particular showing the tendency of endpoint models to pick out endpoints from separate answer candidates.\n\nReviewer 2 points out that the difference in performance between our model and the Match-LSTM cannot be accounted for by the difference in label type alone, and asks for the other most salient differences between the two approaches. While there are many small differences between the two implementations, the ablations in Table 2.a. suggest that most of this gap is accounted for by the passage independent question representation that is missing in the Match-LSTM. We have added an analysis of this representation in a new Table 3 and we have updated our discussion of the Match LSTM to clarify the basis of our comparison.", "title": "Review response"}, "rkl-5LXXg": {"type": "rebuttal", "replyto": "SJtjsYAMg", "comment": "1. Assuming that you are referring to w_q in equation 10, this is a learned weighted vector that is used to score the output of the feed-forward neural network from the same equation. We have updated the submission to clarify this. The attention mechanism in the passage-independent representation is simply one method of aggregating the sequence of LSTM outputs into a single embedding. We explored alternatives, such as using the first and last LSTM outputs, or averaging, but these performed significantly worse.\n\n2. We mean greedy to be the following: (1) During training, the end index prediction is always conditioned on the gold start index, and the model is not exposed the scenario where the model chooses a non-gold start index. (2) During decoding, greedily choosing the highest scoring start index could lead to an overall sub-optimal pair of endpoints. \n\nOur endpoints predictor chooses start and end indices independently, meaning that the choice of the end index does not depend on the start index. Making these choices independently is not as good as making a global decision about the best answer span, but it avoids relying on a potentially suboptimal start index which a greedy model would not have been exposed to.\n\nDue to significant differences between the PtrNet architecture and RaSoR, we cannot attribute all of the increase in performance to the non-greedy decoding strategy. However, we believe that it is a significant factor in the model's better performance.", "title": "Model clarifications"}, "SJtjsYAMg": {"type": "review", "replyto": "HkIQH7qel", "review": "1.\tI found the descriptions of passage independent representation a bit confusing. What is word_q in equation 10? Why do we need to use attention structure here?\n2.\tIn page 6, the authors mention that the reason why Wang & Jiang\u2019 model does not perform so well is due to \u201cboth training and evaluation are greedy, making their system susceptible to search errors when decoding.\u201d However, the endpoints version of RASOR seems also to be greedy, but it seems to perform pretty well, as it outperformed Wang & Jiang (2016) already. Could the authors clarify this point for me?\nThe authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones. \n\n", "title": "Pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1hc_Pb4x": {"type": "review", "replyto": "HkIQH7qel", "review": "1.\tI found the descriptions of passage independent representation a bit confusing. What is word_q in equation 10? Why do we need to use attention structure here?\n2.\tIn page 6, the authors mention that the reason why Wang & Jiang\u2019 model does not perform so well is due to \u201cboth training and evaluation are greedy, making their system susceptible to search errors when decoding.\u201d However, the endpoints version of RASOR seems also to be greedy, but it seems to perform pretty well, as it outperformed Wang & Jiang (2016) already. Could the authors clarify this point for me?\nThe authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones. \n\n", "title": "Pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}