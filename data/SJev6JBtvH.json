{"paper": {"title": "Testing For Typicality with Respect to an Ensemble of Learned Distributions", "authors": ["Forrest Laine", "Claire Tomlin"], "authorids": ["forrest.laine@berkeley.edu", "tomlin@eecs.berkeley.edu"], "summary": "We show theoretically and empirically that testing for typicality with respect to an ensemble of learned distributions can account for learning error in the hypothesis testing. ", "abstract": "Good methods of performing anomaly detection on high-dimensional data sets are\nneeded, since algorithms which are trained on data are only expected to perform\nwell on data that is similar to the training data. There are theoretical results on the\nability to detect if a population of data is likely to come from a known base distribution, \nwhich is known as the goodness-of-fit problem, but those results require\nknowing a model of the base distribution. The ability to correctly reject anomalous\ndata hinges on the accuracy of the model of the base distribution. For high dimensional \ndata, learning an accurate-enough model of the base distribution such that\nanomaly detection works reliably is very challenging, as many researchers have\nnoted in recent years. Existing methods for the goodness-of-fit problem do not ac-\ncount for the fact that a model of the base distribution is learned. To address that\ngap, we offer a theoretically motivated approach to account for the density learning \nprocedure. In particular, we propose training an ensemble of density models,\nconsidering data to be anomalous if the data is anomalous with respect to any\nmember of the ensemble. We provide a theoretical justification for this approach,\nproving first that a test on typicality is a valid approach to the goodness-of-fit\nproblem, and then proving that for a correctly constructed ensemble of models,\nthe intersection of typical sets of the models lies in the interior of the typical set\nof the base distribution. We present our method in the context of an example on\nsynthetic data in which the effects we consider can easily be seen.", "keywords": ["anomaly detection", "density estimation", "generative models"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a new method for testing whether new data comes from the same distribution as training data without having an a-priori density model of the training data. This is done by looking at the intersection of typical sets of an ensemble of learned models. \n\nOn the theoretical side, the paper was received positively by all reviewers. The theoretical results were deemed strong, and the ideas in the paper were considered novel. The problem setting was considered relevant, and seen as a good proposal to deal with the shortcoming of models on out of distribution data. \n\nHowever, the lack of empirical results on at least somewhat realistic datasets (e.g. MNIST) was commented on by all reviewers. The authors only present a toy experiment. The authors have explained their decision, but I agree with R1 that it would be appropriate in such situations to present the toy experiment next to a more realistic dataset. This also means that the effectiveness of the proposed method in real settings is as of yet unclear. Although the provided toy example was considered clear and illuminating, the clarity of the text could still be improved.\n\nAlthough the reviewers had a spread in their final score, I think they would all agree that the direction this paper takes is very exciting, but that the current version of the paper is somewhat premature. Thus, unfortunately, I have to recommend rejection at this point. \n\n"}, "review": {"rygtsAw7YB": {"type": "review", "replyto": "SJev6JBtvH", "review": "This paper proposes to use ensembles of estimated probability distributions in hypothesis testing for anomaly detection.\nWhile the problem of density estimation with its application to anomaly detection is relevant, I have a number of concerns listed below:\n\n- Overall, this paper is not clearly written and it is difficult to follow.\n    Discussion is not straightforward at many points.\n    In particular, the objective of experiments on synthetic data in Section 3 is unclear. What is the proposal and how to evaluate it in the experiments?\n    There are also many grammatical mistakes, which also deteriorates the quality of the paper.\n- Technical quality is not high.\n    * Equations (3) and (4) are wrong. The distribution p should be not the ground truth but the empirical distribution.\n    * In experiments, only a simple Gaussian mixture model has been examined. A variety of distributions should be examined.\n    * How strong are the assumptions in Theorem 2 in practical situations?\n- There is no experimental evaluation for the proposed method. Hence the effectiveness of the proposed method is not clear.\n\nMinor comments:\n- P.2, L.3 in Section 2: \", The\" -> \", the\"\n- P.7, L.-6: \"q_1(x; \\theta_1,\" -> \"q_1(x; \\theta_1),\"\n- P.8, L.1 in Theorem 2: \"x \\in X, Consider\" -> \"x \\in X, consider\"\n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "H1l0rTEhjr": {"type": "rebuttal", "replyto": "SJev6JBtvH", "comment": "We will address the response from all of you in a single comment here to avoid redundancy. First and foremost, thanks to each of you for taking the time to read the paper and offer your thoughts on our work. \n\nIt is clear that the main concern of all of the reviewers is that the paper lacks empirical results. We acknowledge that this is a fair criticism. However, we wish to defend a little our choice of this example, and respond to some of the proposals for other experiments. First, to evaluate the effectiveness of our proposed method, knowing the pdf of the base distribution is necessary to evaluate how well the learned distributions approximate the base. This makes evaluating on standard image datasets difficult. Being constrained to datasets in which the pdf of the base is known, we decided to stick with the most simple example in which the phenomenon we describe occurs. We made this choice for primarily three reasons: clarity, space, and computational resources. We thought it unnecessary to make the base distribution excessively complicated for fears that it would make the result seem contrived and potentially an artifact of the choice of base distribution. \n\nR3 suggested that high-dimensional examples are still needed, but we argue that the 100 dimensional example shown is at least not low-dimensional. We felt that this example was sufficiently high dimensional to exhibit the effects of high-dimensional distributions, but low enough as to not require excessive computation and a finicky learning procedure. \n\nAlso on this note, R1 suggested that we make a dataset-to-dataset comparison as is done in other anomaly detection works. We believe that such comparisons are in a sense tangential to this work. While the comparisons made in those works do demonstrate the ability for the evaluated methods to reject samples from other chosen distributions, such a demonstration does not give confidence that the method will reject samples from some other distribution that was not evaluated against. In other words, such a comparison does not show how a model will perform on unknown unknowns, as a theoretical argument must be made for such a claim. Because prior work has already evaluated the effectiveness of using the typical set of a distribution as an acceptance region for one-sample tests, we felt that the most valuable use of space in our paper for experiments should go towards demonstrating the effectiveness of our method in better approximating the typical set itself. \n\nFinally, we would also like to acknowledge the many other comments made by all of the reviewers about other or more minor things, which have all been received and for the most part we agree with. However, there are a few things we would like to respond to. \n\nFirst, equations 3 and 4 are correct. Equation 3 is simply an objective. The objective that we wish to minimize can be anything we want -- whether we can evaluate and optimize for it is another question. The fact that in practice what is actually minimized is the KL between the empirical distribution of p and the parameterized distribution q is one interpretation. Another equally valid interpretation is that the objective that is  evaluate in practice is a sample approximation of the objective listed in (3), which is the view we take. We agree with R1 that to say this view is incorrect is itself an incorrect statement. \n\nFinally, there was some questions about the application of the theoretical results to practical use. For example, R1 wondered why the result of theorem 1 never showed up again. It more or less does show up in the form of theorem 3, which states that for an ensemble of distributions which are sufficiently different will have low intersection. The metric that is used to show \"difference\" in distributions in this case is the same as that given in theorem 1. \n\nThe direct applicability of the theorems to practical situations, given as they are in a sense asymptotic results, is a fair thing to question, as R2 did. While we wish we were able to come up with strong bounds that are valid for any size sample, such analysis is very hard and might even be impossible without making further assumptions. This is left for future work. Instead, we just offer theorem 1, for example, as a means to give motivation to the test for typicality, which otherwise had no theoretical motivation that we are aware of, and theorems 2 and 3 as motivation for the idea of taking intersections of typical sets. \n\nNoting all of that, we wish to again thank all of the reviewers for taking the time to think about the results we present. We are thankful for the criticisms as they are important for elevating our work. \n", "title": "Response to the reviewers"}, "B1gQLJXzYB": {"type": "review", "replyto": "SJev6JBtvH", "review": "Summary:  This paper analyzes and extends a recently proposed goodness-of-fit test based on typicality [Nalisnick et al., ArXiv 2019].  Firstly, the authors give bounds on the type-II error of this test, showing it can be characterized as a function of KLD[q || p_true] where p is the true data generating process and q is an alternative generative process.  The paper then shifts to the main contribution: an in-depth study of a Gaussian mixture simulation along with accompanying theoretical results.  The simulation shows that maximum likelihood estimation (MLE)---due to it optimizing KLD[p_true || p_model]---does not penalize the model for placing probability in places not occupied by p_true.  This means that while samples from p_true should fall within the model\u2019s typical set, the model typical set may be broader than p_true\u2019s.  Table 1 makes this clear by showing that only 30-40% of samples from the model fall within the typical set of p_true.  Yet >93% of samples from p_true fall within the models\u2019 typical sets.  The paper then makes the observation that the models do not have high overlap in their typical sets, and thus p_true\u2019s typical set could be well approximated by the intersection of the various models\u2019 typical sets.  Applying this procedure to the Gaussian mixture simulation, the authors observe that ~95% of samples drawn from the intersection of the ensemble fall within p_true\u2019s typical set.  Moreover, ~97% of samples from p_true are in the ensemble (intersection) typical set.  The paper closes by proving that the diversity of the ensemble controls the overlap in their typical sets, and hence increasing diversity should only improve the approximation of p_true\u2019s typical set.             \n\n____\n\nPros:  This paper contributes some interesting ideas to a recent topic of interest in the community---namely, that deep generative models assign high likelihood to out-of-distribution (OOD) data [Nalisnick et al., ICLR 2019] and how should we address this problem if we are to use them for anomaly detection, model validation [Bishop, 1994], etc.  This paper makes some careful distinctions between the true data process, the model, and the alternative distribution, which I have not seen done often in this literature.  And while the mass-covering effect of MLE on the resulting model fit is well known, this paper is the first with which I am aware that translates that fact into a practical recommendation (i.e. their intersection method).  Furthermore, this connection to ensembling may provide important theoretical grounding to other ensemble-based methods for OOD detection [Choi et al., ArXiv 2019].   \n\n____\n\nCons:  The primary deficiency in the paper is experimental.  While the text does make some compelling arguments in the Gaussian mixture simulations, some validation on real data must be provided.  Ideally experiments on CIFAR-10 vs SVHN (OOD) and FashionMNIST vs MNIST (OOD) should be reported as these data set pairings have become the benchmark cases in this line of literature.  \n\nBesides the lack of experiments on real data, I find the paper\u2019s material to be a bit disjointed and ununified.  For instance, Theorem 1 is never discussed again after it is presented in Section 2.1.  I thought for sure the presence of the KLD-term would be referenced again to relate the ensembling methodology back to the bound on the type-II error.  For another example, normalizing flows are discussed in Section 2.3 and the change-of-variables formula given in Equation 5.  However, normalizing flows are never mentioned again except in passing in the Related Work section.      \n\n____\n\nFinal Evaluation:  While I find the paper to contain interesting ideas, it is too unfinished for me to recommend acceptance at this time.  Experiments on real data must be included and the overall coherence of the draft improved.\n", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 3}, "BkxifG02KH": {"type": "review", "replyto": "SJev6JBtvH", "review": "Summary:\n\nI machine learning, we often have training data representative of an underlying distribution, and we want to test whether additional data come from the same distribution as the training data (e.g. for outlier/anomaly detection, or model checking). One way to do this is to learn a model of the underlying distribution, and test whether the additional data fall within the typical set of the model. This paper points out that the typical set of the model may be very different from the typical set of the underlying distribution if the model is learned by maximum likelihood, in which case a test of typicality with respect to the model would be a poor test of typicality with respect to the underlying distribution. The paper shows theoretically that the intersection of the typical sets of an ensemble of models lies within the typical set of the underlying distribution, provided that (a) each model is a good enough approximation to the underlying distribution, and (b) the models are all sufficiently different from each other. Based on that, the paper argues that a better test of typicality would be to test whether the additional data fall within the intersection of the typical sets of the ensemble of models.\n\nPros:\n\nThe paper addresses an interesting problem in a sound and well motivated way. There is a lot of work on outlier/anomaly detection that uses the model's probability density to determine whether a dataset is out-of-distribution or not, which is known to not be a good proxy for typicality, because atypical data can have high probability density. In contrast, this paper uses a well-founded notion of typicality based on the information-theoretic definition of a typical set.\n\nThe toy example that is used to illustrate the problem is clear and illuminating, and motivates the paper well. In particular, the example clearly illustrates the issue of local minima when training models, and the mass-covering behaviour of maximum-likelihood training.\n\nThe idea of using the intersection of the typical sets of an ensemble of models is interesting and clever, and backed by strong theoretical results.\n\nCons:\n\nEven though I appreciate the paper's theoretical contribution, there are no empirical results other than the motivating example. In particular, the paper proposes an idea and theory to back it up, but it doesn't really propose a practical method, and as a result it doesn't test the theory in practice.\n\nTheorems 2 and 3 provide a solid foundation for the proposed idea, but it's not clear how they can be used in practice. Specifically:\n- How can we verify that in practice the KL between the models and the underlying distribution is small enough as required by theorem 2 when we can't usually evaluate it?\n- In practice, how should we construct an ensemble such that the individual models in the ensemble are different enough from each other as required by theorem 3?\n- Both theorem 2 and 3 are valid \"for large enough n\". However, in practice we may want to check e.g. individual datapoints for typicality (in which case n=1). Are the theorems relevant for small n?\n\nThe paper is generally well written, but some statements made are either inaccurate or subjective, and I worry that they might mislead readers. Later in my review I will point out exactly which statements I'm referring to. I strongly encourage the authors to fix or moderate these statements before the paper is published.\n\nDecision:\n\nI believe the paper to be an important contribution, but the work is clearly incomplete. For this reason, my recommendation is weak accept, with an encouragement to the authors to continue the good work.\n\nInaccuracies or subjective statements that I encourage the authors to fix/moderate:\n\n\"we are still bad at reliably predicting when those models will fail\"\n\"we are unable to detect when the models are presented with out-of-distribution data\"\nThese statements may come across as too strong. I suggest making the statements about our current methods, rather than about the ability of the research community, and be more specific in what ways the current methods are inadequate.\n\n\"detecting out-of-distribution data [...] is formally known as the goodness-of-fit problem\"\nI'm not sure that detecting our-of-distribution data and goodness-of-fit are synonymous. Goodness-of-fit testing can be used in situations other than outlier detection, e.g. for testing whether a proposed model is a good fit to a dataset.\n\n(Second bullet-point of section 1) \"distributions having low KL divergence must have non-zero intersection\"\nTo be more precise, the typical sets must have non-zero intersection, not the distributions.\n\n\"determining which of two hypotheses are more probable\"\n\"H0 is deemed more probable\"\nClassical hypothesis testing does not assign a probability to a hypothesis, which would be a Bayesian approach instead. Therefore, it's technically incorrect to talk about the probability of a hypothesis in this context.\n\n\"which accepts the null-hypothesis\"\n\"f correctly accepting H0\"\nHypothesis testing doesn't accept a hypothesis, it merely decides whether to reject the null hypothesis in favour of the alternative hypothesis. Therefore, it may \"fail to reject\" the null hypothesis, but it never accepts it.\n\n\"the KL-divergence is equal to zero if and only if p(x) = q(x; \u03b8) \u2200x \u2208 X\"\nThe KL is equal to zero if and only if the distributions are equal, but the densities may still differ in at most a set of measure zero. Therefore, it's not a requirement that the densities match for all x for the KL to be zero.\n\n\"For example, by looking at the form of the KL-divergence, there is no direct penalty for q(x; \u03b8) in assigning a high density to points far away from any of the \u00afxi\u2019s\"\nThe problem that this statement is talking about is the problem of overfitting, which is the problem of the model learning the specifics of the training data rather than the underlying distribution. However, the statement preceding the above is about the problem of local minima when optimizing then parameters of a model. These two problems are distinct and shouldn't be conflated, as they are here.\n\n\"this requires direct knowledge of p(x) to evaluate the objective\"\nHowever we can evaluate the objective up to an additive constant when p(x) is known up to a multiplicative constant, which is enough to optimize it.\n\n\"as do all divergences other than the forward KL, to the best of our knowledge\"\n\"This makes the forward KL-divergence special in that it is the only divergence which can directly be optimized for.\"\nI don't think this is true. For example, the Maximum Mean Discrepancy is a divergence, since it's non-negative and zero if and only if the two distributions are equal, but it only involves expectations under p(x) and can be directly optimized over the parameters of q(x; \\theta). Moreover, the second statement doesn't follow from the first: it's incorrect to conclude that the forward KL is the only one that can be directly optimized for, based only on one's state of knowledge.\n\n\"Variational Auto-encoders [...] map a lower-dimensional, latent random variable\"\nThere is no fundamental reason why the latent variable of a VAE has to be low-dimensional. We may do this often in practice, but a VAE with a high-dimensional latent variable may also be used.\n\n\"Because the image of any non-surjective function necessarily has measure zero\"\nThis is not true; the absolute-value function is not surjective but its image doesn't have measure zero in the set of real numbers. I understand what the statement is trying to say, but it's important that it's said accurately.\n\n\"autoregressive models, such as PixelCNN\"\nAutoregressive models can also be used to model discrete variables in which case they can't be thought of as flows. In fact, PixelCNN as first proposed is a model of discrete variables.\n\n\"all of these models rely on optimizing the forward KL-divergence in order to learn their parameters\"\nNot necessarily, flow-based models don't have to be optimized by minimizing the forward KL. For example, they can be trained adversarially in the same way as GANs, and in principle can be trained with other divergences or integral probability metrics. The model and the loss are (at least in principle) orthogonal choices.\n\n\"advancements in the expressivity of the models are unlikely to fix the undesired effects\"\nThis is a subjective assessment, and is not sufficiently backed by arguments where it first appears. I understand that the arguments are presented later in section 3, so I would at least suggest that a forward reference to the argumentation in section 3 is given here.\n\nFigure 5 gives the impression that the model samples have less variance than the ground-truth samples. Isn't that surprising given that the problem is that minimizing the forward KL leads to mass-covering behaviour? I suspect that the problem here is that there are more ground-truth samples than model samples, and the ground-truth samples saturate the scatter plot. If that's the case, I believe that figure 5 is very misleading.\n\n\"we see that the learning procedure converged\"\nWe know however that the learning procedure hasn't really converged, instead it is stuck at a saddle point (where the model is using a single mode to cover two modes of the underlying distribution). In other words, it appears to us that the learning procedure has converged, even though it hasn't, and possibly if we wait for long enough we will see rapid improvement when the procedure escapes the saddle point. Therefore, I would at least say \"we see that the learning procedure has appeared to converge\".\n\nI would expect the bottom-right entry of table 1 to be higher than 90% like the other diagonal elements, so I suspect that it might be a typo.\n\nIn eq. (7), shouldn't each log q_k be divided by n?\n\n\"in practice we find that it is much easier to find an ensemble of models such that the multi-typical set approximates the ground-truth typical set than the bounds require\"\nThere is no empirical evidence presented in the paper in support of this statement.\n\n\"least probable density\"\n\"least typical density\"\nI understand what the intended meaning of these terms is, but these terms make little sense mathematically nevertheless. I would suggest that the statement is rewritten in a more precise and direct way.\n\n\"This measure only corresponds to measuring typicality if the bijection is volume preserving\"\nI'm not sure that the distance from a Gaussian mean is a valid measure of typicality. In high dimensions, the region around the mean is very atypical.\n\nMinor errors, typos, and suggestions for improvement:\n\nThe phrase \"the authors in Smith et al. (2019) propose\" is a bit awkward. Better say \"Smith et al. (2019) propose\", as Smith et al are indeed the authors.\n\nMissing full stop in first bullet-point of section 1.\n\nIt would be good to provide more details of the experiment in section 3. Specifically:\n- What training algorithm was used to maximize the likelihood? SGD or EM?\n- How many training datapoints were used?\n\n\"to index the 5 experiments ran\" --> run\n\n\"refer the k-th learned density\" --> refer to\n\ninterestig --> interesting\n\nMissing closing bracket in point 1 of section 4.\n\nCapital C in \"Consider\" in theorem 2.\n\n\"if every model in a density of learned distributions\" --> an ensemble of learned distributions\n\n\"where as the method we propose\" --> whereas\n\n\"can be found in in\", double \"in\"", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "Skeymf3J5B": {"type": "rebuttal", "replyto": "BylKbgx8KH", "comment": "Dear Dr. Zhu,\n\nThank you so much for your comments, and especially for linking your recent paper on the asymptotically optimal one- and two-sample kernel-based tests. In a final version of our paper we will be sure to reference your work, as it is very interesting and relevant. I will try to address all of your comments here, and will definitely address them in the final version of our paper, since they are all valid points.\n\n1. You are correct that the two-sample problem directly addresses the problem of discerning distributions when only samples from the base distribution are given. The angle that we approached our work from was in response to recent proposals in the deep learning community that learning a succinct representation of the base distribution from samples to then use in one-sample testing could be advantageous over two-sample testing for computational reasons. I think that such a proposal is not made explicit in those works (nor as you point out, in our own), perhaps due to the known computational issues with sample-to-sample comparisons, e.g. nearest-neighbors.  I agree that we made a mistake in not mentioning two-sample testing at all, or the computational motivation in the test we propose. That being said, I do still think that the test we propose is still well-motivated, since for applications with massive, high-dimensional datasets, computing MMD- or KSD-based tests could be prohibitively slow to compute, where as the test we propose would not.  We will be sure to make this point very explicit in an updated version of the paper.\n\n2. Again, you are correct that \"error-rate\" might not be the most appropriate term. Instead \"probability of error\" is a more accurate way to describe the term. Either way, while there is an additive $3\\epsilon$ term on the bound, and a stronger bound would not include this additive term, the bound is still valid and gives some insight into what we can say about the power of the test on typicality proposed in (Nalisnick, 2019). I am curious about what you mentioned regarding tests based on the entropy typical set being far from optimal. Would you mind sharing a reference that includes that negative result? I would be very interested in reading about that. \n\n3.  The mixture-of-gaussians example was chosen since it is the simplest example we could think of that demonstrated the phenomenon we were interested in showing. The same phenomenon can be seen for much more complicated examples, although in all examples the true base distribution must be known. This makes such an effect difficult to show on real-world datasets, and therefore examples can quickly become seemingly contrived. We thought that for clarity we would show that such effects are evident in one of the simplest of cases, but we understand that there are also limitations to focusing on such type of examples. Your comment is helpful, and in the final revision of our work we can instead/additionally demonstrate the effect on more complicated examples that are more similar to real-world datasets one might encounter in practice. \n\nYour comment regarding the optimization of parameters is also valid. In that example we optimized parameters using a gradient-based method with momentum (Adam) as is commonly used when optimizing the parameters of large flow-based generative models, and when the structure of the base-distribution is not known a priori.\n\n4.  In the definition of multi-typicality, we do mean $\\max$. This results in an acceptance region which is the intersection of the typical sets of each member in the ensemble. Theorem 2 gives sufficient conditions for such an intersection of typical sets to also have non-zero intersection with the typical set of the base distribution. These conditions do not guarantee that the resulting multi-typical set has large probability with respect to the base distribution, but provide a means to under-approximate the acceptance region defined by the test on entropy typicality ( as opposed to over-approximate the acceptance region, which is the usual result of using a learned approximation of the base typical set). \n\n5.  This condition is to indicate that it may be impossible to define tests with non-trivial power for differentiating from the null hypothesis if alternate distributions can be assumed to be arbitrarily close to the base distribution. Admittedly, as you point out, we did not give this detail adequate consideration in the submitted version, and will address it in the revised version. \n\n6. All noted and good points, will fix in revised version.\n\nAgain, thank you very much for taking the time to read our submission and making thoughtful and informed comments. Each of your points are well founded and we look forward to incorporating them, and in doing so, strengthening our work. \n\nKindly, \nThe authors of submission 1992", "title": "Thank you very much for your helpful comments"}}}