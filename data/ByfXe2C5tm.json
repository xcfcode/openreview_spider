{"paper": {"title": "NLProlog: Reasoning with Weak Unification for Natural Language Question Answering", "authors": ["Leon Weber", "Pasquale Minervini", "Ulf Leser", "Tim Rockt\u00e4schel"], "authorids": ["leonweber@posteo.de", "p.minervini@gmail.com", "leser@informatik.hu-berlin.de", "tim.rocktaeschel@gmail.com"], "summary": "We introduce NLProlog, a system that performs rule-based reasoning on natural language by leveraging pretrained sentence embeddings and fine-tuning with Evolution Strategies, and apply it to two multi-hop Question Answering tasks.", "abstract": "Symbolic logic allows practitioners to build systems that perform rule-based reasoning which is interpretable and which can easily be augmented with prior knowledge. However, such systems are traditionally difficult to apply to problems involving natural language due to the large linguistic variability of language. Currently, most work in natural language processing focuses on neural networks which learn distributed representations of words and their composition, thereby performing well in the presence of large linguistic variability. We propose to reap the benefits of both approaches by applying a combination of neural networks and logic programming to natural language question answering. We propose to employ an external, non-differentiable Prolog prover which utilizes a similarity function over pretrained sentence encoders. We fine-tune these representations via Evolution Strategies with the goal of multi-hop reasoning on natural language.  This allows us to create a system that can apply rule-based reasoning to natural language and induce domain-specific natural language rules from training data. We evaluate the proposed system on two different question answering tasks, showing that it complements two very strong baselines \u2013 BIDAF (Seo et al., 2016a) and FASTQA (Weissenborn et al.,2017) \u2013 and outperforms both when used in an ensemble.", "keywords": ["symbolic reasoning", "neural networks", "natural language processing", "question answering", "sentence embeddings", "evolution strategies"]}, "meta": {"decision": "Reject", "comment": "This paper combines Prolog-like reasoning with distributional semantics, applied to natural language question answering. Given the importance of combining neural and symbolic techniques, this paper provides an important contribution. Further, the proposed method complements standard QA models as it can be easily combined with them.\n\nThe reviewers and AC note the following potential weaknesses:\n(1) The evaluation consisted primarily on small subsets of existing benchmarks, \n(2) the reviewers were concerned that the handcrafted rules were introducing domain information into the model, and (3) were unconvinced that the benefits of the proposed approach were actually complementary to existing neural models. \n\nThe authors addressed a number of these concerns in the response and their revision. They discussed how OpenIE affects the performance, and other questions the reviewers had. Further, they clarified that the rule templates are really high-level/generic and not \"prior knowledge\" as the reviewers had initially assumed. The revision also provided more error analysis, and heavily edited the paper for clarity. Although these changes increased the reviewer scores, a critical concern still remains: the evaluation is not performed on the complete question-answering benchmark, but on small subsets of the data, and the benefits are not significant. This makes the evaluation quite weak, and the authors are encouraged to identify appropriate evaluation benchmarks. \n\nThere is disagreement in the reviewer scores; even though all of them identified the weak evaluation as a concern, some are more forgiving than others, partly due to the other improvements made to the paper. The AC, however, agrees with reviewer 2 that the empirical results need to be sound for this paper to have an impact, and thus is recommending a rejection. Please note that paper was incredibly close to an acceptance, but identifying appropriate benchmarks will make the paper much stronger."}, "review": {"S1x2IuwcnX": {"type": "review", "replyto": "ByfXe2C5tm", "review": "Updated after reading author revisions:\nI appreciate the clarifications, the response answered almost all of my small technical questions.  That plus the new error analysis increases my opinion about the paper, and I'm no longer concerned that the rule templates are hand-generated given their generality and small number.  I am still concerned that we don't actually know how well the methods work, because the test sets are small and the performance differences between the methods (in Table 1) are quite close.  I will raise my score one point.\n\nThe authors might try to evaluate using k-fold cross-validation with the training set, to obtain more examples for evaluation.\n\nOriginal review:\n\nThe paper presents a technique for using prolog along with neural representations and Open IE to perform reasoning with weak unification.\n\nI like the basic direction of trying to combine prolog with neural models, and the weak unification notion.  The approach seems sufficiently novel, and the GRL is a reasonable heuristic.\n\nI do, however, have significant concerns about the experiments.  The data sets are selected subsets of other standard benchmarks, rather than the entire benchmarks, and the test sets are quite small (e.g., the \"developer\" column where the NLProlog approach shows some of the larger wins -- when ensembled with previous techniques -- is based on a test set of only 29 examples).  Given the hand-annotated nature of much of the input knowledge (the rule templates), this introduces an important concern that the experimental wins will not be robust in more realistic settings where different knowledge may be required.\n\nMinor comments/questions\npage 2: \"without the need to transforming\"\nI did not understand how individual symbols, predicates and entities, have embeddings that come from sentence vectors (Section 4.1).\nThe learning objective in Section 4.2 seems reasonable, but I did not understand how \"evolution\" was part of the strategy there.\nThe example rule template for transitivity isn't actually transivity unless p_i=p_j for all i,j, I found that a little confusing.\nWhere are \"t-norms\" (mentioned at the top of page 6) used?  I did not see this.\n\"candidates entities\" -> \"candidate entities\"\n", "title": "Interesting direction, experiments are unconvincing", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJlraS3Ch7": {"type": "review", "replyto": "ByfXe2C5tm", "review": "Update:\nI appreciate the through error analysis the authors have done in the revision, which addressed my major previous concerns. I've updated my score accordingly.\n\nThis paper presents an approach to combine Prolog-like reasoning with distributional semantics. First, extracted fact triples are unified (i.e. mapped to) predicates and entities. Next, reasoning is performed with rule templates, where predicates and entities are abstracted. Since the reasoning process is non-differentiable, zero-oder optimization is used to fine-tune the predicate / entity embeddings.\n\nThe general idea of combining logical reasoning with neural models is quite appealing. A sketch of the algorithm is to first build structured knowledge from the text, then do reasoning over it to answer queries. In this work, the first step is completely relied on an off-the-shelf tool, Open-IE. It would be useful to see whether this step is the bottle neck of such approaches. One possibility is to apply the model to knowledge graph reasoning, which would remove any noise introduced from the knowledge extraction step, and solely focus on evaluating reasoning.\n\nThe results are a bit restricted, as in only a subset of the datasets are evaluated. I suspect part of the reason is that most of the QA datasets which claims to require multi-step reasoning don't really need much reasoning... However, it would be useful to do some simple (perhaps qualitative) analysis on the data quality, and make sure that it indeed tests what it intended to. For the ensemble results in Table 1, usually even ensembling same models trained with different seed would show improvements, so I'm not completely convinced that BiDAF and NLProlog are complementary - would be nice to see error analysis here.\n\nQuestion:\nWhat is the size of hand-coded predicates and rules? What's the coverage of these rules on the datasets, i.e. are there questions unanswerable by the provided rules?\n\nOverall, while the results are limited, the approach is interesting, and hopefully will spur more work towards interpretable models with explicit reasoning.", "title": "interesting approach towards combining neural networks with logic reasoning", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1xQHGlYh7": {"type": "review", "replyto": "ByfXe2C5tm", "review": "\n\nUPDATE: Given the authors' rebuttal and the clear improvements to their paper, I've increased my rating of the work.\n\n=======================\n\nThis paper presents NLProlog, a method that combines logical rules with distributed representations for reasoning on natural language statements. Natural language statements (first converted to logical triples) and templated logical rules are embedded in a vector space using a pretrained sentence encoder. These embedded \"symbols\" can be compared in vector space, and their similarity used in a theorem prover (Prolog) modified to support weak unification. The theorem prover determines the answer to a natural language query by constructing a proof according to its logical rules.\n\nTraining through the non-differentiable theorem prover occurs via an \"evolutionary strategy,\" which enables the model to fine-tune its sentence encoders and learn domain-specific logic rules directly from text. The authors also propose a Gradual Rule Learning (GRL) algorithm that seems necessary for the optimization process to converge on good solutions.\n\nDespite the model's complexity, the paper was fairly clear to me.\n\nAlthough the proposed model is a conglomeration of pre-existing parts, the combination is original to my knowledge. The use of Open Information Extraction to transform natural language statements to logical statements, which amenable to theorem provers, is novel and also circumvents the complicated preprocessing required by previous related works.\n\nThe authors evaluate the proposed approach on subsets of the Wikihop dataset and BABI-1k. NLProlog performs competitively with neural models, similarly augmented with Sent2Vec but lacking explicit logical rules, only on the 'country' subset of Wikihop. It does not compete with or clearly outperform these models in general. As the authors state, it further \"struggles to find meaningful rules for the predicates 'developer' and 'publisher'.\"\n\nNLProlog demonstrates strong performance on a subset of problems labelled unanimously by annotators to require multi-hop reasoning. Unfortunately, this is only done for the \"country\" subset of Wikihop, on which the model was already shown to have the strongest performance. I'd find this more convincing if similar improvements were shown on the other subsets (publisher, developer).\n\nTaking into account also that the BABI subset was used only for ablation, the limited results call into question the significance of the work. It would definitely benefit from more extensive experimental validation. On the other hand, it's very positive to see that NLProlog seems to succeed where the neural models fail, and vice versa, so that the two approaches can be combined in an ensemble to achieve state-of-the-art results. This suggests that the paper's line of research has something to add to the community and should be pursued further. I'd find this result more interesting if an error analysis elucidated some characteristics of the examples that each approach does well/poorly on.\n\nI'd like to see more analysis in general, that answers questions including:\n- How reliable is the Open IE system and how does its performance impact the end task?\n- How well-specified must the a priori rule structures be to achieve good performance? Further, how does the number and structure of the rules (a hyperparameter in this work) affect performance?\n- What is the run-time/complexity of the exhaustive proof search during training?\n- Relatedly, you state that you limit the rule complexity to two body atoms in the rule templates for BABI. Can you estimate what rule complexity is required in the Wikihop tasks?\n\nI would like to recommend this work more confidently because it tackles such an important problem and does so in an interesting, well-conceived way. My reluctance arises from the limited experimental validation and analysis. Given more analysis details and experimental evidence from the authors, I'm happy to raise my recommendation.\n\nPros:\n- the method complements standard deep QA models to achieve state-of-the-art results in an ensemble.\n- unifying neural representations with logical/symbolic formalisms is an important research direction.\n- a code release is planned.\n\nCons:\n- a very complex model, whose details are occasionally unclear; the algorithms in Appendix A are helpful but they are not in the main text.\n- the model expresses only a limited subset of first order logic; dynamically changing world states are not supported (yet).\n- limited experimental validation.\n- it's good to be able to incorporate prior knowledge, but it seems like it's quite necessary to pre-specify rules (in template form).\n\nMinor quibble: Evolutionary learning strategies, such as genetic algorithms, go back a long way. It's strange using only a reference from 2017 to introduce them.", "title": "Promising unification of neural representations with learned logical rules", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJlfVSl8C7": {"type": "rebuttal", "replyto": "ByfXe2C5tm", "comment": "Summary of changes, Nov 23, 2018\nWe thank all three reviewers for their detailed and insightful feedback. We used it to update our submission by introducing the following changes:\n\n- We added an extensive error analysis (Section 5.6) which elucidates the strengths and weaknesses of NLProlog and the neural QA models and provides additional evidence that the approaches are indeed complementary. Additionally, the error analysis revealed that the OpenIE step is a likely bottle neck of NLProlog, which shows a path for future improvement.\n\n- We updated Section 5.7 with an additional experiment on bAbI which studies the effect of varying the size of the rule templates, finding that the number and structure of rules has a strong effect on the convergence speed and predictive performance. \n\n- We updated various sections to improve the clarity of the paper, especially with regards to the details of the proposed method: we clarified the structure of the employed rule templates in Section 4.3, added Section A.2 to discuss the runtime of the proof search step and applied optimizations, as well as expanded on the initialization of the symbol embeddings in Section 4.1.\n", "title": "In-depth error analysis, additional experiments, extensive improvements in clarity"}, "S1gEDWeLAm": {"type": "rebuttal", "replyto": "BJlraS3Ch7", "comment": "Thank you for your feedback and suggestions. It is great to hear that you found our paper interesting.\n\nQ14. In this work, the first step is completely relied on an off-the-shelf tool, Open-IE. It would be useful to see whether this step is the bottleneck of such approaches.\n\nThank you for this useful suggestion. We performed an in-depth error analysis of NLProlog which revealed that indeed a large portion of errors are due to a failure in the OpenIE step (see Q3). We added this analysis to Section 5.6.\n\nQ15. For the ensemble results in Table 1, usually even ensembling same models trained with different seed would show improvements, so I'm not completely convinced that BiDAF and NLProlog are complementary - would be nice to see error analysis here.\n\nThis is an important observation and question. Indeed, the error analysis we now added to the paper supports our hypothesis that the two approaches are complementary on our evaluation data (see Section 5.6.). The proposed method and neural Q&A models seem to have orthogonal inductive bias, and thus complementary strengths and weaknesses.\n\nQ16. Question: What is the size of hand-coded predicates and rules?\n\nPlease also see Q4, Q6, and Q9: in all experiments, we use the same set of just six rule templates, since rules with two body atoms are sufficient for expressing arbitrarily complex rules [1].\n\n[1] Evans and Grefenstette. Learning Explanatory Rules from Noisy Data. JAIR 2018\n\nQ17. What is the coverage of these rules on the datasets, i.e. are there questions unanswerable by the provided rules?\n\nFor \u201ccountry\u201d questions, the six rule templates in principle were sufficient to learn rules that can answer all problems but four -- see the new error analysis for the reasons where the actual results deviate from this statement. As stated in the paper, rule induction did not perform well for \u201cdeveloper\u201d and \u201cpublisher\u201d questions, either because OpenIE was not able to extract relevant facts (in the case of \u201cpublisher\u201d), or a low proportion of answerable multi-hop questions (in the case of \u201cdeveloper\u201d). For more detail, please see Section 5.6. We think that, given the correct rules, the provided rule templates would be sufficient for answering all studied queries. See Q6 for an additional discussion of this issue.\n", "title": "Hybrid NLProlog-Neural models, OpenIE, and rules"}, "rJgkJWxI0Q": {"type": "rebuttal", "replyto": "S1x2IuwcnX", "comment": "Thank you a lot for your feedback!\n\n\nQ8. The data sets are selected subsets of other standard benchmarks, rather than the entire benchmarks, and the test sets are quite small \n\nWe agree that a more extensive evaluation would strengthen our work. However, we also note that NLProlog is a technique specifically designed to support multi-hop reasoning (see also Q2 above). As this today is a non-standard feature, most evaluation datasets we are aware of do not contain (or just a few) structured predicates which require such capabilities. For all performed evaluations in which our framework was applicable, we observe consistent improvements of NLProlog when ensembled with neural Q&A models. Following Q3 (above) and Q14 (below), we added to the paper a detailed error analysis demonstrating that NLProlog has strengths that are directly complementary to neural Q&A models. \n\nQ9. Given the hand-annotated nature of much of the input knowledge (the rule templates), this introduces an important concern that the experimental wins will not be robust in more realistic settings where different knowledge may be required. \n\nOur method indeed requires the manual specification of rule templates. However, we actually use the same set of just six rule templates across all tasks (minus the ablation study). Rule instantiation as required by a specific task is performed automatically during the learning phase. We rephrased Section 4.3 to make this difference more clear. \n\nWhether the proposed approach would work if multiple query predicates are involved indeed is an open yet very interesting question. We added this thought as future work to Section 6.\n\nQ10. I did not understand how individual symbols, predicates and entities, have embeddings that come from sentence vectors (Section 4.1)\n\nWe associate every entity and predicate to an embedding vector, initialised with the Sent2Vec sentence encoder: starting from text, we extract the relevant facts via OpenIE, and encode their predicate and entities using Sent2Vec. We apologise if this was not clear enough and rephrased the description in Section 4.1.\n\nQ11. The learning objective in Section 4.2 seems reasonable, but I did not understand how \"evolution\" was part of the strategy there.\n\n\u201cEvolution Strategies\u201d is a gradient estimation method proposed in [1], which is commonly also interpreted under an evolutionary computing perspective [2]. In this case, \u201cEvolution\u201d stems from the fact that the gradient is a function of a population of sampled model parameters. We rephrased Section 1 to make these issues clearer.\n\n[1] Salimans et al., Evolution Strategies as a Scalable Alternative to Reinforcement Learning, 2017\n[2] Eiben, Agoston E., and James E. Smith. Introduction to evolutionary computing. Vol. 53. Berlin: springer, 2003.\n\nQ12. The example rule template for transitivity isn't actually transitivity unless p_i=p_j for all i,j, I found that a little confusing. \n\nThank you for pointing this out\u2014indeed \u201ctransitivity\u201d is not the best term here. We changed it to \u201cmulti-hop rule\u201d throughout the paper.\n\nQ13. Where are \"t-norms\" (mentioned at the top of page 6) used? I did not see this.\n\nSorry, this was a typo from an older version of the manuscript. While our aggregation functions are t-norms in a mathematical sense [3], we have replaced the mention by \u201caggregation function\u201d to be consistent with the rest of the paper.\n\n[3] Sessa, Maria I. 2002. \u201cApproximate Reasoning by Similarity-Based SLD Resolution.\u201d Theoretical Computer Science 275 (1): 389\u2013426.\n", "title": "Datasets and rule annotations"}, "HkgkdleUAX": {"type": "rebuttal", "replyto": "H1xQHGlYh7", "comment": "Q5. What is the run-time/complexity of the exhaustive proof search during training?\n\nAs in most logic programming frameworks, the number of candidate proofs essentially grows exponentially with the depth of the proof trees. This is a particular problem in our setting where in principle all predicates match with all others (to a certain degree). However, we do not use an exhaustive proof search but a simple pruning heuristic which disregards all proof steps with a score lower than a given threshold. This threshold is updated dynamically during the search step. We now make this part of our method more clear in the appendix (see Section A.2). \n\nQ6. Relatedly, you state that you limit the rule complexity to two body atoms in the rule templates for bAbI. Can you estimate what rule complexity is required in the Wikihop tasks?\n\nAs mentioned above, we hypothesized that direct entailment, symmetry, and transitivity are the most important types of reasoning. In our analysis we found that, for answering most questions on the considered WikiHop predicates, a limited number of rules of the form  \u2018p(X,Z) :- p(X,Y), p(Y,Z)\u2019 is sufficient. This is because the WikiHop dataset was constructed by traversing the graph connecting the entities mentioned in the support texts, a property that can be well exploited by our method.\n\nQ7. Minor quibble: Evolutionary learning strategies, such as genetic algorithms, go back a long way. It's strange using only a reference from 2017 to introduce them.\n\nThank you for pointing this out. We updated our references to better reflect the long tradition in studying evolutionary learning (See Section 1). Any further suggestions are welcome.\n", "title": "Rules and run-time complexity"}, "SJxXSeeUR7": {"type": "rebuttal", "replyto": "H1xQHGlYh7", "comment": "Thanks a million for your detailed feedback and suggestions. We are glad to hear that you found our paper to be clear and this line of work interesting, particularly the fact that NLProlog succeeds where existing neural models fail. \n\nQ1. Unfortunately, this is only done for the \"country\" subset of WikiHop, on which the model was already shown to have the strongest performance. I'd find this more convincing if similar improvements were shown on the other subsets (publisher, developer).\n\nExtending our evaluation with further relationships would certainly be a worthwhile extension of our paper. Following your advice, we performed additional experiments on the predicate record_label. In these experiments, OpenIE was not able to extract the relevant facts, so it was not possible to apply our framework. Following Q3 (below), we performed an extensive error analysis to also explaining results on \u201cdeveloper\u201d and \u201cpublisher\u201d questions (see Section 5.6).\n\nQ2. On the other hand, it's very positive to see that NLProlog seems to succeed where the neural models fail, and vice versa, so that the two approaches can be combined in an ensemble to achieve state-of-the-art results.\n\nThanks for your comment. Indeed, the last two rows of Table 1 show consistently strong results for an ensemble of the proposed logic-based approach and deep neural models. We rephrased Section 5.5 to better explain our ensembling strategy and added a detailed qualitative analysis (see Q3 below) demonstrating that the two approaches have complementary strengths.\n\nQ3. I'd find this result more interesting if an error analysis elucidated some characteristics of the examples that each approach does well/poorly on.\n\nThank you for this excellent suggestion. We added to the paper an extensive error analysis, see Section 5.6. The takeaway message is that (up to issues with data quality) the OpenIE component causes the majority of NLProlog errors, while neural Q&A models are robust even when crucial information is missing in the support documents.  However, neural Q&A models often also pick up spurious data artifacts which sometimes coincidentally even lead to correct results (see Section 5.6). We show that NLProlog is much less affected by such effects. Furthermore, NLProlog provides proofs for its predictions which make it easy for users to spot errors. \n\nQ4. How well-specified must an priori rule structure be to achieve good performance? Further, how does the number and structure of the rules (a hyperparameter in this work) affect performance?\n\nPlease note that we use the same rule templates in all evaluation results - we apologize that this was mentioned only in the appendix in our submission. Actually, the model can learn more complex rules by composing simpler rules [1]. We employ two templates for each of the following three structures \u2018p(X, Y) :- q(X, Y)\u2019, \u2018p(X, Y) :- q(Y, X)\u2019, and \u2018p(X,Z) :- q(X,Y), r(Y,Z)\u2019, because these capture the basic reasoning steps of entailment, symmetry and transitivity (see Q12 regarding the appropriateness of this term). We studied the impact of having more/less structures in the ablation study on bAbI and found that these have a significant impact on convergence speed and overall performance. We expand on this in Section 5.7 of the revised paper.\n\n[1] Evans and Grefenstette, Learning Explanatory Rules from Noisy Data, 2017", "title": "Evaluation and error analysis"}}}