{"paper": {"title": "The loss surface of residual networks: Ensembles and the role of batch normalization", "authors": ["Etai Littwin", "Lior Wolf"], "authorids": ["etai.littwin@gmail.com", "liorwolf@gmail.com"], "summary": "Residual nets are dynamic ensembles", "abstract": "Deep Residual Networks present a premium in performance in comparison to conventional\nnetworks of the same depth and are trainable at extreme depths. It has\nrecently been shown that Residual Networks behave like ensembles of relatively\nshallow networks. We show that these ensemble are dynamic: while initially\nthe virtual ensemble is mostly at depths lower than half the network\u2019s depth, as\ntraining progresses, it becomes deeper and deeper. The main mechanism that controls\nthe dynamic ensemble behavior is the scaling introduced, e.g., by the Batch\nNormalization technique. We explain this behavior and demonstrate the driving\nforce behind it. As a main tool in our analysis, we employ generalized spin glass\nmodels, which we also use in order to study the number of critical points in the\noptimization of Residual Networks.", "keywords": ["Deep learning", "Theory"]}, "meta": {"decision": "Reject", "comment": "The paper presents an analysis of residual networks and argues that the residual networks behave as ensembles of shallow networks, whose depths are dynamic. The authors argue that their model provides a concrete explanation to the effectiveness of resnets. \n \n However, I have to agree with reviewer 1 that the assumption of path independence is deeply flawed. In my opinion, it was also flawed in the original paper. Using that as a justification to continue this line of research is not the right approach. We cannot construct a single practical scenario where path independence may be expected to hold. So we should not be encouraging papers to continue this line of flawed reasoning.\n \n I thus cannot recommend acceptance of this paper."}, "review": {"H1iGpyxPg": {"type": "rebuttal", "replyto": "HyxZunJDl", "comment": "We respectfully expected the reviewer to explain and support the broad assertions that were made. Instead, the comment above only reflects opinions (\"my mind\"; \"I still think\").\n\n1. The reviewer did not provide the details that are required in order to hold the discussion on a factual level. For example, we asked to know which theorems \"were already introduced\" and where the \"main theoretical results mentioned there were in fact already proved\".\n2. It is hard to tell from the one line comment which parts of our very detailed and factual response were unconvincing to the reviewer.\n3. We find the title of the reviewer's comment to be overly aggressive and in sheer contrast to the amount of factual support provided in the review.", "title": "We were hoping for more information"}, "SyxYYVDVl": {"type": "rebuttal", "replyto": "ryTj8pINe", "comment": "We thank AnonReviewer3 for the supportive review. The reviewer asked that we provide CIFAR-10 results for ResNets without Batch Normalization. These results are provided below. The experiments are almost identical to the previous experiments with a few exceptions: (1) All Batch Normalization layers were removed; (2) Since it is harder to train without Batch Normalization, we focus on ResNets of depth 20; and (3) We employ an initial learning rate that is ten times smaller, otherwise NaN occurs after a few epochs.\n\nCIFAR-10 Norm of the convolutional layers\u2019 weights per layer for multiple epochs http://imgur.com/JdRAM7j \nCIFAR-10 Mean norm of weights per epoch http://imgur.com/46ngxb6 \nCIFAR-100 Norm of weights per layer for multiple epochs http://imgur.com/ANKeRz0\nCIFAR-100 Mean norm of weights per epoch http://imgur.com/MzHOFgE\n\nNote that there is a steady increase in the weight norm until epoch 81 when the learning rate is reduced, at which point the norms stabilize. This could stem from the networks being slower to train without Batch Normalization and epoch 81 being too early for reducing the learning rate.\n", "title": "CIFAR-10/CIFAR-100 without batch normalization"}, "BybRFjUVx": {"type": "rebuttal", "replyto": "B15BdW8Vx", "comment": "We completely disagree with the novelty assertions made by the AnonReviewer1. Most of our main results are very distant from the results of Choromanska et al. and are entirely novel both technically and conceptually. For example, the detailed study of the driving force in ResNets.\n\nWe also disagree with the assertion that \u201cThe authors also did not get rid of lots of assumptions from Choromanska et al.\u201d, since, as the previous discussion on openreview.net reveals, we make selective use of these assumptions and Theorems 3 and 4 are virtually assumption free. Please refer to the discussion section of the revised manuscript, uploaded a few days ago.\n\nWe are at a great disadvantage in this discussion since the assertions of AnonReviewer1 are made in general terms, which makes it difficult for us to understand (1) which results are deemed as incremental and (2) in what way these are judged as incremental, (3) which theoretical techniques are claimed to have already been done and (4) where these were done. \n\nWe would be grateful if the reviewer could point to specific theorems that might be problematic and would agree to weigh their importance in the context of the entire body of presented results. In order to promote such a discussion, we address below each theorem and some lemmas along the dimensions of closest analogy in the literature and technical novelty.  We believe that the facts clearly support the novelty of our work both conceptually and technically. If AnonReviewer1 disagrees, we respectfully ask to consider addressing specific results.\n\nTheorem 1\n========\n\nWhat it shows: An expression for the effective depth of a deep ResNet, in the limit of infinite depth.\nClosest analogy in the literature: It has been argued that ResNets exhibit the properties of ensembles, however we are not aware of a similar mathematical analysis.\nTechnical novelty: This theorem presents a novel mathematical treatment of the effective depth of ResNets.\n\nTheorem 2\n========\n\nWhat it shows: Deep ResNets behave as ensembles concentrated around a narrow band near the maximum.\nClosest analogy in the literature: We are not aware of a similar claim.\nTechnical novelty: The proof of this claim is quite involved and is not trivial.\n\nTheorem 3\n========\n\nWhat it shows: The driving force behind the capacity increase of residual nets during training, when batch normalization is applied.\nClosest analogy in the literature: As far as we are aware, there is no other work which points out to this type dynamic behaviour in ResNets or elsewhere. \nTechnical novelty: We are not aware of any similar analysis. Nothing in the proof follows existing results.\n\nTheorem 4\n========\n\nWhat it shows: The driving force behind the capacity increase of residual nets during training, without batch normalization.\nClosest analogy in the literature: As far as we are aware, no similar analysis exists.\nTechnical novelty: We are not aware of any similar analysis.\n\nTheorem 5\n========\n\nWhat it shows: The loss surface of ensembles, in the context of general spin glass models. Here we use results of spin glass theory to demonstrate the landscape of the loss in ensembles, compared with single models, in terms of the number of critical points. \nClosest analogy in the literature: This comparison presents a novel viewpoint on ensembles, as far as we are aware.\nTechnical novelty: In this theorem we use the results of spin glass theory. Most of the technical heavy lifting for this specific theorem was done in Auffinger 2013\n\nLemma 1+2\n=========\n\nWhat is shown: The similarity between the loss of ResNets, and the hamiltonian of a general spin glass model.\nClosest analogy in the literature: Although a similar analysis was performed of traditional networks, this analogy presented additional technical difficulties, and is novel in its claim.\nTechnical novelty: Some technical aspects of this analogy are novel and not presented in the original work of Choromanska et al. especially Lemma 2.\n\nLemma 4\n========\n\nWhat it shows: The effective depth of ResNets can be controlled through weight scaling.\nClosest analogy in the literature: We are not aware of a similar claim.\nTechnical novelty: We are not aware of any similar analysis.\n", "title": "We need more details in order to be able to properly address the review"}, "r1xIto8Ee": {"type": "rebuttal", "replyto": "Sk6KdW8Nl", "comment": "We acknowledge that the assumption that paths are independent of the input is an unrealistic one. However, the assumption already appears as useful in promoting deep learning research in the literature and was used by Choromanska et al throughout their analysis.\n\nNote that we use this assumption selectively. Theorems 3 and 4, which can be considered as our main contributions, do not employ any unrealistic assumptions. ", "title": "path-independence assumptions"}, "Sk6KdW8Nl": {"type": "review", "replyto": "Sk8csP5ex", "review": "The path-independence assumption is not realistic. Do the authors have any ideas how these assumptions can be relaxed ?This paper shows how spin glass techniques that were introduced in Choromanska et al. to analyze surface loss of deep neural networks can be applied to deep residual networks. This is an interesting contribution but it seems to me that the results are too similar to the ones in Choromanska et al. and thus the novelty is seriously limited. Main theoretical techniques described in the paper were already introduced and main theoretical results mentioned there were in fact already proved. The authors also did not get rid of lots of assumptions from Choromanska et al. (path-independence, assumptions about weights distributions, etc.).", "title": "path-independence assumptions", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "B15BdW8Vx": {"type": "review", "replyto": "Sk8csP5ex", "review": "The path-independence assumption is not realistic. Do the authors have any ideas how these assumptions can be relaxed ?This paper shows how spin glass techniques that were introduced in Choromanska et al. to analyze surface loss of deep neural networks can be applied to deep residual networks. This is an interesting contribution but it seems to me that the results are too similar to the ones in Choromanska et al. and thus the novelty is seriously limited. Main theoretical techniques described in the paper were already introduced and main theoretical results mentioned there were in fact already proved. The authors also did not get rid of lots of assumptions from Choromanska et al. (path-independence, assumptions about weights distributions, etc.).", "title": "path-independence assumptions", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Hytj9ZVEe": {"type": "rebuttal", "replyto": "Sk8csP5ex", "comment": "We just revised our manuscript based on the very meaningful discussions we had with the reviewing team. For convenience, the changes are marked in red.\n\nIn addition to incorporating everything we have promised so far and adding discussion based on the reviewers\u2019 suggestions, we were able to relax the assumptions used for the results of Sec.  4. This change addresses a concern raised by AnonReviewer3 and was done by using the assumption-free expression for the output of the network in Eq. 9 in order to compute the loss in Lemma 3.\n\nOverall, believe that what can be considered as the main results of our manuscript, i.e., the dynamic behavior during the training of ResNets, is now essentially assumption free and is also well supported experimentally. We thank the reviewers for their crucial role in improving the manuscript.\n", "title": "Title: revised version"}, "ry7fzJmVl": {"type": "rebuttal", "replyto": "rkva93GNg", "comment": "Specifically for the proof of Lemma 2, \\beta is the multiplicity of each input value from the p-dimensional input vector x in the expression \\xi. We will change it to another symbol and clarify this point. The next version to be released very soon, will include these additional clarifications and we hope that the issue will thus be resolved.\n\nIn our framework, as you pointed out, the skipping connectiones affect the number of paths of a specific length. This number is manifested in the parameter gamma_r.  The architecture used in our analysis includes a skip connection per layer and gives rise to paths of every length between 1 the r. This simplifies the notation and was therefore beneficial in our analysis. Exactly the same analysis as in Sec. 3 can be performed with 2 layers or more per skipped block. The difference would change gamma_r while keeping everything else the same. \nThe results of Sec. 5 employ the general spherical spin glass model as presented and analyzed in  [Auffinger 2013]. This model includes every order of spin interaction between 1 and infinity, and thus fits the one skip per layer model, for large p. The extrapolation of Theorem 5 to the case of skipping a multi-layer block is technically possible, however it requires additional approximations.\n\nSec. 4, Theorems 3 and 4, as pointed out, studies each skip connection by itself. It can be applied to a block of arbitrary size. Since it holds for each block individually, it holds also for a network of multiple blocks. \n\n\n", "title": "Thank you for your review and for the very constructive comments."}, "BJRYkKW4l": {"type": "rebuttal", "replyto": "Sk8csP5ex", "comment": "We looked back at the comment of AnonReviewer3. We are the first to analyze the dynamic behavior of ResNets during training and point to a novel phenomenon, which we analyze theoretically. We reveal both the underlying reason for this phenomenon and its profound effect on the training process. However, we might not have demonstrated convincingly enough the existence of this phenomenon on conventional datasets. We therefore ran a few last minute experiments to show exactly this.\n\nWe took the ResNet code of https://github.com/facebook/fb.resnet.torch and trained on either CIFAR-10 or CIFAR-100 for networks of depth 32. As noted in our paper, the dynamic behavior can be present in the Batch Normalization gamma coefficient or in the weight matrices themselves\n\u201cthe mechanism for [the] dynamic property of residual networks can also be observed without the use of batch normalization, as a steady increase in the L2 norm of the weights, as shown in Fig. 1(e).\u201d\n\nIn the experiments we just ran, it seems that until the learning rate is reduced, the dynamic behavior is manifested in the Batch Normalization coefficient gamma and then it moves to the convolution layers themselves. For the following plots, we therefore absorb the gamma into the convolutional layer using https://github.com/e-lab/torch-toolbox/tree/master/BN-absorber. \n\nThere are two types of plots: one depicts the magnitude of the various convolutional layers for multiple epochs (similar in type to Fig. 1(d) in the paper); the second type plots the sum of these norms over all convolutional layers as a function of epoch (similar to Fig. 1(e)).\n\nCifar10 norm per layer http://imgur.com/X306qCt each graph is a different epoch, waving is due to the interleaving archtecture of the convolutional layers. \nCifar 10 mean norm per epoch http://imgur.com/PFFGlnv\nCifar 100 norm per layer http://imgur.com/lz462NJ each graph is a different epoch\nCifar 100 mean norm per epoch http://imgur.com/DBY0z68\n\nAs can be seen, the dynamic phenomenon we describe is very prominent in the public ResNet implementation when applied to conventional datasets: the dominance of paths with fewer skip connections increases over time. We find it very interesting that once the learning rate is reduced in epoch 81 the phenomenon we describe speeds up. \n\nBelow are the gamma values when not absorbed. The graphs report the norms of the gamma coefficient vectors. \n\nCifar 10 gamma norm per layer http://imgur.com/p1bb9BM each graph is a different epoch (since there is no monotonic increase between the epochs in this graph, it is harder to interpret)\nCifar 10 mean gamma norm per epoch http://imgur.com/LVB35UU\nCifar 100 gamma norm per layer http://imgur.com/tUWbRya each graph is a different epoch\nCifar 100 mean gamma norm per epoch http://imgur.com/qO9DFlj\n\nAs future work, we would like to better understand why the gammas start to decrease once the learning rate is reduced. As shown above, taking the magnitude of the convolutions into account the dynamic phenomenon we found actually becomes more prominent. The change of location from the gamma coefficient of the Batch Normalization layers to the convolutions themselves is a fascinating phenomenon by itself, which might indicate that Batch Normalization is no longer required at this point. Indeed, Batch Normalization enables larger training rates and this shift happens exactly when the training rate is reduced. A complete analysis is out of the scope of the current paper.\n\n[Please note that due to notation overloading, the Batch Normalization coefficient gamma is denoted in our paper as lambda, and following Choromanska et al (2015), gamma denotes the number of paths.]", "title": "More runs"}, "SkijR88Qg": {"type": "rebuttal", "replyto": "BJxBDnH7g", "comment": "In our setup we have skip connections skipping every layer but the first, and so the shortest path is when the input goes through the first layer and skips all the rest (path of length 1).", "title": "Thank you for your question"}, "BJxBDnH7g": {"type": "review", "replyto": "Sk8csP5ex", "review": "Hi, sorry for the delay!\n\nA question on equation 8 and gamma_r = \\binom{p-1}{r-1}n^r\n\nIt seems like the assumption is that there are p-1 skip connections skipping a single layer each. So if we are counting paths from input to output, surely there are no paths with length < p/2? (So the length sum should be from r=p/2 in equation 8?)Summary:\nIn this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.\n\nClarity:\nThis paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.\n\nSpecific Comments:\n- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)\n\n- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)\n\n- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?\n\n\n", "title": "Question on equation (8) gamma_r ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkva93GNg": {"type": "review", "replyto": "Sk8csP5ex", "review": "Hi, sorry for the delay!\n\nA question on equation 8 and gamma_r = \\binom{p-1}{r-1}n^r\n\nIt seems like the assumption is that there are p-1 skip connections skipping a single layer each. So if we are counting paths from input to output, surely there are no paths with length < p/2? (So the length sum should be from r=p/2 in equation 8?)Summary:\nIn this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.\n\nClarity:\nThis paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.\n\nSpecific Comments:\n- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)\n\n- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)\n\n- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?\n\n\n", "title": "Question on equation (8) gamma_r ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJen2DX7g": {"type": "rebuttal", "replyto": "BkJKRfZ7x", "comment": "\nThank you for your comments!\n\n1.In this work we use spin glass analysis in order to understand the dynamic behavior ResNets display during training and to study their loss surface. In particular, we use at one point or another the assumptions of redundancy in network parameters, near uniform distribution of network weights, independence between the inputs and the paths and independence between the different copies of the input as described in Choromanska(AISTATS 2015). The last two assumptions, i.e., the two independence assumptions, are deemed in COLT 2015 [1] as unrealistic, while the remaining are considered plausible. \n\n\nOur analysis of critical points in ensembles (Sec. 5) requires all of the above assumptions. However, theorems 1 and 2, as well as the analysis of the dynamic behaviour of residual nets (Sec. 4), which constitute our main contributions, do not assume the last assumption, i.e., the independence between the different copies of the input.\n\n\n2. The assumption that the expectation in Lemma 2 is the specified lower bound is realistic when applying the assumptions of redundancy and uniformity in network parameters. Indeed, given that Lambda=n it holds exactly, since all weight configurations of a particular length in Eq.9 will appear the same number of times. When Lambda is not a n, uniformity dictates that each configuration of weights would appear approximately equally regardless of the inputs, and the expectation values would be very close to the lower bound.\n\n\n3. Our work is theoretical and is centered around our analysis. We did present experimental evidence that the novel phenomenon we point to, i.e., the dynamic behavior of ResNets during training, is indeed real.\n\n\n4. Fig. 1(d) and 1(e)  report the experimental results of a straightforward setting. The task is to classify a mixture of 10 multivariate Gaussians in 50D. The input is therefore of size 50. The loss employed is the cross entropy loss of ten classes. The network has 10 blocks, each containing 20 hidden neurons, a batch normalization layer, and a skip connection. Training was performed on 10,000 samples, using SGD with minibatches of 50 samples. Note that Fig. 1(a-c), 1(f) are calculated and not estimated empirically.\n\n\n5. Our results are aligned with some of the results shown in the fractal net paper. In Sec. 4.3 there, the authors note empirically that the deepest column trains last. This is reminiscent of our claim that the deeper networks of the ensemble become more prominent as training progresses. The authors of FractalNets hypothesize  that this is a result of the shallower columns being stabilized at a certain point of the training process. In our work, we explicitly define the exact driving force that comes into play.\n\n\nIn addition, our work offers an insight into the mechanics of the recently proposed densely connected networks. Following the analysis we provide at Sec. 3, the additional shortcut paths decrease the initial capacity of the network by offering many more short paths from input to output, thereby contributing to the ease of optimization when training starts. The driving force mechanism described in Sec. 4.2 will then cause the effective capacity of the network to increase. \n\n\nNote that the analysis presented in Sec. 3 can be generalized to architectures with arbitrary skip connections, including dense nets. This is done directly by including all of the induced sub networks in Eq. 8. The reformulation of Eq. 9 would still holds, given that \\Psi_r is modified accordingly.", "title": "assumptions and claims"}}}