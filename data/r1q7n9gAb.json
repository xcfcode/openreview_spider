{"paper": {"title": "The Implicit Bias of Gradient Descent on Separable Data", "authors": ["Daniel Soudry", "Elad Hoffer", "Mor Shpigel Nacson", "Nathan Srebro"], "authorids": ["daniel.soudry@gmail.com", "elad.hoffer@gmail.com", "mor.shpigel@gmail.com", "nati@ttic.edu"], "summary": "The normalized solution of gradient descent on logistic regression (or a similarly decaying loss) slowly converges to the L2 max margin solution on separable data.", "abstract": "We show that gradient descent on an unregularized logistic regression\nproblem, for almost all separable datasets, converges to the same direction as the max-margin solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss. Furthermore,\nwe show this convergence is very slow, and only logarithmic in the\nconvergence of the loss itself. This can help explain the benefit\nof continuing to optimize the logistic or cross-entropy loss even\nafter the training error is zero and the training loss is extremely\nsmall, and, as we show, even if the validation loss increases. Our\nmethodology can also aid in understanding implicit regularization\nin more complex models and with other optimization methods. ", "keywords": ["gradient descent", "implicit regularization", "generalization", "margin", "logistic regression", "loss functions", "optimization", "exponential tail", "cross-entropy"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper is tackling an important open problem.\n\nAnonReviewer3 identified some technical issues that led them to rate the manuscript 5 (i.e., just below the acceptance threshold). Many of these issues are resolved by the reviewer in their review, and the author response makes it clear that these fixes are indeed correct.  However, other issues that the reviewer raises are not provided with solutions.  The authors address these points, but in one case at least (regarding w_infinity), I find the new text somewhat hand-waivy. Regardless, I'm inclined to accept the paper because the issues seem to be straightforward. Ultimately, the authors are responsible for the correctness of the results."}, "review": {"S1jezarxG": {"type": "review", "replyto": "r1q7n9gAb", "review": "The paper offers a formal proof that gradient descent on the logistic\nloss converges very slowly to the hard SVM solution in the case where\nthe data are linearly separable. This result should be viewed in the\ncontext of recent attempts at trying to understand the generalization\nability of neural networks, which have turned to trying to understand\nthe implicit regularization bias that comes from the choice of\noptimizer. Since we do not even understand the regularization bias of\noptimizers for the simpler case of linear models, I consider the paper's\ntopic very interesting and timely.\n\nThe overall discussion of the paper is well written, but on a more\ndetailed level the paper gives an unpolished impression, and has many\ntechnical issues. Although I suspect that most (or even all) of these\nissues can be resolved, they interfere with checking the correctness of\nthe results. Unfortunately, in its current state I therefore do not\nconsider the paper ready for publication.\n\n\nTechnical Issues:\n\nThe statement of Lemma 5 has a trivial part and for the other part the\nproof is incorrect: Let x_u = ||nabla L(w(u))||^2.\n  - Then the statement sum_{u=0}^t x_u < infinity is trivial, because\n    it follows directly from ||nabla L(w(u))||^2 < infinity for all u. I\n    would expect the intended statement to be sum_{u=0}^infinity x_u <\n    infinity, which actually follows from the proof of the lemma.\n  - The proof of the claim that t*x_t -> 0 is incorrect: sum_{u=0}^t x_u\n    < infinity does not in itself imply that t*x_t -> 0, as claimed. For\n    instance, we might have x_t = 1/i^2 when t=2^i for i = 1,2,... and\n    x_t = 0 for all other t.\n\nDefinition of tilde{w} in Theorem 4:\n  - Why would tilde{w} be unique? In particular, if the support vectors\n    do not span the space, because all data lie in the same\n    lower-dimensional hyperplane, then this is not the case.\n  - The KKT conditions do not rule out the case that \\hat{w}^top x_n =\n    1, but alpha_n = 0 (i.e. a support vector that touches the margin,\n    but does not exert force against it). Such n are then included in\n    cal{S}, but lead to problems in (2.7), because they would require\n    tilde{w}^top x_n = infinity, which is not possible.\n\nIn the proof of Lemma 6, case 2. at the bottom of p.14:\n  - After the first inequality, C_0^2 t^{-1.5 epsilon_+} should be \n    C_0^2 t^{-epsilon_+}\n  - After the second inequality the part between brackets is missing an\n    additional term C_0^2 t^{-\\epsilon_+}.\n  - In addition, the label (1) should be on the previous inequality and\n    it should be mentioned that e^{-x} <= 1-x+x^2 is applied for x >= 0\n    (otherwise it might be false).\nIn the proof of Lemma 6, case 2 in the middle of p.15:\n  - In the line of inequality (1) there is a t^{-epsilon_-} missing. In\n    the next line there is a factor t^{-epsilon_-} too much.\n  - In addition, the inequality e^x >= 1 + x holds for all x, so no need\n    to mention that x > 0.\n\nIn Lemma 1:\n  - claim (3) should be lim_{t \\to \\infty} w(t)^\\top x_n = infinity\n  - In the proof: w(t)^top x_n > 0 only holds for large enough t.\n\nRemarks:\n\np.4 The claim that \"we can expect the population (or test)\nmisclassification error of w(t) to improve\" because \"the margin of w(t)\nkeeps improving\" is worded a little too strongly, because it presumes\nthat the maximum margin solution will always have the best\ngeneralization error.\n\nIn the proof sketch (p.3):\n  - Why does the fact that the limit is dominated by gradients that are\n    a linear combination of support vectors imply that w_infinity will\n    also be a non-negative linear combination of support vectors?\n  - \"converges to some limit\". Mention that you call this limit\n    w_infinity\n\n\nMinor Issues:\n\nIn (2.4): add \"for all n\".\n\np.10, footnote: Shouldn't \"P_1 = X_s X_s^+\" be something like \"P_1 =\n(X_s^top X_s)^+\"?\n\nA.9: ell should be ell'\n\nThe paper needs a round of copy editing. For instance:\n  - top of p.4: \"where tilde{w} A is the unique\"\n  - p.10: \"the solution tilde{w} to TO eq. A.2\"\n  - p.10: \"might BOT be unique\"\n  - p.10: \"penrose-moorse pseudo inverse\" -> \"Moore-Penrose\n    pseudoinverse\"\n  \nIn the bibliography, Kingma and Ba is cited twice, with different years.\n", "title": "An interesting paper, but issues with correctness and presentation", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HyBrwGweG": {"type": "review", "replyto": "r1q7n9gAb", "review": "Paper focuses on characterising behaviour of the log loss minimisation on the linearly separable data. As we know, optimisation like this does not converge in a strict mathematical sense, as the norm of the model will grow to infinity. However, one can still hope for a convergence of normalised solution (or equivalently - convergence in term of separator angle, rather than parametrisation). This paper shows that indeed, log-loss (and some other similar losses), minimised with gradient descent, leads to convergence (in the above sense) to the max-margin solution. On one hand it is an interesting property of model we train in practice, and on the other - provides nice link between two separate learning theories.\n\nPros:\n- easy to follow line of argument\n- very interesting result of mapping \"solution\" of unregularised logistic regression (under gradient descent optimisation) onto hard max margin one\n\nCons:\n- it is not clear in the abstract, and beginning of the paper what \"convergence\" means, as in the strict sense logistic regression optimisation never converges on separable data. It would be beneficial for the clarity if authors define what they mean by convergence (normalised weight vector, angle, whichever path seems most natural) as early in the paper as possible.", "title": "Very interesting characterisation of limiting behaviour of the log-loss minimisaton", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkS9oWtef": {"type": "review", "replyto": "r1q7n9gAb", "review": "(a) Significance\nThe main contribution of this paper is to characterize the implicit bias introduced by gradient descent on separable data. The authors show the exact form of this bias (L_2 maximum margin separator), which is independent of the initialization and step size. The corresponding slow convergence rate explains the phenomenon that the predictor can continue to improve even when the training loss is already small. The result of this paper can inspire the study of the implicit bias introduced by gradient descent variants or other optimization methods, such as coordinate descent. In addition, the proposed analytic framework seems promising since it may be extended to analyze other models, like neural networks.\n\n(b) Originality\nThis is the first work to give the detailed characterizations of the implicit bias of gradient descent on separable data. The proposed assumptions are reasonable, but it seems to limit to the loss function with exponential tail. I\u2019m curious whether the result in this paper can be applied to other loss functions, such as hinge loss.\n\n(c) Clarity & Quality \nThe presentation of this paper is OK. However, there are some places can be improved in this paper. For example, in Lemma 1, results (3) and (4) can be combined together. It is better for the authors to use another section to illustrate experimental settings instead of writing them in the caption of Figure 3.1. \n\nMinor comments: \n1. In Lemma 1 (4), w^T(t)->w(t)^T\n2. In the proof of Lemma 1, it\u2019s better to use vector 0 for the gradient L(w)\n3. In Theorem 4, the authors should specify eta\n4. In appendix A, page 11, beta is double used\n5. In appendix D, equation (D.5) has an extra period\n", "title": "This paper analyzes the implicit regularization introduced by gradient descent for optimizing the smooth monotone exponential tailed loss function with separable data. The proposed result is very interesting since it illustrates that using gradient descent to minimize such loss function can lead to the L_2 maximum margin separator. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SymFQAPfz": {"type": "rebuttal", "replyto": "S1jezarxG", "comment": "We thank the reviewer for acknowledging the significance of our results, and for investing significant efforts in improving the quality of this manuscript. We uploaded a revised version in which all the reviewer comments were addressed, and the appendix was further polished. Notably,\n\n[Lemma 5 in appdendix]\n\n- Indeed, the upper limit of the sum over x_u should be 'infinity' instead of 't'.\n\n- It should be 'x_t -> 0', not 't*x_t -> 0'.\n\n[Definition of tilde{w} Theorem 4]\n\n- tilde{w} is indeed unique, given the initial conditions. We clarified this in Theorem 4 and its proof.\n\n- alpha_n=0 for the support vectors is only true for a measure zero of all datasets (we added a proof of this in appendix F). Thus, we clarified in the revision that our results hold for almost every dataset (and so, they are true with probability 1 for any data drawn from a continuous-valued distribution).\n\n[Why does the fact that the limit is dominated by gradients that are a linear combination of support vectors imply that w_infinity will also be a non-negative linear combination of support vectors?]\n\nWe clarified in the revision: \u201c...The negative gradient would then asymptotically become a non-negative linear combination of support vectors. The limit w_{\\infinity} will then be dominated by these gradients, since any initial conditions become negligible as ||w(t)||->infinity (from Lemma 1)\u201d.", "title": "Comments addressed in revision"}, "ByMhM0wGz": {"type": "rebuttal", "replyto": "HyBrwGweG", "comment": "We thank the reviewer for the positive review and for the helpful comment. We uploaded a revised version in which clarified in the abstract that the weights converge \u201cin direction\u201d to the L2 max margin solution.", "title": "Comment addressed in revision"}, "HJxIf0wGz": {"type": "rebuttal", "replyto": "HkS9oWtef", "comment": "We thank the reviewer for the positive review and for the helpful comments. We uploaded a revised version in which all the reviewer comments were addressed.\n\n[\u201cI\u2019m curious whether the result in this paper can be applied to other loss functions, such as hinge loss.\u201d]\n\nWe believe our results could be extended to many other types of loss functions (in fact, we are currently working on such extensions). However, for the hinge loss (without regularization), gradient descent on separable data can converge to a finite solution which is not to the max margin vector. For example, if there is a single data point x=(1,0), and we start with a weight vector w=(2,2), the hinge loss and its gradient are both equal to zero. Therefore, no weight updates are performed, and we do not converge to the direction of the L2 max margin classifier: w=(1,0).\n\n[\u201cIt is better for the authors to use another section to illustrate experimental settings instead of writing them in the caption of Figure 3.1. \u201c]\n\nWe felt it is easier to read if all details are summarized in the figure, and wanted to save space to fit the main paper into 8 pages. However, we can change this if required.", "title": "Comments addressed in revision"}}}