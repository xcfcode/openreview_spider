{"paper": {"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "authors": ["Jean Harb", "Doina Precup"], "authorids": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"], "summary": "Analyze the effects of using eligibility traces different optimizations in Deep Recurrent Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.  We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "keywords": ["Reinforcement Learning", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The reviewers agree that the paper is clear and well-written, but all reviewers raised significant concerns about the novelty of the work, since the proposed algorithm is a combination of well-known techniques in reinforcement learning. It is worth noting that the use of eligibility traces is not very heavily explored in the deep reinforcement learning literature, but since the contribution is primarily empirical rather than conceptual and algorithmic, there is a high bar for the rigorousness of the experiments. The reviewers generally did not find the evaluation to be compelling enough in this regard. Based on this evaluation, the paper is not ready for publication."}, "review": {"B1zgkh2mg": {"type": "rebuttal", "replyto": "Hkh8mMJXx", "comment": "1. We didn't tune the hyper-parameters, as such a search is quite expensive and would overfit to the specific games we trained on. As for the difference in parameters, RMSprop was kept consistent with DQN parameters, while the Adam parameters are simply the suggested parameters from the original paper.\n\n2. Indeed, we truncate the traces as in Watkins Q(\\lambda).\n\n3. We haven't tried comparing lambda-returns with n-step returns. It would be interesting to compare the effects of each method.", "title": "Parameter details"}, "By7g6ohQg": {"type": "rebuttal", "replyto": "By_SyRkml", "comment": "1. Lambda was not tuned. We selected 0.8 as it decays to a weight of 0.01 in just over 20 steps, allowing for a good truncated sequence length. For every new algorithm, a hyper-parameter search should be completed, but it's quite expensive to do so.\n\n2. We have not yet tested the algorithm on other games. This remains as future work.\n\n3. DQN runs at 1700 fps, while the RNN without traces runs at 560 fps and the RNN with traces runs at 500 fps. The code computing the traces could be further optimized to speed up the algorithm to speeds closer to the trace free model.\n\n4. This is kept consistent with DQN, where an epoch corresponds to 250k steps (actions) or 1M frames.", "title": "implementation details"}, "ryBSwoh7e": {"type": "rebuttal", "replyto": "Bk2DXskmx", "comment": "1.  If I understand correctly, you're asking for results on all Atari games. Unfortunately, it's simply too computationally demanding to perform training on all of them, right now. As such, it remains as future work.\n\n2. The architecture was kept as close as possible with DRQN, with the exception that we have the feed forward layer with 512 neurons, used in DQN, before feeding it into the RNN. The training was also slightly different as we used multiple frames as input to the RNN that aren't used for error, but only to fill the hidden state.", "title": "games and architecture"}, "By_SyRkml": {"type": "review", "replyto": "HJrDIpiee", "review": "1. Was the value of lambda tuned? How would other values of lambda compare?\n\n2. How does this method work on other Atari games? Are there any scenarios where elibility traces, recurrence, or Adam decreases performance?\n\n3. How does the wall-clock time compare between each of the three methods?\n\n4. How much experience (e.g. in terms of roll-outs or timesteps) does one epoch correspond to?This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.\n\nThe experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.", "title": "Some questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Sy6yFzzEe": {"type": "review", "replyto": "HJrDIpiee", "review": "1. Was the value of lambda tuned? How would other values of lambda compare?\n\n2. How does this method work on other Atari games? Are there any scenarios where elibility traces, recurrence, or Adam decreases performance?\n\n3. How does the wall-clock time compare between each of the three methods?\n\n4. How much experience (e.g. in terms of roll-outs or timesteps) does one epoch correspond to?This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.\n\nThe experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.", "title": "Some questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bk2DXskmx": {"type": "review", "replyto": "HJrDIpiee", "review": "1. Can you please show how the presented method compares with of the box state of the art for the Atari games?\n2. Can you please clarify the differences between the used architecture used and the in Hausknecht and Stone (2015), aside from adding the eligibility traces step?The paper presents a deep RL with eligibility traces. The authors combine DRQN with eligibility traces for improved training. The new algorithm is evaluated on a two problems, with a single set of hyper-parameters, and compared with DQN.\n\nThe topic is very interesting. Adding eligibility traces to RL updates is not novel, but this family of the algorithms have not been explored for deep RL. The paper is written clearly, and the related literature is well-covered. More experiments would make this promising paper much stronger. As this is an investigative, experimental paper, it is crucial for it to contain a wider range of problems, different hyper-parameter settings, and comparison with vanilla DRQN, Deepmind's DQN implementation, as well as other state of the art methods. ", "title": "Comparison with other methods", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Bya5vnbVg": {"type": "review", "replyto": "HJrDIpiee", "review": "1. Can you please show how the presented method compares with of the box state of the art for the Atari games?\n2. Can you please clarify the differences between the used architecture used and the in Hausknecht and Stone (2015), aside from adding the eligibility traces step?The paper presents a deep RL with eligibility traces. The authors combine DRQN with eligibility traces for improved training. The new algorithm is evaluated on a two problems, with a single set of hyper-parameters, and compared with DQN.\n\nThe topic is very interesting. Adding eligibility traces to RL updates is not novel, but this family of the algorithms have not been explored for deep RL. The paper is written clearly, and the related literature is well-covered. More experiments would make this promising paper much stronger. As this is an investigative, experimental paper, it is crucial for it to contain a wider range of problems, different hyper-parameter settings, and comparison with vanilla DRQN, Deepmind's DQN implementation, as well as other state of the art methods. ", "title": "Comparison with other methods", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Hkh8mMJXx": {"type": "review", "replyto": "HJrDIpiee", "review": "1) How did you select the hyperparameters for Adam and RMSprop? Did you tune them individually? It seems potentially odd to use a square gradient momentum of 0.95 in RMSprop and 0.999 in Adam since those hyperparameters are somewhat analogous.\n2) Do you truncate the eligibility traces as Watkin's Q(\\lambda) prescribes?\n2) Have you done any experiments to separate the effect of using n-step returns on faster credit assignment from combining different n-step returns using eligibility traces? The benefits of using n-step returns have already been shown both with and without the use of neural networks.This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016.", "title": "Questions", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryolYUbVe": {"type": "review", "replyto": "HJrDIpiee", "review": "1) How did you select the hyperparameters for Adam and RMSprop? Did you tune them individually? It seems potentially odd to use a square gradient momentum of 0.95 in RMSprop and 0.999 in Adam since those hyperparameters are somewhat analogous.\n2) Do you truncate the eligibility traces as Watkin's Q(\\lambda) prescribes?\n2) Have you done any experiments to separate the effect of using n-step returns on faster credit assignment from combining different n-step returns using eligibility traces? The benefits of using n-step returns have already been shown both with and without the use of neural networks.This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016.", "title": "Questions", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}