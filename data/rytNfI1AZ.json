{"paper": {"title": "Training wide residual networks for deployment using a single bit for each weight", "authors": ["Mark D. McDonnell"], "authorids": ["mark.mcdonnell@unisa.edu.au"], "summary": "We train wide residual networks that can be immediately deployed using only a single bit for each convolutional weight, with signficantly better accuracy than past methods.", "abstract": "For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware,  each learned weight parameter should ideally be represented and stored using a single bit.  Error-rates usually increase when this requirement is imposed. Here, we report large improvements in error rates on multiple datasets, for deep convolutional neural networks deployed with 1-bit-per-weight. Using wide residual networks as our main baseline, our approach simplifies existing methods that binarize weights by applying the sign function in training; we apply  scaling factors for each layer with constant unlearned values equal to the layer-specific standard deviations used for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with 1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve error rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We also considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test results of 0.27%, 1.9%, and 41.3% / 19.1%  respectively. For CIFAR, our error rates halve previously reported values, and are within about 1% of our error-rates for the same network with full-precision weights. For networks that overfit, we also show significant improvements in error rate by not learning batch normalization scale and offset parameters. This applies to both full precision and 1-bit-per-weight networks. Using a warm-restart learning-rate schedule, we found that training for 1-bit-per-weight is just as fast as full-precision networks, with better accuracy than standard schedules, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR-10/100. For full training code and trained models in MATLAB, Keras and PyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .", "keywords": ["wide residual networks", "model compression", "quantization", "1-bit weights"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper presents a way of training 1bit wide resnet to reduce the model footprint while maintaining good performance. The revisions added more comparisons and discussions, which make it much better. Overall, the committee feels this work will bring value to the conference."}, "review": {"ByDm13EBG": {"type": "rebuttal", "replyto": "SJ2dCsEHz", "comment": "Good suggestion - we will do that at the next opportunity for revision.", "title": "Re: thanks"}, "SkaoIrVHz": {"type": "rebuttal", "replyto": "HkvE8wI4G", "comment": "Regarding speedup on GPUs: we did all our work using the standard approach of using 32-bit GPU implementations to simulate the 1-bit case, in which case there's no speedup.  The reason is that custom GPU code needs to be written, presumably in cuda; we didn't need to do this to conduct our study.\n\nHowever, we found a paper submitted to this ICLR: \"Espresso: Efficient Forward Propagation for Binary Deep Neural Networks\" (https://openreview.net/forum?id=Sk6fD5yCb) that reports a 5x speed increase for optimized GPU code for binary networks applied to CIFAR.  ", "title": "Response to response to response"}, "BJyxkbFxz": {"type": "review", "replyto": "rytNfI1AZ", "review": "This paper introduces several ideas: scaling, warm-restarting learning rate, cutout augmentation. \n\nI would like to see detailed ablation studies: how the performance is influenced by the warm-restarting learning rates, how the performance is influenced by cutout. Is the scaling scheme helpful for existing single-bit algorithms?\n\nQuestion for Table 3: 1-bit WRN 20-10 (this paper) outperforms WRN 22-10 with the same #parameters on C100. I would like to see more explanations. \n", "title": "Mixed ideas", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkGtH2Kxf": {"type": "review", "replyto": "rytNfI1AZ", "review": "The authors propose to train neural networks with 1bit weights by storing and updating full precision weights in training, but using the reduced 1bit version of the network to compute predictions and gradients in training. They add a few tricks to keep the optimization numerically efficient. Since right now more and more neural networks are deployed to end users, the authors make an interesting contribution to a very relevant question.\n\nThe approach is precisely described although the text sometimes could be a bit clearer (for example, the text contains many important references to later sections).\n\nThe authors include a few other methods for comparision, but I think it would be very helpful to include also some methods that use a completely different approach to reduce the memory footprint. For example, weight pruning methods sometimes can give compression rates of around 100 while the 1bit methods by definition are limited to a compression rate of 32. Additionally, for practical applications, methods like weight pruning might be more promising since they reduce both the memory load and the computational load.\n\nSide mark: the manuscript has quite a few typos.\n", "title": "Solid work", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJ0pVRqxM": {"type": "review", "replyto": "rytNfI1AZ", "review": "The paper trains wide ResNets for 1-bit per weight deployment.\nThe experiments are conducted on CIFAR-10, CIFAR-100, SVHN and ImageNet32.\n\n+the paper reads well\n+the reported performance is compelling \n\nPerhaps the authors should make it clear in the abstract by replacing:\n\"Here, we report methodological innovations that result in large reductions in error rates across multiple datasets for deep convolutional neural networks deployed using a single bit for each weight\"\nwith\n\"Here, we report methodological innovations that result in large reductions in error rates across multiple datasets for wide ResNets deployed using a single bit for each weight\"\n\nI am curious how the proposed approach compares with SqueezeNet (Iandola et al.,2016) in performance and memory savings.\n\n", "title": "a single bit for each weight", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rymXcljQz": {"type": "rebuttal", "replyto": "rytNfI1AZ", "comment": "1. Note first that changes made in response to specific comments from reviewers are written in our response to each reviewer. In summary, significant changes in this regard include:\n1a. the addition of a figure for an ablation study, as requested by a reviewer\n1b. the addition of a section comparing our work to SqueezeNet in the Discussion, as requested by a reviewer\n1c. more emphasis on our improvements to full-precision training, by not learning batch-norm parameters\n\n2.  We also updated all results, finished experiments on ImageNet, updating the abstract accordingly.", "title": "Summary of changes to paper following review"}, "rkAgKpHQG": {"type": "rebuttal", "replyto": "BJyxkbFxz", "comment": "Thankyou for your comments and questions.\n\n***\n\nReviewer Comment: \u201cI would like to see detailed ablation studies: how the performance is influenced by the warm-restarting learning rates, how the performance is influenced by cutout\u201d\n\nAuthor Response: \n\nWe already separated out the influence of cutout in the original submission. We only used cutout for CIFAR 10/100, and Table 1 showed separate columns for results without cutout (indicated by superscript +) and those with cutout (indicated by superscript ++).  Figure 5 (right panel) shows how the use of cutout influences convergence.\n\nWe did not originally provide comparisons with and without warm-restart, because the benefits of the warm-restart method (both faster convergence and better accuracy) for CIFAR 10/100 has already been established by Loshchilov and Hutter (2016) who compared the approach with a more typical schedule with a learning rate of 0.1/0.01/0.001 for 80/80/80 epochs. However, the reviewer has a point that the comparison has not previously been done for the case of single-bit-weights and hence we have now conducted some experiments.\n\nAuthor actions:\n\n1.\tWe have added a section in Results called \u201cAblation Studies\u201d and included a new figure for CIFAR-100. The figure highlights that the warm-restart method does not provide a significant accuracy benefit for the full-precision case but does in the single-bit-weights case. The figure also shows a comparison of learning and not learning the batch-norm offsets and gains, in response to another question by this Reviewer, responded to below.\n\n***\n\nReviewer question: \u201cIs the scaling scheme helpful for existing single-bit algorithms? \n\nAuthor Response: Our Section 2.1 describes how our approach builds on and enables the improvement of existing single-bit algorithms. Our new Section 4.1 shows how our use of warm-restart accelerates convergence, and provides  best accuracy, especially for CIFAR-10.  Our Section 5.2 discusses the specific case of how our method compares with Rastegari et al (2016).\n\nAuthor Action: we have added Section 4.1 and revised Section 5.2.\n\n***\n\nReviewer question: \u201cQuestion for Table 3: 1-bit WRN 20-10 (this paper) outperforms WRN 22-10 with the same #parameters on C100. I would like to see more explanations.\u201d\n\nAuthor response: In this initial submission, we only explained this in general terms in Section 5.3 as being a result of our approach described in 3.2.1. So we agree that the reviewer has a point. We did not highlight this aspect very much in the original submission, nor explain it in the specific cases tabulated, as we wanted the central emphasis to be on our single-bit-weight results. However, on reflection, improvements to the baseline approach are surely of interest to the community and worth emphasis.\n\nFor the specific case mentioned by the Reviewer, we remark that our 20-10 network is essentially the same as the 22-10 comparison network, where the extra 2 conv layers appear due to the use of learnt 1x1 convolutional projections in downsampling residual paths, whereas we use average pooling instead.\n\nTo directly answer the question, there is one single factor that enabled us to significantly lower the error rate for the width-10 wide ResNet architecture for CIFAR, which is that we turn off the learning of the batch norm parameters, as we found this reduces overfitting. \n\nAuthor actions:\n\n1.\tWe have now highlighted this contribution in the abstract.\n2.\tWe have now highlighted the specific case mentioned in the Discussion in Section 5.3.\n3.\tWe have added results in a new Section (\u201cAblation studies\u201d) that show how the test error rate changes through training with and without learning of the batch-norm scale and offset.\n", "title": "Response to AnonReviewer1"}, "rkzl-zvXG": {"type": "rebuttal", "replyto": "SkGtH2Kxf", "comment": "Thankyou for your comments.\n\n***\n\nReviewer Comment:  \u201ccould be a bit clearer\u2026 the text contains many important references to later sections\u201d\n\nAuthor Action: We have edited the text to improve this aspect.\n\n***\n\nReviewer Comment: \u201cI think it would be very helpful to include also some methods that use a completely different approach to reduce the memory footprint\u2026. for practical applications, methods like weight pruning might be more promising since they reduce both the memory load and the computational load\u201d\n\nAuthor Response: \n\nAs well as reducing model size, our approach is strongly motivated by significantly reducing computational load by a different approach to reducing parameter number. The key point is that performing convolutions using 1-bit weights can be implemented using adders rather than multipliers. Removing the need for multipliers offers enormous benefits in terms of chip size, speed and power consumption in custom digital hardware implementations of trained networks, and also offers substantial speedups even if implemented on GPUs. This has been demonstrated by Rastegari et al in \u201c XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\u201d (arxiv: 1603.05279, 2016).\n\nExisting pruning methods do not automatically offer the opportunity to avoid use of multiplications, since the un-pruned parameters are learned using full precision. It remains an open question beyond the scope of the current submission to determine whether pruning can be successfully applied to 1-bit models like ours to in turn reduce the number of parameters.\n\nQuestion to Reviewer:  we have been unable to find methods that reduce the size of all-convolutional networks by a factor of 100. This magnitude of reduction is, to our knowledge, only available in networks with very large fully-connected layers, such as AlexNet and VGGnet. For example, one submission to ICLR 2018 \u201cTo Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression\u201d uses pruning applied to an Inception network that reduces the number of non-zero parameters from 27M to 3M, which is a factor of 9x. Can you please clarify if you know of papers that achieve this scale of pruning in all-convolutional  networks such as ResNets?\n\nAuthor Action: we have added comments strengthening our motivation of reducing the use of multiplications, and added to our discussion of pruning in the prior work and discussion.\n\n***\n\nReviewer Comment: \u201cthe manuscript has quite a few typos..\u201d\n\nAuthor Action: we have carefully reviewed the entire manuscript and corrected typos.\n", "title": "Response to AnonReviewer2"}, "ryrbVpBmG": {"type": "rebuttal", "replyto": "HJ0pVRqxM", "comment": "Thankyou for your comments. \n\n***\n\nReviewer comment: \"+the reported performance is compelling\": \n\nAuthor Response: To reinforce this aspect, since initial submission we have found the following ways to surpass the performance we initially reported: \n\n1. We now have conducted experiments on the full Imagenet dataset and have surpassed all previously published results for a single-bit per weight. Indeed, we provide the first report, to our knowledge, of a top-5 error rate under 10% for this case.\n2. For Imagenet32, we realised that the weight decay we used was set to the larger CIFAR value of 0.0005. We repeated our experiments with the usual Imagenet value of 0.0001 and achieved improved results.\n3. For experiments on CIFAR with cutout, we realised our previous experiments did not uniformly sample all pixels for cutout; after fixing we achieved further reduced error rates. \n4. We have also completed experiments with CIFAR 10/100 for ResNets with depth 26. We found the extra layers provided no benefit for the full-precision case, but a small advantage in the single-bit case.\n\nAuthor Actions: We have updated the results tables in the revised manuscript, modified our descriptions of the use of CutOut, clarified our weight-decay values, and added comments in the Discussion section comparing aspects of the enhanced results.\n\n***\n\nReviewer Comment: \u201cPerhaps the authors should make it clear in the abstract\u2026\u201d\n\nAuthor Response: You have a point that our experiments in the main text were all on wide ResNets. This followed from our strategy to commence with a near state-of-the-art baseline. However, our training approach is general and not specific to ResNets. For example, we provided some results for all-conv-nets in the Appendix B on the final page.\n\nAuthor Actions: To improve clarity as suggested, we have added the phrase \"Using depth-20 wide residual networks as our main baseline\" to our revised manuscript, but have retained the term \"deep convolutional neural networks.\"\n\n***\n\nReviewer Comment: \u201cI am curious how the proposed approach compares with SqueezeNet (Iandola et al.,2016) in performance and memory savings. \u201c\n\nAuthor Response: The Squeezenet paper focuses on memory savings relative to AlexNet. It uses two strategies to produce a memory-saving smaller model than an AlexNet: (1) replacing many 3x3 kernels with 1x1 kernels; (2) deep compression.\n\nRegarding Squeezenet memory-saving strategy (1), we note that SqueezeNet is an all-convolutional network. We tried our single-bit-weights approach in many all-convolutional variants (e.g. plain all-conv, Squeezenet, MobileNet, ResNeXt) and found its effectiveness relative to full-precision baselines to be comparable for all variants. We also observed in many experiments that the total number of learnt parameters correlates very well with classification accuracy. When we applied a SqueezeNet variant to CIFAR-100, we found that to obtain the same accuracy as our ResNets, we had to increase the \"width\" until the SqueezeNet had approximately the same number of learnt parameters as the ResNet. We conclude that our method therefore reduces the model size of the baseline SqueezeNet architecture (i.e. when no deep compression is used) by a factor of 32, albeit with an accuracy gap.\n\nRegarding SqueezeNet memory-saving strategy (2), the SqueezeNet paper reports that Deep Compression reduces the model size by approximately a factor of 10 with no accuracy loss. Our method reduces the same model size by a factor of 32, but with a small accuracy loss that typically becomes larger as the full-precision accuracy gets smaller. It would certainly be interesting to explore whether Deep-Compression might be applied to our 1-bit models, but our own focus is on methods that minimally alter training, and we leave investigation of more complex methods for future work.\n\nRegarding SqueezeNet performance, the best accuracy reported in the SqueezenNet paper is 39.6% top-1 error, requiring 4.8MB for the model\u2019s weights. Our single-bit-weight models achieve better than 33% top-1 error, and require 8.3 MB for the model\u2019s weights. \n\nAuthor Actions: We added these comments to a new subsection in the Discussion section of our paper.", "title": "response to AnonReviewer3"}}}