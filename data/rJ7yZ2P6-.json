{"paper": {"title": "Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus", "authors": ["JIANXIONG DONG", "Jim Huang"], "authorids": ["jdongca2003@gmail.com", "ccjimhuang@gmail.com"], "summary": "Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue", "abstract": "Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ", "keywords": ["next utterance selection", "ubuntu dialogue corpus", "out-of-vocabulary", "word representation"]}, "meta": {"decision": "Reject", "comment": "This paper's idea is to augment pre-trained word embeddings on a large corpus with embeddings learned on the data of interest. This is shown to yield better results than the pre-trained word embeddings alone. This contribution is too limited to justify publication at iclr."}, "review": {"rJdjmmLez": {"type": "review", "replyto": "rJ7yZ2P6-", "review": "Summary:\nThis paper proposes an approach to improve the out-of-vocabulary embedding prediction for the task of modeling dialogue conversations. The proposed approach uses generic embeddings and combines them with the embeddings trained on the training dataset in a straightforward string-matching algorithm. In addition, the paper also makes a couple of improvements to Chen et. al's enhanced LSTM by adding character-level embeddings and replacing average pooling by LSTM last state summary vector. The results are shown on the standard Ubuntu dialogue dataset as well as a new Douban conversation dataset. The proposed approach gives sizable gains over the baselines.\n\n\nComments:\n\nThe paper is well written and puts itself nicely in context of previous work. Though, the proposed extension to handle out-of-vocabulary items is a simple and straightforward string matching algorithm, but nonetheless it gives noticeable increase in empirical performance on both the tasks. All in all, the methodological novelty of the paper is small but it has high practical relevance in terms of giving improved accuracy on an important task of dialogue conversation.", "title": "Good paper with important practical and engineering relevance. Little methodological novelty, though.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkomChuxf": {"type": "review", "replyto": "rJ7yZ2P6-", "review": "The paper considers a setting (Ubuntu Dialogue Corpus and Douban Conversation Corpus) where most word types in the data are not covered by pretrained representations. The proposed solution is to combine (1) external pretrained word embeddings and (2) pretrained word embeddings on the training data by keeping them as two views: use the view if it's available, otherwise use a zero vector. This scheme is shown to perform well compared to other methods, specifically combinations of pretraining vs not pretraining embeddings on the training data, updating vs not updating embeddings during training, and others. \n\nQuality: Low. The research is not very well modularized: the addressed problem has nothing specifically to do with ESIM and dialogue response classification, but it's all tangled up. The proposed solution is reasonable but rather minor. Given that the model will learn task-specific word representations on the training set anyway, it's not clear how important it is to follow this procedure, though minor improvement is reported (Table 5). \n\nClarity: The writing is clear. But the point of the paper is not immediately obvious because of its failure to modularize its contributions (see above).\n\nOriginality: Low to minor.\n\nSignificance: It's not convincing that an incremental improvement in the pretraining phase is so significant, for instance compared to developing a novel better architecture actually tailored to the dialogue task. ", "title": "A minor solution to resolving OOV word representations", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1RMVeqgz": {"type": "review", "replyto": "rJ7yZ2P6-", "review": "The main contributions in this paper are:\n1) New variants of a recent LSTM-based model (\"ESIM\") are applied to the task of response-selection in dialogue modeling -- ESIM was originally introduced and evaluated for natural language inference. In this new setting, the ESIM model (vanilla and extended) outperform previous models when trained and evaluated on two distinct conversational datasets.\n\n2) A fairly trivial method is proposed to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to these conversational datasets.\nThe method itself is to combine d1-dimensional word embeddings that were pretrained on a large unannotated corpus (vocabulary S) with distinct d2-dimensional word embeddings that are trained on the task-specific training data (vocabulary T). The enhanced (d1+d2)-dimensional representation for a word is constructed by concatenating its vectors from the two embeddings, setting either the d1- or d2-dimensional subvector to zeros when the word is absent from either S or T, respectively. This method is incorporated as an extension into ESIM and evaluated on the two conversation datasets.\n\nThe main results can be characterized as showing that this vocabulary extension method leads to performance gains on two datasets, on top of an ESIM-model extended with character-based word embeddings, which itself outperforms the vanilla ESIM model.\n\nThese empirical results are potentially meaningful and could justify reporting, but the paper's organization is very confusing, and too many details are too unclear, leading to low confidence in reproducibility. \n\nThere is basic novelty in applying the base model to a new task, and the analysis of the role of the special conversational boundary tokens is interesting and can help to inform future modeling choices. The embedding-enhancing method has low originality but is effective on this particular combination of model architecture, task and datasets. I am left wondering how well it might generalize to other models or tasks, since the problem it addresses shows up in many other places too...\n\nOverall, the presentation switches back and forth between the Douban corpus and the Ubuntu corpus, and between word2vec and Glove embeddings, and this makes it very challenging to understand the details fully.\n\nS3.1 - Word representation layer: This paragraph should probably mention that the character-composed embeddings are newly introduced here, and were not part of the original formulation of ESIM. That statement is currently hidden in the figure caption.\n\nAlgorithm 1:\n- What set does P denote, and what is the set-theoretic relation between P and T?\n- Under one possible interpretation, there may be items in P that are in neither T nor S, yet the algorithm does not define embeddings for those items even though its output is described as \"a dictionary with word embeddings ... for P\". This does not seem consistent? I think the sentence in S4.2 about initializing remaining OOV words as zeros is relevant and wonder if it should form part of the algorithm description?\n\nS4.1 - What do the authors mean by the statement that response candidates for the Douban corpus were \"collected by Lucene retrieval model\"?\n\nS4.2 - Paragraph two is very unclear. In particular, I don't understand the role of the Glove vectors here when Algorithm 1 is used, since the authors refer to word2vec vectors later in this paragraph and also in the Algorithm description.\n\nS4.3 - It's insufficiently clear what the model definitions are for the Douban corpus. Is there still a character-based LSTM involved, or does FastText make it unnecessary?\n\nS4.3 - \"It can be seen from table 3 that the original ESIM did not perform well without character embedding.\" This is a curious way to describe the result, when, in fact, the ESIM model in table 3 already outperforms all the previous models listed.\n\nS4.4 - gensim package -- for the benefit of readers unfamiliar with gensim, the text should ideally state explicitly that it is used to create the *word2vec* embeddings, instead of the ambiguous \"word embeddings\".\n\n", "title": "Promising results but insufficient clarity and focus in write-up", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1RKt4ffM": {"type": "rebuttal", "replyto": "ryX6HZzzM", "comment": "For reference and result reproducibility (ESIM^a in Table 3 in the paper),  I pasted the logs of performance evaluation on the validation every 1000 steps during the training.  It took about 13 hours 41 minutes to reach 23000 training steps.\n\nstep: 1000\nMAP (mean average precision: 0.735673771383\tMRR (mean reciprocal rank): 0.735673771383\tTop-1 precision: 0.607566462168\tNum_query: 19560\n\nStep: 2000\nMAP (mean average precision: 0.762894553186\tMRR (mean reciprocal rank): 0.762894553186\tTop-1 precision: 0.643149284254\tNum_query: 19560\n\nStep: 3000\nMAP (mean average precision: 0.781005473594\tMRR (mean reciprocal rank): 0.781005473594\tTop-1 precision: 0.666462167689\tNum_query: 19560\n\nStep: 4000\nMAP (mean average precision: 0.791324840945\tMRR (mean reciprocal rank): 0.791324840945\tTop-1 precision: 0.679396728016\tNum_query: 19560\n\nStep: 5000\nMAP (mean average precision: 0.793004146785\tMRR (mean reciprocal rank): 0.793004146785\tTop-1 precision: 0.680112474438\tNum_query: 19560\n\nStep: 6000\nMAP (mean average precision: 0.806250669491\tMRR (mean reciprocal rank): 0.806250669491\tTop-1 precision: 0.698108384458\tNum_query: 19560\n\n....\nStep: 9000\nMAP (mean average precision: 0.819590433992\tMRR (mean reciprocal rank): 0.819590433992\tTop-1 precision: 0.717791411043\tNum_query: 19560\n\nStep: 10000\nMAP (mean average precision: 0.818069269971\tMRR (mean reciprocal rank): 0.818069269971\tTop-1 precision: 0.714008179959\tNum_query: 19560\n\nStep: 11000\nMAP (mean average precision: 0.818855596942\tMRR (mean reciprocal rank): 0.818855596942\tTop-1 precision: 0.714979550102\tNum_query: 19560\n\nStep: 12000\nMAP (mean average precision: 0.821677885708\tMRR (mean reciprocal rank): 0.821677885708\tTop-1 precision: 0.719325153374\tNum_query: 19560\n\nStep: 13000\nMAP (mean average precision: 0.8232087472\tMRR (mean reciprocal rank): 0.8232087472\tTop-1 precision: 0.721523517382\tNum_query: 19560\n\nStep: 14000\nMAP (mean average precision: 0.825161326971\tMRR (mean reciprocal rank): 0.825161326971\tTop-1 precision: 0.724948875256\tNum_query: 19560\n\nStep: 15000\nMAP (mean average precision: 0.825991109975\tMRR (mean reciprocal rank): 0.825991109975\tTop-1 precision: 0.725051124744\tNum_query: 19560\n\nStep: 16000\nMAP (mean average precision: 0.824983891648\tMRR (mean reciprocal rank): 0.824983891648\tTop-1 precision: 0.722750511247\tNum_query: 19560\n\nStep: 17000\nMAP (mean average precision: 0.827094653812\tMRR (mean reciprocal rank): 0.827094653812\tTop-1 precision: 0.727198364008\tNum_query: 19560\n\nStep: 18000\nMAP (mean average precision: 0.829552151297\tMRR (mean reciprocal rank): 0.829552151297\tTop-1 precision: 0.730981595092\tNum_query: 19560\n\nStep: 19000\nMAP (mean average precision: 0.830157512903\tMRR (mean reciprocal rank): 0.830157512903\tTop-1 precision: 0.73200408998\tNum_query: 19560\n\nStep: 20000\nMAP (mean average precision: 0.82902826468\tMRR (mean reciprocal rank): 0.82902826468\tTop-1 precision: 0.729703476483\tNum_query: 19560\n\nStep: 21000\nMAP (mean average precision: 0.832002669848\tMRR (mean reciprocal rank): 0.832002669848\tTop-1 precision: 0.734918200409\tNum_query: 19560\n\nStep: 22000\nMAP (mean average precision: 0.830050982731\tMRR (mean reciprocal rank): 0.830050982731\tTop-1 precision: 0.731339468303\tNum_query: 19560\n\nStep: 23000\nMAP (mean average precision: 0.832678571429\tMRR (mean reciprocal rank): 0.832678571429\tTop-1 precision: 0.735736196319\tNum_query: 19560\n\nStep: 24000\nMAP (mean average precision: 0.828641116467\tMRR (mean reciprocal rank): 0.828641116467\tTop-1 precision: 0.728936605317\tNum_query: 19560\n\nStep: 25000\nMAP (mean average precision: 0.826601259454\tMRR (mean reciprocal rank): 0.826601259454\tTop-1 precision: 0.725766871166\tNum_query: 19560\n", "title": "Reply to \"Reproducibility Summary\""}, "BJzBG4Gfz": {"type": "rebuttal", "replyto": "ryX6HZzzM", "comment": "Thank Hugo et al very much for reproducing the results. \n\n> The paper does not detail the computing infrastructure that was used.\nLocal machine :  Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz  * 2\n                               RAM: 32G    ( 8  * 4G (DDR4, 2133 MHz)\n                               One GPU : nvidia P5000 (16 G GPU RAM)\n\nYou used Telsa K80 (with 24G GPU RAM). I have not compared the performance between P5000 and Tesla K80.\n\n> Accuracy and cost over the validation set and over a subset of the training set were employed to evaluate the training of the model.\n\nIn our experiments,  we evaluated the accuracy,  MRR, P@1 on the validation set every 1000 steps and saved the model with the highest MRR.  In your code, you saved the model with the best accuracy on the validation set every 50 steps. \nMy suggestion:  \n    1) use MRR\n    2) perform the evaluation on the validation set every K steps (K could be larger to reduce the computational cost since evaluation on the validation set is slow). This will help you speed up the training.\n\n> In training the character embeddings using Word2Vec,\n>we used all the default hyperparameters, and trained each\n> context/response as distinct inputs such that each context/response\n>pair takes one line in the input data file.\n\nI assume that there is a typo here.  'character embedding' may be 'word embedding'.\nIn our algorithm 1, we used Word2vec to generate word embedding on the training set and concatenated them with pre-built GloVe vectors. Character Embedding is used in our ESIM. Since you only evaluated the baseline ESIM model, character embedding would not be used.\n\n>  the training of character-composed embeddings is briefly described only as the concatenation of final state vectors at the BiLSTM.\nThe implementation of character embedding was showed in my first comment. It is relatively easy to integrate them into your code (see: tf_esim.py  Line 43 and Line 44). Character-embedding may consume more memory.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "title": "Reply to \"Reproducibility Summary\""}, "r1lgG9n-z": {"type": "rebuttal", "replyto": "BkomChuxf", "comment": "Thank for your valuable feedback.  \n\n> the addressed problem has nothing specifically to do with ESIM and dialogue response classification, but it's all tangled up. The proposed solution is reasonable but rather minor. \n\nIn order to check whether the effectiveness of the proposed enhanced representation depends on ESIM model and dataset, I uploaded a revision (12/11/2017) to use a very simple model (represent contexts/responses by a simple average of word vectors).  I evaluated it on Ubuntu, Douban and WikiQA datasets.  The results on the enhanced representation are still better on the above three datasets.  This may indicate that the enhanced vectors may fuse domain-specific info into pre-built vectors. Also this process is unsupervised.\n\nSee section \"4.5 EVALUATION OF ENHANCED REPRESENTATION ON A SIMPLE MODEL\"\n\n\n\n\n", "title": "Reply to \"a minor solution to resolving OOV word representations\""}, "HJAQJq2Wf": {"type": "rebuttal", "replyto": "SJEsevIZf", "comment": "I uploaded the revision on 12/11/2017 to address whether  the effectiveness of the proposed enhanced representation depends on ESIM model and datasets.\n\nI added a section \"4.5 EVALUATION OF ENHANCED REPRESENTATION ON A SIMPLE MODEL\". Here I used  a very simple model : represent contexts (or responses) by a simple average of word vectors. Cosine-similarity is used to rank candidate responses.  The results on the enhanced vectors are still better. I also tested it on WikiQA dataset.", "title": "Reply to \"Promising results but insufficient clarity and focus in write-up\""}, "rJswuIPbG": {"type": "rebuttal", "replyto": "HJAg9rDZM", "comment": "> 1. We used stanford CoreNLP's library \nWe wrote a java program based on CoreNLP library to perform PTBTokenizer, other than command-line interface (CLI). For CLI, it is not easy to create input-output correspondence.\nSee java API example (https://stanfordnlp.github.io/CoreNLP/api.html)\nProperties props = new Properties();\nprops.put(\"annotators\", \"tokenize, ssplit, lemma\"}\n\n> Regarding word2vec, did you use any non-default hyperparameters? \nuse the default. Iter=20\n> did you train contexts and responses as distinct inputs or concatenate the context-response pairs to train?\ndistinct inputs. Each context/response takes one line in the input data file.\n\n\n\n\n\n", "title": "Reply to \"Further clarification\""}, "SJEsevIZf": {"type": "rebuttal", "replyto": "B1BaLmQWz", "comment": "I uploaded a new revision on Dec. 6.\nOn Table 5,  added performance comparison with FastText vectors.\n\nUsed the fixed pre-built FastText vectors ( https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip) where word vectors for out-of-vocabulary words were computed based on built model.\nThat is,\nall_words_on_ubtuntu_dataset|./fasttext  print-word-vectors wiki.en.bin > ubuntu_fastText_word_vectors.txt\n(see: https://github.com/facebookresearch/fastText)\n\nThe performance of the proposed method is better.", "title": "Reply to \"\"Promising results but insufficient clarity and focus in write-up\""}, "B1BaLmQWz": {"type": "rebuttal", "replyto": "H1RMVeqgz", "comment": "Thank for your feedback.  I have uploaded a new revision based on your suggestions.\n\n>  The embedding-enhancing method has low originality but is effective on this particular combination of model architecture, task and datasets. I am left wondering how well it might generalize to other models or tasks, since the problem it addresses shows up in many other places too...\n\nGood point. I will test embedding-enhanced method on other benchmark set/task to check whether it is still effective. I will report results here.\n\n> S3.1 - Word representation layer: This paragraph should probably mention that the character-composed embeddings are newly introduced here\nI updated it in revised version based on your advice.\n\n> What set does P denote, and what is the set-theoretic relation between P and T?\nP: all words in training/validation/testing sets (number of unique words could be large)\nT:  words with word2vec embedding on the training set.  T is a subset of P.  Word2vec also uses word document frequency to remove some low frequency words.\n\nIn the revised version,  I change output  to \" dimension d1 + d2 for (S\\cap P) \\cup T\" and added notes \"The remaining words which are in P and not in the above output dictionary are initialized with zero vectors\".  Here we did not store word with zero vector in the above dictionary to save space in the output dictionary. This initialization is usually done during neural network initialization stage.\n\n> S4.1 - What do the authors mean by the statement that response candidates for the Douban corpus were \"collected by Lucene retrieval model\"?\nBased on your advice, I added the following sentences in the revised paper\n\"That is, the last turn of each Douban dialogue with additional keywords extracted from the context on the test set was used as query to retrieve 10 response candidates from the Lucene index set (Details are referred to section 4 in (Wu et al., 2017)).\" \n\nDouban data was created by Wu et al., not by us (paper: https://arxiv.org/pdf/1612.01627.pdf, \nSee section 4: Response Candidate retrieval and Section 5.2 Douban Conversation Corpus). On this dataset,  response negative candidates on the training/validation sets were random sampled whereas the retrieved method was used for testing set. \n\n> S4.2 - Paragraph two is very unclear. In particular, I don't understand the role of the Glove vectors here when Algorithm 1 is used, since the authors refer to word2vec vectors later in this paragraph and also in the Algorithm description.\n\nHere GloVe vectors are just pre-trainined word embedding ones from a general large dataset.\n\nFor the clarification,  I added the following sentence  in Section 3.2\n\"Here the pre-trainined word vectors can be from known methods such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013) and FastText (Bojanowski et al., 2016).\".\n\nOn the training set we used word2vec in Algorithm 1 though other methods (GloVe and FastText) can be used too. \n\n> S4.3 - It's insufficiently clear what the model definitions are for the Douban corpus. Is there still a character-based LSTM involved, \nI used the same model layout and hyper-parameters for Douban and Ubuntu corpus.  In Section 4.2 \n\"The same hyper-parameter settings are applied to both Ubuntu Dialogue and Douban conversation corpus.\"\n\nOnly the differences are pre-trained embedding vectors and word2vec generated on the training sets.   Wu et al's Douban dataset (Chinese) have been already tokenized so that it is easy for us to run word2vec based on gensim. \n\n> does FastText make it unnecessary?\nFor western languages such as English, Germany, FastText generates ngram (character) internal embeddings and are used to address out-of-vocabulary issue.  For OOV (a word is out of FastText pre-trained embeddings), we can use average of word ngram to obtain its representation. For Ubuntu corpus, I can test it if you think that it is useful.\nFor Douban, it is not easy for us to do it since dataset has been tokenized by Chinese tokenizer.\n\n> S4.3 - \"It can be seen from table 3 that the original ESIM did not perform well without character embedding.\" \nThanks.  I changed it to \"\nIt can be seen from table 3 that character embedding enhances the performance of original ESIM.\"\n\"\n\n> S4.4 - gensim package -- for the benefit of readers unfamiliar with gensim, the text should ideally state explicitly that it is used to create the *word2vec* embeddings, \nI updated it in revised version based on your advice.\n\n\n\n\n\n\n\n\n\n\n\n\n", "title": "Reply to \"Promising results but insufficient clarity and focus in write-up\""}, "BkpgMaheG": {"type": "rebuttal", "replyto": "B15ZQj2gf", "comment": "Hi, Alex,\n      I could not see your email in open review profile (\"a****4@cs\").  Open source the code in the paper  is in progress.  I don't know whether I can share the code for this reproduction challenge now and need to check the legal department in my company. \n\n> 1. What random seed did you use to generate the Ubuntu corpus?\njust used the default one (default = 1234) (see: https://github.com/rkadlec/ubuntu-ranking-dataset-creator) so that results are comparable with others.\n\n> 2. How did you implement the character-composed embedding? \n> 3. could you clarify on the concatenation of word and character embeddings?\n\nI used the tensorflow (tf.nn.bidirectional_dynamic_rnn)  to  conduct all experiments in the paper.\nFor example, you can define function below:\ndef lstm_layer(inputs, input_seq_len, rnn_size, dropout_keep_prob, scope, scope_reuse=False):\n    with tf.variable_scope(scope, reuse=scope_reuse) as vs:\n        fw_cell = tf.contrib.rnn.LSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True, reuse=scope_reuse)\n        fw_cell  = tf.contrib.rnn.DropoutWrapper(fw_cell, output_keep_prob=dropout_keep_prob)\n        bw_cell = tf.contrib.rnn.LSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True, reuse=scope_reuse)\n        bw_cell  = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob=dropout_keep_prob)\n        rnn_outputs, rnn_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, cell_bw=bw_cell,\n                                                                inputs=inputs,\n                                                                sequence_length=input_seq_len,\n                                                                dtype=tf.float32)\n        return rnn_outputs, rnn_states\n\nThen\n#context_char_embedded: [batch_size * max_sequence_length, max_word_length, embed_char_dim]\n#context_char_length: [batch_size * max_sequence_length] (define number of character per word)\n#charRNN_size:  40 \n#max_word_length: 18\n#max_sequence_length: 180\n#dropoutput_keep_prob: 1.0\n#embed_char_dim: 69\n#batch_size: 128\nchar_rnn_output_context,  char_rnn_state_context = lstm_layer(context_char_embedded, context_char_length, charRNN_size,  dropout_keep_prob, charRNN_scope_name, scope_reuse=False)\n\n#response_char_embedded: [batch_size * max_sequence_length, max_word_length, embed_char_dim]\n#response_char_length: [batch_size * max_sequence_length]\n\nchar_rnn_output_response, char_rnn_state_response = lstm_layer(response_char_embedded,\nresponse_char_length, charRNN_size,  dropout_keep_prob, charRNN_scope_name, scope_reuse=True)\n\n#context char representation\nchar_embed_dim = charRNN_size * 2\n#context_char_state: [batch_size * max_sequence_length,  char_embed_dim]\ncontext_char_state = tf.concat(axis=1, values=[char_rnn_state_context[0].h, char_rnn_state_context[1].h])\n#reshape \ncontext_char_state = tf.reshape(context_char_state, [-1, max_sequence_length, char_embed_dim])\n\nThe similar operations are applied to char_rnn_state_response.\n\nFor word embedding, I assume that you can get \"context_word_output and response_word_output\"\nBoth tensors will have shape [batch_size, max_sequence_length, word_embedding_dim]\nThen you can use tf.concat to get the combined representation.\n\n> Regarding your ESIM, what settings did you use for the following hyper-parameters: patience, gradient clipping threshold, max epochs?\nI am not familiar with patience.  No gradient clipping was used.  In my experiments, training usually achieved the best performance (MRR) on the validation set at around 22000 - 25000 batch steps. \n\nNote:  tensorflow version (tensorflow-gpu (1.1.0)). \n\nIf you share your email, we can communicate through email or another channel.\n\n\n\n\n\n\n\n\n", "title": "Reply to \"Request for Code\""}, "rk9R5BjgM": {"type": "rebuttal", "replyto": "HkFtNMclM", "comment": "Hi, Alex,\n     Thank for your interest in our paper. Open source approval process in our company may take time.  At the same time,  if further clarification about technical implementation details (e.g hyper-parameter setting) is needed, feel free to ask here. We like to help you reproduce the results in the paper.", "title": "Reply to \"Request  for code\""}, "r1UNAmogz": {"type": "rebuttal", "replyto": "HkFtNMclM", "comment": "We are extremely excited that you have selected our paper for reproducibility.  We are going through our employer's open source approval process which will take a much longer time than the Dec 15 deadline.  Few questions that may help us with alternatives.  \n\n1. Do we need to stay anonymous to continue further our correspondence?\n2. Are you open for us to enter a legal contractual agreement to access our source code between our employer and your school for the \"reproducibility\" purpose?  This would be potentially a faster process to give you access to our source code.  I can explore this route to get more affirmative answers on the timing if you are open to enter a legal contractual agreement, like \"no cost collaboration\".\n\nAt this time, we believe open source process would take beyond your Dec 15 deadline, but we hope to finish the open source approval for the conference date.  \nIf you have any further questions about our paper, please let us know as well.  \n", "title": "Reply to \"Request for Code\""}}}