{"paper": {"title": "Gradient-free Neural Network Training by Multi-convex Alternating Optimization", "authors": ["Junxiang Wang", "Fuxun Yu", "Xiang Chen", "Liang Zhao"], "authorids": ["jwang40@gmu.edu", "fyu2@gmu.edu", "xchen26@gmu.edu", "lzhao9@gmu.edu"], "summary": "We propose a novel Deep Learning Alternating Minimization (DLAM) algorithm to solve the fully- connected neural network problem with convergence guarantee", "abstract": "In recent years, stochastic gradient descent (SGD) and its variants have been the dominant optimization methods for training deep neural networks. However, SGD suffers from limitations such as the lack of theoretical guarantees, vanishing gradients, excessive sensitivity to input, and difficulties solving highly non-smooth constraints and functions. To overcome these drawbacks, alternating minimization-based methods for deep neural network optimization have attracted fast-increasing attention recently. As an emerging and open domain, however, several new challenges need to be addressed, including 1) Convergence depending on the choice of hyperparameters, and 2) Lack of unified theoretical frameworks with general conditions. We, therefore, propose a novel Deep Learning Alternating Minimization (DLAM) algorithm to deal with these two challenges. Our innovative inequality-constrained formulation infinitely approximates the original problem with non-convex equality constraints, enabling our proof of global convergence of the DLAM algorithm under mild, practical conditions, regardless of the choice of hyperparameters and wide range of various activation functions. Experiments on benchmark datasets demonstrate the effectiveness of DLAM.", "keywords": ["neural network", "alternating minimization", "global convergence"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a new learning algorithm for deep neural networks that first reformulates the problem as a multi-convex and then uses an alternating update to solve. The reviewers are concerned about the closeness to previous work, comparisons with related work like dlADMM, and the difficulty of the dataset. While the authors proposed the possibility of addressing some of these issues, the reviewers feel that without actually addressing them, the paper is not yet ready for publication. "}, "review": {"HyxSFgn2YS": {"type": "review", "replyto": "rJlYsn4YwS", "review": "This paper proposes Deep Learning Alternating Minimization (DLAM) algorithm. First, deep learning optimization problems are formulated as multi-convex Problem 2, by introducing additional variables, constraints and relaxations. Second, alternating update is then used to solve Problem 2, the analysis shows that weights of update converge to a critical point with O(1/k) rate. Finally, some experiments are conducted on MNIST and Fashion-MNIST to show that DLAM is better than SGD, Adagrad, Adadelta and ADMM.\n\n1. DLAM follows dlADMM in Wang et al. (2019), and the only difference between Problem 2 and Problem 2 in Wang et al. (2019) is the indicator function rather than squared l2 loss for a_l. The update is quite similar with dlADMM, with the same convergence result. The technical contribution is incremental.\n\n2. The author claim that Problem 2 is multi-convex. However, I did not see why this is a good point. First, multi-convexity does not imply that gradient update can find global optimum. Second, the results presented here is standard, i.e., O(1/k) convergence to a critical point, which does not show any advantage over Problem 2 in Wang et al. (2019).\n\n3. The experiments are also questionable.\na) The most related dlADMM (Wang et al. (2019)) is not compared here, which can not empirically show why Problem 2 here is a better choice than dlADMM.\nb) The accuracy on MNIST is pretty low. In Wang et al. (2019), the results are about 0.9x, here only 0.7x. Why? And such low accuracy on MNIST seems not convincing to claim \"convergence\".\nc) Please compare with Adam.\nd) What is the ADMM update in the experiments? Directly use ADMM on Problem 1 or 2?\n\n4. The term \"global convergence\" seems misleading. It is not convergence to global optima.\n\nOverall, I found the formulation quite similar with dlADMM (Wang et al. (2019)), the contribution is incremental, the analysis did not show why multi-convex formulation Problem 2 is good (the rate is the same as standard results), and the experiments are questionable and also did not show advantages of the formulation and proposed method.\n\n\n=======Update=======\nThanks for the rebuttal. I keep my rating since there is no updated version of the paper to address my concerns.\n", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 2}, "S1g6FG9jiB": {"type": "rebuttal", "replyto": "HyxSFgn2YS", "comment": "Dear Reviewer:\n               Thank you so much for your suggestions. Our answers are shown as follows:\nQ1. I don\u2019t understand why Problem 2 is multi-convex is a good point.\nA1. The multi-convexity of Problem 2 makes every subproblem convex and solvable. Otherwise, for the sigmoid and tanh activation functions, the z-subproblem can not be solved exactly in the dlADMM algorithm, and a lookup table is required.\nQ2. The experiments are also questionable.\na) The most related dlADMM (Wang et al. (2019)) is not compared here, which can not empirically show why Problem 2 here is a better choice than dlADMM.\nb) The accuracy on MNIST is pretty low. In Wang et al. (2019), the results are about 0.9x, here only 0.7x. Why? And such low accuracy on MNIST seems not convincing to claim \"convergence\".\nc) Please compare it with Adam.\nd) What is the ADMM update in the experiments? Directly use ADMM on Problem 1 or 2?\nA2. \na) We may compare our DLAM with dlADMM in the future version.\nb). In Figure 2(c), we show that the accuracy of the DLAM is above 0.8 for the MNIST dataset, and the actual accuracy is  0.91.\nc). We may compare our DLAM with Adam in the future version.\nd). The ADMM in the experiments is detailed in the reference Taylor et al. (2016) in our original paper. It applies ADMM neither on Problem 1 or Problem 2, instead, the ADMM is applied to the relaxed problem of Problem 1 in the reference.\nQ3. The term \"global convergence\" seems misleading. It is not convergence to global optima.\n\nA3. The term \u201cglobal convergence\u201d refers to the convergence however parameters are initialized, it does not mean the convergence to the global optima. [1]\nReference:\n1. https://en.wikipedia.org/wiki/Local_convergence.", "title": "Thank You for the Feedback"}, "SJgiOJ9isS": {"type": "rebuttal", "replyto": "HyxgOkVPcS", "comment": "Dear Reviewer:\n       Thank you so much for your suggestions. Our answers are shown as follows:\n\nQ1: penalty parameters need to be tuned as ADMM does.\nA1: Yes. The choice of penalty parameters are given in Section 4.2.1.\n\nQ2: The regularization like dropout, batch-norm can not be added into this algorithm.\nA2:  We may discuss how to add dropout and batch-norm in our future version.\n\nQ3: large scale of image like ImageNet could cause problem, Certainly  it would strengthen the paper if there were more comparisons of  DLAM on more complex dataset.\nA3:  We may add the imageNet and other datasets to our paper in the future version.\n", "title": "Thank you for the Feedback"}, "HyxgOkVPcS": {"type": "review", "replyto": "rJlYsn4YwS", "review": "The motivation for this paper is as follows: \n\nWhy SGD:\n1. Easy\n2. applied in online settings \n\nHowever:\n1. Convergence proof has been done by others, but the assumptions of their proofs cannot be applied to problems involving deep neural networks, which are highly nonsmooth and nonconvex\n2. Suffer from gradient vanish\n3. sensitive to the input\n\nThe paper is based on new ideas  on alternative minimization method. In particular the  loss function is reformulated as nested function associated with multiple linear and nonlinear transformations. This nested structure is then decomposed into a series of linear and nonlinear equality constraints by introducing auxiliary variables and penalty hyperparameters. Then  multiple subproblems are generated which can be minimized alternatively. (using ADMM, BCD)\nHowever, these methods suffer from problems:\n1. Convergence properties are sensitive to penalty parameters.\n2. Lack of unified theoretical frameworks with general conditions. (Not fully proved the\nconvergence. Most of the work is based on some assumptions)\n\n\nMain Assumption: Activation functions are quasilinear functions. \n\n\npros (+):\n1. Generic, easy to extend it to convolutional layers and recurrent layers.\n\n2. Transform the Neural network optimization problem into inequality constrained\nproblem (new point of view)\n\n3. Ensure convexity of subproblems. (quadractic approximation of activation function).\n4. Ensure global minima, and converges to the critical point (where 0 is in the set of\npartial differential of F(F(W,b,,z,a) is the augment loss function) with respect to W*,\nand set of partial differential of F with respect to b*).\n5. The convergence rate is O(1/k), k is the number of iterations of updating (W,b,z,a).\n6. inequality-constraint prevents the output of a nonlinear function from changing much\nand reduces sensitivity to the input.\n7. Matrix inversion is avoided by quadratic approximation\n8. No need strict and complex condition, such as KL properties to prove convergence.\nInstead, this algorithm needs simple and mild conditions to guarantee convergence,\nand cover most of the loss function and activation functions.\n9. The choice of hyperparameters has no effect on convergence.\n10. Much better performance on MNIST\n\ncons(-):\n1. penalty parameters need to be tuned as ADMM does.\n\n2. The regularizations $\\omega$ is l1 and l2, otherwise, it will not be closed-form\nsolutions. The regularization like dropout, batch-norm can not be added into this algorithm.\n\n3. The dataset is relatively naive. Only MNIST and Fashion MNIST. It would be better to include little more sophisticate dataset like ImageNet or Cifar10. Since the computation complexity is O(d^2) where d is the dimension of the features, large scale of image like ImageNet could cause problem. (Comparison: features of MNIST: 196, features of Fashion MNIST:784, features of ImageNet: 65536).\n\n4. The Adagrad has better performance than DLAM on 500*500 on Fashion MNIST and close performance on MNIST. As the authors suggest: DLAM performed competitively for the Fashion MNIST dataset. This can be interpreted as follows:  the proposed algorithm performs well on easy datasets, however the performance is not much better than other algorithms  on Fashion MNIST, which is little more challenging. What would be your comment about this criticism? Can you provide any theoretical insight?  Certainly  it would strengthen the paper if there were more comparisons of  DLAM on more complex dataset. ", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}}}