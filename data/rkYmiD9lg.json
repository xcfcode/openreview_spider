{"paper": {"title": "Exponential Machines", "authors": ["Alexander Novikov", "Mikhail Trofimov", "Ivan Oseledets"], "authorids": ["novikov@bayesgroup.ru", "mikhail.trofimov@phystech.edu", "i.oseledets@skoltech.ru"], "summary": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "keywords": ["Supervised Learning", "Optimization"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "Modeling nonlinear interactions between variables by imposing Tensor-train structure on parameter matrices is certainly an elegant and novel idea. All reviewers acknowledge this to be the case. But they are also in agreement that the experimental section of this paper is somewhat weak relative to the rest of the paper, making this paper borderline. A revised version of this paper that takes reviewer feedback into account is invited to the workshop track."}, "review": {"rJdIo1nIe": {"type": "rebuttal", "replyto": "rJjjlvkVl", "comment": "> On a related note, it'd be interesting to know what capacity your model has as a function of the TT-rank: does the generalization error depend in a nice way on the TT-rank, or does it blow up?\n\nI wanted to add that in the new version of the paper we added a TT-rank vs. test AUC plot for the MovieLens dataset.\n\nThanks for your interest and all the best,\nAlexander", "title": "Update"}, "Skdlsy38g": {"type": "rebuttal", "replyto": "HJod5BH4l", "comment": "Thank you for your time and thorough evaluation of our work.\n\nNote that it\u2019s fairly easy to make a fast inference algorithm for sparse data by building a segment tree on the TT-cores (but we believe it is out of the scope of this already pretty packed paper) and one can readily adapt the SGD learning algorithm to it. However, we don\u2019t know if it\u2019s possible to adapt the Riemannian approach to the sparse setting.\n\nWe agree with your (and some other reviewer\u2019s) criticism of the experimental evaluation and significantly revised the experiments section.\n1) We added a more detailed description of the datasets and preprocessing steps\n2) We added the test set performance for all the datasets\n3) We did our best to evaluate each of the characteristics of the proposed methods on each of the datasets.\n4) We found that after a slight change to the learning procedure dropout becomes unnecessary, so we decided to remove it whatsoever from all the experiments (see the description of the updates made to the paper).\n", "title": "Author reply"}, "By88q13Ig": {"type": "rebuttal", "replyto": "BJMAByP4l", "comment": "Thank you for your review, I\u2019m glad you liked our work.\nThe computational complexity is indeed important, we updated the paper to add it (it\u2019s O(d r^2 (r + M)) per step of the Riemannian optimization where d is the number of features, r is the TT-rank, and M is the size of the mini-batch).", "title": "Author reply"}, "B1Iz91nUx": {"type": "rebuttal", "replyto": "B12cSftVl", "comment": "Hi Mathieu,\n\nThanks for your interest in our work!\n\nI agree that comparing the model sizes would be interesting, but I\u2019m not sure if there is a fair way to do it. Since the models are different, it seems impossible to compare the number of parameters while fixing the flexibility of the models to some level. And comparing the model sizes as is may be misleading (EMs have O(d r^2) params while m-way FMs have O(d m r), but the ranks r have completely different nature in the two models).\nAlso, according to our experiments, m-way FMs for m > 30 are hard to train, so extremely high-order FMs may not be that practical even with a reasonable model size.\n\nGood point about the Riemannian optimization, we\u2019ve updated this statement to be more accurate: we wanted to say that FMs do not allow for Riemannian optimization for m >= 3.\n\nWe added all three papers you mentioned into the related works section, thanks for suggesting them.\n\nAll the best,\nAlexander", "title": "Answer"}, "B1IqtJ2Ig": {"type": "rebuttal", "replyto": "BylWr2YHg", "comment": "We would like to thank the reviewer for a thorough review and valuable suggestions.\n\nRegarding the comparison with FMs optimized over the manifold of positive definite matrices, we believe this comparison is out of the scope of our work since we mainly aimed at modeling high-order interactions, while the Riemannian optimization is possible only for the second-order FMs.\n\nYou are very right to question the random initialization results. We further investigated this effect and found out that it is not that important to initialize the learning from the linear solution built for the dataset at hand, but we may as well generate a random linear model and initialize the learning from it.\nSo there is nothing special about the initialization we had chosen, but it\u2019s important to initialize the model from a proper distribution.\nTwo reasons that may cause this effect:\na) The Tensor Train is a product of a large number of factors (especially in the UCI experiment with 160 TT-cores), and it may raise the problem of vanishing and exploding gradients. It may be interesting to explore how our proposed initialization (a random linear model) is connected to the Xavier initialization from the Neural Networks community.\nb) If we initialize the model to put too much weight on a large number of high-order interactions, it may be hard to recover by gradient optimization: each gradient step would largely affect the high-order interactions and slightly affect the low-order interactions. On the other hand, it seems important to firstly approximate the linear part of the model, and only then try to model the residuals with interactions of a higher order.\n\nAs you suggested, we tried a vanilla feedforward neural network baseline on the synthetic dataset and ran into severe overfitting: it\u2019s easy to optimize the training loss to zero, while the test set performance was 0.5 AUC in all our experiments.\n\n> A few typos: 'Bernoulli distrbution', 'reproduce the experiemnts', 'generilize better'.\nThank you for pointing out the typos, we fixed them in the new version of the paper.", "title": "Author reply"}, "r1wWKynUx": {"type": "rebuttal", "replyto": "Sy9wO0tSl", "comment": "Thank you for this very thorough review and for the numerous comments, suggestions, and insightful questions. We\u2019ve updated the paper to address them.\n\nRegarding the experimental evaluation, we added more baselines, restructured the text and added experiments to assess different aspects of the model on all the datasets considered, and investigated the influence of the rank on the proposed model performance. \n\n\n> -formula 2: Obvious comment: learning the parameters of the model in (1) can be done as in (2), but also in other ways, depending on the approach you are using.\n\nThanks, we clarified this part.\n\n> -the fact that the rank is bounded by 2r, before formula 9, is explained in Lubich et al., 2015?\n\nThat\u2019s right, we clarified this in the new version.\n\n>-after formula 10: why the N projections in total they cost O(dr^2(r+N)), it should be O(Ndr^2(r+1)), no? since each of the elements of the summation has rank 1, and the cost for each of them is O(dr^2(r+TT_rank(Z)^2)), where TT-rank(Z)=1. Am I wrong?\n\nGood point, I haven\u2019t explained it properly in the original submission (fixed in the new version).\nThe reason why it\u2019s O(d r^2 (r+N)) is because the projection consists of two parts: 1) preprocessing, which cost O(d r^3); 2) and the actual projection, which cost O(d r^2 TT_rank(Z)^2).\nSince we are projecting all the gradients on the same tangent space defined by W, we have to do the preprocessing step just once.\n\n>-section 6.2: can you explain why the random initialization freezes the convergence? This seems interesting but not motivated. Any guess?\n\nThat\u2019s an excellent question, and we believe that we found an answer (and added it to the new version of the paper). We observed that it is not that important to initialize the learning from the linear solution built for the dataset at hand, but we may as well generate a random linear model and initialize the learning from it.\nSo there is nothing special about the initialization we had chosen, but it\u2019s important to initialize the model from a proper distribution.\nTwo reasons that may cause this effect:\na) The Tensor Train is a product of a large number of factors (especially in the UCI experiment with 160 TT-cores), and it may raise the problem of vanishing and exploding gradients. It may be interesting to explore how the initialization from a random linear model is connected to the Xavier initialization from the Neural Networks community.\nb) If we initialize the model to put too much weight on a large number of high-order interactions, it may be hard to recover by gradient optimization: each gradient step would largely affect the high-order interactions and slightly affect the low-order interactions. On the other hand, it seems important to firstly approximate the linear part of the model, and only then try to model the residuals with interactions of a higher order.\n\n> -section 6.3: you adopt dropout: can you comment in particular on the advantages it gives in the context of the exponential machines? did you use it on real datasets?\n> -section 8.3: \"we report that dropout helps\".. this is quite general statement, only tested on a synthetic dataset\n\nWe found that after a slight change to the learning procedure dropout becomes unnecessary, so we decided to remove it whatsoever from all the experiments (see the description of the updates made to the paper).\n\n> -how do you choose r_0 in you experiments? with a validation set?\n\nWe tried a few reasonable options by hand and chose the best one according to the test error.\nI agree, using the validation set would be fairer, but the overfitting is unlikely to occur in this situations because of the small number of variants we tried and because of the robustness of the model to the choice of the TT-rank (see the experiment we added to investigate this).\n\n> -in section 7: why you don't have x_1 x_2 among the variables?\n\nActually, we have all terms from the previous model (\\hat{y}(x)) plus some new terms which involve logarithms. Thank you for spotting this clarity issue, we made this point more clear in the new version.\n\n> -section 8: there is a typo in \"experiments\"\n> -section 8.1: \"We simplicity, we binarized\" I think there's a problem with the English language in this sentence\n\nThanks, we fixed these typos.\n\n> -section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time? or test wrt other algorithms?\n\nWe added the training and inference time and also the performance of the logistic regression.", "title": "Author reply"}, "ryscP1n8e": {"type": "rebuttal", "replyto": "rkYmiD9lg", "comment": "We would like to thank all the reviewers and commenters for their time, feedback, and ultimately for making our paper better.\n\nWe updated the paper to address the questions raised in the reviews.\n\nWe found out that the backtracking algorithm is beneficial only for the small-scale setting when the mini-batch size is of the order of the full dataset (e.g. UCI experiments). Otherwise, backtracking is (locally) tuning the learning rate too well and overfits to each mini-batch, which stagnates the convergence process.\nFor this reason, we removed the backtracking from the paper text and from all the experiments.\nWe also found out that after removing the backtracking, the dropout becomes unnecessary, and we removed it as well.\n\nOther changes:\n1) We added the complexity of each step of the Riemannian gradient descent.\n2) Investigated the effect of the proper initialization on the model and proposed a random initialization that works on par with the initialization from the solution of the linear model.\n3) Added the validation loss for the comparison of optimizers on the UCI datasets.\n4) Added the convergence plots to compare Riemannian optimization vs SGD baseline on the synthetic dataset.\n5) Added 3 suggested papers to the related work section.\n6) Plotted the TT-rank vs. accuracy graph on the MovieLens 100K dataset.\n7) Restructured the experiments section to make it easier to assess different aspects of the model on different datasets.\n8) Added a feedforward neural network baseline for the synthetic dataset.\n9) Fixed typos and made a few clarifications. \n", "title": "Update"}, "B12cSftVl": {"type": "rebuttal", "replyto": "rkYmiD9lg", "comment": "Hi Alexander,\n\nYour paper is an interesting addition to the literature on low-rank polynomial models. Good work!\n\nI agree with a previous comment that it would be worth adding more comments on the difference between FMs and EMs in Section 9. For instance, EMs model all d-combinations while FMs model combinations up to some degree. It would also be informative to compare the total model size of both models.\n\nI am not sure I agree that \"TT-format allows for Riemannian optimization\" is an advantage of tensor trains. For example, it should be possible to train FMs over the positive definite matrix manifold.\n\nAdmittedly, the literature is fairly recent but here are a few relevant prior works:\n\n- \"On the Computational Efficiency of Training Neural Networks\" by Livni et al. https://arxiv.org/abs/1410.1141\n\n-  \"Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms\" by my colleagues and me https://arxiv.org/abs/1607.08810\n\n- \"Supervised Learning with Quantum-Inspired Tensor Networks\" by Stoudenmire and Schwab https://arxiv.org/abs/1605.05775\n\nBest regards", "title": "Some comments and related works"}, "B1ZAI_GVl": {"type": "rebuttal", "replyto": "rkYmiD9lg", "comment": "Yesterday I updated the pdf. The only thing that changed is that I added a paragraph at the end of the related works section to credit the papers suggested by the comment below.", "title": "Revision"}, "r1MCRKbEx": {"type": "rebuttal", "replyto": "rJjjlvkVl", "comment": "Thank you for your comment.\nIndeed, we didn't outperform the High-Order Factorization Machines in the experiments. However, we obtained the same results as they did (up to the noise in the experiment runs), and this may serve as a proof-of-the-concept that our approach - to model very high-degree polynomials - may work in principle.\nRegarding the computational time, note that the High Order Factorization Machines are implemented in Tensor Flow in a highly parallel manner, while we use a pure Python+Jit implementation.\n\nThanks a lot for pointing out these very relevant works. Indeed, one can generalize the CP bounds to our case by using the fact that any CP tensor of rank r can be represented as a TT tensor of rank r^2. The bound obtained this way would, however, be rather weak (of order r^2(1+BBx)^d d^3 sqrt(ln(d))/sqrt(n) ), since Tensor Machines assumes that the order of interactions that we model `q` is small. On the other hand, we observed in the experiments that modeling all interactions of every order can give reasonable test accuracy.\nHowever, I do agree with you that careful adaptations of the proof of the CP bound can provide a much tighter generalization bound and it\u2019s an interesting future research direction. \nI\u2019ll add the papers you mentioned to the related works in the following few hours.\n", "title": "Thanks for the comments"}, "rJjjlvkVl": {"type": "rebuttal", "replyto": "rkYmiD9lg", "comment": "From the reported experiments on MovieLens and synthetic data, it seems that higher-order FMs give better accuracy at lower cost than ExMs. Is there experimental evidence that potentially modeling all the feature interactions gives better performances on some problems (on real data)?\n\nOn a related note, it'd be interesting to know what capacity your model has as a function of the TT-rank: does the generalization error depend in a nice way on the TT-rank, or does it blow up?\n\nTwo related works are:\n- \"Tensor machines for learning target-specific polynomial features\" which learns a CP factorization for polynomial regression and proves generalization error bounds; the proof is probably adaptable to your setting.\n- \"Learning multidimensional Fourier series with tensor trains\", which uses TT-decomposition to learn 'random' Fourier features.", "title": "questions and related works"}, "SkLT0VlXg": {"type": "rebuttal", "replyto": "S1HA2symx", "comment": "Neither of both; it is a tensor-train decomposition.  You can read more about it here: http://epubs.siam.org/doi/abs/10.1137/090752286\nShortly, it can be computed using SVD (PARAFAC can not), and does not have intrinsic curse of dimensionality (as Tucker) for really high dimensions.", "title": "It is tensor-train decomposition"}, "S1HA2symx": {"type": "review", "replyto": "rkYmiD9lg", "review": "Is the tensor decomposition in figure 1 Tucker decomposition or Paraface decomposition?This paper proposes to use the tensor train (TT) decomposition to represent the full polynomial linear model. The TT form can reduce the computation complexity in both of inference and model training. A stochastic gradient over a Riemann Manifold has been proposed to solve the TT based formulation. The empirical experiments validate the proposed method.\n\nThe proposed approach is very interesting and novel for me. I would like to vote acceptance on this paper. My only suggestion is to include the computational complexity per iteration.\n", "title": "tensor decomposition", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJMAByP4l": {"type": "review", "replyto": "rkYmiD9lg", "review": "Is the tensor decomposition in figure 1 Tucker decomposition or Paraface decomposition?This paper proposes to use the tensor train (TT) decomposition to represent the full polynomial linear model. The TT form can reduce the computation complexity in both of inference and model training. A stochastic gradient over a Riemann Manifold has been proposed to solve the TT based formulation. The empirical experiments validate the proposed method.\n\nThe proposed approach is very interesting and novel for me. I would like to vote acceptance on this paper. My only suggestion is to include the computational complexity per iteration.\n", "title": "tensor decomposition", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}