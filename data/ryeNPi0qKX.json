{"paper": {"title": "Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis", "authors": ["Kelly W. Zhang", "Samuel R. Bowman"], "authorids": ["kellywzhang@seas.harvard.edu", "bowman@nyu.edu"], "summary": "We throughly compare several pretraining tasks on their ability to induce syntactic information and find that representations from language models consistently perform best, even when trained on relatively small amounts of data.", "abstract": "Recent work using auxiliary prediction task classifiers to investigate the properties of LSTM representations has begun to shed light on why pretrained representations, like ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017), are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives - language modeling, translation, skip-thought, and autoencoding - on their ability to induce syntactic and part-of-speech information.  We make a fair comparison between the tasks by holding constant the quantity and genre of the training data, as well as the LSTM architecture. We find that representations from language models consistently perform best on our syntactic auxiliary prediction tasks, even when trained on relatively small amounts of data. These results suggest that language modeling may be the best data-rich pretraining task for transfer learning applications requiring syntactic information. We also find that the representations from randomly-initialized, frozen LSTMs perform strikingly well on our syntactic auxiliary tasks, but this effect disappears when the amount of training data for the auxiliary tasks is reduced.", "keywords": ["representation learning", "recurrent neural networks", "syntax", "part-of-speech tagging"]}, "meta": {"decision": "Reject", "comment": "Strengths:  \n\n-- Solid experiments \n-- The paper is well written\n\nWeaknesses:\n\n-- The findings are not entirely novel and not so surprising, previous papers (e.g., Brevlins et al (ACL 2018)) have already \nsuggested that LM objectives are preferable and also using LM objective for pretraining is already the  standard practice (see details in R1 and R3). \n\nThere is a consensus between the two reviewers who provided detailed comments and engaged in discussion with the authors.  "}, "review": {"HkgkksjYAm": {"type": "rebuttal", "replyto": "H1e8RYK_0Q", "comment": "Thanks for your clarification. I understand your point now and agree that the per token loss for the language model encoders (in contrast to the encoders for other tasks) could be a reason why LMs perform well on the evaluation tasks. I think the per token loss could also be a reason why LMs are in general able to learn syntax so well. \n\nI also agree that more evaluation tasks would be better as there is always room to do more. In the LM results section of our most recent revision, I made a note of how our evaluation tasks have per token losses and how LM encoders are the only ones with a per token loss.", "title": "response"}, "r1x6pM-I07": {"type": "rebuttal", "replyto": "HylENB7xAQ", "comment": "Thanks for your response! We understand your viewpoint and also agree that accepting high-impact papers is important. Our paper does have some consequences for decisions about whether it makes sense to try to gather more labeled data in service of pretraining, as it is possible to collect much larger MT datasets than we currently have. \n\nAlso, your argument that language modeling is obviously better since it\u2019s easier to train on more data isn\u2019t so obvious, since large unsupervised datasets alone aren\u2019t always better than smaller amounts of finely labelled data. For example, for the closely related area of learning sentence-vector representations, its been found that that training representations on smaller amounts of supervised data, specifically natural language inference data with only 1 million sentence pairs, is better than using more unlabeled data, specifically training skipthought on the Toronto Books Corpus with 74 million sentences (see Conneau et al. 2017, https://arxiv.org/abs/1705.02364). Similarly, it\u2019s been found that providing explanations rather than simply labels for text classifiers can speed up training time by 5-100x (Hancock et al. 2018, https://arxiv.org/abs/1805.03818). Our paper shows that supervised data in the form of translation pairs is not an efficient way to improve sentence encoder representations.\n", "title": "Replying to your response"}, "Hyl2uSVCa7": {"type": "rebuttal", "replyto": "BkedG4CTnQ", "comment": "- All the tasks use the same training data. The data each model is trained on was pre-processed in the same way, word-level tokenization. All the training objectives we compare all have the same loss: average negative log likelihood of the target sequence. For these reasons, we believe our methods were a fair of all the training tasks we examined. Perhaps I am misunderstanding your comment?\n\n- We agree that why language models are a better pretraining objective than translation is an interesting question. It is in general difficult to answer the question of why representations from one neural model are better than those from another. We did our best to thoroughly compare these training objectives and are interested in any methods / techniques to further address the question of why as there is always room to do more.\n\n- Blevins et al. compare translation models trained on WMT 14 English-German with LMs trained on CoNLL 2012\u2019s training set on dependency arc prediction using the Universal Dependencies dataset. It was unclear from their results alone that if LMs were superior to translation models simply due to differences in the domain and amount of training data for the respective models. We updated how we address this in our literature review section to make this point more clear.\n        The surprisingly good performance of random encoders was found by Conneau et al. ACL 2018 for sentence vector representations using different architectures with pretrained fastText embeddings. We show that randomly initialized LSTM hidden state representations with *randomly* initialized embeddings perform quite well on POS and CCG tagging, which puts the results of Belinkov et al. and Blevins et al. that encoders trained on tasks like MT and language modelling learn syntactic information in a new light, since we find that many of these \u201clearned\u201d syntactic properties can even be learned from random LSTMs. We also show that random LSTMs preserve information about neighboring word identities better than trained LSTMs, which raises new questions about what kind of alternative information is learned by LSTMs that helps them with training task performance.\n\n- We believe that the results for LSTMs trained on WMT data is quite representative of neural NLP models trained on most large-scale datasets. Since we use the same dataset for all tasks, we control for any domain effects. It seems unlikely that using one pretraining task over another will lead to much better or worse domain adaptation from the same domain.", "title": "Thanks for your feedback!"}, "Sygh7MNRTX": {"type": "rebuttal", "replyto": "rJl1Yb9O2Q", "comment": "Thank you! \n\nWe used both \u201cLM Forward\u201d models - the larger forward LM we examined on its own and the small forward LM was combined with the small backward LM into the bidirectional LM.\n\nThe input embeddings for the randomly initialized LSTMs are also randomly initialized.\n\nLet us know if you have further feedback!", "title": "Thanks! Let us know if you have further feedback!"}, "SkgklG4CpQ": {"type": "rebuttal", "replyto": "S1lDaxv027", "comment": "Our contribution is a thorough examination of several different pretraining tasks, controlling for the domain, amount of data, and training procedure. When CoVe was released at NIPS last year, it achieved SoTA numbers on several prominent NLP tasks. Although ELMo compares to CoVe in their paper and outperforms CoVe, since CoVe was trained on WMT English-German and ELMo was trained on the One Billion Word Benchmark, it was unclear if the performance gain of ELMo was primarily due to the increased amount of training data. Moreover, without a direct comparison we can\u2019t even be sure that language modeling is better because ELMo could have just been more carefully tuned. Our finding that language modeling, an unsupervised task, outperformed translation models trained on the *same* data is still surprising because the translation models are given the source sentence in a different language and thus have strictly more information than language models. We also agree that the results of our analysis of the randomly-initialized encoder in Section 6 are surprising, and could form the basis for a larger study.\n\nFig 1: You are right. We just fixed the typography in our most recent revision.\n\nFig 2: The upper plot is for POS tagging and the bottom for CCG supertagging. Each of those plots are then split into three columns corresponding to different amounts of classifier training data. Each column has two plots because when we tried plotting all ten lines into one figure it was difficult to read, so we split up the models into two groups: models with attention (plus BiLMs) and models without attention (plus forward LM). We welcome further suggestions for improving our presentation.\n\nFig 4: We included the patterns and bright colors in order to make it easier for the visually impaired to read.", "title": "Thanks for your comments!"}, "S1lDaxv027": {"type": "review", "replyto": "ryeNPi0qKX", "review": "\nI have mixed feelings about this paper. On one hand, it\u2019s a thorough and well-written experimental paper, something which is really important but is also clearly underappreciated in the machine learning community. On the other, it was not really obvious to me why some of objectives tested here are interesting: LM objectives like ELMo have seen a lot of uptake in the NLP community (and this is definitely an NLP paper), but most of the others\u2014like skip-thought, MT, and autoencoders\u2014have not. So the basic research question doesn\u2019t seem like an especially burning one. The trends in Fig. 2 show that these alternatives underperform an LM objective, which suggests that the NLP community can keep using that objective without worry\u2014and everything else in the figure seems as we would expect. \n\nIn short, I think the paper is a well-done study on a hypothesis of perhaps minor interest. The results are sensible but confirm what we already strongly suspected, and they seem unlikely to strongly influence other research, since they confirm that everyone has been the right thing all along. I\u2019m not entirely sure what I learned from this.\n\nTo me, the most interesting experiment is the final one in Section 6. This experiment seems like it could be the germ for a far more interesting paper getting at how these pretraining objectives help with downstream tasks. As it stands, it feels like an interesting nugget tacked on to an otherwise complete (and much less interesting) paper.\n\nPresentational comments:\n\nFig.1: really nitpicky, but the typography of the POS tags and CCG categories is all wrong. These aren\u2019t mathematical symbols!\n\nFig 2. Slightly confused why these are broken up into two separate plots.\n\nFig 4. is hard to read due to the lurid colors and patterns, which require a lot of cross-referencing with the legend. I wonder if this would be better as simply a table. I also found it very confusing at first since the y-axes are out of sync between the two figures\u2014initially it looked as if the legend was overlaid on a set of bars in the left figure that had the same baseline as the right figure. \n", "title": "Well done with few surprises", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkedG4CTnQ": {"type": "review", "replyto": "ryeNPi0qKX", "review": "This paper tests various pretraining objectives (language modeling, machine translation, skip-thought, and autoencoding) on two syntactic tasks: POS tagging and CCG tagging. It finds that language modeling outperforms the other pretraining objectives; additionally, randomly-initializing an encoder achieves decent performance when given a large amount of labeled data for the tagging task. The experiments in this paper are very thorough and explained well. By controlling for pretraining data size, the authors are able to reasonably claim that language modeling is superior to translation as a syntactic transfer learning task. On the other hand, I have some concerns regarding the significance of the paper's contributions, and as such I am borderline on its acceptance. \n\ncomments:\n- the experiments in the paper feel biased towards language modeling. Language modeling is the only token-level prediction task of the four objectives here, but both of the two downstream tasks are at the token level. It is perhaps unsurprising then that language modeling performs best; perhaps the authors could have considered some sentence-level downstream tasks as well to properly control for this? Or added some more word-level pretraining objectives? \n\n- sort of relatedly, the authors do not provide any explanations as to *why* language modeling is a better pretraining objective than translation. What kinds of examples do the tagging models using LM pretraining get right that the translation models do not? Such an analysis could help provide more concrete insights into what kind of information each objective is encoding.\n\n- the claim that LMs > translation is not a new finding. The authors cite Blevins et al, who find the same result on the task of dependency arc prediction. Similarly, the surprisingly good performance of random encoders was also found in Conneau et al., ACL 2018. As the main contribution of this paper seems to be a more controlled study of Blevins et al on different syntactic tasks, I don't think there is enough here for an ICLR submission. \n\n- what is the effect of the specific dataset and architecture on the results? Here we just look at a couple translation datasets (all news data) and LSTM models. Do things change when we move to transformers or more diverse domains? ", "title": "carefully done experiments but is it enough?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJl1Yb9O2Q": {"type": "review", "replyto": "ryeNPi0qKX", "review": "This is nicely written paper analyzing the effect of various pre-training methods and shows that language models are very effective on sequence tagging tasks (POS, CCG). The experiments are well motivated and well described.\n\nRegarding Table 1: which one of the \"LM forward\" models was used in the subsequent experiments? \n\nAre the input embeddings for the random init LSTM pre-trained or are they also randomly initialized?", "title": "official review", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}