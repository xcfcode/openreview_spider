{"paper": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"], "summary": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": " The authors acknowledge that the ideas in the paper are incremental, but assert these are not-trivial improvements upon prior work on pixel CNNs. The reviewers tended to agree with this characterization. The paper presents SOTA pixel likelihood results on CIFAR-10. This work is also coupled with a high quality source code contribution, which also appears to have already been well received by the github community. Reviewer 1 made the point that in terms of raw novelty this work is probably a little below the bar for an oral presentation. A public reviewer rated this paper as a strong accept. Given the statistics, quality and originality of the other papers in my AC batch I recommend poster."}, "review": {"B1ei6LaUx": {"type": "rebuttal", "replyto": "BJrFC6ceg", "comment": "We uploaded a new revision incorporating some of the suggestions made by the reviewers. Thank you for your input!\n\nSpecifically, the revision incorporates additional references and an experimental comparison with continuous mixture models as suggested by reviewer 2, and a more detailed description of the likelihood model, including equations, as requested by reviewer 3.", "title": "new revision incorporating reviewer recommendations"}, "ByZ8IBn8e": {"type": "rebuttal", "replyto": "S1H7heprl", "comment": "I ran the model without discretization of the mixture distribution on the pixels. Using the same model settings as for the 2.92 bits per sub-pixel reported in the paper, it got to 3.11 bits per dim. (This is a variational lower bound, adding small uniform noise to the discrete pixels). The code can be found here: https://www.dropbox.com/sh/tcx5bg76xngmm8a/AADyvRZ0H98rcGU-7wULjvV0a?dl=1, and I will also include this result in the paper.", "title": "with continuous mixture likelihood the model gets to 3.11 bits per dim"}, "B1edzW6Bg": {"type": "rebuttal", "replyto": "B1DF5VFEg", "comment": "Thanks for you review! The ideas in our paper are incremental, but non-trivial, improvements upon existing work. We have deliberately been modest in the presentation of these ideas, rather than overclaiming paradigm shifting insights as already happens all too often in our field. The contributions we make are useful to many people, as evidenced by the SOTA results, the number of github stars, the papers building on our work (e.g. https://arxiv.org/pdf/1611.02731.pdf, https://arxiv.org/pdf/1612.08185v1.pdf), and the large amount of positive personal communication I have received. Taking this into account, I feel the paper contributes more to our field than most ICLR submissions. Could you please reconsider your rating in this light?", "title": "are useful technical contributions not enough for acceptance?"}, "BJ-SalpBx": {"type": "rebuttal", "replyto": "B1wgPCMVx", "comment": "Thanks for the review! I will add a bit more detail to the explanation of the mixture likelihood.", "title": "will try to improve clarity of likelihood explanation"}, "S1H7heprl": {"type": "rebuttal", "replyto": "Bkc_sOZ4l", "comment": "Thanks for the citation suggestions. I will add these, as well as a comparison of the proposed likelihood model to a continuous mixture likelihood. (I have done experiments using continuous mixtures before, but not yet with the final architecture presented in the paper)", "title": "will add comparison + references for continuous mixture models"}, "B1HbaceXg": {"type": "rebuttal", "replyto": "B1IZCB0Ge", "comment": "The reported number in section 3.2 of 2.94 bits per dimension is indeed class-conditional, conditioned on the true class, not marginalized. (Note that this is not the number we compare against the original pixelCNN). The text in this section makes a comparison with our unconditional model, with the reasoning being that conditioning on class labels adds at most log(10)/(32*32*3*log(2))=0.001 bits per dimension to the log-likelihood, which is smaller than the reported precision. This means that 2.94 + 0.001 = 2.94 is an upper bound on the bits per pixel (negative log likelihood when marginalizing out the class label) and is thus comparable to the unconditional model.\nHowever, on second thought, this bound might be pessimistic: If there are examples in the test set that the model, mistakenly, assigns a much larger probability to when conditioning on the wrong label than on the right label, the marginal log-likelihood could be significantly better than what is reported. I will check this when I get back from NIPS.", "title": "No, it's the conditional log-likelihood"}, "B1Z-K9gXg": {"type": "rebuttal", "replyto": "BJyUo7yml", "comment": "The precise structure of the downsampling/upsampling steps can be found at https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/nn.py: We use strided convolution and strided deconvolution, so there is no information leakage. I double-checked this by computing the gradient of the output parameters at each spatial location with respect to all the pixels in the input image: The gradient of the output at location (i,j) is strictly zero for all pixels at locations (i,>=j) and (>i,:).", "title": "double-checked this by computing gradients in randomly initialized model"}, "rJsd0cgmg": {"type": "rebuttal", "replyto": "rJjq2-RMx", "comment": "We used the same ordering as the original PixelCNN and did not explore alternatives. My guess is that it would not matter much, but it's worth trying.", "title": "did not try this"}, "By4JncgXg": {"type": "rebuttal", "replyto": "H1dU8-1Ql", "comment": "1. Early on I experimented with continuous distributions and no discretization, but this didn't work very well. The main benefit of using discretization is to capture the relatively large probability mass of the data distribution at pixel values of 0 and 255: Continuous distributions cannot model this without effectively reserving a mixture component for all possible combinations of edge values.\n2. Yes, ease of evaluation was the main motivation, since the logistic CDF is simply the sigmoid function. The number of mixture components seems to be the dominant factor in determining the distribution expressiveness. Since we saw virtually no effect from adding more than 5 components, we did not further explore methods for making the distribution more flexible. For other types of data, Gaussian scale mixtures might prove very useful though.", "title": "Discretization at the borders is important for optimal performance"}, "BJyUo7yml": {"type": "review", "replyto": "BJrFC6ceg", "review": "Perhaps this is easy to see, but can you provide a bit more detail to ensure us that after down- and upsampling the logistic distribution can still not access the pixel it is trying to predict?Summary:\nThis paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.\n\nReview:\nThis is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.\n\nMy main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).\n\nThe main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has \u201cproved to be a problem for earlier models based on continuous distributions\u201d. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).\n\n60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content \u2013 and this can serve as a good example to demonstrate the usefulness of dropout \u2013 why not use \u201c80 million tiny images\u201d (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model\u2019s likelihood is tractable), so this data could even be used in the class-conditional case.\n\nIt would be interesting to know how fast the different models are at test time (i.e., when generating images).", "title": "Model validity after down- and upsampling", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Bkc_sOZ4l": {"type": "review", "replyto": "BJrFC6ceg", "review": "Perhaps this is easy to see, but can you provide a bit more detail to ensure us that after down- and upsampling the logistic distribution can still not access the pixel it is trying to predict?Summary:\nThis paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.\n\nReview:\nThis is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.\n\nMy main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).\n\nThe main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has \u201cproved to be a problem for earlier models based on continuous distributions\u201d. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).\n\n60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content \u2013 and this can serve as a good example to demonstrate the usefulness of dropout \u2013 why not use \u201c80 million tiny images\u201d (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model\u2019s likelihood is tractable), so this data could even be used in the class-conditional case.\n\nIt would be interesting to know how fast the different models are at test time (i.e., when generating images).", "title": "Model validity after down- and upsampling", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1dU8-1Ql": {"type": "review", "replyto": "BJrFC6ceg", "review": "1. Do you have any experiments where the discretization is dropped?\n2. Why the particular choice of logistic distributions? How do the properties compare to the more flexible family of Gaussian scale mixtures which have been used successfully in very similar work e.g. (Theis et al. 2012) http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0039857 Is it mainly/only about ease of evaluation?Apologies for the late submission of this review, and thank you for the author\u2019s responses to earlier questions.\n\nThis submission proposes an improved implementation of the PixelCNN generative model. Most of the improvements are small and can be considered as specific technical details such as the use of dropout and skip connections, while others are slightly more substantial such as the use of a different likelihood model and multiscale analysis. The submission demonstrates state-of-the-art likelihood results on CIFAR-10.\n\nMy summary of the main contribution:\nAutoregressive-type models - of which PixelCNN is an example - are a nice class of models as their likelihood can be evaluated in closed form. A main differentiator for this type of models is how the conditional likelihood of one pixel conditioned on its causal neighbourhood is modelled:\n\n- In one line of work such as (Theis et al, 2012 MCGSM, Theis et al 2015 Spatial LSTM) the conditional distribution is modelled as a continuous density over real numbers. This approach has limitations: We know that in observed data pixel intensities are quantized to a discrete integer representation so a discrete distribution could give better likelihoods. Furthermore these continuous distributions have a tail and assign some probability mass outside the valid range of pixel intensities, which may hurt the likelihood.\n- In more recent work by van den Oord and colleagues the conditional likelihood is modelled as an arbitrary discrete distribution over the 256 possible values for pixel intensities. This does not suffer from the limitations of continuous likelihoods, but it also seems wasteful and is not very data efficient.\n\nThe authors propose something in the middle by keeping the discretized nature of the conditional likelihood, but restricting the discrete distribution to ones whose CDF that can be modeled as a linear combination of sigmoids. This approach makes sense to me, and is new in a way, but it doesn\u2019t appear to be very revolutionary or significant to me.\n\nThe second somewhat significant modification is the use of downsampling and multiscale modelling (as opposed to dilated convolutions). The main motivation for the authors to do this is saving computation time while keeping the multiscale flexibility of the model. The authors also introduce shortcut connections to compensate for the potential loss of information as they perform downsampling. Again, I feel that this modification not particularly revolutionary. Multiscale image analysis with autoregressive generative models has been done for example in (Theis et al, 2012) and several other papers.\n\nOverall I felt that this submission falls short on presenting substantially new ideas, and reads more like documentation for a particular implementation of an existing idea.", "title": "Comparison to non-discretized distributions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1DF5VFEg": {"type": "review", "replyto": "BJrFC6ceg", "review": "1. Do you have any experiments where the discretization is dropped?\n2. Why the particular choice of logistic distributions? How do the properties compare to the more flexible family of Gaussian scale mixtures which have been used successfully in very similar work e.g. (Theis et al. 2012) http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0039857 Is it mainly/only about ease of evaluation?Apologies for the late submission of this review, and thank you for the author\u2019s responses to earlier questions.\n\nThis submission proposes an improved implementation of the PixelCNN generative model. Most of the improvements are small and can be considered as specific technical details such as the use of dropout and skip connections, while others are slightly more substantial such as the use of a different likelihood model and multiscale analysis. The submission demonstrates state-of-the-art likelihood results on CIFAR-10.\n\nMy summary of the main contribution:\nAutoregressive-type models - of which PixelCNN is an example - are a nice class of models as their likelihood can be evaluated in closed form. A main differentiator for this type of models is how the conditional likelihood of one pixel conditioned on its causal neighbourhood is modelled:\n\n- In one line of work such as (Theis et al, 2012 MCGSM, Theis et al 2015 Spatial LSTM) the conditional distribution is modelled as a continuous density over real numbers. This approach has limitations: We know that in observed data pixel intensities are quantized to a discrete integer representation so a discrete distribution could give better likelihoods. Furthermore these continuous distributions have a tail and assign some probability mass outside the valid range of pixel intensities, which may hurt the likelihood.\n- In more recent work by van den Oord and colleagues the conditional likelihood is modelled as an arbitrary discrete distribution over the 256 possible values for pixel intensities. This does not suffer from the limitations of continuous likelihoods, but it also seems wasteful and is not very data efficient.\n\nThe authors propose something in the middle by keeping the discretized nature of the conditional likelihood, but restricting the discrete distribution to ones whose CDF that can be modeled as a linear combination of sigmoids. This approach makes sense to me, and is new in a way, but it doesn\u2019t appear to be very revolutionary or significant to me.\n\nThe second somewhat significant modification is the use of downsampling and multiscale modelling (as opposed to dilated convolutions). The main motivation for the authors to do this is saving computation time while keeping the multiscale flexibility of the model. The authors also introduce shortcut connections to compensate for the potential loss of information as they perform downsampling. Again, I feel that this modification not particularly revolutionary. Multiscale image analysis with autoregressive generative models has been done for example in (Theis et al, 2012) and several other papers.\n\nOverall I felt that this submission falls short on presenting substantially new ideas, and reads more like documentation for a particular implementation of an existing idea.", "title": "Comparison to non-discretized distributions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1IZCB0Ge": {"type": "rebuttal", "replyto": "BJrFC6ceg", "comment": "Was the class conditional log-likelihood score marginalized over the 10 classes? (Section 3.2)", "title": "Conditional likelihood score"}, "rJjq2-RMx": {"type": "review", "replyto": "BJrFC6ceg", "review": "Did you try different ordering (ex. starting from the bottom right corner instead and going upward and leftward)? If so was the results similar?# Review\nThis paper proposes five modifications to improve PixelCNN, a generative model with tractable likelihood. The authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments. They also reported a new state-of-the-art result on CIFAR-10.\nImproving generative models, especially for images, is an active research area and this paper definitely contributes to it.\n\n\n# Pros\nThe authors motivate each modification well they proposed. They also used ablation experiments to show each of them is important.\n\nThe authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub-pixel instead of a 256-way softmax. This allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub-pixel values. The authors also mentioned it speeded up training time (less computation) as well as the convergence during the optimization of the model (as shown in Fig.6).\n\nThe authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model. This allows them to have a simplified architecture where you don't have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location.\n\n\n# Cons\nIt is not clear to me what the predictive distribution for the green channel (and the blue) looks like. More precisely, how are the means of the mixture components linearly depending on the value of the red sub-pixel? I would have liked to see the equations for them.\n\n\n# Minor Comments\nIn Fig.2 it is written \"Sequence of 6 layers\" but in the text (Section 2.4) it says 6 blocks of 5 ResNet layers. What is the remaining layer?\nIn Fig.2 what does the first \"green square -> blue square\" which isn't in the white rectangle represents?\nIs there any reason why the mixture indicator is shared across all three channels?", "title": "Traversal Ordering", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1wgPCMVx": {"type": "review", "replyto": "BJrFC6ceg", "review": "Did you try different ordering (ex. starting from the bottom right corner instead and going upward and leftward)? If so was the results similar?# Review\nThis paper proposes five modifications to improve PixelCNN, a generative model with tractable likelihood. The authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments. They also reported a new state-of-the-art result on CIFAR-10.\nImproving generative models, especially for images, is an active research area and this paper definitely contributes to it.\n\n\n# Pros\nThe authors motivate each modification well they proposed. They also used ablation experiments to show each of them is important.\n\nThe authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub-pixel instead of a 256-way softmax. This allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub-pixel values. The authors also mentioned it speeded up training time (less computation) as well as the convergence during the optimization of the model (as shown in Fig.6).\n\nThe authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model. This allows them to have a simplified architecture where you don't have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location.\n\n\n# Cons\nIt is not clear to me what the predictive distribution for the green channel (and the blue) looks like. More precisely, how are the means of the mixture components linearly depending on the value of the red sub-pixel? I would have liked to see the equations for them.\n\n\n# Minor Comments\nIn Fig.2 it is written \"Sequence of 6 layers\" but in the text (Section 2.4) it says 6 blocks of 5 ResNet layers. What is the remaining layer?\nIn Fig.2 what does the first \"green square -> blue square\" which isn't in the white rectangle represents?\nIs there any reason why the mixture indicator is shared across all three channels?", "title": "Traversal Ordering", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}