{"paper": {"title": "Optimization on Multiple Manifolds", "authors": ["Mingyang Yi", "Huishuai Zhang", "Wei Chen", "Zhi-ming Ma", "Tie-yan Liu"], "authorids": ["yimingyang17@mails.ucas.edu.cn", "huishuai.zhang@microsoft.com", "wche@microsoft.com", "mazm@amt.ac.cn", "tie-yan.liu@mircosoft.com"], "summary": "This paper introduces an algorithm to handle optimization problem with multiple constraints under vision of manifold.", "abstract": "Optimization on manifold has been widely used in machine learning, to handle optimization problems with constraint. Most previous works focus on the case with a single manifold. However, in practice it is quite common that the optimization problem involves more than one constraints, (each constraint corresponding to one manifold). It is not clear in general how to optimize on multiple manifolds effectively and provably especially when the intersection of multiple manifolds is not a manifold or cannot be easily calculated. We propose a unified algorithm framework to handle the optimization on multiple manifolds. Specifically,  we integrate information from multiple manifolds and move along an ensemble direction by viewing the information from each manifold as a drift and adding them together. We prove the convergence properties of the proposed algorithms. We also apply the algorithms into  training neural network with batch normalization layers and achieve preferable empirical results.", "keywords": ["Optimization", "Multiple constraints", "Manifold"]}, "meta": {"decision": "Reject", "comment": "The paper describes a constrained optimization strategy for optimizing on an intersection of two manifolds.  Unfortunately, the paper suffers from generally weak presentation quality, with the technical exposition seriously criticized by two out of the three reviewers.  (The single positive review is too short and devoid of content to be taken seriously.  Even there, concerns are expressed.) This paper requires substantial improvement before it could be considered for publication."}, "review": {"ByxNEz2p27": {"type": "review", "replyto": "HJerDj05tQ", "review": "The paper proposes a novel algorithm for optimization on multiple manifolds. The moving direction fuses gradient information from each manifolds via correlation. More importantly, the convergence is guaranteed.\n\nHowever, the empirical results seems not very good compared to SGD.\n\nMy concerns: \n\n1) How you ensure each step is descent? \n\n2) How is the performance of the proposed algorithm compared to the ADMM which is well-suited for this problem.\n\nThe presentation needs to be improved.", "title": "a novel algorithm on optimization on multiple manifolds", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hye2YPOu37": {"type": "review", "replyto": "HJerDj05tQ", "review": "\nThe title is misleading, since only two particular manifolds are studied in this work. In addition, the proposed methods cannot be applied to a larger or a general class of manifolds. Therefore, you should update the title.\n\nThere are multiple problem definitions proposed in the paper. They are not compatible with each other and also with the proposed methods. In addition, some of the proposed problem definitions are incorrect, as explained below:\n\nYou should be more precise about the definition of the manifold you consider in this paper. For example, in equation (1), please define your manifold of interest more precisely checking some standard textbooks.\n\nPlease define intersection of manifolds, what do you mean by which intersection of which type of manifolds?\n\nIn the contribution (1); the paper does not introduce an algorithm to deal with optimization on with multiple manifolds, but for a particular type of individual manifolds.\n\nIn the contribution (2): It is not clear why and how the proposed method can be applied to optimization on manifolds with momentum (what do you mean by use of momentum here?), and regularization (what do you mean by regularization?). There are many problems with this claim, but you can simply consider that applying momentum and regularization will affect the geometry of loss landscape.\n\nDefinition of retraction is not precise, please fix it.\nWhat is L in equation (2)?\n\nPlease define neighborhood in U_x in Lemma 2.1.\n\nWhat is || ||in Lemma 2.1?\n\nAs you also noticed on page 3, an intersection of manifolds may not be a manifold. Then, your proposed first problem (1) fails. Therefore, you should completely change your claims on your problem definitions and contributions.\n\nWhat do you mean by \u201cWe add a drift which contains information from the other manifold to the original gradient descent on manifold\u201d? What is \u201cthe information from the manifold\u201d? In equations (3) and (4), you just apply optimization on manifolds individually. \n\nHow do you compute/determine a_k^(1) and a_k^(2)? How do they affect the theoretical and experimental results?\nIn your claim \u201cFrom the construction of bk, we can see that the smaller the correlation between gradf(xk) and hk is, the smaller effect the information from M2 brings\u201d, it is not clear how \u201cthe information from M2\u201d affects? First, again, what is \u201cthe information\u201d? Second, b_k^(1) and b_k^(2) are computed for individual manifolds separately. Then, how \u201cthe information\u201d make an effect?\n\n In Theorem 2.2, what do you mean by \u201cthen xk convergence to a local minimizer\u201c?\n\nWhat is <,> in Theorem 2.2?\n\nWhat is ^ in Theorem 2.3?\n\nWhat is v in proof 6?\n\nWhat is an engine value?\n\nWhat does P (1) xk gradf(yk) denote in computation of h_k? For example, gradf(yk) is a vector on tangent space of the second manifold at yk. Then, how do you project orthogonally this projected vector to the tangent space of the first manifold at xk? \n\nThey may be completely different geometries, and such an \u201corthogonal projection\u201d may not exist in general. Then, how do you compute and calculate that projection?\n\nAll the theoretical results given in the paper are not about convergence of parameters on a manifold at the intersection or product of manifolds but for an individual manifold. For example, x and y belong to manifolds M1 and M2, and convergence results is about x. How are they related to parameters at the intersection or product of manifolds?\n\nThe statements regarding batch normalization are confusing and also sound incorrect:\n\nDo you apply batch normalization on weights on BN(w)?\n\nPlease explain what you mean by \u201cBN(w) has same image space on G(1, n) and St(n, 1)\u201c. There are not such results in the papers Cho & Lee (2017); Huang et al. (2017) you cited for these results.\n\nWhat do you mean by \u201capplying optimization on manifold to batch normalization problem\u201d?\n\nIn your statement \u201cHowever, the property of these two manifold implies that we can actually optimize on the intersection of two manifolds\u201d. Please explain how does this property imply this result more precisely?\n\nPlease define \u201cGrassmann manifold G(1, n)\u201c more precisely. In your notation, together with explanation of the notation for St(n,p), G(1,n) is like a set of 1xn dimensional row vectors, while St(n,1) is an nx1 dimensional column vector, Then, their intersection is an empty set and your proposal for optimization on a vector on their intersection is wrong. \n\nNotation and definitions used in (9) are wrong and confusing. Please check and revise them.\n\nIn the whole paper, the problem, method, solutions, theorems, and contributions are proposed for optimization using parameters which belong to intersection of some manifolds. Then, suddenly, you start considering optimization on product manifolds, and give the results for that;\n\nWhat does the statement \u201cThen we apply Algorithm 1 to update parameters, which means we optimize on a product manifold\u201d mean?\n\nWhat do \u201cG(1, k1) \u00d7 \u00b7 \u00b7 \u00b7 G(1, kn)\u201d and \u201cSt(k1, 1) \u00d7 \u00b7 \u00b7 \u00b7 St(kn, 1)\u201d denote?\n\nDon\u2019t you perform optimization on intersection of manifolds? Why do you ignore your original problem and methods, and consider this problem? \n\nIn addition, how do you use your Algorithm 1 for optimization on product manifolds? Optimization on intersection on manifolds and product manifolds are completely different problems. If they are same or related to each in particular cases in your specific definitions, then you should provide these definitions more precisely.\n\nWhat do you mean by optimization on product manifold of weights of all layers? If you compute a product manifold for spaces of all layers, then you simply perform a shallow optimization on a huge matrix containing millions of dimensions according to this definition. First, how do you do that? Second, how can you train a large network using this approach?\n\nIn the experiments, please first give variance of errors. These results are statistically insignificant.\n\nWhich problem is solved to perform these experiments is not also clear (see above).\n\nThe results reported in the paper are also not good, may be due to the mathematical and algorithmic  problems and errors mentioned above. Please clarify them, and provide additional results, especially using other datasets (small scale mnist and large scale imagenet), and networks (mlp, vgg, resnet etc.)\n\nRelated work is also incomplete, such that many traditional and recent work on optimization on multiple manifolds are omitted. ", "title": "The problems addressed in the paper are interesting and crucial from both practical and theoretical perspectives. However, there are various major mathematical, conceptual and algorithmic problems with this paper.", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "B1l_Mpupo7": {"type": "review", "replyto": "HJerDj05tQ", "review": "This paper considers an optimization problem defined on the intersection of multiple manifolds and the intersection is not a manifold. An optimization algorithm is proposed and its convergence analysis is given. An experiment of neural network with batch normalizatiion is used to demonstrate the performance of the algorithm.\n\nThe problem considered in this paper is interesting. However, there are quite a few fundamental errors about Riemannian optimization. In addition, the convergence analysis is not complete. See more details below. Therefore, I donot think this paper can be published at this stage.\n\n*) P2, Section 2.1, line 2: The statement \"A manifold is a subspace of R^n$ is not true in general.\n*) P2, Section 2.1, line 7: The statement \"manifold is not a linear space\" is not true in general. A manifold can be a linear space, such as the vector space R^n.\n*) P2, Section 2.1, below (2): The statement \"Riemannian gradient is the orthogonal projection of gradient \\nabla f(x) ...\" is not true in general.\n*) P3, (3) and (4): what is the definition of $h_k^{(1)}$ and $h_k^{(2)}$. Are they arbitrary or the ones given on Page 4?\n*) P4, Theorem 2.3: the iterates {x_k} converges in the sense that \\|gradf(x_k)\\| goes to 0. Does {x_k} go to the intersection of the two manifolds \\mathcal{M}_1 and \\mathcal{M}_2? To complete the proofs, the author may need to show that \\|gradf(y_k)\\| goes to 0 and {x_k} and {y_k} have the same limit.\n*) P5, the grassmann manifold with p = 1: G(1, n), is called projective space, and the Stiefel manifold with p = 1: St(n, 1) is called the unit sphere.\n*) P6, the discussion of the intersection of G(1, n) and St(n, 1) does not make sense to me. G(1, n) is a quotient manifold, which is not a submanifold of R^n. Given a quotient manifold, the typical way in optimization framework is to choose representation of the quotient manifold. Fortunately, the projective space has a global orthogonal section, which is the unit sphere. In other words, G(1, n) is diffemorphisic to the unit sphere St(n, 1), and even can be isometric if appropriate Riemannian metrics are used on G(1, n) and St(n, 1). Therefore, I don't understand the notion of the intersection of G(1, n) and St(n, 1).\n\n\n", "title": "The problem considered in this paper is interesting. However, there are quite a few fundamental errors about Riemannian optimization. Therefore, I donot think this paper can be published at this stage.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}