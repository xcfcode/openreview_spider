{"paper": {"title": "Black-Box Optimization Revisited: Improving Algorithm Selection Wizards through Massive Benchmarking", "authors": ["Laurent Meunier", "Herilalaina Rakotoarison", "Jeremy Rapin", "Paco Wong", "Baptiste Roziere", "Olivier Teytaud", "Antoine Moreau", "Carola Doerr"], "authorids": ["~Laurent_Meunier1", "~Herilalaina_Rakotoarison1", "jrapin@fb.com", "paco.pkwong@gmail.com", "broz@fb.com", "~Olivier_Teytaud2", "antoine.moreau@uca.fr", "~Carola_Doerr1"], "summary": "We propose a huge benchmark aggregating many well known benchmarks and derive an algorithm selection tool on it.", "abstract": "Existing studies in black-box optimization for machine learning suffer from low\ngeneralizability, caused by a typically selective choice of problem instances used\nfor training and testing different optimization algorithms. Among other issues,\nthis practice promotes overfitting and poor-performing user guidelines. To address\nthis shortcoming, we propose in this work a benchmark suite, OptimSuite,\nwhich covers a broad range of black-box optimization problems, ranging from\nacademic benchmarks to real-world applications, from discrete over numerical\nto mixed-integer problems, from small to very large-scale problems, from noisy\nover dynamic to static problems, etc. We demonstrate the advantages of such a\nbroad collection by deriving from it Automated Black Box Optimizer (ABBO), a\ngeneral-purpose algorithm selection wizard. Using three different types of algorithm\nselection techniques, ABBO achieves competitive performance on all\nbenchmark suites. It significantly outperforms previous state of the art on some of\nthem, including YABBOB and LSGO. ABBO relies on many high-quality base\ncomponents. Its excellent performance is obtained without any task-specific\nparametrization. The benchmark collection, the ABBO wizard, its base solvers,\nas well as all experimental data are reproducible and open source in OptimSuite.", "keywords": ["black-box optimization", "mujoco", "wizard", "benchmarking", "BBOB", "LSGO"]}, "meta": {"decision": "Reject", "comment": "This paper presents a benchmarking suite, primarily targeting the domain of evolutionary style optimization algorithms, and an effective heuristic algorithm selection procedure ABBO.  The reviewers seemed quite split in their reviews with significant variance, particularly with one outlier review (9) lifting up the average.  They all felt that there was significant value in the work presented and that the benchmark could be useful for designing and evaluating new methods.  However, there were concerns regarding details about the contributions (e.g. a detailed description of ABBO and which contributions to the suite were novel vs obtained from other benchmarks), the relevance of this work to the ICLR community, and choice of algorithms presented (i.e. not SOTA).  \n\nIn general, this seems like a useful contribution for the evolutionary algorithm community but this paper seems off-topic from the conference.  Certainly optimization is important and of interest to the community.  However, there is no machine learning component to the technical contribution of this paper, and it ignores many of the contributions to black-box optimization within this community (see e.g. the citations from AnonReviewer1, and the literature on surrogate-based black-box optimization - i.e. Bayesian optimization).  The RL optimization problems are somewhat relevant, but AnonReviewer1 raises concerns about the reporting of those results and the representation of the current literature.  There is an algorithm proposed in this work, but it's largely heuristic and no comparison is given to state-of-the-art portfolio optimization algorithms from the machine learning community (e.g. P3BO from Angermueller et al., ICML 2019).  A venue such as GECCO seems much more well suited to this work."}, "review": {"mp8Dm6QIXJm": {"type": "review", "replyto": "4D4Rjrwaw3q", "review": "The paper proposes a benchmark suite for black-box optimization that covers more\ndifferent types of problems than existing benchmarks. They derive an algorithm\nselection system for black-box optimization from it and evaluate its performance\nempirically, comparing to other black-box optimization solvers.\n\nThe authors address an interesting problem and demonstrate some good empirical\nresults. It is certainly beneficial to have large benchmark suites to get a\ncomprehensive picture of the performance of different approaches.\n\nWhile the general motivation for a new benchmark suite is clear, the specific\nproperties the authors list are not. Some of the properties in Table 1 seem to\nbe complementary (far-optimum and translation), while it is unclear why others\nare important. It is certainly nice to have an automated dashboard (of what\nexactly?) and one-line reproducibility, but within the general context of the\npaper, which seems to focus more on generalizability of results, this seems\nunimportant. Whether a particular set of benchmarks is complex is to some extent\na question of the definition of \"complex\", which the authors do not make clear.\nI do not know what \"human-excluded/client-server\" means. While the majority of\nproperties listed in Table 1 are obviously important for black-box optimization,\nit seems that the authors started from the properties that their benchmark suite\nhas.\n\nThe benchmarks themselves seem to be mostly existing benchmarks that were\ncombined into a new collection -- it would help if the authors pointed out what\nbenchmarks were specifically created for OptimSuite.\n\nIt is unclear how exactly ABBO was created -- was the set of rules determined\nover all benchmark instances to maximize a performance metric? ABBO is listed as\none of the main contributions of the paper and it should be explained in more\ndetail how it was created and how it works.\n\nThe labels in the figures are too small.\n\nUpdate after rebuttal period: Thank you for your clarifications. I have revised my score accordingly.", "title": "Promising approach with issues", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "sxoPhq8lF18": {"type": "rebuttal", "replyto": "4D4Rjrwaw3q", "comment": "Dear all, \nWe have just submitted a carefully revised version of our work. The revision addresses, in particular, the comments of Reviewers 1 and 2 (other comments were already addressed in our previous revision). We thank all reviewers for their constructive feedback, which has helped us to improve the presentation considerably. In particular, we agree that Table 2 is a nice way to aggregate the results from the plots (which are now available in the appendix). \nWe remain at your disposal for further discussions. \nBest wishes, \nthe author(s)", "title": "New version uploaded"}, "9mP_gZbz6_6": {"type": "rebuttal", "replyto": "lNkh8wBHazu", "comment": "We hope that the following explanation clarifies the properties listed in Table 1. We write these developments in the appendix.\n- Large scale: includes dimension > 1000.\n- Translations: in unbounded continuous domains, a standard deviation has to be provided, for example for sampling the first and second iterates. Given a standard deviation std, we consider that there is translations when optimas are randomly translated by a N(0,std^2) shift. Only interesting for artificial cases. \n- Far-optimum: optima are translated far from the optimum, with standard deviation at least N(0, 25std^2).\n- Symmetrizations / rotations: rotation: with a random rotation matrix M, the function f can be replaced by RotatedF(x) = f(M(x)). Symmetrization: f(x) can be replaced by f(S(x)), with S(x) a diagonal matrix with each diagonal coefficient equal to 1 or -1 with probability 50% if the optimum is at 0. \n- One-line reproducibility: Where reproducibility requires significant coding, it is unlikely to be of great use outside of a very small set of specialists. One-line reproducibility is given when the effort to reproduce an entire experiment does not require more than the execution of a single line. We consider this to be an important feature. \n- Periodic automated dashboard: are algorithms re-run periodically on new problem instances? Some platforms do not collect the algorithms, and reproducibility is hence not given. An automated dashboard is convenient also because new problems can be added \u201con the go\u201d without causing problems, as all algorithms will be executed on all these new problem instances. This feature addresses what we consider to be one of the biggest bottlenecks in the current benchmarking environments.  \n- Complex or real-world: Real-world is self-explanatory; complex means a benchmark involving a complex simulator, even if it is not real world. MuJoCo is in the \u201ccomplex\u201d category.\n- Multimodal: whether the suite contains problems for which there are local optima which are not global optima. \n- Open sourced / no license: Are algorithms and benchmarks available under an open source agreement. BBOB does not collect algorithms, MuJoCo requires a license, LSGO and BBOB are not realworld, Mujoco requires a license, BBComp is no longer maintained, Nevergrad before OptimSuite did not include complex ML problems without license issue before our work: some people have applied Nevergrad to MuJoCo but with our work MuJoCo becomes part of Nevergrad so that people can upload their code in Nevergrad and it will be run on all benchmarks, including MuJoCo.\n- Ask/tell/recommend correctly implemented: The paper \u201cOn Object-Oriented Programming of Optimizer - Examples in Scilab\u201d by Y. Colette et al. develops arguments in favor of the ask and tell format. The idea is that an optimization algorithm should not come under the format Optimizer.minimize(objective_function) because there are many settings in which this is not possible: you might think of agents optimizing concurrently their own part of an objective function, and problems of reentrance, or asynchronicity. All settings can be recovered from an ask/tell optimization method. This becomes widely used. However, as well known in the bandit literature (you can think of pure exploration bandits, as in Bubeck et al. 2009), it is necessary to distinguish ask, tell and recommend: the recommend method is the one which proposes an approximation of the optimum. Let us develop an example explaining why this matters: the domain is {1,2,3,4}, and we have a budget of 20. NoisyBBOB assumes that the optimum is found when \u201cask\u201d returns the optimum arm: no matter what happens afterwards, the status remains \"found\" for ever as soon as the optimum has been asked. So an algorithm which just iteratively \u201casks\u201d 1,2,3,4,1,2,3,4,.... reaches the optimum in at most 4 iterations. This does not mean anything, as the challenge is to figure out which of the four numbers is optimal. With a proper ask/tell/recommend, the optimizer chooses an arm at the end of the budget. A simple regret is then computed. Actually this also matters in the noise-free case, but the issue is more critical in noisy optimization. The case of continuous noisy optimization also has counter-examples and all the best noisy optimization algorithms use ask/tell/recommend.  We add the reference to the paper above.\n- Human excluded / client-server: The problem instances are truly black-box. Algorithms can only suggest points and observe function values, but neither the algorithm nor its designer have access to any other information about the problem apart from the number of variables, their type, ranges, and order. It is impossible to repeat experiments for tuning hyperparameters without \u201cpaying\u201d the budget of the HP tuning. This is something we could not do, as everything is public and open sourced: however, we believe that we mitigate this issue by considering a large number of benchmarks.\n\n(new version of the paper to be submitted soon)", "title": "Properties of benchmarks"}, "C2Jj2y8aHbn": {"type": "rebuttal", "replyto": "snK4Y9jiOlT", "comment": "(new version of the paper to be submitted soon)\n\n\nRelevance: We have added reference to ICLR & NeurIPS papers, about MuJoCo. We include several machine learning tasks which are new in Nevergrad (MuJoCo, Keras, Scikit-Learn, 007). ABBO significantly outperforms Shiwa which was published at GECCO and is shown in our plots. \nHyperparameter tuning and reinforcement learning are usual in machine learning conferences. LMRS was published at ICLR. We claim the best really entirely open sourced results on MuJoCo: in addition our work makes it possible for people to play with MuJoCo by uploading their code if they do not have the license, as Nevergrad periodically reruns everything and our work is integrated in Nevergard.\nOur references include 3 ICML, 3 NeurIPS, 1 ICLR (it is true that there was no ICLR paper before your comment). We do believe that the fact that this work combines references from mathematical programming and evolutionary computation and uses a standard platform from derivative-free optimization is a strength.\n\nHighlighting new task and risk of competing with Nevergrad:\nWe totally agree that it is preferable for our community to have one common interface. This is why we have chosen to integrate all our work in Nevergrad. This way, the functionalities are identical, the algorithms from Nevergrad can be used, and we maintain all strong features of Nevergrad. Please note that our work is a temporary fork of Nevergrad. A large part of OptimSuite has already been merged in Nevergrad, and the remainder is currently under code review. Our fork will be deleted soon, we do not want to compete with Nevergrad: we want to contribute to it. The fact that MuJoCo (and all the other suites mentioned in the paper) are now accessible through Nevergrad is one of our contributions - they were not there before the present work. \n\nABBO handcrafted: We agree that an automatic learning would be more elegant. This is not yet ready, unfortunately. In terms of interpretability, however, we point out that rules might be more interpretable than something automatically learnt on traces of previous runs.\nWe point out that the fully automated nature of ABBO has a strong advantage: we do not run the benchmark plenty of times for tuning hyperparameters for each task specifically. When the dashboard of Nevergrad is created, all algorithms are run, with the same HP - or if the algorithms do something for modifying the HP, their evaluations for doing so is counted in the budget.\nAlgorithm selection is standard in other areas of optimization: we believe it should become standard in black-box optimization.\n\nUnclear plots & appendices: Thanks: we will modify this accordingly. We will upload a new version of the paper in the next days.\n\nMujoco experiments: We agree that this table is misleading. We modify accordingly. Yes, we work in the linear policy setting. We will make it clear our comparisons are for Linear policies in the blackbox setting.\n\nBayesian optimization tools in Nevergrad: We use the Bayesian optimisation chosen by Nevergrad. It is the package https://pypi.org/project/bayesian-optimization/. It uses UCB by default (https://github.com/facebookresearch/nevergrad/blob/master/nevergrad/optimization/optimizerlib.py#L1552). We add the reference. Recently HyperOpt has been added. We agree that Turbo would be a great addition. We also add the reference. Importantly, we do not claim that Bayesian optimization does not work -- the results are just those of one Bayesian optimization library.\n\n", "title": "answer to reviewer 1"}, "lDUQL2DGyEc": {"type": "review", "replyto": "4D4Rjrwaw3q", "review": "\n** Summary **\n\nThis paper proposes a new benchmark suite for black box optimization algorithms, which contains a mixture of existing tasks such as nevergrad functions and RL tasks from MuJoCo. In addition, the authors propose a new automated approach to algorithm selection (ABBO), which uses a series of rules to select the best method.\n\n** Primary Reason for Score**\n\nThe benchmark suite is predominantly an amalgamation of existing tasks, and only appears to be an incremental improvement vs. Nevergrad. The proposed meta-algorithm is built on a set of \u201chand-crafted selection rules\u201d (in the authors\u2019 own words). This does not seem like a meaningful contribution to the ICLR community, but may be interesting in other, more applied venues. \n\n** Strengths **\n\n1) The proposed OptimSuite may be more convenient for users.\n2) The rules-based system works surprisingly well, which begs the question of whether a learned version could perform better, which may be of interest to the ICLR community.  \n\nPOST REBUTTAL:\n3) Including all benchmarks into one codebase could improve reproducibility.\n\n** Weaknesses **\n\n1) I don\u2019t think ICLR is the right venue for this work, which seems more engineering focused and may be more suited to an applied venue.\n2) Most of the benchmarks in this suite are all included elsewhere already. Many recent papers proposing blackbox optimization algorithms include several of them. In particular, using Nevergrad + MuJoCo seems to achieve most of the desirable properties. \n3) The selection wizard is just a set of heuristics/hand-engineered rules. This might be more useful for industrial applications, rather than ICLR. \n4) Given that this paper is about benchmarking optimization algorithms, the presentation of the results is poor. It is almost impossible to read the plots, and there is no central table/comparison of the different methods. Reading this I learn very about the algorithms that are being benchmarked.\n5) I have several issues with the presentation of the RL results:\na) ARS is not even a blackbox optimization algorithm, it uses information about the MDP for state and reward normalization (v2-t). \nb) The \u201cSOTA\u201d results have been copy and pasted from another paper, which pasted them from the ARS paper, which referenced 2017 results. These are nowhere near \u201cSOTA with grad\u201d in RL. In fact, ARS didn\u2019t say they were SOTA, it simply said that was what TRPO got as a baseline. SOTA now is probably MBPO or SAC/TD3, all of which solve these tasks in a fraction of the time.\nc) Missing relevant literature: there have been several approaches to blackbox optimization for RL (and other functions) since ARS. I included ones presented either at ICLR or similar venues, which should be discussed:\n  i) **Gradientless Descent: High-Dimensional Zeroth-Order Optimization**. Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, Qiuyi (Richard) Zhang. *ICLR 2020*.\n  ii)  **Learning to Guide Random Search**. Ozan Sener, Vladlen Koltun. *ICLR 2020*.\n  iii)  **From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization**. Krzysztof M. Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Vikas Sindhwani. *NeurIPS 2019*.", "title": "Lacking novelty and relevance to the ICLR community", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "wXicg_zQdzS": {"type": "rebuttal", "replyto": "8_L_0MpvGUY", "comment": "Thank you very much for the encouraging words! And for the constructive comments. An updated version is now available.", "title": "Thank you"}, "3hrRt9yayW": {"type": "rebuttal", "replyto": "SZ7XIsX3BFK", "comment": "Dear reviewer, thanks for the swift acknowledgement that you have seen our answers. The revised version is now also uploaded. Please do not hesitate to post further comments or questions. We thank you for your efforts in reviewing this paper, and for the constructive comments. ", "title": "Revised version now uploaded"}, "-W8QcIPkUkj": {"type": "rebuttal", "replyto": "sKNvGHU33rt", "comment": "Thank you for the encouraging words and for revising the score. We also want to let you know that the revised version is now uploaded. Please do not hesitate to post further comments or questions. We thank you for your efforts in reviewing this paper, and for the constructive comments. ", "title": "Thank you"}, "OICU4vkbgk": {"type": "rebuttal", "replyto": "LdfbE-ezdMm", "comment": "A short message to let you know that the revised version is now uploaded. Please do not hesitate to post further comments or questions. We thank you for your efforts in reviewing this paper, and for the constructive comments. ", "title": "revised pdf now uploaded"}, "cNsoXs6ygv": {"type": "review", "replyto": "4D4Rjrwaw3q", "review": "The paper proposes a benchmarking suite to overcome the problem low generalizability with black box optimization algorithm. The benchmarking suite consists of standard academic benchmarks to real world optimization problems. It also covers several scenarios such as dynamic-static, small to large-scale, discrete to mixed-integer etc. This is a relevant contribution to the machine learning however there are several drawbacks which pushes back it's acceptance into ICLR.\n\n(a) The state of the art discussion was good but why benchmarking is crucial and how it is implemented in Optimsuite was very short in description. That was supposed to be the main highlight of the paper whereas discussion in that part was not clear at all. Specifically, more descriptions were needed in terms of what features to include/exclude in the benchmarking suite and how that helps in generalizability.\n\n(b) I know author(s) mentioned about interpretability as future work however I felt really challenging to understand the benchmarking suite especially when you have a combination of academic benchmarks and real world optimization problems together. I think that's a very significant challenge in implementing ABBO.\n\n(c) One thing was not clear to me how the tuning parameters that are often associated with several optimization problems are handled here? are you keeping the tuning parameter same across all the competing methods?\n", "title": "A good paper but slightly below the acceptance threshold", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "8g6h-yQIChy": {"type": "review", "replyto": "4D4Rjrwaw3q", "review": "SUMMARY\n\nThe contribution of this work is two-fold: it collects an extremely wide range of benchmark problems for black-box optimization, and it proposes a new algorithm called ABBO. Both contributions are significant.\n\nThe proposed ABBO method is designed to be a swiss army knife solver, suitable for a wide range of different types of problems. To this end, naturally, it builds on existing components. Overall, the combination looks extremely convincing to me.\n\n\nCRITICISM\n\nAlready the first sentence of the abstract is problematic:\n\"Existing studies in black-box optimization suffer from low generalizability, caused by a typically selective choice of problem instances used for training and testing different optimization algorithms.\"\nThis statement is very general. However, black-box optimization is an extremely wide area, ranging (at least) from mathematical optimization over evolutionary computation all the way to machine learning. The statement applied to various degrees to most studies in various subfields. I completely agree with the statement only when restricted to machine learning papers, where experiments are typically limited to very few RL benchmarks. In other areas things are far from perfect, but generally much better (it is understood that avoiding a bias completely is near impossible), since benchmarks e.g. with (YA)BBOB aim at general insights, not (only) at demonstrating peak performance. Please qualify this statement accordingly.\n\nThere is one more problematic statement in the abstract:\n\"A single algorithm therefore performed best on these three important benchmarks, without any task-specific parametrization.\"\nWell, this \"single algorithm\" is really an algorithm selection machine. Technically it is \"a single algorithm\", but it is much more useful to think of ABBO as a selection and configuration method. In my understanding this makes a big difference. For optimal performance we need both: powerful components and powerful configurators. Please make absolutely clear that this \"single algorithm\" really is a configurator, which encompasses multiple components.\n\nPersonally I object some of the methods forming the basis of \"Algorithm 1\", although overall the choices look very solid. [Side note: I very much like the use of Powell's algorithms for fine tuning of approximate solutions found with more robust methods.] I have one suggestion: Diagonal CMA-ES is an outdated method. Please consider low-rank approaches as an alternative, like LM-CMA-ES, VD-CMA-ES and LM-MA-ES.\n@inproceedings{loshchilov2014computationally,\n  title={A computationally efficient limited memory CMA-ES for large scale optimization},\n  author={Loshchilov, Ilya},\n  booktitle={Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},\n  pages={397--404},\n  year={2014}\n}\n@inproceedings{akimoto2016projection,\n  title={Projection-based restricted covariance matrix adaptation for high dimension},\n  author={Akimoto, Youhei and Hansen, Nikolaus},\n  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference 2016},\n  pages={197--204},\n  year={2016}\n}\n@article{loshchilov2018large,\n  title={Large scale black-box optimization by limited-memory matrix adaptation},\n  author={Loshchilov, Ilya and Glasmachers, Tobias and Beyer, Hans-Georg},\n  journal={IEEE Transactions on Evolutionary Computation},\n  volume={23},\n  number={2},\n  pages={353--358},\n  year={2018},\n  publisher={IEEE}\n}\n\nI appreciate that the code is available. I understand that you start out by forking nevergrad. However, that's not a viable long-term strategy, at least when thinking in terms of utility for a wider community. Please put effort into merging your (im my opinion very significant) contributions back into nevergrad.\n\nAlgorithm configuration and selection for optimization is not entirely new. In the bbcomp results, the AS-AC-CMA-ES by Nacim Belkhir seems to be a very successful competitor method. I do not know whether the code is available or not -- I found Nacim's profile on github, but no code base corresponding to his competition entries. If possible, it would be very interesting to compare to his results.\n\nThis brings me to one of the few weak points of the paper. Experiments are performed for ABBO, but the authors rely (solely) on the nevergrad leader board for comparing with competitors. This has pros and cons. The huge advantage: results are not biased by running competitor methods with sub-optimal parameters. This is a huge plus; in effect, this is a rare case where I fully trust all experimental results. However, this means that some interesting baselines may be missing, in particular methods that predate nevergrad (which is still rather new), like AS-AC-CMA-ES.\n\n\nMINOR POINTS\n\nThe fonts in all plots in figures 2 to 5 are far too small, in particular when printed. On screen I need to zoom in quite a bit. I understand that there are space constraints, but in this form the presentation of the results is of limited value. \n\nI really do not understand why the machine learning community keeps talking about losses (and sometimes regrets) when it comes to optimization. The term \"fitness\" in evolutionary computation is no better. A long-established terminology exists already: the thing we minimize is an \"objective function\", and its value at a specific point is an \"objective value\". I vote for paying more attention to using the standard terminology (in general, not only in this paper), since it is compatible across multiple sub-communities of optimization.\n\nLast paragraph of section 2: \"Rocket\" is listed twice.\n\nIt seems that some of the URLs in the references do not work (any more). The bbcomp website has moved here: https://www.ini.rub.de/PEOPLE/glasmtbl/projects/bbcomp/index.html\nI did not find a replacement for the Artelys link, but maybe referencing the bbcomp results does the job.\n\n\nRECOMMENDATION\n\nOverall this is a very nice and valuable paper with two significant contributions. I strongly recommend to accept the paper.\n", "title": "Solid paper with valuable contributions", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "0h_qcbaNb7y": {"type": "rebuttal", "replyto": "8g6h-yQIChy", "comment": "Abstract 1: we modify the sentence accordingly, pointing out that it applies to machine learning papers.\nWe agree that there are many areas in which the statement should be different. We do believe, however, that OptimSuite is one of the biggest, in particular in terms of integrating machine learning (with MuJoCo, other RL tasks, Keras hyperparameter tuning, Scikit hyperparameter tuning, black-box adversarial attacks).\n \nAbstract 2: We agree that the statement should be different. We modify accordingly. It is important to acknowledge the importance of base algorithms and stay modest.\n\nReferences: we add these references. A new variant of CMA was recently added but none of those ones, and there is a github issue suggesting this. We agree that these 3 new variants are important.\n\nMerging into Nevergrad: We do it. A large part of OptimSuite is actually already merged, and the rest is already in PR under peer reviewing for code quality. As you can see in the code, there are many other new benchmarks under addition.\n\nNacim's work: We completely agree that AS-AC-CMA-ES is a very interesting part of the state of the art. However, his point is a bit different:\nHe works on per instance configuration, using a part of the budget for computing features. Our work is only based on (i) offline choice based on dimension / type of variables / budget / parallelism and such features available a priori (ii) chaining (running several algorithms in a row) (iii) bet and run, i.e. run several algorithms during e.g. 10% of the budget and then keep only the best.\nWe work on continuous, discrete, noisy, etc forms of optimization.\nTherefore, this work is more complementary than comparable. What we should do is to include AS-AC-CMA-ES as one of our base components. We add this comment in the paper.\n\nComparison with only algorithms from Nevergrad: Agree. However we believe that Nevergrad has one of the biggest set of algorithms and the list is growing. HyperOpt has just been added, DEAP is under work, new discrete evolutionary algorithms were also added, as well as NSGA2 in the multiobjective case. The set of algorithms is ever growing, and our interface makes it easy to contribute new algorithms.\n\nMinor points: Action: we entirely revisit the presentation (using the additional allowed page), add text overviews of the figures. Our results are also available in the leaderboard. We remove all occurrences of \u201cfitness\u201d and \u201closs\u201d  in the paper. We keep regret as it has a specific implication (translation by the optimum value when it is known, and expected value after corruption by noise in the noisy setting) . Other suggestions have been taken into account.", "title": "Answer to reviewer 3"}, "3oFlrQ3x3E_": {"type": "rebuttal", "replyto": "lDUQL2DGyEc", "comment": "Learnt rules: this work is in progress. It is useful in terms of getting rid of human contributions and more principled thresholds in the rules. We provide in the dashboard the CSV files of our results, so that anyone can do experiments without running anything: it is in particular possible to run experiments based on training rules on some benchmarks and validating them on another benchmark.\n\n(1) Relevance for ICLR: we believe in high quality benchmarking for increasing the quality of publications. Our paper contradicts a previous NeurIPS 2020 paper claiming poor performance of CMA or Shiwa on MuJoCo. In our platform we immediately get good results for those algorithms. BBOB and LSGO probably contributed a lot for improving the quality of scientific publication in the evolutionary computation community: we believe our framework goes one step beyond by integrating real-world and machine learning and by unifying many benchmarks. Since the time of submission we have added Keras problems and Scikit-Learn tuning in OptimSuite (see updated PDF for details). The fact that our paper incidentally actually outperforms previous publications on black-box MuJoCo says a lot. Using OptimSuite, in contrast, reduces the possibility for deriving and publishing misleading or biased results. In addition, our setup enforces reproducibility of experiments. We also point out other random unexpected scientific outcomes from this big experimental comparison: the surprising effectiveness of Powell\u2019s algorithm, once its asymptotic regime is reached; the effectiveness of Softmax for transforming a discrete optimization problem into a noisy continuous one; and the relevance of bandits methods for the most challenging continuous cases. \n\n\n(2) Benchmarks already available elsewhere: this is not the case for all benchmarks. In addition, this is a lot of existing benchmarks with a common interface. In addition, we fix various issues. For example, two codes are available for LSGO: we found differences between the two, and follow the correct one. Also, Noisy-BBOB has known drawbacks as discussed in the paper and in various posts on internet: our version YANOISYBBOB corrects these flaws. We have also an interface to Pyomo and a long list of problems: we believe this is the only framework with so many benchmarks. In addition, we have a wide range of optimization methods readily available, and we do recompute periodically all benchmarks. \n\n(3) ABBO: we point out that such rules are now the standard procedure in combinatorial optimization and planning: competitions are won by such methods (except of those which explicitly forbid combinations of preexisting works, such as the SAT competitions). In addition, this actually provides a lot of rigor: the poor results of CMA or Shiwa in the LAMCTS paper published at NeurIPS are probably due to a bad interfacing.  \n\n(4) We increase the size of figures for improving the readability. We add arrows specifying where is ABBO in the plots. For additional readability, please note that Nevergrad\u2019s dashboard is publicly visible online. It contains an explicit ranking for every benchmark. It is available at https://dl.fbaipublicfiles.com/nevergrad/allxps/list.html \nThe paper is not only about benchmarking: we believe that alg. selection should become, in black-box optimization, as central as in combinatorial optim. \n\n(5) We agree that your references are relevant and thank the reviewer for them, but we point out that they either skip some of the MuJoCo tests or use an instance-specific hyperparameter tuning which questions its generality. One of the papers with instance specific HP tuning presents results clearly better than ours. However, the entire budget of the grid search should be included, which would make the figures so much different. In addition, parts of the code are not public. \nPapers (i) and (iii) did not evaluate their methods on the high dimensional problems \u201cAnt\u201d (dim 888) and \u201cHumanoid\u201d (dim 6392): they are the two most difficult problems. The paper (ii) reached the targets on all environments with a very limited budget in appearance. We tried to reproduce their results using the github repo. 1st, a part of the code (gradient variance reduction), is not OS. 2nd, it comes out that a grid search was performed to reach the performance announced in the papers, hence biasing the results. Moreover, the optimal HP are not specified in the paper, and seemingly a different tuning was used for each problem with per-problem choice of HP. Using the default HP given in the repo, we reached the target for Half-Cheetah with a budget ~56k: far higher than our results. For Humanoid with HP from ARS-v2, the target is reached at ~768k -- higher than our max-budget 500k. The reproducibility of our work is one of its strengths since there are no tuning of HP and runs are publicly made available.", "title": "Answer to reviewer 1"}, "T8bsDoCRP5m": {"type": "rebuttal", "replyto": "cNsoXs6ygv", "comment": "(a) why benchmarking is crucial: the example of MuJoCo shows how much a correct interfacing of benchmark with optimization methods is important. The results of CMA or Shiwa are excellent in our paper, whereas the results of CMA and Shiwa are poor in the LAMCTS paper and the setting is quite similar: in addition, we have no task-specific parameter tuning, and even no hyperparemeter tuning at all on the test benchmarks (Section 4.2 presents test benchmarks, not used for designing ABBO), so that our results are very reproducible and unbiased. We have a common interface for all methods and all benchmarks. Also, many papers use per-instance hyperparameter tuning: we have a single method, aggregating many base methods, for many benchmarks. This makes a difference as the massive cost of hyperparameter tuning is typically not displayed in experimental results of previous papers. More on this in answers to other reviewers.\n\nWhat to include in OptimSuite (which is basically the new version of Nevergrad): Our main goal is to have a diverse set of benchmark problems and instances in our test suite, with the ambition to cover many different scenarios met in ML. OptimSuite provides a unique interface to these. However, we also need to balance execution times, accessibility of the benchmarks, quality of the available baselines, difficulty of interfacing the suites with nevergrad, etc. Thus, all in all, our choices were to a large extent driven by human expertise. We also emphasize that we do not consider the design process to be finished. New benchmarks may be added in the future. However, please note that the current selection already makes a huge step forward, compared to the isolated benchmark suites that are currently studied and which hardly allow to compare solvers on more than one suite.\nA contribution of our work is that we fix issues where needed. For example, we found that the two implementations of LSGO differ: our version matches the one which appears to be correct. We have YABBOB as an analogous of BBOB, but we fix known issue of the noisy BBOB testbed (see paper for details and references). We include a correct interface to MuJoCo, so that everyone can run it -- and if you can\u2019t run it for some reason, you can upload your algorithm and the platform will run it for you.\n\nDescription: the code is entirely provided as an anonymized code. The experiments are fully described there, in readable Python and docstrings. A large part is already merged inside Nevergrad.\n\n(b) it is true that, at present, the interpretability of ABBO is questionable. We are working on extensions that automatically learn the rules from the dashboard. For the results reported in this paper, we have manually selected the constants, through expert guessing and trial/error on the training instances. We did not yet perform automated algorithm configuration on top. This will be done, but will require significant effort, given that we aim for very broadly applicable designs. We think providing a CSV file of performances (algorithm, problem) is a step forward to this direction.\n \n(c) We use all optimization algorithms implemented in Nevergrad as base methods. That is, for the time being, we do NOT perform hyper-parameter tuning nor do we modify the algorithms in any other way. Sometimes, several variants of a method are proposed, with different parameters (for example DE has many variants). The only thing we do is the list of rules so that ABBO redirects to the relevant optimization method, and a few chaining as mentioned in the description of ABBO. This was done by looking at results on the training benchmarks. We do not contribute to the tuning of each base method: we use the default as in Nevergrad.\n\nModifications of the paper:\n- Develop justifications of why benchmarking is crucial and mention elements in (a) above.\n- Mention that we do not work on the parameters of the base methods -- this is prior work of contributors to Nevergrad.\n- Mention the learnt counterpart of ABBO as further work (already in progress).\n", "title": "Answer to reviewer 4"}, "buXkNUtbREb": {"type": "rebuttal", "replyto": "mp8Dm6QIXJm", "comment": "Far-optimum means that the algorithms have to go far from the center of the search space to find the optimum, whereas translation means that the optimum has been randomly shifted so that an algorithm can not \u201ccheat\u201d by being biased towards a specific part of the domain. \n\nBenchmarks:\nWe distinguish in the paper benchmarks that were used for specifying rules in ABBO, and benchmarks which were reserved as a test set (e.g. power systems, mujoco, etc.): see the difference between 4.1 and 4.2. Instances of artificial benchmarks are randomized (random translations at least, and random rotations in some cases), so that even for benchmarks in the training set the exact instances used for producing plots were not used for the training. \n\nHow ABBO was created: mostly by human expertise, but also by trial and error on the training benchmarks. The instances of the training artificial testbeds, and the instances of the training testbeds preserved for test were not used for creating the plots. Please see our answers below also for possible extensions towards automated learning and hyper-parameter optimization. \nFor OptimSuite, the following benchmark suites were created by us for the present paper or significantly modified and were never used in previous papers: MLTuning, PowerSystems, SimpleTSP. For the following ones we created interfaces to existing platforms/codes: Rocket, Pyomo (with significant new pieces of code as other Pyomo codes are not used through black-box optimization). For LSGO, we recreated a code, exactly matching one of the existing platforms which is, according to us, the correct one. This is now also specified in the paper, at the end of Sec. 2.\n\nModifications in the paper:\n- We increase the size of figures / labels. \n- We modify the explanation of far-optimum.\n- We explain how ABBO was created.\n- We mention which benchmarks were created specifically for OptimSuite, which is basically the new version of Nevergrad.  \n", "title": "answer to reviewer 2"}}}