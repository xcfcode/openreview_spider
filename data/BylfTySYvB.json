{"paper": {"title": "GATO: Gates Are Not the Only Option", "authors": ["Mark Goldstein*", "Xintian Han*", "Rajesh Ranganath"], "authorids": ["goldstein@nyu.edu", "xh1007@nyu.edu", "rajeshr@cims.nyu.edu"], "summary": "Recurrent neural networks can avoid vanishing gradients by not using all of their hidden state in recurrences, together with a residual structure.", "abstract": "Recurrent Neural Networks (RNNs) facilitate prediction and generation of structured temporal data such as text and sound. However, training RNNs is hard. Vanishing gradients cause difficulties for learning long-range dependencies. Hidden states can explode for long sequences and send unbounded gradients to model parameters, even when hidden-to-hidden Jacobians are bounded. Models like the LSTM and GRU use gates to bound their hidden state, but most choices of gating functions lead to saturating gradients that contribute to, instead of alleviate, vanishing gradients. Moreover, performance of these models is not robust across random initializations. In this work, we specify desiderata for sequence models. We develop one model that satisfies them and that is capable of learning long-term dependencies, called GATO. GATO is constructed so that part of its hidden state does not have vanishing gradients, regardless of sequence length. We study GATO on copying and arithmetic tasks with long dependencies and on modeling intensive care unit and language data. Training GATO is more stable across random seeds and learning rates than GRUs and LSTMs. GATO solves these tasks using an order of magnitude fewer parameters.", "keywords": ["Sequence Models", "Vanishing Gradients", "Recurrent neural networks", "Long-term dependence"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a modification of RNN that does not suffer from vanishing and exploding gradient problems. The proposed model, GATO partitions the RNN hidden state into two channels, and both are updated by the previous state. This model ensures that the state in one of the parts is time-independent by using residual connections. \n\nThe reviews are mixed for this paper, but the general consensus was that the experiments could be better (baseline comparisons could have been fairer). The reviewers have low confidence in the revised/updated results. Moreover, it remains unclear what the critical components are that make things work. It would be great to read a paper and understand why something works and not that something works. \n\nOverall: Nice idea, but the paper is not quite ready yet.\n\n"}, "review": {"BJxeizHRtS": {"type": "review", "replyto": "BylfTySYvB", "review": "This paper proposes a modification of RNN that does not suffer from vanishing and exploding gradient problems. The proposed model, GATO partitions the RNN hidden state into two channels, and both are updated by the previous state. This model ensures that the state in one of the parts is time-independent by using residual connections. The experiments on the long copy and adding tasks, as well as language modeling on the Penn TreeBank dataset show the performance improvement against the basic LSTM and GRU. \n\nThe paper tackles an interesting and challenging problem with a novel approach in sequence modeling. The idea is clear and the paper is well-written. The mathematical insights are well reasoned. \n\nThe proposed method outperforms LSTM and RNN with much fewer number of parameters. However, there is no regularization is used for such big LSTM/GRU models. There is a chance that such a big LSTM/GRU model increased the chance of overfitting, and therefore the performance is low. I would like to see the comparison after adding any common regularization that prevents overfitting across the recurrent connections. \n\nThere are many advanced RNN/LSTMs proposed in recent years [1-5] addressing the vanishing gradient problem. It is hard to judge the quality of the proposed method due to the lack of evaluation/comparisons. This paper needs more intensive evaluations with recent RNN-based methods. For instance based on [6], AWD-LSTM [6] and RHN [4] achieved 52.8 and 65.4 test perplexity scores on the Penn TreeBank dataset respectively. The the best score in this paper is 112.85. \n\n[1] \"Phased LSTM: Accelerating recurrent network training for long or event-based sequences.\" 2016.\n[2] \"Fast-slow recurrent neural networks.\" 2017.\n[3] \"Skip RNN: Learning to skip state updates in recurrent neural networks.\" 2017.\n[4] \"Recurrent highway networks.\" 2017.\n[5] \"Dilated recurrent neural networks.\" 2017.\n[6] \"Regularizing and optimizing LSTM language models.\" 2017\n\nAll experiments are performed with 1 or 2 layers. Hierarchical RNN/LSTM performs much better in sequence learning. Is there any reason authors only showed 1 or 2 layers? How does GATO change with more than 2 layers?\n\nHow do other choices of non-linear functions affect the performance in practice?   \n\n\nTypo\nr -> r_t in Eq. 6\n\n--- \nAfter rebuttal:\nOne of my main concerns, weak baselines and unfair comparisons, was partially answered in the updated paper.  I am not fully convinced by their new comparisons.\nFor instance, authors mentioned that 'RHN and EURNN performed poorly because they have unbounded forward propagation'. To overcome this, they introduced 'Bounded RHN' in Append G and it performs similarly to GATO and GRU. However, this 'Bounded RHN' is the one used in the original RHN paper. Overall, it is hard to trust their additional comparisons. \nAlthough, I believe that this paper is well-structured and justified. Also it has high potential for the community. However, the paper itself is not ready to be published.\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 4}, "SJxnjAcnoB": {"type": "rebuttal", "replyto": "BylfTySYvB", "comment": "Hello Reviewers and AC,\n\nWe have noticed that our submission's plots render slowly when viewing the PDF in some browsers. For us, it views OK in Mac Preview, but is slow on Chrome browser.\n\nApologies for the trouble. Thank you!", "title": "Plots in PDF Rendering Slowly in Browser"}, "Syxg9BPojH": {"type": "rebuttal", "replyto": "SJxmtISTFS", "comment": "> [Difference between GATO, SRU/FRU, and RHN]\n\nThank you for these references.\n\nThe difference between SRU with alpha=1 and GATO with respect to the identity Jacobian is subtle. In SRU, if alpha^j = 1, then the hidden state mu^j for that alpha has an identity Jacobian [d mu^j_t / d mu^j_t-1]. But in this case, mu^j stays constant for all t and cannot be used to capture long-term dependencies. When alpha^j does not equal 1, the Jacobian is not an identity matrix. In GATO, the hidden state is broken in h=[r,s]. Though [ds_t / ds_t-1] is an identity matrix, s is updated based on r and x and can be used to capture long term dependencies. Therefore GATO is not a special case of SRU. We have added this discussion to new Appendix Section I.\n \nFRU is a follow-up work to SRU. Each hidden state h_t in FRU depends on h_{t-1} so it does not have the identity matrix Jacobian.\n\nGATO is also different from Recurrent Highway Network. The s_t in GATO does use a skip-connection/residual structure. But the residual part depends only on r_{t-1} and x_t, not on s_{t-1}. This novel residual update renders the identity matrix Jacobian. The highway network does not have this special residual update. \n\n>[Compare with other models that address vanishing gradients such as URNN and SRU]\n\nThanks for the suggestion. We have included comparisons against other recently proposed RNNs that address the vanishing gradient issue: RHN, EURNN (Jing, 2017), and SRU. RHN and EURNN perform poorly and NAN because they have unbounded forward propagation. SRU performs well on MIMIC but not on other tasks. \n\nWe fix RHN\u2019s performance with one principle from our paper. See the Penn TreeBank result.", "title": "Thank You For Your Feedback"}, "B1lAmSwojH": {"type": "rebuttal", "replyto": "SygdaoLEtr", "comment": ">[Explore other periodic functions for decoder]\n\nIn new Appendix B, we observe that sin has similar performance. We believe our criteria are necessary, but not necessarily sufficient. We believe finite sums of cos/sin may work well too.\n\n>[Experiment with different proportion of \u201cpassive\u201d variables in GATO]\n\nIn revised Appendix A, we explore using no passive variables and 1/4 instead of 1/2 of the state. Our findings are that Add and Penn TreeBank suffer with less passive variables. This suggests that having a larger fraction of the state devoted to long-term gradient propagation is important.\n\nExploring fully-interacting GATO is a worthwhile direction.  We believe it is not necessary for the Adding and Copying tasks. For the real data tasks, it would be interesting to study a more powerful model (with interaction) combined with regularization (like recurrent dropout).\n\n>[MIMIC is a seemingly private database of vitals]\n\nSorry for the incomplete information. MIMIC-III is a publicly accessible critical care database.\n\n>[Results do not go beyond \u201cours is better\u201d. Look more closely at what makes the difference]\n\nIn addition to better accuracy and perplexity, we emphasize stability across learning rates and seeds. As mentioned, we added new experiments on varying GATO\u2019s non-linearities and proportion of \u201cpassive\u201d variables.\n\n>[Compare against GRU/LSTM with diagonal weight matrices]\n\nThank you for this suggestion. As mentioned, we have added diagonal LSTM and GRU, in one case matching hidden size and in another matching number of parameters. GATO outperforms all GRU and LSTM variants on our experiments.\n\n\n>[Unfair claims about fewer parameters, compare against models with similar parameter counts]\n\nThis is a great point. As mentioned, we included comparisons against GRU/LSTM where we balanced the parameter counts. We have also removed excess emphasis on fewer parameters on the tasks where held-out metrics are reported.\n\nTheory suggests that optimization is easier in the overparameterized setting. See Arora et al. 2018.\n\nArora et al. On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization. 2018\n\n>[ Multi-layer RNNs are not compared or mentioned in this work ]\n\nWe follow other recent RNN papers (SRU, FRU, EURNN) by focusing on fundamental design choices and basic tasks rather than explore the full range of deep variants or regularization.\n\nWe have added details that some LSTM papers we cited use multiple layers, and mentioned deep variants in our conclusion.", "title": "Thank You For Your Feedback"}, "BJezyHPiir": {"type": "rebuttal", "replyto": "BJxeizHRtS", "comment": ">[Large LSTM/GRU with no regularization may have overfit]\n\nThank you for your comments.\n\nOur goal for this work is not to investigate generalization but rather to design sequence models that capture long-term dependencies.\n\nOn Copy and Add, the models are trained on new samples from the data distribution at each batch. There is no notion of overfitting on these experiments.\n\nFor MIMIC and Penn, we report held-out accuracy and perplexity.\n\nWe have added comparisons against GRUs/LSTMs with similar parameter counts as GATO. GATO still outperforms these models on generalization in this non-regularized setting.\n\nYears of research has been devoted to regularizing GRUs/LSTMs. A future direction is to see whether the same techniques apply to alternate models such as GATO and RHN.\n\n>[Clarify why the lowest perplexity score in this paper is higher than those in other recent works]\n\nThe best held-out perplexity score on Penn TreeBank of 112.85 is for unregularized models with no dropout. [4] and [6] achieve 65.4 and 52.8 using many training and regularization techniques.\n\nRNN Regularization (Zaremba, 2015) reports 114.5 test perplexity for this task for an unregularized LSTM.\n\nWe have included comparisons against other recently proposed RNNs that address the vanishing gradient issue: RHN [4], EURNN (Jing, 2017), and SRU (Oliva, 2017). RHN and EURNN perform poorly and NAN because they have unbounded forward propagation. SRU performs well on MIMIC but not on other tasks. \n\nWe fix RHN\u2019s performance with one principle from our paper. See the Penn TreeBank result.\n\n>[GATO used 1 or 2 layers. How does GATO change with >2 layers?]\n\nOur \u201ctwo layer\u201d variant of GATO was not named clearly. \u201cTwo layer\u201d for GATO referred to increasing the depth of the function used to compute the recurrence. This is different from using N stacked identical RNNs for N layers. The \u201ctwo layers\u201d in GATO is more similar to how the GRU takes several functions of the input state.\n\nWe follow other recent RNN papers (SRU, FRU, EURNN) by focusing on fundamental design choices and basic tasks rather than explore the full range of deep variants or regularization.\n\n>[Other non-linear functions?]\n\nIn new section Appendix B, we replace the sigmoid in the GATO update with tanh, and we replace the decoder cos with sin. We observe little to no variation.\n\n>[typo r -> r_t in Eq. 6]\n\nWe fixed this typo. Thank you.", "title": "Thanks For Your Feedback"}, "Syxn8EDojB": {"type": "rebuttal", "replyto": "BylfTySYvB", "comment": "We thank all of the reviewers and the AC for their time and feedback. The reviewers\u2019 responses are positive in general. \n\nThe main contribution of this work is a list of criteria necessary for sequence models to capture long-term dependencies when trained with gradient-based optimization, along with one instantiation of a model that meets these criteria.\n\nReviewer #2 finds \u201cboth the design and the practical performance of the proposed GATO unit very interesting, and potentially valuable for the ICLR crowd.\u201d and mentions \u201cThis is one of the more convincing RNN papers I have recently read.\u201d\n\nAll reviewers suggested additional experiments. Our added experiments include 3 recent alternative RNN models and LSTM/GRU variants.\n\nThe reviewers asked for ablations on the partition of GATO\u2019s hidden state and choice of non-linear functions. We have added this to Appendices A and B.\n\nTwo points of clarification:\n\n1. We follow other recent RNN papers (SRU, FRU, EURNN) by focusing on fundamental design choices and basic tasks rather than explore the full range of deep variants or regularization. A future direction is to see whether the GRU/LSTM regularization techniques apply to our model or its deep variants.\n\n2.GATO is not a special case of the SRU as mentioned by Reviewer #3. Additional Discussion in Appendix I\n\nWe uploaded a new version of the paper that addresses all of the reviewers' concerns. We cite the papers they mentioned. \n\nOur model competes with or outperforms GRU, LSTM, and other recent RNNs. Our model exhibits stability in cases where the alternatives are unstable. Ablations do not indicate sensitivity of GATO to small choices of non-linearities.\n", "title": "Updated Paper and Response to Reviews"}, "SygdaoLEtr": {"type": "review", "replyto": "BylfTySYvB", "review": "The paper proposes a new RNN architecture designed to overcome vanishing/exploding gradient problems and to improve long-term memory for sequence modelling. The main ideas are (i) to split the hidden state into two parts, one of which does not influence the recurrence relation, and can therefore not blow up or contract by self-feedback; and (ii) to use periodic functions, in particular the cosine, as non-linearity in the decoder, so that the output is bounded but does not saturate.\n\nThe paper puts forward a fairly systematic analysis of the gradients in RNNs. The analysis appears correct, and is in fact quite similar to considerations in earlier RNN work (which is correctly cited), and forms the basis for the proposed GATO unit. There are two loose ends in this part:\n1) the cosine non-linearity results from a purely negative selection - the function should be bounded, but not saturating. The paper does not even ask the question which periodic function might be a good choice.\n2) While the method is presented as a grand theory, with the only constraint that a part of the hidden state does not influence the recurrence function; the actual implementation and experiments are limited to the narrow special case of \"non-interacting\" GATO, where the \"passive\" variables make up exactly half of the hidden vector, and the update of each individual hidden variable is influenced only by a single variable from the previous state. So there are in fact no empirical results, not even on toy data, for the general case that the paper claims to introduce.\n\nIn the experiments, there are two artificial problems (copying, adding) for sequences of symbols. These are illustrative and sensible to verify and analyse  the behaviour of GATO in a controlled setting, but rather far from most real sequence modelling tasks. In a third experiment the task is to classify whether or not patients will stay in intensive care for >1 week, based on a (seemingly private) database of time series of vital parameters. Unfortunately, the results for that experiment do not go beyond the usual \"ours is better\". While the numbers clearly support GATO, it would have been nice to look a bit closer and pinpoint what makes the difference. Also, the comparison is a bit loose. It would have been better to add additional baselines where also LSTM and GRU are restricted to \"non-interacting\", element-wise recurrence (as far as technically feasible). As it stands, it is unfair to claim \"we can do it with much fewer parameters\" - perhaps LSTM / GRU could, too. In fact, it could even be that the task is just simple, so that more restricted model with fewer parameters generally perform better - I do not claim this is the case, but the experiments do not rule it out and, hence, do not confirm that the clever GATO recurrence makes the difference.\n\nA small gap is also that even the \"real\" experiment might not be completely realistic. Nowadays it is a popular strategy to use deep LSTMS / GRUs, i.e., stack multiple levels of recurrence, possibly with temporal sub-sampling, to better capture long-term relations. While this is potentially even more brittle, because of the additional gradient flow across layers, it does seem to work. But deep RNNs are not tested (in fact, not even mentioned) in the paper.\n\nOverall, in spite of a few loose ends, I find both the design and the practical performance of the proposed GATO unit very interesting, and potentially valuable for the ICLR crowd. This is one of the more convincing RNN papers I have recently read.", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 3}, "SJxmtISTFS": {"type": "review", "replyto": "BylfTySYvB", "review": "In this paper, the authors propose a novel recurrent architecture called GATO. \nSpecifically, the authors focused on sequence modeling tasks and developed criteria for RNN models on such tasks. \nTha GATO model can resolve the vanishing/exploding gradient issue and is robust to initializations. \nEmpirical results show GATO can outperform LSTM and RNN in both synthetic datasets and real datasets.\n\nThe key insight of the proposed model is that only part of the hidden states is recurrently updated. \nThe GATO achieves this by adding the skip connection channel (or residual connection) along the temporal dimension.\nGATO summarizes the hidden states r_t by recurrently adding (transformed) r_t to s_t.\nThis idea also appears in many previous RNN models, such as highway RNN/LSTM, Statistical/Fourier Recurrent Units [1][2].\nSpecifically, the proposed GATO is a special case of SRU ( alpha=1). This limits the novelty of the paper and thus make the contribution marginal.\n\nAs for the experimental studies, the authors only provide comparisons with LSTM and GRU. There are a lot of advanced RNN architectures to address vanishing/exploding gradient issues, such as uRNN[3], oRNN[4], Spectral-RNN[5] and SRU/FRU [1][2]. It would be more convincing if the \nauthors could include these models into comparison.\n\nOverall I think this paper should be further improved before being accepted.  \n\n\n\n[1] Oliva, J.B., P\u00f3czos, B. and Schneider, J., The statistical recurrent unit. \nIn ICML 2017 (pp. 2671-2680).\n\n[2] Zhang, J., Lin, Y., Song, Z. and Dhillon, I., Learning Long Term Dependencies via Fourier Recurrent Units. \nIn ICML 2018 (pp. 5810-5818).\n\n[3] Arjovsky, M., Shah, A. and Bengio, Y., Unitary evolution recurrent neural networks. \nIn ICML 2016 (pp. 1120-1128).\n\n[4] Mhammedi, Z., Hellicar, A., Rahman, A. and Bailey, J., Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. \nIn ICML 2017 (pp. 2401-2409).\n\n[5] Zhang, J., Lei, Q. and Dhillon, I., Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization. \nIn ICML 2018 (pp. 5801-5809).", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 4}}}