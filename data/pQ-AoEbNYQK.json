{"paper": {"title": "DiffAutoML: Differentiable Joint Optimization for Efficient End-to-End Automated Machine Learning", "authors": ["Kaichen Zhou", "Lanqing HONG", "Fengwei Zhou", "Binxin Ru", "Zhenguo Li", "Trigoni Niki", "Jiashi Feng"], "authorids": ["~Kaichen_Zhou1", "~Lanqing_HONG1", "~Fengwei_Zhou1", "~Binxin_Ru1", "~Zhenguo_Li1", "niki.trigoni@cs.ox.ac.uk", "~Jiashi_Feng1"], "summary": "", "abstract": "The automated machine learning (AutoML) pipeline comprises several crucial components such as automated data augmentation (DA), neural architecture search (NAS) and hyper-parameter optimization (HPO). Although many strategies have been developed for automating each component in separation, joint optimization of these components remains challenging due to the largely increased search dimension and different input types required for each component. While conducting these components in sequence is usually adopted as a workaround, it often requires careful coordination by human experts and may lead to sub-optimal results. In parallel to this, the common practice of searching for the optimal architecture first and then retraining it before deployment in NAS often suffers from architecture performance difference in the search and retraining stages. An end-to-end solution that integrates the two stages and returns a ready-to-use model at the end of the search is desirable. In view of these, we propose a differentiable joint optimization solution for efficient end-to-end AutoML (DiffAutoML). Our method performs co-optimization of the neural architectures, training hyper-parameters and data augmentation policies in an end-to-end fashion without the need of model retraining.  Experiments show that DiffAutoML achieves state-of-the-art results on ImageNet compared with end-to-end AutoML algorithms, and achieves superior performance compared with multi-stage AutoML algorithms with higher computational efficiency.\nTo the best of our knowledge, we are the first to jointly optimize automated DA, NAS and HPO in an en-to-end manner without retraining.", "keywords": ["Differentiable", "Automated machine learning", "Neural Architecture Search", "Data Augment", "Hyperparameter Optimization"]}, "meta": {"decision": "Reject", "comment": "The paper was discussed by the reviewers that acknowledged the rebuttal and the authors\u2019 responses. In particular, they appreciated the fact that some of their concerns were alleviated (e.g., going beyond the single ImageNet evaluation).\n \nMore generally, while all the reviewers thought that the problem tackled by the paper was of clear interest (i.e., full end-to-end auto-ML encompassing DA, NAS and HPO), they still expressed concerns (even after the rebuttal), in particular:\n\n* _Clarity of the methodology_: None of the reviewers could clearly and fully understand the mathematical formulation of the joint optimization, leading to a series of questions regarding the confusing usage of the training/validation set in the experimental setup. This unfortunately made the assessment of (some aspects of) the paper speculative for the reviewers.\n* _Comparison with AutoHAS_: AutoHAS and DiffAutoML are obviously related methods. Even if AutoHAS has weaknesses compared to the proposed approach DiffAutoML, e.g., discretization of the continuous hyperparameters and no tuning of DA, it is still meaningful to carry out an actual comparison (possibly normalized by the different costs at play since the authors have highlighted the different memory overheads). Though the listed weaknesses of AutoHAS _should_ play in favor of DiffAutoML,  a proper experimental comparison would better support that claim.\n\nGiven those remaining concerns and the overall mixed scores, the paper is recommended for rejection. The detailed comments of the reviewers provide an actionable list of items to improve the paper for a future resubmission.\n"}, "review": {"Ql2O2FrIG2z": {"type": "rebuttal", "replyto": "oSiH4b6T1ln", "comment": "Thanks for your detailed and insightful comments. All code will be\nreleased after acceptance.\n\n**R1.1: Incremental Results**: Compared with previous methods,\nDiffAutoML is the first work showing that \\*\\*end-to-end\\*\\* AutoML is\npossible in a fully differential way. This problem setting is practical\nbut rarely explored before. Specifically, DiffAutoML achieves joint\noptimization of different AutoML components, including neural\narchitecture search, data augmentation, and hyper-parameter\noptimization, and finds better ready-to-use models with fewer computing\nresources. In the experiments, we show around one percent increase on\nImageNet compared with existing end-to-end AutoML algorithms.\n\n**R1.2: Other Computer Vision Tasks**: The main reason why we have not\ncompared DiffAutoML with AutoHAS is that AutoHAS requires much more\ncomputational resources than DiffAutoML and AutoHAS cannot directly\nrealize the hyper-parameter optimization of continuous hyper-parameters\nwhich is discretized during the optimization process in AutoHAS. More\nspecifically, at each iteration, AutoHAS would discretize continuous\nhyper-parameter into a linear combination of discrete hyper-parameters.\nAnd those discrete hyper-parameters will be applied to optimize the\nnetwork. During this process, AutoHAS has to deep copy the network for N\ntimes and N is the number of discretized hyper-parameters. Thus,\ndirectly comparing AutoHAS with DiffAutoML is unfair. ImageNet is the\nmost popular benchmark used to evaluate machine learning algorithms,\nwhich is representative and credible. In order to prove the\ngeneralizability of DiffAutoML, we have applied our model on CIFAR-10\nand the results are shown in **R2.1**.\n\n**R1.3 (Data Augmentation In Other NAS)**: Existing NAS algorithms\nlisted in Table\u00a01 require additional human intervention and\ncomputational cost, such as deciding on the hyperparameters for model\nre-training or choosing the data augmentation policy. On the other hand,\nDiffAutoML provides an end-to-end solution to achieve NAS and automated\nDA simultaneously with few additional human involvement. In order to\nmake fair comparison, we have adapted the random augmentation with\nDiffAutoML, where the data augmentation strategies are sampled from\nuniform distribution same as their level instead of being learned from\nthe training loss function. The results are shown in the following\nTable S2. Moreover, in Table\u00a02, we applied automated DA and HPO to previous\nNAS algorithms, including Darts and DSNAS (see row 2-3, column 3 in\nTable\u00a02). As can be seen, DiffAutoML still outperforms these baselines.\n\nTable S2: Performance of Random-DiffAutoML and DiffAutoML on ImageNet Dataset.\n\n| Algorithm      | Flops | Search Stage(%)    |\n| :---        |    :----:   |          ---: |\n| Random-DiffAutoML      | 321M       | 74.8/92.1   |\n| DiffAutoML  | 318M        | 75.5/92.7      |\n\n**R1.4 (Repetition of Experiments)**: All our experiments have been run\nfor more than twice. As we have not noticed evident variance between\ndifferent experiments, we reported the mean test accuracy in Table\u00a01.\nFor the cases with evident variance, we also reported the standard\ndeviation, together with the mean test accuracy, in Table\u00a02. All the\nsource code and well-trained model will be released, once our paper has\nbeen accepted.\n\n**R1.5 (Validation Set)**: Thanks for pointing it out. To make a fair\ncomparison, we haven\u2019t used an extra validation set. Our validation set\nis sampled from the training set. For the supplementary experiment on\nCIFAR-10, we have adopted the training set and validation set split\ngiven by \u2019Darts: differentiable architecture search\u2019, where they hold\nout half of the CIFAR-10 training data as the validation data. For the\nexperiments on ImageNet, we hold out $5$% of the ImageNet training data\nas the validation data.\n\n**R1.6 (Figure 2)**: In Figure\u00a02, we would like to show that even though\nour model needs more time for one epoch compared with pure NAS, due to\nadditional HPO and automated DA, our model can converge within fewer\nepochs and thus saves the total computation cost.\n\n**R1.7 (Typos)**: Thanks for pointing it out, we will modify the typos\nfor the revised manuscript.\n", "title": "Response to AnonReviewer1"}, "Ob9opb-2c7k": {"type": "rebuttal", "replyto": "DJuwcDufCv", "comment": "Thanks for your detailed and insightful comments. All code will be\nreleased after acceptance.\n\n**R3.1 (Limited Hyper-parameters Are Considered and The Validation\nSet)**: (1) Our algorithm can be applied to diverse continuous\ndifferentiable hyper-parameters including learning rate, momentum,\nweight decay, etc. (2) The main reason why we use training loss for NAS\nis to follow the practice in DSNAS so as to ensure fair comparison. (3)\nIn DiffAutoML, we mainly consider the hyper-parameters in the optimizer\nwhich influence the weights updating process as in [2]. The performance\nof updated weights in the validation set can be a direct evaluation of\nthe hyper-parameter setting.\n\n[1] Revisiting the Train Loss: an Efficient Performance Estimator for\nNeural Architecture Search.\n\n[2] Online Learning Rate Adaptation With Hypergradient Descent.\n\n**R3.2 (DSNAS Not Using Validation Set)**: Thanks for pointing it out.\nWe would like to clarify that our comparison is still fair as we haven\u2019t\nused an extra validation data. Our validation set is sampled from the\ntraining set used by DSNAS. For the experiment on ImageNet, we hold out\n$5$% of the ImageNet training data as the validation data.\n\n**R3.3 (Results Are Taken From DSNAS)**: To make a fair comparison, we\ntest the performance of DSNAS in the test set. The difference between\nour reported results and the reported results in DSNAS is shown in Table\n1.\n\n**R3.4 (More Details in Appendix)**: We would include a more detailed\nexplanation of related works in our Appendix.\n\n**R3.5 (Parents Network In Algorithm and Other Typos)**: Yes, thanks for\npointing them out. We will modify our typos in the camera-ready version.\n", "title": "Response to AnonReviewer3"}, "bNFhNTAGYM1": {"type": "rebuttal", "replyto": "6sG_-GNBQK", "comment": "Thanks for your detailed and insightful comments. All code will be\nreleased after acceptance.\n\n**R4.1 (Different Loss Function)**: The main intuition behind optimizing\nNAS through decreasing the training loss is that, during the early\ntraining stage, sum over training losses (SoTL) are correlated with the\nfinal architecture\u2019s performance. Lower SoTL indicates better final\nperformance of evaluated neural architecture, thus the optimization of\nNAS is realized through the training loss [1]. It is a common practice\nto use training loss in NAS [2-3], suggesting that this would not result\nin the over-fitting problem. As for data augmentation, using training\nloss has already been explored in previous works [4]. By increasing the\ntraining loss, we intend to find those transformation strategies which\nare hard for current network. Through paying more attention to\n\u2019difficult\u2019 transformation, we increase the generalizability of our\nmodel.\n\n[1] Revisiting the Train Loss: an Efficient Performance Estimator for\nNeural Architecture Search.\n\n[2] SNAS: Stochastic Neural Architecture Search.\n\n[3] DSNAS: Direct Neural Architecture Search without Parameter\nRetraining.\n\n[4] Adversarial Autoaugment.\n\n**R4.2 (Mathematical Description & Ablation Studies)**: Thanks for your\nsuggestion. This paper concentrates on proposing a co-optimization\nalgorithm for NAS, DA, and HPO. Mathematical explanation about the\ncoupling relation between different modules like NAS and DA would be\nconsidered in our future research. We provide several ablation studies\nin our paper, including realizing the co-optimization in a sequence\nshown in Table 2 upper part, realizing the DA and HPO in the fixed\nnetwork architecture shown in the lower part of Table 2.", "title": "Response to AnonReviewer4"}, "qMUjiwquYT": {"type": "rebuttal", "replyto": "m-aDBbZiIxU", "comment": "We appreciate your constructive suggestion and we provide the following\nmodification. All code will be released after acceptance.\n\n**R2.1 (More Datasets)**: Our DiffAutoML can be easily applied to other\nsearch spaces based on weight sharing strategy, e.g., the search space\nproposed in \u2019Path-level network transformation for efficient\narchitecture search.\u2019. To illustrate the generalizability of DiffAutoML,\nwe have applied DiffAutoML to CIFAR-10 with the search space in \u2019Darts:\ndifferentiable architecture search\u2019. In the first experiment, we applied\nthe co-optimization in a sequence (Pipeline-DiffAutoML). We first apply\nDARTS to find the structure and then, based on this structure the data\naugmentation strategy and the hyper-parameter optimization strategy are\napplied to tune this learned structure. In the second experiment, we\ndirectly applied DiffAutoML to CIFAR-10 (DiffAutoML). The average\nresults over x seeds are shown in Table\u00a0S1. With a different search\nspace, DiffAutoML still performs better than optimising different\nmodules in sequence (Pipeline-DiffAutoML).\n\nTable S1: Performance of DiffAutoML on CIFAR-10 dataset.\n\n| Algorithm      | Params | Test Error(%)     |\n| :---        |    :----:   |          ---: |\n| Darts      | 3.3M       | 2.80   |\n| Pipeline-DiffAutoML  | 3.3M        | 2.47      |\n| DiffAutoML  | 3.3M        | 2.39      |\n\n**R2.2 (Other Computer Vision Tasks)**: DiffAutoML is a general joint\noptimization framework that can be applied to different tasks, such as\nobject detection, semantic segmentation, or even NLP tasks. The designs\nof the losses for automated DA, NAS, and HPO are task-agnostic. For a\nnew task, we only need to determine the suitable search spaces for DA\nand NAS, and then our method can be applied to this task directly.\n\u2019Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic\nImage Segmentation\u2019 proposes a two-level search space to deal with the\nimage segmentation task and the gradient-based method, e.g., Darts and\nDiffAutoML, can be used to realize this kind of neural architecture\nsearch.\n\n**R2.3 (Grouping NAS and DA)**: Thanks for your question. We haven\u2019t\ngrouped neural architecture search and data augmentation deliberately.\nThe common thing in the neural architecture search and data augmentation\nis that both of them use the training loss for optimization. The\noptimization for neural architecture search aims to decrease the\ntraining loss for the sake of finding better structures, while the\noptimization for data augmentation aims to increase the training loss to\nlet the training process pay more attention to the hard samples.\nMoreover, during the optimization process, different optimizers are used\nfor updating neural architecture parameters and data augmentation\nparameters. Thus there two processes wouldn\u2019t conflict with each other.\nDifferent AutoML components, including NAS, HPO, and Automated DA, are\noptimized jointly in a unified framework.\n\n**R2.4 (Figure\u00a03)**: With Figure 3, we would like to show that with\ndifferent training settings, we can end up with different optimal neural\narchitectures. Thus, it is necessary to consider the co-optimization of\ndifferent AutoML components together, instead of performing them\none-by-one sequentially.\n\n**R2.5 (How To Compare)**: The superiority of the found architectures is\nmainly demonstrated by comparing their final accuracy, their\ncomputational request, and their model size on ImageNet with those of\nother competitors.", "title": "Response to AnonReviewer2"}, "DJuwcDufCv": {"type": "review", "replyto": "pQ-AoEbNYQK", "review": "**Summary**\nThis paper shows how the complex autoML pipeline for neural networks can be trained in an end-to-end manner by combining existing methods. By using backpropagatable discrete sampling methods (Gumbel softmax), input transformed by data augmentation is seamlessly embedded in full backpropagation flow. And a differentiable architecture search algorithm is used, which also incorporates architecture search in full backpropagation flow. On top of this differentiable procedure, an alternating optimization is introduced to train network parameters and hyperparameters.\n\n\n**Strengths**\n1. The paper shows that end-to-end automl is possible in a fully differential way which allows better models with fewer resources.\n2. Even though, intuitively, this joint optimization should perform better than non-joint ones, making such training stable does not seem trivial. The authors appear to successfully have taken care of it.\n\n\n**Weaknesses**\n1. Even though 2 hyperparameters are considered in the ablation study, its capability and stability with a moderate number of hyperparameters are not well-addressed (in the main experiment only 1 hyperparameter is considered). And the use of a validation set only in hyperparameter update step make me question that this hyperparameter optimization component may cause some issues such as training instability. Can the authors share some thought, experience, and intuition on this?\n2. It is questionable whether the comparison in Figure 2 is fair since DiffAutoML utilizes validation data in its hyperparameter optimization step whereas DSNAS seems to not use validation data.\n3. Similar to above, the numbers for DSNAS in Table 1 seem to be taken from DSNAS paper (Table 3) which are numbers from the validation set.\n\n\n**Recommendation**\nThe paper tackles interesting and practical problems and shows the proposed method outperforms baselines. My main concern is whether DiffAutoML uses validation data that is not used by other baselines, which could be a reason for better performance. The concern on considering on a small number of hyperparameters is a minor one if some explanation can be added. As long as the concern on the validation data access is resolved then I will be willing to increase my score favoring acceptance. However, in its current form, I cannot stand for it.\n\n\n**Questions**\n- The numbers reported in Table 1 and Table 2 are test performances?\n\n\n**Additional feedback** (Irrelevant to the decision assessment)\n- Since the most of NAS component in the full pipeline is from DSNAS, it would be better for readers if this is detailed in Related Works or Appendix.\n- At 4th line in Algorithm 1, using the word 'parent' network is more consistent?\n- In the last line in Abstract, en-to-end -> end-to-end\n- At 3 lines above eq. 3. ont-hot -> one-hot\n- In the last paragraph in Introduction, in the 4th line from the last, Differentiable -> differentiable", "title": "Nice paper for fully differentiable autoML pipeline as long as some concerns on empirical evaluations are resolved.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "6sG_-GNBQK": {"type": "review", "replyto": "pQ-AoEbNYQK", "review": "Summary and contributions: \nThis paper proposes a joint differential search method to optimize data augmentation, discrete architecture choices, and hyperparameters. \n\n\nStrengths:\n1). This paper proposes an automated method to search for data augmentation, hyperparameters, and architectures using gradient descent, which is simple and easy to integrate. \n2). It achieves co-optimization on different components in one-stage.\n3). A modest improvement upon SOTA mobile models.\n\nWeaknesses:\nIn the optimization, the proposed method naively uses validation data to optimize the hyperparameters, and use training data to optimize architecture weights, data augmentation weights, and network weights. Wouldn't the architecture choices and data augmentation overfit to the training data in such case? Note that there exists a discrepancy in the optimization direction between NAS and  DAS, where NAS aims to minimize the training loss (e.g. cross entropy) while DAS tends to increase the training loss. Therefore, trying to optimize NAS and DAS under a single objective is intractable. The objectives for the jointly optimization needs to be carefully designed so that the optimization for NAS and DAS can contribute collaboratively to the final testing performance.\n\n\nCorrectness: Are the claims and methods correct? Is the empirical methodology correct? \nI am uncertain the method appears to be correct.\n\nClarity: Is the paper well written? \nYes, although a few aspects could be improved (see feedback).\n\nRelation to prior work: Is it clearly discussed how this work differs from previous contributions? \nYes.\n\nReproducibility: Are there enough details to reproduce the major results of this work? \nNo code is provided.\n\nAdditional feedback, comments, suggestions for improvement, and questions for the authors: \nClearly describe mathematically how the proposed method contribute collaboratively to the final testing performance including its generalizability. The mutual dependency between NAS and DAS arises from that NAS is based on the training data modified by the augmentation policy generated by DAS and the DAS is based on the network generated by NAS. Additionally, provide ablation studies on changing one single component while fixing others. \n\n\n\n\n", "title": "DiffAutoML: Differentiable Joint Optimization for Efficient End-to-End Automated Machine Learning ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "oSiH4b6T1ln": {"type": "review", "replyto": "pQ-AoEbNYQK", "review": "Some of the choices that have to be made when training a neural net based image model are: type of data augmentation, architecture of the neural network, and other hyperparameters such as regularization and optimization hyperparameters (e.g. learning rate). Optimizing all of these is a challenging problem, NAS deals with architecture but ignores the others. More general hyperparameter optimization techniques such as Bayesian Optimization struggle with the dimensionality of the architecture parameters. And optimizing them independently might lead to local minima, and/or be slow.\n\nOne previous work (AutoHAS) already combined NAS with the ability to add differentiable hyperparameter. In this work data augmentation is incorporated as well and tuned jointly with the rest of the architecture and the other hyperparameters.\n\n**Their main contributions** of this paper are: 1. The authors describe a NAS+differentialble hyperparameter tuning technique that also include data augmentation, thus extending the setting somewhat compared to previous results 2. They provide experimental results on ImageNet\n\nThe paper is reasonably clear, and while some of the ideas are interesting (such as how to incorporate data augmentation as a differentiable NAS hyperparameter) I don't think this work should be accepted mainly because:\n* They describe only incremental improvements. Thus no major contribution from the theory/new tecniques part of the paper.\n* Limited experimental results, definitely not strong enough to compensate for the previous point. There is no comparison to AutoHAS, which appears to be the closest method, according to the paper itself. And most results are on ImageNet only. If the paper wants to provide a technique for automatically generating vision pipelines, stronger arguments and experimental results should be brought forward for why this will work on new problems and not just on ImageNet (for witch we can just take one of the existing pretrained models).\n\n\n##### Questions/comments: \n\nIn the results, it seems that the competing method use no data augmentation at all, is that correct? If so it would be more fair if the competing method use untuned but reasonable data augmentation. \n\nDoes baseline in table 2 contain any data augmentation? It's not really clear?\n\nThe results are the average over how many repetitions? It would be nice to know the number of repetitions and standard dev/error, or if a single one at least have the source code available for better reproducibility.\n\nThe validation set is used for selecting hyperparameters, thus should not be used for comparing methods, was a separate test set used for accuracy? It would be good to describe this more clearly in the paper.\n\nI would remove the epoch based plot, what we care about at the end is time.\n\n\n##### Typos:\n\nSection 1\n\"We first jointly optimize three different AutoML components, including\" -> you might want to remove \"including\" if you list all three\nSection 3.1\nFor t-th iteration -> for the t-th iteration", "title": "Incremental paper with limited results", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "m-aDBbZiIxU": {"type": "review", "replyto": "pQ-AoEbNYQK", "review": "Summary\nThis paper focuses on achieving automated \"from data to model\" including different components in modeling, namely data augmentation, Neural Architecture Search, Hyper Parameter Optimization. The proposed approach first use data augmentation to select the data argumentation transformation. It tries to select examples which incurs higher training loss for the model to address hard examples. Then use the DAG for neural architecture search. Given the data and architecture, it then alternatively update the model parameter and hyper parameter. The overall proposed framework is end-to-end. Experiment on ImageNet shows slight performance improvement over existing approaches. The authors also conduct ablation study to show the effectiveness of jointly modeling the three components (data augmentation, neural architecture search, hyper-parameter optimization).\n\nStrengths:\n1) This work considers three important components in modeling process including data argumentation, HPO, and NAS. The different components may interact with each other to impact the performance.\n2) This paper is clearly written and easy to follow.\n3) The experiment shows performance improvement with less time needed. \n\nWeakness\n1) The experiment is limited with only 1 task and data. Is it possible to show the results on more than 1 dataset? Otherwise, the result might still be dataset specific. However, the proposed approach should not be \n\nQuestions\nWill the proposed approach generalize beyond computer vision task?\nWhy are the data augmentation and neural architecture search grouped together? I was wondering what will happen if you group neural architecture search and hyper parameter optimization first?\nHow should I interpret figure 3? How could you demonstrate that the selected architecture by the proposed approach is better or make sense?\n\n", "title": "Overall this paper is clearly written and easy to follow. The motivation makes sense and result support the claims. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}