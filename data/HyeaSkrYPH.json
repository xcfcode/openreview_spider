{"paper": {"title": "Certified Defenses for Adversarial Patches", "authors": ["Ping-yeh Chiang*", "Renkun Ni*", "Ahmed Abdelkader", "Chen Zhu", "Christoph Studor", "Tom Goldstein"], "authorids": ["pchiang@cs.umd.edu", "rn9zm@cs.umd.edu", "akader@cs.umd.edu", "chenzhu@cs.umd.edu", "studer@cornell.edu", "tomg@cs.umd.edu"], "summary": "", "abstract": "Adversarial patch attacks are among one of the most practical threat models against real-world computer vision systems. This paper studies certified and empirical defenses against patch attacks. We begin with a set of experiments showing that most existing defenses, which work by pre-processing input images to mitigate adversarial patches, are easily broken by simple white-box adversaries. Motivated by this finding, we propose the first certified defense against patch attacks, and propose faster methods for its training. Furthermore, we experiment with different patch shapes for testing, obtaining surprisingly good robustness transfer across shapes, and present preliminary results on certified defense against sparse attacks. Our complete implementation can be found on: https://github.com/Ping-C/certifiedpatchdefense.", "keywords": ["certified defenses", "patch attack", "adversarial robustness", "sparse defense"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents a certified defense method for adversarial patch attacks. The proposed approach provides certifiable guarantees to the attacks, and the reviewers particularly find its experiments results interesting and promising. The added new experiments during the rebuttal phase strengthened the paper. There still is a remaining concern that is novelty is limited as this paper could be viewed as the application of the original IBP to patch attacks, but the reviewers believe in that its empirical results are important."}, "review": {"r1lTNkw6tS": {"type": "review", "replyto": "HyeaSkrYPH", "review": "This paper attempts to extend the Interval Bound Propagation algorithm from (Gowal et al. 2018) to defend against adversarial patch-based attacks. In order to defend against patches which could appear at any location, all the patches need to be considered. This is too computationally expensive, hence they proposed to use a random subset of patches, or a U-net to predict the locations of the patches and then use those patches to train. The algorithm is tested on the MNIST and CIFAR-10 datasets and it was shown that sometimes the IBP approach is useful for defense, although often with a significant loss on accuracy on clean data (e.g. on CIFAR the loss on clean accuracy is an astounding 300% -- from 66.5% - 35.7%).\n\nI think the technical contribution of this paper is a bit weak in that they mostly followed the original IBP and the only novelties are the random patch training and guided patch training. I partially like how the experiments are conducted, especially the one that generalizes to other shapes. On the other hand, the networks that are tested seem pretty poor by any standard. An experiment that is definitely missing is a CIFAR network that performs a little better than the current one. Clean accuracy of only 66.5% and 47.2% are very lousy for CIFAR.\n\nAnother missing experiment is one that would test on different epsilon values. I couldn't find what are the current epsilon values used?\n\nBesides, since this work is testing on adversarial patches, I would like to at least have it applied to some real-life images with patches that are of real-life size. I could care a bit less on how good it is, but one can still make an empirical test (e.g. certified defense accuracy on 5x5 patches, but empirical test using real-life sized patches 40x40 or 80x80) and see how the results would be. All the experiments mentioned above would significantly strengthen the experiments section of the paper.\n\nI don't think I read anywhere a confirmation that the testing is performed on all patches of the prescribed size. Could the authors please confirm whether this is true?\n\nMinor: \nThere is a typo in Eq. (5) and Eq. (6), where the second term multiplied by |W^(k)| should be \\underline{z}^(k-1) - \\bar{z}^(k-1) instead of \\underline{z}^(k-1) + \\bar{z}^(k-1)\n\nYou should mention that |W^(k)| stand for element-wise absolute value when it first appears.", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "rkxL6t_njB": {"type": "rebuttal", "replyto": "HyeaSkrYPH", "comment": "We updated the manuscript based on valuable conversations with the official reviewers:\n\n- We include additional result supporting our main claims using experiments with a larger model\n\n- We compare against an interesting training approach, based on a bound pooling formulation proposed by R2, which did not improve upon our top performing model.\n\n- We report on experiments with multiple 1x1 patches, also proposed by R2, that our approach can extended to multi-patch setting. In the specific case of 1 x 1 patch, the multi-patch setting is equivalent to defending against $l_0$ attacks.\n\n- We update all results, with only marginal differences, after fixing some minor issues in our codes.", "title": "Update"}, "B1gxVBPnir": {"type": "rebuttal", "replyto": "rJgZlGvhoH", "comment": "Thank you for supporting our paper!\n\n>>> We will make sure to formulate the bounds for this case more formally, and include that in the manuscript.", "title": "Thank you!"}, "rkeTrpE2iH": {"type": "rebuttal", "replyto": "SJxfCOehsr", "comment": "Thanks for following up. We were able to get new results for multiple 1x1 patches as suggested, and will include them in the manuscript upon acceptance.\n\nR2: \"Please make sure to mention it in the main text for the final revision, as I believe it is an important contribution. \"\n\nAuthor(s): Yes, we will make sure to include it in the manuscript upon acceptance.\n\nR2: \"However I believe multiple 1x1 boxes are doable.\"\n\nAuthor(s): Indeed, you are correct. Thanks for pointing this out. We have included results for both MNIST & CIFAR for the 1,4,10, 25 1x1 patches below. We will also update this in the main text upon acceptance.\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551              \u2551 k  \u2551 Model          \u2551 Clean Accuracy  \u2551 Certified Accuracy  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 MNIST \u2551 1  \u2551 MLP             \u2551 98.4%                   \u2551  96.0%                      \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 MNIST \u2551 4  \u2551 MLP             \u2551 97.8%                   \u2551 90.8%                       \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 MNIST \u2551 10\u2551 MLP             \u2551 95.2%                  \u2551 86.8%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 MNSIT \u2551 25\u2551 MLP             \u2551 68.2%                  \u2551 35.6%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 MNIST \u2551 1  \u2551 Conv3x3      \u2551 97.0%                  \u2551 88.3%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 MNIST \u2551 4  \u2551 Conv3x3      \u2551 92.7%                  \u2551 75.9%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CIFAR  \u2551 1  \u2551 MLP              \u2551 48.4%                  \u2551 40.0%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CIFAR  \u2551 4  \u2551 MLP              \u2551 42.2%                  \u2551 31.2%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CIFAR  \u2551 10 \u2551 MLP             \u2551 37.0%                  \u2551 25.6%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CIFAR  \u2551 1  \u2551 Conv11x11  \u2551 34.8%                  \u2551 27.4%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CIFAR  \u2551 4  \u2551 Conv11x11  \u2551 25.1%                  \u2551 18.3%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CIFAR  \u2551 10 \u2551 Conv11x11 \u2551 17.2%                  \u2551 13.8%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CIFAR  \u2551 1  \u2551 Conv13x13  \u2551 38.6%                  \u2551 29.7%                        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CIFAR  \u2551 4   \u2551 Conv13x13 \u2551 28.1%                  \u2551 19.6%                        \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nIt is interesting to note that fully connected only architecture performs the best among all architectures that we experimented in the past few hours. The approach does not perform very well when applied to the convolutional networks because the matrix is very sparse leading to very loose interval bounds with the top-k approach. Moreover, the interval bounds would be vacuous when the number of pixels k is larger than the size of the convolutional kernel. It seems like increasing the size of the convolutional kernel (in the first layer only) improves the performance, but still does not match that of the fully connected. We think it is a very interesting direction to adapt the L0 interval bound to work better with the convolutional layer, and we will further investigate this in our future work.\n\n>>> We will also add the method and results in the manuscript upon acceptance.\n", "title": "Thanks for point this out!"}, "r1lKQwihYS": {"type": "review", "replyto": "HyeaSkrYPH", "review": "The paper proposes a certified defense for adversarial patch attacks.\nTechnically, the authors use the well developed IBP based methods (Gowal et\nal., 2018, Mirman et al., 2018, Zhang et al. 2019).  The technique is simple\nbut effective. Since the number of possible patches are quadratic w.r.t image\ndimension, to reduce the number of bounds to propagate, the authors propose a\nU-Net based NN to predict the worst case scenario, and only propagate \"worst\ncase\" bounds predicted by the U-net.\n\nEmpirically, the proposed method gets good results, with certified accuracy\nsometimes even higher than empirical accuracy by previous methods.  The authors\nalso provide results for transferring robustness properties to shapes that are\nnot included during training.\n\nOverall, the contribution of this paper is novel, and results are promising,\nbut it still has some missing components, especially the idea of combining\nmultiple IBP bounds into one, which can be very effective for adversarial\npatches, as I will elaborate below.\n\nSuggestions and Questions:\n\nThe core idea behind IBP is that for whatever input perturbation is given (any\nLp norm or semi-norm, or non-norm based perturbations like patches at arbitrary\nlocations), it converts them to per-neuron lower and upper bounds after the\nfirst linear/conv layer.  For example, if the input perturbation is *two* patches\nB_1 and B_2, after propagating them through the first layer of the network, we\ngot two lower bounds l_{i,1}, l_{i,2} and two upper bounds u_{i,1}, u_{i,2} for\nthe i-th neuron. We then take the worst case bound, l_i = min(l_{i,1},\nl_{i,2}), u_i = max(u_{i,1}, u_{i,2}) and propagate only one set of bounds l_i\nand u_i to the next layer. The authors should explore on this direction, as\ndetailed below:\n\n1. For the exhaustive patch enumeration in (11), we can actually greatly reduce\nthe computation cost by combining the bounds of different patches after the\nfirst layer of the network, as I mentioned above. At the input layer, the\nnumber of bounds (each for one possible location of patch) are large; but after\nthe first linear/conv layer, we can compress them to one or a small group of\nbounds by taking the worst cast of them, like l_i = min(l_{i,1}, ...,\nl_{i,|L|}), u_i = max(u_{i,1}, ..., u_{i,2}). The patches close with each other\nshould also produces similar lower and upper bounds, so taking min or max over\nthem will not make the combined bounds much worse. This is better than U-net\nprediction since we are guaranteed to include the worst case scenario.\n\n2. Considering multiple patches (at different locations) on a single input. In\nthe simplest case, consider multiple 1x1 patches (in fact, this is equivalent\nto bounded L0 norm threat model); since each patch is 1x1 (only changing one\npixel), the bounds should be relatively tight after the first layer, and after\nthe first linear/conv layer we got 28*28 or 32*32 bounds which will be combined\ninto one or very few sets of lower and upper bounds that will be propagate into\nlater layers.  Multiple larger patches (2x2, 5x5) can be difficult since bounds\nare looser; multiple 1x1 in my opinion is both technically feasible and\npractically important, and should definitely be included in this paper.\n\n\nMinor issues:\n\n1. Eq. (5) and (6) are incorrect; the second term should be \\overline{z} -\n\\underline{z}. Also it is missing the bias term.\n\n2. In related works (section 4.1, page 3), (Weng et al., 2018) is not a defense\nmethod (it is a certification method, and no training is involved).\n\n3. In Table 3, it is better if the authors can provide empirical adversarial\naccuracy to IBP defended networks as well.\n\nOverall, I think it is a good paper but the authors should explore more to\nstrengthen their contributions. I gave a weak reject but I will not hesitate\nto recommend an accept as long as the authors can provide additional results\nmentioned above.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 4}, "B1g8i7IoiB": {"type": "rebuttal", "replyto": "r1lTNkw6tS", "comment": "Thanks for the careful remarks with a healthy dose of skepticism that helped improve the manuscript. We ran additional experiments using a larger model, as requested, and elaborated on the significance of the reported results w.r.t. state-of-the-art in certified accuracy.\n\n*** Main Comments ***\n\nR1: \u201cAn experiment that is definitely missing is a CIFAR network that performs a little better\u201d\n\nAuthor(s): Per your suggestion, in the revised manuscript, we have trained a new model which performs considerably better in terms of validation accuracy on benign examples. The new defended model which is a considerably larger model achieves 66.5% accuracy* on the benign examples (a 16.2% increase compared to the narrower convolutional models in the original submission). The certified accuracy for the new model on the CIFAR 2x2 task is also improved to 51.9% (a 10.3% increase). The large model significantly outperforms the previous all-patch trained model, even though it has only been trained using guided patches. Note that we were not able to complete all-patch training during the rebuttal phase on the large model as it takes almost 15 days to complete the training on our workstation.\n\n>>> We have updated our manuscript to include the experimental results for the large model in Appendix A.4. Upon acceptance, we will include all-patch results in the camera-ready version.\n\n(*) For the non-defended model, the large model achieve 88.9% clean accuracy.\n\n\nR1: \u201csometimes the IBP approach is useful for defense, although often with a significant loss in accuracy on clean data\u201d\n\nAuthor(s): The trade-off between robustness and generalization has been argued to exist both from a theoretical perspective and also empirically [3, 4]. Some loss in accuracy in certified robustness compared to clean accuracy is typical for all certified (and most empirical) defenses. Our proposed certified defense, which is the first certified defense against adversarial patch attacks, is no exception to this. To put our reported results in perspective, we consider state-of-the-art results for (a) certified robustness against L-inf perturbations, and (b) (non-certifying) empirical defenses against patch attacks; see below. \n\n(a) The State-of-the-art certified model for CIFAR with 16/255 L-inf perturbation using deterministic algorithms [5] achieves clean accuracy of 34.0%, whereas our clean accuracy for our most robust CIFAR 5 \u2a09 5 model is 47.8% (for the updated large model). To further appreciate the performance of our model, note that our threat model allows the attacker to change each pixel within the patch to any desired value, whereas the L-inf perturbation model of [5] assumes the adversary can only change each pixel by at most some epsilon, e.g., 16/255. \n\n>>> This remark was added to the manuscript at the bottom of Section 5.1.\n\n(b) State-of-the-art empirical (non-certified) defenses against patch attacks achieve much lower robustness (i.e, adversarial accuracy) than our approach. In particular, local gradient smoothing (LGS) [2] achieves 24.2% empirical accuracy on CIFAR 2\u2a092 while our method achieves 51.9% certified accuracy (for the largest model trained). Similarly, digital watermarking (DW) [1] achieves 15.6% empirical accuracy on CIFAR 5\u2a095 while our method achieves 30.3% certified accuracy (for the largest model trained).\n\nFinally, we acknowledge that certified defenses may not be ready for real world applications.  However, our contribution makes an important step towards securing ML systems against adversarial patch attacks by substantially improving both the clean and robust accuracy achievable by a certified model.\n\n\nReferences\n\n[1] Jamie Hayes. On visible adversarial perturbations & digital watermarking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1597\u20131604, 2018.\n\n[2] Muzammal Naseer, Salman Khan, and Fatih Porikli. Local gradients smoothing: Defense against localized adversarial attacks. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1300\u20131307. IEEE, 2019.\n\n[3] Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness the cost of accuracy?\u2013a comprehensive study on the robustness of 18 deep image classification models. In Proceedings of the European Conference on Computer Vision (ECCV), pages 631\u2013648, 2018.\n\n[4] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. Theoretically principled trade-off between robustness and accuracy. ICML, 2019.\n\n[5] Zhang, Huan, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. \"Towards Stable and Efficient Training of Verifiably Robust Neural Networks.\" arXiv preprint arXiv:1906.06316 (2019).\n\n", "title": "Response to Reviewer 1 (1/2)"}, "H1eib7IisS": {"type": "rebuttal", "replyto": "r1lTNkw6tS", "comment": "Continued .. \n\nR1: \u201ccontribution of the paper is a bit weak in that they mostly followed the original IBP and the only novelties are the random patch training and guided patch training\u201d \n\nOur contributions are a mix of technical advancements and empirical improvements.\n\nOur framework is indeed based off the IBP concept, however the proposed adaptation to patch attacks (a more realistic attack model in many applications) is non-trivial, and we explore three different strategies towards practical adversarial patch training. Compared to the straightforward all-patch defense, under a fixed computational budget, the accuracy of the proposed random-patch training and guided-patch training is significantly improved, which is not necessarily obvious or expected. Even the seemingly interesting idea of incorporating bound pooling after the first layer, as proposed by the expert Reviewer #2, did not reduce computation enough to scale to larger models.\n\nWe also make a range of empirical observations that we think are important enough to count as contributions. Namely, we show that state-of-the-art empirical patch defenses are easily breakable, and quantify this effect.\n\nFinally, the clean and robust accuracies we achieve exceed other L-infinity certified models and existing patch defense methods. As such, we believe that our work makes an important step towards patch defenses.\n\nR1: \u201creal-life images with patches that are of real-life size\u201d\n\nAuthor(s): As CIFAR images are only 32\u2a0932, we are unable to experiment with patch size this big. If we want to do certification for 40x40, we would have to conduct our experiments on ImageNet. As it currently stands, IBP has difficulty extending to imagenet sized datasets, and we leave this as future work. Note that CIFAR-10 is currently a widely used benchmark for adversarial defense (both empirical and certified).\n\n*** Other Comments ***\n- We have fixed the typos in equation (5) & (6)\n- Yes, we forward pass all patches in order to get a certificate for all of our experiments. We have included an additional paragraph in section 4.2 to clarify.\n- Our lower and upper bound is simply the image domain (0-1), and we don't really have an epsilon.\n- We added description for the element-wise absolute value\n- There was a minor bug in our code that didn\u2019t have a significant impact on any of the results. All tables were updated with new runs.", "title": "Response to Reviewer 1 (2/2)"}, "S1xAU9vqoB": {"type": "rebuttal", "replyto": "r1lKQwihYS", "comment": "Thanks for the support and valuable suggestions!\n\n*** Main Comments ***\n\nR2: \u201cstill has some missing components, especially the idea of combining multiple IBP bounds into one [..] we can actually greatly reduce the computation cost by combining the bounds of different patches after the first layer of the network. [..] This is better than U-net prediction ..\u201d\n\nAuthor(s): The proposed bound pooling still scales quadratically w.r.t. image dimension, although it does reduce computational costs compared to the straightforward all-patch training. In contrast, the proposed random- and guided-patch approaches scale better allowing the training of larger models.\n\nA new Appendix A.3 includes extra experiments with bound pooling: We tested bound pooling over 2\u2a092/4\u2a094 patches in the first layer, 2\u2a092 patches in the first layer and 4\u2a094 patches in the second layer. Compared to all-patch training, this implementation of bound pooling indeed reduces the computational cost by 25-35% while being only slightly worse in terms of certified accuracy; please refer to Table 5.\n\n\nR2: \u201cConsidering multiple patches (at different locations) on a single input. [..] multiple 1x1 in my opinion is both technically feasible and practically important, and should definitely be included in the paper\u201d \n\nAuthor(s): Unfortunately, this interesting setup is computationally infeasible. We actually considered this formulation earlier but could not pursue it due to the combinatorial explosion in the number of patches, see below for more details.\n\nIn the specific example of 1\u2a091 patch, when we take the maximum of upper bounds generated by all patches, we are not getting the upper bounds for moving all pixels at the same time, rather we get the worst case bounds when we are allowed to move a single pixel, but not moving any of them together. To give an actual certificate, we have to consider (32\u2a0932) position for the first patch and (32\u2a0932-1) position for the second patch, and we would have to forward pass all of these possible combinations forward in order to get an actual certificate. Unfortunately, this is computationally infeasible, but is very interesting nonetheless.\n\n\n*** Other Comments ***\n- We have removed reference to Weng's\n- We have fixed the typos in equation (5) & (6)\n- We have included empirical adversarial accuracy to Table 3\n- There was a minor bug in our code that didn\u2019t have a significant impact on any of the results. All tables were updated with new runs.\n", "title": "Response to Reviewer #2"}, "Byg569Dcsr": {"type": "rebuttal", "replyto": "rJefUZ0iFr", "comment": "Thanks for the positive assessment and supportive remarks!\n\nR3: \u201chave to conduct many forward pass of the network with respect to all patches at all locations?\u201d\n\nAuthor(s): Yes, we forward pass all patches in order to get a certificate. We have included an additional paragraph in section 4.2 to clarify.\n\n***Other Comments***\n- There was a minor bug in our code that didn\u2019t have a significant impact on any of the results. All tables were updated with new runs.\n", "title": "Response to Reviewer #3"}, "rJefUZ0iFr": {"type": "review", "replyto": "HyeaSkrYPH", "review": "This paper proposed a certified defense method for adversarial patches. The paper is motivated by the finding that several existing works on adversarial patch defenses are easily \"breakable\" by in the white-box setting. The idea of the proposed method is derived Interval Bound Propagation (IBP), which is originally proposed for certified defense against adversarial noise. To simplify the certificate training in the patch defense setting (which original scales quadratically with respect to the image size), two randomized training methods are proposed. Lastly, experimental results indeed verify the effectiveness of the proposed method.\n\nThe paper is well-written and very well-organized. It is interesting to see supposedly strong adversarial patch defense methods \"break down\" in a very simple setting. And the contribution of the paper is significant to the field.\n\nI do have a question with regard to the randomized training method:\nAlthough random patches (or selected worse-case patches at random location) can be used for certificate training, in order to create a certificate during testing, does it mean that you do have to conduct many forward pass of the network with respect to all patches at all locations?", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}}}