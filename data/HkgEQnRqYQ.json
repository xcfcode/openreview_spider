{"paper": {"title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space", "authors": ["Zhiqing Sun", "Zhi-Hong Deng", "Jian-Yun Nie", "Jian Tang"], "authorids": ["1500012783@pku.edu.cn", "zhdeng@pku.edu.cn", "nie@iro.umontreal.ca", "jian.tang@hec.ca"], "summary": "A new state-of-the-art approach for knowledge graph embedding.", "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.", "keywords": ["knowledge graph embedding", "knowledge graph completion", "adversarial sampling"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a knowledge graph completion approach that represents relations as rotations in a complex space; an idea that the reviewers found quite interesting and novel. The authors provide analysis to show how this model can capture symmetry/assymmetry, inversions, and composition. The authors also introduce a separate contribution of self-adversarial negative sampling, which, combined with complex rotational embeddings, obtains state of the art results on the benchmarks for this task.\n\nThe reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper.\n\nThe reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation.\n\nWith the revision, the reviewers agreed that this is a worthy paper to include in the conference.\n"}, "review": {"HJxJOMP-jr": {"type": "rebuttal", "replyto": "SkeaA0NWsr", "comment": "Hi Rajiv,\n\nThis is an implementation choice for the modulus constraint for the relation embeddings. In this repository, we use these real-valued vectors to represent the phases of the relation embeddings, while use doubled real-valued vectors to represent complex-valued embeddings.\n\nA relevant discussion can be found at https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding/issues/7", "title": "This is an implementation for the modulus constraint for the relation embeddings"}, "HJxiR1c9TV": {"type": "rebuttal", "replyto": "SJlXT2Yc64", "comment": "````\"\"\nWe re-implement a 50-dimension TransE model with the margin-based ranking criterion that was used in (Cai & Wang, 2017), and evaluate its performance on FB15k-237, WN18RR and WN18 with self-adversarial negative sampling.\n\"\"", "title": "Their embedding dimensions are different"}, "BkewKmXwH4": {"type": "rebuttal", "replyto": "ByxtN4yRR7", "comment": "Here is our results on FB15k training set:\n\nTask Prediction Head (MRR) Prediction Tail (MRR) MRR\nRelation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N Overall\nRotatE 0.998 1.000 0.969 0.999 0.998 0.961 1.000 0.999 0.995", "title": "Yes, RotatE can perfectly fit the training set"}, "rkgm5j1aAQ": {"type": "rebuttal", "replyto": "Hkxa6j_oAQ", "comment": "Thanks for your understanding! You are right! \u2018ordinal\u2019 is not sufficient in the case when the true triple comes earlier in the list, especially when the true triplet is put in the beginning of the list. The ConvKB\u2019s updated new eval.py [1] suffers this problem by always putting the true triplet in the first position (see the codes below).\n\n#thus, insert the valid test triple again, to the beginning of the array\nnew_x_batch = np.insert(new_x_batch, 0, x_batch[i], axis=0)\nnew_y_batch = np.insert(new_y_batch, 0, y_batch[i], axis=0)\n\nIn this case, \u2018ordinal\u2019 is essentially equivalent to \u2018min\u2019, so it\u2019s not sufficient. However, this problem can be easily addressed by randomly shuffling the list. \n\n[1] https://github.com/daiquocnguyen/ConvKB/commit/c7ee60526ee81b46c2b0075cca2e387b0dbc6e90\n", "title": "You are right!!"}, "H1e_OITis7": {"type": "review", "replyto": "HkgEQnRqYQ", "review": "# Summary\nThis paper presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base, whereas previous methods were only able to support a subset. The method achieves state of the art on FB15k-237, WN18RR and Countries benchmark knowledge bases. I think this will be interesting to the ICLR community. I particularly enjoyed the analysis of existing methods regarding the expressiveness of relational patterns mentioned above.\n\n# Strengths\n- Improvements over prior neural link prediction methods\n- Clearly written paper\n- Interesting analysis of existing neural link prediction methods\n\n# Weaknesses\n- As the authors not only propose a new scoring function for neural link prediction but also an adversarial sampling mechanism for negative data, I believe a more careful ablation study should have been carried out. There is an ablation study showing the impact of the negative sampling on the baseline TransE, as well as another ablation in the appendix demonstrating the impact of negative sampling on TransE and the proposed method, RotatE, for the FB15k-237. However, from Table 10 in the appendix, one can see that the two competing methods, TransE and RotatE, in fact, perform fairly similarly once both use adversarial sampling it still remains unclear whether the gains observed in table 4 and 5 are due to adversarial sampling or a better scoring function. Particularly, I want to see results of a stronger baseline, ComplEx, equipped with the adversarial sampling approach. Ideally, I would also like to see multiple repeats of the experiments to get a sense of the variance of the results (as it has been done for Countries in Table 6).\n\n# Minor Comments\n- Eq 5: Already introduce gamma (the fixed margin) here.\n- While I understand that this paper focuses on knowledge graph embeddings, I believe the large body of other relational AI approaches should be mention as some of them can also model symmetry, anti-symmetry, inversion and composition patterns of relations as well (though they might be less scalable and therefore of less practical relevance), e.g. the following come to mind:\n  - Lao et al. (2011). Random walk inference and learning in a large scale knowledge base.\n  - Neelakantan et al. (2015). Compositional vector space models for knowledge base completion.\n  - Das et al. (2016). Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks. \n  - Rocktaschel and Riedel (2017). End-to-end Differentiable Proving.\n  - Yang et al. (2017). Differentiable Learning of Logical Rules for Knowledge Base Completion.\n- Table 6: How many repeats were used for estimating the standard deviation?\n\n\nUpdate: I thank the authors for their response and additional experiments. I am increasing my score to 7.", "title": "Is it the RotatE scoring function or the adversarial sampling?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1gmLa420X": {"type": "rebuttal", "replyto": "SJluiXkhRm", "comment": "We first would like to provide some theoretical analysis to show that the RotatE model can also somehow model the 1-to-N relations. Taking a 1-to-N relation r as an example. The triplets having the head entity x and relation r are denoted as: r(x, y1), r(x, y2) \u2026. r(x, yn). When the optimization converges, it could be easily to find out that the embeddings of y1, y2, \u2026, yn will be evenly distributed on the surface of a hypercube (or a hypersphere in the case of L-2 norm) centered at rx. In other words, ||rx - y1|| = ||rx - y2|| = .. = ||rx - yn||. This phenomenon is the same as in semantic matching models, like ComplEx, where the scores <r,x,\\bar{y1}>=<r,x,\\bar{y2}>=..=<r,x,\\bar{yn}>. Therefore, the RotatE model can somehow deal with 1-to-N relations just like ComplEx, as well as TransE.\n\nA more elegant and rigorous approach to model the 1-to-N, N-to-1, and N-to-N relations is to leverage a probabilistic framework to model the uncertainties of the entities, where each predicted entity is represented as a Gaussian distribution. This has been proved quite effective in [1]. Our RotatE model can easily leverage this framework to mitigate this issue. \n\nAnother thing to note is that the focus of this paper is to model and infer the different types of relation patterns, but not the 1-to-N, N-to-1, and N-to-N relationships. However, we will conduct further experiments to compare the performance of different methods (TransE, ComplEx and RotatE) on the 1-1, 1-to-N, N-to-1, and N-to-N relationships. \n\n[1] Shizhu He, Kang Liu, Guoliang Ji and Jun Zhao, Learning to Represent Knowledge Graphs with Gaussian Embedding\n", "title": "The RotatE model is somehow able to model 1-to-N relations."}, "Sye6N04n0m": {"type": "rebuttal", "replyto": "rkgeGrg_Tm", "comment": "Thanks for such a good question! We have provided some theoretical analysis to show that the RotatE model can also somehow model the 1-to-N relations. Please refer to our response to Reviewer2.", "title": "Please refer to our reply to Reviewer2"}, "SJxD-H5t0m": {"type": "rebuttal", "replyto": "HJlFFR7167", "comment": "Thanks for your comments!! The difference between RotatE and ComplEx can be summarized as follows:\n\n(1)ComplEx belongs to the semantic matching model while RotatE belongs to the distance-based model. Most of existing knowledge graph embedding models can be roughly classified into two categories: Translational(Transformational) Distance Models and Semantic Matching Models [1]. The former measure the plausibility of a fact as a translation(transformation) between two entities, while the latter measure the plausibility of facts by matching latent semantics of entities and relations. RotatE and ComplEx are in different categories. Actually, we can find that the relation between ComplEx and RotatE is in analogy to the relation between TransF [2] and TransE, where the former can be regarded as a slack version of the latter.\n\n(2) As a result, the biggest difference between ComplEx and RotatE addressed in this paper is that, the RotatE model can infer the composition pattern of relations, while the ComplEx model cannot. A simple counterexample could illustrate this point.\n\nLet\u2019s assume r1(x, y), r2(y, z) and r3(x, z) hold, and then according to ComplEx we have\n\nRe(<r1, x, \\bar{y}>)  >  Re(<r1, x\u2019, \\bar{y\u2019}>)\nRe(<r2, y, \\bar{z}>)  >  Re(<r2, y\u2019, \\bar{z\u2019}>)\nRe(<r3, x, \\bar{z}>)  >  Re(<r3, x\u2019, \\bar{z\u2019}>)\n\nwhere r1(x\u2019, y\u2019), r2(y\u2019,z\u2019) and r3(x\u2019, z\u2019) are negative triplets.\n\nFrom the above equations, we can find that the ComplEx model does not model a bijection mapping from h to t via relation r.  For example, let x=-1+i, y=1, z=1+i, r1=-1-0.8i, r2= 0.2+i, r3=-0.8-i, we have r1(x, y), r2(y, z) and r3(x, z) hold, because\n\n<r1, x, \\bar{y}> = 1.8 -  0.2i\n<r2, y, \\bar{z}> = 1.2 + 0.8i\n<r3, x, \\bar{z}> = 2 - 1.6i\n\nHowever, r1 * r2 = 0.6 - 1.16i, r3= - 0.8 - i do not show the supposed pattern r1 \\circ r2 = \\alpha r3 here.\n\nAs for the comparison with TransE, the rotation in the RotatE model is in the complex plane of each embedding vector element, as the same as TransE. This is different from the rotation is in the whole embedding space by matrix multiplication.\n\n\u201cAbout experiments, for fair comparisons, results should be reported on common and standard settings, especially with and without new negative sampling method\u2026.\u201d\n\nWe have added the results of TransE and ComplEx with the new adversarial negative sampling technique on three datasets in Table 8. \n\n\u201cThe authors should also address how they estimate/or approximate the softmax in Equation 4 of negative sampling method to scale to large datasets, because it is very costly due to the normalization term. ...\u201d\n\np(h\u2019_j , r, t\u2019_j |{(h_i , r_i , t_i)}) is defined as the probability that we sample (h\u2019_j , r, t\u2019_j) from a sampled set {(h_i , r_i , t_i)}, so we calculate the softmax function only on the sampled triplets. This is very efficient.\n\n\u201c It's also not clear what $ f_r $ refers to in Equation 4.\u201d\n\n $f_r$ is the score function introduced in Table 1, which equals to $- d_r$.\n\n[1]  Knowledge Graph Embedding: A Survey of Approaches and Applications\n[2]  Knowledge graph embedding by flexible translation\f", "title": "Thanks for your comments!!"}, "r1xF475K0m": {"type": "rebuttal", "replyto": "HkxYw_tn3Q", "comment": "Thanks for your verification for your model. We do agree that the implementation of your model is correct. However, what we pointed out is that your evaluation is problematic!!\n\nFor your updated eval.py, we find that you used the following code to get the rank for each triplets:\n\nresults_with_id = rankdata(results, method='min')\n\nwhere \u2018min\u2019 represents \u201cThe minimum of the ranks that would have been assigned to all the tied values is assigned to each value. (This is also referred to as \u201ccompetition\u201d ranking.)\u201d according to the official document [1].\n\nHowever, such \"a specific ranking procedure\" tends to rank the true positive triplets in a high position, if there are many triplets with the same score.\n\nA simple example is that a model produce score=b for all triplets, then results_with_id = rankdata(results, method='min') will return the results that all the triplets are ranked in the first position. In other words, in this case MRR = 1, which is definitely wrong.\n\nMoreover, as mentioned in [2], we have fixed the bug in your previous codes and reported the true performance of your model on FB15k-237. We provided the checkpoint file, where you can check that MRR = 40 by your original eval.py, but 24 by our bug-fixed eval.py.\n\nAs for your updated codes, we suggest that you should replace the \u201crankdata\u201d part by:\n\nresults_with_id = rankdata(results, method=\u2019ordinal\u2019)\n\nwhere \u2018ordinal\u2019 represents \u201cAll values are given a distinct rank, corresponding to the order that the values occur in a.\u201d  according to the official document [1]. Although the results may be a little different from the results of our released bug-fixed eval.py [2] (We used quicksort ranking by following you), it would also provide a valid evaluation for your model.\n\nFor the previous codes, We opened a pull that fix the bug (https://github.com/daiquocnguyen/ConvKB/pull/3), but it was closed. For your new codes, we also opened a pull to fix the bug (https://github.com/daiquocnguyen/ConvKB/pull/4).\n\nFinally, we want to emphasize again that we did not intend any offence to your work. The truth is that we found a problem, and we want to make it right.  \n\n[1]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rankdata.html\n[2]: https://github.com/KnowledgeBaseCompleter/eval-ConvKB", "title": "Your updated codes still have the same problem."}, "Bkxn0G5YR7": {"type": "rebuttal", "replyto": "HJlUq8jq3X", "comment": "Thanks for your appreciation to our work and the great comments. We\u2019ve revised the introduction part on the representations in complex domain. \n\n\u201cThe optimization section does not mention how constraints are imposed.\u201d\n\nSince each relation is modeled as a rotation in the complex vector space, we represent each relation r according to its polar form with its modulus as 1, i.e., \nRe(r) = sine(\\theta), and Im(r) = cosine(\\theta), where \\theta is the phase of relation r. With the polar form representation, the constraints can be easily satisfied.\n\n\u201cIn experiments, how does the effective number of parameters that are used to express representations compare when the representations are a complex vs a real number \u2026.\u201d\n\nIf the same number of dimension is used for both the real and imaginary parts of the complex number as the real number, the number of parameters for the complex embedding would be twice the number of parameters for the embeddings in the real space. To make a fair comparison, in the process of grid search for finding the optimum embedding dimension, we double the range of the search space  for models represented in real space such as TransE. \n\n\u201cSince the method is reported to beat several number of competitors, it is useful to provide the code.\u201d\n\nYes, we will definitely release our code and share it with the entire community. ", "title": "Thanks for your appreciation and great suggestions!"}, "Skl2_GqtAm": {"type": "rebuttal", "replyto": "H1e_OITis7", "comment": "\u201cParticularly, I want to see results of a stronger baseline, ComplEx, equipped with the adversarial sampling approach\u2026.\u201d\n\nWe have added the experimental results of TransE and ComplEx on three datasets in our paper (Table 8). We can see that our proposed approach still outperforms ComplEx with the new adversarial approach, especially on the data set FB15k-237 and Countries. The reason is that  FB15k-237 and Countries contain many composition patterns, which cannot be modeled by ComplEx but can be effectively modeled by RotatE.\n\n\u201cIdeally, I would also like to see multiple repeats of the experiments to get a sense of the variance of the results...\u201d\n\nWe also added the variance of the results of our model on different data sets, which are summarized into Table 12 in the appendix. We can see that the variance of the results are very small, 0.001 at maximum. \n\n\u201cTable 6: How many repeats were used for estimating the standard deviation?\u201d\n\nOnly 3 are used. Since the variance are very small, the same results are obtained with more repeats.\n\n\u201cWhile I understand that this paper focuses on knowledge graph embeddings, I believe the large body of other relational AI approaches should be mention\u2026.\u201d\n\nWe have added some discussion on these methods in the related work section.", "title": "Thanks for the great comments and suggestions!"}, "BJx75b5FCX": {"type": "rebuttal", "replyto": "HJlYlIhn2X", "comment": "Thanks for your appreciation to our work and your great comments on improving the paper. We have added the experimental results of TransE and ComplEx with self-adversarial negative sampling on three datasets in our paper (Table 8). We have also added the contribution of the self-adversarial negative sampling into both the abstract and introduction.\n\nRegarding TorusE, thanks again for bringing it to our attention, which we did not notice before. It is indeed relevant to our model, which is a concurrent work. We have discussed this model in the related work section. The difference between TorusE and RotatE can be summarized as below:\n\n(1) The TorusE model constraints the embedding of objects on a torus, and models relations as translations, while the RotatE model embeds objects on the entire complex vector space, and models relations as rotations.\n\n(2) The TorusE model requires embedding objects on a compact Lie group [2] while the RotatE model allows embedding objects on a non-compact Lie group, which has much more representation capacity. The TorusE model is actually very close to a special case of our model, i.e., pRotatE, which constraints the modulus of the head and entity embeddings fixed. As shown in Table 5, it is very important for modeling and inferring the composition patterns by embedding the entities on a non-compact Lie group. We can also compare the results of TorusE and RotatE on the FB15k and WN18 data sets (Table 3 in the TorusE paper and Table 4 in our paper), we can see that our RotatE model significantly outperforms TorusE on the two data sets.\n\n(3) The motivations of the TorusE paper and this paper are quite different. The TorusE paper aims to solve the regularization problem of TransE, while our paper focuses on inferring and modeling three important and popular relation patterns.\n\n[1] Ebisu, Takuma, and Ryutaro Ichise. \"Toruse: Knowledge graph embedding on a lie group.\" arXiv preprint arXiv:1711.05435 (2017).\"\n[2] https://en.wikipedia.org/wiki/Compact_group#Compact_Lie_groups", "title": "Thanks for your appreciation to our work and mentioning another relevant work!"}, "HJlYlIhn2X": {"type": "review", "replyto": "HkgEQnRqYQ", "review": "The authors propose to model the relations as a rotation in the complex vector space. They show that this way one can model symmetry/antisymmetry, inversion and composition. Another contribution is the so-called self-adversarial negative sampling.\n\nPros:  The problem that they raise is important and the solution is relevant. The results considering the simplicity of the proposed model are impressive. The experiments, proof of lemmas and general overview are easy to follow, well-written and well-organized.  The improvement given the negative sampling approach is also noteworthy.\n\nCons: Nevertheless, this approach is very similar to TorusE [1], since the element-wise rotation on the complex plane is somehow related to transformation on high-dimensional Torus. Therefore, it is expected from the authors to investigate the differences between these two approaches.\n\nSuggestions:\nAlso, it is important to note the result of ablation study on Table 10 in supplementary materials, since part of the improvement does not come only from how the authors model the relation but also from the negative sampling(which could improve the results of other works as well). Maybe it is even better if Table 10 is presented in the main paper. \nAnother suggestion is to mention the negative sampling contribution also in the abstract.\n\n\n[1] Ebisu, Takuma, and Ryutaro Ichise. \"Toruse: Knowledge graph embedding on a lie group.\" arXiv preprint arXiv:1711.05435 (2017).\"\n", "title": "This paper is an important new contribution to the field.   The results should be compared to TorusE.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJlUq8jq3X": {"type": "review", "replyto": "HkgEQnRqYQ", "review": "The paper proposes a method for graph embedding to be used for link prediction, in which each entity is represented as a vector in complex space and each relation is modeled as a rotation from the head entity to the tale entity. \nFrom the modeling perspective, the proposed model is rich as many type of relations can be modeled with it. In particular, symmetric and anti-symmetric relations can be modeled. It is also possible to model the inverse of a relation and the composition of two relations with this setup. Empirical evaluation demonstrates that method is effective and beats a number of well known competitors.\n\nThis is a solid work and could be of interest in the community. Modeling is elegant and experimental results are strong.\nI have not seen it proposed before.\n\n- The presentation of paper could be improved, in particular the first paragraph of page 2 where the representation in complex domain is introduced is hard to follow and could be improved by inserting formulations instead of merely text.  \nIt would be nice to explicitly mention the number of real and imaginary dimensions of the complex vectors and provide explicit formulation for the Hadamard product on the complex domain, since the term elementwise could be ambiguous.\n- The optimization section does not mention how constraints are imposed. This is an important technicality and should be clarified.\n- In experiments, how does the effective number of parameters that are used to express representations compare when the representations are a complex vs a real number? Each complex number is presented with two parameters and each real number with one parameter. How is that taken into account in experiments\n- Since the method is reported to beat several number of competitors, it is useful to provide the code.\n\n \nBased on the results above, I vote for the paper to be accepted.\n", "title": "Solid work", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyekBjPinm": {"type": "rebuttal", "replyto": "HJxnXzWVnX", "comment": "Hi Dai, \n     Thanks for the verification. In the above comment, sorry we meant that many triplets have the same score, which equals to the bias of your model, i.e., b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\") in your model.py code. The reasons is that in many cases all the nonlinear RELU units are not activated. In addition, we found that this problem would only occur when the nonlinear activation Relu is used in the model. This explains why the evaluation of other models, including TransE, TransR, TransH and STransE, are correct. \n\nWe suggest you to re-evaluate your model without replicating the true triplets. We\u2019ve fixed this debug in your code and put the updated codes in https://github.com/KnowledgeBaseCompleter/eval-ConvKB . \n\nBy the way, we appreciate your work, which we find is really interesting.  We did not intend any offence to your work. We hope we can push forward this exciting direction together. We look forward to your feedback.\n", "title": "Sorry we meant that many triples have the same score, and an open evaluation code is now available."}, "HklyVUAX2m": {"type": "rebuttal", "replyto": "rkerutP2tQ", "comment": "Thanks for pointing this out! We\u2019re aware of the result of ConvKB, which achieves a very high MRR on FB15k (0.396). The reason that we did compare with ConvKB [1] is that there is a bug in ConvKB\u2019s evaluation.\n\nWe tried to reproduce their results from their published code [2], but found that the ConvKB tends to assign the same score, i.e., 0,  to many triplets. The reason is that the RELU activation function is used in the convolution layers, which tends to have very sparse output, i.e., the output of many neurons are zero. This brings a big problem in the evaluation.\n\nFor evaluation, given a query (h,r, ?), the goals is to identify the rank of the true positive triplets (h, r, t) among all the possible (h, r, t\u2019) triplets. Since the scores of many triplets given by ConvKB equal to 0 (typo, should be \"the same score\" or \"bias\"), the true positive triplets and many other false triplets are all ranked the first position at the same time. A reasonable solution would be to randomly pick a triplet among those triplets as the first ranked triplet, and so on. However,  we find that a specific ranking procedure is used by ConvKB, which tends to rank the true positive triplets in a high position. As a result, the performance evaluated in this way is really high, which is not true in reality. We strongly suggest the authors of ConvKB to take a look at this issue and fix their results.\n\nFor the results of Reciprocal ComplEx-N3, thanks again for pointing this out, which we are not aware of before the submission. However, note that the focus of the Reciprocal ComplEx-N3 and this paper is different. Our paper proposes a new distance function for learning knowledge graph embedding, and our proposed RotatE is able to infer three relation patterns including composition, symmetry/asymmetry, and inversion, which offers good model interpretability. The focus of Reciprocal ComplEx-N3, however, is on different regularization techniques, which could be potentially applied to our proposed RotatE model. For example, on the FB15k data set, the performance of RotatE increases from 0.797 to 0.815 with the N3-regularizer, which outperforms  the performance of ComplEx-N3 on FB15k (0.80).  We are still in the process of implementing the reciprocal setting for our RotatE model, which seems to be pretty effective according to [3]. \n\n[1] A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network\n[2] https://github.com/daiquocnguyen/ConvKB\n[3] Canonical Tensor Decomposition for Knowledge Base Completion\n", "title": "Thanks for pointing this out!"}}}