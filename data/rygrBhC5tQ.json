{"paper": {"title": "Composing Complex Skills by Learning Transition Policies", "authors": ["Youngwoon Lee*", "Shao-Hua Sun*", "Sriram Somasundaram", "Edward S. Hu", "Joseph J. Lim"], "authorids": ["lee504@usc.edu", "shaohuas@usc.edu", "sriramso@usc.edu", "hues@usc.edu", "limjj@usc.edu"], "summary": "Transition policies enable agents to compose complex skills by smoothly connecting previously acquired primitive skills.", "abstract": "Humans acquire complex skills by exploiting previously learned skills and making transitions between them. To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at. We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills. The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments. We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition .", "keywords": ["reinforcement learning", "hierarchical reinforcement learning", "continuous control", "modular framework"]}, "meta": {"decision": "Accept (Poster)", "comment": "Strengths: The paper tackles a novel, well-motivated problem related to options & HRL.\nThe problem is that of learning transition policies, and the paper proposes\na novel and simple solution to that problem, using learned proximity predictors and transition\npolicies that can leverage those. Solid evaluations are done on simulated locomotion and\nmanipulation tasks. The paper is well written.\n\nWeaknesses: Limitations were not originally discussed in any depth. \nThere is related work related to sub-goal generation in HRL.\nAC: The physics of the 2D walker simulations looks to be unrealistic;\nthe character seems to move in a low-gravity environment, and can lean\nforwards at extreme angles without falling. It would be good to see this explained.\n\nThere is a consensus among reviewers and AC that the paper would make an excellent ICLR contribution.\nAC: I suggest a poster presentation; it could also be considered for oral presentation based\non the very positive reception by reviewers."}, "review": {"r1lb_e98Tm": {"type": "rebuttal", "replyto": "BkghwsHAnQ", "comment": "We thank the reviewer for the feedback and address the concerns in detail below.\n\n> Reviewer 3 (R3): \u201c... the choice of exponential (\u201cdiscounted\u201d) proximity function. Wouldn\u2019t a linear function of \u201cstep\u201d be more natural here?\u201d\n\nThe proximity predictor is used to reward the ending state of a transition trajectory in how close it is to the initiation set of the next primitive as well as actions that increase proximity. As R3 suggested, both linear and exponential functions are valid choices for a proximity function. \n\nWe have experimentally compared the linear and exponential proximity functions. Our model is able to learn well with both functions and they perform similarly. We added the result to our website (Ablation study on Proximity functions: https://sites.google.com/view/transitions-iclr2019#h.p_qGO2W2Dk2q8G ) and will it add to the supplementary.\n\nOriginally, we opted for the exponential proximity function with the intuition that the faster initial decay near the initiation set would help the policy discriminate successful states from failing states near the initiation set. Also, in our experiments, as we use 0.95 as a decaying factor, the proximity is still reasonably large (e.g., 0.35 for 20 time-steps and 0.07 for 50 time-steps).\n", "title": "Response to Reviewer 3"}, "SyxGmZcUTm": {"type": "rebuttal", "replyto": "Ske4BidpnQ", "comment": "We thank the reviewer for the feedback and address the concerns in detail below.\n\n> Reviewer 1 (R1): \u201cIn the metapolicy, what ensures consistency, \u2026 ?\u201d\n\nOur meta-policy executes a primitive policy and waits for a termination signal from the primitive policy before choosing the subsequent one. In other words, a termination signal (success/failure of the primitive policy) comes from the primitive policy, i.e. the walker falls down or the arm picks up a box. This call-and-return style [1-3] of execution ensures the same policy is utilized in consecutive steps until its completion. Hierarchical reinforcement methods have employed this call-and-return style when sub-policies are learned for well-defined sub-tasks that do not require a context switch during their execution.\n\n> R1: \u201c... the weaknesses and the limits of the method?\u201d\n\nWe discuss a few assumptions that we made and good follow-up directions below. We will also add the discussion to the revised version.\n\nOur model-free transition policies rely on random exploration. Specifically, we made an assumption that successful transition trajectories between two consecutive policies should be achievable by random exploration (i.e. an initiation set of a primitive policy should be reachable from the ending states of the previous policies). As soon as a transition policy succeeds once, the proximity predictor will learn what good states are and subsequently the transition policy will succeed more frequently. To alleviate the exploration problem with sparse rewards, our transition policy training can incorporate exploration methods that utilize count-based exploration bonuses [4-6], curiosity-driven intrinsic rewards [7-10], etc.\n\nOur current framework is designed to focus on acquiring transition policies that can connect a given set of primitive policies. We believe that additionally enabling an agent to adaptively augment its primitive set [11-12] based on a new environment or task is a promising future direction.\n\nWe assume our primitive policies return a signal that indicates whether the execution should be terminated or not, similar to [1-3, 13]. Without access to this termination signal, the transition policy would learn from very sparse and delayed reward. \n\n\n[1] Oh et al. \u201cZero-shot task generalization with multi-task deep reinforcement learning\u201d, ICML 2017\n[2] Andreas et al. \u201cModular multitask reinforcement learning with policy sketches\u201d, ICML 2017\n[3] Kulkarni et al. \u201cHierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation\u201d, NIPS 2016\n[4] Strehl Littman \u201cAn analysis of model-based interval estimation for markov decision processes\u201d, Journal of Computer and System Sciences (JCSS) 2008\n[5] Bellemare et al \u201cUnifying Count-Based Exploration and Intrinsic Motivation\u201d, NIPS  2016 \n[6] Martin et al. \u201cCount-Based Exploration in Feature Space for Reinforcement Learning\u201d, IJCAI 2017\n[7] Schmidhuber \u201cA possibility for implementing curiosity and boredom in model-building neural controllers\u201d, From animals to animats: Proceedings of the first international conference on simulation of adaptive behavior, 1991\n[8] Pathak et al. \u201cCuriosity-driven Exploration by Self-supervised Prediction\u201d, ICML 2017\n[9] Achiam and Sastry \u201cSurprise-Based Intrinsic Motivation for Deep Reinforcement Learning\u201d, NIPS Workshop 2016\n[10] Stadie et al. \u201cIncentivizing exploration in reinforcement learning with deep predictive models\u201d, NIPS Workshop 2015\n[11] Hausman et al. \u201cLearning an Embedding Space for Transferable Robot Skills\u201d, ICLR 2018\n[12] Gudimella et al. \u201cDeep reinforcement learning for dexterous manipulation with concept networks\u201d, arXiv 2017\n[13] Le et al. \u201cHierarchical Imitation and Reinforcement Learning\u201d, ICML 2018\n", "title": "Response to Reviewer 1"}, "Bkgfie58TX": {"type": "rebuttal", "replyto": "SJltXWIi3X", "comment": "We thank the reviewer for the feedback. We are glad that the reviewer found the idea novel and useful for enabling the smooth composition of skills and that the reviewer recognized the importance of utilizing previously learned skills to compose complex skills.\n", "title": "Response to Reviewer 2"}, "BkghwsHAnQ": {"type": "review", "replyto": "rygrBhC5tQ", "review": "The paper proposes a scheme for transitioning to favorable starting states for executing given options in continuous domains. Two learning processes are carried out simultaneously: one learns a proximity function to favorable states from previous trajectories and executions of the option,  and the other learns the transition policies based on dense reward provided by the proximity function.\n\t\nBoth parts of the learning algorithms are pretty straightforward, but their combination turns out to be quite elegant. The experiments suggest that the scheme works,  and in particular does not get stuck in local minima. \n\nThe experiments involve fairly realistic robotic applications with complex options,  which renders credibility to the results.    \n\nOverall this is a nice contribution to the options literature. The scheme itself is quite simple and straightforward, but still useful. \n\nOne point that I would like to see elaborated is the choice of exponential (\"discounted\") proximity function. Wouldn't a linear function of \"step\" be \n more natural here? The exponent loses sensitivity as the number of steps away increases, which may lead to sparser rewards.\n  \n", "title": " Useful  learning scheme for transitioning between options in continuous domains.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Ske4BidpnQ": {"type": "review", "replyto": "rygrBhC5tQ", "review": "The paper presents a method for learning policies for transitioning from one task to another with the goal of completing complex tasks. In the heart of the method is state proximity estimator, which measures the distance between states in the originator and destination tasks. This estimator is used in the reward for the transition policy. The method is evaluated on number of MojoCo tasks, including locomotion and manipulation.\n\nStrengths:\n+ Well motivated and relevant topic. One of the big downsides in the current state of the art is lack of understanding how to learn complex tasks. This papers tackles that problem.\n+ The paper is well written and the presentation is clear.\n+ The method is simple, yet original. Overall, an elegant approach that appears to be working well.\n+ Comprehensive evaluations over several tasks and several baselines.\n\nQuestions:\n- In the metapolicy, what ensures consistency, i.e. it selects the same policy in the consecutive steps?\n- Can the authors comment on the weaknesses and the limits of the method?", "title": "An elegant method with comprehensive evaluations", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJltXWIi3X": {"type": "review", "replyto": "rygrBhC5tQ", "review": "** Summary **\nThe authors propose a new training scheme with a learned auxiliary reward function to optimise transition policies, i.e. policies that connect the ending state of a previous macro action/option with good initiation states of the following macro action/option.\n\n** Quality & Clarity **\nThe paper is well written and features an extensive set of experiments.\n\n** Originality **\nI am not aware of similar work and believe the idea is novel.\n\n** Significance **\nSeveral recent papers have proposed to approach the topic of learning hierarchical policies not by training the hierarchy end-to-end, but by first learning useful individual behavioural patterns (e.g. skills) which then later can be used and sequentially chained together by higher-level policies. I believe the here presented work can be quite helpful to do so as the individual skills are not optimised for smooth composition and are therefore likely to fail when naively used sequentially.", "title": "Potentially very useful idea", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}