{"paper": {"title": "Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization", "authors": ["Lisha Li", "Kevin Jamieson", "Giulia DeSalvo", "Afshin Rostamizadeh", "Ameet Talwalkar"], "authorids": ["lishal@cs.ucla.edu", "kjamieson@berkeley.edu", "desalvo@cims.nyu.edu", "rostami@google.com", "ameet@cs.ucla.edu"], "summary": "", "abstract": "Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters.  While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation.  We present Hyperband,  a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound.  Hyperband is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations.  We compare Hyperband with state-of-the-art Bayesian Optimization methods on several hyperparameter optimization problems.  We observe that Hyperband can provide over an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems.  ", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents a simple strategy for hyperparameter optimization that gives strong empirical results. The reviewers all agreed that the paper should be accepted and that it would be interesting and useful to the ICLR community. However, they did have strong reservations about the claims made in the paper and one reviewer stated that their accept decision was conditional on a better treatment of the related literature. \n \n While it is natural for authors to argue for the advantages of their approach over existing methods, some of the claims made are unfounded. For example, the claim that the proposed method is guaranteed to converge while \"methods that rely on these heuristics are not endowed with any\n theoretical consistency guarantees\" is weak. Any optimization method can trivially add this guarantee by adopting a simple strategy of adding a random experiment 1/n of the time (in fact, SMAC does this I believe). This claim is true of random search compared to gradient based optimization on non-convex functions as well, yet no one optimizes their deep nets via random search. Also, the authors claim to compare to state-of-the-art in hyperparameter optimization but all the comparisons are to algorithms published in either 2011 or 2012. Four years of continued research on the subject are ignored (e.g. methods in Bayesian optimization for hyperparameter tuning have evolved considerably since 2012 - see e.g. the work of Miguel Hern\u2021ndez Lobato, Matthew Hoffman, Nando de Freitas, Ziyu Wang, etc.). It's understood that it is difficult to compare to the latest literature (and the authors state that they had trouble running recent algorithms), but one can't claim to compare to state-of-the-art without actually comparing to state-of-the-art.\n \n Please address the reviewers concerns and tone down the claims of the paper."}, "review": {"ry0K38Scl": {"type": "rebuttal", "replyto": "H1fYnGU_g", "comment": "We have addressed the stated concerns in the camera ready version of our paper, which we've just uploaded.  We thank the reviewers once again for their feedback.  \n", "title": "Camera Ready Version Posted"}, "Bk1EwP_vl": {"type": "rebuttal", "replyto": "ryD8zJLPg", "comment": "Our main point in emphasizing that CVST is designed for speeding up cross validation is that we believe the algorithm is not fast enough for the problems that we are interested in. This is demonstrated in Krueger et. al. 2015 which shows that CVST is often slower than standard 1/10th holdout.  While CVST does achieve better test performance, it would be infeasible to have to wait over 3 times as long as standard holdout in some cases to get the answer.  In contrast, Hyperband is designed to provide over order of magnitude speedups over standard holdout. Indeed, based on our experimental results comparing CVST with Hyperband, we get comparable accuracy with significant speedups, even on very small datasets\n\nAlso, if we understand correctly, you are suggesting a variant of CVST for iterative learning algorithms that reuses work across folds and relies on a fixed holdout set. If so, then it\u2019s important to note that a crucial component of the CVST approach involves using an adaptive validation set (i.e., larger validation sets for smaller training sets), and thus using a fixed validation set with CVST would be a significant departure from the published algorithm.", "title": "CVST"}, "r1ozGU1wx": {"type": "rebuttal", "replyto": "B1PlDhCLg", "comment": "Thank you for carefully looking at our experimental results. Below we address each of your questions, and we will update the draft to clarify these topics.  \n\nHowever, we would first like to reiterate, as we discuss in the paper, that CVST is designed to speed up multi-fold cross validation and according to the empirical results in Krueger et al 2015, it is often slower than standard holdout.  Our work focuses on the regime where the cost of training a single model is quite expensive.  Hence, we do not view CVST as being particularly well-suited for our regime given the computational burden of cross-validation.\n \n\u201cWhy not run Hyperband as long as CVST and then compare apples with apples?\u201d\n\nWe tried to be as fair as possible when comparing CVST and Hyperband, and feel that we in fact chose an experimental setup that was in some sense favorable to CVST. Recall that CVST is not an anytime algorithm and requires as input a set of putative configurations to evaluate, thus leaving us with two strategies for comparison to Hyperband: (1) fix the set of putative configurations; or (2) fix the time allocated to each method.  Strategy (1) would lead to the two methods running for different amounts of time, but would allow us to use the same set of putative configurations (derived from a 2d grid search) as used in the original Krueger et al 2015 experiments. Strategy (2) would lead to the two methods considering different sets of putative configurations, and moreover would require us to randomly select putative configurations (rather than sticking with the original 2d grid search). In order to replicate the Krueger et al 2015 experiments as closely as possible, we thus opted for strategy (1), and tuned an SVM over a predefined 2d grid of 610 configurations.  We also used the same datasets as in Krueger et al 2015, namely the IDA classification benchmarks.\n\n\u201cDespite Hyperband being an anytime algorithm, the authors ran it much shorter than CVST and got consistently worse mean results...one might already get results within the error bars of CVST by picking a random configuration at no cost at all\u201d\n\nIt is indeed the case that CVST on average has slightly lower mean error, but the differences are quite small relative to the error bars.  Indeed, for 5 of the 7 datasets the errors for CVST and Hyperband are within 0.2%, and for each of the 7 datasets, Hyperband does as well as or better than CVST in over half of the trials.  Hence we do not think Hyperband is trivially achieving this relative performance.   On the other hand, Hyperband is 2.7x - 6.7x faster on average, and these average speedups exceed their associated error bars for all datasets.  \n\n\u201cthe authors ran Hyperband with a different \\eta than for all other experiments. This begs the question: How much do you need to tune the method to work? What would be the result of using the same \\eta=4 as elsewhere?\u201d\n\nThis a good point and we apologize for the confusion. In our paper, we suggest default values of eta = 3 or eta = 4.  In our LeNet experiment, we used an eta of 3 and for the rest of the experiments in the main paper we used eta = 4.  As a rule of thumb, we aimed to run  Hyperband with ~5 brackets across all experiments, and we chose between 3 and 4 accordingly, i.e., we used eta = 4 if R was big enough to yield 5 brackets, and otherwise we used eta = 3.  The IDA datasets in the CVST experiments are small (and thus R is small), so we chose eta = 3 to give us more brackets.  We only ran Hyperband with eta = 3 and ***we did not do any tuning of eta in order to generate the Hyperband results we report.***  Hence, we do not know what the result for eta = 4 looks like (and at this point it is not realistic for us to carefully generate these results before the rebuttal deadline).\n", "title": "Discussion of comparison to CVST"}, "B11urw6Ix": {"type": "rebuttal", "replyto": "Sy5N9GmUl", "comment": "We have updated the Appendix to include new results for CIFAR-10 that extend Hyperband, Successive Halving, and Bayesian Optimization methods out to 100R.  ", "title": "Update to Response"}, "Sy5N9GmUl": {"type": "rebuttal", "replyto": "ry18Ww5ee", "comment": "We thank the reviewers for providing thoughtful comments and feedback for our paper.  Below please find our responses to these comments, grouped by theme. \n\n***Main Contribution***\n\nTo borrow the words of AnonReviewer1, the contribution of this paper is a \"very simple method [with] great empirical results for several deep learning tasks\u201d that is also endowed with theoretical guarantees. While we briefly allude to the theoretical properties of Hyperband in Section 3, a thorough theoretical treatment is beyond the scope of the paper (see the arXiv version for detailed theoretical analysis).  We have updated the introduction to clarify our contributions in this work.\n\n***Related Work***\n\nWe did not intend to suggest that optimizing configuration evaluation is a new idea and recognize that there is a long line of work in this area that stretches back many years.  To address AnonReviewer1\u2019s concerns, we moved the related work to section 2, expanded on existing configuration evaluation approaches, and added further discussion of existing approaches that combine configuration selection and evaluation.  Also, as per the suggestion of AnonReviewer2, we have added a reference to Bergstra & Bengio 2012 for the random search baseline.  \n\n***Parallelism***\n\nThe reviewers are correct that there are non-trivial design decisions in parallelizing Hyperband, and the updated version of the paper only talks about this topic as an interesting and important avenue of future research.\n\n***Worst Case Bound and Theoretical Properties***\n\n- \u201cIt's only [Hyperband\u2019s] worst-case analysis that makes no assumption [on the convergence behavior]\u201d\n\nHyperband is endowed with both worst-case and sample complexity guarantees, and neither of these results rely on any assumptions on the shape or rate of convergence of the validation error. In both cases, our only assumption is that the validation error eventually converges.\n\n- \u201conly improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion\u201d; \n\nHyperband crucially extends the previous SH work by addressing the n vs B/n problem that we discuss in Section 3. Indeed, our assumption-free complexity results for Hyperband match the oracle SH results up to a log factor (which we discuss further in the next paragraph).  We also note that our current work also introduces a novel variant of SH amenable to the finite horizon, where the maximum budget per configuration is limited.\n\nAs a further note regarding this log factor, Hyperband\u2019s guarantee actually shows that it is no worse than 5x the best SH bracket, in hindsight. If the best bracket is 50x faster than uniform allocation with random search, then overall, Hyperband is at least 10x faster than uniform allocation. But we agree that the fall-back guarantee is somewhat pessimistic. Ideally, instead of looping over the different kinds of brackets, one would treat each bracket as a separate \u201cmeta-arm\u201d of an outerloop bandit algorithm so that we could claim something like \u201cthe sub-optimal brackets are only played a finite number of times.\u201d However, this remains as future work.\n\n***Empirical Studies***\n\n- \u201cFigure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is\u201d; \u201cif random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x...it would be worth seeing 3x, 10x, and so forth.\u201d\n\nThe experiments in Section 4 are resource intensive and we chose our comparison set to be as informative as possible given the high cost of running these experiments (the total cost was over 10k in EC2 credits and the CNN experiments took over 10k GPU hours).  That said, we agree that extending the results would be interesting.  To this end, we are currently running experiments to extend the chart for CIFAR-10 trained using CNN to twice the current x-axis range to bring all competitors closer to convergence.  We will update the paper once the additional trials are complete.\n\n- \u201cThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization\u201d\n\nWe did not compare to Multi-Task Bayesian Optimization for two main reasons: (1) the method mainly addresses the problem of transfer learning/meta-learning; and (2) in its simplest form, it may suffer from the \u201cn vs B/n\u201d problem since trying several subset sizes in conjunction exponentially increases the number of MTBO\u2019s own hyperparameters. Instead, we opted to use the early stopping method presented in Domhan et. al. 2015 to get an idea of how combined configuration selection and configuration evaluation approaches would perform. We have however added a citation for MTBO in our related work.\n\n- \u201cI am looking forward to seeing the details on these [CVST] experiments\u201d\n\nWe have added a comparison CVST to the updated paper (see Section 2 and also Appendix A.1).  \n\n- \u201cbracket b=4 is at least as good (and sometimes substantially better) than Hyperband.\u201d \n\nFor the deep learning and kernel experiments studied in Section 4, bracket s=4 does indeed perform very well and one could conceivably just run SH with that particular point in the n vs B/n tradeoff. However, in the LeNet experiment discussed in Section 3, Figure 2 shows that bracket s=3 performed the best. In practice, if meta-data or previous experience suggests that a certain n vs B/n tradeoff is likely to work well in practice, one can exploit this information and simply run SH with that tradeoff. However, oftentimes, this a priori knowledge is not available, thus motivating the use of Hyperband. Indeed, our empirical results demonstrate the efficacy of Hyperband in this standard setting where the choice of n vs B/n is unknown.", "title": "Response to Reviewer Comments"}, "SkqvBoT7l": {"type": "rebuttal", "replyto": "B1HvaBcQe", "comment": "Adaptive resource allocation approaches exploit situations in which a configuration\u2019s relative accuracy after a small amount of allocated resources is indicative of relative performance after a large number of allocated resources. These approaches can be deployed with different levels of aggressiveness based on how many resources are allocated to each configuration before any configurations start to be discarded.\n\nThe reviewer is correct that these methods thus run the risk of discarding good configurations if they are too aggressive (i.e. if discarding occurs too early).  Moreover, the appropriate level of aggressiveness is problem-specific and is not known a priori.  This \u201cn versus B/n\u201d problem (see the beginning of Section 2 for further discussion) is a challenge that Hyperband tackles by hedging over different values of s which correspond to different levels of aggressiveness. Some values of s may be too aggressive, some not enough, and there is one that is best only in hindsight. Section 2.2 and in particular Figure 2 shows an example of using each value of s alone (i.e. not cycling over them) and illustrates that Hyperband does not incur too much overhead by cycling over them.\n\nMoreover, the reviewer is further correct that nothing comes for free. In extreme settings where anything beyond uniform allocation is overly-aggressive, Hyperband will, in fact, perform modestly worse than uniform allocation (by a log factor). That said, Hyperband offers a significant advantage outside of these adversarial settings, and this is the behavior that we have consistently observed in our empirical studies.", "title": "Response re: intuition around worst-case"}, "B1HvaBcQe": {"type": "review", "replyto": "ry18Ww5ee", "review": "I can appreciate how this racing-style algorithm is faster than random search when early results (e.g. validation curves) give meaningful indication of future performance (e.g. bounds).\n\nI'm not as clear on how hyperband deals with noisy or misleading early indicators, and nothing comes for free so it's helpful to understand what makes hyperband work. Suppose for example that the best configurations are in some region of the configuration space that accounts for 1% of the sampling distribution. For whatever reason, the validation performance of these configurations is terrible for the first few iterations (much worse than the average over the whole space), and only later on do these late-bloomers emerge as the best models. Is it true that in such a scenario random search would be faster than hyperband (at least for some values of S, R, nu, etc.) because random would spend at least a full 1% of time on these configurations, while hyperband would be distracted by promising leads elsewhere in the space?\n\nEdit: training algorithms that are pipelines of trained components might exhibit this kind of behavior, where a lot of up-front computational investment is required before any kind of validation curve starts to go down.This was an interesting paper. The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible.\n\nApproaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!). Still, I'm excited to try it. I'm cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search (http://www.jmlr.org/papers/v13/bergstra12a.html, which this paper should probably cite in connection with their random search baseline.)\n\nI would suggest that the authors remove the (incorrect?) claim that this algorithm is \"embarrassingly parallel\" as it seems that there are number of synchronization barriers at which state must be shared in order to make the go-no-go decisions on whatever training runs are still in progress.  As the authors themselves point out as future work, there are interesting questions around how to adapt this algorithm to make optimal use of a cluster (I'm optimistic that it should carry over, but it's not trivial).\n\nFor future work, the authors might be interested in Hutter et al's work on Bayesian Optimization With Censored Response Data (https://arxiv.org/abs/1310.1947) for some ideas about how to use the dis-continued runs.", "title": "intuition around worst-case", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyF1qboVg": {"type": "review", "replyto": "ry18Ww5ee", "review": "I can appreciate how this racing-style algorithm is faster than random search when early results (e.g. validation curves) give meaningful indication of future performance (e.g. bounds).\n\nI'm not as clear on how hyperband deals with noisy or misleading early indicators, and nothing comes for free so it's helpful to understand what makes hyperband work. Suppose for example that the best configurations are in some region of the configuration space that accounts for 1% of the sampling distribution. For whatever reason, the validation performance of these configurations is terrible for the first few iterations (much worse than the average over the whole space), and only later on do these late-bloomers emerge as the best models. Is it true that in such a scenario random search would be faster than hyperband (at least for some values of S, R, nu, etc.) because random would spend at least a full 1% of time on these configurations, while hyperband would be distracted by promising leads elsewhere in the space?\n\nEdit: training algorithms that are pipelines of trained components might exhibit this kind of behavior, where a lot of up-front computational investment is required before any kind of validation curve starts to go down.This was an interesting paper. The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible.\n\nApproaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!). Still, I'm excited to try it. I'm cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search (http://www.jmlr.org/papers/v13/bergstra12a.html, which this paper should probably cite in connection with their random search baseline.)\n\nI would suggest that the authors remove the (incorrect?) claim that this algorithm is \"embarrassingly parallel\" as it seems that there are number of synchronization barriers at which state must be shared in order to make the go-no-go decisions on whatever training runs are still in progress.  As the authors themselves point out as future work, there are interesting questions around how to adapt this algorithm to make optimal use of a cluster (I'm optimistic that it should carry over, but it's not trivial).\n\nFor future work, the authors might be interested in Hutter et al's work on Bayesian Optimization With Censored Response Data (https://arxiv.org/abs/1310.1947) for some ideas about how to use the dis-continued runs.", "title": "intuition around worst-case", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1M5Tr7Ql": {"type": "rebuttal", "replyto": "HkKi90JQl", "comment": "Hoeffding races of Maron and Moore 1993 are for accelerating the evaluation of already trained models. Training is considered cheap or negligible (e.g., nearest-neighbor classification) and one has a large pool of validation examples in which to evaluate a model (e.g. number of neighbors to use). This is an instance of the stochastic best-arm identification problem. In our earlier work (Jamieson, Talwalkar \u201cNon-stochastic best arm identification and Hyperparameter Optimization\u201d AISTATS 2016) we do compare to this algorithm, but under the name successive elimination from (Even-Dar et al 2003), along with other state-of-the-art solutions to the stochastic version of the problem.\n\nAs for comparing to a Bayesian method that has a configuration evaluation component like Freeze-Thaw, we do compare to the work of Domhan, Springenberg, Hutter \u201cSpeeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves\u201d IJCAI 2015, named SMAC_early in our neural network experiments. The Domhan work also comments on Freeze-Thaw in particular for neural nets. \n\nBoth in our prior work (Jamieson, Talkwalkar AISTATS 2016) and the current work, it was our goal to formulate hyperparameter tuning using as few assumptions as possible and demonstrate that algorithms can perform very well and still enjoy the reliability guarantees that you might expect in a stochastic setting or where more assumptions are made. Hyperband makes no assumptions on the shape or rate of convergence of the validation error, just that it eventually converges. This means that for some very simple optimization task, if one were to replace stochastic gradient descent, presumably with polynomial convergence like 1/sqrt(T), with Newton\u2019s method with exponential convergence, the Hyperband algorithm would require no modification and just work that much faster. In contrast, for example, the Krueger et al and the Freeze-Thaw works semi-parameterize the space of convergence curves and prove rates based on the assumption that the true curve convergence behavior is representable by this set. \n\nIn response to your question, we ran an experiment modeled after the empirical studies in Krueger et al tuning 2 hyperparameters of a kernel SVM to compare CVST (Krueger et al 2015) and Hyperband.  Hyperband is 3-4x faster than CVST on this experiment and the two achieve similar test performance.  Notably, CVST was only 50% faster than standard holdout.   For the experiments in our paper, we excluded CVST due to the aforementioned theoretical differences and because CVST is not an anytime algorithm, but as we perform more experiments, we will update the draft to reflect this comparison.", "title": "Related Work in Configuration Evaluation and Motivation for Hyperband"}, "HyzrjBX7g": {"type": "rebuttal", "replyto": "HJQytw1Ql", "comment": "Yes, if a Bayesian method chooses 1/r fraction of points randomly, then it will be consistent due to the properties of random search, independent of the performance of the Bayesian algorithm. But of course, if random search was performing well, this mixed strategy would find good configurations 1/r as fast as random search alone. The benefit of Hyperband is that it is consistent and has a sample complexity that is often much smaller than that of random search. Like random search, it is also embarrassingly parallel. Without strong assumptions (e.g., knowing the true kernel exactly a priori) we are not aware of any such sample complexity guarantee for Bayesian methods.  \n\n\nIn response to the second question regarding Figure 2, that figure is used to show the performance of each individual bracket repeated indefinitely on its own relative to Hyperband (also plotted) that loops over all the brackets sequentially (i.e., executing s=1,s=1,s=1,s=1,s=1,etc. versus s=0,s=1,s=2,s=3,s=4,s=0,s=1,s=2,etc). One bracket will perform the best if it was just run individually, but of course this is unknowable before any data is collected. The takeaway is that Hyperband, even though it loops over all the brackets naively, manages to not perform much worse than the best individual bracket in hindsight.", "title": "Random Search & Clarification of Figure 2"}, "HkKi90JQl": {"type": "review", "replyto": "ry18Ww5ee", "review": "I am surprised by the baselines you used in your experiments. Work on what you call \"configuration evaluation\" goes back (at least) 23 years to Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" (https://papers.nips.cc/paper/841-hoeffding-races-accelerating-model-selection-search-for-classification-and-function-approximation), so it is a well known problem. Since this is the problem you tackle, I would've expected a comparison to established baselines for that problem beyond your own work on successive halving. The comparison to Bayesian optimization without \"configuration evaluation\" seems like a red herring to me, particularly since there is work on Freeze-Thaw Bayesian optimization that includes this component.\n\nI think a much more meaningful comparison would be against Krueger et al (JMLR 2015), who -- like your work -- also focused on \"configuration evaluation\", also followed a racing approach that starts out with many configurations & drops dominated configurations over time, and also derive theoretical guarantees. Your discussion states that their work relies on a used-defined \"safety zone\", but looking at their paper they are very clear about default choices for all parameters. Did you actually try to experimentally compare against their method (in this or in previous work)?This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?\nI hope to get a response by the authors and see this made clearer in an updated version of the paper.\n\nIn terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) \nThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: \"Multi-Task Bayesian Optimization\" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. \n\nGiven that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says \"configuration evaluation\" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.\n\nAs another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" (https://papers.nips.cc/paper/841-hoeffding-races-accelerating-model-selection-search-for-classification-and-function-approximation). Again, this could be done by a paragraph in the intro. \n\nOverall, I think for this paper having the related work section at the end leads to many concepts appearing to be new in the paper that turn out not to be new in the end, which is a bit of a let-down. I encourage the authors to prominently discuss related work, including the recent trends in Bayesian optimization towards configuration evaluation, in the beginning, and then clearly state the contribution of this paper by positioning it in the context of that related work and saying what exactly is new. (I think the answer is \"very simple method\", \"great empirical results for several deep learning tasks\" and \"much-needed new theoretical results\", which is a very nice contribution.) I'm giving an accepting score trusting that the authors will follow this suggestion.\n\n\nI have some responses to some of the author responses:\n\n1) \"In response to your question, we ran an experiment modeled after the empirical studies in Krueger et al tuning 2 hyperparameters of a kernel SVM to compare CVST (Krueger et al 2015) and Hyperband.  Hyperband is 3-4x faster than CVST on this experiment and the two achieve similar test performance.  Notably, CVST was only 50% faster than standard holdout.   For the experiments in our paper, we excluded CVST due to the aforementioned theoretical differences and because CVST is not an anytime algorithm, but as we perform more experiments, we will update the draft to reflect this comparison.\"\n\nGreat, I am looking forward to seeing the details on these experiments before the decision phase.\n\n2) \"Hyperband makes no assumptions on the shape or rate of convergence of the validation error, just that it eventually converges.\"\n\nIt's only the worst-case analysis that makes no assumption, but of course one would not be happy with that worst-case performance of being 5x worse than random search. (The 5x is what the authors call \"modestly worse, by a log factor\"; it's the logarithm of the dataset size or of the number of epochs, both of which tend to be large numbers). I think this number of 5x should be stated explicitly somewhere for the authors choice of Hyperband parameters. (E.g., at the beginning of the experiments, when Hyperband's parameters are stated.)\n\n3) \"Like random search, it is also embarrassingly parallel.\"\n\nI think this is not quite correct. Let's say I want to tune hyperparameters on ImageNet and each hyperparameter evaluation takes 1 week, but I have 100 GPUs, then random search will give a decent solution (the best of 100 random configurations) after 1 week. However, Hyperband will require 5 weeks before it will give any solution. Again, the modest log factor is a factor of 5. To me, \"embarassingly parallel\" would mean making great predictions after a week if you throw enough resources at it.", "title": "Lack of baselines for \"configuration evaluation\"", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryVZxPfVe": {"type": "review", "replyto": "ry18Ww5ee", "review": "I am surprised by the baselines you used in your experiments. Work on what you call \"configuration evaluation\" goes back (at least) 23 years to Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" (https://papers.nips.cc/paper/841-hoeffding-races-accelerating-model-selection-search-for-classification-and-function-approximation), so it is a well known problem. Since this is the problem you tackle, I would've expected a comparison to established baselines for that problem beyond your own work on successive halving. The comparison to Bayesian optimization without \"configuration evaluation\" seems like a red herring to me, particularly since there is work on Freeze-Thaw Bayesian optimization that includes this component.\n\nI think a much more meaningful comparison would be against Krueger et al (JMLR 2015), who -- like your work -- also focused on \"configuration evaluation\", also followed a racing approach that starts out with many configurations & drops dominated configurations over time, and also derive theoretical guarantees. Your discussion states that their work relies on a used-defined \"safety zone\", but looking at their paper they are very clear about default choices for all parameters. Did you actually try to experimentally compare against their method (in this or in previous work)?This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?\nI hope to get a response by the authors and see this made clearer in an updated version of the paper.\n\nIn terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) \nThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: \"Multi-Task Bayesian Optimization\" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. \n\nGiven that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says \"configuration evaluation\" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.\n\nAs another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" (https://papers.nips.cc/paper/841-hoeffding-races-accelerating-model-selection-search-for-classification-and-function-approximation). Again, this could be done by a paragraph in the intro. \n\nOverall, I think for this paper having the related work section at the end leads to many concepts appearing to be new in the paper that turn out not to be new in the end, which is a bit of a let-down. I encourage the authors to prominently discuss related work, including the recent trends in Bayesian optimization towards configuration evaluation, in the beginning, and then clearly state the contribution of this paper by positioning it in the context of that related work and saying what exactly is new. (I think the answer is \"very simple method\", \"great empirical results for several deep learning tasks\" and \"much-needed new theoretical results\", which is a very nice contribution.) I'm giving an accepting score trusting that the authors will follow this suggestion.\n\n\nI have some responses to some of the author responses:\n\n1) \"In response to your question, we ran an experiment modeled after the empirical studies in Krueger et al tuning 2 hyperparameters of a kernel SVM to compare CVST (Krueger et al 2015) and Hyperband.  Hyperband is 3-4x faster than CVST on this experiment and the two achieve similar test performance.  Notably, CVST was only 50% faster than standard holdout.   For the experiments in our paper, we excluded CVST due to the aforementioned theoretical differences and because CVST is not an anytime algorithm, but as we perform more experiments, we will update the draft to reflect this comparison.\"\n\nGreat, I am looking forward to seeing the details on these experiments before the decision phase.\n\n2) \"Hyperband makes no assumptions on the shape or rate of convergence of the validation error, just that it eventually converges.\"\n\nIt's only the worst-case analysis that makes no assumption, but of course one would not be happy with that worst-case performance of being 5x worse than random search. (The 5x is what the authors call \"modestly worse, by a log factor\"; it's the logarithm of the dataset size or of the number of epochs, both of which tend to be large numbers). I think this number of 5x should be stated explicitly somewhere for the authors choice of Hyperband parameters. (E.g., at the beginning of the experiments, when Hyperband's parameters are stated.)\n\n3) \"Like random search, it is also embarrassingly parallel.\"\n\nI think this is not quite correct. Let's say I want to tune hyperparameters on ImageNet and each hyperparameter evaluation takes 1 week, but I have 100 GPUs, then random search will give a decent solution (the best of 100 random configurations) after 1 week. However, Hyperband will require 5 weeks before it will give any solution. Again, the modest log factor is a factor of 5. To me, \"embarassingly parallel\" would mean making great predictions after a week if you throw enough resources at it.", "title": "Lack of baselines for \"configuration evaluation\"", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HJQytw1Ql": {"type": "review", "replyto": "ry18Ww5ee", "review": "You mention that sequential optimization methods are not guaranteed to be consistent. Couldn't they be made trivially consistent in practice by choosing a random point x% of the time? I believe that SMAC already does this.\n\nWith respect to Figure 2, Hyperband seems to involve sequentially running out each bracket in turn (unless I've missed something). Wouldn't that mean that it would take 5x as long as each bracket in Figure 2? How is the wall time the same as an individual bracket?This paper presents Hyperband, a method for hyperparameter optimization where the model is trained by gradient descent or some other iterative scheme. The paper builds on the successive halving + random search approach of Jamieson and Talwalkar and addresses the tradeoff between training fewer models for a longer amount of time, or many models for a shorter amount of time. Effectively, the idea is to perform multiple rounds of successive halving, starting from the most exploratory setting, and then in each round exponentially decreasing the number of experiments, but granting them exponentially more resources. In contrast to other recent papers on this topic, the approach here does not rely on any specific model of the underlying learning curves and therefore makes fewer assumptions about the nature of the model. The results seem to show that this approach can be highly effective, often providing several factors of speedup over sequential approaches.\n\nOverall I think this paper is a good contribution to the hyperparameter optimization literature. It\u2019s relatively simple to implement, and seems to be quite effective for many problems. It seems like a natural extension of the random search methodology to the case of early stopping. To me, it seems like Hyperband would be most useful on problems where a) random search itself is expected to perform well and b) the computational budget is sufficiently constrained so that squeezing out the absolute best performance is not feasible and near-optimal performance is sufficient. I would personally like to see the plots in Figure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is (if there is one).\n\nI\u2019m not sure I agree with the use of random2x as a baseline. I can see why it\u2019s a useful comparison because it demonstrates the benefit of parallelism over sequential methods, but virtually all of these other methods also have parallel extensions. I think if random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x, etc. I also think it would be worth seeing 3x, 10x, and so forth and how Hyperband fares against these baselines.\n", "title": "Consistency and a question about Figure 2", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyOACaZEx": {"type": "review", "replyto": "ry18Ww5ee", "review": "You mention that sequential optimization methods are not guaranteed to be consistent. Couldn't they be made trivially consistent in practice by choosing a random point x% of the time? I believe that SMAC already does this.\n\nWith respect to Figure 2, Hyperband seems to involve sequentially running out each bracket in turn (unless I've missed something). Wouldn't that mean that it would take 5x as long as each bracket in Figure 2? How is the wall time the same as an individual bracket?This paper presents Hyperband, a method for hyperparameter optimization where the model is trained by gradient descent or some other iterative scheme. The paper builds on the successive halving + random search approach of Jamieson and Talwalkar and addresses the tradeoff between training fewer models for a longer amount of time, or many models for a shorter amount of time. Effectively, the idea is to perform multiple rounds of successive halving, starting from the most exploratory setting, and then in each round exponentially decreasing the number of experiments, but granting them exponentially more resources. In contrast to other recent papers on this topic, the approach here does not rely on any specific model of the underlying learning curves and therefore makes fewer assumptions about the nature of the model. The results seem to show that this approach can be highly effective, often providing several factors of speedup over sequential approaches.\n\nOverall I think this paper is a good contribution to the hyperparameter optimization literature. It\u2019s relatively simple to implement, and seems to be quite effective for many problems. It seems like a natural extension of the random search methodology to the case of early stopping. To me, it seems like Hyperband would be most useful on problems where a) random search itself is expected to perform well and b) the computational budget is sufficiently constrained so that squeezing out the absolute best performance is not feasible and near-optimal performance is sufficient. I would personally like to see the plots in Figure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is (if there is one).\n\nI\u2019m not sure I agree with the use of random2x as a baseline. I can see why it\u2019s a useful comparison because it demonstrates the benefit of parallelism over sequential methods, but virtually all of these other methods also have parallel extensions. I think if random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x, etc. I also think it would be worth seeing 3x, 10x, and so forth and how Hyperband fares against these baselines.\n", "title": "Consistency and a question about Figure 2", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}