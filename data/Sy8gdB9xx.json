{"paper": {"title": "Understanding deep learning requires rethinking generalization", "authors": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"], "authorids": ["chiyuan@mit.edu", "bengio@google.com", "mrtz@google.com", "brecht@berkeley.edu", "vinyals@google.com"], "summary": "Through extensive systematic experiments, we show how the traditional approaches fail to explain why large neural networks generalize well in practice, and why understanding deep learning requires rethinking generalization.", "abstract": "Despite their massive size, successful deep artificial neural networks can\nexhibit a remarkably small difference between training and test performance.\nConventional wisdom attributes small generalization error either to properties\nof the model family, or to the regularization techniques used during training.\n\nThrough extensive systematic experiments, we show how these traditional\napproaches fail to explain why large neural networks generalize well in\npractice. Specifically, our experiments establish that state-of-the-art\nconvolutional networks for image classification trained with stochastic\ngradient methods easily fit a random labeling of the training data. This\nphenomenon is qualitatively unaffected by explicit regularization, and occurs\neven if we replace the true images by completely unstructured random noise. We\ncorroborate these experimental findings with a theoretical construction\nshowing that simple depth two neural networks already have perfect finite\nsample expressivity as soon as the number of parameters exceeds the\nnumber of data points as it usually does in practice.\n\nWe interpret our experimental findings by comparison with traditional models.", "keywords": ["Deep learning"]}, "meta": {"decision": "Accept (Oral)", "comment": "The authors report the experimental findings of a fascinating inquiry on the ability of the deep neural networks to fit randomly labelled data. The investigation is sound, enlightening, and inspiring. The authors propose both a) a theoretical example showing that a simple shallow network with a number of parameters large enough wrt sample size yields perfect finite-sample expressivity; b) a systematic extensive experimental evaluation to support the findings and claims. The experimental evaluation is a model of thoroughness. \n \n This is definitely groundbreaking work, which will inspire many works in the coming years."}, "review": {"MZIQZgjqOIX": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "The authors have decided to challenge the hard fixed notion in the deep learning community about the generalizability of a network, which is the most basic component of a deep neural network. The results they have produced have shaken my pre-existing beliefs and perceptions on the neural networks.\nIt would have been better if they had pointed to a specific example of where the model could classify with random label. I would like to research more about the implications of random label classification and memorization in models.\nEven though a huge model may be able to memorize the data / features, I feel it would not make sense to build a model with wasted capacity to learn random labels.\nThe discovery that the dropout generalization also doesn't help much is what grabbed my attention.\nOverall, it was a great paper challenging a lot of existing beliefs and notions taken for granted in the deep learning realm. I strongly hope it  encourages / challenges all researchers to come up with a better researach to defend the neural architectures.", "title": "The resistance is expected"}, "S1zRtYaBG": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "I expected to like this paper, because I respect the authors, and many people have said good things about it. In addition, I enthusiastically support experimental work to improve our understanding of how deep nets work. I'm sorry to say I was very disappointed.\n\nI agree with Pirmin Lemberger that the results in this paper are completely unsurprising. I'm surprised that the authors were surprised. I'm shocked that at least one reviewer thought this was ground breaking.\n\nSuppose you replace every occurrence of \"neural net\" in this paper with \"decision tree\". Would you still be surprised? Of course trees would be able to memorize the data. Of course the trees would need to be deeper for random labels than for random pixels (because random pixels provide more information for memorizing the data; shuffling the original pixels is just another way of generating random pixels). Of course regularizing the depth of the trees would not make them generalize well. The only difference is that trees would perform very poorly on the original data too, because they lack the right representations to generalize well.\n\nRegularization only prevents overfitting. It does not ensure that there are good hypotheses in the hypothesis space. Your experiments with randomly parameterized convolutional layers show that they DO help ensure that there are good hypotheses in H. \n\nI note in Figure 1 that it takes 5000 steps to fit the original data, and more than 13000 steps to fit random labels. This directly contradicts your statement that training did not slow down substantially. Training time nearly tripled.\n\nIt has long been known that the most effective way to control hypothesis complexity in neural networks is early stopping. Back in the 90s, the standard procedure was to use the biggest hidden layer you could afford, and then do early stopping. (IIRC this appears in the \"Tricks of the trade\" book https://books.google.com/books?isbn=3540494308). \n\nIn your experiment, if we apply early stopping at 5000 steps, we will get a non-trivial Radamacher complexity for this data set. On random labels, the accuracy will be near zero. Did you really not notice this?\n\nI think it is interesting that you did not need to change your hyperparameters to get the network to fit random labels. That suggests that the hyperparameters might depend more on the architecture and not so much on the data. Have people seen this in other cases?", "title": "A very disappointing paper"}, "BkEAgmUyZ": {"type": "rebuttal", "replyto": "SyPCk4gve", "comment": "All questions are related to Section 5.\n\n1.\nRegarding your comment \"linearly separable in the feature space associated with the kernel\":\n\nPlease forgive if this should be obvious from the paper but what is the kernel associated with the experiment that gives 1.2% error rate on the test set?  The paper specifically mentions \"on MNIST with no pre-processing\".  To me this sounds like you are using the raw pixels as features and that the training set is \"linearly separable\" without qualifying with \"in the feature space associated with the kernel.\"  What do I misunderstand here?\n\n2.\nIn section 5. you derive mathematically that for the separable case the linear model obtained with SGD optimization provides the solution with the minimum L2 norm of the weight vector.  Is this the right interpretation of the result?  If yes then why did you not do this experiment using SGD, which would easily fit into the memory of a much lower-spec computer?\n\n3.\nFrom another reply, you are training 10 separate linear models, one for each class.  Are you assigning the class labels during testing as:  class=argmax_i (model_i)?\n\n", "title": "What is the kernel?"}, "ByaqH4Cne": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "The paper looks very interesting and thought provoking to me. However, having an education as a theoretical physics I found it hard at times to follow through all the arguments in the present forum. To clarify my own thoughts about this fascinating topic and to share them with data scientists I work with I wrote a short pedagogical paper arxiv.org/abs/1704.01312 , trying to connect everyday practice of data scientist practice with some basic theoretical knowledge in statistical learning, especially Rademacher complexity. Perhaps this could be helpful to other people without advanced theoretical knowledge in statistical learning. The focus is on explicit mathematical de\ffinitions and on a discussion of relevant concepts.", "title": "An introduction to generalization and regularization in DL for non experts"}, "B1SGObwnl": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "Could the authors report on ImageNet the training accuracy using both AlexNet and Inception with random labels and data augmentation ? Does that work? and do hyper-parameters need to change?\n\nThis has been requested several times in the discussion, but it has never been addressed (only on CIFAR10). \nOnly this experiment may fully substantiate the general claims made by the authors.\n\nThank you.", "title": "pending question"}, "ryy7ywOsg": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "The authors claim to have shown that \"the traditional approaches fail to explain why large neural networks generalize well in practice\". \n\nWhile the paper was generally thought-provoking to read and discuss, the paper did not make it clear to me what these \"traditional approaches\" are and why it is surprising that they fail to explain the performance of neural networks.\n\nThe authors express their surprise at various abilities of neural networks, but consistently fail to provide references or arguments beyond \"conventional wisdom\" that explain why I should be surprised. A few examples:\n\n\"To our surprise, several properties of the training process for multiple standard achitectures is largely unaffected by this transformation of the labels. This poses a conceptual challenge. Whatever justification we had for expecting a small generalization error to begin with must no longer apply to the case of random labels.\"\n\nWhat is the conceptual challenge? It is not obvious to me that it is impossible to reconcile small generalization error with with the ability to fit noise. A reference or reasoning for being surprised would help.\n\n\"Surprisingly, stochastic gradient descent with unchanged hyperparameter settings can optimize the weights to fit to random labels perfectly, even though the random labels completely destroy the relationship between images and labels.\"\n\nWhy is this surprising? We know that the network has more than enough parameters to memorize the random labels, so it is not obvious that this is surprising.\n\n\"As shown in the last three rows of Table 2 in the appendix, although it does not reach the perfect 100% top-1 accuracy, 95.20% accuracy is still very surprising for a million random labels from 1000 categories.\"\n\"Quite surprisingly, fitting the training labels exactly yields excellent performance for convex models.\"\n\"Surprisingly, adding regularization does not improve either model\u2019s performance!\"\n\nAgain, why are these findings surprising? Is there any quantitative reasoning behind any of them?\n\nPerhaps I lack the background in statistical learning theory that may be necessary develop the intuitions that lead the authors to find their results surprising. Consequently, I think the paper would benefit from more rigorous arguments for why the findings are surprising.", "title": "Thought-provoking, but claims of significance not sufficiently backed up"}, "B1zmA3Jcx": {"type": "rebuttal", "replyto": "r14XBwotl", "comment": "I think the main message of the paper is not to show \"that generalization is bad for a problem with random labels\", but rather it shows training accuracy can be as good for a problem with randomized labels. While it is interesting and thought-provoking, I also think it is not super surprising, or groundbreaking. I could be wrong, but it seems the authors did not fully debunk a explaining theory (If they do, please definitely comment and let me know!) that: implicit regularization by SGD plays important role in selection of generalizable models over their counterparts. Namely, with the same cost, models with smaller norm are easy to \"reach\" under SGD training, and that's why current SGD based training would produce generalizable models when they can also memorize.", "title": "A possible explanation for generalization not fully considered in the paper?"}, "S1ANWTkce": {"type": "rebuttal", "replyto": "HJIRwLuwe", "comment": "If the nets cannot be well trained using initialization with norm similarly to ones under good convergence, then the argument (that SGD implicitly regularizes l2 norm) holds, as the norm increases during the training. So the real question is: can it be well learned with large norm initialization? It seems rarely the case in practice?", "title": "Can it be well learned with large norm initialization?"}, "r14XBwotl": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "(I read this paper after knowing that it got accepted to ICLR 2017)\n\nI am quite happy that a paper that actually talks about model complexity can be accepted to ICLR 2017. But I want to express my personal opinion (unrelated to my lab, my advisor etc) that this paper is a bit over-rated. Calling this paper \"groundbreaking\" is a bit ridiculous in my honest opinion, and it is unfair to the theory community. I hold no bad ideas to the authors or the chairs, but only to the technical details in both experiments and theory of the paper.\n\nTL;DR: The paper shows that generalization is bad for a problem with random labels. That is of course true, but uninteresting.\n\nIn section 2.2 2nd paragraph, the claim that the empirical Radamacher complexity is 1 is a bit hand-waving. One detail that is worth noting (but this does not mean the paper is wrong) is that the conclusion assumes that all functions in the hypothesis set H is bounded by 1 in absolute values. This is only true if H is defined as the set of error functions, or if H is the set hypothesis functions but only for a binary classification problem. The experiments with random labels seem to be conducted with multi-class classification. It would be non-trivial to conclude that the complexity is 1 in this case when H is defined as the set of error functions for multi-class classification problem, albeit it seems the result would be true to me (sorry for my hand-waving here).\n\nHowever, Rademacher complexity can also be used to bound the loss (or energy in the energy-based models framework [1]), which is the objective that our algorithm truly optimizes. There are papers that use Rademacher complexity this way. Optimizing a multi-class classification problem will decrease the difference between the upper- and lower-bounds of the loss, therefore decreasing the loss Rademacher complexity, but in very distinct manners between correct label case and random label case. Therefore, the experiments on random labels may not describe anything about loss Rademacher complexity in correct labels.\n\nNote that the previous paragraph linked Rademacher complexity to the dynamics of optimization, not just the hypothesis set. This is because at any stage of optimization (such as at a step of SGD), the set of hypotheses that the algorithm can explore afterwards is limited relative the current hypothesis that the algorithm possesses. There is an evolving upper-bound that the algorithm can never jump out of, because it needs to \"minimize\". This evolving hypothesis set is called \"sublevel hypothesis set\" in [2], which is the reason why the actual Rademacher complexity is evolving with the optimization process, rather than staying static as assumed by this paper or a majority of theoretical papers from the community. Note that all this applies to either correct labels or random labels.\n\nThus far, I think practitioners in deep learning can safely ignore the possible implication from the article that your problem is not bounded in generalization -- that is not yet proven to be true, because the paper misses the piece of ingredient that can combine the dynamics of optimization with complexity measurement, and another ingredient that characterizes the difference between the random-label problem and the correct-label problem to make the result mean anything. These ingradients are discussed in previous research, therefore I think the title about \"rethinking generalization\" is a bit unfair to other people who have contributed to the theory literature.\n\nBelow is a bit \"hand-waving\" on the distinction between random-label problem and correct-label problem. There is one more theorem that I want to mention to motivate more discussions on the difference of optimization between correct labels and random labels. It is the Talagrand's contraction lemma ([3], also a perhaps easier-to-understand version in [2]). It can be used to bound Rademacher complexity R(F(G)) with L * R(G), in which F(G) are the set of composite functions h(g) with h in H and g in G, and L is the maximum possible Liptchitz constant of functions in H.\n\nNote that \"deep\" learning models are essentially function compositions. Talagrand's lemma therefore means that the quality of your deep learning model's generalization bound depends on whether the function compositions have a Liptchitz constant smaller than 1 (that is, \"contracting\"). The conjecture I hope somebody can take a further look at is that, with correct labels, at the final optimization stage you can easily get contracting function compositions, but not with random labels, because the former assumes certain smoothness in some semantic space where classification makes sense.\n\n[1] LeCun, Yann, et al. A tutorial on energy-based learning. Predicting structured data 1 (2006): 0.\n[2] Zhang X. PAC-learning for energy-based models (Master dissertation, Courant Institute of Mathematical Sciences, New York, 2013).\n[3] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer, 1991.", "title": "Not useful"}, "SkTtKOSOl": {"type": "rebuttal", "replyto": "By2V8jpDe", "comment": "Thank you for the references. David. I wasn't questioning 'SGD implicitly regularizes'. My point was, what was observed in the experiments in Section 5 was not due to 'SGD implictly regularizes', but simply because the initial point was set to zero which led to a kind of 'least norm solution/explicit regularization'. ", "title": "What was observed in the experiments in Section 5 was probably not due to 'SGD implictly regularizes'."}, "By2V8jpDe": {"type": "rebuttal", "replyto": "HJIRwLuwe", "comment": "From what I've heard, SGD implicitly regularizes by finding flatter minima, see, e.g.:\nhttp://cims.nyu.edu/~achoroma/NonFlash/Papers/Entropy-SGD.pdf\nhttps://papers.nips.cc/paper/899-simplifying-neural-nets-by-discovering-flat-minima.pdf \n\n", "title": "SGD: implicit regularization via flat-minima"}, "BkkJBPdvx": {"type": "rebuttal", "replyto": "SyxWiYHDe", "comment": "Thanks for patiently explaining and for your contributions to the field. I really appreciate it.", "title": "Thanks"}, "HJIRwLuwe": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "Very inspiring paper. But I got a few questions regarding section 5 and hope that you can help clarify\n\n(1) You showed that the solutions to the linear model solved by SGD lie in the span of the data points, given that the initial point is zero. But this is true not only for SGD, but also for many other first order methods, right? All the analyses related to SGD in this section would hold for many other first order methods as well. So this set of experiments can hardly support your point that SGD acts as implicit regularizer.\n\n(2) You claimed that SGD would often converge to the solution with minimum norm, but I think this happened here mostly because the initial point was set to zero. Any local minimization method would converge to a local minimum that is close to zero(minimum norm solution) if it starts from zero. I do agree SGD may implicitly regularize a model, but I reckon the effect of implicit regularization shown in this set of experiments was probably/mostly because of choosing zero as initial points.\n\n\n", "title": "A few questions regarding section 5"}, "ByocgDLve": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "I cannot see how section 5 relates to the rest of the paper.\n\nYou say: \"it is useful to appeal to the simple case of linear models to see if there are parallel insights that can help\nus better understand neural networks.\" but I do not see any relevant insights in this section.\n\nThe claim seems to be that SGD does some implicit regularization.\nIt has been demonstrated previously that SGD implicitly regularizes by finding \"flatter\" minima.\n\nYou seem to suggest that SGD might also regularize implicitly by finding a minimum l2-norm solution, but then your results suggest that this is not the case, because:\n1. l2-regularization only gives a small improvement\n2. wavelet pre-processing increases performance but increases l2-norm\n\n", "title": "What is the point of section 5?"}, "SyqzC8Lwg": {"type": "rebuttal", "replyto": "Bkw66zgve", "comment": "You should mention this in the text.  You do not say that you use a kernel, or that it is the RBF kernel.\n\nWithout specifying otherwise, X should mean the data in the pixel-space.  This is also confusing because when you use the RBF kernel, d = infinity.\n\nAlso, when you say: \"the l2-norm of the minimum norm solution with no preprocessing is approximately 220\" this is in the RBF-kernel feature space, correct?  How does one compute the (approximate) norm in this space?", "title": "Please clarify this section"}, "SyxWiYHDe": {"type": "rebuttal", "replyto": "r1tHZ8rwl", "comment": "Yes.", "title": "Yes"}, "BJvE9DHPe": {"type": "rebuttal", "replyto": "HJDLxjbUg", "comment": "We mean *test* error, not training error. Even Yann LeCun's page on MNIST results shows that kernel methods achieve test error roughly in this regime. So, this shouldn't strike you as too surprising. These days the entire kernel matrix fits in main memory on a large machine thus making optimization much easier than back when people reported results for MNIST using kernel methods.", "title": "Yes."}, "H1h4dDBPe": {"type": "rebuttal", "replyto": "SJHm677Px", "comment": "1. Universality theorems don't say anything about optimization. For example, our paper gives a very simple finite sample universality theorem showing that for any labeling of the function there *exists* a shallow neural net that can fit that labeling. This theoretical result doesn't mean that we can efficiently find such a labeling using stochastic gradient descent. This is however what the experiments show.\n\n2. The fact that there exist neural networks that can fit any labeling of the data doesn't by itself imply that natural architectures used in practice can do so. But this is again something our experiments imply.", "title": "I'd say 'no' for two reasons"}, "r1tHZ8rwl": {"type": "rebuttal", "replyto": "BkCpSj-wx", "comment": "Let me try summarize your argument.\n\"As conventional practice with deep neural nets on datasets such as image recognition often involves nets with much more parameters than training samples, we performed many experiments fitting a neural net F to random samples X, where the the number #F of parameters of F are greater than the number of samples |X|. Our findings strongly suggest that in this setting, F can fit most random functions on X, and as such, conventional theories like VC dimension which relies on empirical risk minimization and large sample size cannot distinguish between the case of 1) F fits an image classification function on X, and achieves low generalization error, as occurs in practice, and the case of 2) F fits a random function on X, and achieves not much better generalization error than random guessing.\"\n\nWould you say this accurately captures the argument presented in the \"Rademacher complexity and VC-dimension\" section?", "title": "I (think I) get it now"}, "SJHm677Px": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "According to the universality theorem (UT), (shallow) neural networks can approximate any function. In paper, you showed neural networks can fit a randomly labeled dataset. Don't we expect this phenomenon based on UT? ", "title": "Universality Theorem"}, "r15rubMwe": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "Others have pointed out this in comments. But, I'm confused about the kernel solution.\n\nThe authors mentioned that using the kernel method, we can fit any labeling for MNIST. For actual labeling, they got 1.2% test error, and stated that \"this kernel solution has an appealing interpretation in terms of implicit regularization. Simple algebra reveals that it is equivalent to the minimum L2-norm solution of Xw = y.\"\n\nDoes it mean that the minimum L2-norm solution for MNIST yields 1.2% test error? As I understand, Xw = y is not to be solved in the kernel feature space.", "title": "Kernel Solution?"}, "BkCpSj-wx": {"type": "rebuttal", "replyto": "rk4VubZwl", "comment": "We are talking about the regime where, roughly, the vc dimension is larger than the number of training examples. You may think it as finite n and d but d >> n; or both n and d goes to infinite but d at a higher speed.", "title": "Response"}, "rk4VubZwl": {"type": "rebuttal", "replyto": "H15Do_gve", "comment": "We agree on that. So I'm assuming you disagree with my interpretation of your comment? Could you formulate your argument in the paper precisely? (For example, state clearly in your claim whether you are fixing the hypothesis space, whether n goes to infinity, what do you fix as a constant, etc)", "title": "comment"}, "H15Do_gve": {"type": "rebuttal", "replyto": "ryYuM_evx", "comment": "Yes, absolutely. If you *fix the hypothesis space* and *let n goes to infinity*, then learnability is guaranteed provided finite VC-dimension. ", "title": "response"}, "ryYuM_evx": {"type": "rebuttal", "replyto": "BJCY3Qewx", "comment": "Correct me if I'm wrong. It seems to me that you are saying the following:\n\"Because our empirical experiments indicate that NNs with more than n parameters can learn any function on a fixed sample of size n (i.e. shatters the sample), the bound given by VC theory is uninformative because it predicts O(d/E) samples, where E < 1/n, whereas we used only n << d/E samples to learn the target function on the size n sample.\"\n\nIf this is indeed your argument, then it mistakes sample error for global error, which is what VC theory and PAC learning is concerned with. The bound given in my previous post is for learning a function making at most error E under arbitrary *global distribution*, not just on the sampled points. Your experiments show that NNs with more than n parameters indeed have VC dimension greater than n, but in order to learn the correct function with small error globally, you need many more samples, the number of which is given in my last post.\n\nIf this is not your argument, I apologize, and please elaborate what you mean.", "title": "Still confused"}, "B18bt8lvl": {"type": "rebuttal", "replyto": "S1eCK3JPl", "comment": "Thanks for pointing this out. We have updated this missing reference in the paper.", "title": "References"}, "SkAgEBgwl": {"type": "rebuttal", "replyto": "ryKka-rLx", "comment": "We updated the PDF and added an appendix E to include the results on fitting random labels with explicit regularizers (including data augmentation). \n\n\n\n", "title": "Response"}, "SyPCk4gve": {"type": "rebuttal", "replyto": "BJtBTu4Lg", "comment": "Yes, they are linearly separable in the feature space associated with the kernel. MNIST is an easy dataset and commonly used as demonstration or sanity check in the deep learning community.", "title": "Response"}, "BJCY3Qewx": {"type": "rebuttal", "replyto": "r1MQ1b7Ix", "comment": "The bound mentioned here is n = O(d/E). If one solves for E = O(d/n), the conclusion is then clear.", "title": "Response"}, "Bkw66zgve": {"type": "rebuttal", "replyto": "ryfdugePe", "comment": "Sure: the model is linear in the \"lifted\" feature space f(x) where f is a basis of the RKHS defined by the RBF kernel. This is called \"kernelization\" -- there are some good books out there explaining these methods in detail. ", "title": "Kernels is the answer : )"}, "ryfdugePe": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "I am totally confused by the section on linear models.\n\nFor MNIST, d << n, so it seems you *cannot* fit any labelling.\nBut as I understand, you claim to do so, and to achieve 1.2% test error with the result.\n\nCan you please clarify this?", "title": "Linear Models -- what is going on?"}, "S1eCK3JPl": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "This is an important discussion and very significant issue.  It's worth noting our ICLR 2015 paper that already raised this issue and argued, based on different experiments, that generalization is coming from the optimization dynamics rather then a capacity bound on the model class, including discussion of explicit and implicit regularization and an analogy to matrix factorization:\n\nhttps://arxiv.org/pdf/1412.6614v4.pdf\n\nBut even though the issue was raised and discussed before, the experiments in the submission are indeed an excellent illustration of how high the capacity of neural networks are, and drives home the point that generalization is coming from a completely different place.", "title": "Implicit Regularization"}, "ryKka-rLx": {"type": "rebuttal", "replyto": "HJJR_fQUg", "comment": "Please allow me to insist : you didn't answer my concerns on the missing experiments on ImageNet with random labels.\n\n\n- You said : (comment on Dec 16th 2016)\n\"For fitting random labels, we only studied random cropping augmentation on cifar10 as reported in the paper. We think investigating other jitter augmentation is an interesting question that merits future study.\"\n\n- You said : (parent comment)\n\"However, we have been able to get zero training error on augmented versions of CIFAR-10 (including crops, rotations, and flips) using the Inception architecture.\".\n\n- The numbers for cifar10 are nowhere to be found in the paper, yet there is : (abstract of the paper)\n\"convolutional networks [...] easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization\"\n\n- Data augmentation is included in \"explicit regularization\" as the paper says : (Sec 1.1, paragraph \"The role of explicit regularization.\")\n\"We show that explicit forms of regularization, such as weight decay, dropout, and data augmentation, [...].\"\n\n\n**** Question 1) Regarding the experiments that you did on augmented versions of CIFAR-10, can you please give the details on the corresponding setup for reproducing your findings ?\n\nI would like to check, in particular, how the number of parameters compares to the total number of augmented samples (upper bounding the size of the augmented dataset, which you say should be not clear how to measure).\n\nYou said you obtained zero training error in that case.\nCan you please update Table 1 with the missing numbers ?\n\n\n**** Question 2) You say : (Section 2.1)\n\"We also tested random labels on the ImageNet dataset [...]\nThe network also manages to reach 90% top-1 accuracy even with explicit regularizers turned on.\"\n\nI would like your confirmation that I understood section 2.1 correctly: did you manage to get a similar result on ImageNet (low training error with data augmentation and random labels) ?\nHow much did you get ?\n\nCan you please update Table 2 with the missing numbers ?\n\n\n**** Question 3) You say : (Section 1.1)\n\"Our central finding can be summarized as: Deep neural networks easily fit random labels.\"\nThis paper studies a form of overfitting. Standard practice (since Krizhevsky 2012) consists in using data augmentation against overfitting. However I see that you chose to exclude data augmentation from some experiments.\n\nOr did you decide that data augmentation was irrelevant for some experiments ? If yes then why ? Can you discuss this in the paper clearly ?\n\nCan you please point out which of your results are still true when this explicit regularizer is activated in your setups ?\n\n\n", "title": "loose ends"}, "BJtBTu4Lg": {"type": "rebuttal", "replyto": "HJJR_fQUg", "comment": "I have two questions about the statement: \"the minimum L2 norm w which satisfies Xw=y does indeed achieve a test error of 1.2%.\"\n\n1- Do you imply that the training data of MNIST are linearly separable? i.e., there exists a \"w\", for which Xw=y, where X is training data and y the corresponding labels.\n2- If we can get test error of 1.2% by minimum L2 norm solution, why do we bother using more complex classifiers, such as neural networks?", "title": "Minimum L2 norm solution Xw=y"}, "HJJR_fQUg": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "We thank the reviewers for their careful reading of our paper and many helpful questions and suggestions.\n\nWith regards to Reviewer 1\u2019s first question, we believe that this is indeed the most important open question in machine learning.  We intend on pursuing exactly this question in future work.  \n\nBoth Reviewer 2 and a public commenter asked for some clarification for what we mean by random labels.  Specifically, we take the same set of classes as in the original problem.  For each example, we assign to it a random class label.  For example, for CIFAR 10, there are ten classes.  For each example, we choose a random integer between 1 and 10, and assign that example to be in the class indexed by our random integer.  This assignment remains consistent across epochs throughout training.\n\nReviewer 3 requested some clarification of our experiments with kernels.  Indeed, we used a one-versus all classification scheme.  For each class a target vector was constructed with a 1 if the example was in the class and a -1 if the example was not in the class.  Hence, there was a different weight vector for each class.\n\nOne public comment asked about our MNIST experiments.  We clarify that the minimum l2 norm w which satisfies Xw=y does indeed achieve a test error of 1.2%.\n\nWe would also like to respond an additional insightful public comment about data augmentation.   Understanding the benefits of data augmentation is important, but it is not cut and dry as to how to count the number of examples after augmentation.  Since each augmented example is highly correlated  with an existing training example, it is not clear how to measure the size of the augmented data set.  However, we have been able to get zero training error on augmented versions of CIFAR-10 (including crops, rotations, and flips) using the Inception architecture.   Regardless, we note that the conceptual problems raised by our experiments are not resolved whether or not data augmentation is taken into account.\n\nFinally, another excellent public comment raised the issue that non-parametric classifiers can memorize labels.  We fully agree.  That popular neural net architectures behave like non-parametric classifiers is what we are trying to argue in the paper and is somewhat surprising.  Note that it is easy to construct a neural net that cannot memorize---the simplest example being linear regression with a rank-deficient design matrix.  Moreover, the comment suggests that capacity of nonparametric classifiers can be controlled by regularization.  We show in our experiments that the amount of regularization used in practice is not sufficient to control the capacity of neural net models.  ", "title": "Response to comments and reviews"}, "r1MQ1b7Ix": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "Congratulations on an excellent paper!\n\nI have a question that I hope one of the authors can humor me. I'm not sure I understand the claim made in the implications section. For example, the following appears in the paragraph *Rademacher complexity and VC-dimension*.\n\n> Deep neural networks with more than n parameters have VC-dimension greater than n, which leads to trivial uniform convergence bounds.\n\nIf we replace \"deep neural networks\" with, say, \"linear thresholds\", the premise of this statement still holds, but the clause following \"which\" is false, as standard VC arguments show that linear thresholds are PAC learnable. More precisely, the class of linear threshold functions on R^d has d+1 parameters and VC dimension d+1, and by (essentially) the Sauer-Shelah Lemma, the number of samples required to learn linear thresholds to within error E and with (1 - D) confidence is O(E^-1 log D^-1 + d E^-1 log E^-1), which is a nontrivial \"uniform convergence bound.\" (Kearns and Vazirani 1994)\n\nCould the authors clarify what is meant in the above statement and perhaps present their arguments more explicitly for the reader?\n\n[1] Kearns, Michael, and Umesh Vazirani. 1994. An Introduction to Computational Learning Theory.\n", "title": "Question"}, "HJjHdrzIl": {"type": "rebuttal", "replyto": "H1S_oDbNg", "comment": "Hi,\n\nI can't find the section where you studied random cropping augmentation on cifar10 for fitting random labels.\nI don't see the numbers in Table 1. Can you add them ?", "title": "where ?"}, "HJDLxjbUg": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "Hi,\n\nIn paper, you derived a kernel solution XX^T a = y, and then mentioned that \"On MNIST with no preprocessing, we are able to achieve a test error of 1.2% by simply solving (3).\" Is it a typo that you achieved test error of ~1%? Isn't it the training error? \n\nYou then mentioned that \"Note that this kernel solution has an appealing interpretation in terms of implicit regularization. Simple algebra reveals that it is equivalent to the minimum L2-norm solution of Xw = y\". So, one may wonder how \"minimum L2-norm solution\" yields ~99% accuracy on test data?!\n\nThanks.", "title": "Is test accuracy of the Kernel method really ~99%?"}, "BkwBqzZ8e": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "Dear authors,\n\nAny non-parametric classifier (with number of parameters = O(number of data)) can memorize random labels. For instance, one can reproduce the result in this submission with 1-nearest neighbour and random forest classifiers in less than 10 lines of Python. Do 1-nearest neighbour and random forest classifiers \"require rethinking generalization\"? It is unsurprising that the Rademacher complexity of non-parametric classifiers is constant, since their number of parameters grows with data, and their capacity is controlled via regularization.\n\n```\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.neighbors import KNeighborsClassifier as kn\nimport numpy as np\n\nnp.random.seed(0)\n\nn = 1000\nx = np.random.randn(n,224*224)  # random ImageNET-sized images\ny = np.random.randint(2,size=n) # random labels\n\nprint(rf(n_estimators=n).fit(x,y).score(x,y)) # training accuracy = 1.0\nprint(kn(n_neighbors=1).fit(x,y).score(x,y))    # training accuracy = 1.0\n```", "title": "A classic result of non-parametric methods?"}, "Skag0RK4e": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "-Could you please clarify how did you do the \"random labels\" modification to the input labels? are \"all\" labels from a given class mapped to the \"same\" random label? (more specifically, two examples have the same label before modification, will they have same random label after the modification) or a label is replaced with a random one regardless of its original label (i.e. two examples have same labels before modifications, will likely get two different random labels)? \n- Let's take cifar10 as an example. The original data set has 10 classes, after the \"random label\" modification how many classes are there? 10? >10? <10? \n\n", "title": "Some questions about random labels"}, "SysXLR-Nl": {"type": "rebuttal", "replyto": "H1S_oDbNg", "comment": "I think there is something deeper here and a result of major importance is missing, that could make the paper significantly stronger.\n\n---\n\nI am glad that you ran the ImageNet experiment with AlexNet because it raises the following issue :\n- I believe [Krizhevsky et al. 2012] sets the standard for \"practice\" in training neural networks and I hope we all agree on this. This paper uses data augmentation as part of the training scheme, which means that the number of data points evaluated by the neural network is 2048 * 1.2 million = over 2 billion.\n=> AlexNet has 60 million parameters. \n=> Data augmentation is used to exactly combat overfitting, which in some sense is what this paper seems to analyze.\n=> If my understanding is correct, in practice the number of data points far exceeds the number of parameters.\n=> This fact seems to contradict what is in the abstract : \"as soon as the number of parameters exceeds the number of data points *as it usually does in practice*\" (emphasis added) so I am afraid it might be a little misleading for readers, and I hope the authors can either point where I am wrong and/or clarify this issue in the paper.\n\n---\n\nThe abstract says that \"Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization [...]\"\n\nOne of the most spectacular results in this paper is that AlexNet can learn a random labeling of ImageNet. (This result was notably shown in a slide of a presentation from one of the authors of the paper, which made a bit of a splash.)\n\nHowever, I noticed in the appendix that in the Table 2, the experiment \"ImageNet 1000 classes with random labels\" was never run with data augmentation, which is the most popular explicit regularization scheme. This table looks incomplete to me.\n\nI am very eager to see the result of this experiment because I believe it would make the paper much stronger and substantiate the claims much better.\n\n---\n\nIt is my opinion that either the claims should be weakened or this result needs to be included. My hope is that the setup built by the authors should be ready to run this experiment and the paper can be amended accordingly by the acceptance deadline.\n", "title": "follow-up"}, "H1S_oDbNg": {"type": "rebuttal", "replyto": "rkxMxtu7x", "comment": "For fitting random labels, we only studied random cropping augmentation on cifar10 as reported in the paper. We think investigating other jitter augmentation is an interesting question that merits future study.", "title": "response"}, "HJGjCmpQe": {"type": "rebuttal", "replyto": "r1nuL7-Qx", "comment": "1) Yes, we monitored both the objective function (cross entropy) and the accuracy. The training objective in all cases converges to a near-zero quantity.  The cross entropy on the test set tracks the training error and then curves upward.  However, this increase in the test cross entropy is because the model becomes more confident about labels it has labeled incorrectly.  \n\n2) For simplicity, we assumed infinite precision in our construction.  In finite precision, one could repeat the construction O(log(n)) times to avoid collisions.", "title": "Response"}, "rkxMxtu7x": {"type": "rebuttal", "replyto": "Sy8gdB9xx", "comment": "Can you still fit random labels on the training set when using jittering data augmentation ?", "title": "Data augmentation + random labels"}, "r1nuL7-Qx": {"type": "review", "replyto": "Sy8gdB9xx", "review": "1) Your experiments consider classification error, but the loss being optimized is always a different surrogate loss (e.g. conditional log likelihood). Did you look at the effect on the actual training objective being optimized? Can you comment on that?\n\n2) Regarding the result on effective finite sample expressivity. This results relies on the ability to uniquely identify each of your n d-dimensional training data points, after projection, as a single unique real number. Isn\u2019t this feasible primarily because they are \u00ab\u00a0true\u00a0\u00bb real values. Would it not break at some point with finite-limited-precision values?\nThis paper offers a very interesting empirical observation regarding the memorization capacity of current large deep convolutional networks. It shows they are able to perfectly memorize full training-set input-to-label mapping, even with random labels (i.e. when label has been rendered independent of input), using the same architecture and hyper-parameters as used for training with correct labels, except for a longer time to convergence. \nExtensive experiments support the main argument of the paper. \n\nReflexions and observations about finite-sample expressivity and implicit regularization with linear models fit logically within the main theme and are equally thought-provoking.\n\nWhile this work doesn\u2019t propose much explanations for the good generalization abilities of what it clearly established as overparameterized models,\nit does compel the reader to think about the generalization problem from a different angle than how it is traditionally understood.\n\nIn my view, raising good questions and pointing to apparent paradox is the initial spark that can lead to fundamental progress in understanding. So even without providing any clear answers, I think this work is a very valuable contribution to research in the field.\n\nDetailed question: in your solving of Eq. 3 for MNIST and CIFAR10, did you use integer y class targets, or a binary one-versus all approach yielding 10 discriminant functions (hence a different alpha vector for each class)?\n", "title": "What about the optimizaiton objective? And finite-limited-precision?", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJwljOv4x": {"type": "review", "replyto": "Sy8gdB9xx", "review": "1) Your experiments consider classification error, but the loss being optimized is always a different surrogate loss (e.g. conditional log likelihood). Did you look at the effect on the actual training objective being optimized? Can you comment on that?\n\n2) Regarding the result on effective finite sample expressivity. This results relies on the ability to uniquely identify each of your n d-dimensional training data points, after projection, as a single unique real number. Isn\u2019t this feasible primarily because they are \u00ab\u00a0true\u00a0\u00bb real values. Would it not break at some point with finite-limited-precision values?\nThis paper offers a very interesting empirical observation regarding the memorization capacity of current large deep convolutional networks. It shows they are able to perfectly memorize full training-set input-to-label mapping, even with random labels (i.e. when label has been rendered independent of input), using the same architecture and hyper-parameters as used for training with correct labels, except for a longer time to convergence. \nExtensive experiments support the main argument of the paper. \n\nReflexions and observations about finite-sample expressivity and implicit regularization with linear models fit logically within the main theme and are equally thought-provoking.\n\nWhile this work doesn\u2019t propose much explanations for the good generalization abilities of what it clearly established as overparameterized models,\nit does compel the reader to think about the generalization problem from a different angle than how it is traditionally understood.\n\nIn my view, raising good questions and pointing to apparent paradox is the initial spark that can lead to fundamental progress in understanding. So even without providing any clear answers, I think this work is a very valuable contribution to research in the field.\n\nDetailed question: in your solving of Eq. 3 for MNIST and CIFAR10, did you use integer y class targets, or a binary one-versus all approach yielding 10 discriminant functions (hence a different alpha vector for each class)?\n", "title": "What about the optimizaiton objective? And finite-limited-precision?", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}