{"paper": {"title": "Beyond Fine Tuning: A Modular Approach to Learning on Small Data", "authors": ["Aryk Anderson", "Kyle Shaffer", "Artem Yankov", "Court Corley", "Nathan Hodas"], "authorids": ["aryk.anderson@eagles.ewu.edu", "kyle.shaffer@pnnl.gov", "artem.yankov@pnnl.gov", "court@pnnl.gov", "nathan.hodas@pnnl.gov"], "summary": "A better way to do deep learning with small amounts of training data", "abstract": "In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of  domain-specific hand-engineered features. Here we take the  approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.  ", "keywords": ["Deep learning", "Supervised Learning", "Transfer Learning"]}, "meta": {"decision": "Reject", "comment": "The method was developed to provide an alternative for fine-tuning by augmenting a pre-trained network with new capacity. The differential from other related methods is low, and the evaluated baselines were not well-chosen, so this is not a strong submission."}, "review": {"HJ2EcFbQl": {"type": "rebuttal", "replyto": "B1d9e21ml", "comment": "Thank you for your question.\n\nWe did not include explicit comparisons between our module approach and fully retraining the entire network with both the source and transfer dataset.  If both the source and transfer dataset were available, we agree the outcome would be at least as good as the modular approach. That said, the modular approach is intended when the source dataset is either a) no longer be available or b) infeasible to properly retrain from scratch. \n\nThe reviewer is correct that the module is intended to add capacity -- the increased capacity allows the network to learn new features without forgetting old ones.  Regarding the comparison between traditional fine-tuning, the module approach, and fine-tuning+module, we will prepare an experiment to compare all three.  We will update the manuscript as soon as that is ready.  We anticipate that the module + fine-tuning will perform similarly to the module. (after all, the module + fixed weights  is a sub-set of module+fine-tune original network).", "title": "Baselines -- Reply"}, "HkyDuFeQg": {"type": "rebuttal", "replyto": "SyEr_DJ7x", "comment": "This paper shows complementary evidence for progressively growing representations, with promising results in the small data regime. A substantial number of machine learning applications are in this regime, and standard deep learning approaches tend to struggle. Hence, this paper provides a welcome alternative. \n\nFurthermore, deep networks extract general representations which can be reused at many levels of granularity, and this paper shows that the appropriate level needs not be a hyperparameter, as it is the case with fine-tuning approaches; instead, lateral connections to an appropriately chosen representation together with standard validation of other hyperparameters (e.g. model size) can result in substantial improvements in generalization. \n\nWhile differences to progressive nets are indeed in the architecture details, the message of the paper as a whole is complementary, since it is not at all clear how a result in online deep reinforcement learning will apply to supervised learning with standard datasets. Actually, there are many reasons why it would not, e.g. online RL uses large amounts of data which is not i.i.d. and optimizes a drifting objective function based on a Bellman update.\n\nMy take away from this paper is that when I was learning from fMRI data with 12 examples/class I should have progressively extended a frozen and well regularized network trained on large amounts of data from a related task, rather than default to linear models or fine-tuning. \n", "title": "Interesting complementary evidence for progressive transfer learning"}, "B1d9e21ml": {"type": "review", "replyto": "ryZqPN5xe", "review": "Do you have a comparison to just training the network from the start on both the source and transfer dataset?  and/or on only the transfer dataset?\n\nWhat about a baseline where the original weights are allowed to change (be fine-tuned) but the same network with the additional modules/weights is used? In Fig. 4, I'm not sure whether the 'module' approach wins due to your approach of keeping the original network fixed, or simply the use of a larger and higher-capacity network.This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard \u201cfine-tuning\u201d approach.\n\nPros:\n-The method is simple and clearly explained.\n-Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest.\n-Experiments are performed in multiple domains -- vision and NLP.\n\nCons:\n-The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the \u201cstiched\u201d network).  These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important.\n\n-Given these large additional costs, the core of the idea is not sufficiently validated, to me.  In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed:\n(1) Allowing the original network weights to be learned for the target task, as well as the additional module.  Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network.\n(2) Training the full module/stitched network from scratch on the *source* task, then fine-tuning it for the target task.  Outperforming this baseline would verify that having a set of weights which never \u201csees\u201d the source dataset is useful.\n\n-The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks.  I\u2019ve never seen networks pre-trained on CIFAR deployed anywhere, and it\u2019s hard to know whether the method will be practically useful for computer vision applications based on CIFAR results -- often improved performance on CIFAR does not translate to ImageNet.  (In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the \u201cpractical\u201d end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.)\n\nOverall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don\u2019t simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application.\n\n===============\n\nEdit (1/23/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet -- thanks for the correction.  However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the \"stitched network\" idea which is the real novelty of the paper.  Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG (\"ResNet+VGG > ResNet\" would be a stronger result, but still not surprising). Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don't sufficiently validate the stitched network idea, in my opinion.", "title": "Baselines", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJk2VBm4g": {"type": "review", "replyto": "ryZqPN5xe", "review": "Do you have a comparison to just training the network from the start on both the source and transfer dataset?  and/or on only the transfer dataset?\n\nWhat about a baseline where the original weights are allowed to change (be fine-tuned) but the same network with the additional modules/weights is used? In Fig. 4, I'm not sure whether the 'module' approach wins due to your approach of keeping the original network fixed, or simply the use of a larger and higher-capacity network.This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard \u201cfine-tuning\u201d approach.\n\nPros:\n-The method is simple and clearly explained.\n-Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest.\n-Experiments are performed in multiple domains -- vision and NLP.\n\nCons:\n-The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the \u201cstiched\u201d network).  These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important.\n\n-Given these large additional costs, the core of the idea is not sufficiently validated, to me.  In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed:\n(1) Allowing the original network weights to be learned for the target task, as well as the additional module.  Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network.\n(2) Training the full module/stitched network from scratch on the *source* task, then fine-tuning it for the target task.  Outperforming this baseline would verify that having a set of weights which never \u201csees\u201d the source dataset is useful.\n\n-The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks.  I\u2019ve never seen networks pre-trained on CIFAR deployed anywhere, and it\u2019s hard to know whether the method will be practically useful for computer vision applications based on CIFAR results -- often improved performance on CIFAR does not translate to ImageNet.  (In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the \u201cpractical\u201d end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.)\n\nOverall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don\u2019t simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application.\n\n===============\n\nEdit (1/23/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet -- thanks for the correction.  However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the \"stitched network\" idea which is the real novelty of the paper.  Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG (\"ResNet+VGG > ResNet\" would be a stronger result, but still not surprising). Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don't sufficiently validate the stitched network idea, in my opinion.", "title": "Baselines", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyEr_DJ7x": {"type": "rebuttal", "replyto": "Hy3CoWAze", "comment": "We thank the reviewer for reading our paper. We agree that our approach and Progressive Nets (and other works) share a common idea: we want to expand our neural net\u2019s domain without losing its previous training.  Rusu et al demonstrate an exciting approach for ensuring your network grows with sufficient capacity to master an entirely new skill without forgetting what it learned before, an essential task for RL, but overkill for small datasets. In fact, their approach wouldn\u2019t work very well for small datasets.  This is where our work fills an important gap \u2013 we demonstrate how to grow neural networks to leverage *small* amounts of data.  This remains a very important challenge in deep learning, and one not addressed by the approach of Rusu et al.", "title": "Comparison to Progressive Nets -- reply"}, "Hy3CoWAze": {"type": "review", "replyto": "ryZqPN5xe", "review": "I think it'd be nice to understand what the actual differences are with the Progressive Nets work by Rusu et al, as the works do look quite similar.This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).\n\n\n", "title": "Comparison with Progressive nets", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJCUVL0Xe": {"type": "review", "replyto": "ryZqPN5xe", "review": "I think it'd be nice to understand what the actual differences are with the Progressive Nets work by Rusu et al, as the works do look quite similar.This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).\n\n\n", "title": "Comparison with Progressive nets", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}