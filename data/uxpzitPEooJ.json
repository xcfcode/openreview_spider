{"paper": {"title": "Graph Coarsening with Neural Networks", "authors": ["Chen Cai", "Dingkang Wang", "Yusu Wang"], "authorids": ["~Chen_Cai1", "wang.6150@osu.edu", "~Yusu_Wang1"], "summary": "We significantly improve the quality of existing graph coarsening algorithms with graph neural network.", "abstract": "As large scale-graphs become increasingly more prevalent, it poses significant computational challenges to process, extract and analyze large graph data. Graph coarsening is one popular technique to reduce the size of a graph while maintaining essential properties. Despite rich graph coarsening literature, there is only limited exploration of data-driven method in the field. In this work, we leverage the recent progress of deep learning on graphs for graph coarsening. We first propose a framework for measuring the quality of coarsening algorithm and show that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. Motivated by the observation that the current choice of edge weight for the coarse graph may be sub-optimal, we parametrize the weight assignment map with graph neural networks and train it to improve the coarsening quality in an unsupervised way. Through extensive experiments on both synthetic and real networks, we demonstrate that our method significantly improves common graph coarsening methods under various metrics, reduction ratios, graph sizes, and graph types. It generalizes to graphs of larger size (more than $25\\times$ of training graphs), adaptive to different losses (both differentiable and non-differentiable), and scales to much larger graphs than previous work.", "keywords": ["graph coarsening", "graph neural network", "Doubly-weighted Laplace operator"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents a way to use GNNs to learn edge weights of a coarsened graph given the node mapping from the original graph to the coarsened graph.  The paper is well-written and the approach is well-motivated as learning makes it easy to adapt the edge weights to different tasks and objectives, as illustrated in the graph Laplacian and Rayleigh quotient examples.  All the reviewers gage positive reviews for this paper, hence I recommend accepting this paper.\n\nThe reason for not promoting this paper further to spotlight or oral is that the paper addressed a relatively small problem, learning the edge weights given the node mapping, and the proposed method is quite simple.  Therefore this paper\u2019s impact could be limited.\n\nOne suggestion to the authors is to present more results on downstream tasks, i.e. how does the proposed coarsening algorithm improve downstream task performance, instead of just losses defined without a downstream task in mind.  Example things to consider: does this approach improve graph classification accuracy?  Does this improve downstream GNN model\u2019s efficiency without sacrificing accuracy?"}, "review": {"Fpf0PuXmWr": {"type": "rebuttal", "replyto": "J5iuoElp4e", "comment": "Thank you for your question. Indeed, we controlled the number of parameters: In fact, the number of parameters of MLP is roughly 30 percent larger than that of GOREN. As far as training error is concerned, the MLP and GOREN have similar training errors. In the table below, we list the training error and the improvement ratio (in parenthesis) in each entry; so for example, \"0.03 (62.9%)\" means that the training error is 0.03, while the improvement over testing datasets is 62.9% (this number is the larger the better). All these indicate that including graph convolutional layers (that leverage the topology of subgraphs) is crucial to achieving better generalization in our GOREN framework.\n\nBL     \tAffinity                  Algebraic                                 Heavy-edge        Local-var(edges)    Local-var (nbrs)\n\nWS + MLP      0.03(62.9%)  0.04 (64.1%)      0.03 (15.9%)       \t0.03 (31.2%)   \t   0.03 (31.6%)    \t0.06 (58.5%)\n\nWS + GOREN 0.03(62.9%)  0.04 (82.1%)     0.04 (60.6%)        \t0.03 (51.8%)   \t  0.04 (69.9%)    \t0.08 (84.2%)\n\nShape + MLP   0.02(78.4%)  0.02 (-11.6%)\t 0.02 (67.6%)      \t0.02 (83.2%)   \t  0.01 (44.2%)    \t0.01 (-1.9%)\n\nShape + GOREN  0.02(91.4%)0.01 (89.8%)  \t0.02 (82.2%)           0.02 (88.2%)        0.01 (80.2%)         0.01 (79.4%)", "title": "training error"}, "MTinSxrMLpG": {"type": "review", "replyto": "uxpzitPEooJ", "review": "Comment before review: This submission seems to use a different margin. The margins of tables and figures are also tiny which makes the submission hard to read. For example, see Table 1. UPDATE: seems like the authors have corrected the margin issue.\n\nReview:\n\nThis submission proposes a machine learning framework to learn the edge weights of coarsened graphs. The authors propose to use graph neural network (specifically, Graph Isomorphism Network) to embed the nodes being coarsened and use the sub-graph embeddings to compute coarsened graph edges. The proposed method leads to a reduction of the eigen error of the coarsened graphs. As far as I know, the proposed method is novel. \n\nComment on writing: \nI think the current presentation complicates the introduction to the proposed method. The description of the proposed method does not appear until page 5 (aside from the introduction). I recommend the authors to condense the discussion in section 3.1 through section 3.3 and focus more on their own contribution\n\nStrength of the submission:\n- the proposed method is novel\n- the proposed method demonstrates empirical improvement \n\nWeakness of the submission:\n- My main concern with this submission is that it lacks an understanding of the proposed method. This paper points out that learning-based method can further reduce the eigenerror by assigning better weights to the coarsened graph, which I am convinced. However, I am less convinced about the proposed learning process. Is the use of a graph neural network really necessary? Considering the input to the GNN here is just simple degree statistics, I doubt if the network can learn much. The author should consider using linear model or simpler models like MLP to learn the edge weights. Without a comprehensive comparison to these baseline, I would not be convinced.  \n\nTypos:\n- In abstract, \"adaptive to different loss\" -> \"is adaptive / adapts to different loss\"\n\n\nUpdated review:\n\nI thank the author for their new experiments during the discussion period. Given the superior performance of GNN over MLP, I am more convinced that the usage of GNNs in this application is justified. I have updated my review rating from 4 -> 6 to reflect this.\n\nBut just to harass the authors a bit more, I have this curious question:\n- Is the worse performance of MLP due to generalization or expressive power? In other words, can the MLP fit the training data well? Also, when comparing MLP and GOREN, are the authors controlling the number of parameters when comparing MLP and GOREN? As the authors mentioned in their reply, which I agree, \"MLP generally works better than LR due to model capacity\". We want to make sure that MLP and GOREN have similar model capacity but GOREN captures better inductive biases.\n\nI believe this submission finds an interesting application for GNNs. I encourage the authors to bring out the full potential of this idea by having solid, rigorous empirical studies. \n", "title": "A ML framework for learning coarsen graph edge weights", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "kUJNeAOKu-V": {"type": "rebuttal", "replyto": "9TQjPsDyFcD", "comment": "Thank you for your review, and thank you very much for the clarification in the response above. \n\nFirst, we would like to apologize for the margin issue in the original submission. It was unintentional -- we copied some macros from our other papers, which included commands that adjusted the margins. It was fixed once it was brought to our attention.  \n\nRegarding your question on a more informed baseline for the edge-assignment map, thank you for the suggestion. To summarize: we have performed new experiments with two new baselines (one linear regression based, one MLP based), and have also added results/discussion in a new subsection (Section 4.5) in the revised submission. (The results using linear regression are not reported in the revision, but are included below.) \n\nMore specifically, recall that we use GNN to represent an edge-weight assignment map for an edge $(\\hat{u}$, $\\hat{v}$) between two super-nodes $\\hat{u}$ and $\\hat{v}$ in the coarse graph $\\widehat{G}$. The input will be the subgraph $G_{\\hat{u},\\hat{v}}$ in the original graph $G$ spanning the clusters of nodes $\\pi^{-1}(\\hat{u})$, $\\pi^{-1}(\\hat{v})$ (i.e, the set of original nodes mapped to $\\hat{u}$ and to $\\hat{v}$, respectively), and the crossing edges among them. The goal is to compute the weight of edge ($\\hat{u}$, $\\hat{v}$) based on this subgraph $G_{\\hat{u}, \\hat{v}}$. \n\nGiven that the input is a local graph $G_{\\hat{u}, \\hat{v}}$, a GNN is perhaps a most natural choice to parameterize this edge-weight assignment map. Nevertheless, in principle, any architecture applicable to graph regression can be used for this purpose. \nTo better understand if it is necessary to use the power of GNN, we replace GNN with the following two baselines for graph regression. The first baseline is the composition of mean pooling of node features in the original graph and linear regression (LR). The second baseline is a composition of mean pooling of node features in the original graph and a 4-layer MLP with embedding dimension 50 and ReLU nonlinearity. Note that while MLP is more sophisticated than LR, both baselines ignore the detailed graph structure which our GNN will leverage. The results for reduction ratio of 0.5 are shown below. See table 6 in the revised paper for full results (results on LR are not included in the revised manuscript as they are generally worse than those of MLP).\n\n BL \tAffinity                  Algebraic                                 Heavy-edge        Local-var(edges)    Local-var (nbrs)\n\nWS + LR         0.45 (42.9%)   \t0.09 (51.9)         0.09 (1.3%)             0.52 (10.3%)        0.09 (28.7%)         0.11 (52.2%)\n\nWS + MLP      0.45 (62.9%)\t0.09 (64.1%)      0.09 (15.9%)           0.52 (31.2%)       0.09 (31.6%)           0.11 (58.5%)\n\nWS + GOREN 0.45 (62.9%)\t0.09 (82.1%)      0.09 (60.6%)           0.52 (51.8%)        0.09 (69.9%)          0.11 (84.2%)\n\n\nShape + LR        0.23 (80.4%)  0.08 (-11.6%)   0.06 (-71.3%)               0.17 (78.2%)      0.04 (-263.3%)         0.08 (-1.9%)\n\nShape + MLP      0.23 (78.4%) 0.08 (-11.6%)     0.06 (67.6%)           0.17 (83.2%)       0.04 (44.2%)           0.08 (-1.9%)\n\nShape + GOREN  0.23(91.4%) 0.08 (89.8%)      0.06 (82.2%)           0.17 (88.2%)        0.04 (80.2%)          0.08 (79.4%)\n\nAs we can see, LR/MLP works reasonably well in most cases, indicating that learning the edge weights is crucial for improvement. But MLP generally works better than LR due to model capacity. On the other hand, we see using GNN to parametrize the map generally yields a larger improvement over the MLP, as MLP does not utilize the topology of subgraph while GNN does. A systematic understanding of how different models such as various graph kernels [1, 2] and graph neural networks affect the performance is an interesting question that we will leave for future work. \n\n[1] Vishwanathan, S. Vichy N., et al. \"Graph kernels.\" The Journal of Machine Learning Research 11 (2010): 1201-1242.\n\n[2] Kriege, Nils M., Fredrik D. Johansson, and Christopher Morris. \"A survey on graph kernels.\" Applied Network Science 5.1 (2020): 1-42.\n\n", "title": "Learning based baselines"}, "xeABvKWGkWW": {"type": "rebuttal", "replyto": "V70WqmsgR1p", "comment": "Thank you very much for your comments and feedback!\n\nThank you for the reference of misc-GAN. Indeed, there is certain high-level similarity in the sense that both use coarse graphs, and both are unsupervised methods. Misc-GAN is a GAN based deep generative model which constructs coarse graphs via the algebraic multigrid method for the purpose of preserving hierarchical network structures. However, the coarsened graphs themselves are not learned. While in our approach, we propose to learn a coarsened graph in the sense of learning the edge-weight assignment map, which is parameterized by a GNN. To our best knowledge, learning such a coarsened graph in an unsupervised manner has not been considered before. Nevertheless, we think this is a good point -- We have now added a paragraph in the related work section on the deep generative models for graphs, including the misc-GAN reference [1]. \n\n\n[1] Zhou, Dawei, et al. \"Misc-GAN: A Multi-scale Generative Model for Graphs.\" Frontiers in Big Data 2 (2019): 3.\n", "title": "Responses to Reviewer4"}, "S7TXqAmjCD-": {"type": "rebuttal", "replyto": "YQapnG9LlYv", "comment": "Thank you very much for your positive feedback!\n", "title": "Thank you!"}, "wC3Mt61eHaR": {"type": "rebuttal", "replyto": "EUxGJTtMfiT", "comment": "\nFirst, thank you very much for your comments and feedback!\n\n#### Reduction ratio: \nThis is a very good question. Right now we are following [2] on the choice of reduction ratio. Selecting the appropriate reduction ratio with controlled error would require a more rigorous theoretical analysis. Such theoretical analysis could be interesting future problems to study. Some challenges include:\n\nFirst, to talk about the performance of our model on test graphs, we will likely need to assume some generative models for graphs, say graphs sampled from graphons. The generalization error will depend both on the distributions of graphs, and the coarsening algorithm selected, which appears challenging. \n\nSecond, even for the training graphs, what is the minimal loss we can achieve through the GNN is not very clear yet. If we don\u2019t use a GNN learned approach, but use only an optimization-based approach (which cannot generalize to test graphs, that is, we have to run this expensive optimization procedure for each test graph), we have some theoretical analysis in appendix F. \n\n#### Scale and Runtime: \nIn terms of the running time, note that once the neural network is trained, it needs very little additional time on test graphs (other than the node-coarsening algorithm we use).\n\nSo the time is mostly on training our framework on training graphs. Here, most of the computation is spent on precomputing eigenvectors. But we only need to compute those eigenvectors once. Furthermore, as we showed in the paper, we could train our model on reasonably small graphs but apply to test graphs of much larger sizes. Note that we have already tested our GOREN framework to real graphs e.g, PubMed, Flickr etc.\n\nMore concretely, the time complexity for training our GOREN framework in one batch is O(|E|k) where |E| is the number of edges and k is the number of eigenvectors. For synthetic graphs, it takes a few minutes to train the model. For real graphs like CS, Physics, PubMed, it takes around 1 hour. For the largest network Flickr of 89k nodes and 899k edges, it takes about 5 hours for most coarsening algorithms and reduction ratios. We also updated the time complexity part in appendix E. \n\n#### Downstream tasks: \n(1) In general, smaller representations of large graphs are easier for researchers to explore and to analyze their structures, e.g, to explore a huge graph by visualizing its coarsened graph. (2) The resulting graph can be a proxy for solving optimization problems on the original graph, e.g, multi-commodity flow. (3) Another downstream problem we had in mind is to apply our method to improve algebraic multigrid, partially motivated by [1]. \n\n[1] Learning Algebraic Multigrid Using Graph Neural Networks https://arxiv.org/abs/2003.05744\n\n[2] Loukas, Andreas. \"Graph Reduction with Spectral and Cut Guarantees.\" Journal of Machine Learning Research 20.116 (2019): 1-42.\n", "title": "Responses to Reviewer2"}, "aBLmGxTrzH": {"type": "rebuttal", "replyto": "MTinSxrMLpG", "comment": "Thank you for your comments and feedback! We will be responding to your and all other reviews  (and our thanks to all the reviewers) soon. But first we hope to get clarification on one of your comments. \n\nWe are wondering whether you could clarify what you mean by a linear (or other) model as a baseline to learn the edge weight in your main concern? Note that we are learning the edge weight for edge (u, v) based on the local neighborhoods of both super-node u and super-node v. Such a neighborhood also contains the cluster of nodes from the original graph collapsed (mapped) to each supernode u and v, as well as the crossing edges among the two corresponding clusters. This neighborhood is naturally modeled as a small local graph, and that is why to have a learnable component that can take in such a neighborhood and predict a value (edge weight), GNN is the most natural choice. \n\nIn particular, note that this neighborhood is unstructured and of varying size for different edges -- it is not clear how to have a simple linear model or MLP directly on such input. One alternative way we see is to model this neighborhood as a set and use DeepSet-like architecture (as the set is permutation invariant). However, note that it will then ignore the local connections, using only the collection of node features or edge features, and also it is not clear that DeepSet is much simpler than GNN either. \n\nAnother way is to use the node feature in the coarse graph and build an MLP based on it. In particular, for two supernodes u, v in the coarse graph, we build an MLP to map from f_u + f_v (or sth. similar) to the edge weight, where f_* is the node feature for node *. This baseline doesn\u2019t utilize the neighborhoods at all, not the set of original graph nodes mapped to these super-nodes. Thus we think that it may not be informative to predict good weight for edges among super nodes. Is this what you have in mind? We would be happy to implement and compare with such a baseline if you could help to clarify.\n", "title": "Clarification"}, "V70WqmsgR1p": {"type": "review", "replyto": "uxpzitPEooJ", "review": "The paper studied the problem of graph coarsening in the context of data-driven deep generative models. The authors studied a family of Laplacian operators and differentiable losses in order to construct high-quality coarse graphs. The paper is well-motivated by providing extensive theoretical analysis to support the rationale of the proposed model. The proposed model is developed based on GAN, which automatically learns a mapping function from the fine-grained graph to the coarse-grained graph. Experimental results show the effectiveness of the proposed model across a bunch of datasets (both synthetic and real ones) and a set of evaluation metrics. \n\nIn general, I believe this paper is well-written, and the results are strong. \nMy only concern comes from the technical contribution of the proposed algorithm. In particular, the authors claimed that \" we are the first to propose and develop a framework to learn coarse graphs with GNN in an unsupervised manner\". However, similar ideas (e.g., Misc-GAN) have already been approached in the network generation setting.  Although, in the graph generation setting, the previous work constructs coarse graphs for the purpose of preserving hierarchical network structures, while in the graph coursing setting, the goal is to alleviate the computational challenges in dealing with massive graphs. But, regarding the framework design, they share some commonalities at a high-level (i.e., GAN-based models for constructing coarse graphs). Please correct me if I am wrong here. \n\nWithout that, I have no question regarding this paper. Moreover, if the author can clear my only concerns above, I would like to higher my score to 7. ", "title": "Solid paper with theoretical and experimental results. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "YQapnG9LlYv": {"type": "review", "replyto": "uxpzitPEooJ", "review": "Summary of the paper: To solve the problem of graph coarsening, this paper proposes a data-driven framework to: 1) measure the quality of the coarsening algorithm and 2) provide a graph neural network (GNN)-based method to handle the suboptimal problem of edge weight occurring in current methods. The proposed model can handle larger graphs than previous methods. The experimental results demonstrate the effectiveness of the proposed method. \n\nRecommendation: I think contributions of this paper on graph coarsening are new and technically solid. The authors propose a new way (GNN model) to do graph coarsening. Some strong points: (1) The authors provide three new projection operators on graph coarsening. (2) A new framework is proposed to learn better edge weights of the coarse graphs by using a GNN model in an unsupervised manner. (3) Empirical results support their findings (most results are significantly better than baseline methods). Based on these observations, I tend to accept this paper.\n\n\n\n", "title": "Graph Coarsening with Neural Networks ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "EUxGJTtMfiT": {"type": "review", "replyto": "uxpzitPEooJ", "review": "This paper studies a graph coarsening strategy where a new way of assigning weights to a coarse graph is proposed. By focusing on preserving properties of the Laplace operator, appropriate projection/lift operators are presented. Based on the observation that better-informed weights enable us to obtain better Laplace operators for coarse graph, a GNN-based weight adjustment method is proposed. The proposed method called GOREN learns the weight-assignment map $\\mu$ from a collection of input graphs in an unsupervised manner, and can be generalized to test graphs of larger size than the training graphs. Experimental results show that GOREN improves common graph coarsening methods under different evaluation metrics.\n\nOverall, the paper is well-written, and the contributions are concrete. The Laplace operator is considered to be one of the most important operators because it can explain much about the graph structure. When a graph is converted to a coarse graph, preserving the properties of the Laplace operator can be a critical issue. This paper nicely formulates this issue and the theoretical analysis seems to be technically sound (I did not thoroughly check all the details, though). \n\nIn terms of the graph reduction ratio, the values of (0.3, 0.5, 0.7) are chosen in the experiments and the authors simply enumerate the results according to the reduction ratios. I'm wondering if there is a way to find an appropriate reduction ratio by theoretical analysis (e.g., returning an appropriate reduction ratio given a desirable error bound).\n\nI'm wondering how the proposed method scales to many real graphs. It would be helpful to know running times of the proposed method on different sizes of graphs.\n\nIt would be great if the authors can explain how the proposed method can be utilized in downstream tasks. Any specific examples/applications will be helpful to understand the practical value of the proposed method.", "title": "Nice formulations with reasonable approaches", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}