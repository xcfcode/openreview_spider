{"paper": {"title": "Word translation without parallel data", "authors": ["Guillaume Lample", "Alexis Conneau", "Marc'Aurelio Ranzato", "Ludovic Denoyer", "Herv\u00e9 J\u00e9gou"], "authorids": ["glample@fb.com", "aconneau@fb.com", "ranzato@fb.com", "ludovic.denoyer@upmc.fr", "rvj@fb.com"], "summary": "Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.", "abstract": "State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.", "keywords": ["unsupervised learning", "machine translation", "multilingual embeddings", "parallel dictionary induction", "adversarial training"]}, "meta": {"decision": "Accept (Poster)", "comment": "There is significant discussion on this paper and high variance between reviewers:  one reviewer gave the paper a low score.  However the committee feels that this paper should be accepted at the conference since it provides a better framework for reproducibility, performs more large scale experiments than prior work.  One small issue the lack of comparison in terms of empirical results between this work and Zhang et al's work, but the responses provided to both the reviewers and anonymous commenters seem to be satisfactory."}, "review": {"SyE3AHgxG": {"type": "review", "replyto": "H196sainb", "review": "This paper presents a new method for obtaining a bilingual dictionary, without requiring any parallel data between the source and target languages. The method consists of an adversarial approach for aligning two monolingual word embedding spaces, followed by a refinement step using frequent aligned words (according to the adversarial mapping). The approach is evaluated on single word translation, cross-lingual word similarity, and sentence translation retrieval tasks.\n\nThe paper presents an interesting approach which achieves good performance. The work is presented clearly, the approach is well-motivated and related to previous studies, and a thorough evaluation is performed.\n\nMy one concern is that the supervised approach that the paper compares to is limited: it is trained on a small fixed number of anchor points, while the unsupervised method uses significantly more words. I think the paper's comparisons are valid, but the abstract and introduction make very strong claims about outperforming \"state-of-the-art supervised approaches\". I think either a stronger supervised baseline should be included (trained on comparable data as the unsupervised approach), or the language/claims in the paper should be softened. The same holds for statements like \"... our method is a first step ...\", which is very hard to justify. I also do not think it is necessary to over-sell, given the solid work in the paper.\n\nFurther comments, questions and suggestions:\n- It might be useful to add more details of your actual approach in the Abstract, not just what it achieves.\n- Given you use trained word embeddings, it is not a given that the monolingual word embedding spaces would be alignable in a linear way. The actual word embedding method, therefore, has a big influence on performance (as you show). Could you comment on how crucial it would be to train monolingual embedding spaces on similar domains/data with similar co-occurrence statistics, in order for your method to be appropriate?\n- Would it be possible to add weights to the terms in eq. (6), or is this done implicitly?\n- How were the 5k source words for Procrustes supervised baseline selected?\n- Have you considered non-linear mappings, or jointly training the monolingual word embeddings while attempting the linear mapping between embedding spaces?\n- Do you think your approach would benefit from having a few parallel training points?\n\nSome minor grammatical mistakes/typos (nitpicking):\n- \"gives a good performance\" -> \"gives good performance\"\n- \"Recent works\", \"several works\", \"most works\", etc. -> \"recent studies\", \"several studies\", etc.\n- \"i.e, the improvements\" -> \"i.e., the improvements\"\n\nThe paper is well-written, relevant and interesting. I therefore recommend that the paper be accepted.\n\n", "title": "Good and interesting work (a few issues on the paper's claims)", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJEg3TtxM": {"type": "review", "replyto": "H196sainb", "review": "The paper proposes a method to learn bilingual dictionaries without parallel data using an adversarial technique. The task is interesting and relevant, especially for in low-resource language pair settings.\n\nThe paper, however, misses comparison against important work from the literature that is very relevant to their task \u2014 decipherment (Ravi, 2013; Nuhn et al., 2012; Ravi & Knight, 2011) and other approaches like CCA. \n\nThe former set of works, while focused on machine translation also learns a translation table in the process. Besides, the authors also claim that their approach is particularly suited for low-resource MT and list this as one of their contributions. Previous works have used non-parallel and comparable corpora to learn MT models and for bilingual lexicon induction. The authors seem aware of corpora used in previous works (Tiedemann, 2012) yet provide no comparison against any of these methods. While some of the bilingual lexicon extraction works are cited (Haghighi et al., 2008; Artetxe et al., 2017), they do not demonstrate how their approach performs against these baseline methods. Such a comparison, even on language pairs which share some similarities (e.g., orthography), is warranted to determine the effectiveness of the proposed approach.\n\nThe proposed methodology is not novel, it rehashes existing adversarial techniques instead of other probabilistic models used in earlier works. \n\nFor the translation task, it would be useful to see performance of a supervised MT baseline (many tools available in open-source) that was trained on similar amount of parallel training data (60k pairs) and see the gap in performance with the proposed approach.\n\nThe paper mentions that the approach is \u201cunsupervised\u201d. However, it relies on bootstrapping from word embeddings learned on Wikipedia corpus, which is a comparable corpus even though individual sentences are not aligned across languages. How does the quality degrade if word embeddings had to be learned from scratch or initialized from a different source?", "title": "Review", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1Qhqm9ez": {"type": "review", "replyto": "H196sainb", "review": "An unsupervised approach is proposed to build bilingual dictionaries without parallel corpora, by aligning the monolingual word embeddings spaces, i.a. via adversarial learning.\n\nThe paper is very well-written and makes for a rather pleasant read, save for some need for down-toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general: it's a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales-pitching itself at times.\n\nThere are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction, e.g. the work by Vulic & Moens (2016).\n\nThe evaluation is for the most part intrinsic, and it would be nice to see the approach applied downstream beyond the simplistic task of English-Esperanto translation: plenty of outlets out there for applying multilingual word embeddings. Would be nice to see at least some instead of the plethora of intrinsic evaluations of limited general interest.\n\nIn my view, to conclude, this is still a very nice paper, so I vote clear accept, in hope to see these minor flaws filtered out in the revision.", "title": "Well-rounded contribution, nice read, incomplete related work", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJfsiaQ7z": {"type": "rebuttal", "replyto": "HkFHDhiGz", "comment": "We thank you for your comment and we are glad to clarify.\n\nThe methodological difference between what we have proposed and Zhang et al.\u2019s method is not just a better stopping criterion, but more importantly, a better underlying method. Here is the detailed comparison between the two approaches:\n- The very first step which is adversarial training with orthogonality constraint, is similar, see figure 1B in sec. 2.1 of our paper and figure 2 in [Zhang et al 2017a] (except for the use of earth mover distance) and figure 2b in [Zhang et al 2017b],  but:\n- the refinement step described in sec. 2.2 and Figure 1C is not present is Zhang et al 2017a/b, nor is\n- the use of CSLS metric addressing the hubness problem, see sec 2.3 and figure 1D.\nIn contrast, we do not use any of the approaches described in Zhang et al. 2017b shown in their figure 2b and 2c.\nEmpirically, we demonstrate in tab. 1 the importance of both the refinement step and the use of CSLS metric to achieve excellent performance.\n\nIn addition to this key differences between the two approaches, we have also proposed a better stopping criterion as pointed out in your comment.  This is actually not just a stopping criterion but a \u201cvalidation\u201d criterion that quantifies the closeness of the source and target spaces, and that correlates well with the word translation accuracy (see Figure 2). We not only use it as a stopping criterion, but also to select the best models across several experiments, which is something that Zhang et al. cannot do. Moreover, their stopping criterion is based on \u201csharp drops of the generator loss\u201d, and it did not work in our experiments to select the best models (see Figure 2 of our paper).\n\nIn terms of evaluation protocol, Zhang et al. compare their unsupervised approach with a supervised method trained on 50 or 100 pairs of words only, which is a little odd given that most papers consider 5000 pairs of words (see Mikolov et al., Dinu et al., Faruqui et al., Smith et al., Artetxe et al., etc.). As a result, they have an extremely weak supervised baseline, while our supervised baseline is itself the new state of the art.\n\nFinally, note that we have released our code and we know that other research groups were able to already reproduce our results, and we have also released our ground-truth dictionaries and evaluation pipeline, which will hopefully help the community make further strides in this area (as today we lack a standardized evaluation protocol as pointed out above, and large scale ground truth dictionaries in lots of different language pairs).", "title": "title"}, "Skw7wkXQG": {"type": "rebuttal", "replyto": "H196sainb", "comment": "We thank all the reviewers for the feedback and comments. We replied to each of them individually, and uploaded a revised version of the paper. In particular, we:\n- Rephrased one of the claims made in the abstract about unsupervised machine translation\n- Added the requested 45.1% result of our unsupervised approach on the WaCky datasets\n- Fixed some typos\n- Added missing citations", "title": "review responses"}, "B1yRBYGQM": {"type": "rebuttal", "replyto": "H1Qhqm9ez", "comment": "We thank the reviewer for the feedback and comments.\n\nAs mentioned in the comments, we added to the paper citations to the work of Ravi & Knight (2011) and some subsequent works on decipherment, and down-toned some claims in the paper.\n\nThank you for pointing the paper of Vulic & Moens, we were not aware of this paper and we added a citation in the updated version of the paper. Note however that the work of Vulic & Moens relies on document-aligned corpora while our method does not require any form of alignment.\n\nWe evaluated the cross-lingual embeddings on 4 different tasks: cross-lingual word similarity, word translation, sentence retrieval, and sentence translation. It is true that the quality of these embeddings on other downstream tasks would be interesting and will be investigated in future work.", "title": "response 3"}, "H1RBrtfmG": {"type": "rebuttal", "replyto": "rJEg3TtxM", "comment": "We thank the reviewer for the feedback and comments.\n\nThe main concern of the review is about the lack of comparisons with existing works.\n- The reviewer reproaches the lack of comparison against CCA, while the comparison against CCA is provided in Table 2. The reviewer also points out the lack of comparison against Artetxe et al. (2017). This comparison is also provided in the paper.\n- We agree that our method could be compared to decipherment techniques, and would have been happy to try the method of Ravi & Knight but there is no open-source version of their code available online (like for Faruqui & Dyer, Dinu et al, Artexte et al, Smith et al). Therefore, considering the large body of literature in that domain, we focused on comparing our approach with the most recent state-of-the-art and supervised approaches, which in our opinion is a fair way to evaluate against reproducible baselines.\n\nThe second reviewer concern is about the performance of the model on non-comparable corpora. We considered that this was redundant with the results on Wikipedia provided in Table 1 and Table 2. As explained in one previous comment, our strategy was to first show that our supervised method (Procrustes-CSLS) is state-of-the-art, and then to compare our unsupervised approach against this new baseline. We added the result of our unsupervised approach (Adv - Refine - CSLS) on non-comparable WaCky corpora in Table 2. In particular, our unsupervised model on the non-comparable WaCky datasets is also state of the art with 45.1% accuracy.\n\nThe reviewer criticises the lack of novelty. To the best of our knowledge, the fact that an adversarial approach obtains state-of-the-art cross-lingual embeddings is new. Most importantly, the contributions of our paper are not limited to the adversarial approach. The CSLS method introduced to mitigate the hubness problem is new, and improves the state-of-the-art by up to 24% on the sentence retrieval task, as well as it improves the supervised baseline. We also introduced an unsupervised criterion that is highly correlated with the cross-lingual embeddings quality, which is also novel as far as we know, and a key element for training.\n\nAl last, please consider that we made our code publicly available and provided high-quality dictionaries for 110 oriented language pairs to help the community, as this type of resources are very difficult to find online.", "title": "response 2"}, "rJ0n4tGQG": {"type": "rebuttal", "replyto": "SyE3AHgxG", "comment": "We thank the reviewer for the feedback and comments.\n\nIt is true that the supervised approach is limited in the sense that it only considers 5000 pairs of words. However, previous works have shown that using more than 5000 pairs of words does not improve the performance (Artetxe et al. (2017)), and can even be detrimental (see Dinu et al. (2015)). This is why we decided to consider 5000 pairs only, to be consistent with previous works. Also, note that we made our supervised baseline (Procrustes + CSLS) as strong as possible, and it is actually state-of-the-art.\n\nRegarding the claim \"this is a first step towards fully unsupervised machine translation\", what we meant is that the method proposed in the paper could potentially be used in a more complex framework for unsupervised MT at the sentence level. We rephrased this sentence in the updated version of the paper.\n\nWe now address the comments / suggestions of the reviewer:\n\n- The abstract could indeed benefit from details about the model. We will add some.\n- The co-occurrence statistics have indeed an impact on the overall performance of the model. This impact is consistent for both supervised and unsupervised approaches. Indeed, our unsupervised method obtains 66.2% accuracy on the English-Italian pair on the Wikipedia corpora (Table 2), and 45.1% accuracy on the UKWAC / ITWAC non-comparable corpora. This result was not in the paper (we thought it was redundant with Table 1), but we added it in Table 2 in the updated version. Figure 3 in the appendix also gives insights about the impact of the similarity of the two domains, by comparing the quality of English-English alignment using embeddings trained on different English corpora.\n- It would indeed possible to add weights in Equation (6). We tried to weight the r_S and r_T terms, but we did not observe a significant improvement compared to the current equation.\n- In the supervised approach, we generated translations for all words from the source language to the target language, and vice-versa (a translation being a pair (x, y) associated with the probability for y of being the correct translation of x). Then, we considered all pairs of words (x, y) such that y has a high probability of being a translation of x, but also that x has a high probability of being a translation of y. Then, we sorted all generated translation pairs by frequency of the source word, and took the 5000 first resulting pairs.\n- We tried to use non-linear mappings (namely a feedforward network with 1 or 2 hidden layers), but in these experiments, the adversarial training was quite unstable, and like in Mikolov et al. (2013), we did not observe better results compared to the linear mapping. Actually, the linear mapping was working significantly better, and since the Procrustes algorithm in the refinement step requires the mapping to be linear, we decided to focus on this type of mapping. Moreover, the linear mapping is convenient because we can impose the orthogonality constraint, which guarantees that the quality of the source monolingual embeddings is preserved after mapping.\n- We did not try to jointly learn the embeddings as well as the mapping, but this is a nice idea and definitely something that needs to be investigated. We think that the joint learning could improve the cross-lingual embeddings, but especially, it could significantly improve the quality of monolingual embeddings on low-resource languages.\n- Our approach would definitely benefit from having a few parallel training points. These points could be used to pretrain the linear mapping for the adversarial training, or even as a validation dataset. This will be the focus of future work.", "title": "response 1"}, "BJ4hFZ5ez": {"type": "rebuttal", "replyto": "SkMy4hKlz", "comment": "The paper by Dinu et al. provides embeddings and dictionaries for the English-Italian language pair. The embeddings they provide have become pretty standard and we found at least 5 previous methods that used this dataset:\nMikolov et al., Faruqui et al., Dinu et al., Smith et al., Artetxe et al.\nThese previous papers provide strong supervised SOTA baselines on the word translation task, and in Table 2 we show results of our supervised method compared to these 5 papers. The row \u201cProcrustes + CSLS\u201d is a supervised baseline, training our method with supervision using exactly the same word embeddings and dictionaries as in Dinu et al. These results show that our supervised baseline works better than all these previous approaches (reaching 44.9% P@1 en-it).\nThe requested unsupervised configuration \u201cAdv - Refine - CSLS\u201d using the same embeddings and dictionary as in Dinu et al. obtains 45.1% on en-it, which is better than our supervised baseline (and SOTA by more than 2%). \n\nHowever, this information is redundant with Table 1, which shows that our unsupervised approach is better than our supervised baseline on European languages. We therefore decided not to incorporate this result, but we will add it back as suggested.\n\nMoreover, using the Wacky datasets (non comparable corpora) to learn embeddings, we improved the SOTA by 11.5% and 26.6% on the sentence retrieval task using our CSLS method, see table 3. Again, these experiments use the very same setting as previously reported in the literature.\n\nMore generally, regarding your comment \u201cthey inexplicably use a different set of embeddings, trained in a different corpus\u201d, note that:\n- As noted above, we did compare using the very same embeddings and settings as others.\n- We did study the effect of using different corpora: see fig. 3\n- As shown in the paper, using our method on Wikipedia improves the results by more than 20%\n- Wikipedia is available in most languages, pretrained embeddings were already released and publicly available, we just downloaded them (while the Wacky datasets are only available for a few languages)\n- We found that the monolingual quality of these pretrained embeddings is better than the one obtained on the Wacky datasets\n\nAs opposed to the 5 methods we compare ourselves against in the paper, Zhang et al. (2017):\n1) used different embeddings and dictionaries which they do not provide, \n2) used a lexicon of 50 or 100 word pairs only in their supervised baseline, which is different than standard practice since Mikolov et al. (2013b) (see Dinu et al., Faruqui et al., Smith et al., etc.) and which is also what we did, namely considering dictionaries with 5000 pairs. As a result, they compare themselves to a very weak baseline.\n3) in the retrieval task they consider a very simplistic settings, with only a few thousands words, as opposed to large dictionaries of 200k words (as done by Dinu et al., Smith et al. and us). \n4) they. do not provide a validation set and, as shown in Figure 2 in our paper, their stopping criterion does not work well.\nWe did try to run their code, but we have not been successful yet.\n\nAs for comparing against Artetxe et al., as reported in table 2 they obtain a P@1 of 39.7% while we obtain 45.1% using the same Dinu\u2019s embeddings.\n\nFinally, we have released our code, along with our embeddings / dictionaries for reproducibility. We will share the link here as soon as the decisions are out in order to preserve anonymity.", "title": "comparison"}, "rJcdzzcCb": {"type": "rebuttal", "replyto": "HkD8ivF0-", "comment": "Thank you for the pointer, we were aware of this work and we will add a citation. Note however that our focus is not to learn to a machine translation system (we just gave a simple example of this application, together with others like sentence retrieval, word similarity, etc.), but to infer a bilingual dictionary without using any labeled data. Unlike Ravi et al. we use monolingual data on both side at training time, and we infer a large bilingual dictionary (200K words). When we say \"this is a first step towards fully unsupervised machine translation\" it does not mean we are the first to look at this problem, we simply meant that our method could be used as a first step in a more complex pipeline. We will rephrase this sentence to avoid confusion.\nIn other words, the two works look at different things: this one is focussed on learning a bilingual dictionary, while the other is focussed on the problem of machine translation.", "title": "response"}}}