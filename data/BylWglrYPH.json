{"paper": {"title": "Symmetry and Systematicity", "authors": ["Jeff Mitchell", "Jeff Bowers"], "authorids": ["jeff.mitchell@bristol.ac.uk", "j.bowers@bristol.ac.uk"], "summary": "We use convolution to make neural networks behave more like symbolic systems.", "abstract": "We argue that symmetry is an important consideration in addressing the problem\nof systematicity and investigate two forms of symmetry relevant to symbolic processes. \nWe implement this approach in terms of convolution and show that it can\nbe used to achieve effective generalisation in three toy problems: rule learning,\ncomposition and grammar learning.", "keywords": ["symmetry", "systematicity", "convolution", "symbols", "generalisation"]}, "meta": {"decision": "Reject", "comment": "Thanks for clarifying several issues raised by the reviewers, which helped us understand the paper.\n\nAfter all, we decided not to accept this paper due to the weakness of its contribution. I hope the updated comments by the reviewers help you strengthen your paper for potential future submission."}, "review": {"ryx5OlGTYr": {"type": "review", "replyto": "BylWglrYPH", "review": "\n======================================== Update after rebuttal =============================================\n\nI have now read the author rebuttal, but my concerns about the paper remain. The training details are not described in anywhere near sufficient detail (optimizer?, batch size?, learning rate?, initialization?, etc). The baseline architectures \u201crecurrent net\u201d or the \u201cmulti-layer perceptron\u201d are not described at all, despite my explicit request to that effect. I had also requested to see the source code for the experiments as this would perhaps have illuminated a lot of the details left out in the paper, but the authors have not provided it. I understand that the authors are not required to provide their code, but this should have been a relatively straightforward request in this case given the simplicity of the experiments and as I mentioned in my initial review, it would have been very useful in evaluating the paper. \n\nIn their rebuttal, the authors also claimed that the results in Fig. 1 and Table 1 are training results (that even though the architecture is \"innate\", the weights are learned), but I'm concerned about this claim. I happen to be doing some experiments along these lines at the moment, and it is not trivial at all to get such crisp results as those shown in Fig. 1 & Table 1 in these kinds of experiments (even when the architecture is correctly specified). Again, it would have been very helpful if the authors had either provided their source code or had described their experimental setup in sufficient detail to allow the reproduction of these results. \n\nGiven these concerns, I have decided to keep my score as it is.\n\n========================================================================================================\n\nThis paper addresses an important problem: systematic generalization in neural networks. However, the paper is very confusing and I have some serious concerns about the models and the results presented in sections 3 and 4. Here are the main issues:\n\n1) In section 3, there are only 10x10=100 possible combinations in this composition task. Yet, the paper says \u201cwe randomly sample 1000 such translation pairs, choose three combinations and remove all instances of them from the training data and then exclusively test on unseen pairings of command and modifier.\u201d How can you sample 1000 pairs out of a possible 100 combinations? Also being able to generalize to 3 held-out combinations out of 100 is not very impressive. On the contrary, it is almost trivial.\n\n2) No details are given about the \u201crecurrent net\u201d or the \u201cmulti-layer perceptron\u201d baselines in section 3. What are these models? The fact that they have exactly zero accuracy is a bit suspicious, especially given the almost trivial nature of the task in section 3. Previous works reported perfect or near perfect accuracy with similar baselines in similar tasks (see e.g., Lake & Baroni, 2018).\n\n3) I'm afraid the proposed model in section 3 also doesn\u2019t make sense to me. It is explicitly acknowledged (Appendix B) that y is an M+1-dimensional vector, g is an Nx(M+1) matrix. Then by all accounts, the convolution of these should be an N-dimensional vector. Yet, somehow, h_t+1 in Equation 2 manages to be an NxM matrix. How? Please clarify this. If possible, making the source code available would be very helpful.\n\n4) What is the semantics of x and y in section 3? What exactly are they supposed to be doing? This is not explained in the paper beyond a vague description.\n\n5) Similar problems arise in section 4. The task is not explicitly described in the text. We only learn from Appendix C that it is actually to predict the next symbol. The task description mentions \u201cstrings of length 15, 17, 19, 21, 23 and 24,...\u201d (p. 5). But, the grammar in Fig. 3 can only generate odd length strings, it cannot generate a string of length 24. Is this a typo?\n\n6) Again in section 4, I have no idea how the proposed model is actually supposed to work. The motivation for the model and its description are not clear at all.\n\n7) The model in section 2 is hand-coded. It is not shown that it can actually learn this solution from data. What happens if the sequences are longer or if the rules are different, for example? Then you have to hand-code a completely different architecture.\n\n8) Which brings me to another important issue I have with this paper (and with similar papers): this whole set-up is very misguided in my mind. I think the real problem is not to come up with an architecture that would generalize systematically in a very specific (and usually toy) problem. It is to come up with an architecture that would learn to generalize systematically in a much broader set of problems. The learning aspect in sections 3-4 is a step in the right direction, but there\u2019s no evidence that the models proposed there can learn to generalize in any task other than the very specific tasks they were designed for (if they can actually do that).", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 2}, "SyeDvfMhoB": {"type": "rebuttal", "replyto": "B1xUuaAtir", "comment": "Thanks for your response and suggestions.\n\n>  the input is treated as a bag of words\n\nThis is a misunderstanding. A bag-of-words approach would not be able to learn to distinguish ABB from ABA on the training set, never mind generalise to the test set. Under a bag-of-words approach, wo fe fe looks the same as fe wo fe, so ABB and ABA are indistinguishable.\n\nThe permutation we consider in our paper is not permutation within the input, but permutation of the symbols themselves, i.e. wo is replaced with la. As I explained above, we do not share weights across time steps (as in a bag-of-words approach) but instead between symbols. \n\nThus we are talking about a different sort of permutation, having considerably different applications, from bag or set representations. Indeed, that form of permutation is not really relevant to the issue of systematicity. Thus, these works are only superficially related to this paper.", "title": "Relation between bag-of-words, Deep Sets and our approach."}, "SJgWsDRKor": {"type": "rebuttal", "replyto": "B1ePQyfk9S", "comment": "Thanks for your comments.\n\n> While this shows efficacy of modeling symmetry, I'd be curious about performance graphs as the training data increases in size.\n\nThe key point about Marcus's task is that the syllables encountered at test time are not present in the training data. So, given a one-hot representation, the units representing those unseen syllables are never active during training and in a standard MLP or RNN that means the weights connected to those units are never updated. No additional quantities of training data change this, and their performance remains at random. \n\nConvolution solves this problem by sharing weights between syllables, allowing an abstract pattern to be learned across all syllables. Test time performance on the unseen syllables is identical to the other syllables, because the symmetry ensures all the syllables are equivalent.\n\nThere are other ways to solve this problem, for example by changing the representation. But this avoids the central problem of learning rules that can be applied to novel inputs. Our solution imposes a symmetry of symbolic systems onto the net and as a consequence learns abstract rules of the kind that Marcus was interested in.\n\n\n> Curiously, the recurrent baseline seems to perform better than 0% accuracy (if still poorly) on the original SCAN task which is much harder than the proposed task in this paper.\n\nActually, many of the instances in the original SCAN test sets are easier than ours. \n\nFor example, 'jump left and look left twice' occurs in one of the SCAN test sets, while 'jump' on its own and 'walk left and look left twice' occur in the corresponding training set. Generalisation in this case is easier, because 'jump' and the context it occurs in can be translated independently.\n\n> I still cannot intuitively understand why convolutions in the forget architecture would learn about symmetry related to structured repetition produced by a CFG.\n\nIn a width 3 convolution, the filter 100 corresponds to shifting values to the right and 001 corresponds to shifting to the left. In the stack of memory cells, these shifting operations can be used to move the contents up and down the stack. Along with reading and writing only from the bottom of the stack this mimics a Push Down Automata, and the correspondence between PDAs and CFGs is well known.\n\nMore concretely, to predict palindromes like aadcbobcdaa the net learns to push the first half of the string onto the stack until it reaches the central o and then switches direction to pop items off the stack until the end.\n\n> my major concern is that the tasks considered are too simple and at least one complicated large-scale task would have strengthened the paper.\n\nWould a complicated large-scale task have made things clearer?\n\nUltimately, we intend to apply the ideas described here to a more naturalistic task, e.g. language modelling. However, real data brings a host of complications that can obscure the underlying hypotheses. Here we have introduced the idea that symmetry can be a means to gain systematicity, and demonstrated this on three toy problems, which allowed us to investigate specific questions in a controlled manner.\n\nWe found that symmetry allows us to generalise to unseen symbols, to separate content from structure and to generalise to structures that are more complex than those seen at training. If we had rejected the use of controlled experiments then it is unlikely that we could have gained comparable clarity on these questions.\n", "title": "Response to Review #1"}, "HJl2YAI7sS": {"type": "rebuttal", "replyto": "ryx5OlGTYr", "comment": "Thanks for raising these concerns.\n\n> The model in section 2 is hand-coded. It is not shown that it can actually learn this solution from data.\n\nThe architecture is designed for this particular task, but the weights are learned from the data. This is not an unusual setup. Nor are convolution and pooling anomalous architectural choices.\n\nBut the point of the paper is not to sell a particular architecture, it is to investigate how symmetries relevant to symbolic processes can be introduced into neural architectures, and whether that leads to more systematic generalisation.\n\n> It is to come up with an architecture that would learn to generalize systematically in a much broader set of problems.\n\nI agree that this should be a core objective of Machine Learning. And I am happy to concede that I have not presented in this paper an architecture that faithfully reproduces the full robustness of human generalisation capabilities.\n\nToy problems and simple architectures help us to progress towards this goal, because they allow us to investigate specific questions under controlled conditions with comprehensible models.\n\nAll of the models described in this paper are shallow and built out of standard components, and while the reviews suggested this simplicity or lack of novelty was a problem they also managed to be confused by what exactly these apparently trivial models were doing. I am unconvinced a deeper and more sophisticated architecture applied to more complex and heterogeneous tasks would have been easier for readers to understand and so shed more light on the questions posed.\n\n> there\u2019s no evidence that the models proposed there can learn to generalize in any task other than the very specific tasks they were designed for\n\nIt is fairly common for ML papers to focus on a single dataset, i.e. to provide no evidence that the particular innovation they introduce is relevant to any other task.\n\nIn contrast, we showed that symmetry was relevant to obtaining systematic generalisation in three different tasks. We also discussed how the symmetries imposed on the neural architectures related to the properties of symbolic systems. In other words, we provided a range of both theoretical and experimental evidence that imposing the right kind of symmetries can support systematic generalisation.", "title": "Discussion of Methodology"}, "SJeX-qL7jH": {"type": "rebuttal", "replyto": "ryx5OlGTYr", "comment": "Thanks for raising these questions.\n\n> Also being able to generalize to 3 held-out combinations out of 100 is not very impressive. On the contrary, it is almost trivial.\n\nTraining on 97% of the data excludes 'not having seen enough of the data' as an explanation for a failure to generalise. And this is precisely why we chose this regime, as this allow us to focus on the impact of symmetry.\n\nThe results suggest that the problem is trivial for the convolutional architecture, but almost impossible for the MLP and RNN.\n\nIt is trivial for the convolutional architecture, because the symmetry allows it to represent structure (e.g. repeat the same thing twice) independently of the content (e.g. JUMP). So when it encounters 'jump two' at test time it has no problem generalising. The MLP and RNN in contrast learn hidden representations that are typically conjunctive, ie do not represent structure and content separately. So it is very unlikely for them to generalise robustly.\n\nHowever, the difficulty or triviality of the task is not the point of the experiment, which is instead to investigate how symmetry can be used to support composition.\n\n> Previous works reported perfect or near perfect accuracy with similar baselines in similar tasks (see e.g., Lake & Baroni, 2018).\n\nAlthough our task is a simplification of SCAN, this does not mean our task is easier. SCAN is complex because it contains a diverse range of structures, some of which are easier than others.\n\nFor example, 'jump left and look left twice' occurs in one of the SCAN test sets, while 'jump' on its own and 'walk left and look left twice' occur in the corresponding training set. Generalisation in this case is easier, because 'jump' and the context it occurs in can be translated independently.\n\n\n> What is the semantics of x and y in section 3?\n\nAs suggested in paragraph 5 of Section 3, our intention is to represent an action (e.g. JUMP) in terms of a position and a structure (do it twice) in terms of the channels of a convolutional network. So, x represents the action to be performed and y is the structure.\n\n> I have no idea how the proposed model is actually supposed to work. The motivation for the model and its description are not clear at all.\n\nThe basic idea is that CFGs allow nested structures, e.g. a noun phrase contained within another noun phrase. If the same rules apply to all noun phrases, however deeply embedded, then this is a kind of symmetry. In particular, for an LSTM to handle these structures we really need a symmetry over the memory cells, so that it can hold multiple constituents of the same type, as it moves through a nested structure.\n\nA convolution over the memory cells not only supplies the symmetry that makes the idea of the same symbol stored in multiple places meaningful, it also introduces the possibility of shifting symbols across the stack of memory cells. This is important because a context free language can also be defined in terms of a PDA. The stack of the PDA has push and pop operations that correspond to shifting everything one place further into the stack and writing a new symbol at the top, or reading a symbol from the stack and shifting everything one place back. For convolution, these shift operations can be defined in terms of width 3 filters.", "title": "Discussion of Models and Experiments"}, "BklVZr8XiS": {"type": "rebuttal", "replyto": "ryx5OlGTYr", "comment": " Thank you for identifying these issues.\n\n> How can you sample 1000 pairs out of a possible 100 combinations?\n\nWe sample with replacement.\n\n> y is an M+1-dimensional vector ...  Please clarify this.\n\nThis is a typo. It should be an (M+1)xM dimensional vector. \n\n> \u201cstrings of length 15, 17, 19, 21, 23 and 24,...\u201d (p. 5). ...  Is this a typo?\n\nYes, it should be 25.", "title": "Typos and Clarifications from Review #3"}, "Syx8eeAbir": {"type": "rebuttal", "replyto": "HJe7pRBAFH", "comment": "Thank you for your comments.\n\n> For example, the first task is simply using a single convolution layer followed by pooling for sequence prediction. However, using 1D convolution layers are somewhat wide-spread in NLP.\n\nThis is a misunderstanding. Our application of convolution is significantly different to the standard application to sequences in NLP. \n\nIn the standard use, weight sharing happens across time steps, and the function learned is equivariant to time-translations. In our case, weight sharing is between symbols, and the function is equivariant to permutation of symbols.\n\nSo, in Figure 1, the standard way of applying convolution to sequences would take the horizontal syllable dimension as the channels and convolve in the vertical dimension of time steps. In our network, this is reversed, and we take time steps as the channels and convolve across symbols, as explained in paragraph 6 of section 2.\n\nThe standard convolutional approach to sequences (weight sharing across time) will not solve Marcus's challenge, but our approach does.\n\nHowever, the point of the experiment is not to show off a new architectural innovation. It is to demonstrate that imposing a permutation invariance on symbols (as discussed by Tarski) allows us to model the rule learning behaviour studied by Marcus.\n\n> The paper is oblivious to a large body of related work\n\nThanks for the references, some of which may usefully expand our related work section. However, many of these papers are only distantly or superficially related to the problems and architectures we discuss in the paper.\n\n> it is not clear why translation invariance in-memory models the structure in a context-free grammar\n\nThe relation between CFGs and PDAs is well known, and imposing translation invariance on the memory cells turns an LSTM into a PDA. In particular, the push and pop operations of a PDA can be thought of as translation within the stack of memory cells. If we apply the width 3 filter 001 to a vector of values, this will shift all the values one place to the left, while 100 corresponds to a right shift. Along with only reading and writing to one end of the stack, this reproduces the behaviour of a PDA.\n\nMore abstractly, a symmetry across memory locations allows us to treat all instances of a symbol equivariantly. This allows the architecture to exploit memory slots at test time that were not used in training. As a consequence, the network more readily extends the learned grammar to more complex examples (i.e. those requiring more memory), essentially because the symmetry gives meaning to the idea of applying the same rule to all memory slots.", "title": "Response to Review #2"}, "HJe7pRBAFH": {"type": "review", "replyto": "BylWglrYPH", "review": "The paper investigates the idea of using symmetry and invariance in symbolic reasoning. In particular, it considers models where modeling symbolic symmetries through parameter-sharing help with generalization. The three tasks considered are: 1) rule learning: performs sequence-classification. 2) composition: performs sequence-to-sequence with structured input using encoder-decoder architecture, and 3) context-free language learning using memory. In the first two cases, the convolution (of single width) is used to benefit from the symmetry prior, and in the third task, convolution is applied to stack memory structure. In all cases, the proposed architectures were shown to outperform MLP and RNN.\n\nThe paper addresses an important area in deep learning, and the paper is accessible and easy to read. However, there are major issues:\n\n-- I found it challenging to identify a novel contribution. For example, the first task is simply using a single convolution layer followed by pooling for sequence prediction. However, using 1D convolution layers are somewhat wide-spread in NLP. \n\n-- The paper is oblivious to a large body of related work in the area of relational learning and invariant/equivariant deep learning. Here are some examples: Permutation invariant model for sets [1,2], and the link between parameter-sharing and invariance [3,4] is theoretically studied in several works. Note that convolution with a filter of width one followed by pooling is exactly invariant to the symmetric group. There are related works that extend these ideas to graphs [5,6], and relational learning [7]. Invariance has also been explored as it relates to memory [8]. Another relevant direction to discussions of the paper is the idea of attention in various architectures, such as transformers.\n\n-- There are vague or misleading claims. In particular, for some tasks, it is not clear why the proposed architecture addresses the targeted symmetry. For example, it is not clear why translation invariance in-memory models the structure in a context-free grammar. \n\n\n[1] Zaheer, Manzil, et al. \"Deep sets.\" Advances in neural information processing systems. 2017. \n[2] Qi, Charles R., et al. \"Pointnet: Deep learning on point sets for 3d classification and segmentation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. \n[3] Shawe-Taylor, John. \"Building symmetries into feedforward networks.\" 1989 First IEE International Conference on Artificial Neural Networks,(Conf. Publ. No. 313). IET, 1989.\n[4] Ravanbakhsh, Siamak, Jeff Schneider, and Barnabas Poczos. \"Equivariance through parameter-sharing.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. \n[5] Kondor, Risi, et al. \"Covariant compositional networks for learning graphs.\" arXiv preprint arXiv:1801.02144 (2018). \n[6] Maron, Haggai, et al. \"Invariant and equivariant graph networks.\" arXiv preprint arXiv:1812.09902 (2018). \n[8] Kazemi, Seyed Mehran, and David Poole. \"RelNN: A deep neural model for relational learning.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018. \n[9] Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. \"Order matters: Sequence to sequence for sets.\" arXiv preprint arXiv:1511.06391 (2015).\n", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 2}, "B1ePQyfk9S": {"type": "review", "replyto": "BylWglrYPH", "review": "This paper focuses on modelling invariances or symmetry between various components for solving tasks via convolutions and weight sharing.\nThe proposed tasks are toyish in nature although they do give insights into importance of modeling symmetry for better generalization. The first task is a symbol substitution which considers a permutation in source symbols and maps them to either \"ABA\" or  \"ABB\" categories i.e binary classification. While this task does require generalizability, it is surprising that the mlp and recurrent net baselines are so much inferior (basically random) to the convolution baseline. While this shows efficacy of modeling symmetry, I'd be curious about performance graphs as the training data increases in size.\nThe second task is an artificially created task inspired from the SCAN dataset. The task is to translate a verb-number pair into number repetitions of the verb. The encoder decoder network uses convolution in the recurrences to capture the notion of generalizability. The input and output space is very small (10 verbs and 10 numbers) but shows superiority of convolution and weight sharing over other baselines. Curiously, the recurrent baseline seems to perform better than 0% accuracy (if still poorly) on the original SCAN task which is much harder than the proposed task in this paper. Maybe, the number of examples (1000) is too small recurrent networks but this makes me a little surprised. More details about the architecture and training procedure for baselines would be helpful to ensure that the comparison is fair across baselines.\nThe final task is CFG modeling where convolutions are used to model the forget gate of an LSTM which seems to endow the network with PDA like properties and the convolutions are more effective than baselines at modeling this.\n\nApart from the concerns related to the results mentioned above, my major concern is that the tasks considered are too simple and at least one complicated large-scale task would have strengthened the paper.\nAlso, for tasks 2 and 3, the motivation behind using convolutions is not as clean as in task 1. So more analysis and insights into model performance, the weights learned, ablation studies etc. would have helped in understanding how the convolutions are modeling the symmetry. This should be informative and tractable because of simplicity of the tasks involved.\n\nFinally, as mentioned above, I still cannot intuitively understand why convolutions in the forget architecture would learn about symmetry related to structured repetition produced by a CFG. Hence, more analysis or a better motivation would have helped. ", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}}}