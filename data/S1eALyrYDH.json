{"paper": {"title": "RNA Secondary Structure Prediction By Learning Unrolled Algorithms", "authors": ["Xinshi Chen", "Yu Li", "Ramzan Umarov", "Xin Gao", "Le Song"], "authorids": ["xinshi.chen@gatech.edu", "yu.li@kaust.edu.sa", "ramzan.umarov@kaust.edu.sa", "xin.gao@kaust.edu.sa", "lsong@cc.gatech.edu"], "summary": "A DL model for RNA secondary structure prediction, which uses an unrolled algorithm in the architecture to enforce constraints.", "abstract": "In this paper, we propose an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea of E2Efold is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, we demonstrate the superior performance of E2Efold: it predicts significantly better structures compared to previous SOTA (especially for pseudoknotted structures), while being as efficient as the fastest algorithms in terms of inference time.", "keywords": ["RNA secondary structure prediction", "learning algorithm", "deep architecture design", "computational biology"]}, "meta": {"decision": "Accept (Talk)", "comment": "This paper proposes a RNA structure prediction algorithm based on an unrolled inference algorithm. The proposed approach overcomes limitations of previous methods, such as dynamic programming (which does not work for molecular configurations that do not factorize), or energy-based models (which require a minimization step, e.g. by using MCMC to traverse the energy landscape and find minima).\n\nReviewers agreed that the method presented here is novel on this application domain, has excellent empirical evaluation setup with strong numerical results, and has the potential to be of interest to the wider deep learning community. The AC shares these views and recommends an enthusiastic acceptance. "}, "review": {"B1g-0PdH3r": {"type": "review", "replyto": "S1eALyrYDH", "review": "RNA Secondary Structure Prediction by Learning Unrolled Algorithms\n\nThis paper proposes E2Efold, which is an RNA secondary structure prediction algorithm based on an unrolled algorithm. Previous methods rely on dynamic programming (which does not work for molecular configurations that do not factorize) or rely on energy-based models (which require a minimization step, e.g. by using MCMC to traverse the energy landscape and find minima). The former does not work for all molecules and the latter can be difficult to optimize. The method presented here is novel, shows strong SOTA performance, and would be of interest to the wider deep learning community.\n\nThe method is based on an unrolled algorithm, which is motivated by the inclusion of three inductive biases / constraints important underlying RNA folding. These constraints limit the wide RNA search space. The first component of the method is a \u201cDeep Score Network\u201d which uses a stack of Transformer encoders (with relative and exact positional embeddings) followed by 2D convolutional layers to output a L x L symmetric matrix describing the \u201cscores\u201d of base pairing. As these scores may not obey the rules of RNA folding, a second post-processing network is trained end-to-end together with the \u201cDeep Score Network\u201d to enforce constraints. This network starts with a transformation that symmetrizes the matrix and applies a constraint-enforcing mask. The problem is transformed into an unconstrained problem by using Lagrange multipliers; it is then solved using a proximal gradient. Finally, a recurrent cell is defined that implements this algorithm in a deep learning framework. This method is creative, could be applied to other tasks with constraints, and would be interesting to the wider deep learning community.\n\nIn addition to developing the deep score network and post-processing network, the authors also develop a differentiable F1 loss, so that the network can directly optimize for precision and recall on the task. The performance of this method significantly outperforms previous methods. There was a fruitful discussion on OpenReview regarding whether this was a result of overfitting on the task. Indeed, it is critical in deep learning applications to carefully construct train/test sets to avoid high performance by memorization alone. To address this, the authors train on RNAStralign and test on ArchiveII. As the original ArchiveII dataset contains subsequences of other RNA sequences, which can result in overfitting, the authors re-ran their experiment with that removed, and similar results were achieved. To support the hypothesis that ArchiveII and RNAStralign capture different distributions, they perform a permutation test on the unbiased empirical Maximum Mean Discrepancy estimator, finding that the distributions are different. I do wonder why they did not check if P(ArchiveII) = P(ArchiveII) as they do check if P(RNAStr_train) = P(RNAStr_train). On the specific task of pseudoknot prediction, the method also performs well (F1 is >0.23 over the baseline). On sequence length-weighted F1, the model does even better.\n\nThe paper is rich with ablations. The analysis of the number of unrolling iterations T helps support the use of an unrolled method and builds intuition for its importance - it would be useful to include this in the appendix of the paper. I also appreciated the visualizations, which are a good sanity check that the model correctly handles pseudoknots. The performance of the method is broken down by RNA family, which is also quite interesting -- the method outperforms LinearFold on all classes, besides 5S RNA, SRP, and Group I intron. Further analysis is required to better understand why the method is weaker on those datasets. Additionally, further work should explore training on one set of families and testing on a held-out set of families. This was pointed out by public comments on this paper. This is potentially a limitation of E2EFold (the authors do not seem to have tried this suggested experiment) and further exploration is required. Exploring this limitation (even if it is not overcome) would make this paper even more rich.\n\nThat said, I recommend acceptance of this paper due to the extensive experiments, polished writing, novel method, and strong results, which can inspire future research. \n", "title": "Official Blind Review #4", "rating": "8: Accept", "confidence": 2}, "B1lJZQthiB": {"type": "rebuttal", "replyto": "r1eofYOniB", "comment": "Thank you very much for your positive feedback! Yes, we agree with you and we will keep refining the manuscript to make the final version more comprehensive.", "title": "Thank you very much for your positive feedback! "}, "BkxrY85dsH": {"type": "rebuttal", "replyto": "S1eALyrYDH", "comment": "We would like to thank all the reviewers for their careful reading, detailed comments, and overall positive assessment. We have responded to every raised question and concern, and incorporated your constructive suggestions into the revised version. The major revisions are summarized:\n\n\n***More Details***\n\nWe\u2019ve improved the clarity of the paper by adding\n- More related works in Section 2 and Appendix A.\n- Appendix B: Derivation Of The Proximal Gradient Step\n- Appendix C: Implementation And Training Details\nWe will also release our code for others to reproduce all the experimental results.\n\n\n***More Experimental Results***\n\nWe\u2019ve included additional results of experiments suggested by both the reviewers and the public comments.\n- Appendix D.2. Two-sample Hypothesis Testing. (For better understanding data distributions in RNAStralign and ArchiveII)\n- Appendix D.3. Performance On Long Sequences: Weighted F1 Score\n- Appendix D.4. ArchiveII Results After Domain Sequences Are Removed\n- Appendix D.5. Per-family Performances\n\n\n***Flow of the paper***\n\nAs suggested, we add a new section \u201cRNA Secondary Structure Prediction Problem\u201d which follows the Related Work Section to formally state the problem and concrete constraints earlier than the previous version.\n\n\nThank all the reviewers for the constructive suggestions on paper refinement!", "title": "Summary of major revisions"}, "HJgMRKJuiB": {"type": "rebuttal", "replyto": "SJlmuE8k5S", "comment": "We thank reviewer 1 for careful reading and constructive suggestions for paper refinement! We separate our response to reviewer 1 into two parts. \n\nThe first part of reviewer 1\u2019s comments is about missed important details. We are sorry for the lack of clarity and thank the reviewer for pointing them out! Now the details are included in the revised paper, and we also explained them below:\n\n\n***Compare to existing approaches for unrolling optimization problems\n\nWe explained the novelty/difference of E2Efold compared to the existing approaches below. They are now included in the main text and appendix A.\n\nFirst, our view of incorporating constraints to reduce output space and to reduce sample complexity is novel. Previous works [Belanger et al., 2017; Pillutla et al., 2018; Ingraham et al., 2018]  did not discuss these aspects. The most related work which also integrates constraints is OptNet [Amos & Kolter, 2017], but it\u2019s very expensive and can not scale to the RNA problem. Therefore, our proposed approach is a simple and effective one.\n\nSecond, compared to [Andrychowicz et al., 2016, Chen et al., 2018; Shrivastava et al., 2019], our approach has a different purpose of using the unrolled algorithm. Their goal is to learn a better algorithm, so they commonly make the architecture *more flexible* than the original algorithm for the room of improvement. However, we aim at enforcing constraints. To ensure that constraints are nicely incorporated, we keep the original structure of the algorithm and only make the hyperparameters learnable.\n\nFinally, although all works consider end-to-end training, none of them can directly optimize the F1 score. We proposed a differentiable loss function to mimic the F1 score/precision/recall, which is effective and also very useful when negative samples are much fewer than positive samples (or the inverse).\n\n\n***Details on the unrolling constant T\n\nFor the explanation on the choice/effect of unrolling constant T, please kindly refer to our response to this common question posted above.\n\n\n***Details on the design of the training process\n\nNow all the training details are included in Appendix C of the revised paper. \n\nConnecting the score network and PPN, the whole network is very deep. Both the pre-training and the augmented loss are the tricks that we use to speed up and stabilize the training process. \n- Pre-train: Since we expect $U_{\\theta}(i,j)$ to be higher if $(x_i,x_j)$ are truly paired, we use the true secondary structure to pre-train the score network to quickly get a fairly good U.\n- Augmented loss: During joint training, we use (loss in Eq 9) + $c \\cdot$(logistic regression loss on $U_{\\theta}$) where we set $c=1$. The second term is estimated at the *intermediate output*. Although this term is optional, we think it can help stabilize the training of the *very deep* network and also make each gradient step more efficient.\n\nBesides, we\u2019ve submitted a link to our code through a private comment to reviewers. We will get this code well-organized and released to the public for others to reproduce all the experimental results.", "title": "Response To Official Blind Review #1 (part 1)"}, "S1eeyK1_iH": {"type": "rebuttal", "replyto": "S1xh9zkV9H", "comment": "\nWe thank the reviewer for the positive comments about the approach and the suggestions for paper refinement! We\u2019ve included all the important details suggested by the reviewer in either the main paper or appendix. Experimental results are demonstrated to show how we choose the unrolling constant T.\n\n\n***Q1. details of the configuration\n\nWe\u2019ve added a section (Appendix C) to explain our configurations including hyperparameters, MLP details, and the \u2018pairwise concatenation\u2019 details. \n\nBesides, we\u2019ve submitted a link to our code through a private comment to reviewers. We will get this code well-organized and released to the public for others to reproduce all the experimental results.\n\n\n***Q2. \u201cWhat unrolling constant T is used? \u201d\n\nFor the explanation on the choice/effect of unrolling constant T, please kindly refer to our response to this common question posted above.\n\n\n***Q3. details on how U_\\theta is trained by itself\n\nWe are sorry for the lack of clarity in our first submission. Now all the training details are included in the revised paper. Please kindly refer to Appendix C. Briefly speaking, the training process of $U_{\\theta}$ is the same as E2Efold, which consists of two steps. First, quickly pre-train $U_{\\theta}$ using logistic regression. Second, use the loss in eq 9 to jointly train $U_{\\theta}$ and $PP_{\\phi}$ end-2-end (when $U_{\\theta}$ is trained by itself we can simply set T=0 in the PP network).\n\n\n***Q4. English grammar and writing style\nWe appreciate the reviewer for the very close reading! We believe the typos/errors are now fixed.\n\n\nPlease expect the revised paper posted soon.", "title": "Response To Official Blind Review #2"}, "rJge8iyuoB": {"type": "rebuttal", "replyto": "Bklpej_AKH", "comment": "We would like to thank the reviewer for the overall positive comments, constructive suggestions on paper refinement and references to interesting related works! \n\n\n***Q1.  \u201c...output constraints doesn't occur until late...\u201d\n\nThank you for your suggestion on the flow of our paper! In the revised version, we state the RNA secondary structure prediction problem, including the concrete constraints, in a newly added section \u201c3 RNA Secondary Structure Prediction Problem\u201d, which follows the Related Work section. \n\n\n***Q2. Related work in NLP\n\nThank you for referring us to these related works in NLP! We also noticed the relation to NLP as we mentioned at the end of the introduction section. This indeed motivates us to use transformers and also the trick mentioned in BERT to compute position information by a series of base functions. However, it is interesting to know about projective parsing vs. non-projective parsing which we didn\u2019t notice before! We\u2019ve added a paragraph in the related work section to discuss this aspect. Thank you for pointing it out, which can help us relate our work to a larger range of problems in ML.\n\n\n***Q3. Related work on Graphical Models and ISTA\n\nThanks! We found the \u201cdeep unfolding\u201d work very related and cited it in the revised paper.\nIn our first submission, we\u2019ve cited the unrolled ISTA paper \u201cTheoretical linear convergence of unfolded ISTA and its practical weights and thresholds.\u201d\n\n\n***Q4. Motivation for the setup in \"Test On ArchiveII Without Re-training\"\n\nOne can think of ArchiveII as a separate held-out test dataset. E2Efold is only learned from RNAStralign training set, but can directly generalize to ArchiveII, and obtain the best test results. \n\nIn fact, testing on the ArchiveII dataset is a more challenging test for generalization, because the data distribution in ArchiveII can have a larger difference with the RNAStralign training set. To see this, we performed additional statistical hypothesis tests using Maximum Mean Discrepancy (MMD)  [1] and attached the results in Appendix D.2. \n\nMore specifically, we computed the empirical MMD to evaluate the differences between\n(a) RNAStralign_train and RNAStralign_test, where the MMD is 0.0025 (*can not reject* null hypothesis of no difference with p-value 0.1)\n(b) RNAStralign_train and ArchiveII, where the MMD is 0.0296 (*reject* null hypothesis of no difference with p-value < 0.001)\nThese tests suggest that the difference between RNAStralign training set and ArchiveII is much larger.\n\nTherefore, the data distribution in ArchiveII is very different from the RNAStralign training set. A good performance on ArchiveII shows a significant generalization power of our model.\n\n\n***Q5. the number of optimizer iterations \n\nFor the explanation on the choice/effect of the number of optimizer iterations, please kindly refer to our response to this common question posted above.\n\n\n***Q6. Does it work to train with a fixed number of unrolled iters, but at test time run the optimizer until convergence?\u201d\n\nIt\u2019s very interesting that the reviewer asked this question! We were also curious about this before and tried it empirically. We trained the model on T=20 and used it for T=50 iterations during the test phase. However, the performance is not as good as keeping T=20 for the test. \n\nWe think the reason could be: we choose the discounting factor $\\gamma=1$ in equation 9, which is also a default choice in some related works. It gives the output at each time step T=t an equal weight. With a smaller $\\gamma$, the outputs at later steps can gain more weights. In this case, it is possible that the trained network will output a progressively closer approximation of the ground truth and further generalize to a larger number of iterations. We would investigate this option in the future.\n\n\n***Finally\n\nYes, we also like the differentiable F1 score! Imbalanced data (more negative samples than positive samples) is a common issue in many computational biology problems (e.g. [4,5]) and our proposed method is very simple and effective in this case.\n\n\n[1] Gretton, Arthur, et al. \"A kernel two-sample test.\" JMLR (2012)\n[4] Zhou, Jian, and Olga G. Troyanskaya. \"Predicting effects of noncoding variants with deep learning\u2013based sequence model.\" Nature methods 12.10 (2015): 931.\n[5] Armenteros, Jos\u00e9 Juan Almagro, et al. \"SignalP 5.0 improves signal peptide predictions using deep neural networks.\" Nature biotechnology 37.4 (2019): 420.\n\n\nPlease expect the revised paper posted soon.", "title": "Response To Official Blind Review #3"}, "HJeLA51_iS": {"type": "rebuttal", "replyto": "HJgMRKJuiB", "comment": "Here we provided our responses to the second part of reviewer 1\u2019s comments.\n\n***Overstates the improvements\n\nAccording to the reviewer\u2019s suggestion, we\u2019ve moderated our statement by simply saying \u201cbetter than previous SOTA in terms of F1 scores\u201d. \nBtw, the performances on RNAStralign are estimated on a held-out test set, so we think the comparison on this test set is also valuable.\n\n\n***performance per RNA category\n\nTo balance the performance among different families, during the training phase we conducted weighted sampling of the data based on their family size.  With weighted sampling, the overall F1 score (with shifted) is 0.83, which is the same as when we did i.i.d. sampling. The per-family F1 scores are provided below. More numbers are included in Appendix D.5 in the revised paper.\n\nTable 1: RNAStralign (test set): per-family F1 score\n                         |16S rRNA| tRNA | 5S RNA |  SRP | tmRNA |  Grp I  | RNaseP | telomerase \nfamily size      |    11620  |  9385 |   6443   |  468   |    572   |  1502   |     434    |   37\nE2Efold           |    0.783   | 0.917 |   0.906   | 0.550 |  0.588  | 0.387  |   0.565   |  0.954   \nLinearFold     |   0.493    | 0.734 |   0.713   | 0.618 |  0.393  | 0.565  |   0.567   |  0.515\nMfold              |    0.362   | 0.662 |   0.356   | 0.350 |  0.290  | 0.483  |   0.562   |  0.403\nRNAstructure|    0.464   | 0.709 |   0.578   | 0.579 |  0.400  | 0.566  |   0.589   |  0.512\nRNAfold          |    0.430   | 0.695 |   0.592   | 0.617 |  0.411  | 0.589  |   0.544   |  0.471 \nCONTRAfold  |    0.529   |  0.758 |   0.717  | 0.563 |  0.463  |  0.603 |   0.645   |  0.529\n\nE2Efold performs significantly better than other methods in 16S rRNA, tRNA, 5S RNA, tmRNA, and telomerase, and these results are from *a single model*. \n\nIn the future, we can view it as multi-task learning and further improve the performance by learning multiple models for different families and learning an additional classifier to predict which model to use for the input sequence.\n\n\n***Q3. Derivation for the proximal gradient steps\n\nWe added include the derivation steps for Eq. 3-5 in Appendix B in the revised version.\n\n\n***Q4. Why is there a need to introduce an l_1 penalty term to make A sparse?\n\nThis is mainly due to the *fixed* number of iterations T. We prefer using a comparatively *small T* (e.g. T=20) as we explained above. In this case, we use a sparse penalty to help the algorithm quickly incorporate the constraints to the output within T iterations, since it is our prior knowledge that the global constraints (e.g. each $x_i$ can only be paired with at most *one* base $x_j$) will give us a sparse output.\n\nFor example, within T=20 iterations, we compare the differences between adding and not adding the sparse penalty:\n\nTable 2: Constraints-check. \n           \t\t                   | $\\max_{A}c(A)$ | $\\text{mean}_{A}c(A)$\nWith sparse penalty       |           4         |     0.65\nWithout sparse penalty | \t       28        |    3.975           \n \nIn Table 1, $c(A):=\\text{sum}(\\text{relu}(A\\mathbf{1} - \\mathbf{1}))$ is measuring whether the constraints are satisfied. The smaller the better. The benefit of adding the penalty term is obvious, in the case when T is fixed and comparatively small. \n\n\n***Q5. On which data is Table 6?\n\nRNAStralign. We are sorry for the lack of clarity. We\u2019ve indicated the dataset in the caption of Table 6 in the revised version.\n\n\n***grammar error/typo\n\nYes! In the equation above Equation 2, it should be $-\\rho||\\hat{A}||_1$ instead of plus! Thank you for pointing all the typos/errors out! We believe they are now fixed. \n\n\nPlease expect the revised paper posted soon.", "title": "Response To Official Blind Review #1 (part 2) "}, "SyxL1Okuor": {"type": "rebuttal", "replyto": "S1eALyrYDH", "comment": "\nWe explained the choice of the number of unrolling iterations T in this post, since all reviewers asked questions about this constant.\n\nThe performance that we reported in the paper is when T=20. The overall ideas are:\n- to ensure that the constraints are mostly satisfied, T can not be too small. \n- a very large T will make the neural network very deep and the training process more expensive and unstable.\n\nWhat we actually did is:\n- We first pre-train the score network $U_{\\theta}$ without considering the post process network.\n- Then we fix this trained $U_{\\theta}$ and try different T without further training  (Table 1).\n- After the best T is selected, the score network and the post-processing network are trained end-2-end. \n\nTable 1: Constraints-check and validation F1 scores\n           | $\\max_{A}c(A)$ | $\\text{mean}_{A}c(A)$ | validation F1\nT = 5   |\t      160\t     |       28.53      |    0.772\nT = 10 |         33          |        7.78       |     0.791\nT = 20 | \t       4\t     |       0.65        |     0.806                 \nT = 30 |         1\t     |       0.03        |     0.809\nT = 50 | \t       1\t     |       0.03        |     0.808\n\nIn Table 1,\n- $c(A):=\\text{sum}(\\text{relu}(A\\mathbf{1} - \\mathbf{1}))$ is measuring whether the constraints are satisfied. The smaller the better.\n- validation F1 scores for T=20-50 are similar. This motivates us to choose T=20 for more efficient training. \n\n\nIn addition, we\u2019ve also tried to train the model for T=50 end-to-end. \nTable 2: T=20 vs T=50\n           | time per gradient step | best training loss | best validation loss | validation F1\nT = 20 | \t         0.64s\t               |\t       0.63\t          |\t       0.64                   |    0.88\nT = 50 | \t         0.98s\t               |\t       0.65            |           0.66\t                 |    0.86\n\nIn Table 2:\n- The training time is evaluated on the Titan Xp card with batch size = 8. (When the batch size is 16, it will be out of memory for the case T=50.)\n- The final performances are indeed similar. It is not that sensitive to the choice of T. \n- The performance for T=50 is a bit worse, which might be caused by the depth of the network, making the optimization harder.", "title": "The choice of the number of unrolling iterations T"}, "HJxLplGwor": {"type": "rebuttal", "replyto": "HJelWTWPsB", "comment": "We are happy that Thomas found our method novel, but we can not agree with some of his statements. \n\n***setting 1 is a reasonable and real-world setting***\n\n(1) We can not agree with the conclusion that: the comparison is unfair since E2Efold can incorporate information from seen sequences and others can not. The setting is fair as long as in practice we have access to data like that. Especially, the dataset of RNA structures is continually growing.\n\n(2) As we mentioned earlier, some methods we compared with use a fitted energy function. We note that these energy functions can be fitted from a much larger set of available datasets that are not used in E2Efold. In some sense, these methods may actually have an advantage over E2Efold in terms of datasets used for training. \n\n(3) We do not think traditional models are *designed for* setting 2 as Thomas stated. Instead, they are designed in that way and are commonly adopted because it is more interpretable and biologists care about interpretability. In contrast, deep learning models have better abilities to learn the common pattern from data and make more accurate predictions, while it is less interpretable. (However, DL is a hot topic and now there are many methods to explain the prediction of DL models.)\n\nIn conclusion, in terms of prediction accuracy, our comparison is fair and predicting structures for unseen sequences is the real-world setting.\n\n\n***predicting structures for unseen sequences in setting 1 is not a trivial task***\n\nPlease refer to the results of *CDPfold* in Table3 and the result of *$U_{\\theta}$+PP* in Table 6. Both of them are deep learning methods. If it is a trivial task and neural networks only need to memorize and overfit the training data, there won't be such a performance gap between those DL models and E2Efold. They should also work extremely well as long as the neural network is not so small. Especially, *$U_{\\theta}$+PP* has exactly the same architecture as E2Efold, except that the gradient is not pushed through the post-process during the training phase.\n\nTherefore, designing a suitable deep learning model to better learn the pattern of the structures is not easy. We should not take its performance for granted.\n", "title": "Setting 1 is a reasonable, read-world setting and the solution is not trivial"}, "HJelWTWPsB": {"type": "rebuttal", "replyto": "H1gcAa_yoS", "comment": "We thank Thomas for referring us to more related methods. We\u2019ve investigated and tried these methods (see more details below), although they are not commonly used as baselines for RNA folding.\n\n***Turbofold is extremely slow and performs particularly bad on long sequences***\n\nWe investigated the TurboFold package. It has serious flaws compared to our approach. TurboFold is extremely slow, requiring more than 15 minutes for only 5 sequences. This is true even for the parallel version, TurboFold-smp, for which we used 32 processors. Without multi-processing, the program won\u2019t return any results for just 5 16S-rRNA inputs in a reasonable time. The running time of TurboFold seems to increase exponentially in the sequence length. Also, to run TurboFold for a set of sequences, the RNA class for each sequence must be known. Its performance, 0.75 F1, is worse than E2Efold. In addition, it performs particularly bad on *long* sequences, such as 16S rRNA (F1=0.5). E2Efold achieves 0.78 F1 in this class. \n \nThe binary version of TurboFold has a specific input format requirement. We spent a very long time making the program successfully run on the whole testing dataset. Here is our code for running TurboFold: \nhttps://drive.google.com/open?id=1HouOnpkN1vUf_NkZ3s6uKY0KL-c-YQD_\n\n\n***Infernal can not be used to predict RNA secondary structure***\n\nWe\u2019ve looked into the Infernal package. However, we found that (this is also pointed out in [1]):\n\nInfernal incorporates *the information of RNA secondary structure* (including that of the input sequence) to build a better homolog model than others for *searching similar sequences*.\n\nTherefore, it is obvious that it can not be used to predict RNA secondary structure since it requires inputting the RNA secondary structure for the input sequence (for test).\n\n\n***we spent efforts on another more reasonable homology model***\n\nWe think it possible to use another homology model, e.g., BLAST [2]. We also incorporated the sequence alignment method in [3] to predict the structure. However, we found that for many sequences in the test set, BLAST fails to return a homolog from the training set. Even when we set the E value of the BLAST program as 10, which is expected to return some hits with low qualities, we did not receive any homolog hits for those sequences. That means the homology-based method can not resolve the RNA secondary structure prediction problem alone, even for those known RNA families, whose structures have presented in the database. \n\nHowever, we are still interested in the overall performance of the alignment-based method on our test set since its performance is usually *missed* in the previous publication, even in those papers about homology-based methods [4]. By combining BLAST, Clustal, and the contact map formulation from E2Efold, we managed to obtain the performance of the alignment-based methods, whose F1 score is 0.79. Indeed, this performance is good, but it\u2019s still worse than E2Efold. Notice that although we transferred some ideas from E2Efold to the alignment-based method to make it work, E2Efold\u2019s performance is still better than the mixed method, which suggests the effectiveness of our framework.\nIf you are interested in running this alignment-based method proposed by us, the code is here:\nhttps://drive.google.com/open?id=19d8nBYQx2qHEtEq-cMsmUkQ8bS9F-UEZ\n\n\n[1] Fallmann, Joerg, et al. \"Recent advances in RNA folding.\" Journal of biotechnology (2017)\n[2] Altschul, Stephen F., et al. \"Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.\" Nucleic acids research (1997)\n[3] Sievers, Fabian, and Desmond G. Higgins. \"Clustal Omega, accurate alignment of very large numbers of sequences.\" Multiple sequence alignment methods. 2014 \n[4] Zhen Tan, Yinghan Fu, Gaurav Sharma, David H. Mathews. \"TurboFold II: RNA structural alignment and secondary structure prediction informed by multiple homologs.\" Nucleic Acids Research (2017)", "title": "Results of the methods suggested by Thomas"}, "Bklpej_AKH": {"type": "review", "replyto": "S1eALyrYDH", "review": "*Summary*\nThe authors perform RNA secondary prediction using deep learning. The outputs are subject to hard constraints on which nucleotides can be in contact with others. They unroll a sophisticated optimization algorithm for a relaxation of the task of finding the optimal contact map subject to these constraints. This work is in a long line of work demonstrating that end-to-end training of models that incorporate application-specific optimization routines as sub-modules is very useful. In particular, it outperforms an approach where the inputs to this optimization problem come from a network that was trained using a simple loss that ignores the fact that it will feed into this structured optimizer. The paper also considers an application domain that will be unfamiliar to many ICLR readers interested in deep structured prediction, and may serve as a call to arms for the community engaging with additional problems in this field.   \n\n*Overall Assessment*\nThe paper is well written, well executed, and part of a general research thread that ICLR readers care about. There are a number of technical details, such as the loss function in (8) that will be of general interest. I advocate for acceptance.\n\n*Comments*\nThe actual specification of the output constraints doesn't occur until late in the paper. Before then, the discussion of them is very abstract. Given that the constraints are easy to describe, the exposition would be improved notably if you described the specific constraints earlier on. This would help me understand the problem domain better.\n\nFyi, the idea of nested structures vs. non-nested structures appears in NLP in terms of projective parsing vs. non-projective parsing. There may be some relevant reading for you to do there. Your specific work (minus the unrolled constraint enforcement) is similar to \"Dozat et al. 2017. Deep biaffine attention for neural dependency parsing.\"\n\nThe idea of backpropping through some constraint-enforcing process is reminiscent of backpropping through belief propagation. See, for example, Domke's \"Learning Graphical Model Parameters with Approximate Marginals Inference.\" Or Hershey et al. \"Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures.\" You should also cite work using unrolled ISTA to learn sparse coding dictionaries. They have terms similar to (5). \n\nWhat exactly was your motivation for the setup in \"Test On ArchiveII Without Re-training?\"\n\nHow sensitive is performance to the number of optimizer iterations? Does it work to train with a fixed number of unrolled iters, but at test time run the optimizer until convergence?\n\n(8) is cool!\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 4}, "SJlmuE8k5S": {"type": "review", "replyto": "S1eALyrYDH", "review": "The authors proposed an end-to-end method (E2Efold) to predict RNA secondary structure. The method consists of a Deep Score Network and a Post-Process Network (PPN). The two networks are trained jointly. The score network is a deep learning model with transformer and convolution layers, and the post-process network is solving a constrained optimization problem with an T-step unrolled algorithm. Experimental results demonstrate that the proposed approach outperforms other RNA secondary structure estimation approaches.\n\nOverall I found the paper interesting. Although the writing can be improved and some important details are missing.\n\nMajor comments\nAs the authors point out, several existing approaches for unrolling optimization problems have been proposed. It would be helpful to clarify the methodological novelty of the proposed algorithm compared to those.\n\n\nTraining details and implementation details are missing; these hinder the reproducibility of the proposed approach. The author stated pre-training of the score network, how is the PPN and score network updated during the joint training? Does the model always converge? The authors vaguely mentioned add additional logistic regression loss to Eq9 for regularization. What is a typical number of T? How does varying T affect the performance, both in terms of training time (and convergence) and in terms of accuracy/F1?\n\nMinor comments\nThe 29.7% improvement of F1 score overstates the improvements compared to non-learning approaches.. This performance was computed on the dataset (RNAStralign) on which E2Efold was trained. A fair comparison, as the authors also stated, is on the independent ArchiveII data. On this data, E2Efold has F1 score 0.686 versus 0.638 for CONTRAfold. The author should report performance improvement under this line.\n\n\nIt would be helpful to report performance per RNA category, both for RNAstralign data and ArchiveII data, while the ArchiveII data should still remain independent. Different models may have their strengths and weaknesses on different RNA types.\n\n\nIt is not obvious to me how the proximal gradient was derived to (3)-(5). It would be helpful if the authors show some details in the supplements.\n\n\nWhy is there a need to introduce an l_1 penalty term to make A sparse?\n\n\nOn which data is Table 6?\n\nTypos, etc.\nThe references are not consistently formatted\n\u201cstructure a result\u201d -> \u201cstructure is a result\u201d\n\u201ca few hundred.\u201d -> \u201ca few hundred base pairs.\u201d\n\u201cobjective measure the\u201d -> \u201cobjective measures the\u201d\n\u201csection 5\u201d -> \u201cSection 5\u201d (in several places)\nIn the equation above Equation 2, should it be -\\rho||\\hat{A}||_{1} instead of plus? Otherwise, the \u201cmax\u201d could be made arbitrarily large.\n", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 1}, "S1xh9zkV9H": {"type": "review", "replyto": "S1eALyrYDH", "review": "This paper introduces an end-to-end method to predict the secondary structure of RNA, by mapping the nucleotide sequence to a binary affinity matrix. The authors decompose this problem into two part: (i) predicting an affinity score between each base pair in the input sequence, using a combination of a transformer sequence encoder network and a convolutional decoder, and (ii) a post-processing step that ensures that structural local and global constraints are enforced. An innovation is to express this post-processing as an unrolled sequence of proximal gradient descent steps, which are fully differentiable, and allow the full combination of (i)+(ii) to be trained end-to-end. A thorough set of experiments validate the approach.\n\nOverall, the paper is well written and easy to follow. The approach of unrolling structural constraints as shown in the paper is interesting and applicable to much wider domains than secondary structure prediction. The proposed approach appears to provide a novel, convincing and non-obvious solution to RNA secondary structure prediction, and subject to suggestions below, would represent a valuable contribution to ICLR.\n\nThe principal area for improvement would be to include additional detail (perhaps in appendix) on the model hyperparameter configurations that were used in the experiments. Moreover, more details on the set of \\psi functions, and the MLP details for P_i (e.g. number of hidden units, activation function, the use of dropout, batch normalization, etc) should be given, as well as more information on the specifics how how the \u201cpairwise concatenation\u201d is carried out in the output layer. What unrolling constant T is used? Finally, in the ablation study (p. 8) details on how U_\\theta is trained by itself (without the post-processing step) should be given. \n\nDetailed comments:\n* Overall, the whole paper should be thoroughly reviewed for English grammar and writing style; a subset of suggested changes follow.\n* p. 1: structure a result ==> structure is a result\n* p. 2: energy based methods ==> energy-based methods\n* p. 2: energy function based approaches ==> energy function-based approaches\n* p. 2: view point ==> viewpoint\n* p. 2: E2Efold is flexible ==> E2Efold are flexible\n* p. 2: nearly efficient ==> nearly efficiently\n* p. 3: typically scale ==> typically scale as\n* p. 3: few hundred. ==> few hundreds.\n* p. 4: all binary matrix ==> all binary matrices\n* p. 4: output space can help ==> output space could help\n* p. 5: formulation are the ==> formulation are that the\n* p. 6: eq. (7) should contain quantities indexed by $t$ in the RHS\n* p. 8: pesudoknotted ==> pseudoknotted\n* p. 9 ff: in the bibliography, all lowercase rna should be uppercase RNA. Use {RNA} in bibtex entries.\n", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 1}, "r1e3VSpiqr": {"type": "rebuttal", "replyto": "Hyx-fHaiqS", "comment": "\n\n*** Probknot vs RNAstructure\n\n\"The authors seem to report Probknot as RNAstructure, but the latter (by default) can't predict pseudoknots.\" : this statement is not true! \n\nRNAstructure web-server has been constantly updated, and Probknot is now included in the RNAstructure web-server: \nhttps://rna.urmc.rochester.edu/RNAstructureWeb/\nIt does predict pseudoknots as shown in visualizations and table 5. \n\n*** Data requirement for DL-based methods\n\nThe training data requirement cannot be avoided for DL models, so we do not deny the fact (in the discussion section) that E2Efold does not perform well on telomerase where there are only 37 data points. This problem needs to be resolved by future studies on few-shot learning or curriculum learning. Meanwhile, we expect that such training data will be accumulating in the future, too. \n\nFor the past decades, DP-based algorithms have dominated the RNA secondary structure prediction. We believe that it is worth trying to apply deep learning to this problem even when some RNA families do not contain enough data at this moment. Furthermore, our methods present a more sensible bias-variance trade-off (deep while incorporating problem structure in the architecture), and thus a better result is reasonable.  \n\n*** Methodology contribution\n\nFinally, we believe that the methodology proposed in our paper is of broad interest to the audience of the ML community. Though our work is originally motivated by the challenges in the RNA folding problem, we believe our ideas of\n(i) incorporating hard constraints to reduce the output space and thus making it more data-efficient; and\n(ii) using an unrolled algorithm for solving constrained programming as a building block in a neural network\nwill be inspiring for other structured prediction problems and potentially useful for architecture design in a wider range of DL problems. Therefore, we think that our work is well-suited for ICLR.", "title": "Response: a further clarification (part 2)"}, "Hyx-fHaiqS": {"type": "rebuttal", "replyto": "B1xm6dZqqS", "comment": "\nWe thank Liang Huang for your comments. We have carefully designed our experiments, and clarified further the settings below:\n\n\n*** Generalization ability\n\nArchiveII is a separate held-out test dataset. E2Efold is only learned from RNAStralign training set, but can directly generalize to ArchiveII, and obtain the best test results. \n\nWe note that LinearFold is not a learning-based method, but one energy function used in LinearFold is a learning-based energy from another paper (i.e. CONTRAfold), which can potentially be fitted from a much larger number of available datasets. Thus the per-family error of LinearFold is not a cross-validation error in the strict learning sense.  \n\n\n*** No redundant sequences\n\nAs stated in Sec 5, \"After removing redundant sequences and structures...\", we've carefully removed redundant sequences and structures, which reduces the RNAStralign dataset size from 37149 to 30451. Also, when we test the learned model on ArchiveII, we've excluded sequences that are overlapped with the RNAStralign dataset.\n\nThe similarity between sequences is a characteristic of the dataset. Some level of similarity between sequences is also a basis for model generalization. \n\n\n*** Accuracy dominated by short-sequence?\n\nFor long sequences, E2Efold still performs better than other methods. In fact, we've computed F1 scores *weighted by the length of sequences* (Table 1), such that the results are more dominated by longer sequences (we've conducted this experiment before the submission, but did not report them in the original submission due to page limit). For instance, for the RNAStralign dataset: \n\nTable 1. RNAStralign: F1 after a weighted average by sequence length.\n                 |E2Efold|CDPfold|LinearFold| Mfold |RNAstructure|RNAfold|CONTRAfold\noriginal   |  0.821  |  0.614   |    0.609     |  0.420  |       0.550       |   0.540  |     0.633\nweighted|  0.720  |  0.691   |    0.509     |  0.366  |       0.471       |   0.444  |     0.542\nchange    |-12.3% | +12.5% |   -16.4%   | -12.8% |     -14.3%       | -17.7%  |   -14.3% \n\n3rd row reports how much F1 score drops after reweighting. \n\n\n*** Subsequences\n\nSince subsequences in ArchiveII are explicitly labeled, we filtered them out in ArchiveII and recomputed the F1 scores (Table 2) as Liang suggested. \n\nTable 2. ArchiveII: F1 after subsequences are filtered out.\n              |E2Efold|CDPfold|LinearFold| Mfold |RNAstructure|RNAfold|CONTRAfold\noriginal| 0.704   |  0.597   |    0.647     |  0.421 |        0.613       |   0.615   |    0.662\nfiltered | 0.723   |  0.605   |    0.645     |  0.419 |        0.611       |   0.615   |    0.659\n\nThe results do not change too much before or after filtering out subsequences.\n\nIn the original submission, we do not exclude subsequences by default because we want to follow RNA structure prediction benchmark. More specifically, \n- as suggested in the latest review by [Mathews, 2019], \"The collection of benchmarking structures we collected, called ArchiveII, is available for download from our lab website at https://rna.urmc.rochester.edu/publications.html\", ArchiveII is considered the benchmarking dataset and by default both domain sequences and full sequences are included. \n- the subsequences in ArchiveII are domains, whose secondary structure can be predicted independently since under most circumstances there is no cross-domain base-pairing.\n\n[Mathews, 2019] How to benchmark RNA secondary structure prediction accuracy, Methods, 2019\n\n\n*** Runtime comparison\n\nIn practice, training needs to be done much less often than testing, and hence we focus on reporting the testing time. For instance, deep learning models for Imagenet need to be trained for a long time, but inference (testing) occurs more often, and lots of researches are done for improving inference time.  \n\nFurthermore, it is undeniable that Dynamic Programming (DP) based algorithms are not easy to be parallelized or sped up by GPU due to their *sequential decision* nature. Thus, the runtime of DP-based algorithms has a significant dependency on the length n. \n\nIn contrast, the operations in E2Efold are mainly *matrix computations*, which can be easily sped up by GPU using modern programming languages such as pytorch and tensorflow.\n\nIndeed, E2Efold will have even more advantages if we only report the runtime on long sequences. For example, we run E2Efold and LinearFold for sequences of length from 1500 to 1800 (\"Long\" in the table):\n          |E2Efold (Python)|LinearFold (C)| \n    All |           0.40s          |        0.43s        |\nLong|           0.41s          |        1.25s        |\n\nIn addition, other methods are even implemented in C, which is by nature faster than Python that we used for E2Efold.", "title": "Response: a further clarification (part 1)"}}}